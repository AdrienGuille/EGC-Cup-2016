Partition BIC optimale de l’espace
des pre´dicteurs
Gilbert Ritschard
∗De´partement d’e´conome´trie, Universite´ de Gene`ve
gilbert.ritschard@themes.unige.ch
Re´sume´. Cet article traite du partitionnement optimal de l’espace de
pre´dicteurs cate´goriels dans le but de pre´dire la distribution a poste-
riori d’une variable re´ponse elle-meˆme cate´gorielle. Cette partition op-
timale doit re´pondre a` un double crite`re d’ajustement et de simplicite´ que
prennent pre´cise´ment en compte les crite`res d’information d’Akaike (AIC)
ou baye´sien (BIC). Apre`s avoir montre´ comment ces crite`res s’appliquent
dans notre contexte, on s’inte´resse a` la recherche de la partition qui mi-
nimise le crite`re retenu. L’article propose une heuristique rudimentaire
et de´montre son efficacite´ par une se´rie de simulations qui comparent le
quasi optimum trouve´ au vrai optimum. Plus que pour la partition elle-
meˆme, la connaissance de cet optimum s’ave`re pre´cieuse pour juger du
potentiel d’ame´lioration d’une partition, notamment celle fournie par un
algorithme d’induction d’arbre. Un exemple sur donne´es re´elles illustre ce
dernier point.
1 Introduction
En apprentissage supervise´, des techniques comme l’analyse discriminante, la re´gres-
sion logistique multinomiale, les mode`les baye´siens ou les arbres de de´cisions induits
de donne´es (arbres d’induction) apprennent la distribution a posteriori de la variable
a` pre´dire, l’objectif e´tant d’affecter un cas avec profil x en termes de pre´dicteurs a` la
classe yi ayant la plus forte probabilite´ a posteriori p(Y =yi|x). La re´gression logistique,
l’analyse discriminante et les mode`les baye´siens par exemple, mode´lisent la distribution
a posteriori sous forme d’une fonction vectorielle continue de x. Par contraste, les arbres
d’induction conduisent a` un ensemble fini de distributions, chaque distribution e´tant
associe´e a` une classe d’une partition «apprise» de l’ensemble des profils x admissibles.
Nous nous plac¸ons dans ce dernier contexte et nous inte´ressons a` la de´termination de la
partition optimale. Nous examinons tout d’abord les crite`res d’optimalite´ qui peuvent
s’ave´rer pertinents. Parmi ceux-ci, nous porterons un inte´reˆt particulier aux crite`res
d’information du type AIC et BIC qui permettent d’arbitrer entre qualite´ d’ajuste-
ment et complexite´. Le calcul des AIC et BIC pour une partition quelconque se fait
par simple adaptation du principe de l’arbre e´tendu introduit dans Ritschard et Zighed
(2003, 2002) pour le cas des arbres.
La recherche de la partition optimale par exploration exhaustive des partitions
e´tant de complexite´ non polynomiale, il convient de recourir a` des heuristiques. Pour
cette premie`re approche du proble`me, on envisage ici une proce´dure ascendante dont
on examine les performances par une analyse de simulations. L’heuristique est rudi-
Patition BIC optimale
mentaire, mais s’ave`re suffisamment performante pour traiter jusqu’a` une centaine de
profils diffe´rents.
La porte´e du concept meˆme de partition optimale est quelque peu limite´e en raisons
de possibles difficulte´s d’interpre´tation. Contrairement aux partitions ge´ne´re´es par les
arbres, la description d’une partition quelconque peut ne´cessiter en effet des combinai-
sons souvent difficilement interpre´tables de conditions. L’optimum fournit cependant
et dans tous les cas des indications pre´cieuses sur le potentiel d’ame´lioration qu’on peut
apporter a` une partition. Cet inte´reˆt de la partition globalement optimale est illustre´
par une e´tude de la re´ussite des e´tudiants de premie`re anne´e a` la Faculte´ des sciences
e´conomiques de Gene`ve.
L’article est organise´ comme suit. La section 2 introduit le cadre formel et les
notations. La section 3 pre´cise le concept de partition optimale et de´finit formellement
les crite`res a` optimiser. La section 4 traite de la de´termination de l’optimum. On y
propose une heuristique dont on analyse empiriquement l’efficacite´ avec des simulations.
La section 5 explicite les limites et l’inte´reˆt du concept de partition optimale qui est
illustre´ a` la section 6. Enfin, la section 7 donne quelques pistes de recherche future.
2 Cadre formel et notations
On se place dans le cadre de l’apprentissage supervise´ avec une variable re´ponse
Y et p pre´dicteurs x1, . . . , xp et une base de donne´es d’apprentissage de taille n. On
conside`re plus pre´cise´ment le cas ou` la variable a` pre´dire Y et les pre´dicteurs sont tous
cate´goriels. On note ` le nombre de valeurs distinctes de Y , ck, le nombre de valeurs
distinctes du pre´dicteur xk, k = 1, . . . , p, et c ≤
∏
k ck le nombre de profils admissibles
x = (x1, . . . , xp).1
Si l’objectif de l’apprentissage est en re`gle ge´ne´rale la construction d’un classifieur
f(x) qui attribue un et un seul e´tat de la variable Y a` chaque profil x, les connaissances
recherche´es peuvent dans certains cas porter sur les probabilite´s des divers e´tats de Y
conditionnellement aux valeurs x des pre´dicteurs. Ceci est en particulier le cas dans
les sciences de comportement (sociologie, sciences politiques, marketing, histoire, ...)
qui cherche a` de´crire les me´canismes qui re´gissent les phe´nome`nes e´tudie´s. Nous nous
plac¸ons dans ce contexte est conside´rons donc le proble`me de la pre´diction de la distri-
bution de probabilite´ a posteriori de Y , c’est-a`-dire de p(Y |x) = (p1(x), . . . , p`(Y x)),
ou` l’on note pi(x) la probabilite´ p(Y = yi|x).
Notons que de nombreuses techniques de classification s’appuient de fac¸on plus ou
moins explicite sur la distribution a posteriori p(Y |x), la classification consistant a`
attribuer les cas a` la cate´gorie la plus probable argmaxi pi(x) compte tenu de x. C’est
le cas notamment de la re´gression logistique, de l’analyse discriminante line´aire ou
quadratique, des k plus proches voisins, des arbres et graphes d’induction ou encore
des re´seaux baye´siens. Cet aspect est en particulier bien mis en e´vidence dans Hastie
et al. (2001).
Dans le cas de variables cate´gorielles, les donne´es d’apprentissage peuvent eˆtre re-
pre´sente´es de fac¸on synthe´tique sous forme d’une table de contingence T de taille `× c
1Le croisement de tous les pre´dicteurs peut donner lieu a` des profils non admissibles d’effectif
structurellement nul, par exemple (homme, enceinte). Ceci explique l’ine´galite´ utilise´e ici.
RNTI - 1
Gilbert Ritschard
croisant la variable re´ponse y avec les profils x. Par exemple, la table 1 pre´sente un jeu
de 240 donne´es utilise´es pour e´tudier la distribution du statut marital selon le sexe et
le secteur d’activite´.
homme femme
marie´ primaire secondaire tertiaire primaire secondaire tertiaire total
non 50 40 6 0 14 10 120
oui 5 5 12 50 30 18 120
total 55 45 18 50 44 28 240
Tab. 1 – Exemple de table de contingence T
Un e´le´ment de la table T est note´ nij et repre´sente le nombre de cas avec profil
xj qui dans les donne´es prennent la valeur yi de la variable re´ponse. Les totaux des
lignes et des colonnes sont respectivement note´s ni· et n·j . Les estimations du maximum
de vraisemblance p˜(Y = yi|xj) = nij/n·j , c’est-a`-dire les distributions colonnes de la
table T donnent l’information la plus fine sur les distributions conditionnelles de la
variable re´ponse au sein de l’ensemble d’apprentissage. Aucun degre´ de liberte´ n’est
laisse´ dans ces estimations qui, e´tant alors entie`rement lie´es a` l’e´chantillon peuvent
eˆtre tre`s instables lorsqu’on change d’e´chantillon d’apprentissage. Pour obtenir des
estimations plus stables et donc plus pertinente en ge´ne´ralisation, il convient de gagner
des degre´s de liberte´ ce qui peut eˆtre fait en regroupant des colonnes ou si l’on veut
en partitionnant l’ensemble des profils x admissibles. Se pose alors naturellement la
question de l’optimalite´ de la partition ainsi que de sa pertinence.
3 Le concept de partition optimale
On se propose ici de pre´ciser les proprie´te´s que l’on attend de la partition cherche´e
est plus formellement d’introduire les crite`res qu’il s’agira d’optimiser.
Intuitivement, la partition cherche´e doit
1. re´duire autant que possible l’incertitude quant a` la valeur prise par Y dans chaque
classe ;
2. avoir le plus petit nombre de classes pour assurer une meilleure stabilite´ des
estimations en laissant le plus de degre´s de liberte´ possibles.
De plus, en particulier lorsqu’on s’inte´resse plus a` la compre´hension des relations
de de´pendance qu’a` la classification, la partition devrait permettre une caracte´risa-
tion aussi simple que possible des classes pour en facilite´ l’interpre´tation. Nous nous
concentrons dans un premier temps sur les points e´nume´re´s ci-dessus et reviendrons
sur l’aspect interpre´tation a` la section 5.
La re´duction de l’incertitude ne peut eˆtre juge´e que par rapport aux donne´es d’ap-
prentissage. Selon le point 1, on attend donc de la partition qu’elle reproduise le mieux
possible les distributions colonne de la table T observe´e, ce qui est un proble`me de
qualite´ d’ajustement. Quant au point 2, il a trait a` la complexite´ de la partition qui
doit eˆtre le plus simple possible pour assurer la meilleure stabilite´ des distributions
RNTI - 1
Patition BIC optimale
estime´es. Ce dernier objectif s’oppose au premier dans la mesure ou` tout affinement de
la partition ne peut que re´duire l’incertitude.
Mesure de la qualite´ d’ajustement
Examinons tout d’abord le proble`me de l’ajustement. La qualite´ d’ajustement des
distributions observe´es (les colonnes de la table T) peut se mesurer selon le principe
du tableau e´tendu de´fini dans Ritschard et Zighed (2003) pour le cas particulier des
partitions produites par les arbres. Ce principe est le suivant.
Soit T la table cible. Pour une partition des profils (colonnes de T) en q < c classes,
on ge´ne`re la tableTa de dimension `×q qui croise la variable re´ponse y avec la partition.
L’objectif e´tant de juger de la qualite´ de l’ajustement de la table cible par cette table
re´duite, on transforme cette dernie`re en une table e´quivalente de meˆme dimension que
T. La transformation consiste a` ventiler chaque e´le´ment naik de T
a entre les colonnes qui
forment la k-e`me classe de la partition, la ventilation se faisant proportionnellement
aux fre´quences marginales des profils concerne´s. On obtient ainsi la table pre´dite Tˆ
d’e´le´ment ge´ne´rique
nˆij =
n·j∑
j′∈Jk n·j′
naik (1)
ou` Jk est l’ensemble des indices des colonnes de T qui sont regroupe´es dans la meˆme
classe k de la partition. Pour l’exemple du tableau 1, la partition{ {(hom,pri),(hom,sec)}, {(hom,ter),(fem,sec),(fem,ter)}, {(fem,pri)} }
conduit ainsi a` la table pre´dite Tˆ donne´e au tableau 2.
homme femme
marie´ primaire secondaire tertiaire primaire secondaire tertiaire total
non 49.5 40.5 6 0 14.67 9.33 120
oui 5.5 4.5 12 50 29.33 18.67 120
total 55 45 18 50 44 28 240
Tab. 2 – Table pre´dite Tˆ
La qualite´ de l’ajustement de T peut eˆtre e´value´e par des statistiques de divergence
du khi-2 Cressie et Read (1984), en particulier par les statistiques X2 de Pearson ou
G2 du rapport de vraisemblance :
G2 = 2
∑`
i=1
c∑
j=1
nij ln
(
nij
nˆij
)
, X2 =
∑`
i=1
c∑
j=1
(nij − nˆij)2
nˆij
.
Sous l’hypothe`se que le mode`le est correct, c’est-a`-dire que la partition mode´lise cor-
rectement les distributions conditionnelles, et sous certaines conditions de re´gularite´,
voir par exemple Bishop et al. (1975, chap. 14), ces statistiques suivent une meˆme dis-
tribution du khi-2. Nous avons montre´ dans Ritschard et Zighed (2003) que les degre´s
de liberte´ sont dans ce cas (c− q)(`− 1).
RNTI - 1
Gilbert Ritschard
La divergence entre les tableaux Tˆ et T des tableaux 2 et 1 est par exemple de
G2 = 0.23 (on a aussi X2 = 0.23), pour 3 degre´s de liberte´ ce qui donne un degre´ de
signification de plus de 97% et indique un ajustement presque parfait.
On peut songer a` d’autres indicateurs pour juger de l’e´cart entre les deux tableaux,
par exemple, une diffe´rence simple ou normalise´e d’entropie entre les deux tables. L’inte´-
reˆt des statistiques du khi-2 est qu’elles permettent, lorsque les conditions de re´gularite´
sont satisfaites, de tester la significativite´ statistique de la divergence.
Arbitrage avec la complexite´
La partition optimale du seul point de vue de la qualite´ de l’ajustement est e´videm-
ment la partition la plus fine qui pre´dit exactement la table cible. Comme mentionne´
plus haut, il convient de tenir e´galement compte de la taille de la partition. Une stra-
te´gie peut ainsi consister a` chercher la partition de taille minimale qui assure une
de´viance non statistiquement significative. Le proble`me avec cette approche est double
comme le souligne en particulier Raftery (1995). D’une part, lorsque n devient grand le
moindre e´cart devient statistiquement significatif. D’autre part, de multiples mode`les
(il faut entendre partitions dans notre cas) ajustent de fac¸on satisfaisante la table cible.
On se trouve de`s lors confronte´ a` une incertitude quant au bon mode`le. Kass et Raf-
tery (1995) ont montre´ que la minimisation du crite`re BIC permet dans une approche
baye´sienne de minimiser l’incertitude lie´e au mode`le pour les donne´es observe´es. Ce
crite`re initialement introduit par Schwarz (1978), s’exprime dans notre cas comme la
combinaison de la de´viance G2 et d’une pe´nalisation pour la complexite´ mesure´e par
(q`− q + c) (voir Ritschard et Zighed, 2003). Le crite`re AIC de Akaike (1983) est une
variante qui pe´nalise la complexite´ moins fortement et inde´pendamment de n.
AIC = G2 + 2(q`− q + c) et BIC = G2 + (q`− q + c) log(n) .
La minimisation de l’un ou l’autre de ces crite`res re´pond parfaitement a` notre
objectif d’optimalite´ de la partition.2
4 De´termination de la partition optimale
L’exploration exhaustive de toutes les partitions est de complexite´ non polynomiale
(np-complet). Le nombre B(c) de partitions des c profils est donne´ par la formule
ite´rative de Bell (1938) B(c) =
∑c−1
k=0
(
c−1
k
)
B(k). La figure 1 montre clairement que ce
nombre explose totalement au dela` d’une dizaine de profils.
Limite de l’approche par les arbres
Parmi les diverses me´thodes d’apprentissage, les arbres d’induction ont la particu-
larite´ de travailler avec un nombre fini de distributions a posteriori p(Y |x) par opposi-
tion avec des approches comme la re´gression logistique ou l’analyse discriminante qui
2Dans les situations classiques, le crite`re AIC est connu pour eˆtre biaise´. Par exemple, dans le
cadre des mode`les line´aires, AIC asymptotiquement se´lectionne des mode`les plus complexes que le
vrai mode`le.
RNTI - 1
Patition BIC optimale
0 10 20 30 40 50
70000
60000
50000
40000
30000
20000
10000
0
nombre c de profils
partitions explore´es
exhaustif
pas a` pas
Fig. 1 – Nombre de partitions explore´es par la proce´dure exhaustive et borne supe´rieure
de ce nombre pour l’approche pas a` pas
impliquent des fonctions continues de x. Les arbres sont donc bien adapte´s a` notre
contexte. Ils construisent les partitions de fac¸on descendante. En partant du nœud
initial constitue´ de tous les profils, ils proce`dent par e´clatements successifs des nœuds
jusqu’a` ce qu’un crite`re d’arreˆt soit atteint. Les e´clatements successifs, c’est-a`-dire pour
chaque nœud le choix d’un pre´dicteur et le partitionnement du nœud selon les modalite´s
de ce pre´dicteur, se font par optimisation d’un crite`re local, par exemple la significati-
vite´ d’un khi-2 dans CHAID Kass (1980) ou le ratio de gain dans C4.5 Quinlan (1993).
 
 
 
 
 

G46
	 
 
 G46
  

 
 
	
 

 	

 

 
 
 
 
 
 

 

 
 
 
	 


Fig. 2 – Pre´diction de la distribution du statut marital : une partition de l’espace des
pre´dicteurs non re´alisable avec un arbre
Dans l’optique de de´terminer la partition globalement optimale, on peut songer
a` remplacer le crite`re local par un crite`re global du type AIC ou BIC. Les arbres
pre´sentent cependant l’inconve´nient de ne pas pouvoir ge´ne´rer toutes les partitions et
donc de rater e´ventuellement la partition globalement optimale. La figure 2 par exemple,
illustre une partition qui ne peut pas eˆtre obtenue avec un arbre, tandis que la figure 3
RNTI - 1
Gilbert Ritschard
 
 
 
 

 

G46

  

 
 G46
  

 
 
	
 

 	

 

 
 
 
 
 
 

 

 
 


Fig. 3 – Pre´diction de la distribution du statut marital : une partition de l’espace des
pre´dicteurs re´alisable avec un arbre
caracte´rise le type de partition produite par un arbre. On notera que la partition de
la figure 2 est pre´cise´ment celle qui donne lieu a` la table pre´dite Tˆ du tableau 2. Les
valeurs de AIC et BIC des deux partitions illustre´es sont respectivement de 18.2 et 49.6
pour la partition de la figure 2 contre 20.2 et 55 pour celle de la figure 3. On a donc
ici un exemple de situation ou` l’on ne peut pas atteindre la partition optimale avec un
arbre.
Proce´dure pas a` pas arrie`re
Une solution alternative est de proce´der par regroupements successifs deux a` deux
des classes en partant de la partition la plus fine jusqu’a` ce que le crite`re AIC ou BIC
ne puisse plus eˆtre re´duit. Il s’agit d’une version simplifie´e de l’heuristique propose´e
dans Ritschard et al. (2001) et e´tudie´e dans Ritschard (2001) ou` l’on explorait simul-
tane´ment les regroupements en lignes et en colonnes. Les regroupements se font ici
uniquement sur les colonnes (profils) de la table cible T. En effet, des regroupements
sur la variable a` pre´dire modifieraient la nature des distributions a` pre´dire et rendrait
caduque la comparaison des AIC ou BIC.
Performance de l’heuristique
La borne supe´rieure du nombre de cas explore´s par l’heuristique est 1+
∑c
i=2
(
i
2
)
=
1+ c(c
2−1)
6 et sa complexite´ est donc polynomiale en O(c
3). C’est e´videmment plus
complexe que les arbres, mais suffisant tant que le nombre de partitions admissibles c
n’exce`de pas une centaine, contre 8 ou 9 pour la proce´dure exhaustive. On peut voir
l’e´volution de cette borne supe´rieure avec c dans la figure 1.
Pour juger de la capacite´ de l’heuristique a` trouver la partition globalement opti-
male, nous avons proce´de´ par simulation. Une se´ries de 200 tables de contingence de
n = 10′000 cas ont e´te´ ge´ne´re´es ale´atoirement. On a ge´ne´re´ des tables de ` = 4 lignes
et c = 7 colonnes, le nombre 7 de colonnes e´tant le plus grand qui permette d’appli-
quer la proce´dure exhaustive sur les 200 tables en un temps raisonnable. Pour chaque
table nous avons compare´ la valeur du crite`re (AIC ou BIC) quasi-optimale trouve´e
par l’heuristique a` la valeur optimale de´termine´e avec la proce´dure exhaustive.
RNTI - 1
Patition BIC optimale
En allouant les 10′000 cas entre les 4 · 7 = 28 cases du tableau selon une distribu-
tion uniforme, on ge´ne`re des tables caracte´rise´es par une absence totale de structure
pour lesquelles tout regroupement de profils donne lieu a` un de´ficit d’ajustement trop
important pour pouvoir eˆtre compense´ par la re´duction de taille de la partition. De
fac¸on donc non surprenante, l’heuristique a trouve´ les BIC et AIC optimaux dans les
200 cas, la partition optimale e´tant dans presque tous les cas la partition la plus fine, la
valeur moyenne des 200 crite`res optimaux e´tant de 254.8 contre 257.8 pour la partition
la plus fine.
Pour introduire un peu de structure, nous avons proce´de´ a` une seconde se´ries de
simulation en ge´ne´rant les tables selon des distributions uniformes conditionnelles em-
boˆıte´es : un pourcentage ale´atoire entre 0 et 100% des cas est alloue´ a` la premie`re ligne,
puis un pourcentage ale´atoire des cas restant a` la seconde ligne et ainsi de suite jusqu’a`
la dernie`re ligne, le total de chaque ligne e´tant ensuite re´parti de la meˆme fac¸on dans
la ligne. On ge´ne`re ainsi des tables ou` les gros effectifs ont tendance a` se concentrer en
haut et a` gauche.
AIC BIC
Ecarts non nuls seulement :
proportion 6% 12.5%
maximum 1.55 8.79
moyenne 0.62 1.62
e´cart type 0.44 1.73
asyme´trie 0.48 0.979
Ensemble des e´carts nuls et non nuls :
moyenne 0.04 0.20
e´cart type 0.18 0.95
asyme´trie 5.72 6.21
Ecarts relatifs : maximum 0.035 0.048
moyenne 0.013 0.009
Valeur initiale du crite`re 56 257.9
Moyenne des optima globaux 50.98 213.5
Tab. 3 – Simulations : e´carts entre optima et quasi-optima du AIC et du BIC
Le tableau 3 re´sume les re´sultats de ces simulations. On constate que si la proportion
d’optima manque´s est significative avec respectivement 6% et 12.5%, les e´carts entre la
solution quasi-optimale de l’heuristique et l’optimum global reste faible dans tous les cas
avec un e´cart relatif maximal de 1.3% pour l’AIC et de 4.8% pour le BIC. De meˆme,
les e´carts moyens restent faibles en regard de la re´duction moyenne (respectivement
5.02 et 44.4) de la valeur initiale du crite`re. Nous avons obtenus des re´sultats tre`s
similaires, en fait meˆme le´ge`rement moins bons, avec des tables 4× 6. Ceci est plutoˆt
encourageant, dans la mesure ou` cela indique que ces performances ne semblent pas
devoir se de´te´riorer lorsque c augmente.
RNTI - 1
Gilbert Ritschard
5 Porte´e et limites du concept de partition optimale
On a examine´ jusqu’ici comment de´finir formellement la partition optimale et les
possibilite´s et difficulte´s que soule`vent sa de´termination. Nous nous proposons mainte-
nant de revenir sur l’aspect interpre´tation en discutant brie`vement les enseignements
que nous apporte la connaissance de cet optimum global.
En premier lieu, il importe de pre´ciser que dans notre approche la complexite´ est
mesure´e en termes de taille de la partition. Ceci n’assure pas la simplicite´ de description
des classes comme l’illustrent notamment les exemples des figures 2 et 3, la partition en
quatre de la seconde figure e´tant plus facile a` de´crire que celle en trois de la premie`re.
La difficulte´ dans ce dernier cas tient au fait que l’une des classes est de´finie par une
alternative de conditions et non uniquement en termes de conjonctions de conditions
comme dans le cas des arbres.
Si cette difficulte´ de de´crire simplement la partition optimale limite la porte´e du
concept de partition optimale, la connaissance de la valeur optimale du crite`re (AIC ou
BIC) fournit par contre une indication pre´cieuse pour juger de la qualite´ d’une solution
produite par un arbre d’induction. En effet, une solution proche de la valeur optimale
nous indiquera qu’on a peu de chances de pouvoir ame´liorer les choses en jouant sur
les parame`tres de controˆle, tandis qu’un grand e´cart par rapport a` la valeur optimale
pourra justifier des efforts dans ce sens. Dans cette optique nous sugge´rons de calculer
les deux valeurs optimales AIC et BIC ainsi que la taille des partitions correspondantes.
6 Illustration
Afin d’illustrer les enseignements apporte´s par les AIC et BIC optimaux, nous
donnons quelques re´sultats obtenus avec des donne´es relatives aux 762 e´tudiants qui
ont commence´ leur premie`re anne´e d’e´tudes a` la Faculte´ des sciences e´conomiques et
sociales de Gene`ve en 1998. Il s’agit de donne´es administratives re´unies par Petroff
et al. (2001). L’objectif e´tait d’e´valuer les chances de respectivement re´ussir, redoubler
ou eˆtre e´limine´ a` la fin de la premie`re anne´e d’e´tudes. La figure 4 montre l’arbre obtenu
avec la proce´dure CHAID (Kass, 1980) imple´mente´e dans Answer Tree (SPSS, 2001).
Parmi une trentaine de pre´dicteurs potentiels, CHAID en a se´lectionne´ 5 dont deux
quantitatifs, l’anne´e d’immatriculation a` l’universite´ et l’aˆge a` l’obtention du diploˆme
de l’e´cole secondaire. Les cinq variables avec les discre´tisations et regroupements de
modalite´s propose´s par CHAID sont le type de diploˆme secondaire (3 modalite´s), l’aˆge
de son obtention (4), la date d’immatriculation (2), le tronc commun choisi (2) et la
nationalite´ (2). La table cible de´finie par ces variables contient 88 colonnes. Elle a 3
lignes correspondant aux 3 situations possibles de l’e´tudiant apre`s un an d’e´tude.
La valeur du AIC et du BIC pour la partition de´finie par l’arbre est donne´e dans le
tableau 4. On donne e´galement la taille de la partition, la de´viance G2 avec ses degre´s de
liberte´ et son degre´ de signification, ainsi que le pseudo R2 ajuste´. Ces valeurs peuvent
eˆtre compare´es a` celles de deux variantes, CHAID2 qui est CHAID sans l’e´clatement du
sommet 4 (nationa /∈ {GE, hors Europe}) et CHAID3 sans l’e´clatement des sommets 4
et 5 (nationa ∈ {GE, hors Europe}). Le mode`le sature´ correspond a` la partition la plus
fine et le mode`le d’inde´pendance au cas ou` tous les profils sont regroupe´s en seul groupe.
RNTI - 1
Patition BIC optimale
pseudo
Mode`le q d G2 sig(G2) R2ajust AIC BIC
Sature´ 88 0 0 1 1 528 1751.9
Meilleur AIC 14 148 17.4 1 .941 249.4 787.2
CHAID 9 158 177.9 0.133 .336 390.0 881.3
CHAID2 8 160 187.4 0.068 .309 395.4 877.5
CHAID3 7 162 195.2 0.038 .289 399.2 872.1
Meilleur BIC 6 164 75.2 1 .745 275.2 738.8
Inde´pendance 1 174 295.1 0.000 0 475.8 892.3
Tab. 4 – SES 98 : qualite´s d’ajustement d’un choix de mode`les
On constate tout d’abord que CHAID et ses deux variantes ont des AIC ou BIC tre`s
voisins, sensiblement meilleurs que ceux du mode`le sature´ et dans une moindre mesure
que ceux du mode`le d’inde´pendance. A priori il est difficile de dire si pour ame´liorer
la partition il vaut mieux l’affiner ou au contraire re´duire sa taille. C’est ici que les
valeurs des AIC et BIC optimaux s’ave`rent utiles. Elles indiquent par exemple que
les deux options peuvent eˆtre envisage´es. Le meilleur AIC correspond a` une partition
plus fine de 14 classes qui montre notamment un potentiel important d’ame´lioration
de la qualite´ d’ajustement. Le meilleur BIC indique que l’on peut atteindre une qualite´
comparable d’ajustement avec une partition plus sommaire de 6 classes seulement. On
a dans tous les cas une indication forte qu’il est possible d’ame´liorer sensiblement la
qualite´ de la partition produite par CHAID. Cet exemple semble aussi confirmer une
bilan oct.99
dipl. second.regroup.
Adj. P-value=0.0000, Chi-square=50.7197, df=2
économique;moderne,<missing>
AGEDIP
Adj. P-value=0.0090, Chi-square=11.0157, df=1
>20,<missing><=20
classic .latine;scientifique
AGEDIP
Adj. P-value=0.0067, Chi-square=14.6248, df=2
>19(18,19]<=18
étranger,autre;dipl. ing.
nationalité regoup.
Adj. P-value=0.0011, Chi-square=16.2820, df=1
Genève;hors Europe
tronc commun
Adj. P-value=0.0188, Chi-square=5.5181, df=1
sc.socialessc.écon. + HEC
ch-al.+Tessin;Europe;Suisse Romande
date d'immatriculation
Adj. P-value=0.0072, Chi-square=9.2069, df=1
>97<=97
Page 1, 1
Tree 01 - BIL_99
Fig. 4 – Bilan apre`s une anne´e en SES : arbre CHAID
RNTI - 1
Gilbert Ritschard
tendance du AIC a` se´lectionner un mode`le trop complexe.
7 Conclusion
Cet article pre´sente une premie`re approche assez rustre pour de´terminer la partition
optimale dans une optique de pre´diction de la distribution a posteriori de la variable
re´ponse. L’heuristique propose´e doit certainement pouvoir eˆtre ame´liore´e et d’autres
approches du type descendant notamment me´riteraient e´galement d’eˆtre explore´es. Il
convient ici de mentionner en particulier les approches qui a` l’instar de Sipina (Zighed
et Rakotomalala, 2000) ge´ne`rent des graphes d’induction plutoˆt que des arbres en
autorisant des fusions comme celles illustre´es a` la figure 2. On peut s’attendre a` ce que
le couplage de la strate´gie graphe d’induction avec des crite`res du type AIC ou BIC
donnent des re´sultats performants tant du point de vue du temps de calcul que des
possibilite´s d’approcher la solution optimale.
Re´fe´rences
Akaike, H. (1983). Information measures and model selection. Bulletin of the Interna-
tional Statistical Institue 50, 277–290.
Bell, E. T. (1938). The iterated exponential numbers. Ann. Math. 39, 539–557.
Bishop, Y. M. M., S. E. Fienberg, et P. W. Holland (1975). Discrete Multivariate
Analysis. Cambridge MA : MIT Press.
Cressie, N. et T. R. Read (1984). Multinomial goodness-of-fit tests. Journal of the
Royal Statistical Society 46, 440–464.
Hastie, T., R. Tibshirani, et J. Friedman (2001). The Elements of Statistical Learning.
New York : Springer.
Kass, G. V. (1980). An exploratory technique for investigating large quantities of
categorical data. Applied Statistics 29 (2), 119–127.
Kass, R. E. et A. E. Raftery (1995). Bayes factors. Journal of the American Statistical
Association 90 (430), 773–795.
Petroff, C., A.-M. Bettex, et A. Korffy (2001, Juin). Itine´raires d’e´tudiants a` la faculte´
des sciences e´conomiques et sociales : le premier cycle. Technical report, Universite´
de Gene`ve, Faculte´ SES.
Quinlan, J. R. (1993). C4.5 : Programs for Machine Learning. San Mateo : Morgan
Kaufmann.
Raftery, A. E. (1995). Bayesian model selection in social research. In P. Marsden (Ed.),
Sociological Methodology, pp. 111–163. Washington, DC : The American Sociological
Association.
Ritschard, G. (2001). Performance d’une heuristique d’agre´gation optimale bidimen-
sionnelle. Extraction des connaissances et apprentissage 1 (4), 185–196.
RNTI - 1
Patition BIC optimale
Ritschard, G. et D. A. Zighed (2002). Qualite´ d’ajustement d’arbres d’induction. Tech-
nical report, Groupe Gafo Qualite´, CNRS, Paris. 16p.
Ritschard, G. et D. A. Zighed (2003). Mode´lisation de tables de contingences par arbres
d’induction. Revue des sciences et technologies de l’information — ECA 17 (1–3),
381–392.
Ritschard, G., D. A. Zighed, et N. Nicoloyannis (2001). Maximisation de l’association
par regroupement de lignes ou colonnes d’un tableau croise´. Revue Mathe´matiques
Sciences Humaines 39 (154/155), 81–97.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6,
461–464.
SPSS (Ed.) (2001). Answer Tree 3.0 User’s Guide. Chicago : SPSS Inc.
Zighed, D. A. et R. Rakotomalala (2000). Graphes d’induction : apprentissage et data
mining. Paris : Hermes Science Publications.
Summary
This paper is concerned with the partitioning of the predictor space best suited to
generate reliable estimates of class posteriors. This optimal partition has to face a
double goodness-of-fit and simplicity criteria. We thus focus on the Akaike (AIC)
and Bayesian (BIC) information criteria that specifically take these two aspects into
account. We show how they apply in our framework and then investigate how to reach
their optimal value. The paper provides a crude heuristic and studies its efficiency by
comparing the quasi optimum with the true optimum on a series of simulated tables.
The optimum provides useful insight on how much a partition, for instance the partition
provided by an induction tree algorithm, could be improved. This point is illustrated
with a real dataset.
RNTI - 1
