Ã‰valuation de la rÃ©gression bornÃ©e
Thierry Foucart
UMR 6086, UniversitÃ© de Poitiers, S P 2 M I, bd 3 tÃ©lÃ©port 2
BP 179, 86960 Futuroscope, Cedex FRANCE
RÃ©sumÃ©. Le modÃ¨le linÃ©aire est trÃ¨s frÃ©quemment utilisÃ© en statistique et par-
ticuliÃ¨rement dans les secteurs de lâ€™assurance, de la banque et du marketing. Il
permet de dÃ©terminer les variables explicatives qui interviennent dans le risque
mesurÃ© chez les assurÃ©s et dans les choix effectuÃ©s par la clientÃ¨le. Le problÃ¨me
considÃ©rÃ© dans cet article apparaÃ®t lorsque ces variables sont liÃ©es statistique-
ment, par exemple le revenu et la catÃ©gorie socioprofessionnelle. Les estimations
donnÃ©es par le critÃ¨re des moindres carrÃ©s ordinaires deviennent alors instables
et peuvent prendre des valeurs en contradiction avec les valeurs rÃ©elles. Il existe
de nombreuses mÃ©thodes adaptÃ©es Ã  ce type de donnÃ©es. Nous proposons ici
dâ€™Ã©valuer lâ€™efficacitÃ© de la rÃ©gression bornÃ©e en procÃ©dant par simulation. Les
rÃ©sultats sont clairs : le gain en prÃ©cision et en stabilitÃ© des coefficients de rÃ©-
gression est impressionnant.
1 Introduction
Le modÃ¨le linÃ©aire est une des mÃ©thodes statistiques les plus employÃ©es dans les sciences
de lâ€™homme et de la sociÃ©tÃ©. Il donne en effet une rÃ©ponse Ã  la question rÃ©currente de lâ€™effet
propre dâ€™une variable sur une autre. En assurance automobile par exemple, la question pour-
rait Ãªtre : lâ€™Ã¢ge du conducteur joue-t-il un rÃ´le dans le risque dâ€™accident indÃ©pendamment des
autres facteurs ? Ce risque dÃ©pend-il de son sexe toutes choses Ã©gales par ailleurs ? Pour rÃ©-
pondre Ã  ces questions, on effectue la rÃ©gression du risque par les facteurs dâ€™accident, et on
Ã©tudie chacun des coefficients de rÃ©gression de lâ€™Ã¢ge et du sexe : la rÃ©ponse est considÃ©rÃ©e
comme positive si ce coefficient est significativement non nul.
Lâ€™hypothÃ¨se Â« toutes choses Ã©gales par ailleurs Â», formalisÃ©e par le choix des variables explica-
tives du modÃ¨le linÃ©aire, est toutefois trÃ¨s discutÃ©e depuis fort longtemps parce quâ€™elle ouvre la
porte Ã  des abus flagrants (Simiand, 1932). Sa formalisation demande beaucoup de prÃ©cautions
pour Ã©viter des contradictions internes. Ces derniÃ¨res se manifestent au plan mathÃ©matique par
une relation linÃ©aire exacte entre les variables explicatives. Dans ce dernier cas, lâ€™analyse sta-
tistique est impossible, la matrice de corrÃ©lation nâ€™Ã©tant pas inversible.
Ces contradictions ne sont pas toujours totales. Il existe des situations dans lesquelles les va-
riables explicatives ne sont pas liÃ©es au sens linÃ©aire du terme (il nâ€™existe pas de combinaison
linÃ©aire strictement Ã©gale Ã  0), mais le sont au sens statistique (il existe une combinaison li-
nÃ©aire Â«presque Â»Ã©gale Ã  0). Lâ€™estimateur des moindres carrÃ©s ordinaires devient alors peu
prÃ©cis, et on est amenÃ© Ã  utiliser dâ€™autres estimateurs dont les plus classiques sont ceux de la
- 131 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
rÃ©gression orthogonale (les variables explicatives sont choisies parmi les composantes princi-
pales de variance suffisante) et de la rÃ©gression bornÃ©e que nous Ã©tudions ci-dessous.
AprÃ¨s avoir dÃ©fini lâ€™estimateur de la rÃ©gression bornÃ©e (ou ridge regression) introduite par
Hoerl et Kennard en 1970, nous donnons quelques exemples des consÃ©quences de la colinÃ©a-
ritÃ© statistique entre les variables explicatives dâ€™un modÃ¨le linÃ©aire. Pour pouvoir comparer
les estimations aux valeurs rÃ©elles des coefficients de rÃ©gression, nous avons procÃ©dÃ© par si-
mulation. La gÃ©nÃ©ralisation de cette procÃ©dure donne un Ã©chantillon de lâ€™estimateur bornÃ© et
par suite une estimation de lâ€™erreur quadratique moyenne. La comparaison de cette estimation
avec lâ€™erreur quadratique de lâ€™estimateur des moindres carrÃ©s ordinaires montre clairement que
la rÃ©gression bornÃ©e amÃ©liore considÃ©rablement les estimations lorsquâ€™il existe une colinÃ©aritÃ©
statistique entre les variables explicatives.
Les donnÃ©es analysÃ©es dans cet article et les logiciels utilisÃ©s sont disponibles Ã  lâ€™adresse sui-
vante : http ://foucart.thierry.free.fr (rubrique colinÃ©aritÃ©).
2 ColinÃ©aritÃ© et modÃ¨le linÃ©aire.
2.1 Estimateurs dans le modÃ¨le linÃ©aire
Le modÃ¨le linÃ©aire consiste Ã  reprÃ©senter une relation entre une variable expliquÃ©e notÃ©e Y
et p variables explicatives X1, . . . , Xp par lâ€™Ã©quation ci-dessous
Y = Î²0 + Î²1X1 + . . .+ Î²pXp + Îµ,
dans laquelle
1. les coefficients Î²j , j = 1, . . . , p sont des paramÃ¨tres thÃ©oriques appelÃ©s coefficients de
rÃ©gression ;
2. la variable Îµ est une variable alÃ©atoire appelÃ©e variable rÃ©siduelle, centrÃ©e et de variance
Ïƒ2 appelÃ©e variance rÃ©siduelle, indÃ©pendante des variables explicatives ;
3. on suppose frÃ©quemment que la variable Îµ suit la loi normale N(0, Ïƒ).
Dans toute la suite du texte, les variables explicatives sont centrÃ©es et rÃ©duites et la variable
rÃ©siduelle suit la loi normale.
On considÃ¨re un Ã©chantillon de taille n du vecteur (Y,X1, . . . , Xp). On note X la matrice de n
lignes et p colonnes contenant les observations xi,j(i = 1, . . . , n et j = 1, . . . , p) des variables
Xj et Y la matrice colonne contenant les observations yi(i = 1, . . . , n) de la variable Y . La
matrice R dÃ©finie ci-dessous est la matrice des corrÃ©lations observÃ©es :
R =
1
n
XtX.
Lâ€™estimateur B des moindres carrÃ©s ordinaires du vecteur Î² = (Î²1, . . . , bp)t est Ã©gal Ã  :
B =
1
n
Râˆ’1XtX
Câ€™est un estimateur efficace (de variance minimale dans la classe des estimateurs sans biais).
On note b = (b1, . . . , bp)t lâ€™observation du vecteur B et b0 lâ€™estimation de Î²0 dÃ©duite des
- 132 -RNTI-A-1
T. Foucart
moyennes observÃ©es des variables Y et Xj , j = 1, . . . , p. La matrice variance VB de lâ€™esti-
mateur B est Ã©gale Ã  :
VB =
Ïƒ2
n
Râˆ’1.
Le coefficient de dÃ©termination notÃ© R2 est le carrÃ© du coefficient de corrÃ©lation entre les
valeurs observÃ©es yi, i = 1, . . . , n et les valeurs yâ€²i estimÃ©es par le modÃ¨le :
yâ€²i = b0 + b1xi,1 + . . .+ bjxi,j + . . .+ bpxi,p.
Les rÃ©sidus ei (i = 1, . . . , n) sont les diffÃ©rences entre les valeurs observÃ©es de Y et les valeurs
estimÃ©es :
âˆ€i = 1, . . . , n i = yi âˆ’ yâ€²i
On sait que les rÃ©sidus sont centrÃ©s et non corrÃ©lÃ©s aux variables explicatives. Lâ€™estimation sans
biais de la variance rÃ©siduelle Ïƒ2 est donnÃ©e par :
s,2 =
1
nâˆ’ pâˆ’ 1
nâˆ‘
i=1
2i .
2.2 ColinÃ©aritÃ© et estimateur bornÃ©
Les effets de la colinÃ©aritÃ© entre les variables explicatives rÃ©sultent de lâ€™inversion de la ma-
trice R dans le calcul de lâ€™estimateur B et de sa matrice varianceVB. La colinÃ©aritÃ© crÃ©e tout
dâ€™abord une grande instabilitÃ© des estimations des coefficients de rÃ©gression : les variances des
estimateurs, proportionnelles aux termes diagonaux de Râˆ’1, sont particuliÃ¨rement Ã©levÃ©es.
Les signes des coefficients estimÃ©s peuvent mÃªme Ãªtre contraires Ã  ceux des vraies valeurs. Le
coefficient de dÃ©termination R2 peut aussi devenir trÃ¨s instable (Foucart, 2000). Lâ€™interprÃ©ta-
tion des rÃ©sultats est finalement sujette Ã  caution. Il nâ€™est pas toujours facile de dÃ©tecter cette
colinÃ©aritÃ© par une simple lecture de la matrice R. En effet, cette colinÃ©aritÃ© apparaÃ®t lorsquâ€™un
coefficient de corrÃ©lation est proche dâ€™une des bornes de lâ€™intervalle dans lequel il peut va-
rier conditionnellement aux autres (Foucart, 1997). On la recherche en examinant les valeurs
propres de la matrice R et des indices comme les facteurs dâ€™inflation (termes diagonaux de
la matrice Râˆ’1) , lâ€™indice de conditionnement (inverse de la plus petite valeur propre de la
matrice R) et lâ€™indice de multicolinÃ©aritÃ© (moyenne des facteurs dâ€™inflation) : une petite va-
leur propre et des indices Ã©levÃ©s indiquent une colinÃ©aritÃ© statistique. On pourra sur ces points
consulter lâ€™ouvrage de Tomassone Â«La rÃ©gression Â»(Masson, 1992). Les procÃ©dures de simu-
lation utilisÃ©e ci-dessous visualisent dans un premier temps ces propriÃ©tÃ©s connues au plan
mathÃ©matique. Elles montrent aussi que la rÃ©gression bornÃ©e (ou ridge regression) proposÃ©e
par Hoerl et Kennard (1970) donne de bien meilleurs rÃ©sultats que la rÃ©gression des moindres
carrÃ©s ordinaires dans le cas de donnÃ©es statistiquement colinÃ©aires.
Lâ€™idÃ©e gÃ©nÃ©rale est la suivante : un estimateur efficaceX dâ€™un paramÃ¨tre rÃ©el m est un estima-
teur de variance minimale dans la classe des estimateurs sans biais (E(X) = Âµ), mais il ne
minimise pas lâ€™erreur quadratique dÃ©finie par E(â€–X â€² âˆ’ Âµâ€–2), dans laquelle X â€² est un estima-
teur quelconque de Âµ. On peut donc chercher un estimateur biaisÃ©X â€² dont lâ€™erreur quadratique
- 133 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
est plus petite.
Cette situation se prÃ©sente dans le cas du modÃ¨le linÃ©aire lorsque les variables explicatives
sont statistiquement colinÃ©aires. Nous allons vÃ©rifier par simulation que lâ€™estimateur Br =
(Br1, . . . , Brp)t dÃ©fini dans la rÃ©gression bornÃ©e donne alors de meilleures estimations que
lâ€™estimateur efficace B = (B1, . . . , Bp)t des moindres carrÃ©s ordinaires. Lâ€™estimateur Br est
obtenu suivant le critÃ¨re des moindres carrÃ©s sous contrainte de norme :
â€–Brâ€–2 = b2r1 + b2r2 + . . .+ b2rp â‰¤M
En fait, ce nâ€™est pas le majorantM que lâ€™on fixe : on montre en effet quâ€™il suffit de remplacer
dans la formule de lâ€™estimateur des moindres carrÃ©s ordinaires la matrice R par la matrice
R+kI, oÃ¹ I est la matrice identique et k une constante rÃ©elle positive, pour limiter la norme de
lâ€™estimateurBr. En pratique, on recherche la meilleure constante k Ã  lâ€™aide de la reprÃ©sentation
graphique des coefficients de rÃ©gression en fonction de k, appelÃ©e ridge trace. Les formules
concernant lâ€™estimateur bornÃ© sont les suivantes
Br =
1
n
[R+ kI]âˆ’1XtY VBr =
Ïƒ2
n
[R+ kI]âˆ’1R[R+ kI]âˆ’1
3 Effets de la colinÃ©aritÃ©. Exemples.
3.1 ColinÃ©aritÃ© et matrice de corrÃ©lation
On considÃ¨re quatre variables X1, X2, X4 et X4 dont les coefficients de corrÃ©lation sont
donnÃ©s ci-dessous La colinÃ©aritÃ© entre les variables ne peut guÃ¨re Ãªtre dÃ©celÃ©e par un simple
X1 X2 X3 X4
X1 1.000
X2 0.500 1.000
X3 0.500 0.500 1.000
X4 -0.500 0.400 0.300 1.000
TAB. 1 â€“ Matrice de corrÃ©lation des variables explicatives.
examen de la matrice. Elle peut Ãªtre mise en Ã©vidence de plusieurs faÃ§ons :
1. Les facteurs dâ€™inflation fj associÃ©s Ã  chaque coefficient de rÃ©gression sont Ã©levÃ©s (f1 =
62, f2 = 26, f3 = 14, f4 = 50) ;
2. lâ€™indice de multicolinÃ©aritÃ© I , Ã©gal Ã  1 en lâ€™absence de toute colinÃ©aritÃ©, est Ã©levÃ© : I =
38 ;
3. la matrice R possÃ¨de une valeur propre trÃ¨s faible (Î»4 = 0.007). La combinaison li-
nÃ©aire des variables X1, X2, X3 et X4 dÃ©finie par la quatriÃ¨me composante principale
est donc presque constante et Ã©gale Ã  0 : on ne peut pas choisir une valeur deX4 en toute
libertÃ© lorsque les trois autres sont fixÃ©es ;
4. On utilise souvent lâ€™indice de conditionnement, dont on trouvera une analyse dans Bels-
ley (1980) : Îº = 1/Î»4 = 148.83.
- 134 -RNTI-A-1
T. Foucart
3.2 Effet de la colinÃ©aritÃ© sur les coefficients de rÃ©gression estimÃ©s
Les donnÃ©es ridge1 contiennent les observations de cinq variables sur cent individus sta-
tistiques obtenues par simulation. On veut expliquer la cinquiÃ¨me variable, Y , par les quatre
premiÃ¨res X1, X2, X3 et X4, dont la matrice de corrÃ©lation est Ã©gale Ã  la prÃ©cÃ©dente. Les va-
riables explicatives sont centrÃ©es et rÃ©duites. Les coefficients de corrÃ©lation observÃ©s entre les
variables explicatives et la variable expliquÃ©e sont donnÃ©s dans le tableau ci-dessous
X1 X2 X3 X4
y 0.540 0.216 -0.107 -0.491
TAB. 2 â€“ Coefficients de corrÃ©lation observÃ©s entre Y et X1, X2, X3, X4, (donnÃ©es ridge1).
La rÃ©gression des moindres carrÃ©s ordinaires donne les rÃ©sultats suivants :
degrÃ© de libertÃ© Somme des carrÃ©s Variance estimÃ©e Pourcentage de variance totale
Tot 99 229.7305 2.320510 1
Exp 4 112.7185 1.088805 0.490655
Res 95 117.0120 1.231705 0.509345
TAB. 3 â€“ Analyse de variance (donnÃ©es ridge1, n = 100).
Estimation Ã©cart-type t de Student facteur dâ€™inflation
b1 1.6339 0.8739 1.870 62.00
b2 -0.1482 0.5659 -0.262 26.00
b3 -1.0375 0.4153 -2.498 14.00
b4 0.4439 0.7848 0.566 50.00
b0 -0.1650 0.1110 -1.486
TAB. 4 â€“ Estimation des coefficients de rÃ©gression (donnÃ©es ridge1).
Les variables explicatives Ã©tant rÃ©duites et lâ€™Ã©cart-type de la variable expliquÃ©e Ã©gal Ã  1.516,
on peut apprÃ©cier intuitivement la taille des coefficients de rÃ©gression. Les coefficients de rÃ©-
gression b1, b3 prennent des valeurs Ã©levÃ©es en valeur absolue. Le coefficient de rÃ©gression
b2 est nÃ©gatif, malgrÃ© un coefficient de corrÃ©lation positif entre X2 et Y (0.216), et b4 est po-
sitif malgrÃ© un coefficient de corrÃ©lation entre X4 et Y fortement nÃ©gatif (âˆ’0.491). Seul b3
est significativement non nul pour un risque de premiÃ¨re espÃ¨ce Î± = 5% (t = âˆ’2.498). Le
coefficient de dÃ©termination (R2 = 0.49) est hautement significatif. Ces rÃ©sultats peuvent
sâ€™expliquer par la forte colinÃ©aritÃ© statistique entre X1, X2, X3, et X4 dÃ©tectÃ©e en paragraphe
3.1.
3.3 Effet de la colinÃ©aritÃ© sur les variances
On Ã©tudie maintenant les donnÃ©es ridge2, obtenues par simulation suivant le mÃªme modÃ¨le
que prÃ©cÃ©demment. La matrice de corrÃ©lation entre les variables explicatives reste Ã©gale Ã  R,
les coefficients de rÃ©gression thÃ©oriques sont les mÃªmes, mais les corrÃ©lations observÃ©es entre
la variable expliquÃ©e et les variables explicatives sont les suivantes :
- 135 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
X1 X2 X3 X4
y 0.486 0.084 -0.199 -0.584
TAB. 5 â€“ Coefficients de corrÃ©lation observÃ©s entre Y et X1, X2, X3, X4, (donnÃ©es ridge 2).
La rÃ©gression des moindres carrÃ©s ordinaires donne les rÃ©sultats suivants
degrÃ© de libertÃ© Somme des carrÃ©s Variance estimÃ©e Pourcentage de variance totale
Tot 99 188.1299 1.900302 1
Exp 4 94.14017 0.9109364 0.500400
Res 95 93.98971 0.9893653 0.499600
TAB. 6 â€“ Analyse de variance (donnÃ©es ridge 2, n = 100).
Estimation Ã©cart-type t de Student facteur dâ€™inflation
b1 0.4638 0.7832 0.592 62.00
b2 0.3674 0.5072 0.724 26.00
b3 -0.5204 0.3722 -1.398 14.00
b4 -0.5594 0.7033 -0.795 50.00
Cst -0.0985 0.0995 -0.990
TAB. 7 â€“ Estimation des coefficients de rÃ©gression (donnÃ©es ridge 2).
La situation est paradoxale : le coefficient de dÃ©termination R2 est hautement significatif
(R2 = 0.50, n = 100), mais aucun des coefficients de rÃ©gression nâ€™est significativement
non nul. On peut apporter comme explication une surestimation des Ã©carts-types des estima-
teurs Bj . Les estimations b1, b2, b3 et b4 ne paraissent pas en effet spÃ©cialement grandes (les
variables explicatives sont rÃ©duites, et lâ€™Ã©cart-type de la variable expliquÃ©e est Ã©gal Ã  1.372), et
lâ€™augmentation des variances des estimateurs due Ã  la colinÃ©aritÃ© a pour effet de diminuer les t
de Student, les rendant ainsi non significatifs.
4 Simulation dâ€™un Ã©chantillon de lâ€™estimateur bornÃ©
4.1 DÃ©marche
On peut, par simulation, visualiser de faÃ§on plus complÃ¨te lâ€™effet de la colinÃ©aritÃ© sur les
coefficients de rÃ©gression. La dÃ©marche est la suivante
1. on choisit le nombre de variables explicatives p, le vecteur de rÃ©gression thÃ©orique Î² =
(Î²1, . . . , Î²p)t, le coefficient constant Î²0, le coefficient de dÃ©termination R2 et le nombre
dâ€™observations n. On fixe la matrice de corrÃ©lationR entre les variables explicatives. On
en dÃ©duit la variance rÃ©siduelle thÃ©orique Ïƒ2 ;
2. on simule un Ã©chantillon des variables explicatives xi,j de matrice de corrÃ©lation Ã©gale
Ã  R. Il suffit pour cela de simuler un Ã©chantillon de taille n dâ€™un vecteur alÃ©atoire quel-
conque de dimension p, dâ€™en effectuer lâ€™analyse en composantes principales pour obtenir
un vecteur Z dont la matrice de covariance est strictement Ã©gale Ã  la matrice identique
- 136 -RNTI-A-1
T. Foucart
I, et dâ€™effectuer une transformation linÃ©aire de ce dernier de faÃ§on Ã  obtenir un tableau
de donnÃ©es X dont la matrice de corrÃ©lation est strictement Ã©gale Ã  R. Cette transfor-
mation est dÃ©finie par la matrice triangulaire infÃ©rieure T obtenue par lâ€™algorithme de
Cholesky(Ciarlet, 1989)
R = TTt X = ZTt;
3. on simule un Ã©chantillon indÃ©pendant (Îµi) de taille n de la v.a. Îµ suivant la loi normale
N(0, Ïƒ), et on en dÃ©duit les valeurs simulÃ©es yi, i = 1, . . . , n de la variable Y pour les
valeurs xi,j du tableauX
âˆ€i = 1, . . . , n yi = b0 + b1xi,1 + ...+ bpxi,p + Îµi;
4. on calcule le vecteur de rÃ©gression estimÃ© b = (b1, . . . ,bp)t ;
5. on recommence la simulation effectuÃ©e en 3) pour obtenir une autre simulation du vec-
teur de rÃ©gression avec le mÃªme tableauX, etc.
Chaque Ã©chantillon de la variable expliquÃ©e donne une estimation b du vecteur de rÃ©gression Î²
pour les mÃªmes valeurs des variables explicatives. On en dÃ©duit lâ€™erreur quadratique â€–bâˆ’Î²â€–2.
En rÃ©pÃ©tant m fois cette opÃ©ration, on dispose donc dâ€™un Ã©chantillon de m vecteurs bl, l =
1, . . . ,m conditionnellement Ã  X. On peut calculer les erreurs quadratiques dl = â€–bl âˆ’ Î²â€–
pour l = 1, . . . ,m en choisissant comme estimateur lâ€™estimateur de la rÃ©gression bornÃ©e Br
pour diffÃ©rentes valeurs de k (pour k = 0, lâ€™estimateur de la rÃ©gression bornÃ©e est confondu
avec lâ€™estimateur des moindres carrÃ©s ordinaires). Certains auteurs donnent des indications sur
le choix de cette constante k (Nordberg, 1982).
4.2 Exemple
Dans lâ€™exemple ci-dessous, la matrice de corrÃ©lation R entre les variables explicatives est
donnÃ©e dans le tableau 1. Les coefficients de rÃ©gression et la constante choisis sont les suivants :
Î²0 = 0 Î²1 = 0.5 Î²2 = 0.5 Î²3 = âˆ’0.5 Î²4 = âˆ’0.5
Le modÃ¨le thÃ©orique est donc Ã©gal Ã 
Y = 0.5X1 + 0.5X2 âˆ’ 0.5X3 âˆ’ 0.5X4 + Îµ .
Le coefficient R2 Ã©tant fixÃ© Ã  0.5, la variance rÃ©siduelle thÃ©orique est Ã©gale Ã  0.95. On effectue
ensuite la rÃ©gression linÃ©aire bornÃ©e des donnÃ©es simulÃ©es en effectuant la dÃ©marche prÃ©cÃ©-
dente. Le coefficient constant estimÃ© b0 nâ€™est pas nÃ©cessairement nul, mais nâ€™est pas pris en
compte dans le calcul des distances entre les vecteurs de rÃ©gression.
La taille de lâ€™Ã©chantillon Ã©tant fixÃ©e Ã  n = 100, nous donnons ci-dessous les rÃ©sultats de la
rÃ©gression sur un Ã©chantillon obtenus par lâ€™estimateur des moindres carrÃ©s ordinaires (k = 0)
et par lâ€™estimateur bornÃ© pour diffÃ©rentes valeurs de la constante k.
- 137 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
Coefficients thÃ©oriques 0.5 0.5 -0.5 -0.5 CarrÃ© de la distance
Coefficients estimÃ©s br1 br2 br3 br4 d2
k= 0 (MCO) 0.123 0.850 -0.445 -0.826 0.374
k = 0.01 0.356 0.688 -0.539 -0.611 0.070
k = 0.05 0.452 0.575 -0.543 -0.498 0.010
TAB. 8 â€“ coefficients de rÃ©gression estimÃ©s pour diffÃ©rentes valeurs de k. En derniÃ¨re colonne :
carrÃ© de la distance au vecteur de rÃ©gression thÃ©orique.
4.3 GÃ©nÃ©ralisation
Pour gÃ©nÃ©raliser ces rÃ©sultats, nous avons gÃ©nÃ©rÃ©, pour les mÃªmes valeurs xi,j des variables
explicatives, cinquante Ã©chantillons de la v.a. Y , puis, pour chaque valeur de k, effectuÃ© les
cinquante rÃ©gressions et calculÃ© les carrÃ©s d2l des distances, leur moyenne et leur variance :
k moyenne variance
0 1.383 4.005
0.01 0.242 0.107
0.05 0.045 0.002
TAB. 9 â€“ moyennes et variances des carrÃ©s des distances d2l (l = 1, . . . , 50) pour k =
0, 0.01 et 0.05.
Dâ€™aprÃ¨s le tableau prÃ©cÃ©dent, la rÃ©gression classique donne des estimations beaucoup plus
Ã©loignÃ©es en moyenne des coefficients de rÃ©gression thÃ©oriques que la rÃ©gression bornÃ©e. La
variance des carrÃ©s des distances est trÃ¨s Ã©levÃ©e par rapport aux autres variances, et le meilleur
estimateur des trois prÃ©cÃ©dents est celui de la rÃ©gression bornÃ©e pour k = 0.05.
La fonction de rÃ©partition observÃ©e des carrÃ©s des distances du vecteur de rÃ©gression estimÃ©
par le critÃ¨re des moindres carrÃ©s ordinaires au vecteur de rÃ©gression thÃ©orique est donnÃ©e en
figure 1 ci-dessous. Elle met en Ã©vidence la frÃ©quence de vecteurs de rÃ©gression estimÃ©s par
les moindres carrÃ©s ordinaires trÃ¨s diffÃ©rents du vecteur thÃ©orique. La valeur maximale des
carrÃ©s des distances obtenues par lâ€™estimateur bornÃ© et calculÃ©es sur les cinquante Ã©chantillons
en posant k = 0.05 est Ã©gale Ã  0.208 : dans plus de 60% des cas, la rÃ©gression des moindres
carrÃ©s ordinaires donne un estimation moins bonne du vecteur thÃ©orique que la pire donnÃ©e par
la rÃ©gression bornÃ©e.
Le tableau 10 contient les dix vecteurs de rÃ©gression obtenus par les moindres carrÃ©s ordi-
naires les plus Ã©loignÃ©s du vecteur thÃ©orique. Le coefficient de dÃ©termination R2 et le coeffi-
cient constant b0 sont Ã  peu prÃ¨s correctement estimÃ©s. Ce nâ€™est pas le cas des coefficients de
rÃ©gression, trÃ¨s mal reconstruits. Dans tous les vecteurs de rÃ©gression estimÃ©s, un au moins des
coefficients est de signe contraire au coefficient thÃ©orique et certains autres sont trÃ¨s Ã©levÃ©s en
valeur absolue. Dans la pratique, le risque dâ€™obtenir ce genre de rÃ©sultats est loin dâ€™Ãªtre nÃ©gli-
geable, puisque ces Ã©chantillons reprÃ©sentent 20% du nombre total dâ€™Ã©chantillons. Lâ€™idÃ©e qui
vient naturellement est de dÃ©terminer sur ces cinquante Ã©chantillons la valeur de k qui donne
les meilleurs rÃ©sultats. En faisant varier k dans lâ€™intervalle [0, 1] avec un incrÃ©ment de 0.001,
on obtient k = 0.078 (tableau 11) :
La moyenne des carrÃ©s des distances est trÃ¨s faible par rapport Ã  celle que lâ€™on obtient par les
estimateurs des moindres carrÃ©s ordinaires (0.039 au lieu de 1.383), et ces distances varient
- 138 -RNTI-A-1
T. Foucart
FIG. 1 â€“ fonction de rÃ©partition des carrÃ©s des distances (rÃ©gression des moindres carrÃ©s
ordinaires, Ã©chantillon simulÃ© de 50 termes).
R2 b0 b1 b2 b3 b4 d
2
valeurs thÃ©oriques 0.500 0.000 0.500 0.500 -0.500 -0.500
nâ—¦ 50 0.597 -0.105 -0.444 1.135 -0.041 -1.495 2.494
nâ—¦ 5 0.416 -0.231 -0.643 1.089 0.190 -1.413 2.963
nâ—¦ 44 0.534 -0.020 -0.611 1.166 0.010 -1.690 3.353
nâ—¦ 32 0.402 0.095 -0.841 1.248 -0.042 -1.413 3.400
nâ—¦ 46 0.594 -0.081 -0.667 1.348 -0.045 -1.728 3.795
nâ—¦ 47 0.424 0.117 1.693 -0.341 -1.118 0.714 3.986
nâ—¦ 25 0.560 0.189 1.898 -0.336 -1.233 0.828 4.956
nâ—¦ 42 0.589 0.071 -1.028 1.488 0.008 -1.910 5.556
nâ—¦ 40 0.626 0.110 2.474 -0.642 -1.421 1.097 8.600
nâ—¦ 23 0.407 0.119 -1.523 1.488 0.448 -2.124 8.605
TAB. 10 â€“ les dix vecteurs de rÃ©gression les plus Ã©loignÃ©s du vecteur thÃ©orique (rÃ©gression
des moindres carrÃ©s ordinaires)
beaucoup moins : lâ€™intÃ©rÃªt de la rÃ©gression bornÃ©e est Ã©vident et considÃ©rable. La simulation
montre aussi la robustesse de la mÃ©thode : la moyenne des carrÃ©s des distances diminue trÃ¨s
rapidement lorsque k varie de 0 Ã  0.05, reste Ã  peu prÃ¨s constante lorsque k varie de 0.05 Ã  0.1
environ, et augmente ensuite lentement Ã  partir de 0.1 (cf. figure 2 ci-dessous). La variance des
- 139 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
k moyenne variance
0.078 0.039 0.001
TAB. 11 â€“ moyenne et variance des carrÃ©s des distances d2k pour la valeur optimale k =
0.078 (m = 50).
distances, minimale aussi pour k = 0.078, suit la mÃªme Ã©volution. La recherche prÃ©cise de la
meilleure valeur de la constante k ne prÃ©sente visiblement guÃ¨re dâ€™intÃ©rÃªt. Nous avons procÃ©dÃ©
Ã  plusieurs simulations identiques : les rÃ©sultats ont toujours Ã©tÃ© analogues aux prÃ©cÃ©dents.
 
FIG. 2 â€“ moyenne des carrÃ©s des distances entre le vecteur thÃ©orique et le vecteur estimÃ© en
fonction de k.
5 Applications
Dans le cas de donnÃ©es rÃ©elles quelconques, on ne connaÃ®t ni le vecteur de rÃ©gression
thÃ©orique, ni la valeur optimale de la constante k. Pour choisir k, on utilise les ridge traces :
lâ€™absence de colinÃ©aritÃ© se traduisant par une ridge trace trÃ¨s rÃ©guliÃ¨re, on va choisir comme
valeur celle pour laquelle les coefficients de rÃ©gression sont stabilisÃ©s.
- 140 -RNTI-A-1
T. Foucart
5.1 RÃ©gression bornÃ©e en lâ€™absence de colinÃ©aritÃ© (donnÃ©es ridge0)
Effectuons dâ€™abord la rÃ©gression bornÃ©e dans le cas oÃ¹ les variables explicatives ne sont pas
colinÃ©aires. Les donnÃ©es analysÃ©es ci-dessous (fichier ridge 0) ont Ã©tÃ© obtenues par simulation
en supposant que les variables explicatives sont non corrÃ©lÃ©es. Les coefficients de rÃ©gression
et le coefficient de dÃ©termination thÃ©oriques sont les mÃªmes que ceux choisis prÃ©cÃ©demment
pour crÃ©er les donnÃ©es ridge1 et ridge 2.
 
FIG. 3 â€“ ridge trace (donnÃ©es ridge 0).
La figure 3 ci-dessus est la ridge trace obtenue en effectuant les rÃ©gressions bornÃ©es pour des
valeurs de k variant de 0 Ã  1. On observe une trÃ¨s grande stabilitÃ© des coefficients de rÃ©gression
par rapport Ã  la constante k. Le choix de cette derniÃ¨re nâ€™intervient guÃ¨re dans les estimations.
5.2 RÃ©gression bornÃ©e des donnÃ©es ridge 1
Revenons au donnÃ©es traitÃ©es dans le paragraphe 2.1. La figure 4 ci-dessous donne la re-
prÃ©sentation graphique des estimations b1, b2, b3 et b4 des coefficients de rÃ©gression suivant les
valeurs de k. Pour k = 0, ces valeurs sont celles que lâ€™on obtient par la rÃ©gression des moindres
carrÃ©s ordinaires.
On observe lâ€™instabilitÃ© de ces coefficients pour les faibles valeurs de k. Les coefficients de
rÃ©gression b1 et b3 diminuent trÃ¨s rapidement en valeur absolue, au contraire de b2 et b4. On
- 141 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
 
FIG. 4 â€“ ridge trace (donnÃ©es ridge 1).
recherche sur ce graphique une valeur de k pour laquelle les coefficients de rÃ©gression sont
stabilisÃ©s : on peut prendre ici k = 0.1. La rÃ©gression bornÃ©e donne alors les rÃ©sultats suivants :
br1 br2 br3 br4 br0
Estimation 0.585141 0.405312 -0.480752 -0.426282 -0.164952
Ã©cart-type 0.05550 0.06988 0.07351 0.05690
t de Student 6.955 3.826 -4.315 -4.943
TAB. 12 â€“ RÃ©sultats de la rÃ©gression bornÃ©e (donnÃ©es ridge1, k = 0.1).
Le biais de lâ€™estimateurBr apparaÃ®t dans le coefficient de corrÃ©lation non nul entre les rÃ©sidus
et la variable expliquÃ©e estimÃ©e par le modÃ¨le : r = 0.088. Les Ã©carts-types indiquent une trÃ¨s
grande stabilitÃ© de Br autour de son espÃ©rance E(Br), et les t de Student montrent que tous
les coefficients sont significatifs. Les simulations prÃ©cÃ©dentes montrent que les coefficients
obtenus sont largement plus proches des vraies valeurs que les estimations donnÃ©es par le
critÃ¨re des moindres carrÃ©s ordinaires.
- 142 -RNTI-A-1
T. Foucart
5.3 RÃ©gression bornÃ©e des donnÃ©es ridge2
La ridge trace (figure 5) montre une bonne stabilitÃ© des coefficients de rÃ©gression, et les
effets de la colinÃ©aritÃ© concernent donc surtout les variances des estimateurs.
 
FIG. 5 â€“ ridge trace (donnÃ©es ridge 2).
On peut le vÃ©rifier en choisissant une petite valeur de k, par exemple, k = 0.01 ou k = 0.02.
Les rÃ©sultats pour chacune de ces deux valeurs sont donnÃ©es dans les tableaux ci-dessous
br1 br2 br3 br4 br0
Estimation 0.468614 0.353795 -0.514348 -0.547913 -0.098518
Ã©cart-type 0.23287 0.16324 0.13169 0.21075
t de Student 1.467 1.580 -2.848 -1.895
TAB. 13 â€“ RÃ©sultats de la rÃ©gression bornÃ©e pour k = 0.01 (donnÃ©es ridge 2).
Il y a trÃ©s peu de diffÃ©rences entre les estimations suivant les valeurs de k, mais les valeurs
sont bien plus stables pour k = 0.02. La colinÃ©aritÃ© entre les variables explicatives exerce ici
un effet sur les seules variances, et les estimations obtenues suivant le critÃ¨re des moindres
- 143 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
br1 br2 br3 br4 br0
Estimation 0.466987 0.344683 -0.505688 -0.542312 -0.098518
Ã©cart-type 0.14950 0.11658 0.10273 0.13704
t de Student 2.277 2.156 -3.589 -2.885
TAB. 14 â€“ RÃ©sultats de la rÃ©gression bornÃ©e pour k = 0.02 (donnÃ©es ridge 2)
carrÃ©s ordinaires sont beaucoup plus proches des valeurs thÃ©oriques que les Ã©carts-types des
estimateurs ne lâ€™indiquent.
6 Conclusion
Les coefficients de rÃ©gression thÃ©oriques de ces applications sont en rÃ©alitÃ© connus : le
modÃ¨le utilisÃ© pour crÃ©er les donnÃ©es ridge1 et ridge 2 est celui qui a Ã©tÃ© prÃ©cisÃ© dans le
paragraphe 3.2. Le coefficient de dÃ©termination est fixÃ© Ã  0.5, et les coefficients de rÃ©gression
thÃ©oriques sont :
Î²0 = 0 Î²1 = 0.5 Î²2 = 0.5 Î²3 = -0.5 Î²4 = -0.5
Les estimations obtenues dans le premier cas par la rÃ©gression bornÃ©e (donnÃ©es ridge 1) sont
beaucoup plus proches des valeurs thÃ©oriques que celles qui sont dÃ©duites du critÃ¨re des
moindres carrÃ©s ordinaires. Dans le second (donnÃ©es ridge 2), elles sont beaucoup plus stables.
Compte tenu des rÃ©sultats des simulations donnÃ©s dans le paragraphe 3, on pouvait sâ€™y attendre.
Lâ€™intÃ©rÃªt de la rÃ©gression bornÃ©e apparaÃ®t finalement sur trois points :
1. Lorsque les coefficients estimÃ©s par le critÃ¨re des moindres carrÃ©s ordinaires sont trÃ¨s
diffÃ©rents des vraies valeurs, elle donne des estimations bien meilleures ;
2. elle permet de contrÃ´ler la stabilitÃ© des estimations ;
3. lorsque les variables explicatives sont non corrÃ©lÃ©es, elle ne modifie quasiment pas les
estimations ;
4. La stabilitÃ© des rÃ©sultats par rapport Ã  la constante k limite lâ€™importance dâ€™en rechercher
la meilleure valeur : une approximation mÃªme grossiÃ¨re, dÃ©duite simplement de la ridge
trace, donnera des rÃ©sultats en moyenne bien meilleurs que la rÃ©gression des moindres
carrÃ©s ordinaires. Par suite, en effectuant systÃ©matiquement une rÃ©gression bornÃ©e pour
une faible valeur de la constante k (par exemple k = 0.01) , les estimations des coef-
ficients de rÃ©gression ne peuvent Ãªtre que meilleures, mÃªme lorsque la colinÃ©aritÃ© entre
les variables explicatives nâ€™est pas trÃ¨s forte.
Toutefois, lorsque les valeurs thÃ©oriques des coefficients de rÃ©gression sont elles-mÃªmes Ã©le-
vÃ©es en valeur absolue, la rÃ©gression bornÃ©e est Ã  Ã©viter. Câ€™est le cas par exemple lorsque les
coefficients de rÃ©gression thÃ©oriques sont Ã©gaux aux valeurs estimÃ©es sur les donnÃ©es ridge 1 :
la ridge trace est la mÃªme, et par suite la rÃ©gression bornÃ©e donne de trÃ¨s mauvais rÃ©sultats. Il
est donc indispensable dâ€™analyser a priori la taille des coefficients de rÃ©gression en suivant une
dÃ©marche critique.
Ces simulations montrent le danger dâ€™interprÃ©ter le signe des coefficients de rÃ©gression sans
- 144 -RNTI-A-1
T. Foucart
prÃ©caution. MÃªme lorsque toutes les hypothÃ¨ses mathÃ©matiques sont satisfaites (distribution
gaussienne de la variable rÃ©siduelle, linÃ©aritÃ© des liaisons) ce qui est le cas dans les exemples
donnÃ©s puisquâ€™ils sont construits Ã  partir de ces hypothÃ¨ses, il est trÃ¨s possible dâ€™obtenir des
estimations trÃ¨s diffÃ©rentes des valeurs thÃ©oriques. Lorsque ces hypothÃ¨ses ne sont quâ€™approxi-
mativement vÃ©rifiÃ©es, ce qui est le cas gÃ©nÃ©ral des donnÃ©es rÃ©elles, la statistique produit des
rÃ©sultats quâ€™il est indispensable dâ€™examiner avec prudence et de ne pas prendre pour certains
mÃªme sâ€™ils sont largement significatifs.
RÃ©fÃ©rences
Belsley D.A., Kuh E., Welsh R.E. (1980). Regression diagnostics : identifying influential data
and sources of collinearity.Wiley, New York.
Ciarlet P.G. 1989). Introduction to Numerical Linear Algebra and Optimisation, London, Cam-
bridge University Press.
Foucart T. (1997). Numerical Analysis of a Correlation Matrix. Statistics, 29/4, p. 347-361.
Foucart T. (2000). ColinÃ©aritÃ© et InstabilitÃ© NumÃ©rique dans le ModÃ¨le LinÃ©aire, RAIRO Ope-
rations research, Vol.34, 2, p. 199-212.
Hoerl A.E., R.W. Kennard (19701). Ridge regression : biased estimation for nonorthogonal
problems. Technometrics, 12, 55-67.
Hoerl A.E., R.W. Kennard (19702). Ridge regression : Applications to nonorthogonal pro-
blems. Technometrics, 12, 69-82.
Nordberg L., 1982. A procedure of determination of a good ridge parameter in linear regres-
sion. Commun, Statist. Simula. Computa. 11(3), p. 285-289.
Simiand F.,1932. Le salaire, lâ€™Ã©volution sociale et la monnaie. Lien internet
http ://www.uqac.uquebec.ca/zone30/Classiques_des_sciences_sociales/index.html.
Tomassone R., Lesquoy E. et Millier C. (1992) : La rÃ©gression. Nouveaux regards sur une an-
cienne mÃ©thode statistique,Masson, Paris, 2e ed. .
Summary
The linear model is very frequently used in statistics and particularly in insurance, bank
and marketing. It makes it possible to determine the explanatory variables which play part
in the risk measured in the policy-holders and in the choices carried out by the customers.
The problem considered in this article appears when these variables are dependent statistically,
for example the income and the socio-professional group. Then, the estimates given by the
- 145 - RNTI-A-1
Ã‰valuation de la rÃ©gression bornÃ©e
criterion of ordinary least squares become not very reliable and can take values in contradiction
with the real values. There are many methods adapted to this type of data. We propose here to
evaluate the effectiveness of the ridge regression while proceeding by simulations. The results
are clear: the gain in precision and in stability of the regression coefficients is impressive.
- 146 -RNTI-A-1
