© MODULAD, 2005 - 28 - Numéro 33 
 
La régression PLS 1, cas particulier de 
la régression linéaire séquentielle orthogonale 
(RLSO)  
Jacques Goupy 
ReConFor, 24 avenue Perrichont. 75016 Paris. France. 
jacques@goupy.com 
RÉSUMÉ 
La régression linéaire classique fournit une seule solution souvent basée sur le 
critère des moindres carrés. Lorsqu’il y a beaucoup de variables, on risque d’obtenir 
un modèle surparamétré, c’est-à-dire modélisant les erreurs. Pour éviter cette 
surparamétrisation, la régression PLS a été introduite car étant une régression 
séquentielle, elle permet d’arrêter le processus de régression avant de modéliser 
l’erreur. Mais la PLS ne propose que quelques solutions et ne permet pas  vraiment 
d’optimisation. Nous proposons dans cet article une régression séquentielle 
orthogonale beaucoup plus souple que la PLS, la régression linéaire séquentielle 
orthogonale ou RLSO. Cette régression permet d’éviter la surparamétrisation et elle 
facilite l’optimisation de la modélisation car les directions de projection peuvent être 
choisies au mieux des intérêts de l’étude. Les solutions possibles proposées par la 
RLSO sont en nombre infini. Ce qui laisse la possibilité d’utiliser toutes les 
techniques d’optimisation pour trouver la ou les meilleures solutions. 
Comme la régression linéaire séquentielle orthogonale (RLSO) est nouvelle, 
nous avons développé toutes les étapes de calcul.  
 
Mots clés: Régression séquentielle, Régression PLS1, Régression orthogonale. 
 
ABSTRACT 
The classical linear regression provides just one solution often based on the 
least squares criterion. When there are many variables, one can obtain a model with 
too many surparameters and consequently a model which modelizes the error. To 
avoid such an over parameterization, the PLS regression has been introduced. PLS is 
a sequential regression and it enables to stop the regression process before the error 
modelling. But the PLS regression has only a few numbers of solutions and the 
optimization is not really possible. It is why a new type of sequential regression is 
proposed in this article: the sequential orthogonal linear regression or SOL 
regression. It is much more versatile than the PLS ; it enables to avoid 
surparametrization and facilitate the modelling optimization because the directions 
of projection can be chosen at the best for the study. The number of the solutions 
proposed by the SOL regression is infinite and all the techniques of optimization can 
be used. 
As the SOL regression is new, all the calculations have been largely explained. 
 
Key words: sequential regression, PLS1 regression, orthogonal regression. 
© MODULAD, 2005 - 29 - Numéro 33 
 
1.  INTRODUCTION 
La régression linéaire a pour objectif de trouver la relation mathématique 
linéaire qui représente le mieux le lien entre une grandeur d'intérêt (ou réponse), y, et 
des grandeurs explicatives (ou facteurs), x. La régression possède une terminologie 
très riche et très diverse pour désigner les grandeurs d'intérêt et les grandeurs 
explicatives (Tableau 1).  
 
Grandeur d'intérêt Variable 
y x 
  
Variable expliquée Variable explicative 
Variable endogène Variable exogène 
Variable dépendante Variable indépendante 
Réponse Facteur 
Observation Régresseur 
Variable d’intérêt Prédicteur 
 Contrôle 
Tableau 1 : Principales terminologies des grandeurs d'intérêt et des variables 
 
Dans cet article nous avons choisi la terminologie des plans d'expériences 
c'est-à-dire "réponse" pour grandeur d'intérêt et "facteurs" pour variables 
explicatives. Ces facteurs ont des niveaux, xi, définis ou mesurés par l'observateur. 
On introduit un écart, e, pour tenir compte du fait que le modèle retenu n'est pas 
exactement le modèle réel et que la grandeur d'intérêt est une grandeur aléatoire. On 
a donc  
exxxy nf += ),...,,( 21  
Dans la régression linéaire, on suppose que la fonction f est une fonction 
linéaire. Pour établir cette fonction, c'est-à-dire trouver la valeur des coefficients du 
modèle, il faut résoudre un système d'équations linéaires dont les termes connus sont 
les y et les xi, et dont les termes inconnus sont les coefficients et les écarts. Nous 
supposons que toutes les grandeurs sont centrées et réduites, ce qui ne retire aucune 
généralité à la théorie ou aux calculs. On écrit le système à résoudre sous forme 
matricielle 
eaXy +=  {1} 
où 
•  y   est le vecteur (n,1) des réponses mesurées ou observées. C'est le 
vecteur à régresser. Cette matrice-vecteur est centrée et réduite. 
•  X  est la matrice (n,p) des niveaux des facteurs et du modèle 
mathématique. Cette matrice est centrée et réduite. Elle est supposée 
de plein rang. 
•  a   est le vecteur (p,1) des coefficients. C'est le vecteur à déterminer. 
•  e  est le vecteur (n,1) des écarts. Cette matrice est inconnue. 
 
© MODULAD, 2005 - 30 - Numéro 33 
S'il y a n réponses y et p coefficients dans le modèle, il y a n équations et n + p 
inconnues. Pour trouver les p équations manquantes, on utilise la méthode des 
moindres carrés qui consiste à minimiser la somme des carrés des écarts. Le système 
de la régression linéaire classique est donc  
⎪⎩
⎪⎨
⎧
=
+=
0
t
a
ee
eaXy
∂
∂  {2} 
En faisant appel à quelques hypothèses simplificatrices, on obtient la solution 
de ce système, c'est-à-dire que l'on obtient les coefficients : 
yXXXa tt 1)(ˆ −=  {3} 
Ayant ces coefficients, on a la possibilité d'obtenir les réponses calculées 
connaissant les niveaux des facteurs : 
aXy ˆˆ =   {4} 
où 
•  yˆ   est le vecteur (n,1) des réponses calculées avec la méthode des 
moindres carrés. C’est la solution classique des moindres carrés. 
•  aˆ  est le vecteur (p,1) des coefficients calculés avec la méthode des 
moindres carrés. 
 
A partir de ces résultats, on peut écrire plusieurs relations qui sont fort utiles 
pour obtenir les valeurs numériques des différentes grandeurs. On introduit la 
matrice de projection H , ou matrice "hat", telle que : 
[ ] XXXXH tt 1−=  {5} 
La relation {4} peut alors s'écrire  
yHy =ˆ   {6} 
La matrice H associée à la matrice unité I permet également de calculer le 
vecteur des résidus. 
yHIe )(ˆ −=   {7} 
Les coefficients du modèle peuvent également être exprimés par la relation 
yXXXa ˆ)(ˆ 1 tt −=  {8} 
Ces formules matricielles peuvent être illustrées par des représentations 
géométriques donnant ainsi un éclairage intéressant de la régression ainsi qu’un 
support visuel à la réflexion et à l’analyse. 
2.  LES CONCEPTS 
Supposons que le modèle ne comporte qu'un coefficient et que deux mesures 
de la réponse aient été réalisées, l'une au niveau x1,1 et l'autre au niveau x1,2. Les 
valeurs des réponses sont respectivement y1 et y2. Le système d'équations est : 
⎩⎨
⎧
+=
+=
22,112
11,111
exay
exay
 
On peut écrire ce système d'équations sous forme matricielle pour faire 
apparaître les vecteurs : 
© MODULAD, 2005 - 31 - Numéro 33 
[ ] ⎥⎦
⎤⎢⎣
⎡+⎥⎦
⎤⎢⎣
⎡=⎥⎦
⎤⎢⎣
⎡
2
1
1
2,1
1,1
2
1 a
e
e
x
x
y
y
 
avec 
⎥⎦
⎤⎢⎣
⎡=
2
1
y
y
y  ⎥⎦
⎤⎢⎣
⎡=
2,1
1,1
x
x
X  [ ]1a=a   ⎥⎦
⎤⎢⎣
⎡=
2
1
e
e
e  
Pour illustrer cette relation, introduisons un référentiel dont le premier axe 
porte la réponse y1 et dont le second, orthogonal au premier, porte la réponse y2. Ce 
référentiel sera appelé « référentiel des réponses ». On peut tracer le vecteur y des 
réponses mesurées (Figure 1) qui a pour composantes y1 et y2. 
 
1
y
O
y
Réponse 1
2y
A
Réponse 2
 
Figure 1 : Le vecteur OA  représente les réponses mesurées dans le référentiel des 
réponses. 
Le vecteur y est la somme de deux vecteurs, le vecteur des réponses possibles 
[ ]1
2,1
1,1 a⎥⎦
⎤⎢⎣
⎡
x
x
 et le vecteur des écarts ⎥⎦
⎤⎢⎣
⎡
2
1
e
e
. 
Les niveaux x1,1 et x1,2 sont constants et parfaitement définis selon les 
hypothèses de la régression. Le rapport de ces niveaux est également constant : 
constant
2,1
1,1 =
x
x
 
La valeur du coefficient a1 est inconnue et c'est elle que l'on cherche à 
déterminer. Si l'on donne la valeur unité à ce coefficient, on détermine le point M1 
de coordonnées x1,1 et x1,2 (Figure 2). Si l'on donne la valeur 2 à ce coefficient, on 
détermine un point M2 de coordonnées 2x1,1 et 2x1,2. Si l'on donne la valeur zéro à 
ce coefficient, on obtient le point O, origine du référentiel.  
Si l'on fait varier la valeur de ce coefficient de – ∞ à + ∞, le point N parcourt 
une droite D. Cette droite peut être graduée en fonction des valeurs du coefficient a1. 
Chaque point de la droite D représente une solution possible du système d'équations, 
c'est pourquoi cette droite s'appelle « l'espace des réponses calculées possibles » ou 
plus simplement « l'espace des réponses possibles » ou ERP. 
 
© MODULAD, 2005 - 32 - Numéro 33 
1x 1,1
x 1,2
M
y
O
y
Réponse 1
2
y
D
N
A
Réponse 2
1
M2
a = 01
a = 1
a = 2
1
1
 
Figure 2 : La droite D représente l'ensemble de toutes les réponses calculées 
possibles. 
On appellera "vecteur niveau", le vecteur 1OM  dont les composantes sont x1,1 
et x1,2. C'est ce vecteur qui définit la direction de la droite D. C'est donc aussi le 
vecteur niveau qui définit l'ERP. 
⎥⎦
⎤⎢⎣
⎡==
2,1
1,1
11 OM x
x
x  
Le vecteur OA  est égal au vecteur ON  augmenté du vecteur NA  (Figure 3), 
on a  
NAONOA +=  
Le vecteur OA  est une représentation de la matrice y des réponses mesurées. 
Le vecteur ON  est une représentation de la matrice des réponses calculées possibles, 
'y , et le vecteur NA  est une représentation de la matrice e des écarts.  
1
y
O
y
Réponse 1
2
y
D
N
A
y'
e
Réponse 2
 
Figure 3 : Le vecteur des réponses mesurées est la somme vectorielle du vecteur des 
réponses possibles et du vecteur des écarts. 
Dans le cadre du critère des moindres carrés, on suppose que la solution la 
plus vraisemblable est donnée par le vecteur des écarts le plus petit possible. On 
obtient cette solution lorsque le vecteur e est orthogonal à la droite D. La solution est 
© MODULAD, 2005 - 33 - Numéro 33 
alors le vecteur que nous avons noté yˆ . Le vecteur des écarts devient le vecteur des 
résidus noté eˆ. Le point N prend la position particulière H (Figure 4). 
1
y
O
y
Réponse 1
2
y
D
H
A
y
e
Réponse 2
 
Figure 4 : Le vecteur des résidus, eˆ, est orthogonal à la droite D des réponses 
possibles ainsi qu'au vecteur des réponses calculées yˆ . 
Il est important de noter, que dans le cadre de la théorie des moindres carrés, le 
vecteur des résidus eˆ est orthogonal au vecteur des réponses calculées, yˆ . On peut 
dire aussi que le vecteur des réponses calculées, yˆ , est la projection orthogonale sur 
la droite D du vecteur des réponses mesurées, y . On peut dire aussi que régresser un 
ensemble de données, c'est projeter orthogonalement le vecteur représentatif de la 
matrice des réponses mesurées sur l'espace des réponses possibles. 
Les trois côtés du triangle rectangle OHA jouent un rôle essentiel dans la 
théorie de la régression et nous les retrouverons constamment : 
 
• Le vecteur y  des réponses mesurées ou vecteur à régresser est 
l’hypoténuse du triangle rectangle OHA.  
• Le vecteur yˆ  ou vecteur régressé est le côté de l'angle droit qui est 
dans l'ERP.  
• Le vecteur eˆ ou vecteur des résidus est le côté de l'angle droit qui n'est 
pas dans l'ERP. 
 
La relation {4}, aXy ˆˆ = , peut être interprétée de la manière suivante : Le 
vecteur OH  représente aussi bien le vecteur aˆ  que le vecteur yˆ. Le vecteur aˆ  est 
défini dans l’ERP et possède 1 composante. Le vecteur yˆ est défini dans le 
référentiel des réponses et il possède 2 composantes. La matrice X transforme donc 
un vecteur de l’ERP en son équivalent dans le référentiel des réponses. 
3.  LA REGRESSION LINEAIRE CLASSIQUE 
Les concepts introduits à l'aide de deux réponses et d'un coefficient peuvent 
facilement être étendus à n réponses et à p coefficients. 
Le référentiel des réponses possède n dimensions et c’est un référentiel 
orthonormé. Le vecteur des réponses mesurées possède également n dimensions. 
© MODULAD, 2005 - 34 - Numéro 33 
⎥⎥
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎣
⎡
=
ny
y
y
y
#
3
2
1
y  
On peut définir p droites D engendrant l'ERP qui possède donc p dimensions. 
Les directions de ces p droites sont définies par les vecteurs niveaux, 1x , 2x ,    , px . 
Ces vecteurs n’étant généralement ni orthogonaux ni de mêmes normes, l’ERP est 
défini par un référentiel oblique non normé. Les droites D sont graduées en fonction 
des valeurs des coefficients correspondants et chaque point de l'ERP représente un 
jeu possible de coefficients pour le modèle de régression, donc une solution 
possible.  
Les vecteurs 1x , 2x ,    , px  sont les colonnes de la matrice X. L'espace des 
réponses possibles est donc défini par les colonnes de la matrice X. 
0
D
Réponse 1
Réponse k
Réponse n
D
i
jy
y
e
A
H
x j
xi
Dk
xk
 
Figure 5 : Le vecteur des réponses mesurées, y , est décomposé en deux vecteurs : 
le vecteur des réponses calculées, yˆ  et le vecteur des résidus, eˆ . 
La régression classique consiste à trouver le vecteur des écarts ayant la norme 
la plus petite possible. Comme dans le paragraphe précédent, c'est la projection 
orthogonale de y sur l'ERP qui donne la solution classique des moindres carrés. On 
obtient ainsi le vecteur des réponses calculées, yˆ , et le vecteur des résidus, eˆ. Les 
vecteurs yˆ et eˆ sont orthogonaux (Figure 5). 
4.  REGRESSION LINEAIRE SUR UNE DIRECTION 
Au lieu de régresser le vecteur y sur l'hyperplan de l’EPR, on peut choisir une 
direction particulière dans cet espace. Choisissons une direction, ∆, située dans 
l'espace des réponses possibles. Cette direction est définie par son vecteur unitaire t. 
Le vecteur t est repéré par ses paramètres directeurs ti.  
© MODULAD, 2005 - 35 - Numéro 33 
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
pt
t
t
#
2
1
t  
L'ERP ayant p dimensions, le vecteur unitaire t possède p paramètres 
directeurs ti.  
Régresser le vecteur y sur la droite ∆, c’est le projeter orthogonalement sur 
cette droite. La projection de y sur ∆ est un vecteur de composantes b que l'on 
cherche à déterminer. Ces composantes sont égales, à un coefficient de 
proportionnalité près, aux paramètres directeurs de la direction choisie sur laquelle 
on va projeter :  
tb k=  
Le facteur de proportionnalité n'est pas connu. On le calcule à l'aide de la 
méthode des moindres carrés. Il faut donc résoudre le système d'équations suivant : 
⎪⎪⎩
⎪⎪⎨
⎧
=
=
+=
tb
ee
ebXy
k
k
0
t
∂
∂  {9} 
•  y   matrice (n,1) des réponses mesurées. C'est le vecteur à régresser. 
•  X  matrice (n,p) des niveaux des facteurs et du modèle mathématique. 
•  b  matrice (p,1) des coefficients. Ces coefficients sont proportionnels 
aux paramètres directeurs de t . 
•  t  matrice (p,1) des paramètres directeurs de la droite sur laquelle on 
projette. Ces paramètres sont connus puisque choisis par 
l'expérimentateur. 
•  e   matrice (n,1) des écarts. 
•  k coefficient de proportionnalité. C’est l’inconnue à déterminer. 
 
La solution des moindres carrés est la suivante (voir détail du calcul en annexe 
1) : 
[ ] yXttXXt ttttk 1~ −=  {10} 
D’où (Figure 6) 
•  les composantes du vecteur régressé : tb k~~ =  
•  le vecteur projeté sur D, solution de la régression  tXbXy k~~~ ==  
La relation, bXy ~~ = , peut être interprétée de la manière suivante : Les 
vecteurs b~  et t  sont définis dans l’ERP, ils ont donc p composantes. Le 
vecteur y~  est défini dans le référentiel des réponses, il possède donc n 
composantes. La matrice X transforme le vecteur b~  (ou tk~ ) de l’ERP 
en un vecteur équivalent, y~ , du référentiel des réponses. La 
représentation graphique ne différentie pas ces deux vecteurs qui sont 
parfaitement superposés. La matrice X transforme un vecteur de p 
composantes en un vecteur de n composantes. 
© MODULAD, 2005 - 36 - Numéro 33 
Au vecteur t  (p composantes) défini dans l’ERP correspond le 
transformé tX  qui est repéré dans le référentiel des réponses (n 
composantes). Le référentiel des réponses étant orthonormé, nous 
effectuerons les calculs dans ce repère. 
 
•  le vecteur des résidus de la régression yye ~−=  
 
0
D
D
i
j
y
y
e
A
∆
t
x
xj
i
Axe de projection
B
xk
Dk
 
Figure 6 : Régression du vecteur y sur la droite ∆ de paramètres directeurs t. 
Le triangle rectangle OBA est rectangle. On retrouve : 
 
Le vecteur à régresser : y   
Le vecteur régressé y~  
Le vecteur des résidus e  
 
On peut également introduire la matrice de projection ∆H  sur ∆ : 
[ ] XtXtXttXH tttt 1−∆ =   {11} 
On a le vecteur régressé 
yHy ∆=~   {12} 
et le vecteur des résidus 
yHIe )( ∆−=   {13} 
Les coefficients du modèle correspondant au vecteur y~  sont donnés par  
yXXXb ~)(~ 1 tt −=   {14} 
La matrice ( XXX tt 1)− transforme le vecteur y~  de n composantes en son 
équivalent, le vecteur b~  à p composantes. 
 
© MODULAD, 2005 - 37 - Numéro 33 
5.  REGRESSION LINEAIRE SEQUENTIELLE ORTHOGONALE 
Au lieu d'obtenir immédiatement la solution des moindres carrés comme dans 
la régression classique, on peut procéder par étapes. On projette d'abord sur une 
direction quelconque de l'ERP puis, on cherche à retrouver progressivement le 
vecteur yˆ . Nous prendrons d'abord le cas d'un modèle à deux coefficients, l'EPR ne 
possède alors que deux dimensions et nous verrons que le vecteur régressé de la 
régression classique, yˆ , est décomposé en deux vecteurs orthogonaux. 
Puis, nous étendrons notre étude à un modèle à trois coefficients. Le vecteur 
yˆ est alors décomposé en trois vecteurs orthogonaux. Enfin, nous verrons le cas du 
modèle à n coefficients et d'une décomposition de yˆ  en n vecteurs orthogonaux.  
5.1 DEUX COEFFICIENTS 
Dans le cas d'un modèle à deux coefficients, on a deux vecteurs niveaux, x1 et 
x2, portés par 2 droites D1 et D2. Ces 2 droites définissent l'ERP1. La régression 
classique de y conduit au vecteur yˆ  situé dans l'ERP1 (Figure 7).  
0
∆
1
1,2
y
y
y
A
B
H
∆
1
2B
1,1
D
D
1,1
2
y
1,2
 
Figure 7 : Le vecteur y  est régressé sur la droite ∆1,1, ou sur la droite ∆1,2, ou sur 
une droite ∆1,i du plan D1  D2. 
On projette y sur une première direction ∆1,1. On obtient un premier vecteur 
des réponses calculées, 1,11 ~OB y= . Les triangles OB1H et OB1A sont rectangles en 
B1. 
On projette y sur une deuxième direction ∆1,2. On obtient un deuxième vecteur 
des réponses calculées, 2,12 ~OB y= . Les triangles OB2H et OB2A sont rectangles en 
B2. On pourrait projeter y sur n'importe quelle autre direction ∆ 1,i de l’ERP1. 
Les angles droits OB1H et OB2H sont sous tendus par le même segment OH 
qui n'est autre que le vecteur yˆ . Les points B1 et B2 sont donc sur un cercle de 
diamètre OH (Figure 8) et l'on a  
© MODULAD, 2005 - 38 - Numéro 33 
HB~ˆ 11,1 += yy    ou  HB~ˆ 22,1 += yy  
D’une manière générale, quel que soit le point Bi, on a  
HB~ˆ ,1 ii += yy  
C'est-à-dire que le vecteur yˆ  peut être décomposé en deux vecteurs 
orthogonaux, i,1~y  et HBi . 
0
1,1
y
B
H
∆1,2
y 1,1
1 ∆
∆ 1,iB2
B
y 1,2
y 1,i
i
 
Figure 8 : le vecteur yˆ  est décomposé en deux vecteurs orthogonaux. 
Choisissons une direction particulière ∆1 pour projeter le vecteur y. On obtient 
le vecteur régressé, 1~y , et le vecteur des résidus 1e . Régressons maintenant le 
vecteur 1e  sur une direction ∆2 orthogonale à ∆1 et définit par BH. On obtient le 
vecteur régressé, 2~e , et le vecteur des résidus 2e  (Figure 9).  
 
0
∆
D
1
2
y
y
e (e  )
y
A
B
H
e 1
1
D1
e
2
2
∆ 2
 
Figure 9 : Le vecteur 1e  est régressé sur la direction ∆2 parallèle à la droite BH. 
© MODULAD, 2005 - 39 - Numéro 33 
Examinons le triangle AHB rectangle en H. On constate que la régression de 
1e  sur la direction du vecteur BH conduit au vecteur régressé BH lui-même et que le 
vecteur des résidus, 2e , n’est autre que le vecteur des résidus, eˆ , de la régression 
classique. 
La Figure 10 extrait deux triangles importants de la Figure 9 pour faciliter la 
compréhension : 
• le triangle OBH qui rassemble le vecteur projeté de la régression 
classique, yˆ  ; le vecteur projeté sur la direction ∆1, 1~y  ; le vecteur 2~e  
(régressé de 1e ) orthogonal à ∆1. 
• le triangle AHB qui rassemble le vecteur des résidus de la régression de 
y sur la direction ∆1, 1e  ; le vecteur des résidus de la régression 
classique, eˆ  ; le vecteur régressé de la régression de 1e  sur la direction 
BH, 2~e . 
 
0
y e
y
B
H
e 1
1
A
e 2
(e  )2
HB
e 2
 
Figure 10 : Triangles OBH et AHB extraits de la figure 9. 
Pour réaliser la régression de 1e  sur la direction de BH, il faut connaître les 
paramètres directeurs de cette direction. Pour cela, on régresse les vecteurs niveaux 
x1 et x2 sur ∆1. On obtient deux vecteurs des résidus 1,1r  et 2,1r  qui sont orthogonaux 
à ∆1 et qui sont donc parallèles à BH puisque l'on opère dans un plan, le plan D1 D2 
(Figure 11). On a ainsi les paramètres directeurs (identiques à ceux de BH) de la 
direction ∆2, orthogonale à ∆1.  
 
© MODULAD, 2005 - 40 - Numéro 33 
∆
0
B
H
D
D1
1
2
1
x
2
x
1,1
r
r
1,1
1,2
x 1,2
x
∆2
 
Figure 11 : Les vecteurs niveaux 1x  et 2x  sont décomposés en deux vecteurs 
projetés 1,1~x et 1,2~x et deux vecteurs des résidus 1,1r et 2,1r   
Finalement, pour retrouver yˆ  en effectuant une régression séquentielle 
orthogonale, il faut réaliser deux projections successives sur deux directions 
orthogonales : ∆1 et ∆2. 
Analysons ces deux projections. 
Projections sur ∆1 
Nous allons considérer deux projections (ou régressions) sur la droite ∆1 de 
paramètres directeurs 1t . La première projection correspond à la régression du 
vecteur y qui fournit une première solution 1~y . La seconde projection correspond à 
la construction d’un nouvel ERP orthogonal à la droite ∆1. 
Régression de y sur ∆1 
Nous avons déjà indiqué les relations donnant le vecteur régressé 1~y , sur la 
droite ∆1 et le vecteur des résidus 1e  correspondant : 
yHy 11~ =   {15} 
yHIe )( 11 −=   {16} 
avec la matrice de projection sur ∆1 
[ ] XtXtXttXH tttt 111111 −=   {17} 
Le choix de cette première direction de projection est entièrement libre et peut 
être déterminé au mieux des intérêts de l'étude.  
La matrice )( 1HI −  est la matrice de projection sur un espace orthogonal à ∆1. 
Régression de x1 et x2 sur ∆1 
Cette régression a pour objectif d'obtenir un ERP orthogonal à ∆1 et par 
conséquent, dans le cas de deux coefficients, parallèle à BH. Lorsqu’on régresse les 
vecteurs x1 et x2 sur ∆1, on obtient les vecteurs régressés 1,1~x  et 2,1~x qui sont portés 
© MODULAD, 2005 - 41 - Numéro 33 
par la droite ∆1 et les vecteurs des résidus 1,1r  et 2,1r  qui sont orthogonaux à cette 
même droite (Figure 12). On a 
111,1
~ xx H=   111,1 )( xr HI −=  
212,1
~ xx H=   212,1 )( xr HI −=  
Le nouvel espace des réponses possibles est orthogonal à ∆1 et son origine est 
en B. Il a une dimension de moins que ERP1, et par conséquent, il n’a qu’une 
dimension. C’est donc une droite, la droite ∆2. L’ERP2 est engendré par les vecteurs 
des résidus 1,1r  et 2,1r . Ces vecteurs peuvent être regroupés dans une matrice 1 rX  
telle que [ ]2,11,11 rr r =X   
Cette matrice permet d’écrire la relation suivante qui montre que l’ERP1 est 
projeté sur un espace orthogonal à ∆1 par la matrice de projection )( 1HI −  : 
XHIX )( 11 −= r   {18} 
Les vecteurs des résidus 1,1r  et 2,1r  sont dans le plan défini par les droites D1 
et D2, étant orthogonaux à la droite ∆1, ils sont parallèles entre eux. Ils ne sont donc 
pas linéairement indépendants et la matrice 
1 r
X  n'est pas de plein rang. 
Au vecteur 1t  (2 composantes) défini dans l’ERP1 correspond le transformé 
1tX  qui est repéré dans le référentiel des réponses et qui possède n composantes. 
 
∆
0
B
H
A
y
y
y
D
D1
1
2
1
x
2
x
1
e1
e
r
r
1,1
1,2
e
∆2
2
 
 
Figure 12 : Régression des vecteurs x1 et x2 sur la direction de la droite ∆2. 
Projections sur ∆2 
On considère une projection (ou régression) sur la droite ∆2 appartenant à 
ERP2 et dont les paramètres directeurs 2t  sont définis dans l’ERP1. Cette projection 
correspond à la décomposition du vecteur des résidus 1e  et elle conduit à la seconde 
solution 2~y  de la régression qui, dans ce cas, est yˆ . 
Le choix des paramètres directeurs de 2t  est entièrement libre. 
© MODULAD, 2005 - 42 - Numéro 33 
Régression de 1e  sur la direction ∆2 
Le vecteur BH peut être regardé comme le régressé de 1e  sur la direction de 
mêmes paramètres directeurs que les vecteurs 1,1r  et 2,1r . Cette régression s'effectue 
dans un plan orthogonal à ∆1 en B. Le résidu de cette régression est le résidu eˆ de la 
régression classique de y.  
On obtient le vecteur régressé : 
122
~BH eHe ==  
avec [ ]
1111 2
1
2222 rrrr XttXXttXH
tttt −=   {19} 
Au vecteur 2t  de l’ERP1 correspond le vecteur 21 tXr  (n composantes) défini 
dans le référentiel des réponses. 
Réduction dimensionnelle des ERP 
La projection )( 1HI −  projette les vecteurs de l’ERP1 sur une droite 
orthogonale à ∆1 (Figure 13). Elle transforme le vecteur 2t  défini dans ERP1, en un 
vecteur '2t  orthogonal à ∆1 situé dans l’ERP2 qui ne possède qu’une seule 
dimension. La droite ∆2 est parallèle à '2t . 
∆
0
B
D
D1
1
2
1
x
2x
r
r
1,1
1,2 ∆2
t1
t2
t'2
 
 
Figure 13 : La projection )( 1HI −  transforme l’ERP1 ayant deux dimensions  en 
ERP2 n’ayant plus qu’une dimension. 
Les solutions de la régression 
Il y a deux vecteurs pouvant prétendre être solution de la régression : le 
vecteur 1~y  projeté sur ∆1 et le vecteur yˆ de la solution classique (Figure 14). On a 
yHy 11~ =  
12121
~~~ˆ eHyeyy +=+=  
© MODULAD, 2005 - 43 - Numéro 33 
0
y
y
B
H
1
e 2
 
Figure 14 : Illustration de la décomposition du vecteur yˆ  . 
Coefficients des vecteurs régressés 
Les coefficients 1
~b du modèle mathématique donnant le vecteur régressé 1~y , 
sont fournis par la relation 
[ ] yXtXtXtttb ttttk 1111111 ~~ −==  
Les coefficients s’obtiennent également avec la relation : 
1
1
1
~)(~ yXXXb tt −=  
Les coefficients aˆ  du modèle mathématique donnant le vecteur régressé yˆ , 
sont fournis par la relation classique: 
[ ] yXXXa tt 1ˆ −=  
Résumé des principales opérations 
Le Tableau 2 et le Tableau 3 indiquent les principales opérations effectuées 
dans ERP1 et ERP2. 
 
 Projection sur ∆1 Projection ⊥ à ∆1 
 1H  )( 1HI −  
y 1~y  1e  
xi  i,1~x  i,1r  
X  
1 r
X  
Tableau 2 : Opérations dans ERP1 et obtention de ERP2 
 Projection sur ∆2 Projection ⊥ à ∆2 
 2H  )( 2HI −  
1e  2~e  eˆ  
Tableau 3 : Opérations dans ERP2  
© MODULAD, 2005 - 44 - Numéro 33 
 
5.2 TROIS COEFFICIENTS 
Dans le cas de 3 coefficients, il y a trois vecteurs-niveaux, x1, x2 et x3, portés 
par 3 droites D1, D2 et D3. Ces 3 droites définissent l'ERP initial ou ERP1. La 
régression classique de y conduit au vecteur yˆ  situé dans l'ERP1.  
On projette y sur une première direction ∆1. On obtient un premier vecteur des 
réponses calculées, 1~OB y=  (Figure 15). Si l'on choisit une autre droite de 
projection, on a un second vecteur des réponses calculées. Le lieu des extrémités de 
ces vecteurs est une sphère S1 de diamètre OH. 
Les vecteurs orthogonaux à OB en B sont situés dans un plan orthogonal à OB 
en B. Ce plan passe par la droite HA et il coupe la sphère S1 selon un cercle de 
diamètre BH. On peut alors décomposer le vecteur BH  en 2 vecteurs orthogonaux, 
BC  et CH . On a donc 
CHBCOBˆ ++=y  
Les 3 vecteurs OB , BC  et CH  étant orthogonaux entre eux, le vecteur yˆ  est 
décomposé en trois vecteurs orthogonaux. 
Finalement, pour retrouver yˆ  en effectuant une régression séquentielle 
orthogonale, on exécute trois projections successives sur trois directions 
orthogonales : ∆1, ∆2 et ∆3. 
 
D1
D
2
D3
C
∆1
O
y
B
H
S 1
 
Figure 15 : le vecteur y~  est décomposé en trois vecteurs orthogonaux. 
Projections sur ∆1 
Il y a deux projections (ou régressions) sur ∆1 de paramètres directeurs 1t . La 
première correspond à la régression du vecteur y qui fournit une première solution 
© MODULAD, 2005 - 45 - Numéro 33 
1
~y . La seconde correspond à la construction d’un nouvel ERP orthogonal à la droite 
∆1 . 
Régression de y sur ∆1 
On choisit la première direction de projection ∆1. On obtient, comme 
précédemment, le vecteur régressé 1~y  et le vecteur des résidus 1e correspondant. 
On a 
yHy 11~ =  
yHIe )( 11 −=  
avec la matrice de projection 
[ ] XttXXttXH tttt 111111 −=  
Régression des vecteurs niveaux sur ∆1 
Cette régression a pour objectif d'obtenir un nouvel ERP, l’ERP2, orthogonal à 
∆1. Cet espace a une dimension de moins que l'ERP1, donc ici deux dimensions. 
L'ERP2 est donc un plan orthogonal à ∆1 passant par le point B. 
On régresse les trois vecteurs niveaux x1, x2 et x3 sur ∆1. On obtient les 
vecteurs régressés 1,1~x , 2,1
~x  et 3,1~x qui sont portés par la droite ∆1 et les vecteurs 
des résidus 1,1r , 2,1r  et 3,1r  qui sont orthogonaux à cette même droite  
111,1
~ xx H=   111,1 )( xr HI −=  
212,1
~ xx H=   212,1 )( xr HI −=  
313,1
~ xx H=   313,1 )( xr HI −=  
B
r
∆ 2
1,1
r1, 2
r1, 3
∆ 1
 
 
Figure 16 : Les directions ir ,1  définissent un plan orthogonal à ∆1. 
Le nouvel espace des réponses possibles, orthogonal à ∆1 en B, est engendré 
par les vecteurs des résidus 1,1r , 2,1r  et 3,1r . On peut les regrouper dans la matrice 
1 r
X  telle que 
© MODULAD, 2005 - 46 - Numéro 33 
[ ]3,12,11,11 rrr r =X  
Les vecteurs des résidus 1,1r , 2,1r  et 3,1r  sont dans le plan orthogonal à la 
droite ∆1 et ils définissent l’ERP2 (Figure 16). Ils ne sont pas linéairement 
indépendants.  
On choisit librement les paramètres directeurs de 2t  qui toutefois ne doivent 
pas être tous nuls et qui doivent être différents de ceux de 1t . On définit ainsi la 
droite ∆2 située dans ERP2.  
On a la relation  
XHIX )( 11 −= r  
Projections sur ∆2 
Deux projections (ou régressions) sur ∆2 vont être considérées. La première 
projection correspond à la décomposition du vecteur des résidus 1e  et elle conduit à 
la seconde solution 2~y  de la régression. La seconde projection correspond à la 
construction d’un nouvel ERP orthogonal à la droite ∆2.  
Régression de 1e  sur la direction ∆2 
La régression du premier résidu 1e  sur la direction ∆2 conduit au vecteur 
régressé 2~e  porté par ∆2 et au vecteur des résidus 2e , orthogonal à ∆2 (Figure 17). 
 
B
∆ 2
e1
e2
e2 C
 
Figure 17 : Le résidu 1e  est décomposé en deux vecteurs orthogonaux 2e et 2~e . 
Le vecteur 2~e , régressé de 1e  sur ∆2, est égal à : 
122
~ eHe =   {20} 
Le vecteur des résidus 2e  correspondant à la régression de 1e  sur ∆2, est égal à 
: 
122 )( eHIe −=   {21} 
avec la matrice de projection 
[ ]
1111 2
1
2222     rrrr
XttXXttXH tttt −=   {22} 
© MODULAD, 2005 - 47 - Numéro 33 
Le premier vecteur des réponses calculées est le vecteur 1~y , projection de y  
sur la droite ∆1. La seconde régression introduit le vecteur 2~e  porté par la droite ∆2. 
La somme de ces deux vecteurs est un nouveau vecteur des réponses calculées que 
l'on peut dénommer 2~y  (Figure 18). Ce nouveau vecteur peut être considéré comme 
une solution de la régression 
121212
~~~~ eHyeyy +=+=   {23} 
 
0
1
y
y
e
∆
t
1
2 e2y1
2∆
1
B
t'2
C
A
e2
 
Figure 18 : La deuxième solution 2~y  est la somme vectorielle de 1~y  et de 2~e . 
Régression des vecteurs des résidus i,1r sur ∆2 
Cette régression a pour objectif de définir un nouvel ERP, l’ERP3, orthogonal 
à ∆2. Cet espace possède une dimension de moins que l'espace précédent, donc ici, il 
n’a qu’une seule dimension. L’ERP3 se réduit à la droite ∆3. C'est donc une droite 
orthogonale à ∆2 et l'on sait qu'elle passe par le point H de la régression classique.  
 
On régresse les trois vecteurs résidus 1,1r , 2,1r  et 3,1r  sur ∆2. On obtient les 
vecteurs régressés 1,2~x , 2,2~x et 3,2~x  qui sont portés par la droite ∆2 et de nouveaux 
vecteurs des résidus 1,2r , 2,2r  et 3,2r  qui sont orthogonaux à cette même droite  
1,121,2
~ rx H=   1,121,2 )( rr HI −=  
2,122,2
~ rx H=   2,122,2 )( rr HI −=  
3,123,2
~ rx H=   3,123,2 )( rr HI −=  
Le nouvel espace des réponses possibles est orthogonal à ∆2 en C et il est 
engendré par les vecteurs des résidus 1,2r , 2,2r  et 3,2r . Ces vecteurs sont regroupés 
dans la matrice 
2 r
X  telle que 
[ ]3,22,21,22 rrrr =X  
© MODULAD, 2005 - 48 - Numéro 33 
Les vecteurs des résidus 1,2r , 2,2r  et 3,2r  sont orthogonaux à la droite ∆2. Ils 
sont parallèles entre eux et définissent l’ERP3 qui ne possède qu’une seule 
dimension. Les vecteurs des résidus 1,2r , 2,2r  et 3,2r  ne sont pas linéairement 
indépendants et la matrice 
2 r
X  n’est pas de plein rang. 
On a 
12
)( 2   rr XHIX −=   {24} 
XHIHIX )()( 122 −−=r   {25} 
Projections sur ∆3 
C’est la dernière projection (ou régression) et elle sera exécutée sur ∆3 . Elle 
correspond à la décomposition du vecteur des résidus 2e  et elle conduit à la 
solution classique de la régression : yˆ .  
Régression de 2e  sur la direction ∆3 
On obtient le vecteur régressé (Figure 19): 
233
~CH eHe ==  {26} 
Le vecteur des résidus 3e  correspondant à la régression de 2e  sur ∆3, est égal 
à : 
233 )( eHIe −=   {27} 
avec [ ]
2222 3
1
3333 rrrr XttXXttXH
tttt −=   {28} 
 
3
e ∆1
e 2
2∆
B
C
A
e 3
H
e
e 2
(e  )3
 
Figure 19 : Le vecteur 2e  est décomposé en un vecteur régressé 3~CH e=  et un 
vecteur des résidus, 3e , qui est ici eˆ . 
 
© MODULAD, 2005 - 49 - Numéro 33 
Réduction dimensionnelle des ERP 
La projection )( 1HI −  projette les vecteurs de l’ERP1 sur un plan orthogonal à 
∆1. Elle transforme le vecteur 2t  défini dans ERP1, en un vecteur '2t orthogonal à ∆1 
et situé dans l’ERP2 qui ne possède que deux dimensions. Pour faire les calculs on 
passe dans le référentiel des réponses, le vecteur 2t  est transformé en 2tX . On a  
21
'
2 )( tXHItX −=  
La projection )( 1HI −  transforme le vecteur 3t  de ERP1 (trois dimensions) en 
un vecteur '3t  de ERP2 (deux dimensions). 
31
'
3 )( tXHItX −=  
La projection )( 2HI −  transforme le vecteur '3t  de ERP2 (deux dimensions) 
en un vecteur "3t  de ERP3 (une dimension). 
312
'
32
"
3 ))(()( tXHIHItXHItX −−=−=  
On peut donc choisir au mieux des intérêts de l’étude les directions ∆i définies 
par les paramètres directeurs it . 
Les solutions de la régression 
Il y a trois vecteurs candidats comme solution de la régression : le vecteur 1~y  
projeté sur ∆1, le vecteur 2~y  qui résulte de la seconde projection et le vecteur yˆ qui 
est la solution classique. On a 
yHy 11~ =  
121212
~~~ eHyHeyy +=+=  
32132
~~~~~ˆ eeyeyy ++=+=  
23121ˆ eHeHyHy ++=  
Pour faciliter la compréhension, on peut représenter les trois vecteurs solutions 
dans un plan ne prenant en compte que les vecteurs iy~  et les vecteurs ie~  (Figure 
20). 
0
y
y
B
1
e 3
C
y
2 e 2
 
Figure 20 : Illustration, dans un plan, des trois solutions de la régression. 
 
© MODULAD, 2005 - 50 - Numéro 33 
Coefficients des vecteurs régressés 
Les coefficients 1
~b  du modèle mathématique donnant le premier vecteur 
régressé 1~y , sont fournis par la relation [ ] yXttXXtttb ttttk 1111111 ~~ −==  
Les coefficients 2
~b  du modèle mathématique donnant le second vecteur 
régressé 2~y , sont fournis par la relation : 
[ ] 212 ~~ yXXXb tt −=  
Les coefficients aˆ  du modèle mathématique donnant le troisième vecteur 
régressé yˆ , sont fournis par la relation classique : 
[ ] yXXXa tt 1ˆ −=   
Résumé des principales opérations 
Le  
Tableau 4, le Tableau 5 et le Tableau 6 indiquent les principales opérations 
effectuées dans ERP1, ERP2 et ERP3. 
 
 Projection sur ∆1 Projection ⊥ à ∆1 
 1H  )( 1HI −  
y 1~y  1e  
xi  i,1~x  i,1r  
X  
1 r
X  
 
Tableau 4 : Opérations dans ERP1 et obtention de ERP2 
 Projection sur ∆2 Projection ⊥ à ∆2 
 2H  )( 2HI −  
1e  2~e  2e  
i,1r  i,2~x  i,2r  
1 r
X   
 2r
X  
Tableau 5 : Opérations dans ERP2 et obtention de ERP3 
 
© MODULAD, 2005 - 51 - Numéro 33 
 
 Projection sur ∆3 
 
Projection ⊥ à ∆3 
 3H  )( 3HI −  
2e  3~e  eˆ  
Tableau 6 : Opérations dans ERP3  
5.3 N COEFFICIENTS 
Les raisonnements que nous avons tenus peuvent s’appliquer lorsqu’il y a n 
coefficients. Dans ce cas, on a n vecteurs-niveaux, x1, x2,…, xn, portés par n droites 
D1, D2, …, Dn. Ces n droites définissent l'ERP1. La régression classique de y 
conduit au vecteur yˆ  situé dans l'ERP1.  
On projette y sur une première direction ∆1. On obtient un premier vecteur des 
réponses calculées OB . Si l'on considère toutes les directions possibles, le lieu des 
extrémités des vecteurs iOB  est une hypersphère S1 à p dimensions de diamètre 
OH. Les vecteurs orthogonaux à OB en B sont dans un hyperplan orthogonal à OB 
en B. Cet hyperplan passe par la droite HA et il coupe l'hypersphère S1 selon une 
hypersphère S2 de dimensions p-1 de diamètre BH. On peut alors décomposer BH en 
p-1 vecteurs orthogonaux en passant par des hypersphères dont les dimensions 
diminuent d'une unité à chaque fois. On termine ce processus de décomposition par 
un cercle et deux vecteurs orthogonaux. Le vecteur yˆ  peut ainsi être décomposé en 
p vecteurs orthogonaux.  
Dans l’ERP1, on régresse y sur une direction ∆1. On obtient un vecteur 
régressé, 1~y  porté par ∆1, et un vecteur résidu 1e  situé dans l'hyperplan orthogonal à 
∆1 en B. On a 
yHy 11~ =  
yHIe )( 11 −=  
On définit le nouvel ERP, l’ERP2, en régressant les vecteurs niveaux x1, x2, … 
xp sur ∆1. Les vecteurs des résidus obtenus 1,1r , 2,1r  et p,1r  engendrent l’ERP2 qui à 
p-1 dimensions et qui est orthogonal à ∆1. On définit une direction de projection ∆2 
sur laquelle on projette le résidu 1e . On obtient un vecteur régressé 2~e  porté par la 
droite ∆2 et un vecteur des résidus 2e  orthogonal à ∆2.  
122
~ eHe =  
122 )( eHIe −=  
On obtient ainsi une seconde solution de la régression 
121212
~~~~ eHyeyy +=+=  
On peut poursuivre la décomposition en définissant un troisième ERP, l’ERP3. 
On obtient ERP3 en régressant les vecteurs des résidus 1,1r , 2,1r  et p,1r  sur ∆2. Cette 
© MODULAD, 2005 - 52 - Numéro 33 
régression fournit les résidus 1,2r , 2,2r  et p,2r  qui sont orthogonaux à ∆2 et qui 
engendrent l’ERP3 qui ne comprend plus que p-2 dimensions. On choisit une 
troisième direction de projection ∆3. 
Le vecteur des résidus 2e  est projeté sur la nouvelle direction, ∆3, 
orthogonale à ∆1 et à ∆2. On obtient un vecteur projeté, 3~e , et un nouveau vecteur 
des résidus, 3e  (Figure 21).  
233
~ eHe =  
233 )( eHIe −=  
∆
2
e
A
B
H
e
1
e
e
C
2
∆
e
A
H
e
e
D
3
3
C
2e
2
3
Figure 21 : A gauche le vecteur 1e  est régressé sur la direction ∆2. Le vecteur des 
résidus 2e  de cette régression est à son tour régressé sur direction ∆3 (figure de 
droite). Le vecteur des résidus 3e  ainsi obtenu sera, lui aussi, régressé sur une 
nouvelle direction ∆4. 
On obtient ainsi une troisième solution, 3~y  (Figure 22) : 
321323
~~~~~~ eeyeyy ++=+=  
0
y
e2
2
e3
y
3∆
3
y
e3
C
D
A
 
 
Figure 22 : La troisième solution 3~y  est la somme vectorielle de 2~y  et de 3~e . 
© MODULAD, 2005 - 53 - Numéro 33 
A son tour le résidu, 3e  est décomposé en un vecteur projeté, 4
~e , et un 
vecteur des résidus, 4e . De décomposition en décomposition, on arrive à retrouver 
le vecteur yˆ  de la régression classique. La Figure 23 illustre les solutions 
successives proposées par la régression séquentielle orthogonale. 
0
y
e
2
e2
y
3∆
3
y
∆ 2
1∆
y
1
y
y
4
∆ 4
e 3
e4
 
Figure 23 : La régression séquentielle orthogonale propose plusieurs solutions pour 
lier la réponse aux facteurs. 
Réduction dimensionnelle des ERP 
La projection )( 1HI −  projette les vecteurs de ERP1 sur un espace orthogonal 
à ∆1. Elle transforme le vecteur it  (p composantes dans ERP1et n composantes dans 
le référentiel des réponses) de l’espace ERP1 de p dimensions en un vecteur 'it  
orthogonal à ∆1 et situé dans l’espace ERP2 de p-1 dimensions. Elle transforme la 
matrice X en une matrice 
1 r
X  dont les colonnes, considérées comme des vecteurs, 
définissent ERP2. 
ii tXHItX )( 1
' −=  XHIX )( 11 −= r  
La projection )( 2HI −  transforme le vecteur 'it  de ERP2 (p-1 dimensions) en 
un vecteur "it  de ERP3 (p-2 dimensions). Elle transforme 1 rX  en 2 rX  qui définit 
ERP3. 
 
'
2
" )( ii tXHItX −=  ou ii tXHIHItX ))(( 12" −−=  
12
)( 2   rr XHIX −=  ou XHIHIX )()( 122 −−=r  
 
Les projections se poursuivent jusqu’à obtenir l’ERP( j ). Le vecteur ')1j( −it  est 
projeté (j-1) fois, on a donc 
© MODULAD, 2005 - 54 - Numéro 33 
ii tXHIHIHItX ))(()( 121j
')1j( −−−= −− "  
La matrice 
1-jr
X est obtenue après (j-1) projections, on a donc 
XHIHIHIX )()()( 121j1j −−−= −− "r  
Les solutions de la régression 
Il y a p solutions de la régression depuis le vecteur 1~y  projeté sur ∆1 jusqu’au 
vecteur yˆ de la solution classique. On a simplement 
∑=
=
+=
ki
i
ik
2
1
~~~ eyy  
On peut également écrire les solutions successives avec les matrices de projections :  
yHy 11~ =  
yHIHyHeHyeyy )(~~~~ 121121212 −+=+=+=  
yHIHIHyHIHyHy )()()(~ 1231213 −−+−+=  
etc. 
La régression séquentielle orthogonale propose donc plusieurs solutions 
possibles puisque chaque vecteur iy~  représente un modèle mathématique permettant 
d'obtenir des réponses calculées proches des réponses mesurées.  
On peut représenter, dans un plan, toutes les solutions de la régression en ne 
prenant en compte que les vecteurs iy~  et les vecteurs ie~  (Figure 24). 
 
0
y
y
B
p-1
e 4
C
e 2y 2
y
1
y
3
e
3e
p
y
4
E
D
G
H
 
Figure 24 : Illustration, dans un plan, des p solutions de la régression 
 
© MODULAD, 2005 - 55 - Numéro 33 
Coefficients des vecteurs régressés 
Les coefficients aˆ  du modèle mathématique donnant le vecteur régressé 
classique yˆ , sont fournis par la relation classique : 
yXXXa tt 1)(ˆ −=  
Les coefficients ib
~  du modèle mathématique correspondant aux diverses 
solutions possibles sont donnés par 
[ ] itti yXXXb ~~ 1−=  
6.  REGRESSION PLS1 
La régression PLS1 est une simple application de la régression séquentielle 
orthogonale. Dans ce cas, on décide que les paramètres directeurs de la première 
direction de projection ∆1 sont proportionnels aux covariances entre le vecteur des 
réponses mesurées, y, et les vecteurs-niveaux. On a donc 
11
,
,2
,1
1
,1
2,1
1,1
1
)(
)(
)(
Cov
y
y
y
t κκ =
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
pp Cov
Cov
Cov
t
t
t
x
x
x
##   {29} 
•  1t matrice (p,1) des paramètres directeurs de la droite ∆1 sur laquelle 
on projette. 
•  1κ  scalaire de proportionnalité. 
•  1Cov  matrice (p,1) des covariances entre le vecteur des réponses 
mesurées, y , et les vecteurs-niveaux, ix . Ces covariances sont 
connues. 
 
Dans ce cas la matrice de projection est égale à 
[ ] XCovXCovXCovXCovH tttt 111111 −=  
D’où le vecteur 1~y , projection de y  sur la droite ∆1 : 
[ ] yXCovXCovXCovXCovy tttt 111111~ −=  
Ce vecteur des réponses calculées, 1~y , obtenu par régression sur la droite ∆1, 
est appelé premier vecteur des réponses PLS 1.  
Le vecteur des résidus 1e  est situé dans l'hyperplan orthogonal à ∆1 en B et il 
est égal à 
yHIe )( 11 −=  
C'est ce vecteur des résidus, 1e , qui va être décomposé à nouveau dans un ERP 
de p-1 dimensions défini par les premiers vecteurs des résidus 1,1r , 2,1r ,…, p,1r . 
Dans la régression PLS1, on définit la seconde direction de projection par les 
covariances entre le vecteur des résidus 1e  et les vecteurs des résidus i,1r .  
© MODULAD, 2005 - 56 - Numéro 33 
22
1,1
12,1
11,1
,2
2,2
1,2
2
),(
),(
),(
Cov
er
er
er
t κ=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
pp Cov
Cov
Cov
t
t
t
##  
On obtient un vecteur régressé 2~e  et un vecteur des résidus 2e . Comme dans 
la régression séquentielle orthogonale, on poursuit les décompositions jusqu'à 
obtenir le vecteur yˆ de la régression classique. Pour faciliter le choix de la meilleure 
solution il existe des critères statistiques tels que le Q2 ou le PRESS. L'idée générale 
de ces critères est de s'arrêter quand les vecteurs ie~  que l'on ajoute deviennent égaux 
ou inférieurs à l'erreur. 
On retrouve toutes les relations que nous avons indiquées en remplaçant it  par 
sa valeur : 
ii
ipi
ii
ii
pi
i
i
i
Cov
Cov
Cov
t
t
t
Cov
er
er
er
t κ=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
−−
−−
−−
),(
),(
),(
1,1
12,1
11,1
,
2,
1,
##   {30} 
 
Si la régression PLS conduit à ne retenir que deux axes de projections pour 
obtenir le modèle le plus proche de la meilleure solution, il est possible avec la 
RLSO de n’effectuer qu’une seule projection. En effet, il suffit de projeter le vecteur 
des réponses mesurées sur un axe dont les paramètres directeurs sont proportionnels 
aux coefficients du vecteur solution de la PLS. Dans ce cas on pourra choisir 
d’autres directions de projection de manière à entourer la solutions PLS. 
L’estimation du meilleur modèle sera ainsi réalisée sur des projections voisines de la 
solution PLS mais pouvant être meilleures encore. La régression PLS ne donne que 
des solutions discontinues alors que la régression RLSO permet d’obtenir toutes les 
solutions possibles et de choisir la meilleure en accord avec le critère d’optimisation. 
7.  COMPARAISON DES REGRESSIONS PLS1 ET RLSO 
La différence la plus importante entre ces deux régressions séquentielles est le 
libre choix de la direction de projection offert par la RLSO. En effet, les deux 
régressions offrent le même nombre de solutions possibles mais la PLS1 impose 
l’emplacement de ces solutions dans l’espace défini par le référentiel des réponses. 
Au contraire, la RLSO offre la possibilité de placer les solutions en une infinité de 
positions. Il est ainsi possible de regrouper les solutions dans un faible espace 
favorable et de choisir la meilleure solution. Ce choix peut se faire en utilisant les 
critères mis au point pour la PLS1 et en les associant à des plans d’expériences pour 
trouver la meilleure solution ou le meilleur compromis possible. Avec la RLSO, il 
est aussi possible d’imposer des contraintes comme, par exemple, la valeur d’un ou 
de plusieurs coefficients. En particulier il est facile d’éliminer tous les coefficients 
non significatifs et de trouver la meilleure solution utilisant l’influence de quelques 
facteurs et répondant à des impératifs bien précis. La souplesse de la RLSO s’oppose 
à la rigidité de la PLS1 et offre par là une multitude de possibilités nouvelles à tous 
les utilisateurs de la régression. 
 
© MODULAD, 2005 - 57 - Numéro 33 
 
ANNEXE 1 
CALCUL DU COEFFICIENT k~  DE LA REGRESSION SUR UNE DIRECTION 
La solution des moindres carrés est la suivante : 
)()( XbyXbyee −−= tt  
)()( XbyXbyee −−= tttt  
XXbbXybyXbyyee ttttttt +−−=  
∆∆∆∆ +−−= XtXtyXttXyyyee ttttttt kkk 2  
∆∆∆∆ +−−=∂
∂ XtXtyXtyXtee ttttt
t
k
k
2  
0~2 =+−− ∆∆∆∆ XtXtyXtyXt ttttt k  
Or, y étant une matrice (n,1) ; X une (n,p) et t une (p,1), on a 
XytyXt ttt ∆∆ =  
car un scalaire est égal à son transposé. D’où 
∆∆∆ = XtXtXyt tttt k~  
[ ] yXttXXt ttttk ∆−∆∆= 1~  
k~ est bien un scalaire : 
(1,p)(p,n)(n,p)(p,1)(1,p)(p,n)(n,1) = (1,1) 
D’où les composantes du vecteur régressé 
∆= tb k~~  
ANNEXE 2 
DEFINITION DES DIRECTIONS 
Les directions ont été définies par les paramètres directeurs des vecteurs 
unitaires t . Mais il est tout à fait possible d’utiliser d’autres vecteurs à condition, 
bien sûr, qu’ils possèdent la même direction que t . Si on a utilisé un vecteur v 
parallèle à t  mais n'étant pas unitaire, on a 
vt κ=  
Les coefficients du modèle mathématique s'écrivent alors 
[ ] yXvvXXvvtb ttttk κκκκ 11~~ −==  
soit 
[ ] yXvvXXvvb tttt 1~ −=  
Le coefficient de proportionnalité a disparu de la matrice des coefficients.  
© MODULAD, 2005 - 58 - Numéro 33 
Considérons maintenant la matrice de projection 
[ ] XvvXXvvXH tttt κκκκ 1−=  
soit 
[ ] XvXvXvvXH tttt 1−=  
Le coefficient de proportionnalité a disparu de la matrice de projection.  
 
A la place de t , on peut employer les caractéristiques d’un vecteur parallèle. 
ANNEXE 3 
ORTHOGONALITE DES DIRECTIONS DE PROJECTION 
On peut vérifier l’orthogonalité des directions ∆i. On opère dans le référentiel 
orthonormé des réponses et on utilise les transformés itX  des vecteurs it . 
Calculons le produit scalaire PS du vecteur 1t  de l’ERP1 avec le vecteur 2t  projeté 
dans l’ERP2. On a [ ]'21 1 tXXt rttPS =  
soit 
[ ]'211 )( tXHIXt −= ttPS  
[ ] XttXXttXH tttt 111111 −=  
[ ][ ]'2111111 )( tXXtXtXttXIXt ttttttPS −−=  
[ ][ ]'21111111 )( tXXtXtXttXXtXt ttttttttPS −−=  
[ ]'211 )( tXXtIXt ttttPS −=  
[ ]'211 )( tXXtXt ttttPS −=  
0)( '2 == tX0PS  
ANNEXE 4 
Dans la Figure 24 les triangles OBC, OCD, …OGH sont tous rectangles. On a 
donc : 
yyyyy ˆ~~~~ 1321 ≤≤≤≤≤ −p"  
C’est donc le vecteur de la régression classique qui a la plus grande norme. 
ANNEXE 5 
Le triangle OBA de la figure 6 est rectangle, on a donc la relation de 
Pythagore : 
11
~ eyy +=  
1111
~~ eeyyyy ttt +=  
© MODULAD, 2005 - 59 - Numéro 33 
Quand on centre et que l’on réduit les réponses brutes (avec n-1 pour le calcul 
de la variance), la somme des carrés des réponses centrées réduites est égale à n-1, 
soit  
1−= nt yy  
D’après la figure 17, on a donc la relation de Pythagore : 
222211
~~ eeeeee ttt +=  
333322
~~ eeeeee ttt +=  
33332211
~~~~ eeeeeeee tttt ++=  
eeeeeeeeeeee ˆˆ~~~~~~~~ 33332211
t
pp
ttttt +++++= "  
 
121212
~~~~ eHyeyy +=+=  
321323
~~~~~~ eeyeyy ++=+=  
221122
~~~~~~ eeyyyy ttt +=  
332233
~~~~~~ eeyyyy ttt +=  
443344
~~~~~~ eeyyyy ttt +=  
44332244
~~~~~~~~ eeeeyyyy tttt ++=  
4433221144
~~~~~~~~~~ eeeeeeyyyy ttttt +++=  
pp
ttttt
pp
tt eeeeeeeeyyyyyy ~~~~~~~~~~~~ˆˆ 44332211 +++++== "  
On a aussi 
1ˆˆˆˆ~~~~~~ 111122221111 −=+=+==+=+= −−−− nttpptpptttttt eeyyeeyyeeyyeeyyyy "  
ANNEXE 6 
Connaissant X, y et les ti,  on peut calculer toutes les réponses solutions. 
Première réponse 
[ ] XtXtXttXH tttt 111111 −=  
yHy 11~ =  
yHIe )( 11 −=  
Deuxième réponse 
XHIX )( 11 −= r  
[ ]XXtXtXttXIX tttt 11111 )(1 −−= r  
© MODULAD, 2005 - 60 - Numéro 33 
122
~ eHe =  
121212
~~~~ eHyeyy +=+=  
122 )( eHIe −=  
2H   peut être exprimé avec X, y et les ti,   
Troisième réponse 
XHIHIX )()( 122 −−=r  
[ ]
2222 3
1
3333 rrrr XttXXttXH
tttt −=  
233
~ eHe =  
321323
~~~~~~ eeyeyy ++=+=  
233 )( eHIe −=  
3H   peut être exprimé avec X, y et les ti,   
k ième réponse 
XHIHIHIX )()()( 1211 −−−= −− "kkr  
[ ]
1111
1
−−−−
−=
kkkk
t
k
t
k
t
k
t
kk rrrr XttXXttXH  
1
~
−= kkk eHe  
kkkk eeeyeyy ~~~~~~~ 3211 ++++=+= − "  
1)( −−= kkk eHIe  
kH   peut être exprimé avec X, y et les ti,   
ANNEXE 7 
Les grandeurs 
ir
X  et ke   suivent la même évolution au cours des calculs. En 
effet, on a  
ii tXHIHIHItX ))(()( 121j
')1j( −−−= −− "  
XHIHIHIX )()()( 121j1j −−−= −− "r  
112 )()()( eHIHIHIe −−−= "kk  
Avec les matrices de projection, on a  
112
~ eHHHe "ii =  
 
 
© MODULAD, 2005 - 61 - Numéro 33 
ANNEXE 8 
Si l’on note pt  les vecteurs t mesurés dans l’espace des réponses possibles 
ERP1 de p dimensions, la norme de ces vecteurs est est égale à  
pp
t
p ttt =  
Si l’on note nt  les vecteurs t mesurés dans le référentiel des réponses de n 
dimensions, on a  
nn Xtt =  
Par conséquent, la norme de ces vecteurs est est égale à 
n
t
n
t
n XtXtt =  
On a donc  
n
t
n
t
pp
t XtXttt =  
La matrice XXt  est, à un facteur n-1 près, égale au tenseur fondamental ou 
tenseur métrique de l’espase des réponses possibles.  
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
=
22
22
t
21
21
t
21
21
t
11
11
t
xx
xx
xx
xx
xx
xx
xx
xx
G  
En effet, les éléments du tenseur métrique sont les produits scalaires des 
vecteurs de base normés de la nouvelle base. Ce tenseur peut également être écrit : 
⎥⎦
⎤⎢⎣
⎡=
1),(cos
),(cos1
21
21
xx 
xx 
G  
On reconnait ici la matrice de variance covariance des vecteurs niveaux. 
ANNEXE 9 
La régression PLSl est illustrée par le fichier Excel « PLS1_4_coef ». 
Le lecteur pourra entrer les 4 coefficients d’un modèle et retrouver les 3 
solutions PLS ainsi que la solution classique de la régression. Il pourra constater que 
la quatrième solution PLS1 est égale à la solution classique. 
ANNEXE 10 
La RLSO (Régression Linéaire Séquentielle Orthogonale) est illustrée par le 
fichier Excel « RLSO_4_coef ». 
 
Le lecteur pourra entrer un modèle de 4 coefficients et modifier à sa guise les 
directions de projections. Le logiciel calculera les 3 solutions RLSO ainsi que la 
solution classique de la régression. Il pourra constater que la quatrième solution 
RLSO est toujours égale à la solution classique.  
Lorsque les directions de projection sont celles de la régression PLS1, la 
RLSO fournit les mêmes solutions que la régression PLS1. 
Ces exemples sous Excel ont bénéficié de l’aide de Henry Aubert et de 
Jacques Vaillé.  
© MODULAD, 2005 - 62 - Numéro 33 
 
Bibliographie 
Régression classique 
1.  DRAPER Norman and SCHMITT H 
"Applied Regression Analysis" 
John Wiley and Sons. New-York. 708 pages. (1981). 
 
2.  BATES Douglas M. and WATTS Donald G. 
"Nonlinear Regression analysis and its applications" 
John Wiley and Sons. New-York. 365 pages. (1988). 
 
3.  WILLIAMS E.J. 
"Regression Analysis" 
John Wiley and Sons. New-York. (1967). 
 
Régression PLS 
 
4. TENENHAUS Michel 
"La régression PLS. Théorie et pratique" 
Editions Technip. 254 pages (1998. )Paris. ISBN 2-7108-0735 
 
5.  WOLD H. 
"Soft modeling. The basic design and some extensions" 
In vol II of Joreskog K.G.and Wold H Eds. 
System under indirect observation. North Holland, Amsterdam. (1982). 
 
6.  WOLD Swante, RUHE A., DUNN III W. J. and WOLD H. 
"The collinearity problem in Linear Regression. The Partial Least Square Approach to 
Generalized Inverses" 
SIAM J. Sci. Stat. Comp. 5 (1984). p. 735-743. 
 
7.  HOSKULDSSONN A. 
"PLS Regression Methods" 
J. Chemometrics 2, p. 211-228 (1988).  
 
8.  TENENHAUS M., GAUCHI Jean-Pierre et MENARDO C. 
"Régression PLS et applications" 
Rev. Statistique Appliquée vol. XLIII (1), p.7-63. (1995). 
 
9.  GAUCHI Jean-Pierre 
"Utilisation de la régression PLS pour l'analyse des plans d'expériences en chimie de 
formulation" 
Rev. Statistique Appliquée vol. XLIII (1), p.65-89. (1995). 
 
10.  BRANDVIK P.J. and DALING P.S. 
"Optimising oil spill dispersants as a function of oil type and weathering degree: a 
multivariate approach using partial least squares (PLS)." 
Chemometrics and Intelligent Laboratory Systems 42 (1998): 73-91. 
 
11.  LINDGREN F., GELADI P. WOLD S. 
"The kernel algorithm for PLS" 
J. Chemometrics. 7, p. 45-59. (1993). 
 
12.  CHAVENT Marie et PATOUILLE Brigitte. 
"Calcul des ciefficients de régression et du Press en régression PLS1" 
Modulad n° 30 .  p. 1-11. (2003). 
 
13.  BASTIEN Philippe, VINZI Vincenzo Esposito, TENENHAUS Michel 
"PLS generalised linear regression" 
Computational statistics and data analysis.  48, p. 17-46. (2005). 
