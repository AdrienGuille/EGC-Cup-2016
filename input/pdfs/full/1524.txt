Inférence dans les HMM hiérarchiques et factorisés :
changement de représentation vers le formalisme des
Réseaux Bayésiens.
Sylvain Gelly∗, Nicolas Bredeche∗, Michèle Sebag∗
∗Equipe Inference&Apprentissage - Projet TAO (INRIA futurs),
LRI, Université Paris-Sud, 91504 Orsay Cedex
(gelly,bredeche,sebag)@lri.fr
1 Présentation du problème
Une limite essentielle des HMM, et plus généralement des modèles de Markov,
concerne le passage à l’échelle, l’impossibilité de la prise en compte efficace de l’influence
de phénomènes indépendants et la difficulté de généralisation.
Pour répondre à ces problèmes, plusieurs extensions existent. En particulier, nous
nous intéresserons dans ce qui suit à la hiérarchisation (Theocharous et al. 2001, 2004)
et à la factorisation (Ghahramani 1996).
La hiérarchisation permet de réduire le nombre de liens entre états nécessaires dans
un HMM et par là même de réduire la complexité algorithmique de l’apprentissage ainsi
que l’imprécision. Quant à la factorisation, le principe est d’expliquer les observations
par plusieurs causes plutôt qu’une seule. C’est à dire qu’on remplace le P (Y |X) des
HMM par P (Y |X1, X2, ..., Xn). Les X i sont des variables cachées pouvant être gérées
indépendamment. Les P (X it+1|X
i
t) sont alors différents pour chaque i.
– L’existence de dépendances multiples dans les FHHMM entraîne à priori une
explosion combinatoire du nombre de paramètres à apprendre, ce qui est d’autant
plus problématique lorsque peu d’exemples sont à notre disposition (ceci est une
propriété inhérente à la robotique) ;
– La présence de circuits dans les dépendances conditionnelles entre les variables
d’un FHHMM empêchent la modélisation directe par un réseau bayesien. Il est
à noter que ces dépendances ne concernent les variables qu’à un même pas de
temps (synchrones).
Dans la suite de cet article, nous ne ferons pas de différence entre les dépendances
synchrones et les transitions temporelles, les deux types étant des dépendances condi-
tionnelles entre deux variables.
On ne peut ainsi pas adapter directement les algorithmes existants dans le cas des
HMM factorisés, ou hiérarchiques.
Un aspect important du problème est que notre système apprend à partir de don-
nées éparses car nous faisons l’hypothèse que nous ne disposons que d’un petit nombre
d’exemples pour apprendre. Ceci se justifie par le domaine d’application (la robotique
située), où le processus d’échantillonnage des données est contrôlé par un compor-
tement dépendant entre autres de l’environnement et des capacités du robot qui ne
permet pas d’obtenir beaucoup d’exemples. Par conséquent, nous souhaitons exprimer
un compromis entre précision et vitesse de l’apprentissage.
- 57 - RNTI-E-5
Inférence dans les FHHMM
2 Changement de représentation vers les
Réseaux Bayésiens
L’approche que nous proposons consiste à changer de formalisme de représentation
en transformant un graphe orienté et cyclique (i.e. HMM hiérarchique factorisé dont on
fait abstraction du typage des dépendances) en un réseau bayésien. Le formalisme des
réseaux bayésiens s’inscrit en effet dans un cadre théorique développé et bien connu,
ce qui laisse espérer une résolution plus facile.
Toutefois, deux problèmes se posent lors d’un tel changement de représentation : (1)
le coût de la prise en compte des dépendances multiples qui peuvent exister pour une
variable (i.e. calculer P (A|B1, B2, ..., Bn) soit 2n paramètres dans le cas de variables
binaires) et (2) celui de la reformulation des éventuels circuits du graphe en vue de leur
modélisation dans le cadre d’un réseau bayésien.
La solution que nous proposons repose sur un assouplissement des contraintes liées
aux dépendances multiples lors du changement de représentation. En effet, nous pro-
posons de décomposer les dépendances multiples en les prenant en compte deux à
deux (i.e. en traitant séparément P (A|B1), P (A|B2), ..., P (A|Bn), soit 2n paramètres,
toujours dans le cas de variables binaires – et en introduisant par ailleurs certaines
contraintes de mise à jour).
Soit V1, V2, ..., Vn, n variables aléatoires discrètes, de modalités respectives m1, ...,
mn. On suppose connus les pi = P (Vi) (vecteur de taille mi), pour tout i, ainsi que
certains pi,j = P (Vj |Vi), j ∈ Ii ⊂ {1, ..., n} (pi,j est une matrice de taille (mi,mj)).
Ce système peut être représenté par un graphe, dans lequel les noeuds sont les
variables aléatoires Vi, et les arcs ai,j représentent les pi,j . La structure induite par
les probabilités conditionnelles n’est pas contrainte (il peut par exemple exister des
circuits). Pour simplifier la notation, on introduit le terme de Réseaux Bayésiens
Aplatis (ou RBA) pour désigner les réseaux ainsi générés dans la suite de cet article.
Il reste maintenant à déterminer comment exprimer ce système dans le formalisme des
Réseaux Bayésiens .
Pour chaque couple de variables dépendantes (A,B), une variable additionnelle de
parents A et B est ajoutée. Ceci présente le double avantage de limiter la complexité de
la prise en compte des dépendances multiples (au prix d’une approximation), et d’éviter
les circuits puisque que seules les variables additionnelles ont des noeuds parents dans le
nouveau formalisme. Une fois cette reformulation réalisée, il est alors trivial de réaliser
l’inférence souhaitée.
Pour chaque variable Vi du graphe d’origine, nous associons une variable du réseau
bayésien, de même modalité, que nous continuerons à noter Vi.
Pour chaque arc ai,j nous associons une variable additionnelle booléenne dans le
réseau bayésien, que nous noterons Ai,j . Les Ai,j ont exactement deux parents dans
le réseau bayésien, qui sont Vi et Vj (i.e. une V-structure). Ces variables sont artifi-
ciellement observées dans le but d’induire une dépendance entre les variables Vi et Vj
(valeur d’observation fixée à vrai).
Une fois les variables additionnelles ajoutées, il reste à calculer leurs probabilités
conditionnelles pour terminer la transformation du réseau bayésien. C’est à dire cal-
culer les P (Ai,j |Vi, Vj). Nous utilisons un système axiomatique à satisfaire pour que
- 58 -RNTI-E-5
Gelly et al.
Fig. 1 – Cas de distribution défavorable - Apprentissage avec peu d’exemples.
les probabilités P (Ai,j |Vi, Vj) atteignent un point fixe (i.e. stable). Ce point fixe est
atteint par itérations successives d’un algorithme inspiré de EM. La satisfaction de ce
système axiomatique garantit un comportement cohérent du réseau au regard de la
prise en compte des dépendances deux à deux par rapport à une prise en compte dans
un réseau classique.
Les axiomes et le système d’équation induit par les axiomes sont donnés en détail
dans l’article publié dans les actes de l’atelier EGC 2005 sur les Modèles Probabilistes.
3 Expérience d’apprentissage avec peu d’exemples
Nous avons choisi d’évaluer la capacité d’apprentissage des réseaux bayésiens aplatis
en évaluant leur performance sur l’approximation d’une distribution générée par un
réseau bayésien classique de taille 4. (cf. fig. 1). Ce générateur représente le pire des cas
pour l’apprentissage par RBA car du fait de sa structure, la distribution jointe peut
être quelconque.
Pour chaque réseau apprenant, l’erreur est définie comme la distance de Kullback-
Leibler entre la distribution jointe de celui-ci et de celle du réseau générateur. Les
résultats sont issus d’une étude exhaustive du comportement de l’ensemble des réseaux
bayésiens classiques et aplatis sur les données issues du générateur (chaque réseau est
évalué plusieurs fois pour bien approximer les résultats).
La figure 1 permet d’observer que les réseaux bayésiens aplatis apprennent mieux
(à la fois en moyenne et pour le meilleur) que les réseaux bayésiens classiques lors-
qu’il y a peu d’exemples. En revanche, les performances s’inversent avec un grand
nombre d’exemples. Ces résultats illustrent notre compromis entre rapidité et préci-
sion d’apprentissage puisqu’ils montrent bien que les RBA apprennent mieux avec peu
- 59 - RNTI-E-5
Inférence dans les FHHMM
d’exemples pour ensuite perdre progressivement cet avantage.
4 Conclusions
Dans le cadre de cet article, nous nous sommes intéressés à la transformation d’un
graphe (en pratique un HMM hiérarchique et factorisé) en un réseau bayésien selon cer-
taines contraintes afin de modéliser différemment les dépendances multiples et les cir-
cuits. Nous avons présenté un algorithme de changement de représentation permettant
de construire ce que nous appelons des réseaux bayésiens aplatis ainsi que les axiomes
garantissant une modélisation adéquate des dépendances multiples reformulées. Cette
modélisation est basée sur un compromis entre précision et vitesse d’apprentissage qui
repose sur la prise en compte des dépendances multiples en les exprimant deux à deux
seulement.
Les résultats obtenus sont prometteurs puisqu’ils montrent que les réseaux bayésiens
aplatis ont les propriétés suivantes :
– prise en compte et modélisation des circuits, ceux-ci étant fréquents dans les
HMM considérés ;
– apprentissage plus rapide avec peu d’exemples, au prix, il est vrai, d’une perte de
précision à long terme. En robotique, ce compromis est avantageux puisque l’on
dispose souvent d’exemples peu nombreux et/ou biaisés.
Nous pouvons de plus remarquer que nous avons présenté ici des expériences compa-
rant les performances de la représentation en RBA et en RB. Cependant, le formalisme
final étant toujours celui des RB (avec des variables additionnelles et un calcul des
paramètres satisfaisant des axiomes), nous pouvons très bien envisager des représenta-
tions "hybrides" dans lesquelles certaines dépendances (pour lesquelles peu de données
sont disponibles) sont exprimées dans le cadre des RBA, tandis que d’autres sont expri-
mées de façon classique. Ainsi, cette méthode permettrait de tirer parti du compromis
expressivité/apprentissage avec peu d’exemples des RBA tout en gardant une expres-
sivité pouvant atteindre celle des RB lorsque le nombre d’exemples disponibles devient
suffisant.
Références
Ghahramani Z. and Jordan M. I. (1996), Factorial Hidden Markov Models, Machine
Learning, vol. 29. 1996, pages 245-273.
Theocharous G. and Rohanimanesh K. and Mahadevan S. (2001), Learning Hierarchi-
cal Partially Observable Markov Decision Processes for robot navigation, Procee-
dings of the IEEE Conference on Robotics and Automation (ICRA-2001). IEEE
Press.
Theocharous G. and Murphy K. and Kaelbling L. (2004), Representing hierarchical
POMDPs as DBNs for multi-scale robot localization, Proc. of the IEEE interna-
tional Conference on Robotics and Automation (ICRA’04).
- 60 -RNTI-E-5
