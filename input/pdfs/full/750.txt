Pourquoi les mode`les de me´lange
pour la classification ?
Christophe Biernacki
CNRS & Universite´ de Lille 1, Villeneuve d’Ascq, France
biernack@math.univ-lille1.fr
Re´sume´ Les mode`les de me´lange apportent une re´ponse rigoureuse, flexible et in-
terpre´table pour les multiples besoins de la classification : classification supervise´e
ou non, nature des donne´es, choix du nombre de groupes, etc. Les domaines d’ap-
plications sont de plus en plus nombreux, aide´s en cela par le de´veloppement de
solutions logicielles adapte´es.
Mots cle´s : classification supervise´e, classification non supervise´e, algorithme EM,
choix de mode`les.
Abstract Mixture models provide a mathematical-based, flexible and meaningful
approach for the wide variety of classification requirements: Unsupervised or super-
vised classification, data features, number of classes selection, etc. Fields in which
mixture models have been successfully applied are numerous and specific softwares
are now available.
Keywords: Classification, clustering, EM algorithm, model selection.
1 Introduction
La classification a pris aujourd’hui une place importante en analyse des donne´es explo-
ratoire et de´cisionnelle, tant au niveau des domaines d’applications que des de´veloppements
me´thodologiques. L’objectif exploratoire est typiquement repre´sente´ par la classification
dite non supervise´e, ou classification automatique, qui vise a` de´couvrir une partition hy-
pothe´tique dans un ensemble d’objets. Le but de´cisionnel quant a` lui se re´fe`re plutoˆt a`
la classification supervise´e, ou analyse discriminante, qui cherche ge´ne´ralement a` affecter
tout nouvel objet a` des groupes pre´alablement de´finis. Pour cela, il est donc ne´cessaire de
disposer de me´thodes s’adaptant a` la diversite´ des donne´es a` analyser comme l’abondance,
ou au contraire la rarete´, des objets (individus) disponibles ainsi que le nombre et/ou le
type de descripteurs (variables) pour chacun d’entre eux.
Graˆce a` leur flexibilite´, les me´langes finis de distributions de probabilite´ re´pondent a`
ces exigences (voir [36] et les nombreuses re´fe´rences associe´es). Ils sont devenus aujour-
d’hui un outil populaire et utilise´ avec succe`s dans un nombre croissant de disciplines
comme l’astronomie, la biologie, la ge´ne´tique, l’e´conomie, les sciences de l’inge´nieur, le
marketing, la reconnaissance d’images. . .En outre, le fait que des logiciels comme mix-
mod1 [5] mettent a` disposition ces me´thodes sous forme de code performant et portable
1Site web du logiciel mixmod : http ://www-math.univ-fcomte.fr/mixmod/index.php
c© Revue MODULAD, 2009 -1- Nume´ro 40
aide largement a` leur diffusion. D’un point de vue the´orique, les mode`les de me´lange
permettent de be´ne´ficier de l’ensemble des re´sultats de la statistique mathe´matique : lois
multivarie´es parame´triques, estimation, choix de mode`les. Ils permettent aussi de retrou-
ver, voire de ge´ne´raliser, de nombreuses me´thodes de classification classiques non probabi-
listes a` la base car plutoˆt ge´ome´triques. Ils font partie de la famille des mode`les ge´ne´ratifs
puisque l’ensemble du processus ayant ge´ne´re´ les donne´es (observe´es et manquantes) est
totalement mode´lise´. Ce cadre formel apporte clairement un confort d’utilisation pour
le praticien qui obtient sans effort supple´mentaire un re´sume´ exhaustif et ge´ne´ralement
tre`s interpre´table de sa structure de classification au travers des parame`tres du mode`le.
En outre, la rigidite´ apparente de cette mode´lisation est efficacement compense´e par la
mise en œuvre de me´thodes de choix de mode`les. Cependant, bien que cela soit possible,
il serait dangereux de re´sumer la proble´matique de la classification a` un simple sous-
produit d’un mode`le de me´lange. En effet, il peut eˆtre efficace d’adjoindre aussi bien dans
la phase d’estimation que de choix de mode`les des informations supple´mentaires en rap-
port avec l’objectif initial recherche´. Cette optique est par ailleurs source de nombreux
de´veloppements me´thodologiques spe´cifiques re´cents.
Dans la suite de l’expose´, la Section 2 est de´volue a` la pre´sentation du mode`le de
me´lange de lois de probabilite´s, a` l’estimation de ses parame`tres et aux me´thodes habi-
tuelles de choix de mode`les. La section suivante abordera la proble´matique de la clas-
sification non supervise´e avec ce type de mode`le. En particulier, on soulignera le lien
avec des me´thodes ge´ome´triques classiques dans le cas gaussien et on pre´sentera aussi
des crite`res de choix de mode`les spe´cifiques. Une illustration nume´rique finalisera cette
partie. La Section 4 reprend le meˆme sce´nario pour la classification supervise´e cette fois.
Enfin, la Section 5 dresse un bilan de cette pre´sentation ainsi que des nombreux nouveaux
de´fis qui attendent les me´thodes de classification (donne´es en haute dimension, contexte
semi-supervise´, choix de variables, . . .) et les perspectives de re´ponses dans un cadre de
mode`le de me´lange.
2 Mode`le de me´lange
2.1 Pre´sentation ge´ne´rale
2.1.1 De´finition
Une loi de me´lange fini p sur un espace X est une loi de probabilite´ s’exprimant comme
une combinaison line´aire de plusieurs lois de probabilite´ p1, . . . , pg sur X . Autrement dit,
il existe g coefficients π1, . . . , πg (πk > 0 et
∑g
k=1 πk = 1) tels que, pour tout x1 ∈ X ,
p(x1) =
g∑
k=1
πkpk(x1). (1)
Les πk et pk sont respectivement appele´es proportions et composantes du me´lange.
c© Revue MODULAD, 2009 -2- Nume´ro 40
2.1.2 Interpre´tation ge´ne´rative
La loi me´lange p peut aussi s’interpre´ter comme la loi marginale de la variable ale´atoire
X1 obtenue a` partir de la loi du couple de variables ale´atoires (X1,Z1) ou`
Z1 ∼Mg(1, π1, . . . , πg) et X1|Z1 = z1 ∼ p{k:z1k=1}, (2)
avec Mg(1, π1, . . . , πg) la loi multinomiale de dimension g et d’ordre 1 de parame`tre
(π1, . . . , πg) et zi = (zi1, . . . , zin) un vecteur binaire de dimension g. Cette interpre´tation
permet de ge´ne´rer en deux e´tapes tre`s simples un e´chantillon x = (x1, . . . ,xn) tels que les
xi soient des re´alisations i.i.d. de X1 (i = 1, . . . , n) :
1. Ge´ne´rer un e´chantillon z = (z1, . . . , zn) tel que les zi soient des re´alisations i.i.d. de
Z1 ;
2. Ge´ne´rer un e´chantillon x = (x1, . . . ,xn) tel que les xi soient des re´alisations inde´pen-
dantes de p{k:zik=1}.
Il est important de remarquer pour la suite que les vecteurs zi identifient la composante
pk a` partir de laquelle est ensuite ge´ne´re´ chaque xi. On parlera parfois d’e´tiquettes.
2.1.3 L’exemple gaussien et multinomial
Ge´ne´ralement, on suppose en outre que chaque composante pk appartient a` une famille
parame´tre´e p(.;αk) et on note p(.; θ) la loi me´lange associe´e a` ce parame´trage, θ =
(π1, . . . , πg,α1, . . . ,αg) de´signant le parame`tre de ce mode`le. Le choix se restreint a` des
familles p(.;αk) conduisant a` des lois me´langes identifiables, ou tout au moins qui le sont
dans la plupart des situations d’inte´reˆt. Il existe aussi des choix semi-parame´triques ou
non parame´triques pour les pk (voir [10]) mais nous ne les traiterons pas ici.
−6
−4
−2 0 2 4 6 8 −6
−4
−2
0
2
4
6
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
x2
x1
de
ns
ite
Fig. 1 – Un me´lange gaussien dans R2 avec trois composantes.
Dans le cas continu ou` X = Rd, le mode`le parame´trique le plus utilise´ est la loi
multinormale :
p(.;αk) = N (μk,Σk), (3)
c© Revue MODULAD, 2009 -3- Nume´ro 40
avec αk = (μk,Σk), μk ∈ Rd de´signant la moyenne de la composante k et Σk ∈ Rd×d
la matrice de variance correspondante. La figure 1 donne une illustration d’un me´lange
gaussien dans R2 avec trois composantes.
Dans le cas qualitatif, X = {1, . . . , m1} × . . .× {1, . . . , md} correspond a` d variables
cate´gorielles chacune posse´dant mj modalite´s (j = 1, . . . , d) et les donne´es x sont code´es
par le tableau disjonctif complet n× p suivant :
xijh =
{
1 si l’individu i prend la modalite´ h pour la variable j
0 sinon,
(4)
ou` h = 1, . . . , mj. Dans ce contexte, il est naturel de mode´liser la loi de la variable
qualitative X1j de la composante k par une loi multinomiale Mmj (1, αkj1, . . . , αkjmj),
ou` αkjh = p(X1jh = 1|Z1k = 1). Le mode`le des classes latentes [26] fait l’hypothe`se
supple´mentaire que les variables qualitatives X1j sont inde´pendantes conditionnellement
a` chaque composante k, d’ou` l’expression de la loi de chaque composante :
p(x1;αk) =
d∏
j=1
mj∏
h=1
(αkjh)
x1jh , (5)
avec αk = (αkjh; j = 1, . . . , d; h = 1, . . . , mj).
Notons que bien d’autres mode´lisations existent par ailleurs comme la loi de poisson,
la loi exponentielle, la loi de Student multivarie´e, etc. (Voir [36].)
2.2 Estimation des parame`tres par EM
2.2.1 Principe
En statistique, le parame`tre θ est inconnu et l’objectif est de l’estimer a` partir de
l’e´chantillon x. Pour cela, on peut maximiser la log-vraisemblance donne´e par :
L(θ;x) =
n∑
i=1
ln
(
g∑
k=1
πkp(xi;αk)
)
. (6)
On parle aussi de log-vraisemblance observe´e car calcule´e uniquement sur les donne´es
observe´es x.
Cependant, l’optimisation directe sur θ est difficile : il n’y a ge´ne´ralement pas de
solution analytique et les algorithmes ge´ne´ralistes d’optimisation de type Newton peuvent
eˆtre complique´s a` mettre en œuvre (calcul du hessien par exemple) et d’une efficacite´
toute relative dans ce cadre particulier. L’algorithme EM (Espe´rance-Maximisation) est
un algorithme spe´cifique a` la maximisation de la vraisemblance dans le cadre de donne´es
manquantes qui conduit a` de bons re´sultats en pratique [21]. Sa mise en œuvre est tre`s
simple a` partir du moment ou` la maximisation de la log-vraisemblance comple´te´e qui serait
calcule´e sur l’ensemble des donne´es (x, z) issues du processus sous-jacent de ge´ne´ration
ale´atoire de x serait facile, ce qui est le cas pour nombre de lois parame´triques usuelles
retenues pour les composantes p(.;αk). On note cette log-vraisemblance par
L(θ;x, z) =
n∑
i=1
g∑
k=1
zik ln (πkp(xi;αk)) . (7)
Ainsi, partant d’un parame`tre initial θ− arbitraire, l’algorithme ite`re sur les e´tapes
suivantes :
c© Revue MODULAD, 2009 -4- Nume´ro 40
– E´tape E (espe´rance) : calculer t+ik = E[Zik|xi; θ−] ;
– E´tape M (maximisation) : calculer θ+ = argmaxθ L(θ;x, t
+) ou` {t+}ik = t+ik.
L’algorithme s’arreˆte apre`s un nombre pre´de´fini d’ite´rations ou bien a` la stationarite´ du
crite`re de log-vraisemblance observe´e. Cette seconde option de´coule de la proprie´te´ de
croissance de L(θ;x) a` chaque ite´ration.
2.2.2 Interpre´tation
Il est informatif de remarquer que l’espe´rance tik calcule´e a` l’e´tape E peut aussi s’in-
terpre´ter comme la probabilite´ conditionnelle que l’individu xi ait e´te´ ge´ne´re´ par la com-
posante k :
tik = p(Zik = 1|Xi = xi; θ) = πkp(xi;αk)
p(xi; θ)
. (8)
Ainsi, les e´tiquettes manquantes z indiquant la composante d’origine des xi ont simple-
ment e´te´ remplace´es par des probabilite´s de provenir de chacune de ces composantes. Cela
explique aussi la simplicite´ de l’e´tape M en pratique car ces probabilite´s servent simple-
ment de poids a` chaque individu dans la phase d’estimation. Par exemple, les proportions
du me´lange obtenues a` l’e´tape M sont donne´es par les fre´quences suivantes :
π+k =
n+k
n
, (9)
avec n+k =
∑n
i=1 t
+
ik l’effectif « flou » provenant de la composante k.
L’estimation des parame`tres αk des composantes de´pend quant a` elle des lois retenues
mais ne pose ge´ne´ralement pas plus de difficulte´. Ainsi, dans le cas gaussien, on obtient
μ+k =
1
n+k
n∑
i=1
t+ikxi et Σ
+
k =
1
n+k
n∑
i=1
t+ik(xi − μ+k )(xi − μ+k )′, (10)
et dans le cas multinomial d’inde´pendance conditionnelle
α+kjh =
1
n+k
n∑
i=1
t+ikxijh. (11)
2.2.3 Proprie´te´s
On a de´ja` signale´ que chaque ite´ration de EM fait croˆıtre la log-vraisemblance. Il
converge en outre vers un point stationnaire de la vraisemblance qui peut eˆtre typique-
ment le maximum global, un maximum local ou un point selle. Pour une recherche plus
efficace du maximum global, il est donc pre´fe´rable de faire plusieurs essais en changeant
les parame`tres initiaux, puis de ne retenir que la solution donnant la plus grande vraisem-
blance. C’est aussi un algorithme qui converge line´airement, donc il peut souffrir d’une
certaine lenteur au voisinage de la stationnarite´.
On retrouvera nombre de proprie´te´s plus de´taille´es sur EM dans [35] ou encore des
propositions de strate´gies pour bien initialiser l’algorithme [7]. Enfin, on pourra utilement
se reporter a` l’algorithme SEM [14], variante stochastique de EM, pour obtenir sans
estimation de hessien une e´valuation de la variabilite´ des estimateurs.
c© Revue MODULAD, 2009 -5- Nume´ro 40
2.3 Choix de mode`les
2.3.1 Proble´matique
Il est cependant fre´quent que la structure de mode´lisation d’ou` sont issues les donne´es
x soit elle-meˆme incertaine. Par exemple, on ignore le nombre g de composantes du mode`le
et ce nombre doit donc eˆtre lui aussi estime´. De fac¸on ge´ne´rique, un mode`le m repre´sente
un ensemble de contraintes sur l’espace du parame`tre θ.
Il faut garder a` l’esprit par ailleurs que le mode`le de me´lange a pu eˆtre simplement
utilise´ pour estimer une loi de probabilite´ inconnue graˆce a` sa grande flexibilite´ et donc
qu’il n’y a pas de « vrai »mode`le parmi ceux propose´s. Cela oblige alors a` de´finir autrement
la notion meˆme de « bon » mode`le. C’est sur ce point que les approches divergent en
ge´ne´ral.
2.3.2 Deux crite`res de re´fe´rence
Pour choisir parmi plusieurs mode`les en compe´tition, on peut utiliser des crite`res
tre`s classiques en statistique mathe´matique qui s’expriment ge´ne´ralement comme une
pe´nalisation de la log-vraisemblance maximale de la forme :
*ICm = L(θˆm;x)− νm.f(n), (12)
ou` θˆm correspond a` l’estimateur du maximum de la vraisemblance sous le mode`le m, νm
donne le nombre de parame`tres a` estimer et f est une fonction de n. On retient alors
le mode`le ayant obtenu la plus grande valeur du crite`re. Fondamentalement, ce type de
crite`re tente de re´aliser un compromis entre l’ade´quation du mode`le aux donne´es mesure´
par la log-vraisemblance maximale et la complexite´ de celui-ci, mesure´ par sa dimension.
Le point de vue adopte´ pour ajuster ce compromis permettra de de´finir la fonction f :
– Du point de vue fre´quentiste, un « bon »mode`le est celui qui re´alise un compromis en
terme de « biais-variance ». Dans ce cadre, le crite`re AIC (An Information Criterion)
[1] est propose´ avec f(n) = 1. La pe´nalite´ ne de´pend donc pas de n.
– Du point de vue baye´sien, un « bon »mode`le est celui qui maximise la vraisemblance
inte´gre´e (sur les parame`tres) lorsque chaque mode`le en compe´tition est e´quiprobable
a priori. Le crite`re BIC (Bayesian Information Criterion) [39] propose alors la
pe´nalite´ f(n) = 0.5 ln(n), de´pendant cette fois de n.
2.3.3 Proprie´te´s
Tous deux sont des crite`res dits asymptotiques en le sens que les proprie´te´s ide´ales
dont ils sont issues sont atteintes seulement asymptotiquement. Ainsi, asymptotiquement,
AIC retiendra le mode`le minimisant l’e´cart de Kullback moyen avec la vraie loi inconnue
tandis que BIC se´lectionnera le mode`le minimisant l’e´cart de Kullback avec la vraie loi.
En ce sens, BIC est convergeant si le vrai mode`le est dans la liste [31].
Il faut noter que les arguments de construction et de proprie´te´s de AIC et BIC sont
affaiblis dans le cas du choix de g pour les me´langes (voir une discussion de´taille´e a` ce sujet
dans [36], Chap. 6). En pratique, les deux crite`res donnent cependant de bons re´sultats,
avec une pre´fe´rence assez marque´s pour BIC par les utilisateurs, AIC favorisant souvent
des mode`les trop complexes.
c© Revue MODULAD, 2009 -6- Nume´ro 40
3 Classification non supervise´e
3.1 Proble´matique
3.1.1 Donne´es et objectif
L’objectif de la classification non supervise´e, appele´e aussi classification automatique,
est de partitionner l’ensemble des n objets de x en g classes G1, . . . , Gg. D’autres structures
que la partition peuvent aussi eˆtre recherche´es, par exemple les hie´rarchies qui sont des
emboˆıtements de partitions, mais ces situations ne seront pas conside´re´es ici. On peut
e´galement de´signer la partition estime´e sous forme d’indicatrices avec le tableau binaire
zˆ et on a bien suˆr l’e´quivalence suivante entre les notations :
xi ∈ Gk ⇔ zˆik = 1. (13)
L’objectif de la classification non supervise´e est illustre´ sur la figure 2 dans le cas bivarie´
pour trois classes recherche´es.
−6 −4 −2 0 2 4 6
−5
−4
−3
−2
−1
0
1
2
3
4
5
−6 −4 −2 0 2 4 6 8
−6
−4
−2
0
2
4
6
 
x (x, zˆ)
Fig. 2 – Classification non supervise´e : illustration de l’objectif pour des donne´es x de
R
2 et une partition estime´e zˆ en trois classes.
−→
Cet objectif n’est pas seulement formel. Il faut comprendre que l’inte´reˆt sous-jacent de
la de´marche de classification est d’aider le praticien a` analyser des donne´es. Le regroupe-
ment en classes est pour lui une fac¸on de synthe´tiser pour isoler l’information pertinente.
En effet, il est difficile pour un humain d’appre´hender directement cette information en
pre´sence de donne´es parfois nombreuses, e´ventuellement de´crites par de multiples dimen-
sions dans des espaces eux-meˆmes un peu complexes. La classification automatique est
donc ge´ne´ralement une proce´dure mathe´matique qui propose une ou plusieurs partitions
parmi la multitudes de possibilite´s offertes a priori.
Au-dela` de cet objectif de base, toute aide supple´mentaire permettant de faciliter
l’interpre´tation des partitions elles-meˆmes est la bienvenue pour le praticien. Il n’est pas
difficile a` imaginer en effet qu’il calculera des statistiques sur la partition obtenue pour
mieux la comprendre. Par exemple, une simple moyenne par classe fera apparaˆıtre la
classe « des grands » en opposition a` la classe « des petits ». Ainsi, outre le fait d’obtenir
c© Revue MODULAD, 2009 -7- Nume´ro 40
une partition, une mode´lisation plus abstraite de la notion de classe aidera certainement
pour l’interpre´tation ulte´rieure. Cela fait partie des avantages qu’apportent les mode`les
de me´lange comme nous allons le voir maintenant.
3.1.2 Mode´lisation par les me´langes
Dans sa pre´sentation ge´ne´rale, la classification non supervise´e est typiquement un
proble`me mal pose´ car sa mise en œuvre repose implicitement sur l’e´valuation de la
« qualite´ » d’une partition zˆ et pourtant aucun crite`re objectif n’est spe´cifie´ au de´part.
La re´solution du proble`me de classification passe donc par la de´finition d’un tel crite`re.
Une de´finition pre´cise en sera donne´e dans la section 3.2 mais nous nous concentrons pour
le moment sur la mode´lisation des classes.
Remarquons tout d’abord que, dans le processus d’acquisition des donne´es, les objets
e´tudie´s sont ge´ne´ralement tire´s ale´atoirement dans une certaine population. Ainsi, il est
naturel d’interpre´ter ces objets comme la re´alisation d’une expe´rience probabiliste. Dans
ce cadre, les mode`les de me´lange finis apportent alors une hypothe`se simplificatrice sur
la distribution de la population en faisant ressortir explicitement une structure en sous-
populations, hypothe`se proche de l’objectif de la classification. En effet, une de´finition
pre´cise est donne´e de cette fac¸on a` la notion de classe puisque des individus appartiennent
au meˆme ensemble si et seulement si ils sont tire´s dans la meˆme sous-population [9].
Le proble`me de classification initial se rame`ne donc a` estimer la partition z qui a
permis la re´alisation de l’e´chantillon x de fac¸on i.i.d. suivant une loi me´lange p. Ce pose
alors maintenant le choix des composantes pk associe´es.
Ce choix de´pend d’une part de la nature des donne´es (continues, cate´gorielles, ou
autres). D’autre part, c’est aussi l’occasion de fournir au praticien une mode´lisation in-
terpre´table et re´duite de la classe. Le cadre parame´trique atteint naturellement ces deux
objectifs. En effet, pour les donne´es continues, le choix d’une loi multinormale est parti-
culie`rement informatif pour l’utilisateur : la moyenne μk de´finit la position centrale de la
classe k et la matrice de variance Σk informe sur sa dispersion autour de ce centre. On
peut ainsi facilement en de´duire une zone (ellipso¨ıde) de confiance autour du centre. Donc,
en peu de parame`tres, la classe est re´sume´e. Pour les donne´es cate´gorielles, le mode`le des
classes latentes de´crit explicitement les classes par la probabilite´ αkjh de chacune des mo-
dalite´s, variable par variable. Dans tous les cas, les proportions πk apportent de plus une
information directement interpre´table : l’effectif relatif de chaque classe.
3.1.3 Ge´ome´trie d’une classe : mode`les parcimonieux gaussiens
On vient de voir l’inte´reˆt de la mode´lisation parame´trique d’une classe pour aider
a` son interpre´tation. Dans le cas gaussien, des caracte´ristiques ge´ome´triques fines de la
classe ellipso¨ıdale de centre μk peuvent eˆtre en outre controˆle´es graˆce a` une de´composition
spectrale de la matrice de variance Σk.
Suivant [3, 18], chaque matrice de variance des composantes du me´lange peut s’e´crire :
Σk = λkDkAkD
′
k (14)
avec λk = |Σk|1/d, Dk e´tant la matrice des vecteurs propres de Σk et Ak e´tant une
matrice diagonale, telle que |Ak| = 1, dont la diagonale est constitue´e des valeurs propres
normalise´es de Σk range´es en ordre de´croissant. Le parame`tre λk caracte´rise le volume
c© Revue MODULAD, 2009 -8- Nume´ro 40
de la ke classe, Dk son orientation et Ak sa forme. Une illustration dans le plan de
la ge´ome´trie d’une classe est donne´e sur la figure 3. En permettant a` ces parame`tres
de varier ou non entre les classes, on obtient des mode`les plus ou moins parcimonieux.
En tout, [18] proposent quatorze mode`les diffe´rents en permettant a` tout ou partie des
termes ge´ome´triques de varier ou non entre classes et en les associant aussi a` des mode`les
tre`s simples comme des matrices de variances diagonales (les Dk sont des matrices de
permutation) ou encore sphe´riques (Ak = I).
αkλk
ak
λkak
μk x
x
1
2
Fig. 3 – Illustration ge´ome´trique dans le plan du parame´trage de l’ellipse isodensite´ de la
classe k par la de´composition spectrale de Σk avec Ak une matrice diagonale de termes
diagonaux a et 1/a, et Dk une matrice de rotation d’angle α.
Ainsi, chaque ge´ome´trie particulie`re, repre´sente´e par chaque mode`le, permet au pra-
ticien de caracte´riser simplement non seulement les classes mais aussi leurs similitudes
e´ventuelles, et ainsi d’aider a` leur interpre´tation.
3.1.4 Centre et dispersion dans le cas multinomial
Les notions de centre et de dispersion d’une classe sont tre`s intuitives pour appre´hender
rapidement les caracte´ristiques d’une classe et les diffe´rences entre plusieurs d’entre elles.
Par analogie au cas gaussien, ces notions ont e´te´ introduites dans le cas Bernoulli pour
donne´es binaires [15] puis e´tendues au cas multinomial pour donne´es qualitatives ge´ne´rales
([27], Chap. 9).
Le principe est de contraindre tous les vecteurs (αkj1, . . . , αkjmj) a` prendre la forme(
εkj
mj − 1 ,
εkj
mj − 1 , ..,
εkj
mj − 1 , 1− εkj,
εkj
mj − 1 , ...,
εkj
m− 1
)
(15)
avec εkj <
mj−1
mj
. Les vecteurs des probabilite´s sont alors simplement caracte´rise´s par une
modalite´ majoritaire (le mode ou le centre) et un terme de dispersion εjk qui repre´sente la
probabilite´ d’en eˆtre diffe´rent.
Comme pour le mode`le de me´lange gaussien, il est possible d’imposer des contraintes
supple´mentaires sur la dispersion : elle peut eˆtre inde´pendante de la variable, de la classe,
voire d’aucun des deux (voir de nouveau [27], Chap. 9).
c© Revue MODULAD, 2009 -9- Nume´ro 40
3.2 Estimation d’une partition
3.2.1 Approche « me´lange »
Ayant immerge´ la proble´matique de classification dans un mode`le de me´lange, il s’agit
maintenant d’en tirer partie pour estimer z. Dans un premier temps, l’algorithme EM per-
met d’obtenir une estimation θˆ du parame`tre de me´lange sous-jacent. Notons que l’e´tape
M sera spe´cifique au mode`le conside´re´, en particulier pour les mode`les parcimonieux
gaussiens et multinomiaux pre´ce´dents tout en gardant en ge´ne´ral une grande simplicite´
de calcul (voir [18] et [27], Chap.9). On en de´duit ensuite une estimation tˆ des probabilite´s
conditionnelles d’appartenance aux composantes du me´lange, donc aux classes. Rappelons
que {tˆ}ik = tˆik avec tˆik = p(Zik = 1|Xi = xi; θˆ). Une partition zˆ est alors obtenue par le
principe du MAP (Maximum A Posteriori), c’est-a`-dire en affectant chaque individu xi a`
la classe de plus grande probabilite´ conditionnelle estime´e :
zˆ = MAP(tˆ) ⇔ zˆik =
{
1 si k = argmaxk′=1,...,g tˆik′
0 sinon.
(16)
Remarquons, qu’au dela` d’une estimation zˆ de la partition, le mode`le fournit aussi
deux informations supple´mentaires inte´ressantes pour le praticien :
– une estimation θˆ des parame`tres dont nous savons maintenant toute l’importance
pour aider a` l’interpre´tation des classes ;
– une estimation tˆ des probabilite´s conditionnelles d’appartenance aux classes, ce qui
permet d’e´valuer le risque de classement de chaque individu.
3.2.2 Approche « classification »
L’approche me´lange pre´ce´dente conside`re le proble`me de classification uniquement
comme un sous-produit du mode`le de me´lange. Cependant, il n’est pas pris en conside´ration
explicitement l’objectif final de la classification qui est de faciliter l’analyse des donne´es
(voir sous-section 3.1.1). Par exemple, l’approche me´lange peut tout a` fait conduire a` esti-
mer des classes tre`s imbrique´es, c’est-a`-dire dont les parame`tres sont tre`s similaires. Dans
ce cas, le praticien aura du mal a` interpre´ter ce qui caracte´rise des classes si imbrique´es
et aura le penchant naturel de les regrouper. Cela signifie que la partition qui lui a e´te´
fournie n’est pas comple`tement pertinente de son point de vue.
L’approche classification introduite par [40] peut eˆtre vue comme une me´thode permet-
tant d’obtenir explicitement des classes peu imbrique´es comme nous le de´taillons mainte-
nant. Pour toute partition z, on obtient la de´composition suivante de la log-vraisemblance
en une log-vraisemblance comple´te´e et un terme entropique [29] :
L(θ;x) = L(θ;x, z) + E(t, z) (17)
avec E(t, z) = −∑ni=1∑gk=1 zik ln tik un terme d’entropie de partition mesurant l’e´cart
entre la partition z et les probabilite´s d’appartenance t. Ide´alement, obtenir des classes
totalement non imbrique´es revient a` obtenir des probabilite´s d’appartenance t telles que
E(t, z) = 0 pour une partition z particulie`re. Sous cette contrainte, la relation (17)
indique qu’il est e´quivalent de maximiser la log-vraisemblance L(θ;x) en θ ou la log-
vraisemblance comple´te´e L(θ;x, z) sur le couple (θ, z). Passant outre la contrainte sur
θ impose´e par E(t, z) = 0, l’approche classification se focalise sur la maximisation de la
log-vraisemblance comple´te´e L(θ;x, z) pour le couple (θ, z).
c© Revue MODULAD, 2009 -10- Nume´ro 40
En pratique, L(θ;x, z) est optimise´e par l’agorithme CEM [16], sorte de version clas-
sification de l’algorithme EM. Partant d’un parame`tre initial θ− arbitraire, l’algorithme
CEM ite`re sur les e´tapes suivantes :
– E´tape E (espe´rance) : calculer t+ik comme EM ;
– E´tape C (classification) : calculer la partition z+ = MAP(t+) ;
– E´tape M (maximisation) : calculer θ+ = argmaxθ L(θ;x, z
+).
L’e´tape M est tre`s similaire a` celle de EM puisqu’il s’agit en pratique de remplacer les t+
par les z+ dans les formules permettant d’obtenir θ+. De plus, l’algorithme est stationnaire
et fait croˆıtre a` chaque ite´ration la vraisemblance comple´te´e. On peut donc l’arreˆter a` la
stabilite´ de la partition. En pratique, sa convergence est beaucoup plus rapide que celle de
EM ce qui peut eˆtre utile dans des situations ou` le temps d’estimation a une importance.
On ne s’e´tonnera pas que les parame`tres θ˜ estime´s par CEM soient biaise´s, meˆme
asymptotiquement, ce biais e´tant d’autant plus fort que les composantes sous-jacentes
du me´lange sont imbrique´es [13]. Cependant, l’approche classification peut donner de
meilleurs re´sultats sur l’estimation du parame`tre et aussi sur l’estimation de la partition
dans le cas de petits e´chantillons. En effet, dans ce cas, rechercher explicitement une
partition semble apporter une information utile pour re´aliser le ne´cessaire compromis
biais-variance [17].
Notons enfin, que cette approche ne prive pas l’utilisateur des sous-produits habituels
d’une mode´lisation par me´lange :
– des parame`tres θ˜ pour aider a` l’interpre´tation des classes ;
– des probabilite´s d’appartenance aux classes t˜ pour e´valuer le risque de classement ;
– une partition z˜ = MAP(t˜).
3.2.3 Lien avec des approches a` base de distances
Il est possible de faire des liens entre l’approche classification pre´ce´dente et certaines
me´hodes de classification standards propose´es ante´rieurement dans un cadre non probabi-
liste et s’appuyant plutoˆt sur des me´triques. Ce type de rapprochement permet de re´ve´ler
des hypothe`ses qui a l’origine n’e´taient pas explicites. Il est alors envisageable d’e´tendre
ces me´thodes ge´ome´triques on utilisant toute la varie´te´ des mode`les parcimonieux.
Ainsi, dans le cadre des donne´es continues, la me´thode des centres mobiles [43] se´lection-
ne la partition z maximisant le crite`re d’inertie intraclasse trace (W(z)) ou`W(z) de´signe
la matrice d’inertie intraclasse donne´e par
W(z) =
n∑
i=1
g∑
k=1
zik(xi − x¯k)(xi − x¯k)′ (18)
avec x¯k la moyenne arithme´trique des individus de la classe Gk. Il s’agit pre´cise´ment de
la partition retenue par maximum de vraisemblance comple´te´e avec proportions e´gales et
un mode`le gaussien sphe´rique de meˆme volume entre composantes. De plus, l’algorithme
CEM est exactement l’algorithme des centres mobiles. De la meˆme fac¸on, le crite`re |W(z)|
[24] s’identifie au mode`le a` proportions e´gales et avec mode`le gaussien homosce´dastique
(Σ1 = . . . = Σg). D’autres correspondances sont encore possibles (voir [27], Chap. 9).
Dans le cadre des donne´es cate´gorielles, [15] ont montre´ que l’algorithme CEM avec
mode`le des classes latentes maximise la` aussi un crite`re d’information classique, proche
d’une me´trique du χ2.
c© Revue MODULAD, 2009 -11- Nume´ro 40
3.3 Choix de mode`les
On dispose maintenant d’un ensemble de mode`les a` disposition graˆce a` des contraintes
pleines de sens sur les parame`tres du me´lange. Cependant, il n’est pas rare que la structure
la plus ade´quate soit elle-meˆme ignore´e du praticien. De la meˆme fac¸on, le nombre de
classes peut eˆtre inconnu, seule une borne supe´rieure gsup e´tant ge´ne´ralement disponible.
On est alors face a` une proble´matique de choix d’un mode`le m, regroupant la structure
des classes et/ou le nombre des classes.
3.3.1 Crite`res « me´lange »
Les crite`res informationnels classiques comme BIC ou AIC sont disponibles pour aider
a` se´lectionner m. Cependant, ces crite`res souffrent du meˆme de´faut que celui e´voque´ en
phase d’estimation a` savoir la perte de l’objectif de classification. En effet, le crite`re BIC
e´tant convergeant (au moins en pratique), il de´tectera toutes les composantes du me´lange,
meˆme si certaines sont tre`s imbrique´es, et ce de`s que la taille de l’e´chantillon sera suffi-
samment grande. La partition qui sera de´duite du mode`le retenu sera donc insatisfaisante
pour le praticien car il y a moins de classes interpre´tables dans les donne´es que de classes
pre´sentes dans le me´lange.
Plus ennuyeux encore, l’hypothe`se de me´lange n’est probablement pas vraiment ve´rifie´e
dans la population sous-jacente aux donne´es. Dans ce cas, on peut s’attendre a` ce que
les crite`res classiques se´lectionnent un nombre tre`s important de composantes pour le
me´lange afin de re´aliser une bonne estimation de la loi inconnue de la population (voir
des re´sultats sur l’estimation consistante de densite´ par BIC dans [38]).
On pourrait avancer que l’estimation par approche classification, s’il elle a e´te´ utilise´e, a
de´ja` produite des classes plutoˆt se´pare´es. Cependant, le parame`tre estime´ par maximum de
vraisemblance comple´te´e (θ˜) n’est pas utilisable en toute rigueur a` la place du parame`tre
estime´ par maximum de vraisemblance (θˆ) dans les expressions de BIC et AIC.
3.3.2 Crite`re « classification »
L’ide´e est de reporter l’objectif de classification de la me´thode d’estimation vers la
me´thode de choix de mode`le. En d’autres termes c’est a` la me´thode de choix de mode`le
de retenir un mode`le produisant des classes bien se´pare´es tout en respectant « au mieux »
la distribution des donne´es.
Dans cette perspective, [6] ont propose´ le crite`re ICL (Integrated Complete Likelihood)
qui pe´nalise la log-vraisemblance comple´te´e calcule´e en θˆ (et non pas calcule´e en θ˜) par
le meˆme terme de complexite´ que le crite`re BIC. Ce crite`re, a` maximiser, s’e´crit sous les
trois formes e´quivalentes suivantes :
ICLm = L(θˆm;x, zˆm)− 0.5νm ln(n) (19)
= L(θˆm;x)− E(tˆm, zˆm)− 0.5νm ln(n) (20)
= BICm −E(tˆm, zˆm). (21)
La seconde ligne utilise la relation (17) entre log-vraisemblance et log-vraisemblance
comple´te´e et permet ainsi d’interpre´ter ICL comme une pe´nalisation du maximum de
log-vraisemblance par un terme de complexite´ du mode`le et un terme d’imbrication des
classes. La dernie`re ligne interpre`te ICL comme un crite`re BIC pe´nalise´ par ce terme
c© Revue MODULAD, 2009 -12- Nume´ro 40
d’imbrication. D’une part, on remarque donc que ICL est tout aussi simple a` calculer que
BIC. D’autre part, on de´duit que ICL pe´nalisera les mode`les produisant des classes trop
imbrique´es, ce que ne faisait pas BIC.
3.3.3 Exemples illustratifs
Il s’agit maintenant d’illustrer experimentalement les principales diffe´rences de com-
portement entre BIC et ICL. Plus ge´ne´ralement, c’est aussi l’occasion de montrer des
exemples de mise en situation des me´langes gaussiens.
Δμ 2.9 3.0 3.1 3.2 3.3
n BIC ICL BIC ICL BIC ICL BIC ICL BIC ICL
100 94 23 96 31 97 44 95 45 97 60
400 100 9 100 21 100 48 100 70 100 85
700 100 8 100 15 100 39 100 72 100 96
1000 100 6 100 16 100 56 100 75 100 91
Tab. 1 – Nombre de se´lections (parmi 100 essais) de deux composantes, au lieu d’une
seule, dans un me´lange gaussien univarie´ bimodal ou` π1 = π2 = 0.5, |μ2 − μ1| = Δμ et
Σ1 = Σ2 = 1.
Dans un premier temps, les expe´riences sugge`rent que ICL, au contraire de BIC, n’est
pas ne´cessairement convergeant, la pe´nalite´ entropique d’ICL empeˆchant la se´lection de
classes trop imbrique´es. En effet, ICL convergera vers le bon nombre de classes si ces
dernie`res sont « suffisamment » se´pare´es tandis qu’il sous-estimera ce nombre dans le
cas contraire, meˆme asymptotiquement. Ce comportement est illustre´ par des donne´es
simule´es dans la table 1.
2 2.5 3 3.5 4 4.5 5
45
50
55
60
65
70
75
80
85
90
95
eruption
a
tte
nt
e
2 2.5 3 3.5 4 4.5 5
45
50
55
60
65
70
75
80
85
90
95
eruption
a
tte
nt
e
BIC ICL
Fig. 4 – Ellipses iso-densite´ des classes retenues par BIC et ICL sur les donne´es du geyser.
Cependant, la pe´nalite´ entropique permet de robustifier le choix de mode`le en faisant
un compromis entre l’ade´quation mode`les/donne´es et la se´paration des classes. Ceci est
illustre´ dans le cas du jeu de donne´es re´elles constitue´ de 272 eruptions de the Old Faithful
c© Revue MODULAD, 2009 -13- Nume´ro 40
geyser du Yellowstone National Park (la version utilise´e par [42]), chaque observation e´tant
de´crite par deux mesures : la dure´e de l’e´ruption et le temps d’attente entre deux e´ruptions
(tous deux en minutes). En mettant en compe´tition les 28 mode`les gaussiens combine´s a`
un choix du nombre de classes entre 1 et gsup = 6, BIC retient trois classes avec un mode`le
homosce´dastique a` proportions e´gales tandis que ICL se´lectionne un mode`le a` deux classes
a` proportions libres et orientations e´gales. On remarque sur la figure 4 que la solution
donne´e par ICL fait alors ressortir la pre´sence de deux groupes bien se´pare´s tandis que la
solution retenue par BIC illustre l’e´cart a` la normalite´ de ses deux groupes plutoˆt que la
pre´sence d’une 3e classe d’inte´reˆt pour la classification.
4 Classification supervise´e
4.1 Proble´matique
4.1.1 Donne´es et objectif
En classification supervise´e, appele´e aussi analyse discriminante, le couple (x, z) des
n objets et de leur e´tiquette respective est connu (voir par exemple [34] ou [27], Chap.
7). L’objectif est d’estimer le groupe zn+1 de tout nouvel individu xn+1 de X arrivant
ulte´rieurement a` (x, z) et dont le groupe de provenance serait inconnu. En d’autres termes,
il s’agit donc d’estimer une re`gle de classement r de´finie par
r : X −→ {1, . . . , g}
xn+1 −→ r(xn+1). (22)
L’estimation s’appuiera sur l’ensemble des donne´es disponibles (x, z), appele´ souvent pour
l’occasion ensemble d’apprentissage. L’objectif de la classification supervise´e est illustre´
sur la figure 5 avec X = R2 et trois groupes.
−6 −4 −2 0 2 4 6 8
−6
−4
−2
0
2
4
6
?
−4 −2 0 2 4 6
−5
−4
−3
−2
−1
0
1
2
3
4
5
1
2
3
(x, z) et xn+1 rˆ et zˆn+1
Fig. 5 – Classification supervise´e : illustration de l’objectif pour des donne´es (x, z) de R2
en trois classes. Le nouvel individu a` classer xn+1 est repre´sente´ par le symbole « • ».
−→
Au dela` ce cet objectif de´cisionnel, on identifie parfois un objectif comple´mentaire,
c© Revue MODULAD, 2009 -14- Nume´ro 40
voire alternatif, que l’on qualifie de descriptif. Dans ce cas, il s’agit plutoˆt de donner une
description des groupes ou de la frontie`re qui les se´pare.
4.1.2 Mode´lisation probabiliste ge´ne´rative
Le mode`le probabiliste classique et naturel pour re´soudre le proble`me de la classifi-
cation supervise´e s’appuie sur les proportions πk des groupes et les lois conditionnelles
pk. Dans ce cadre, on retient la re`gle de classement r qui minimise l’erreur moyenne de
classement donne´e par
e(r) = 1− E(X1,Z1)[Z1r(X1)]. (23)
La re`gle de classement optimale r∗, dite re`gle de Bayes, est celle qui minimise e(r). Elle
correspond tout simplement a` la re`gle du MAP de´ja` pre´sente´e en (16) pour z et que nous
re´exprimons ici pour r :
∀xn+1 ∈ X r(xn+1) = arg max
k∈{1,...,g}
p(Zn+1k = 1|Xn+1 = xn+1). (24)
Bien entendu, le praticien peut prendre des liberte´s par rapport a` la re`gle automatique
du MAP en refusant de classer un individu xn+1 trop proche de la frontie`re de de´cision.
On parle parfois dans ce cas de rejet d’ambigu¨ıte´ [22]. Il est possible de prendre e´galement
en conside´ration des couˆts de mauvais classement, ce que nous ne ferons pas ici.
Dans le mode`le probabiliste, la de´finition de la re`gle optimale de classement passe
donc par la connaissance des probabilite´s conditionnelles d’appartenance aux groupes. Il
est ge´ne´ralement efficace pour la pre´diction et riche pour l’interpre´tation d’imposer une
forme parame´trique a` ces probabilite´s conditionnelles. On distingue alors deux approches
pour la de´finir :
– Approche pre´dictive : les probabilite´s conditionnelles de groupes sont directement
parame´tre´es sans de´finition pre´cise de la loi pk des groupes, c’est pourquoi on parle
ge´ne´ralement de mode`le semi-parame´trique. Le mode`le de re´gression logistique [2]
en fait partie.
– Approche ge´ne´rative : on parame´trise pre´cise´ment cette fois la loi des groupes (pk =
p(.;αk)) et on de´duit la forme parame´trique des probabilite´s conditionnelles d’ap-
partenance par la formule (8) combinant les p(·;αk) et les πk. La discrimination
gaussienne (voir [41] par exemple) et le mode`le d’inde´pendance conditionnelle [25],
que nous allons de´crire tous deux dans la section suivante, en font partie.
Les deux approches permettent une caracte´risation fine de la re`gle de classement. L’ap-
proche ge´ne´rative fournit en outre plusieurs avantages notables pour le praticien :
– Elle fournit une description pre´cise et concise des groupes eux-meˆmes graˆce a` la
parame´trisation des pk, facilitant ainsi leur interpre´tation.
– Elle permet d’identifier des individus xn+1 atypiques car tre`s en dehors des zones de
confiance associe´es aux groupes. L’individu peut par exemple provenir d’un groupe
non mode´lise´. La re`gle du MAP aurait ge´ne´ralement classe´ sans he´sitation cet in-
dividu dans une des classes existantes tandis que cette information supple´mentaire
incitera le praticien a` plus de prudence. On parle alors de rejet de distance [22] si la
de´cision est de ne pas classer l’individu en fin de compte.
– Enfin, elle permet de prendre en conside´ration de fac¸on naturelle pour estimer r
l’information apporte´e par des individus x dont certaines appartenances z ne seraient
pas connues. On parle de classification semi-supervise´e, ce qui sort un peu de notre
c© Revue MODULAD, 2009 -15- Nume´ro 40
contexte actuel mais me´rite d’eˆtre signale´ car cette proble´matique de classement se
rencontre de plus en plus fre´quemment [20].
Nous nous focalisons maintenant sur l’approche ge´ne´rative.
4.1.3 Ge´ome´trie des groupes et de la frontie`re de classement
Dans le cadre ge´ne´ratif, la discrimination gaussienne et le mode`le d’inde´pendance
conditionnelle sont certainement les plus utilise´s. La discrimination gaussienne consiste
simplement a` mode´liser la loi d’un groupe par la loi multinormale (3) et s’applique
donc aux donne´es continues. La discrimination avec mode`le d’inde´pendance condition-
nelle mode´lise la loi d’un groupe par (5) et concerne donc les donne´es cate´gorielles. Dans
les deux cas, on peut utiliser les proprie´te´s ge´ome´triques de´crites en 3.1.3 et 3.1.4 pour
aider a` l’interpre´tation des groupes et il est donc inutile d’en parler plus en de´tail ici.
On de´duit aussi de ces mode´lisations des expressions parame´triques explicites et in-
terpre´tables des re`gles de classement car la re`gle de classement est devenue elle-meˆme une
fonction de θ : r = r(.; θ). Par exemple, dans le cas gaussien homosce´dastique a` deux
groupes (Σ = Σ1 = Σ2), on a, pour tout xn+1 ∈ X :
r(xn+1; θ) = 1 si xn+1Σ
−1(μ1 − μ2)− 1
2
(μ1 + μ2)
′Σ−1(μ1 − μ2) + ln π1
π2
> 0. (25)
La forme ge´ome´trique de la frontie`re de classement s’en de´duit alors directement. Comme
illustre´ sur la figure 6, un mode`le homosce´dastique gaussien correspond a` une frontie`re
line´aire entre deux groupes (un hyperplan de Rd) tandis qu’un mode`le a` volume libre
donne une frontie`re quadratique entre deux groupes (une quadrique de Rd).
classe 1
classe 2
−6 −4 −2 0 2 4 6
−5
−4
−3
−2
−1
0
1
2
3
4
5
x1
x2
Fig. 6 – Illustration ge´ome´trique dans le plan de la forme de la frontie`re de classement
dans le cas gaussien avec deux groupes : la frontie`re line´aire correspond au cas sphe´rique
a` volume e´gal et la frontie`re quadratique au cas sphe´rique a` volume libre.
c© Revue MODULAD, 2009 -16- Nume´ro 40
4.2 Estimation d’une re`gle de classement
4.2.1 Estimation par maximum de vraisemblance
Une des me´thodes les plus simples pour estimer r(.; θ) est celle dite du plug-in. Il s’agit
tout d’abord d’estimer θ en maximisant la log-vraisemblance observe´e
L(θ;x, z) =
n∑
i=1
g∑
k=1
zik ln (πkp(xi;αk)) , (26)
puis d’injecter l’estimateur θˆ obtenu dans la re`gle de classement :
rˆ = r(.; θˆ). (27)
La maximisation de la vraisemblance est imme´diate car elle peut s’interpre´ter simplement
comme une unique e´tape M de EM ou` les poids t+ ont e´te´ remplace´s par les vraies
appartenances z (voir 2.2.2). Cependant, il faut veiller au processus d’e´chantillonnage
sous-jacent pour estimer convenablement les proportions πk : si les donne´es x proviennent
bien de la loi me´lange p(.; θ), les proportions sont estime´es comme indique´es auparavant
tandis que dans le cas d’un tirage cas-te´moin (en me´decine typiquement), les proportions
re´elles de groupes doivent eˆtre connues par ailleurs (voir [19] par exemple). En outre,
dans le cas gaussien, on peut aussi parfois remplacer les estimateurs du maximum de
vraisemblance des matrices de variance par leurs expressions corrige´es permettant de
supprimer le biais d’estimation.
Au dela` de la me´thode du MAP, la connaissance de θˆ permet bien entendu de mettre
en œuvre e´galement des strate´gies de rejet en ambigu¨ıte´ ou en distance.
4.2.2 Lien avec d’autres me´thodes
Tout comme en classification non supervise´e, il est possible d’e´tablir des connexions
entre l’approche ge´ne´rative et certaines me´thodes ge´ome´triques a` base de distance. Ainsi,
le cas homosce´dastique avec deux groupes correspond a` la discrimination line´aire de Fisher
[23] s’appuyant sur la distance de Mahalanobis dans un espace euclidien.
Notons aussi que la discrimination gaussienne et la discrimination multinomiale avec
inde´pendance conditionnelle sont aussi lie´es a` la re´gression logistique line´aire (voir [34]
par exemple).
4.3 Choix de mode`les et de variables
On s’inte´resse maintenant au choix d’un mode`lem. Il s’agit typiquement de se´lectionner
l’un des mode`les gaussiens ou multinomiaux parcimonieux. Cependant, on peut e´tendre le
choix de mode`le a` la mise en compe´tition d’un mode`le ge´ne´ratif a` un mode`le non ge´ne´ratif,
par exemple le mode`le logistique. Enfin, il est aussi crucial en classification supervise´e de
s’inte´resser au choix des variables les plus discriminantes.
4.3.1 Crite`res me´lange
Dans le cas ge´ne´ratif parame´trique, on peut envisager des crite`res de vraisemblance
pe´nalise´e comme le crite`re BIC pour choisir m. Cependant, comme c’e´tait le cas en clas-
sification non-supervise´e, BIC se focalise sur la forme de la distribution des donne´es sans
c© Revue MODULAD, 2009 -17- Nume´ro 40
preˆter attention a` l’objectif qui est de se´lectionner ici une re`gle de classement d’erreur
faible.
En ce qui concerne le choix entre un mode`le ge´ne´ratif parame´trique et un mode`le non
ge´ne´ratif, le crite`re BIC n’est pas utilisable car les vraisemblances ne s’expriment pas sur
les meˆmes espaces probabilise´s. Pour la meˆme raison, BIC ne peut s’appliquer tel quel a`
la se´lection de variables.
4.3.2 Crite`res lie´s a` l’erreur de classement
Une technique universelle pour se´lectionner un mode`le ge´ne´ratif et/ou un mode`le non
ge´ne´ratif et/ou des variables en classification supervise´e s’appuie sur une estimation du
taux d’erreur de classement associe´ a` la re`gle rˆ estime´e. Pour se faire, on peut s’appuyer sur
des strate´gies de re´e´chantillonnage comme la validation croise´e. Le principe est de diviser
ale´atoirement l’ensemble (x, z) en v parties (approximativement) e´gales puis, notant θˆm
( = 1, . . . , v) l’estimateur de θ a` partir de (x, z) correspondant a` (x, z) prive´ de sa e
partie, le crite`re de validation croise´e est de´fini par :
VCm = 1− 1
v
v∑
=1
∑
{i:(xi,zi)∈(x,z)}
Zir(xi;θˆm). (28)
On retient alors le mode`le et/ou les variables conduisant a` la valeur du crite`re la plus
faible. Lorsque n est petit, il est recommande´ de prendre v = n, ce qui revient a` la
proce´dure dite du leave-one-out consistant a` soustraire une unique observation a` chaque
fois. Lorsque l’e´chantillon est suffisamment grand, on peut cependant se contenter de la
me´thode de l’e´chantillon test consistant a` diviser l’e´chantillon en deux parties seulement,
la premie`re servant a` apprendre tandis que la seconde sert a` e´valuer la qualite´ de la re`gle.
Remarquons que la proce´dure peut eˆtre relativement couˆteuse en temps de calcul car
ne´cessitant de nombreuses estimations de la re`gle de classement. Cependant, il est parfois
possible, en particulier dans le cas ge´ne´ratif, d’e´viter de recalculer entie`rement la re`gle
de de´cision a` chaque fois [8]. Signalons aussi qu’il est risque´ d’utiliser ensuite VCm pour
estimer le taux d’erreur associe´ au mode`le retenu car il souffre d’un biais d’optimisme.
On aura alors recours a` une me´thode de double validation croise´e ([27], pp. 210–211).
4.3.3 Exemple illustratif
On va maintenant e´tudier un jeu de donne´es issu d’une proble´matique marketing2 pour
illustrer l’utilisation du mode`le multinomial d’inde´pendance conditionnelle et les me´thodes
de choix de mode`les. Il est constitue´ de n = 6876 familles de la baie de San Francisco
de´crites par d = 12 variables cate´gorielles relatives au mode de vie et comportant deux
variables ou plus : statut marital, classe d’aˆge, niveau d’e´ducation,. . . Les groupes sont
au nombre de trois (g = 3) et repre´sentent le niveau de revenu de la famille : revenus
faibles (moins de 19 999$), moyens (entre 20 000$ et 39 999$) et forts (plus de 40 000$). La
figure 7 repre´sente les donne´es dans le 1er plan de l’analyse en correspondance multiple
(ACM).
On met en compe´tition deux mode`les multinomiaux d’inde´pendance conditionnelle (le
mode`le ge´ne´ral non contraint et le mode`le a` modalite´ majoritaire de´fini par (15)) et le
2Origine : Impact Resources, Inc., Columbus, OH (1987). (From questionnaires containg 502 questions
filled out by shopping mall customers in the San Francisco Bay area.)
c© Revue MODULAD, 2009 -18- Nume´ro 40
−1 −0.5 0 0.5 1 1.5 2 2.5
−1
0
1
1st correspondance analysis axis
 
 
 
 
 
 
 
 
 
 
 
2n
d 
ax
is
 
 
Low income
−1 −0.5 0 0.5 1 1.5 2 2.5
−1
0
1
1st correspondance analysis axis
 
 
 
 
 
 
 
 
 
 
 
2n
d 
ax
is
 
 
Average income
−1 −0.5 0 0.5 1 1.5 2 2.5
−1
0
1
1st correspondance analysis axis
 
 
 
 
 
 
 
 
 
 
 
2n
d 
ax
is
 
 
High income
Fig. 7 – Les donne´es marketing sur les deux premiers axes de l’ACM du nuage global.
Les trois groupes sont repre´sente´s dans des sous-figures diffe´rentes pour faciliter leur
comparaison.
mode`le logistique. Comme l’e´chantillon est d’assez grande taille, on utilise la me´thode
de l’e´chantillon test en prenant un e´chantillon d’apprentissage de taille 4000 individus et
on conservant les 2876 individus restant pour e´valuer la re`gle de classement de chaque
mode`le. La table 2 donne l’estimation de l’erreur pour les trois mode`les et la valeur du
crite`re BIC pour les deux mode`les ge´ne´ratifs. BIC se´lectionne le mode`le ge´ne´ratif le plus
complexe, tout comme le crite`re d’erreur.
Mode`le ê(rˆ) (%) BIC
Multinomial ge´ne´ral 36.79 -56619
Multinomial a` modalite´ majoritaire 39.78 -62459
Logistique 39.36 (incomparable)
Tab. 2 – Classification supervise´e sur les donne´es marketing. L’erreur est estime´e par la
me´thode de l’e´chantillon test et le crite`re BIC est calcule´ sur l’e´chantillon au complet.
En outre, il est possible de caracte´riser facilement chacun des groupes avec le mode`le
multinomial retenu en regardant de pre`s les parame`tres estime´s : la table 3 donne les
proportions πk estime´es et la table 4 donne les parame`tres αkjh estime´s pour la variable
« statut marital ». On peut faire de meˆme pour les autres variables.
5 Bilan et perspectives
Les mode`les de me´lange apportent une re´ponse cohe´rente au proble`me de la classi-
fication : mode´lisation explicite et interpre´table, outils de la statistique mathe´matique
c© Revue MODULAD, 2009 -19- Nume´ro 40
Revenus faibles moyens forts
πk (% ) 34.63 28.40 36.98
Tab. 3 – Proportions estime´es des groupes avec le mode`le multinomial d’inde´pendance
conditionnelle ge´ne´ral pour les donne´es marketing.
Revenus \ modalite´s marie´ en couple divorce´ veuf ce´libataire
Faibles 10.90 7.15 9.10 3.90 68.95
Moyens 37.06 8.54 13.82 3.43 37.15
Forts 62.27 6.90 6.22 1.49 23.12
Tab. 4 – Parame`tres αkjh estime´s (en %) du mode`le multinomial d’inde´pendance condi-
tionnelle ge´ne´ral pour la variable « statut marital » avec les donne´es marketing.
a` disposition, ge´ne´ralisation de me´thodes traditionnelles, mise en œuvre simple par des
logiciels de´die´s. Tout cela contribue a` la diffusion croissante de ce type de me´thode dans
de nombreux domaines d’applications.
Comme partout, la classification est constamment confronte´e a` de nouveaux de´fis et
les mode`les de me´lange apparaissent encore comme une approche suffisamment flexible
et rigoureuse pour apporter des re´ponses pertinentes. Ainsi, du point de vue de la na-
ture des donne´es, les mode`les de me´lange gaussien peuvent eˆtre adapte´s a` des donne´es
de tre`s haute dimension (plusieurs milliers de variables) [11, 12], a` des donne´es bruite´es
par l’introduction d’une « composante de bruit » [3] ou encore a` des donne´es partielle-
ment e´tiquete´es (classification semi-supervise´e) [20]. Les objectifs peuvent aussi e´voluer :
classification croise´e des individus et des variables [28], choix de variables en classification
non supervise´e (voir [37, 33] pour une re´interpre´tation comme un choix de mode`les dans
le cas gaussien), classification supervise´e ou non de plusieurs e´chantillons a` la fois par
e´tablissement de liens entre populations [4, 32, 30].
Re´fe´rences
[1] H. Akaike. A new look at statistical model identification. IEEE Transactions on
Automatic Control, AC-19 :716–723, 1974.
[2] J.A. Anderson. Separate sample logistic discrimination. Biometrika, 59 :19–35, 1972.
[3] J. D. Banfield and A. E. Raftery. Model-based Gaussian and non-Gaussian clustering.
Biometrics, 49 :803–821, 1993.
[4] C. Biernacki, F. Beninel, and V. Bretagnolle. A generalized discriminant rule when
training population and test population differ on their descriptive parameters. Bio-
metrics, 58(2) :387–397, 2002.
[5] C. Biernacki, G. Celeux, A. Anwuli, G. Govaert, and F. Langrognet. Le logiciel
mixmod d’analyse de me´lange pour la classification et l’analyse discriminante. La
Revue de Modulad, 35 :25–44, 2006.
c© Revue MODULAD, 2009 -20- Nume´ro 40
[6] C. Biernacki, G. Celeux, and G. Govaert. Assessing a mixture model for clustering
with the integrated completed likelihood. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(7) :719–725, 2000.
[7] C. Biernacki, G. Celeux, and G. Govaert. Choosing starting values for the EM
algorithm for getting the highest likelihood in multivariate gaussian mixture models.
Computational Statistics and Data Analysis, 41 :561–575, 2003.
[8] C. Biernacki and G. Govaert. Choosing models in model-based clustering and discri-
minant analysis. J. Statis. Comput. Simul., 64 :49–71, 1999.
[9] H. Bock. Probabilistic aspects in cluster analysis. In O. Opitz, editor, Conceptual
and numerical analysis of data, pages 12–44. Springer-Verlag, Berlin, 1989.
[10] L. Bordes, S. Mottelet, and P. Vandekerkhove. Semiparametric estimation of a two
components mixture model. Annals of Statistics, 34 :1204–1232, 2006.
[11] C. Bouveyron, S. Girard, and C. Schmid. High-dimensional data clustering. Com-
putational Statistics and Data Analysis, 52(1) :502–519, 2007.
[12] C. Bouveyron, S. Girard, and C. Schmid. High dimensional discriminant analysis.
Communications in Statistics : Theory and Methods, 36(14) :2607–2623, 2007.
[13] P.G. Bryant and J.A. Williamson. Asymptotic behaviour of classification maximum
likelihood estimates. Biometrika, 65 :273–281, 1978.
[14] G. Celeux and J. Diebolt. The SEM algorithm : A probabilistic teacher algorithm
derived from the EM algorithm for the mixture problem. Computational Statistics
Quarterly, 2 :73–82, 1985.
[15] G. Celeux and G. Govaert. Clustering criteria for discrete data and latent class
models. Journal of Classification, 8(2) :157–176, 1991.
[16] G. Celeux and G. Govaert. A classification EM algorithm for clustering and two
stochastic versions. Computational Statistics and Data Analysis, 14(3) :315–332,
1992.
[17] G. Celeux and G. Govaert. Comparison of the mixture and the classification maxi-
mum likelihood in cluster analysis. Journal of Statistical Computation and Simula-
tion, 47 :127–146, 1993.
[18] G. Celeux and G. Govaert. Gaussian parsimonious clustering models. Pattern Re-
cognition, 28(5) :781–793, 1995.
[19] G. Celeux and J.P. Nakache. Analyse discriminante sur variables qualitatives. Poly-
technica, 1994.
[20] O. Chapelle, B. Scho¨lkopf, and A. Zien. Semi-Supervised Learning. MIT Press, 2006.
[21] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete
data via the EM algorithm (with discussion). Journal of the Royal Statistical Society,
B 39 :1–38, 1977.
[22] B. Dubuisson. Decision with rejects options. In Eusipco, Barcelone, 1990.
[23] R. A. Fisher. Multiple measurements in taxinomie problems. Annals of Eugenics,
7(2) :179–188, 1936.
[24] H. P. Friedman and J. Rubin. On some invariant criteria for grouping data. Journal
of American Statistical Association, 62 :1159–1178, 1967.
c© Revue MODULAD, 2009 -21- Nume´ro 40
[25] M. Goldstein and W.R. Dillon. Discrete discriminant analysis. John Wiley & Sons,
New York, 1978.
[26] L. A. Goodman. Exploratory latent structure models using both identifiable and
unidentifiable models. Biometrika, 61 :215–231, 1974.
[27] G. Govaert. Analyse des donne´es. Hermes, Paris, 2003.
[28] G. Govaert and M. Nadif. Clustering with block mixture models. Pattern Recognition,
36(2) :463–473, 2003.
[29] R.J. Hathaway. Another interpretation of the em algorithm for mixture distributions.
Statistics & Probability Letters, 4 :53–56, 1986.
[30] J. Jacques and C. Biernacki. Extension of model-based classification for binary data
when training and test populations differ. Journal of Applied Statistics, (a` paraˆıtre),
2009.
[31] E. Lebarbier and T. Mary-Huard. Une introduction au crite`re BIC : fondements
the´oriques et interpre´tation. Journal de la SFdS, (a` paraˆıtre), 2006.
[32] A. Lourme and C. Biernacki. Gaussian model-based classification when training
and test population differ : Estimating jointly related parameters. In In First joint
meeting of the Socie´te´ Francophone de Classification and the Classification and Data
Analysis Group of SIS, Caserta, Italy, 2008.
[33] C. Maugis, G. Celeux, and M.L. Martin-Magniette. Variable selection for clustering
with gaussian mixture models. Biometrics, (a` paraˆıtre), 2009.
[34] G. J. McLachlan. Discriminant Analysis and Statistical Pattern Recognition. Wiley,
2004.
[35] G. J. McLachlan and K. Krishnan. The EM Algorithm. Wiley, New York, 1997.
[36] G. J. McLachlan and D. Peel. Finite Mixture Models. Wiley, New York, 2000.
[37] A.E. Raftery and N. Dean. Variable selection for model-based clustering. Journal of
the American Statistical Association, 101 :168–178, 2006.
[38] K. Roeder and L. Wasserman. Practical bayesian density estimation using mixtures
of normals. Journal of the American Statistical Association, 92(439) :894–902, 1997.
[39] G. Schwarz. Estimating the number of components in a finite mixture model. Annals
of Statistics, 6 :461–464, 1978.
[40] A. J. Scott and M. J. Symons. Clustering methods based on likelihood ratio criteria.
Biometrics, 27 :387–397, 1971.
[41] R. Tomassone, M. Danzard, J.J. Daudin, and J.P. Masson. Discrimination et clas-
sement. Masson, 1988.
[42] W. N. Venables and B. D. Ripley. Modern Applied Statistics with S. Springer, 1994.
[43] J. Ward. Hierarchical grouping to optimize an objective function. Journal of the
American Statistical Association, 58 :236–244, 1963.
c© Revue MODULAD, 2009 -22- Nume´ro 40
