Classification topologique probabiliste pour des donnÃ©es
catÃ©gorielles
Nicoleta Rogovschi et Mohamed Nadifâˆ—
LIPADE, UniversitÃ© Paris Descartes
45, rue des Saints PÃ¨res, 75006 Paris, France
PrÃ©nom.Nom@parisdescartes.fr
RÃ©sumÃ©. Cet article prÃ©sente une carte auto-organisatrice probabiliste pour lâ€™ana-
lyse et la classification topologique des donnÃ©es catÃ©gorielles. En considÃ©rant un
modÃ¨le de mÃ©langes parcimonieux nous introduisons une nouvelle carte auto-
organisatrice (SOM) probabiliste. Lâ€™estimation des paramÃ¨tres de notre modÃ¨le
est rÃ©alisÃ©e Ã  lâ€™aide de lâ€™algorithme EM classique. Contrairement Ã  SOM, lâ€™al-
gorithme dâ€™apprentissage proposÃ© optimise une fonction objective. Ces perfor-
mances ont Ã©tÃ© Ã©valuÃ©es sur des donnÃ©es rÃ©elles et les rÃ©sultats obtenus sont
encourageants et prometteurs Ã  la fois pour la classification et pour la modÃ©lisa-
tion.
1 Introduction
Data visualization is an important step in the exploratory phase of data analysis. This step
is more difficult when it involves binary data and categorical variables (Andreopoulos et al.,
2006; Saund, 1995). Self-organizing maps are being increasingly used as tools for visualiza-
tion, as they allow projection over small areas that are generally two dimensional. The basic
model proposed by (Kohonen, 2001), was only designed for numerical data, but it has been
successfully applied to treating textual data, (Kaski et al., 1998). This algorithm has also been
applied to binary data following transformation of the original data (Ibbou et Cottrell, 1995;
Lebbah et al., 2000). Developing generative models of the Kohonen map has long been an
important goal. These models vary in the form of the interactions, and they assume the hid-
den generators may follow in generating the observations. Some extensions and reformulations
of the Kohonen model have been described in the literature. They include probabilistic self-
organizing maps (Anouar et al., 1997) which define a map as a gaussian mixture and use the
maximum likelihood approach to define an iterative algorithm.
In Verbeek et al. (2005), the authors propose a probabilistic generalization of Kohonenâ€™s
SOMwhich maximizes the variational free-energy that sums data log-likelihood and Kullback-
Leibler divergence between a normalized neighbourhood function and the posterior distribu-
tion on the given data for the components. We have also Soft topographic vector quantization
(STVQ), which uses some measure of divergence between data items and cells to minimize a
new error function (Heskes, 2001; Graepel et al., 1998). Another model, often presented as the
probabilistic version of the self-organizing map, is the Generative Topographic Map (GTM)
(Bishop et al., 1998; Kaban et Girolami, 2001). However, the manner in which GTM achieves
- 359 -
Classification topologique probabiliste pour des donnÃ©es catÃ©gorielles
the topographic organization is quite different from those used in the SOM models. In GTM
mixture components are parameterized by a linear combination of nonlinear functions of the
locations of the components in the latent space. The GTM was developed for continuous data.
A specific GTM model was subsequently developed for binary data by adopting a variational
approximation to the binomial likelihood (Graepel et al., 1998).
Also, in Kaban et al. (2004), the authors concentrate on modelling binary coded data where
only the presence or absence of a variable is of interest. In contrast to other approaches, the
model is linear. The model is seen as a Bernoulli analogue of the multinomial decomposition
model. In Jollois et Nadif (2007), the main of the proposed method is to speed-up convergence
of EM, and second to yield same results (or not so far) than traditional EM using categorical
data. Others similar techniques have been developed to cluster large data sets (Kostiainen et
Lampinen, 2002; Hofmann, 2001).
Here, we concentrate on modelling qualitative data using binary coding. This model in-
volves use of the probabilistic formalism of the topological map used in (Anouar et al., 1997);
therefore, it consists of estimating the parameters of the model by maximizing the likelihood
of the data set. The learning algorithm that we propose is an application of the EM standard
algorithm, (McLachlan et Krishman, 1997). Some variants are proposed to speed-up EM in
reducing the time spent in the E-step in the case of categorical data, (Jollois et Nadif, 2007). In
this paper we proposed a new method called WeCSOM (Weighted Categorical Self-Organizing
Map) which combine the benefits of SOMs, K-mode (Huang, 1998) algorithm and mixture
models to design a new mixture for categorical data. This approach is based on the model of
the self-organizing maps and uses a parsimonious mixture models which has the advantage
of being directly applicable to the categorical data without using a specific encoding a priori.
The proposed learning algorithm is an application of the classical EM algorithm that allows to
weight the variables considering the number of modes of each one during the learning process,
thus achieving an optimized classification of the data.
The rest of this paper is organized as follows: we present the principle of probabilistic map
and categorical data in section 2. Our proposed approach is presented in sections 2.1 and 2.2.
In sections 3, we present different results and, finally the paper ends with a conclusion and
some future works for the proposed methods.
2 Categorical data and Probabilistic self-organizing map
As with a traditional self-organizing map, we assume that the lattice C has a discrete topo-
logy (discrete output space) defined by an undirect graph. Usually, this graph is a regular grid
in one or two dimensions. We denote the number of cells in C as Ncell. For each pair of cells
(c,r) on the map, the distance Î´(c,r) is defined as the length of the shortest chain linking cells
r and c.
2.1 General probabilistic formalism
To define the model of topological maps based on mixture models we associate to each cell c
of the map C a density function fc(x) = p(x|Î¸c) whose parameters are denoted by Î¸. Follo-
wing the bayesian formalism, presented in (Luttrel, 1994; Anouar et al., 1997), we assume that
- 360 -
Rogovschi et al.
each observation x is generated by the following process: We start by associating to each cell
c âˆˆ C a probability p(x|c) where x is a vector in the data space. Next, we pick a cell câˆ— from
C according to the prior probability p(câˆ—). For each cell câˆ—, we select an associated cell c âˆˆ C
following the conditional probability p(c|câˆ—). All cells c âˆˆ C contribute to the generation of x
with p(x|c) according to the proximity to câˆ— described by the probability p(c|câˆ—). Thus, a high
proximity to câˆ— implies a high probability p(c|câˆ—), and therefore the contribution of c to the
generation of x is high.
Due to the "Markov" property, p(x|c,câˆ—) = p(x|c), the probability distribution of the ob-
servations generated by a cell câˆ— of C is a mixture pcâˆ—(x|câˆ—) of probabilities completely defined
from the map as:
pcâˆ—(x|câˆ—) =
âˆ‘
câˆˆC
p(c|câˆ—)p(x|c).
The generative model considers the mixture of probabilities, given by :
p(x) =
âˆ‘
c,câˆ—âˆˆC
p(c,câˆ—,x) =
âˆ‘
c,câˆ—âˆˆC
p(x|c)p(c|câˆ—)p(câˆ—) =
âˆ‘
câˆ—âˆˆC
p(câˆ—)pcâˆ—(x), (1)
with
pcâˆ—(x) = p(x|câˆ—) =
âˆ‘
câˆˆC
p(c|câˆ—)p(x|c), (2)
where the conditional probability p(c|câˆ—) is assumed to be known. To introduce the self-
organizing process in the mixture model learning, we assume that p(c|câˆ—) can be defined as:
p(c|câˆ—) = K
T (Î´(c,câˆ—))âˆ‘
râˆˆCKT (Î´(r,câˆ—))
,
where KT is a neighbourhood function depending on the parameter T (called temperature):
KT (Î´) = K(Î´/T ), where K is a particular kernel function which is positive and symmetric (
lim
|x|â†’âˆž
K(x) = 0). ThusK defines for each cell câˆ— a neighbourhood region in C. The parameter
T allows control of the size of the neighbourhood influencing a given cell on the map. As with
the Kohonen algorithm, we decrease the value of T between two values Tmax and Tmin.
2.2 The proposed model
In the following, let we focus on categorical data. Let be a set of N instances x1, . . . ,xN
described by n categorical attributes x1, . . . ,xn. The data matrix is noted x and defined by
x = {(xji ); i = 1, . . . ,N ; k = 1, . . . ,n}. Each instance i is represented as [x1i , . . . ,xni ] and
for each attribute xj , we note cj the number of categories. We consider a restricted latent class
model [16], then the conditional distribution in p(xi|c) is now given as the product of univariate
single distributions
p(xi|c) = fc(xi|wc,c) =
nâˆ
k=1
fc(x
k
i |wkc ,kc ),
wherewc = (w1c , . . . ,wnc ) represents the vector of categories and c = (Îµ1c , . . . ,Îµnc ) is a vector
of probabilities. Taking
fc(x
k
i |wkc ,Îµkc ) = (1âˆ’ Îµkc )1âˆ’d(x
k
i ,w
k
c )
(
Îµkc
ck âˆ’ 1
)d(xki ,wkc )
,
- 361 -
Classification topologique probabiliste pour des donnÃ©es catÃ©gorielles
where d(a,b) = 1 if a = b and 0 otherwise, we define a parsimonious model where the parame-
ter c consists of (wc,c) with wc is the mode of the the component and c is a k-dimensional
vector of probabilities indicating the degree of heterogeneity. The density fc(xi|wc,c) ex-
presses that, for c, the attribute xk takes category wkc with the greatest probability (1âˆ’ kc ) and
takes each other category with the same probability 
k
c
(ckâˆ’1) . Note that, setting the clustering
problem under the classification maximum likelihood approach, the authors in [16] have defi-
ned a generalization of the kmodes criterion and proposed better fit criteria. In our situation,
we can assume that the parameter Îµkc depends only on a cell c âˆˆ C. Then, the model mixture
generator becomes:
p(x) =
âˆ‘
câˆ—âˆˆC
p(câˆ—)
âˆ‘
câˆˆC
p(c|câˆ—)fc(x,wc,c). (3)
Therefore, the parameters Î¸ = Î¸C âˆª Î¸Câˆ— which define the model mixture generator (3) are
constituted of the parameters (Î¸C = {Î¸c,c = 1..Ncell}, where Î¸c = (wc,c)), and all the prior
probabilities, also called mixing coefficients (Î¸Câˆ— = {Î¸câˆ— ,câˆ— = 1..Ncell} where Î¸câˆ— = p(câˆ—)).
The difficulty now is to define the cost function and the learning algorithm for estimating all
these parameters dedicated to categorical data. Our WeCSOM algorithm was inspired by a
probabilistic SOM model proposed by (Anouar et al., 1997) and represents a generalization of
the model proposed by (Lebbah et al., 2007).
2.3 Cost function and optimization algorithm
The learning algorithm is based on maximizing the likelihood of the observations by ap-
plying the EM algorithm (Dempster et al., 1977). Learning is facilitated by introducing N
hidden variables Îž = (Î¾1, . . . ,Î¾N ); each hidden variable Î¾ = (c,câˆ—) indicates which of the
cell pairs c and câˆ—, generate the corresponding data observation x. We introduce the hidden
variable Î¾ = (c,câˆ—) in expression (3):
p(x) =
âˆ‘
Î¾âˆˆCÃ—C
p(x,Î¾) =
âˆ‘
c,câˆ—âˆˆC
p(câˆ—)p(c|câˆ—)fc(x,wc,Îµc). (4)
We define a binary indicator variable Î±(c,c
âˆ—)
i which indicates the hidden generator that may
follow in generating the observation xi as: Î±(c,c
âˆ—)
i =
{
1 for Î¾i = (c,câˆ—)
0 otherwise . Using expres-
sion (4), and the binary indicator Î±(c,câˆ—)i , we can define the classification likelihood of the
observations using the hidden variables as follows:
LT (,Îž; Î¸) =
Nâˆ
i=1
âˆ
câˆ—âˆˆC
âˆ
câˆˆC
[
Î¸c
âˆ—
p(c|câˆ—)fc(x,wc,c)
]Î±(c,câˆ—)
i
.
The log-likelihood becomes:
lnLT (,Îž; Î¸) =
Nâˆ‘
i=1
âˆ‘
c,câˆ—âˆˆC
Î±
(c,câˆ—)
i
[
ln(Î¸c
âˆ—
) + ln
(
KT (Î´(câˆ—,c))
Tcâˆ—
)
+ ln(fc(x,wc,c))
]
,
- 362 -
Rogovschi et al.
where Tcâˆ— =
âˆ‘
râˆˆCK
T (Î´(r,câˆ—)). The application of the EM algorithm [7] for the maximiza-
tion of log-likelihood requiresQT (Î¸t,Î¸tâˆ’1) to be maximised for a fixed temperature T defined
as:
QT (Î¸t,Î¸tâˆ’1) = E
[
lnLT (,Îž; Î¸t)|,Î¸tâˆ’1] ,
where Î¸t is the set of the parameters estimated at the tth step of the learning algorithm.
However, the E-step calculates the expectation of log-likelihood with respect to the hidden
variable while maintaining the established parameter Î¸tâˆ’1. During the M-step, after upda-
ting QT (Î¸t,Î¸tâˆ’1) from the previous step, we maximize the QT (Î¸t,Î¸tâˆ’1) with respect to Î¸t,
(Î¸t = argmaxÎ¸(QT (Î¸,Î¸tâˆ’1))). The two-steps increase the function likelihood. The function
QT (Î¸t,Î¸tâˆ’1) is defined as:
QT (Î¸t,Î¸tâˆ’1) =
Nâˆ‘
i=1
âˆ‘
câˆ—âˆˆC
âˆ‘
câˆˆC
E(Î±
(c,câˆ—)
i |xi,Î¸tâˆ’1)
Ã—
[
ln(Î¸c
âˆ—
) + ln
(
KT (Î´(câˆ—,c)
Tcâˆ—
)
+ ln(fc(x,wc,c))
]
where E(Î±(c,c
âˆ—)
i |xi,Î¸tâˆ’1) = p(Î±(c,c
âˆ—)
i = 1|xi,Î¸tâˆ’1) = p(c,câˆ—|xi,Î¸tâˆ’1), with
p(c,câˆ—|xi,Î¸tâˆ’1) = p(c
âˆ—)p(c|câˆ—)p(x|c)
p(x)
.
The function QT (Î¸t,Î¸tâˆ’1) breaks into three terms
QT (Î¸t,Î¸tâˆ’1) = QT1 (Î¸
C ,Î¸tâˆ’1) +QT2 (Î¸
Câˆ— ,Î¸tâˆ’1) +QT3 (Î¸
tâˆ’1) (5)
where
QT1 (Î¸
C ,Î¸tâˆ’1) =
nâˆ‘
k=1
Nâˆ‘
i=1
âˆ‘
câˆˆC
âˆ‘
câˆ—âˆˆC
p(c,câˆ—|xi,Î¸tâˆ’1) ln(fc(xk,wkc ,Îµkc )),
QT2 (Î¸
Câˆ— ,Î¸tâˆ’1) =
Nâˆ‘
i=1
âˆ‘
câˆ—âˆˆC
âˆ‘
câˆˆC
p(c,câˆ—|xi,Î¸tâˆ’1) ln(Î¸câˆ—),
QT3 (Î¸
tâˆ’1) =
Nâˆ‘
i=1
âˆ‘
câˆ—âˆˆC
âˆ‘
câˆˆC
p(c,câˆ—|xi,Î¸tâˆ’1) ln
(
KT (Î´(câˆ—,c)
Tcâˆ—
)
.
The parameters Î¸C and Î¸Câˆ— indicate the parameters estimated at the tth step. The first term
QT1 (Î¸
C ,Î¸tâˆ’1) depends on Î¸c,k = (wkc ,Îµkc ); the second term QT2 (Î¸C
âˆ—
,Î¸tâˆ’1) depends on Î¸câˆ— ,
and the third term is constant. Maximizing QT (Î¸t,Î¸tâˆ’1) with respect to Î¸câˆ— and Î¸c can be
performed separately including the parameter wc and c. The maximization of QT (Î¸t,Î¸tâˆ’1)
leads to the updates that are calculated using the parameters estimated at the tâˆ’ 1th step. The
expressions are defined as follows:
Î¸c
âˆ—
= p(câˆ—) =
âˆ‘
xiâˆˆA p(c
âˆ—|xi,Î¸tâˆ’1)
N
(6)
- 363 -
Classification topologique probabiliste pour des donnÃ©es catÃ©gorielles
where
p(câˆ—|xi,Î¸tâˆ’1) =
âˆ‘
câˆˆC p(c,c
âˆ—|xi,Î¸tâˆ’1) and p(c|xi,Î¸tâˆ’1) =
âˆ‘
câˆ—âˆˆC p(c,c
âˆ—|xi,Î¸tâˆ’1). Each
component of wc = (w1c , . . . ,wkc , . . . ,wnc ) and c = (Îµ1c ,Îµ2c , . . . ,Îµkc , . . . ,Îµnc ) is then compu-
ted as follows:
wkc =e=1,...,ck
Nâˆ‘
i=1
p(c|xi,Î¸tâˆ’1)d(xki ,wkc ) (7)
and
Îµkc =
âˆ‘N
i=1 p(c|xi,Î¸tâˆ’1)d(xki ,wkc )âˆ‘N
i=1 p(c|i,Î¸tâˆ’1)
, (8)
The application of EM for the maximization gives rise to the iterative algorihtm of WeCSOM.
The version of the WeCSOM algorithm for a fixed T parameter is presented in the following
way:
Algorithm 1 Principal stages of the learning algorithm WeCSOM
1. Initialization (iteration t = 0) Choose the initial parameters (Î¸0) and the number of
iterations Niter.
2. Basic Iteration at a constant T (iteration t â‰¥ 1) Calculate all the parameters Î¸t =
{Î¸câˆ— ,wc,c} from the previous parameters Î¸tâˆ’1 associated with each cell c and câˆ— by
applying the formulas: (6), (7) and (8).
3. Repeat the basic iteration until t > Niter.
The WeCSOM learning algorithm allows us to estimate the parameters maximizing the log-
likelihood function for a fixed T . As in the SOM algorithm, we decrease the value of T between
two values Tmax and Tmin, to control the size of the neighbourhood influencing a given cell on
the map. For each T value, we get a likelihood function LT , and therefore the expression varies
with T . When decreasing T , the learning algorithm of WeCSOM is defined in the Algorithm
2.
Algorithm 2 Algorithm WeCSOM varying T
1. Initialization Phase (iteration t = 0): Choose Tmax, Tmin and Niter. Apply the princi-
pal stages of WeCSOM algorithm described above for the value of T fixed to Tmax.
2. Iterative step:We assume that the previous parameter Î¸t are known. Compute the new
value of T by applying the following formula: T = Tmax
(
Tmin
Tmax
) t
Niterâˆ’1
.
For fixed value of the parameter T , apply the basic iteration described in the principal
stages, which estimates the new parameter Î¸t+1 using the formulas (6), (7) and (8).
3. Repeat the Iterative step while t â‰¤ Niter.
We can define two steps in the operating of the algorithm:
â€“ The first step corresponds to high T values. In this case, the influencing neighbourhood
of each cell c on the map is important and corresponds to higher values of KT (Î´(c,r)).
- 364 -
Rogovschi et al.
Formulas (6), (7) and (8) use a high number of observations to estimate model parame-
ters. This step provides the topological order.
â€“ The second step corresponds to small T values. The number of observations in formulas
(6), (7) and (8) is limited. Therefore, the adaptation is very local. The parameters are
accurately computed from the local density of the data.
3 Experimentations and validations
To evaluate the quality of clustering, we adopt the approach of comparing the results to a
"ground truth". We use the clustering accuracy for measuring the clustering results. This is a
common approach in the general area of data clustering.
This procedure is defined by (Jain et Dubes, 1988) as "validating clustering by extrinsic
classification", and has been followed in many other studies (Andreopoulos et al., 2006; Khan
et Kant, 2007).
Thus, to adopt this approach we need labeled data sets, where the external (extrinsic) know-
ledge is the class information provided by labels. Hence, if the WeCSOM finds significant
clusters in the data, these will be reflected by the distribution of classes. Therefore we operate
a vote step for clusters and compare them to the behavior methods from the literature. The
so-called vote step consists in the following. For each cluster c âˆˆ C:
â€“ Count the number of observation of each class l (call it Ncl).
â€“ Count the total number of observation assigned to the cell c (call it Nc).
â€“ Compute the proportion of observations of each class (call it Scl = Ncl|Nc).
â€“ Assign to the cluster the label of the most represented class (lâˆ— = argmaxl(Scl).
A cluster c for which Scl = 1 for some class labeled l is usually termed a "pure" cluster, and a
purity measure can be expressed as the percentage of elements of the assigned class in a cluster.
The experimental results are then expressed as the fraction of observations falling in clusters
which are labeled with a class different from that of the observation. This quantity is expressed
as a percentage and termed "purity percentage" (indicated as Purity% in the results).
To test the performance of our approach we used many publics data sets extracted from the
UCI repository (Asuncion et Newman, 2007). The table 1 summarizes a short description of
these data sets.
TAB. 1 â€“ â€“ Description of the used datasets for the validations.
Data set Size nb. of classes
Zoo 101Ã— 16 7
Congressional vote 435Ã— 16 2
Wisconsis-B-C 699Ã— 9 2
Nursery 12960Ã— 8 2
Car 1728Ã— 6 4
Post-Operative 90Ã— 8 3
To conduct experimental comparison and to verify the efficacy of our proposed model, we
compare our method with the RTC (Relational Topological Clustering), (Labiod et al., 2010).
- 365 -
Classification topologique probabiliste pour des donnÃ©es catÃ©gorielles
We choose this method because it is based on the same principle of the Kohonens model
(conservation of the data topological order) and uses the Relational Analysis formalism by
optimizing a cost function defined by analogy with Condorcet criterion. One disavantage of
the RTC method is that this approach treats all the features equally. We use the same categori-
cal data sets obtained from UCI repository (Asuncion et Newman, 2007) and used in (Labiod
et al., 2010).
For each dataset we learned a map of different sizes (from 5x5 to 10x10) and we indicate in
the table 2 the purity of clustering for RTC technique and WeCSOM. The results illustrate that
the proposed technique increase the purity index compared to the RTC and also presents the
advantage to treat directly the categorical data without using the binary coding.
We compared also the performance of our method with the result provided in (Khan et
Kant, 2007) that used a version of K-modes clustering method dedicated to categorical data.
Table 3 lists the classification error obtained with different methods. We compute the fraction
of observations falling in clusters which are labeled with a class different from that of the ob-
servation. We can observe that our results are much better then the results provided by K-modes
(Khan et Kant, 2007). Also we improve the error rate compared to BinBatch algorithm which
represents the classical SOM approach dedicated to binary data using Hamming distance.
TAB. 2 â€“ â€“ Comparison between RTC et WeCSOM using purity index. RTC : Relational Topo-
logical Clustering dedicated to categorical data using the Relational Analysis formalism.
Purity:% Size map RTC WeCSOM
Zoo (5Ã— 5) 97.84 98.13
Nursery (6Ã— 6) 78.69 81.52
Car (10Ã— 10) 80.17 82.19
Post-Operative (5Ã— 5) 78.21 81.34
TAB. 3 â€“ â€“ Comparison of the classification performances reached by K-modes, BinBatch and
WeCSOM clustering algorithms.
Error rate:% K-modes BinBatch WeCSOM
Wisconsis-B-C 13.2 3.87 2.34
Zoo 16.6 2.97 1.87
Congressional vote 13.2 5.91 5.77
4 Conclusion
This study reports the development of a computationally efficient EM approach to maximize
the likelihood of the data set to estimate the parameters of a probabilistic self-organizing map
model dedicated to categorical variables. This algorithm has the advantage of providing a pro-
totype with the same coding as the input data. The extention of the proposed method to the
co-clustering will be an interesting future work for dealing with large-scale problems.
- 366 -
Rogovschi et al.
RÃ©fÃ©rences
Andreopoulos, B., A. An, et X. Wang (2006). Bi-level clustering of mixed categorical and
numerical biomedical data. International Journal of Data Mining and Bioinformatics 1(1),
19 â€“ 56.
Anouar, F., F. Badran, et S. Thiria (1997). Self-organizing map, a probabilistic approach. In
Proceedings of WSOMâ€™97-Workshop on Self-Organizing Maps, Espoo, Finland June 4-6,
pp. 339â€“344.
Asuncion, A. et D. Newman (2007). UCI machine learning repository.
http://www.ics.uci.edu/âˆ¼mlearn/MLRepository.html.
Bishop, C. M., M. SvensÃ©n, et C. K. I.Williams (1998). GTM: The generative topographic
mapping. Neural Comput 10(1), 215â€“234.
Dempster, A., N. Laird, et D. Rubin (1977). Maximum likelihood from incomplete data via
the em algorithm. Roy. Statist. Soc 39(1), 1â€“38.
Graepel, T., M. Burger, et K. Obermayer (1998). Self-organizing maps: generalizations and
new optimization techniques. Neurocomputing 21, 173â€“190.
Heskes, T. (2001). Self-organizing maps, vector quantization, and mixture modeling. IEEE
Trans. Neural Networks 12, 1299â€“1305.
Hofmann, T. (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine
Learning 42, 177â€“196.
Huang, Z. (1998). Extensions to the k-means algorithm for clustering large data sets with
categorical values. In Data Mining and Knowledge Discovery 2.
Ibbou, S. et M. Cottrell (1995). Multiple correspondance analysis crosstabulation matrix using
the kohonen algorithm. In Verlaeysen, M. Editor proc of ESANNâ€™95, pp. 27â€“32. Dfacto
Bruxelles.
Jain, A. K. et R. C. Dubes (1988). Algorithms for clustering data. Upper Saddle River, NJ,
USA: Prentice-Hall, Inc.
Jollois, F.-X. et M. Nadif (2007). Speed-up for the expectation-maximization algorithm for
clustering categorical data. Journal of Global Optimization 37(4), 513â€“525.
Kaban, A., E. Bingham, et T. HirsimÃ¤ki (2004). Learning to read between the lines: The aspect
bernoulli model. In Proceedings of the Fourth SIAM International Conference on Data
Mining, Lake Buena Vista, Florida, USA.
Kaban, A. et M. Girolami (2001). A combined latent class and trait model for the analysis and
visualization of discrete data. IEEE Trans. Pattern Anal. Mach. Intell 23, 859â€“872.
Kaski, S., T. Honkela, K. Lagus, et T. Kohonen (1998). Websomâ€“self-organizing maps of
document collections. Neurocomputing 21, 101â€“117.
Khan, S. S. et S. Kant (2007). Computation of initial modes for k-modes clustering algorithm
using evidence accumulation. In IJCAI, pp. 2784â€“2789.
Kohonen, T. (2001). Self-organizing Maps. Springer Berlin.
Kostiainen, T. et J. Lampinen (2002). On the generative probability density model in the self-
organizing map. Neurocomputing 48, 217â€“228.
Labiod, L., G. Nistor, et B. Younes (2010). Relational topographic clustering (rtc). Internatio-
nal Joint Conference on Neural Networks, IJCNNâ€™10.
- 367 -
Classification topologique probabiliste pour des donnÃ©es catÃ©gorielles
Lebbah, M., N. Rogovschi, et Y. Bennani (2007). Besom : Bernoulli on self-organizing map.
In IJCNN, pp. 631â€“636. IEEE.
Lebbah, M., S. Thiria, et F. Badran (2000). Topological map for binary data. In Proceedings
European Symposium on Artificial Neural Networks-ESANN 2000, Bruges, April 26-27-28,
pp. 267â€“272.
Luttrel, S. P. (1994). A bayesian analysis of self-organizing maps. Neural Computing 6, 767 â€“
794.
McLachlan, G. et T. Krishman (1997). The EM algorithm and Extensions. Wiley, New York.
Saund, E. (1995). A multiple cause mixture model for unsupervised learning. Neural Com-
put. 7(1), 51â€“71.
Verbeek, J., N. Vlassis, et B. Krose (2005). Self-organizing mixture models. Neurocompu-
ting 63, 99â€“123.
Summary
This paper introduces a probabilistic self-organizing map for topographic clustering, anal-
ysis of categorical data. By considering a parsimonious mixture model, we present a new
probabilistic Self-Organizing Map (SOM). The estimation of parameters is performed by the
EM algorithm. Contrary to SOM, our proposed learning algorithm optimizes an objective
function. Its performance is evaluated on real datasets.
- 368 -
