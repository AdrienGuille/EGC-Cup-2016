Une nouvelle m√©thode divisive en classification non
supervis√©e pour des donn√©es symboliques intervalles
Nathana√´l Kasoro‚àó, Andr√© Hardy‚àó‚àó
‚àóUniversit√© de Kinshasa
D√©partement de Math√©matique et d‚ÄôInformatique
B.P. 190, Kinshasa, R√©publique D√©mocratique du Congo
kasoro.mulenda@yahoo.fr
‚àó‚àóUniversit√© de Namur
Unit√© de Statistique - D√©partement de Math√©matique
8 Rempart de la Vierge - B - 5000 Namur - Belgique
andre.hardy@fundp.ac.be
R√©sum√©. Dans cet article nous pr√©sentons une nouvelle m√©thode de classifica-
tion non supervis√©e pour des donn√©es symboliques intervalles. Il s‚Äôagit de l‚Äôex-
tension d‚Äôune m√©thode de classification non supervis√©e classique √† des donn√©es
intervalles. La m√©thode classique suppose que les points observ√©s sont la r√©ali-
sation d‚Äôun processus de Poisson homog√®ne dans k domaines convexes disjoints
de Rp. La premi√®re partie de la nouvelle m√©thode est une proc√©dure monoth√©-
tique divisive. La r√®gle de coupure est bas√©e sur une extension √† des donn√©es
intervalles du crit√®re de classification des Hypervolumes. L‚Äô√©tape d‚Äô√©lagage uti-
lise un test statistique bas√© sur le processus de Poisson homog√®ne. Le r√©sultat
est un arbre de d√©cision. La seconde partie de la m√©thode consiste en une √©tape
de recollement, qui permet, dans certains cas, d‚Äôam√©liorer la classification obte-
nue √† la fin de la premi√®re partie de l‚Äôalgorithme. La m√©thode est √©valu√©e sur un
ensemble de donn√©es r√©elles.
1 Introduction
Le but de la classification non supervis√©e est de d√©composer un groupe d‚Äôobjets, sur les-
quels onmesure un ensemble de variables, en un nombre relativement restreint de sous-groupes
d‚Äôobjets semblables. De nombreuses m√©thodes de classification ont √©t√© publi√©es dans la litt√©-
rature scientifique. La plupart d‚Äôentre elles utilisent un crit√®re de classification bas√© sur une
mesure de dissimilarit√©. Pour √©viter ce choix (bien souvent arbitraire) d‚Äôune dissimilarit√© nous
utilisons unmod√®le statistique pour la classification bas√© sur le processus de Poisson homog√®ne
(Hardy (1983)). De ce mod√®le est issue la m√©thode de classification des Hypervolumes (Hardy
(1983)). Pir√ßon (2004) a d√©velopp√© une nouvelle m√©thode divisive de classification bas√©e sur
le crit√®re de classification des Hypervolumes. Notre objectif est d‚Äô√©tendre cette m√©thode √† des
donn√©es intervalles. Une variable Y dont le domaine d‚Äôobservation est Y est appel√©e √† valeurs
d‚Äôensemble si ‚àÄxi ‚àà E, Y : E ‚Üí B : xi ‚àí‚Üí Y (xi) o√π B = P(Y) = {U = ‚àÖ | U ‚äÜ Y}.
Une nouvelle m√©thode de classification pour des donn√©es intervalles
Une variable √† valeurs d‚Äôensemble Y est appel√©e une variable intervalle si Y = R et ‚àÄx i ‚àà E,
il existe Œ±, Œ≤ ‚àà R, tels que Y (xi) = [Œ±, Œ≤].
2 Unmod√®le statistique pour la classification bas√© sur le pro-
cessus de Poisson homog√®ne
2.1 D√©finition : le processus de Poisson homog√®ne
N est un processus de Poisson homog√®ne d‚Äôintensit√© q (q ‚àà R) sur un ensemble D ‚äÇ R p
(0 < m(D) <‚àû) si les deux conditions suivantes sont satisfaites (Cox et Isham (1980)) :
‚Äì ‚àÄ A1, . . . , Ak ‚äÇ D, ‚àÄi = j ‚àà {1, . . . , k} o√π Ai ‚à©Aj = ‚àÖ, N(Ai) ‚ä•‚ä• N(Aj).
Les variables al√©atoires qui comptent le nombre de points dans des r√©gions disjointes de
l‚Äôespace sont ind√©pendantes.
‚Äì ‚àÄ A ‚äÇ D, ‚àÄk > 0, P (N(A) = k) = (q m(A))
k
k!
e‚àíqm(A).
La variable al√©atoireN(A) a une distribution de Poisson de moyennem(A) o√πm(.) est
la mesure de Lebesgue multidimensionnelle.
2.2 Le crit√®re de classification des hypervolumes
La m√©thode de classification des Hypervolumes (Hardy et Rasson (1982), Hardy (1983))
suppose que les n observations p-dimensionnelles x1, ..., xn repr√©sentent un √©chantillon al√©a-
toire simple d‚Äôun processus de Poisson homog√®neN dans un ensembleD inclus dans l‚Äôespace
EuclidienRp (avec 0 < m(D) <‚àû ). L‚ÄôensembleD est l‚Äôunion de k domaines convexes com-
pacts disjoints D1, ..., Dk. Le probl√®me statistique consiste √† estimer les domaines inconnus
Di dans lesquels les points ont √©t√© g√©n√©r√©s. On d√©signe par C i ‚äÇ {x1, .., xn} l‚Äôensemble
des points appartenant √† Di (1 ‚â§ i ‚â§ k). Les estimations du maximum de vraisemblance
des k domaines inconnus D1, ..., Dk sont les k enveloppes convexes H(Ci) des k sous-
groupes de points Ci telles que la somme des mesures de Lebesgue des enveloppes convexes
disjointes H(Ci) est minimale. Le crit√®re de classification des Hypervolumes est donc d√©fini
parWk =
‚àëk
i = 1m(H(Ci)). Le probl√®me de classification des Hypervolumes consiste donc
√† trouver la partition P ‚àó telle que P ‚àó = argminPk‚ààPk
‚àëk
i = 1m(H(Ci)) o√π Pk repr√©sente
l‚Äôensemble de toutes les partitions de C en k classes. Par exemple, dans le plan, la mesure de
Lebesgue d‚Äôun domaineD est l‚Äôaire de ce domaine. Donc si on mesure sur chacun des n objets
la valeur de deux variables quantitatives, la m√©thode de classification des Hypervolumes re-
cherche les k groupesCi, contenant tous les points, tels que la somme des aires des enveloppes
convexes des ensembles Ci est minimale.
2.3 Un test statistique pour le nombre de classes : le Gap test
Gr√¢ce au mod√®le statistique pour la classification bas√© sur le processus de Poisson homo-
g√®ne, on peut d√©finir un test du quotient de vraisemblance pour le nombre de classes (Kubu-
shishi (1996)). On testeH0 : les n = n1+n2 points observ√©s sont la r√©alisation d‚Äôun processus
Kasoro
de Poisson homog√®ne dans D contre l‚Äôalternative H1 : n1 points sont la r√©alisation d‚Äôun pro-
cessus de Poisson homog√®ne dans D1 et n2 points dans D2 o√π D1 ‚à©D2 = ‚àÖ. Les ensembles
D,D1, D2 sont inconnus. La statistique du test est donn√©e par Q(x) =
(
1‚àí m()m(H(C))
)n
o√π
 = H(C)\ (H(C1)‚à™H(C2)) est l‚Äôespace vide (Gap space) entre les classes etm la mesure
de Lebesgue multidimensionnelle. La r√®gle de d√©cision est la suivante (Kubushishi (1996)) :
on rejetteH0, au niveau Œ±, si (distribution asymptotique)
nm()
m(H(C))
‚àí log n‚àí (p‚àí 1) log logn ‚â• ‚àí log(‚àí log(1 ‚àí Œ±)).
3 La m√©thode de classification HOPP
HOPP (Pir√ßon (2004)) est une m√©thode de classification non supervis√©e divisive issue du
mod√®le statistique d√©crit ci-dessus. La premi√®re √©tape consiste √† couper successivement les
noeuds de l‚Äôarbre en deux sous-noeuds, jusqu‚Äô√† ce qu‚Äôun crit√®re d‚Äôarr√™t soit v√©rifi√© (le nombre
de points dans un noeud). A chaque coupure on recherche la bipartition de la classe C en
deux sous-classes C1 et C2, qui minimise le crit√®re de classification des HypervolumesW2 =
m(H(C1)) +m(H(C2)). La m√©thode est monoth√©tique ; on choisit chaque fois le noeud et la
variable tels queW2 est minimal.
A la fin du processus de coupure, on obtient un arbre de grande taille. La deuxi√®me √©tape
permet d‚Äô√©laguer l‚Äôarbre. Afin de v√©rifier si les coupures effectu√©es sont valides, on utilise le
Gap test. On teste donc √† chaque noeud les hypoth√®ses suivantes :H0 : les points sont distribu√©s
dans un seul domaineD contre l‚Äôhypoth√®se alternativeH1 : les points sont distribu√©s dans deux
domainesD1 etD2 (D1‚à©D2 = ‚àÖ). Lorsque l‚Äôhypoth√®se nulle n‚Äôest pas rejet√©e, on conclut que
la coupure est mauvaise. Par contre si l‚Äôhypoth√®se nulle est rejett√©e, on d√©cide que la coupure
est bonne. A la fin du proc√©d√© on utilise la r√®gle suivante : √©laguer toutes les branches qui ne
contiennent que des mauvaises coupures.
Dans certain cas la structure naturelle des donn√©es n‚Äôest pas obtenue √† la fin de l‚Äô√©tape
d‚Äô√©lagage. La troisi√®me √©tape est un outil de recollement. Des tests sont effectu√©s uniquement
sur les classes qui ne sont pas issues du m√™me noeud au niveau pr√©c√©dent. Pour ce faire nous
utilisons √† nouveau le Gap test. Si au moins un regroupement est effectu√© dans l‚Äô√©tape de
recollement, HOPP perd son caract√®re hi√©rarchique. Elle devient alors une m√©thode de parti-
tionnement.
4 La m√©thode de classification symbolique SPART
Dans ce paragraphe nous pr√©sentons l‚Äôextension de la m√©thode HOPP √† des donn√©es inter-
valles (Bock et Diday (2000)). Pour ce faire, on repr√©sente chaque intervalle par son centre et
sa demi-longueur, donc par un point dans un espace bidimensionnel.
Comme dans la m√©thode classique, la premi√®re √©tape consiste √† trouver la meilleure bi-
partition d‚Äôune classe C en deux sous-classes C1 et C2. Comme SPART est une m√©thode
monoth√©tique, nous travaillons dans les p (m, ) espaces, dans lesquels les intervalles de-
viennent des points. On consid√®re donc toutes les bipartitions d‚Äôune classe C en deux classes
{C1, C2}, en respectant l‚Äôordre des centres des intervalles. Il s‚Äôagit donc des bipartitions g√©n√©-
r√©es par des droites verticales (figure 1). On d√©finit une extension √† des donn√©es intervalles de
Une nouvelle m√©thode de classification pour des donn√©es intervalles
la mesure de l‚Äôespace videŒî entre les classes de la fa√ßon suivante :mE() = (mi+1‚àími)+
(max(li, li+1)‚àímin(li, li+1)). On choisit l‚Äôintervalle ]mi,mi+1[ tel quemE(Œî) est maximal.
Une valeur de coupure c est prise arbitrairement dans l‚Äôintervalle ]m i,mi+1[. Habituellement
on choisit le centre de l‚Äôintervalle (figure 1).
‚Ä¢  
 
 
 
 
 
 
0
‚Ä¢‚Ä¢
mi mmi+1
i
i+1

Y (xi+1)
Y (xi)
c
FIG. 1 - Bipartition d‚Äôune classe.
Si x ‚àà C, Y (x) = [Œ±, Œ≤] et m = Œ±+Œ≤2 . Une classe C est divis√©e en deux gr√¢ce
√† une question binaire de la forme "m ‚â§ c?" o√π c est la valeur de coupure. On d√©finit une
fonction binaire qc : C ‚àí‚Üí {0, 1} par qc(x) = 0 si m ‚â§ c et 1 sinon. On obtient alors la
bipartition souhait√©e : C1 = {x ‚àà C : qc(x) = 0} et C2 = {x ‚àà C : qc(x) = 1}.
Les √©tapes d‚Äô√©lagage et de recollement sont effectu√©es de la m√™me fa√ßon que dans le cas
classique. On utilise ici aussi une extension symbolique du Gap test √† des donn√©es intervalles
en repr√©sentant chaque intervalle par son centre et sa demi-longueur.
5 Application
On applique la m√©thode SPART √† un jeu de donn√©es r√©elles. Nous comparerons les r√©-
sultats donn√©s par SPART avec ceux obtenus par deux autres m√©thodes de classification non
supervis√©es monoth√©tiques divisives pour des variables intervalles : SCLASS (Rasson et al.
(2007)) est une m√©thode de classification hi√©rarchique monoth√©tique divisive bas√©e sur une
extension √† des variables intervalles du crit√®re de classification g√©n√©ralis√© des Hypervolumes.
DIV (Chavent (1998)) est quant √† elle unem√©thode de classification hi√©rarchiquemonoth√©tique
divisive bas√©e sur une extension du crit√®re de l‚Äôinertie intra-classe.
Le jeu de donn√©es "cars" est constitu√© de 33 voitures disponibles en 2001, sur lesquelles
ont √©t√© mesur√©es 8 variables intervalles. Il est r√©pertori√© dans les bases du logiciel SODAS 2
(SODAS2 (2004)). Les objets sont repris sur la figure 2. Les variables sont les suivantes : prix,
empattement, cylindr√©e, longueur, vitesse maximale, largeur, acc√©l√©ration maximale, hauteur.
Une partition en 4 classes est obtenue apr√®s l‚Äô√©tape d‚Äô√©lagage. L‚Äô√©tape de recollement ne mo-
difie pas cette partition en 4 classes.
Kasoro
  
 	

 
 
 
  

 
 
   !
" 
#$ %& "'
#$ %& "(
#$ %& "((
#
 #)
*+ & )
  

 ,
  
  -
 .
   
-+ 	
/ 



0'//.
. 
# 

1 ,
%! 2
 3
 
	2
( 
#
 #(
#
 # 
*+ & 
*+ & .
  4
  
") 
2


".()(
)) 
5
 /)
FIG. 2 - Arbre pour l‚Äôensemble de donn√©es "Cars"
La premi√®re variable de coupure est le prix des voitures (cher - bon march√©). Pour les
voitures bon march√©, la deuxi√®me variable de coupure est la longueur de la voiture, tandis que
pour les voitures ch√®res, il s‚Äôagit de la hauteur de la voiture.
La figure 2 montre l‚Äôarbre hi√©rarchique produit par SPART. Les 4 classes peuvent √™tre
√©tiquett√©es de la mani√®re suivante : classe 1 : voitures citadines, classe 2 : voitures berline,
classe 3 : mod√®les sport et classe 4 : voitures limousines.
La m√©thode DIV donne la m√™me partition en 4 classes. SCLASS produit une autre parti-
tion dont les classes ne semblent pas correspondre √† une structure utile de l‚Äôensemble des 33
voitures.
6 Conclusion
SPART est une nouvelle m√©thode de classification non supervis√©e pour des donn√©es in-
tervalles. Elle est bas√©e sur une extension aux donn√©es intervalles du crit√®re de classification
des Hypervolumes et du Gap test. L‚Äôoriginalit√© de l‚Äôapproche est double. D‚Äôune part le mod√®le
sous-jacent √† la m√©thode n‚Äôutilise pas de mesure de dissimilarit√© ; le crit√®re de classification
est d√©duit d‚Äôun mod√®le statistique bas√© sur le processus de Poisson homog√®ne. D‚Äôautre part la
m√©thode inclut une √©tape de recollement qui lui permet, dans le cas de structures particuli√®res,
de retrouver les classes naturelles d‚Äôun ensemble de donn√©es multidimensionnelles.
SPART et DIV donnent souvent des r√©sultats semblables. SPART produit cependant des
r√©sultats meilleurs que DIV lorsqu‚Äôon est en pr√©sence de classes allong√©es. Ceci s‚Äôexplique
principalement par le fait que DIV utilise une extension √† des donn√©es intervalles du crit√®re de
la variance intra-classe, et que ce crit√®re est biais√© par rapport aux classes de forme ellipso√Ø-
dale. SCLASS (Rasson et al. (2007)) utilise un mod√®le statistique pour la classification bas√©
sur le processus de Poisson non homog√®ne. Le crit√®re √† minimiser est l‚Äôintensit√© int√©gr√©e du
Une nouvelle m√©thode de classification pour des donn√©es intervalles
processus de Poisson sur les enveloppes convexes des classes. Cette m√©thode exige donc l‚Äôesti-
mation de l‚Äôintensit√© du processus de Poisson non homog√®ne. Elle est donc plus complexe d‚Äôun
point de vue temps calcul. De plus dans sa version actuelle SCLASS ne comporte pas d‚Äô√©tape
d‚Äô√©lagage ni d‚Äô√©tape de recollement. Les r√©sultats obtenus par SCLASS sont donc g√©n√©rale-
ment qualitativement moins bons que ceux produits par SPART. Enfin la proc√©dure SPART
d√©termine automatiquement le nombre de classes, qui doit √™tre fix√© au pr√©alable dans SCLASS
et DIV.
R√©f√©rences
Bock, H. et E. Diday (2000). Analysis of Symbolic Data - Exploratory Methods for Extracting
Statistical Information from Complex Data. Berlin - Heidelberg: Springer-Verlag.
Chavent, M. (1998). A monothetic clustering method. Pattern Recognition Letters 19, 989‚Äì
996.
Cox, D. et V. Isham (1980). Point Processes. London: Chapman and Hall.
Hardy, A. (1983). Statistique et classification automatique : un mod√®le, un nouveau crit√®re,
des algorithmes, des applications. Th√®se de doctorat, FUNDP - Universit√© de Namur.
Hardy, A. et J.-P. Rasson (1982). Une nouvelle approche des probl√®mes de classification auto-
matique. Statistique et Analyse des Donn√©es 7 (2), 41‚Äì56.
Kubushishi, T. (1996). On some Applications of Point Process Theory in Cluster Analysis and
Pattern Recognition. Th√®se de doctorat, FUNDP - Universit√© de Namur.
Pir√ßon, J.-Y. (2004). La classification et les processus de Poisson pour de nouvelles m√©thodes
monoth√©tiques de partitionnement. Th√®se de doctorat, FUNDP - Universit√© de Namur.
Rasson, J.-P., J.-Y. Pir√ßon, P. Lallemand, et S. Adans (2007). Unsupervised divisive classi-
fication. In E. Diday et M. Noirhomme (Eds.), Symbolic Data Analysis and the Sodas 2
Software. Wiley.
SODAS2 (2004). Logiciel (http://www.info.fundp.ac.be/asso).
Summary
We present a new clustering method for symbolic interval data. It is an extension to interval
data of a classical clustering method. The classical method assumes that the observed data
points are a realisation of a homogeneous Poisson point process in k disjoint domains of R p.
The first part of the new method is a monothetic divisive procedure. The cut rule is based on
an extension to interval data of the Hypervolumes clustering criterion. The pruning step uses a
statistical hypothesis test based on the homogeneous Poisson process. The output is a decision
tree. The second part of the method is a merging process, that allows in particular cases to
improve the classification obtained at the end of the first part of the algorithm. The method is
applied to a real data set.
