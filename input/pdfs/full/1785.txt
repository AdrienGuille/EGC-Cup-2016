Calcul des coefficients de régression et
du PRESS en régression PLS1
Marie Chavent, Brigitte Patouille
Mathématiques Appliquées de Bordeaux (UMR 5466)
Universtité Bordeaux 1,
351 cours de la libération,
33405 Talence cedex
Email : chavent@math.u-bordeaux1.fr, be@sm.u-bordeaux2.fr
1 Introduction
Lors de l’implémentation sous Splus de l’algorithme de régression PLS1 tel
qu’il est présenté dans [Tenenhaus, 1998], nous avons utilisé une formule de
récurrence simple pour le calcul des coefficients de régression et précisé cer-
tains choix nécessaires pour le calcul du PRESS. Nous allons rappeler dans un
premier temps l’algorithme PLS1 afin d’introduire les notations, puis donner
dans un second temps la formule de récurrence permettant d’implémenter
le calcul des coefficients de régression. Enfin, nous montrerons dans une
dernière partie les différentes alternatives possibles pour le calcul du PRESS,
et la solution adoptée. Cette solution permet en effet de retrouver certains
résultats numériques présentés dans [Tenenhaus, 1998] et obtenus avec le
logiciel SIMCA-P. Les fonctions Splus suivantes sont disponibles auprès des
auteurs : PLS1(X, y,H) pour le modèle à H composantes et PLS1cv(X, y)
pour le modèle avec choix du nombre de composantes par validation croisée.
1
2 L’algorithme PLS1
La section 2.1 donne le principe de l’algorithme PLS1 sans données man-
quantes et permet d’introduire les écritures matricielles utilisées pour implé-
menter l’algorithme de la section 2.2.
2.1 Le principe de l’algorithme et passage à l’écriture
matricielle
On considère que les vecteurs xj des variables explicatives et que le vecteur
y de la variable à expliquer sont centrés. Évidemment, les variables peuvent
être centrées-réduites, mais seul le centrage des variables est utilisé lors du
passage à l’écriture matricielle. On note X la matrice individus × variables
de dimensions n × p. On va donc chercher le vecteur ah = (ah1, ..., ahp) des
coefficients de régressions du modèle à h composantes.
Étape 1 : On construit la première composante t1 comme une combinai-
son linéaire des p variables explicatives xj. Les coefficients w
′
1 = (w11, ..., w1j, ..., w1p)
de cette combinaison linéaire cherchent à “résumer” au mieux les variables
explicatives xj et à “expliquer” au mieux la variable y :
t1 = w11x1 + ....+ w1pxp
w1j =
cov(xj ,y)√∑p
j=1
cov2(xj ,y)
On effectue ensuite une régression simple de y sur t1 :
y = c1t1 + y1
où y1 est le vecteur des résidus et c1 est le coefficient de régression :
c1 =
cov(y, t1)
σ2t1
On en déduit une première équation de régression :
y =
a11︷ ︸︸ ︷
c1w11 x1 + ...+
a1p︷ ︸︸ ︷
c1w1p xp + y1
Passage à l’écriture matricielle sachant que les vecteurs xj et y sont centrés
:
cov(xj, y) =
∑n
i=1 xijyi = x
′
jy
cov(y, t1) =
∑n
i=1 yit1i = y
′
t1
σ2t1 =
∑n
i=1 t1it1i = t
′
1t1
c© Revue MODULAD, 2003 -2- Numéro 30
d’où :
w1 =
X
′
y
‖X′y‖
t1 = Xw1
c1 =
y
′
t1
t
′
1t1
a1 = c1w1
Étape 2 : On construit une deuxième composante t2, non corrélée à t1
et expliquant bien le résidu y1. Cette composante t2 est combinaison linéaire
des résidus x1j des régressions simples des variables xj sur t1 :
t2 = w21x11 + ...+ w2jx1j + w2px1p
w2j =
cov(x1j ,y1)√∑p
j=1
cov2(x1j ,y1)
Pour calculer les résidus x1j, on réalise une régression linéaire de toutes
les variables xj sur t1 :
xj = p1jt1 + x1j
où x1j est le vecteur de résidus et p1j est le coefficient de régression :
p1j =
cov(xj, t1)
σ2t1
d’où
x1j = xj − p1jt1
On effectue ensuite une régression de y sur t1 et t2:
y = c1t1 + c2t2 + y2
où c1 est le coefficient de régression de la première étape, c2 est le coeffi-
cient de la régression simple de y1 sur t2 et y2 le vecteur des résidus de cette
régression :
y1 = c2t2 + y2
d’où
c2 =
cov(y1, t2)
σ2t2
Nous verrons section 2.3 comment calculer le vecteur a2 des coefficients
de l’équation de régression :
y = a21x1 + ...+ a2pxp + y2
c© Revue MODULAD, 2003 -3- Numéro 30
Passage à l’écriture matricielle sachant que les x1j et y1 sont centrés et
en notant X1 = (x11, ..., x1p) la matrice des résidus x1j :
p1 =
X
′
t1
t
′
1t1
X1 = X − t1p′1
y1 = y − c1t1
w2 =
X
′
1y1
‖X′1y1‖
t2 = X1w2
c2 =
y
′
1t2
t
′
2t2
Étapes suivantes : Cette procédure itérative peut se poursuivre en
utilisant de la même manière les résidus y2 et x21,..., x2p. Le nombre de
composantes t1,...,tH à retenir est habituellement déterminé par validation
croisée. Cette procédure sera présentée section 3.
2.2 L’algorithme
Cette version de l’algorithme PLS1 ne traite pas les données manquantes. Il
s’agit donc d’une version simplifiée de celui p. 99 dans [Tenenhaus, 1998].
Étape 1 : X0 = X et y0 = y
Étape 2 : Pour h = 1, ..., H :
Étape 2.1 : Calcul du vecteur wh = (wh1, ..., whj, ..., whp)
wh =
X
′
h−1yh−1
‖X ′h−1yh−1‖
(1)
Étape 2.2 : Calcul de la composante th
th = Xh−1wh (2)
Étape 2.3 : Calcul du coefficient de régression ch de yh−1 sur th
ch =
y
′
h−1th
t
′
hth
(3)
Étape 2.4 : Calcul du vecteur yh des résidus de la régression de
yh−1 sur th
yh = yh−1 − chth
c© Revue MODULAD, 2003 -4- Numéro 30
Étape 2.5 : Calcul du vecteurs ph des coefficients des régressions
de xhj sur th
ph =
X
′
h−1th
t
′
hth
Étape 2.6 : Calcul de la matrice Xh des vecteurs des résidus des
régressions de xhj sur th
Xh = Xh−1 − thp′h
2.3 Calcul des coefficients de régression
L’algorithme tel qu’il est présenté section 2.2 ne calcule pas de manière ex-
plicite les coefficients de régression ah du modèle à h composantes :
y = ah1x1 + ...+ ahpxp + yh = Xah + yh
On montre que :
ah = c1w
∗
1 + ...+ chw
∗
h (4)
où C ′h = (c1, ..., ch) est le vecteur des coefficients des régressions linéaires
sur les h composantes, et W ∗h = (w∗1, ..., w∗h) est la matrice des h vecteurs w∗h
vérifiant :
th = w
∗
h1x1 + ...+ w
∗
hpxp = Xw
∗
h (5)
On montre que le vecteur w∗h est défini par la formule de récurrence suiv-
ante :
w∗h = wh −
h−1∑
k=1
w∗k(p
′
kwh) (6)
Il suffit alors de rajouter à l’étape 2 de l’algorithme donné section 2.2 les
étapes 2.7 et 2.8 suivantes :
Étape 2.7 : w∗h = wh
Pour k = 1, ..., h
w∗h = w
∗
h − w∗k(p′kwh)
Étape 2.8 : ah = W ∗hCh
Pour retrouver la formule (4) à partir de (5) :
On sait que
y = c1t1 + ...+ chth + yh
th = Xw
∗
h
c© Revue MODULAD, 2003 -5- Numéro 30
d’où
y = c1Xw
∗
1 + ...+ chXw
∗
h + yh
= X(c1w
∗
1 + ...+ chw
∗
h︸ ︷︷ ︸
ah
) + yh
Pour retrouver la formule de récurrence (6) :
• Le calcul de w∗1 est immédiat car t1 = X
w∗1︷︸︸︷
w1
• Calcul de w∗2 : On cherche w∗2 tel que t2 = Xw∗2. On a :
t2 = X1w2
= (X − t1p′1)w2
= (X −Xw1p′1)w2
= X(w2 − w1p′1w2)︸ ︷︷ ︸
w∗2
• Calcul de w∗3 : de la même manière on a :
t3 = X2w3
= (X1 − t2p′2)w3
= (X − t1p′1 − t2p′2)w3
= X (w3 − w1(p′1w3)− (w∗2)(p
′
2w3))︸ ︷︷ ︸
w∗3
• D’où en généralisant on retrouve (6) :
w∗h = wh −
w∗1︷︸︸︷
w1 p
′
1wh − w∗2p′2wh − ...− w∗h−1p′h−1wh
= wh −∑h−1k=1 w∗kp′kwh
3 Choix du nombre de composantes par vali-
dation croisée
Dans cette section, nous allons présenter en détails comment calculer les
critères PRESS, RSS et Q2h afin de retrouver les résultats numériques
obtenus avec le logiciel SIMCA-P sur l’exemple des données de Cornell [Tenen-
haus, 1998].
c© Revue MODULAD, 2003 -6- Numéro 30
3.1 Le principe
La procédure de validation croisée pour le choix du nombre de composantes
[pp.77 et 83, Tenenhaus, 1998] est la suivante. A chaque étape h et donc
pour chaque nouvelle composante th on calcule le critère :
Q2h = 1−
PRESSh
RSSh−1
Pour h = 1, on a RSS0 =
∑n
i=1(yi − y¯i)2 = n − 1 lorsque la variable est
centrée réduite en utilisant la division par n−1 pour le calcul de la variance.
Une nouvelle composante th est significative et donc conservée si Q2h ≥
0.0975.
Il faut donc définir les critères RSSh et PRESSh. Classiquement et
c’est la présentation adoptée dans [Tenenhaus, 1998], on utilise le modèle de
régression de y sur les h composantes suivant
y =
yˆh︷ ︸︸ ︷
c1t1 + ...+ chth+yh (7)
pour calculer la prédiction yˆh = y− yh. En fait, pour chaque observation
i :
• on calcule la prédictions yˆhi de yi à l’aide du modèle (7) obtenu en
utilisant toutes les observations.
• on calcule la prédiction yˆh(−i) de yi à l’aide du modèle (7) obtenu sans
utiliser l’observation i.
Les critères RSSh (Residual Sum of Square) et PRESSh (PRediction
Error Sum Of Squares) sont alors définis par :
RSSh =
n∑
i=1
(yi − yˆhi)2 (8)
et
PRESSh =
n∑
i=1
(yi − yˆh(−i))2 (9)
L’implémentation sous S-plus de ces calculs ne nous permettant pas de
retrouver les valeurs Q2h fournies par SIMCA-P pour les données de Cornell
[pp.83, Tenenhaus, 1998]), nous avons essayé de nombreuses autres méth-
odes de calcul. Finalement, l’idée qui nous a permis de retouver ces résultats
numériques est la suivante. A chaque étape h, c’est-à-dire pour chaque nou-
velle composante th, les critères RSSh et PRESSh sont calculés à partir du
vecteur des résidus y(h−1) de l’étape précédente et non plus à partir de y :
c© Revue MODULAD, 2003 -7- Numéro 30
• Etape 1 : calcul du PRESS et du RSS sur y.
• Etape 2 : calcul du PRESS et du RSS sur y1 vecteur des résidus de
la régression de y sur t1.
• ...
A partir de là, à chaque étape h, trois modèles de régression peuvent être
utilisés pour calculer la prédiction y˜h = y(h−1) − yh selon que l’on considère
la régression du vecteur des résidus yh−1 sur th, sur la matrice des résidus
X(h−1) ou sur la matrice initiale X. En effet, sachant que
th = X(h−1)wh
et que
th = Xw
∗
h
nous avons les trois modèles suivant :
y(h−1) =
y˜h︷︸︸︷
chth+yh (10)
y(h−1) =
y˜h︷ ︸︸ ︷
chX(h−1)wh+yh (11)
y(h−1) =
y˜h︷ ︸︸ ︷
chXw
∗
h+yh (12)
Pour une observation i, la prédiction y˜hi de y(h−1)i est la même avec les
trois modèles obtenus en utilisant toutes les observations. En revanche, la
prédiction y˜h(−i) de y(h−1)i est différente avec les trois modèles obtenus en
retirant l’observation i. En conséquence, le choix du modèle n’intervient pas
dans le calcul du RSS mais uniquement dans le calcul du PRESS. Ces deux
critères sont maintenant définis par :
RSSh =
n∑
i=1
(y(h−1)i − y˜hi)2 (13)
et
PRESSh =
n∑
i=1
(y(h−1)i − y˜h(−i))2 (14)
c© Revue MODULAD, 2003 -8- Numéro 30
3.2 Calcul du RSS
Nous venons de voir qu’il est équivalent de calculer le RSSh à partir de la
formule (13) et de l’un des trois modèles (10), (11) ou (12). De plus, il est
équivalent de calculer RSSh :
• sur y avec avec la formule (8) et le modèle (7) :
y =
yˆh︷ ︸︸ ︷
c1t1 + ...+ chth+yh
d’où
RSSh =
n∑
i=1
(yi − yˆhi)2 =‖ y − yˆh ‖2=‖ yh ‖2
• sur le vecteur des résidus y(h−1) avec la formule (13) et le modèle (10) :
yh−1 = chth︸︷︷︸
y˜h
+yh
d’où :
RSSh =‖ yh−1 − y˜h ‖2=‖ yh−1 − chth ‖2=‖ yh ‖2
Finalement, on calcule :
RSS1 = ‖ y1 ‖2
...
RSSh = ‖ yh ‖2
3.3 Calcul du PRESS
Nous venons de voir qu’il y a trois manières différentes de calculer la prédic-
tion y˜h selon que l’on considère la régression de yh−1 sur th, sur la matrice
des résidus X(h−1) ou sur la matrice initiale X c’est à dire selon que l’on
considère le modèle (10), (11) ou (12). On a donc à chaque étape h et pour
chaque observation i trois manières de calculer la prédiction y˜hi :
y˜hi = chthi
= chX(h−1)iwh (15)
= chXiw
∗
h
où
c© Revue MODULAD, 2003 -9- Numéro 30
• X(h−1)i est la i-ème ligne de la matrice des résidus X(h−1) calculée à
l’étape 2.6 de l’algorithme (à l’étape h− 1), Xi est la i-ème ligne de la
matrice initiale X et thi est la i-ème valeur de la composante th calculée
à l’étape 2.2 de l’algorithme.
• le coefficient ch, les vecteurs wh et w∗h sont calculés respectivement aux
étapes 2.3, 2.1 et 2.7 de l’algorithme.
Il y a donc à chaque étape h et pour chaque observation i, trois manières
non équivalentes cette fois, de calculer la prédiction y˜h(−i). En fait, pour
retrouver les résultats numériques de SIMCA-P sur les données de Cornell
nous avons utilisé l’égalité (15) c’est à dire la régression du vecteur des résidus
yh−1 sur la matrice des résidus Xh. Cette égalité s’écrit alors
y˜h(−i) = ch(−i)X(h−1)iwh(−i) (16)
où
• X(h−1)i est la i-ème ligne de la matrice des résidus X(h−1)
• ch(−i) et wh(−i) sont calculés sur n − 1 individus, l’individu i étant
retiré. Plus précisément, ils sont calculés à partir des formules (3) et
(1) utilisées aux étapes 2.3 et 2.1 de l’algorithme, mais ces formules
sont appliquées à la matrice des résidus X(h−1) privée de la ligne i et
au vecteur des résidus y(h−1) privé de sa i-ème valeur.
Finalement, on note X(h−1)(−i) la matrice des résidus X(h−1) privée de sa
i-ème ligne et y(h−1)(−i) le vecteur yh−1 privé de sa i-ème valeur. Ces notations
sont un peu lourdes, mais le calcul du PRESS est simple et peut s’inclure à
l’algorithme à la suite de l’étape 2.3 :
Étape (2.3)’ : Calcul de PRESSh
• Pour i = 1, ..., n en utilisant les formules (1), (2) et (3) on calcule :
wh(−i) =
X
′
(h−1)(−i)y(h−1)(−i)
‖X′
(h−1)(−i)y(h−1)(−i)‖
th(−i) = X(h−1)(−i)wh(−i)
ch(−i) =
y
′
(h−1)(−i)th(−i)
t
′
h(−i)th(−i)
y˜h(−i) = ch(−i)X(h−1)iwh(−i)
• Puis on calcule :
PRESSh =
n∑
i=1
(y(h−1)i − y˜h(−i))2
où y(h−1)i est la i-ème valeur du vecteur des résidus y(h−1)
c© Revue MODULAD, 2003 -10- Numéro 30
4 Référence bibliographique
TENENHAUS, M. (1998). la régression PLS, Théorie et Pratique. Editions
Technip.
c© Revue MODULAD, 2003 -11- Numéro 30
