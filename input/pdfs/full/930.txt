© Revue MODULAD, 2008 - 194 - Numéro 38 
L’ANALYSE DES CORRESPONDANCES MULTIPLES « À LA  
HOLLANDAISE » : INTRODUCTION A L’ANALYSE 
D’HOMOGENEITE 
 
Dominique Desbois 1
INRA-SAE2, UMR AgroParisTech Economie publique- Bureau du RICA, Service Central des Enquêtes et Etudes Statistiques, 
12, rue Henri ROL-TANGUY, TSA 70007, 93555 MONTREUIL SOUS BOIS CEDEX. 
Courriel :dominique.desbois@agriculture.gouv.fr - Fax : +33 1 49 55 85 00 
 
 
 
RESUMÉ : 
L’analyse des correspondances multiples est une méthode exploratoire multidimensionnelle qui fournit 
une représentation synthétique des catégories issues d’une batterie de critères qualitatifs, référentiel 
d’un protocole d’expérimentation ou d’enquête. Cette note a pour but d'aider les utilisateurs de SPSS 
dans la mise en oeuvre de l’analyse des correspondances multiples au moyen de l’analyse 
d’homogénéité (procédure HOMALS du logiciel SPSS ). Cette mise en oeuvre concerne l'analyse de 
tableaux de données construits à partir de variables nominales. L’équivalence entre l’analyse 
d’homogénéité et l’analyse des correspondances multiples est illustrée à partir d’un exemple répertorié 
dans la littérature statistique. La note est complétée par un exposé algébrique consacré à l’analyse 
d’homogénéité. 
MOT CLEFS : Analyse des correspondances multiples, analyse d’homogénéité, logiciel statistique 
SPSS, mise en oeuvre. 
 
MULTIPLE CORRESPONDENCE ANALYSIS “À LA HOLLANDAISE”: INTRODUCTION 
TO THE ANALYSIS OF HOMOGENEITY 
ABSTRACT : 
The multiple correspondence analysis is a multidimensional exploratory method which provides a 
synthetic representation of the categories issued from a battery of qualitative criteria, belonging to a 
reference frame of an experimentation protocol or an investigation survey. The aim of this note is to 
help the SPSS users in the implementation of the multiple correspondence analysis by means of the 
homogeneity analysis (procedure HOMALS in the SPSS software). Equivalence between the analysis 
of homogeneity and the multiple correspondence analysis is illustrated on the basis of an example 
excerpted from the statistical literature. The note is supplemented by an algebraic addendum devoted 
to the homogeneity analysis.  
KEY WORDS: 
Multiple correspondence analysis, homogeneity analysis, software statistical SPSS, implementation. 
 
 
HOMALS2 [Gifi, 1990] est une procédure itérative basée sur la technique des moindres carrés 
alternées permettant de réaliser une analyse d’homogénéité. L’une des options particulières de 
cette procédure fournit les facteurs d’une analyse des correspondances multiples. L’objectif 
de cette note est donc de présenter  l’analyse d’homogénéité pour les utilisateurs 
francophones de SPSS afin qu’ils puissent utiliser plus aisément cette procédure pour 
dépouiller leurs données d’enquête de façon pertinente, en réalisant des analyses de 
correspondances multiples. 
 
 
                                                          
1  L’auteur remercie Gilbert Saporta pour ses conseils de lecture et ses remarques critiques mais reste le seul responsable 
des éventuelles omissions ou erreurs. 
2 Homogeneity Analysis by Alternating Least Squares– Analyse d’homogénéité par les moindres carrés alternés. 
© Revue MODULAD, 2008 - 195 - Numéro 38 
1. L’ANALYSE D’HOMOGENEITE, POUR UNE REPRESENTATION OPTIMALE DES CATEGORIES. 
Soit un ensemble d’observations décrivant des objets au moyen de catégories issues d’une 
batterie de critères qualitatifs (variables catégorielles). L’analyse d’homogénéité est une 
technique exploratoire d’analyse des données permettant de décrire les relations existant entre 
deux ou plusieurs de ces variables catégorielles en fournissant une représentation graphique 
de leurs catégories, sous la forme d’un nuage de points (points-catégories) projetés dans un 
sous-espace de faible dimension. 
Cette représentation graphique, effectuée dans un système d’axes orthonormés appelés 
« dimensions » est optimale au sens où elle maximise l’écart entre les positions des 
différentes catégories. Dans ce sous-espace particulier, on peut également représenter les 
objets soumis à l’observation (points-objets) en liant leur représentation à celle des catégories 
de référence de l’étude. Pour chaque variable, les catégories d’une même variable scindent le 
nuage des points représentant les objets en sous-nuages de points qui rassemblent les objets 
partageant la même catégorie. Les points représentant les catégories sont situés au centre du 
sous-nuage des points représentant les objets qui appartiennent à la même catégorie. Les 
proximités entres objets reflètent les similarités ou les dissimilarités entre leurs configurations 
respectives de réponse à la batterie de critères qualitatifs. Ainsi, les objets partageant un 
même profil de réponse sont projetés en un même point. Cependant, la réciproque n’est pas 
forcément vérifiée : deux objets dont les scores (valeurs de la projection selon les dimensions) 
sont proches ne sont pas nécessairement similaires. Si une variable possède un bon pouvoir 
discriminant, les objets se situeront à proximité des catégories auxquelles ils appartiennent. 
Idéalement, les objets classés dans la même catégorie doivent se situer à proximité les uns des 
autres, leurs scores étant similaires. Les catégories appartenant à des variables différentes sont 
situées à proximité les unes des autres si elles caractérisent les mêmes sous-ensembles 
d’objets. Ainsi, deux objets ayant des scores similaires pour un critère particulier doivent 
posséder des scores similaires pour les variables qui lui sont homogènes. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
Figure 1 : visualisation des objets, face et profil du petit matériel de quincaillerie 
(extrait de l’ouvrage [Hartigan 1975]). 
 
Le terme d’homogénéité se réfère donc à une situation où les variables fournissent une 
partition de l’ensemble des objets selon les mêmes catégories ou des catégories similaires. 
Historiquement, le concept d’homogénéité est associé à un paradigme selon lequel des 
variables distinctes peuvent mesurer le même phénomène. Par exemple, pour les 
psychométriciens, les performances intellectuelles sont approchées à travers une batterie de 
tests qualifiés d’homogènes, au sens ou la somme des scores obtenus à un sens car elle fournit 
une mesure de ces performances. 
De façon plus formelle, on peut définir l’analyse d’homogénéité, stricto sensu, comme un 
programme de minimisation d’une fonction-objectif, la perte d’homogénéité (cf. infra § 3 
pour une définition), permettant d’obtenir une représentation graphique des catégories qui 
corresponde à la solution optimale présentée antérieurement. La généralisation de cette 
définition fournit un cadre méthodologique où le terme d’analyse d’homogénéité se réfère à 
une famille de techniques d’analyse multivariée partageant, selon différentes formes de 
codage des données et sous des formulations diverses du critère d’optimalité, un paradigme 
commun d’optimisation de l’homogénéité des variables. 
L’analyse d’homogénéité peut être également présentée comme la solution d’un problème de 
décomposition en valeurs propres et en valeur propres singulières, et peut de ce fait être 
rattachée aux méthodes factorielles : ainsi, pour deux critères qualitatifs, l’analyse 
d’homogénéité est équivalente à l’analyse des correspondances ; pour plusieurs critères, elle 
© Revue MODULAD, 2008 - 196 - Numéro 38 
est équivalente à l’analyse des correspondances multiples. A ce titre, elle peut également être 
présentée comme une méthode de positionnement multidimensionnel travaillant à partir d’un 
tableau de « dissimilarités » constitué par les distances du Khi-Deux entre profils-lignes issus 
d’un tableau disjonctif complet codant, pour la population I des objets, les caractéristiques 
observées selon l’ensemble J des modalités ou catégories d’observation. L’analyse 
d’homogénéité peut également être considérée comme une analyse en composantes 
principales sur données nominales (modèle de Guttman). Lorsqu’il n’y a pas de relations 
linéaires entre variables ou lorsque les variables sont nominales, l’analyse d’homogénéité est 
préférable à une analyse en composantes principales normée (i.e. effectuées sur variables 
centrées et réduites). 
 
 
 
Portrait de Louis GUTTMAN, 1916-1987 
(Materials for the History of Statistics, The University of York) 
 
© Revue MODULAD, 2008 - 197 - Numéro 38 
2. UN EXEMPLE D’ANALYSE D’HOMOGENEITE : les petits articles de quincaillerie. 
 
Ce premier exemple illustratif de l’analyse d’homogénéité est basé sur des données décrivant 
de petits articles de quincailleries (clous, vis, boulons, etc.) à l’aide de variables catégorielles 
[Hartigan, 1975] décrivant leur forme et leur dimension. Il y a n=24 objets ou observations et 
p=6 variables descriptives catégorielles, la variable OBJECT identifiant les 24 observations. 
 Nom Valeur Etiquette Position 
OBJECT Objet 1 
THREAD Pointe 2 
N non 
Y oui 
HEAD Forme de la tête 3 
F plate 
O conique 
R ronde 
U coupe 
Y cylindre 
INDHEAD Indentation de la tête 4 
L fente 
N aucune 
T étoile 
BOTTOM Forme de la base 5 
F p
 
late 
S tranchante 
LENGTH Longueur en demi-pouces 6 
1 0,5" 
2 1" 
3 1,5" 
4 2" 
5 2,5" 
BRASS Cuivré 7 
N non 
Y oui 
Tableau 1 : descriptif des données et détail des catégories 
 
Ci-dessous figure, dans l’éditeur de données SPSS, le tableau de ces données descriptives sous 
forme alphanumérique : 
Figure 2 : le tableau des données alphanumériques 
© Revue MODULAD, 2008 - 198 - Numéro 38 
2.1. Pouvoir explicatif des dimensions de la solution 
La représentation graphique que l’on souhaite obtenir de ces données en termes de catégories 
et d’objets, s’effectue dans un repère orthonormé dont on doit préciser le nombre d’axes a , 
 appelé la dimension de la solution. La dimension maximum du sous-espace de 
représentation est égale soit au nombre de catégories (m=19)  moins le nombre de variables 
sans valeurs manquantes (p=6), soit au nombre d’observations (n=24) moins un si celui-ci est 
inférieur, soit a=min{13,23}=13. En pratique, le nombre d’axes utilisé pour la représentation 
est généralement très inférieur à ce maximum car souvent une solution comportant deux ou 
trois dimensions suffit pour synthétiser les traits essentiels de l’information contenue dans le 
tableau des données, l’information additionnelle apportée par des dimensions supplémentaires 
se révélant marginale. 
Les valeurs propres permettent de rendre compte de l’importance relative de chaque 
dimension dans la part d’information statistique pris en compte par la solution. Ces valeurs 
propres prennent des valeurs dans l’intervalle [ ]1;0
                                                          
. La valeur 1 est atteinte par la valeur 
propre triviale qui correspond au vecteur propre reliant le centre de gravité du nuages des 
profils catégoriels et l’origine du repère. Les valeurs propres nulles correspondent à des 
directions indéterminées de la solution3. 
Eigenvalues
Dimension Eigenvalue
© Revue MODULAD, 2008 - 199 - Numéro 38 
Tableau 2 : les deux premières valeurs propres. 
 
Leur rapport avec la somme totale des valeurs propres, appelé le taux d’inertie en analyse 
des correspondances, constitue une mesure pessimiste de la part de variabilité globale prise en 
compte. La procédure HOMALS de SPSS étant limitée à 10 dimensions, le calcul est effectué 
dans ce sous-espace. Néanmoins, les valeurs propres d’ordre supérieur ayant une valeur 
résiduelle, cette approximation ne change pas fondamentalement l’estimation des taux 
d’inertie. 
 
Tableau 3 : taux d’inertie associés au valeurs propres. 
 
 
3  tout vecteur est solution de l’équation aux valeurs propres, donc vecteur propre. 
,621
,368
1
2
Dimension Valeur propre Taux d'inertie Inertie cumulée
1 0,621 0,287 0,287
2 0,368 0,170 0,457
3 0,328 0,151 0,608
4 0,279 0,129 0,737
5 0,197 0,091 0,828
6 0,128 0,059 0,887
7 0,086 0,040 0,927
8 0,084 0,039 0,966
9 0,056 0,026 0,991
10 0,019 0,009 1,000
Ainsi, les deux dimensions retenues permettent de prendre en compte 46% de l’inertie totale à 
travers une représentation graphique plane interprétable en termes de distances entre 
observations. 
 
2.2. Représentation graphique des objets à partir des scores 
Les scores (coordonnées des objets selon les premières dimensions de la solution) permettent 
de repérer les valeurs extrêmes (« outlier ») : l’objet projeté à l’extrémité négative de la 
dimension 2 (D2<0) peut être considéré comme une valeur atypique ou aberrante et, de ce 
fait, éventuellement exclu lors d’une analyse ultérieure (cf. infra). 
 
 
© Revue MODULAD, 2008 - 200 - Numéro 38 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Object Scores
Cases weighted by number of objects.
Dimension 1
2,01,00,0-1,0-2,0
D
im
en
si
on
 2
1
0
-1
-2
-3
-4
-5
« outlier » 
Figure 3 : projection des objets dans le plan des deux premières dimensions. 
 
Cette représentation des objets sous forme de tournesol (le nombre de pétales du tournesol est 
proportionnel au nombre d’objets) est bien adaptée aux ensembles d’objets dont la cardinalité 
n est importante car elle permet de rendre compte des différences de densité au sein du nuage 
des points-objets. 
Si le nombre d’observations est suffisamment faible, il est alors possible de projeter chacune 
des observations avec leur identifiant. Cela permet de vérifier la configuration de réponses 
fournies par des sous-ensembles particuliers d’objets. Ce graphique permet de constater que la 
première dimension (axe horizontal D1) sépare les vis (screw) et les boulons (bolt), qui ont 
un filetage (thread), des clous (nail) et des punaises (tack) qui n’en ont pas. De façon moins 
prononcée, cette première dimension instaure une séparation entre les boulons (bolt) qui ont 
une base plate et tous les autres objets (qui ont une base pointue). La seconde dimension (axe 
vertical D2) sépare les objets screw1 et nail6 de l’ensemble des autres objets : ces deux objets 
sont les plus longs (cf. figure 2). Notons également que screw1 apparaît comme l’objet le plus 
éloigné de l’origine : la configuration des caractéristiques de cet objet apparaît comme très 
spécifique puisqu’elle n’est partagée par aucun autre objet. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
© Revue MODULAD, 2008 - 201 - Numéro 38 
 
 
 
 
Dimension 1
1,51,0,50,0-,5-1,0-1,5
D
im
en
si
on
 2
1
0
-1
-2
-3
-4
-5
screw1  nail6 
  nail7 
 nail8 
  nail4 
 nail5 
   nail3 
  nail2 
  nailb 
  tack1    tack2   nail1   tack   screw4   screw5   screw3 
  screw2 
   bolt1 
   bolt2 
  bolt3 
  bolt4 
 bolt5 
 bolt6 
 screwb 
 
Figure 4 : étiquetage des objets dans le plan des deux premières dimensions. 
 
Cependant, la pratique des variables illustratives (cf. infra § 2.5) dans l’établissement des 
graphiques facilite la synthèse de ces informations : pour chacun de ces graphiques illustratifs, 
les objets sont étiquetés à partir de la palette de valeurs catégorielles issue de la variable 
illustrative sélectionnée. 
La procédure HOMALS permet de spécifier les variables illustratives utilisées pour produire 
une représentation graphique de la densité des différentes modalités de réponse. 
© Revue MODULAD, 2008 - 202 - Numéro 38 
2.3. Mesures du pouvoir discriminant 
La mesure du pouvoir discriminant d’un  critère relativement à une dimension peut se définir 
comme le pourcentage de variance de la dimension expliqué par ce critère. La valeur 
maximum de cet indicateur est égale à 1 si tous les objets se répartissent sur l’ensemble de ces 
catégories (caractère complet de la nomenclature des catégories) et si les objets appartenant à 
la même catégorie se révèlent identiques en termes de configuration descriptive relativement 
aux autres critères. S’il y a des données manquantes dans le tableau analysé, l’indice du 
pouvoir discriminant du critère peut être supérieur à 1. 
Cette mesure du pouvoir discriminant étant calculée comme la moyenne pondérée, par la 
fréquence des catégories, des carrés des coordonnées des catégories (quantifications). Dans le 
langage de l’analyse des correspondances, il s’agit de la moyenne pondérée des qualités de 
représentation des modalités de cette variable sur l’axe factoriel. Le pouvoir discriminant d’un 
critère est d’autant plus élevée= que ses catégories présentent une dispersion importante de 
leurs coordonnées selon la dimension examinée. La moyenne des indices de discrimination 
sur l’ensemble des critères est égale pour chaque dimension à la valeur propre 
correspondante, exprimant ainsi la variance de cette dimension. 
Les dimensions sont ordonnées dans l’ordre décroissant de leur variance , les valeurs propres 
étant extraites par ordre d’importance décroissant : la direction de la première dimension 
correspond au vecteur propre associé à la première valeur propre (la plus élevée) ; la direction 
de la seconde dimension correspond au second vecteur propre associé à la seconde valeur 
propre en importance ; etc. 
Le diagramme des mesures du pouvoir discriminant indique que la première dimension est 
constituée par une synthèse des variables thread (présence d’une pointe) et bottom (forme de 
la base) : les deux variables présentent des niveaux d’indice de discrimination importants pour 
la 1ère dimension et faibles pour la 2nde dimension. Ainsi, les catégories de ces variables sont 
bien dispersées selon l’axe D1 et peu dispersées selon l’axe D2. 
Inversement, la variable length présente une valeur élevée de l’indice de discrimination selon 
l’axe D2 et une valeur faible pour l’axe D1. En conséquence, l’angle entre le vecteur 
correspondant à cette variable et la 2nde dimension est faible, la valeur de l’indice selon l’axe 
D2 correspondant au carré du cosinus de l’angle. Cet indice, assimilable au carré d’un 
coefficient de corrélation (R2), exprime la similarité entre les deux directions, et reflète la 
ségrégation observée selon la 2nde dimension sur le diagramme des objets entre les objets les 
plus longs (situés dans le demi-plan D2<0) et l’ensemble des autres objets (situés dans le 
demi-plan D2>0). 
Remarquons également que les variables concernant la forme et l’indentation de la tête 
présentent des valeurs importantes de leurs indices de discrimination selon les deux 
dimensions. 
Par contre la variable brass située près de l’origine du graphique n’apparaît pas comme 
discriminante dans ce plan des deux premières dimensions, l’ensemble des objets pouvant 
posséder ou  non le caractère cuivré. Pour la même raison, la variable length ne peut être liée 
à la 1ère dimension puisqu’elle ne discrimine les objets que dans la 2nde dimension. 
 
 
 
 
 
 
 
 
 
 Discrimination Measures
Dimension 1
1,0,8,6,4,20,0
D
im
en
si
on
 2
1,0
,8
,6
,4
,2
0,0
Length in ha
BRASSN Bottom shape
Indentation
Head form
THREADN
 
Figure 5 : mesure du pouvoir discriminant selon les deux premières dimensions. 
 
 
Si l’indice de discrimination indique quelle est la part de variance expliquée par une variable 
pour chaque dimension, il ne permet pas de distinguer entre les variables dont les catégories 
présentent une dispersion moyenne selon une dimension et celles dont la plupart des 
catégories ont des coordonnées similaires à l’exception de certaines d’entre elles très 
différentes. 
 
2.4. Quantifications des catégories 
En revanche, les projections graphiques des catégories permettent de caractériser précisément 
les relations entre catégories d’une même variable mais aussi entre catégories de variables 
distinctes, en situant chaque catégorie sur un même graphique au moyen de leurs 
quantifications selon chaque dimension (équivalent des coordonnées factorielles des profils 
catégoriels dans l’analyse des correspondances multiples). 
Ainsi, la variable length possède cinq catégories dont trois sont localisées dans la partie 
supérieure du graphique (demi-plan D2>0) et les deux autres (soit 1,5’’ et 2,5’’ ) se situent 
dans la partie inférieure du graphique (demi-plan D2<0). 
En outre, la catégorie étiquetée 2_1/2_in (soit 2,5’’) située à l’extrémité négative de la 2nde 
dimension, se singularise très nettement par rapport à l’ensemble des autres catégories, 
rejoignant en cela la catégorie STAR (tête en étoile ou cruciforme) de la variable Indentation 
of head (indentation de la tête). En fait, la catégorie 2_1/2_in est située au point moyen 
© Revue MODULAD, 2008 - 203 - Numéro 38 
(barycentre) des localisations des deux objets qui partagent cette spécificité, soit screw1 et 
nail6. 
La catégorie STAR se situe exactement au lieu géométrique de projection de l’objet screw1 
qui est le seul à présenter cette indentation cruciforme de la tête. Cette catégorie STAR se 
différencie des deux autres catégories (SLIT – fente et NONE – sans indentation) selon la 2nde 
dimension. 
 
Quantifications
Dimension 1
2,01,51,0,50,0-,5-1,0-1,5-2,0
D
im
en
si
on
 2
1
0
-1
-2
-3
-4
-5
Length in half-inche
BRASSN
Bottom shape
Indentation of head
Head form
THREADN
2_1/2_in
1_1/2_in
STAR
NONESLIT
Figure 6 : quantification des catégories. 
 
La dispersion des catégories d’une variable selon une dimension particulière reflète la 
variabilité de la configuration des réponses et constitue un indicateur de son pouvoir 
discriminant relatif à cette dimension. 
Ainsi, selon l’axe horizontal D1, les catégories de la variable THREADN (codage numérique 
de la variable thread) sont très dispersées alors qu’elles ne le sont pas selon l’axe vertical D2. 
Il s’en suit que la variable thread discrimine mieux les objets selon la 1ère dimension que 
selon la 2nde dimension. 
En revanche, les catégories de la forme de la tête (Head form) sont autant dispersées selon 
l’axe D1 que selon l’axe D2. On en conclut que le pouvoir discriminant de cette variable est 
équivalent selon les deux dimensions. 
Une variable dont les catégories sont plus dispersées selon une dimension possède un pouvoir 
discriminant plus important selon cette dimension qu’une autre variable dont les catégories 
sont projetées de façon moins dispersées. Par exemple, selon la 1ère dimension, les deux 
catégories de la variable BRASSN (codage numérique de la variable brass - caractère cuivré) 
© Revue MODULAD, 2008 - 204 - Numéro 38 
sont beaucoup moins dispersées que les deux catégories de la variable THREADN, indiquant 
que la variable thread possède un pouvoir discriminant plus important que celui de brass 
selon cette dimension (vérifiable en figure 5, d’après les niveaux relatifs de la mesure de 
discrimination des deux variables considérées). 
 
2.5. Graphiques illustratifs 
On peut éventuellement pousser plus loin l’analyse en consultant les différents graphiques 
illustratifs projetant individuellement, pour chaque variable, les objets étiquetés par le codage 
des catégories. 
L’utilisation de ces variables illustratives montre que la 1ère dimension sépare parfaitement le 
groupe des articles possédant une pointe, étiquetés Yes_Thread et situés dans le demi-plan 
[ D1<0 ], du groupe de ceux qui n’ont pas de pointe, étiquetés No_Thread et situés dans le 
demi-plan [ D1>0 ]. 
Cette différenciation parfaite en fait un indicateur bien corrélé à la 1ère dimension. 
 
© Revue MODULAD, 2008 - 205 - Numéro 38 
Figure 7 : projection des objets, variable illustrative THREADL (« présence d’une pointe »). 
Object Scores Labeled by THREADL
Cases weighted by number of objects.
Dimension 1
3,02,01,00,0-1,0-2,0-3,0
D
im
en
si
on
 2
1
0
-1
-2
-3
-4
Yes_Thread
No_Thread
Yes_Thread
Yes_Thread
s_ r
Yes_ThreadYes_ThreadYes_Thr adYes_Thread
Yes_Thread
No_Thread
No_Thread
No_ThreadNo_ThreadNo_Thread
La projection des objets étiquetés par la forme de la tête (Head form) montre que celle-ci 
discrimine bien les articles dans les deux dimensions. 
Les objets à tête plate (FLAT) sont situés dans le quadrant supérieur droit [ D2>0 & D1>0 ] 
tandis que les articles dont la tête est en coupe (CUP) sont situés dans le quadrant inférieur 
droit [ D2<0 & D1>0 ]. 
Les objets à tête conique (CONE) sont situés dans le quadrant inférieur gauche [ D2<0 & 
D1<0 ] mais on observe que ces objets sont beaucoup plus dispersés que dans les autres 
catégories. 
Dans le quadrant supérieur gauche [ D2>0 & D1<0 ], les objets à tête cylindrique 
(CYLINDER) ne peuvent être distingués des objets à tête ronde (ROUND). 
 
 
Object Scores Labeled by Head form
1
Cases weighted by number of objects.
Dimension 1
2,01,00,0-1,0-2,0
D
im
en
si
on
 2
0
-1
-2
-3
-4
-5
CONE
FLAT
CYLINDER
CONE
ROUND
CYLINDERROUNDCYLINDERROUND
CONE
CUP
CUP
FLATFLATFLAT
  
Figure 8 : projection des objets, variable illustrative HEADL (« forme de la tête »). 
© Revue MODULAD, 2008 - 206 - Numéro 38 
Le graphique selon les catégories de longueur montre que ces catégories se distinguent non 
pas selon l’axe horizontal du graphique mais plutôt selon l’axe vertical. Ce constat confirme 
l’analyse selon laquelle les catégories de la variable length ne discriminent pas les objets 
selon la 1ère dimension mais seulement selon la 2nde, les objets les plus courts étant situés dans 
le demi-plan [D2>0] 
 
Object Scores Labeled by Length in half-inches
Cases weighted by number of objects.
Dimension 1
2,01,51,0,50,0-,5-1,0-1,5-2,0
D
im
en
si
on
 2
1
0
-1
-2
-3
-4
-5
1/2_in
1/2_in
1/2_in
1/2_in
2_in 1_in1_in2_in2_in
2_1/2_in
1_1/2_in
2_1/2_in
1_in2_in1/2_in
 
Figure 9 : projection des objets, variable illustrative LENGHTL« longueur en pouces » 
 
 
Le graphique illustratif à partir de la variable BRASS (caractère cuivré ou non de 
l’objet) ne permet pas de mettre en évidence une différenciation nette des objets selon l’une 
ou l’autre des deux premières dimensions. 
© Revue MODULAD, 2008 - 207 - Numéro 38 
2.6. Filtrage des observations atypiques 
Une fois identifiées les observations atypiques comportant trop de caractéristiques qui leur 
sont propres, on peut les exclure de l’analyse par filtrage, permettant ainsi de se focaliser sur 
les phénomènes dont l’occurrence n’est pas marginale. Si l’on réitère l’analyse d’homogénéité 
après un traitement excluant cette observation jugée atypique, on constate un léger 
changement au niveau des valeurs propres qui ne modifie pas de manière radicale l’ordre de 
grandeur de leur taux d’inertie. Pour autant, on ne doit pas conclure sans examen préalable à 
la quasi-équivalence des deux analyses 
Le graphique des mesures de discrimination indique désormais que l’indentation de la tête 
(« head indentation ») ne discrimine plus les objets selon la 2nde dimension mais seulement 
selon la 1ère dimension, tandis que le caractère discriminant de la variable brass (cuivré ou 
non) se manifeste désormais selon la 2nde dimension. Les indices de discrimination des autres 
variables demeurent inchangés dans ces deux premières dimensions. 
 
 
 
 
 
 
© Revue MODULAD, 2008 - 208 - Numéro 38 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Discrimination Measures
Dimension 1
1,0,8,6,4,20,0
D
im
en
si
on
 2
1,0
,8
,6
,4
,2
0,0
-,2
head indentation 
thread 
head form 
lenght in half inch 
bottom shape 
brass 
Figure 10 : mesures de discrimination, après filtrage de l’objet atypique. 
Le graphique des objets étiquetés par la variable brass montre que les objets cuivrés 
(« YES_Br ») sont désormais projetés à l’extrémité négative de la 2nde dimension (zone [ -
2<D2<-1 ]) alors que les objets non cuivrés (« Not_Br ») sont projetés dans le demi-plan 
[ D2>-1 ], confirmant ainsi le pouvoir discriminant de la variable brass selon la 2nde 
dimension. 
 
 
 
Object Scores Labeled by BRASSL
Cases weighted by number of objects.
Dimension 1
2,01,51,0,50,0-,5-1,0-1,5-2,0
D
im
en
si
on
 2
2,0
1,5
1,0
,5
0,0
-,5
-1,0
-1,5
-2,0
YES_Br YES_Br
Not_Br
Not_Br
Not_Br
Not_Br
Not_BrNot_Br
Not_Br
Not_Br
Not_Br
Not_Br
Not_Br
 
Figure 11 : projection des objets étiquetés par BRASSL, après filtrage de l’objet atypique 
© Revue MODULAD, 2008 - 209 - Numéro 38 
La projection illustrative des objets étiquetés par les catégories relatives à l’indentation de la 
tête (« Indentation of head ») montre que la première dimension permet de discriminer 
parfaitement les objets non indentés (« NONE ») des objets indentés (« SLIT »), comme dans 
l’analyse précédente. 
Cependant, la 2nde dimension ne discrimine plus les catégories d’indentation, à l’inverse de 
l’analyse précédente. 
 
 
Object Scores Labeled by Indentation of head
Cases weighted by number of objects.
Dimension 1
2,01,51,0,50,0-,5-1,0-1,5-2,0
D
im
en
si
on
 2
2,0
1,5
1,0
,5
0,0
-,5
-1,0
-1,5
-2,0
SLIT NONE
SLIT
SLIT
SLIT
SLIT
SLITSLIT
SLIT
NONE
NONE
NONE
NONE
. 
Figure 12 : projection des objets étiquetés par indentation de la tête (« INDHEADL»),  
après filtrage de l’objet atypique 
 
© Revue MODULAD, 2008 - 210 - Numéro 38 
3. L’ANALYSE D’HOMOGENEITE, POUR UNE REPRESENTATION OPTIMALE DES CATEGORIES. 
 
3.1. Le concept d’homogénéité 
Développée par le groupe Albert Gifi4, la procédure HOMALS se base sur le concept 
d’homogénéité, que l’on peut définir de la manière suivante. 
Soit le vecteur z , contenant les observations faites sur les n individus 
d’une population, correspondant à la variable . 
pjj ,,1, L=
jz x
jZ
Le vecteur  est homogène à , vecteur unitaire (de norme 1), si et seulement si 
après une transformation  de normalisation (tel que jt ( )( ) jjt zx = . ), on a jjt z 1=
Si le vecteur  n’est pas homogène à , on définit la perte d’homogénéité comme 
suit : 
jz
( )
x
( )( )j ( )( )jjp
j
t t
p
t zxzxx −= ∑
=1
2 1,σ
jZ jk j jz
jt− . 
 
3.2. La procédure HOMALS 
Soit la matrice  des indicatrices de codage correspondant aux indicatrices de codage d’une 
variable  qualitative à  modalités. La transformation t  du vecteur  peut être définie 
par 
jZ
( ) jjjjt YZz = jkn où  est une matrice à jY ×  coefficients. 
La procédure HOMALS consiste à minimiser la fonction de perte suivante : 
( ) ( )( )[ ]∑ −= p
j
jjj
ttrace
p 1
2 1, YZXYZXYXσ
nI=XX
=
− j  
sous les contraintes d’orthonormalisation t  et de centrage 0=1X . 
 
3.3. Equivalence avec l’analyse des correspondances multiples 
[Gifi, 1990] présente l’analyse d’homogénéité comme la résolution d’un problème de 
décomposition spectrale, soit en valeurs singulières, soit en valeur propres, qui fournit en fait 
les facteurs d’une analyse des correspondances multiples. Cette présentation est issue du 
travail de [Tenenhaus et Young, 1985] qui établit un cadre conceptuel commun pour analyser 
les relations entre différentes méthodes multivariées d’analyse de données catégorielles, 
montrant ainsi l’équivalence entre analyse des correspondances multiples et analyse 
d’homogénéité. L’analyse d’homogénéité peut également être vue comme une technique de 
positionnement multidimensionnel restituant une image euclidienne (à partir de graphiques-
plans) des « dissimilarités » constituées par les distances du Khi-Deux entre profils-lignes. 
 
4. EFFECTUER UNE ANALYSE D’HOMOGENEITE AVEC SPSS 
 
Pour obtenir une analyse d’homogénéité sous SPSS, il convient de créer par recodage, à partir du 
tableau des données alphanumériques (cf. figure 2), un tableau numérique comportant l’ensemble des 
variables à analyser. Pour ce faire, il faut utiliser la procédure de recodage automatique <Automatic 
Recode> du menu de transformation <Transform>, créant ainsi la variable threadn (codage 
                                                           
4 Albert Gifi fût durant quarante années le maître d’hôtel de Sir Françis Galton [Gilham, 2001] avant de devenir 
le nom collectif des membres du Department of Data Theory de l’Université de Leiden (Pays-Bas). Ce groupe, 
constitué autour de Jan de Leeuw a mis au point un système pour l’analyse multivariée non linéaire qui recouvre 
de multiples techniques factorielles allant de l’analyse en composantes principales à l’analyse canonique. Le 
travail de ce groupe est présenté dans l’ouvrage [Gifi, 1990] 
© Revue MODULAD, 2008 - 211 - Numéro 38 
numérique) à partir de la variable thread (codage alphanumérique) par transformation des catégories 
prises dans un ordre lexicographique croissant (cf. figure 13). 
 
 
Figure 13 : recodage des variables alphanumériques en variables numériques. 
 
Figure 14 : variables numériques recodées. 
 
Dans une seconde étape, il faut créer par recopie autant de variables illustratives qu’il y a de 
critères participant à l’analyse. Pour ce faire, il suffit de sélectionner les variables recodées en 
© Revue MODULAD, 2008 - 212 - Numéro 38 
cliquant  avec la touche « Control » maintenue enfoncée (« Ctrl+Clic ») sur les colonnes 
correspondantes de l’éditeur des données (cf. figure 15). 
 
Figure 15 : sélection multiple par Ctrl+Clic des variables numériques recodées. 
 
Ensuite, il faut sélectionner à partir du menu <Edit>, la commande <Copy> (avec le clavier, 
faire un <Ctrl+C>), pour pouvoir coller (menu <Edit>, commande <Paste>, ou équivalent-
clavier faire un <Ctrl+V>), après avoir effectué une sélection multiple de cinq colonnes 
vides : 
 
Figure 16 : fichier des variables numériques, actives et illustratives. 
 
© Revue MODULAD, 2008 - 213 - Numéro 38 
Pour obtenir une analyse d’homogénéité, il faut sélectionner à partir du menu <Analyse>, la procédure 
<Optimal Scaling> du menu <Data Reduction>, en choisissant les options correspondantes (options 
par défaut de la procédure, soit un seul ensemble de variables avec toutes les variables considérées 
comme nominales) : 
 
Figure 17 : options correspondant  à 
l’analyse d’homogénéité 
 
La première étape de la spécification de la procédure consiste à sélectionner les variables 
actives de l’analyse (threadn, headn, indheadn, bottomn, brassn, lenghtn) en définissant pour 
chacune d’entre-elles le nombre de modalités : 
 
Figure 18 : spécification des variables 
actives. 
© Revue MODULAD, 2008 - 214 - Numéro 38 
Dans la seconde étape, on spécifie les variables illustratives de l’analyse (objectl, threadl, headl,  
brassl, lenghtl) en définissant également pour chacune d’entre-elles le nombre de modalités : 
 
La dernière étape de cette spécification concerne le choix du nombre de dimensions (nombre d’axes 
factoriels) choisies pour la représentation graphique des objets, des modalités et des variables. On 
choisit ici une représentation graphique en deux dimensions comme solution particulière au problème 
d’optimisation sous contraintes que pose l’analyse formulée en terme d’homogénéité (cf. §3). 
Les différentes options de traitement peuvent être choisies en utilisant le bouton <Options…>. Ces 
options portent sur les résultats (Display), les graphiques (Plot), la sauvegarde des coordonnées 
factorielles des objets (<Save object scores>) et les critères de contrôle de l’algorithme (Criteria). 
Figure 20 : choix des options. 
Figure 19 : spécification des variables illustratives 
© Revue MODULAD, 2008 - 215 - Numéro 38 
 
Les résultats demandés (cf. section Display de la figure 20) sont les distributions marginales 
obtenues par comptage (Frequencies), les valeurs propres (Eigenvalues), le pouvoir 
discriminant des variables actives (Discrimination measures), les coordonnées factorielles des 
modalités pour chaque variable (Category quantifications), les coordonnées factorielles des 
objets (Object scores). 
 
Les graphiques demandés (cf. section Plot de la figure 20) sont le graphique factoriel des 
modalités de variables actives (Category  quantifications), celui des objets (Object scores) et 
le diagramme du pouvoir discriminant des variables selon chacune des dimensions 
(Discrimination measures). A ces graphiques s’ajoutent autant de graphiques de densité des 
objets étiquetés par les modalités qu’il y a de variables illustratives. 
 
La sauvegarde des coordonnées factorielles demandée (Save object scores) s’effectue dans le 
fichier d’origine, mais peut être ultérieurement sauvegardé dans un fichier spécifique, comme 
suit, pour de nouvelles analyses (classification sur axes factoriels) : 
 
 
Figure 21 : sauvegarde des coordonnées factorielle des objets dans un fichier spécifique. 
©  Numéro 38  Revue MODULAD, 2008 - 216 -
Les macro-instructions du programme SPSS correspondant aux options précédemment définies 
peuvent être sauvegardées dans un fichier de syntaxe en utilisant le bouton <Paste> de la boîte de 
dialogue : 
Figure 22 : sauvegarde des macro-instructions dans un fichier programme (extension « .SPS »). 
 
Le seuil de convergence (Convergence=.00001) et le nombre maximum d’itérations 
(Maximum interations=100) permettent de contrôler l’algorithme itératif des moindres carrés 
alternés de la procédure HOMALS dans la recherche d’une solution. 
Iteration History
© Revu Numéro 38 e MODULAD, 2008 - 217 - 
Dans cet exemple, l’algorithme s’arrête à l’itération n° 19 car l’amélioration de l’indice 
d’ajustement (Fit) est devenue inférieure à la valeur du seuil de convergence. 
,132757 ,132757
,849876 ,717119
,943649 ,093773
,966800 ,023151
,976822 ,010022
,982110 ,005288
,985104 ,002993
,986838 ,001735
,987851 ,001013
,988444 ,000593
,988793 ,000349
,988999 ,000206
,989122 ,000123
,989196 ,000074
,989241 ,000045
,989269 ,000028
,989287 ,000018
,989298 ,000012
,989306 ,000008
Iteration
Difference
from the
Previous
IterationFit
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19a
The iteration process stopped because
the convergence test value was reached.
a. 
 Tableau 4 : historique des itérations 
5. L’algorithme itératif de la procédure HOMALS de SPSS5
L’algorithme itératif HOMALS (Homogeneity Analysis by Means of Alternating Least Squares – 
Analyse d’Homogénéité par Moindres Carrés Alternés) est la version moderne de la procédure 
proposée initialement par Guttman en 1941 pour l’analyse des données catégorielles. Le traitement des 
valeurs manquantes est basé sur l’introduction de pondérations nulles dans la fonction de perte (cf. De 
Leeuw & Van Rickevorsel, 1980). D’autres options pour le traitement des valeurs manquantes existent 
et sont basées sur le recodage (Gifi 1981, Meulman 1982). 
 
5.1. Notations 
En l’absence d’autre convention explicite, nous utilisons dans l’exposé de cet algorithme les notations 
suivantes : 
n
p
  nombre d’observations (ou objets) 
   nombre de variables (ou critères) 
s    nombre de dimensions (ou facteurs) 
mjj ,,1, L=  Pour chaque critère 
© Revue MODULAD, 2008 - 218 - Numéro 38 
jh 1×
jk
Z jkn ×
  vecteur n  des observations catégorielles  
  nombre de catégories (ou modalités)du critère j 
 matrice  des indicatrices de modalités pour le critère j j
( )j
ikz ⎩ sinon0
critèreducatégorielaàappartientnobservatiol'si1 jki
O n
 élément matriciel de jG = ⎨⎧  
 matrice-filtre n×  des indicatrices d’observations pour le critère j  j [ ]( )j
iio  élément matriciel de jM  = ⎩ sinon0
,1intervallel'àappartientnobservatio jki
D
D ∑∑ ×
jj
jk
X
⎨⎧  l'si1
j  matrice diagonale des poids contenant les effectifs marginaux des modalités du critère 
j 
  matrice diagonale des effectifs marginaux des modalités. jk
 
Les matrices de coordonnées factorielles sont : 
  matrice sn  des coordonnées factorielles des observations (objets) selon les s 
dimensions 
×
Y sk j×
Y pk j ×
                                                          
j   matrice des coordonnées factorielles des modalités du critère j selon les s 
dimensions 
  matrice concaténée∑ des coordonnées factorielles de l’ensemble des 
modalités 
j
 
5 Cette section est une libre traduction du document technique correspondant fourni par SPSS 
5.2. Formulation du programme d’optimisation de la fonction objectif 
L’objectif d’HOMALS est de trouver une matrice  et un ensemble de matrices (pour 
) tel que la fonction objectif : 
X jY
pj ,,1L=
( ) ( ) ( )∑ ⎤⎡ −′−= tr YZXYZXYX 1,σ
snpIXOX
⎥⎦⎢⎣j jjjjp  
′ =soit minimale sous la contrainte de normalisation ⊕
∑=⊕
j
jOO
s
, 
où  est la matrice-objet 
et I  est la ss×  matrice identité. 
L’introduction des matrices-filtres O  permet de contrôler qu’aucune des valeurs observées actives 
pour le critère j ne sorte de l’intervalle 
j [ ]jk,1 ⊕O
)1=jii
)0=jii
0=
. La matrice-objet  définit ainsi pour chaque objet i 
l’ensemble des observations actives de l’analyse (o et l’ensemble des observations 
supplémentaires (o . 
′Les coordonnées factorielles de chaque objet sont centrées, ce qui peut s’écrire :  ⊕XOu
u
X
0=′ XO npIXOX
, 
où  est le -vecteur constant de composante scalaire égale à 1. 1×n
 
5.3. Algorithme itératif d’optimisation 
Les principales étapes de l’algorithme d’optimisation sont les suivantes : 
i) Initialisation ; 
ii) Calcul des coordonnées factorielles des objets ; 
iii) Orthonormalisation ; 
iv) Calcul des coordonnées factorielles des modalités 
v) Test de convergence : si oui, poursuivre ; si non, aller en ii) ; 
vi) Rotation. 
 
i) Initialisation 
La matrice  des coordonnées factorielles est initialisée par tirage aléatoire sous contraintes de 
centrage ( ⊕u ) et de normalisation ( s=′ X~
n  
XGDY ~~ 1 jjj ′= −
W
⊕ ). A partir de la matrice normalisée , 
o  obtient une première approximation des coordonnées factorielles des catégories du critère j, soit 
. 
 
ii) Calcul des coordonnées factorielles des objets 
Dans un premier temps, on définit, comme intermédiaire de calcul, une matrice  suivant : 
© Revue MODULAD, 2008 - 219 - Numéro 38 
 ∑←
j
jjj YGOW
~
 
Dans un second temps, on centre cette matrice par rapport à l’ensemble des objets actifs de l’analyse 
en prenant en compte le filtrage réalisé par la matrice-objet O  : ⊕
[ ]   ( )WuOuOuuOOW ′′−← /~  ⊕⊕⊕⊕
Ces deux étapes conduisent à des solutions localement optimales si on n’applique pas de contraintes 
d’orthogonalité. 
 
iii) Orthonormalisation 
La procédure d’orthonormalisation consiste à trouver une matrice , -orthonormale, qui soit la 
plus proche possible, au sens des moindres carrés, de la matrice 
+X ⊕M
W~ . Cette matrice est obtenue en 
appliquant la procédure d’orthormalisation de Gram-Schmidt (procédure GRAM, repris de Björk et 
Golub, 1973), selon l’équation suivante :  
© Revue MODULAD, 2008 - 220 - Numéro 38 
 ( )WMMX ~GRAM 212121 −⊕−⊕+ ← p
+
j
XGD
 
ce qui, à une rotation près, conduit à la solution des moindres carrés. 
 
iv) Calcul des coordonnées factorielles des modalités 
Pour chaque critère j, on calcule la matriceY  des quantifications de ses modalités, comme suit : 
~1
jjj ′= −+    Y
 
v) Test de convergence { ( ) (La différence )}++− YXYX ,~ ~, σσ  entre deux évaluations successives de la fonction objectif est 
comparée à la spécification ε  du seuil de convergence, fournie par l’utilisateur. Les étapes ii) à iv) 
sont réitérées tant que la différence est supérieure au seuil de convergence fixé. 
 
vi) Rotation ( )YX,σLa fonction de perte  étant invariante par rotation simultanée de  et de Y , la procédure 
itérative ne fournit pas nécessairement une orientation correcte pour les axes factoriels. En effet, du 
point de vue théorique, la solution en dimension s fournit les s premiers axes factoriels de la solution à 
s+1 dimensions, ce que ne garantit pas cet algorithme itératif. 
X
L’imbrication des différentes solutions est obtenue par extraction des vecteurs propres de la matrice 
∑ ′
j
jjj YDp
Y1
( ) ( ) ⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −⎟⎟⎠
⎞
⎜⎜⎝
⎛−= ∑ lpkns l
j
j ,max,1minmax
lm jk
2=p
maxs
                                                          
 
le calcul s’effectuant par la méthode de tridiagonalisation de Householder en utilisant l’algorithme QL 
proposé par [Wilkinson, 1965]. 
 
5.4. Diagnostics 
Rang maximum6
Le rang maximun indique le nombre maximum de dimensions qui peuvent être extraites des 
données, soit : 
maxs
 
où  est le nombre de variables sans valeurs manquantes,  est le nombre de catégories distinctes 
du critère j et n, le nombre d’observations. Bien que le nombre de dimensions non-triviales puisse être 
inférieur à s lorsque , la procédure HOMALS permet de spécifier des cardinalités de 
dimension qui vont jusqu’à . 
max
 
 
6 Imprimé en guise d’avertissement lorsque la dimension de la solution demandée excède le rang de l’opérateur 
d’inertie. 
Marges 
Le tableau des sommes de colonne de la matrice D  fournit directement les effectifs marginaux des 
modalités du critère j. La somme des éléments de la matrice O  donne indirectement (en la 
soustrayant de n) le nombre de valeurs manquantes
j
j
7 pour les modalités de chaque critère j. 
 
Pouvoir discriminant 
Le pouvoir discriminant d’un critère j selon une dimension s est défini par : 
© Revue MODULAD, 2008 - 221 - Numéro 38 
 ( ) ( )jsjjsjs n
yDy ′= 12η  
Il est constitué par la variance de la projection du critère j selon la dimension s. 
Compte tenu du fait que la trace est un opérateur invariant par changement de base, la somme des 
valeurs propres peut se calculer comme somme des pouvoirs discriminants sur l’ensemble des critères 
j, soit : 
 ∑ ∑∑=
s j s
jss p
21 ηλ  
( )YX,La valeur minimale de la perte d’homogénéité σ  est égale à ∑∑−
j s
jsp
21 η
                                                          
s . 
 
 
6. REFERENCES BIBLIOGRAPHIQUES 
 
Benzécri J.-P. (1973) L’analyse des données. Tome II L’analyse des correspondances, Dunod,  632 p. 
Bjöِrk A. et Golub G. H. (1973) « Numerical methods for computing angles between linear 
subspaces », Mathematics of Computation, 27: 579–594. 
De Leeuw J. et Van Rijckevorsel, J. (1980) « HOMALS and PRINCALS—Some generalizations of 
principal components analysis », in: Data Analysis and Informatics, E. Diday et al, 
eds. Amsterdam: North-Holland. 
Gillham N.W. (2001) A Life of Sir Francis Galton : from African Exploration to the Birth of Eugenics, 
Oxford University Press. 
Gifi A. (1981) Nonlinear multivariate analysis, Leiden, Department of Data Theory. 
Gifi A. (1990) Nonlinear Multivariate Analysis, Wiley, 579 p. 
Guttman L. (1941) “The quantification of a class of attributes: A theory and method of scale 
construction”. In: The Prediction of Personal Adjustment, P. Horst et al, eds. New 
York: Social Science Research Council. 
Hartigan J.A. (1975) Clustering Algorithms. Wiley, New York, 351 p. 
Lebart L (1975) « L’orientation du dépouillement de certaines enquêtes par l’analyse des 
correspondances multiples », Consommation , n°2, pp. 73-96, Dunod. 
Meulman J. (1982). Homogeneity analysis of incomplete data, Leiden, DSWO Press. 
 
7 Ou encore exclues de l’analyse car les valeurs observées n’appartiennent pas à l’intervalle des catégories 
admises [ ]jk,1 . 
Meulman J.H., Heiser J.H. (2001) SPSS Categories 11.0, SPSS Inc., Chicago, 330 p. 
Nishisato S. (1980).Analysis of categorical data: Dual scaling and its application. University of 
Toronto Press, Toronto. 
SPSS (1994) SPSS 6.1 Categories, SPSS Inc., Chicago, 209 p. 
Tenenhaus M., Young F.W. (1985) « An analysis and synthesis of multiple correspondence analysis, 
optimal scaling, dual scaling, homogeneity analysis, and other methods for 
quantifying categorical multivariate data », Psychometrika, 50, pp. 91-119. 
Wilkinson J. H. (1965) The algebraic eigenvalue problem, Oxford: Clarendon Press. 
 
 
 
Portrait de Sir Francis Galton avec son maître d’hôtel Albert Gifi 
Source : http://www.galton.org/photos 
© Revue MODULAD, 2008 - 222 - Numéro 38 
Annexe algébrique sur l
 
A1  Introduction 
 
A1.1 Le groupe Gifi 
L’analyse de l’homogénéité constitue le paradigme conceptuel du système d’analyse 
multivarié non linéaire développé par le groupe Gifi. Albert Gifi est le nom collectif choisi 
par les membres du Department of Data Theory de l’Université de Leiden (Pays-Bas). Ce 
groupe, constitué autour de Jan de Leeuw a mis au point un système pour l’analyse 
multivariée non linéaire présenté dans l’ouvrage [Gifi, 1990]. La méthodologie développée 
par le groupe Gifi couvre un très large éventail de méthodes d’analyse exploratoire des 
données multivariées, principalement des techniques factorielles allant de l’analyse en 
composantes principales à l’analyse canonique. 
 
A1.2 Le concept d’homogénéité 
Le concept d’homogénéité auquel se réfère ces travaux formalise un des paradigmes 
fondateurs de la psychométrie selon lequel des critères différents peuvent mesurer une même 
caractéristique. Lorsque des variables distinctes (résultats aux tests, réponses aux questions, 
items choisis) semblent plus ou moins mesurer une même caractéristique, elles sont qualifiées 
d’  « homogènes ». 
 
A1.3 L’objectif de l’analyse d’homogénéité 
Supposons que nous ayons rassemblé des données sur une population de n objets (individus, 
produits, régions, etc.) à partir de p critères j présentant un nombre fini de catégories selon 
lesquelles se distribuent les objets étudiés. L’objectif de l’analyse d’homogénéité est de 
représenter la structure que projette sur cette population (i.e. les profils de comportement) la 
batterie des critères d’observation utilisés, ceux-ci pouvant présentant des échelles de mesure 
différentes. 
Les échelles de mesure utilisées par ces critères ou variables catégorielles j à jk  catégories  
peuvent être numériques (les catégories représentent des intervalles de mesure disjoints), 
ordinales (les catégories sont ordonnées) ou nominales (les catégories codent simplement 
l’appartenance à une classe). 
L’objectif de l’analyse d’homogénéité est donc de représenter les objets étudiés et les critères 
d’étude dans un espace euclidien de faible dimension (représentation multivariée à s 
dimensions p
’analyse de l’homogénéité8
s < ) en prenant en compte les contraintes imposées par les différentes échelles 
de mesure utilisées. Cette représentation euclidienne constitue la solution du programme de 
maximisation de l’homogénéité associé à l’analyse d’homogénéité, s étant appelée la 
dimension de la solution. 
 
A1.4 La méthode de représentation 
Le choix de la e de représentation s’effectue par l’intermédiaire de l’optimisation 
d’une fonctio esurant l’homogénéité. Cette procédure d’optimisation  permet de 
calculer des valeurs, scores et quantifications, utilisées pour construire une représentation 
géométrique dans un espace euclidien de faible dimension des relations, respectivement entre 
objets étudiés et entre catégories des critères d’observation.   
                                                          
 méthod
n-objectif m
 
8 Cette annexe s’inspire très largement des ouvrages cités, en particulier de [Meulmann, 1982]. 
© Revue MODULAD, 2008 - 223 - Numéro 38 
En théorie, les valeurs observées pour ces variables catégorielles distinctes mais homogènes 
euvent être remplacées par la valeur unique d’une variable synthétique x . 
1.5 La mesure de l’homogénéité  
atégorielles numériques, un changement d’échelle spécifique opéré par 
it minimale. 
Le déf d’homogénéité peut être assimilé aux différences constatées entre les critères 
écarts internes aux objets). Ces écarts internes aux objets 
 total des différences (somme totale des carrés des écarts). 
En sub uant une variable synthétique à la batterie de critères étudiés, on établit une relation 
éité imparfaite de ces variables catégorielles et la 
 si toutes ces variables peuvent être transformées selon un 
ceptible de les rendre homogènes ; 
les variables numériques ou à 
quantifier les variables ordinales ou nominales (en affectant une valeur numérique 
à chaque catégorie) pour maximiser la mesure de l’homogénéité. 
p
 
A
Pour des variables c
une transformation linéaire peut amener les valeurs de chaque critère à coïncider avec celles 
de la variable synthétique. Ces critères sont alors homogènes. Ce n’est pas toujours le cas, on 
peut alors utiliser des transformations non linéaires pour les rendre homogènes. Les critères 
étudiés sont alors homogénéisables. 
 En pratique, les batteries de critères étudiés ne sont pas toujours parfaitement 
homogénéisables. C’est souvent le cas lorsqu’elles comportent des variables ordinales voire 
nominales. On se contente alors d’une solution approchée  pourvu que la perte d’information 
induite par l’agrégation des différents critères so
aut 
étudiés pour chacun des objets (
doivent être distinguées des différences spécifiques entre objets constatées pour des critères 
homogènes (écarts entre objets). Une mesure possible de ce défaut d’homogénéité consiste à 
rapporter la mesure de ces différences internes (somme des carrés des écarts internes aux 
objets) à celle des différences spécifiques (somme des carrés des écarts entre objets) ou ce 
qui est équivalent au
stit
d’équivalence entre la mesure de l’homogén
perte d’information liée à leur agrégation selon une échelle unique de catégories : maximiser 
l’homogénéité revient à minimiser la perte d’information.  Pour une mesure normalisée de 
l’homogénéité sur un intervalle ]1;0[ , on peut formaliser cette relation d’équivalence par 
l’équation :   
mesure d’homogénéité = 1 – perte d’information      [1] 
 
A1.6 Les principes de l’analyse d’homogénéité 
A l’issue de cet exposé informel, récapitulons les principes qui constituent le fondement de 
l’analyse de l’homogénéité : 
i) une batterie de critères d’observation numériques est dite homogène si toutes les 
variables qui la composent sont liés par une relation linéaire ; ces variables sont 
alors qualifiées d’homogènes ; 
ii) une batterie de critères d’observation numériques est dite homogénéisable si elle 
peut-être rendue homogène au moyen de transformations portant sur ces variables 
numériques ; 
iii) une batterie de critères formée de variables numériques, ordinales ou nominales est 
homogénéisable
processus de quantification  sus
iv) l’homogénéité d’un ensemble de variables centrées est appréciée à l’aune du 
rapport entre la somme des carrés des écarts entre objets  (SCEinter) et la somme 
des carrés des écarts totale (SCEtotal); l’homogénéité parfaite correspond à la valeur 
1 pour ce ratio, i.e. à une valeur nulle pour la somme des carrés des écarts interne 
aux objets (SCEintra) ; 
v) l’analyse d’homogénéité consiste à transformer 
© Revue MODULAD, 2008 - 224 - Numéro 38 
Pour poursuivre l’analyse, il convient de donner une formulation plus précise à l’énoncé de 
ces principes en utilisant le cadre algébrique d’un espace vectoriel où ob etsj  et critères sont 
présentés par des vecteurs et leurs transformations sont représentées par des matrices.  re
 
A2  Analyse de l’homogénéité en dimension 1 
 
A2.1 Concepts 
 
A2.1.1 Le tableau des observations 
Ainsi, le tableau des observations peut être représenté par une matrice H de données 
catégorielles concaténant les vecteurs pjj ,,1, L=h , chaque vecteur hj contenant les 
observations  ijh  correspondant au critère ou variable catégorielle j sur l’individu i de la 
population des n  objets observés : 
 
[ ]
⎥⎥
⎥⎥
⎥⎤
⎢⎢
⎢⎢
⎢⎡
== ipiji
pj
pj hhh
hhh
MMM
LL
MMM
LL
LL 1
1111
1 ,,,, hhhH   
⎥⎦⎢⎣ npnjn hhh LL1
Suivant l’équation [1], maximiser l’homogénéité revient donc à minimiser la perte 
d’information lorsque l’on remplace la batterie de critères { }pj hhh ,,,,1 LL  par une variable 
synthétique  x . 
 
Tableau A1 : le tableau des données catégorielles. 
id th r e a d n h e a dn in d h ea d n b o ttom n n r
  
 
Le tableau d
l’analyse pou
le g th n b a ssn
2 2 1
2 3 1 2 2 1
1 1 1
2 1 1
1
2 2 4 1
3 1 1 2 2 2 1
4 1 1 2 2 2 1
5 1 1 2 2 2 1
6 1 1 2 2 2 1
7 1 4 2 2 5 1
8 1 4 2 2 3 1
9 1 4 2 2 3 1
1 0 2 2 3 2 5 1
1 1 2 3 1 2 4 1
1 2 2 5 1 2 4 1
1 3
1 4 2 5 1 2 2 1
1 5 2 3 1 1 4 1
1 6 2 2 1 1 1 1
1 7 2 5 1 1 1 1
1 8 2 5 1 1 1 1
1 9 2 5 1 1 1 1
2 0 2 5 1 1 1 1
2 1 1 1 2 2 1 2
2 2 1 1 2 2 1 2
2 3 1 1 2 2 1 2
2 4 2 2 1 2 1 2
es données catégorielles ci-dessus code l’appartenance des n=24 objets observés aux catégories de 
r les p =6 critères d’observations retenus. 
© Revue MODULAD, Numéro 38 2008 - 225 - 
 
 
A2.1.2 Une mesure de la perte d’information 
L’élaboration d’une solution acceptable à ce problème de substitution  passe en règle générale 
a rte d’information que l’on appelle 
 stress, et que l’on notera ( )xσ , définie par : 
( )
par l minimisation d’une fonction-objectif mesurant la pe
2  le
∑
=
=
p
j
22 1 hxxσ
    
[2] −
jp 1
où 
2
jhx −  désigne une fonction quadratique des écarts (norme) entre le critère jh  et la 
variable synthétique x . 
Si cette fonction quadratique est la somme des carrés des écarts ( )SCE hxhx −=− 2 , jj
alors il é de l orme euclidienne usuelle notée 
© Revue MODUL 2008 - 226 - Numéro 38 AD,  
s’agit du carr a n 2
2j
hx −  : 
( ) ( ) ∑∑
==
−=−=
p
j
j
p
j
j p
SCE
p 1
2
2
1
2 11 hxhxxσ  
Si la norm s des écarts (e utilisée est la racine carrée de la somme des carré
2
euclidienne du vecteur jhx − ), l’optimum atteint par la fonction objectif est constitué par la 
jhx − , norme 
moyenne arithmétique, notée h .  
 
A2.1.3 Définition de la perte d’homogénéité  
Reliant perte d’informat ogénéité sur la base de cette relation d’équivalen
roupe Gifi a repris la méthode initi ent proposée par Louis Guttman  ([Guttman, 1941] 
ttitudes commune à une batterie de critères 
qualitatifs. 
En synthétisant l’ensemble des observations effectuées sur les individus par une seule et 
même variable qui maximise l’homogénéité d’une batterie de critères, on peut introduire 
simultanément un opérateur de différenciation des individus en cherchant des transformations 
de critères (par exemple, en appliquant un système de poids 
ion et hom ce, le 
g alem
pour la recherche d’un « score », échelle d’a
{ }pj yyy ,,,,1 LL )  qui 
permettent par substitution de la moyenne pondérée des variables catégorielles ainsi 
transformées d’obtenir des scores individuels (valeurs individuelles pour la variable 
synthétique x  calculée après transformations) qui soient les plus différents possibles entre 
objets. En désignant par y  le vecteur des poids { }pj yyy ,,,,1 LL , on aboutit ainsi à un 
programme de minimisation d’une fonction de perte, notée ( )y,2 xσ  , défini par
( )
 : 
2
21
2 1, ∑
=
−=
p
j
jjyp
hxyxσ    [3] 
Les vecteurs jh  supposés centrés et le coefficient jy  per mothétie 
spécifique au vecteur jh . 
 sont met d’effectuer une ho
Il en résulte une transformation linéaire du système de codage des catégories du critère j. La 
olution du programme de minimisation de cette fonction de perte est une moyenne pondérée s
© Revue M  - 227 - Numéro 38 ODULAD, 2008
des jh  : ∑
=
=
j
jjw wp 1
hh  avec  
p1~
∑
=
A2.2 sformations linéaires 
 
.1 Homog
 transform e
r ces (par homothétie). Dans un premier temps, travaillons avec des 
) et procédons au moyen d
pondération sur l’ensemble des variables. Soit  la transformation linéaire par pondération 
j
= p
j
j
j
y
w
1
. 
 
jy
 Tran
A2.2 énéité et transformations linéaires 
Les ations linéaires des variables peuvent modifier à la fois les moyenn s (par 
translation) et les va ian
variables centrées (dont les valeurs sont des écarts à la moyenne e 
jt
spécifique à la variable h , telle que [ ] jjjj y j  la pond ation affectée à la 
variable j . Les différences entre la variable synthétiqu x et les variables numériques 
pjj ,,1L=h  sont alors exprimées par la fonction de p rte suivante :  
yt hh =  avec ér
e 
e
( ) [ ]( ) [ ] (x;2σ=
e tion de pert ous form d’une somme des 
)yhxhxhxx 111; 2
21
2
211
2σ −=−=−= ∑∑∑
===
p
j
jj
p
j
jj
p
j
jj yp
t
p
tSCE
p
t  
Nous pouvons reformuler cett  fo
roduits scalaires des vecteur 
nc e en l’écrivant s e ( )jjy hx −  par eux-mêmes : p
( ) ( ) ( )jj
j
jj
j
jj yyp
y
p
hxhxhxyx −′−=−= ∑∑
== 121
2 11,σ  
En développant cette expression et en remarquant qu’un scalaire est égal à son transposé  
( hhx ′=′
pp 2
xjj ), on peut écrire la fonction de perte sous forme matricielle si l’on définit la 
matrice D  d’ordre pp×  par la diagonale de la matrice HH′  ( [ ]HHD ′= diag ), où H  figure 
la matrice des données d’ordre pn×  et y  le vecteur des poids : 
( ) ( ) ( ) ( ) HyxDyyxxhxhhxxyx ′−′+′=′−′+′= ∑∑
== pp
y
p
yy
p jj
p
j
jj
p
j
jj
121;
11
2σ  
 En minimisant cette fonction, il convient d’imposer une contrainte sur la taille du vecteur x  
ou du vecteur y  pour exclure la solution triviale où x  et y  sont nuls. Cette contrainte peut 
s’exprimer comme une standardisation des scores indivi
2
duels ′xx 1= . Une autre formulation 
possible de cette contrainte est la standardisation des variables transformées, soit 1=′ yDy
s deux approches donnant le même résultat à un facteur d’échelle près, nous travailleron
vec la première contrainte portant sur le vecteur des sco
. 
Le s 
a dividuels
 
e  
ontrainte de normalisation 1
res in . 
A2.2.2 Minimum sous contrainte de normalisation 
En utilisant la technique des multiplicateurs de Lagrang iner le minimum de
la fonction ( )yx;2σ  sous c , on peut déterm′ =xx  : en notant μ   le multi
e la
plicateur 
de Lagrange, cela revient à trouver le minimum d  fonction ( ) ( ) ( )1;,, 2 −′−= xxyxyx μσμf . Les extrema sont obtenus en dérivant la fonction et en 
annulant ses dérivées partielles.  
Soit : 
( ) 0222,, =−−=∂
∂ yHxx
x
yx
p
f μμ  et   ( ) 022,, =′−=∂
∂ HxyD
y
yx
pp
f μ  
(D  nt sy  
© Revue MODULAD, 2008 - Numéro 38  - 228 
éta métrique, on a yD
y
yDy 2=∂
′∂  et on re arquera que ( ) DyyDyD ′=m ′= ). 
équations normales, respectivement : ( )xyHOn en tire les μ−1p  et  DyxH =′=
  
n déduit la transformation xHD ′= −1  
s précédentes, on aboutit à l’équation aux 
 barycentrique :  yLa matriceD  étant diagonale, on e
Par substitution de y  en combinant les équation
valeurs extrémales : ( )xxHHD μ−=′− 11 p . 
Montrons que la fonction de perte ( )yx;2σ  atteint son minimum pour la valeur de μ  à 
l’extrémum. 
En substituant  xH′  à Dy , on obtient : 
( ) HyxxxHyxxHyxxyx ′−′=′−′′+′=
ppp
12;2σ . 
 par ( )x
1
En remplaçant yH μ−1p , on en déduit :  
( ) ( ) μμμ =′=′−−′= xxxxxx 1 . 
on minimum pour la valeur de 
σ yx;2
La fonction de perte ( )yx;2σ  atteint donc s μ  à l’extremum. 
 
A2.2.3 Décomposition en valeu res r singuliè
Soit la décomposition en ngul p-r 
valeurs nulles)   de la matrice 
ières de rang r, éventuellement complétée (par valeur si
21−=HDZ  d’ordre pn×  définie par :  21Λ=VUZ  où 
V est une matrice d’ordre  rn× , orthonormale , i.e. telle que IVV = p′   
U  est une matrice orthonormale d’ordre rp× , i.e. telle que pIUU =′  
t une matrice diagonale d’ordre Λ  es rr×  dont les r valeurs diagonales positives 
rλλλ ρ,,1 L  sont appelées valeurs singu s. 
 
,,L liè
ectrale de la mat ice des produits sc ntre objets 
 (analyse da nℜ ) 
re
A2.2.4 Décomposition sp r alaire e
ns 
Montrons que le vecteur des scores individuels x  est le vecteur propre de la matrice HHD ′−1  
associé à sa plus grande valeur propre ( )μλ −= 11 p . 
En utilisant l’orthonormalité de U , on en déduit 
Commentaire [DD1] : I il 
s’agit de la décomposition en 
valeur singulières d’une matrice 
de rang r, « complétée » par des 
valeurs singulières nulles pour 
atteindre l’ordre p [décomposition 
leurs singulières « maigre » 
SV1, jusqu’au rang r) et 
omposition en valeurs 
gulières pleine (DVS2, 
complétée jusqu’à l’ordre p, cf. 
Jean-François Durand, Eléments 
de calcul matriciel et différentiel 
pour l’analyse factorielle des 
données, Université de 
Montpellier II, 
polyalgmatcomp.pdf] 
en va
(D
déc
sin
UVZ ′Λ= 21 décomposition spectrale 
de  ZZHHD ′=′−1 : 
VVZZHHD ′Λ=′=′−1  
’où une nouvelle formulation de l’équation aux valeurs extrémales 
 et la 
xxVHD λ=′Λ=′−1  
inimisan
D
 ( ) Hxμ =−1p Vx
Si les valeurs singulières sont rangées par ordre de grandeur décroissant, la solution de cette 
équation m t ( ) μσ =yx;  est donnée par   l  a 1λ plus grande valeur propre de 
l’opérateur symétrique ZZ ′   et 1vx =  le vecteur propre associé  (les colonnes de la matrice 
) : V  sont vecteurs propres de ZZ ′
1  xxHHD
1 λ=′−
 
A2.2.5 Scores individuels optimaux 
Ainsi, le vecteur x  des scores individuels optimaux est le vecteur propre de la matrice 
rande valeur HHD ′−1  associé à la plus g propre ( )μλ −= 11 p . 
La fonction de perte ( )yx;σ  est minimisée pour le v  correspon
grande valeur p
ect dant à la plus 
ropre de la HHD ′−1 et elle atteint donc son minimum en 
eur propre x
© Revue MODULAD, 2008 - 229 -  Numéro 38
matrice 
p
11
λ− . 
(y
μ =
 
A2.2.6 Décomposition spectrale de la mat alaires entre critères 
 (analyse dans pℜ ) 
rice des produits sc
En utilisant les équations normales )xH μ−= 1p  et DyxH =′ , on en déduit  l’équation aux 
valeurs extréma  vecteur des qul e antifications :   DyHyH 1es pour l λ=′  . 
onn e ZLes vecteurs col es de la matrice U  sont vecteurs propres d la matrice Z′  des produits 
scalaires entre critères, soit pour le premier vecteur propre associé à 1λ :  111 uuZZ λ=′  . 
Le vecteur y   des s catégorielles equantification st donné par : 1
21
1 uDy
−= λ  
 
A2.2.7 Relations de transition 
 individ ré
de transition xHD ′= −1  et xyH
Le passage des scores uels aux quantifications catégorielles est assu  par les relations 
 :  y 1λ=  
position e singu e d’homogénéit lise 
l’algorithm ue ) déjà m ntionné dans 
   
A2 .2.8 L’algorithme du centrage réciproque 
Plutôt que de calculer la décom n valeurs lières, l’analys é uti
e du centrage réciproq (Reciprocal Averaging - RA e 
[Fisher, 1 elé « és alternés » (Alternating 
Least Squ rith ance itérée  pour calculer 
940]. Un tel algorithme, également app  moindres carr
ares – ALS), peut être vu comme un algo me de la puiss
la décom s
L’utilisation d’un tel algorit a faible complexité en taille 
émoire et son efficacité dans la recherche de la valeur propre maximale. 
 calculer les 
position aux valeurs singulières ([Ni hisato 1980] en donne une preuve). 
hme était initialement justifiée par s
m
Pour minimiser la perte de pouvoir discriminant, il faut à chaque itération l
ca gorielles yquantifications té ~  comme moyenne des scores individuels initiaux ( )0x  
u(arbitrairement choisis en première instance so s la condition ( ) 00 =′ x1 ), puis calc  
nouveaux scores x~  sur la base des quanti rielles obtenues, enfin norma
Commentaire [DD2] :  ( ) xHHyH 1′ ′= − μ =p
 
Commentaire [DD3] :  on 
peut vérifier que ce vecteur y  
e l’équation aux valeurs 
êmes : 
λ
vérifi
extr
uler les
fications catégo liser ces 
ta
1 : 
scores individuels ce qui termine l’itération. 
Pour l’itération l, on a donc  les é pes suivantes : 
 ( )lxHy ′= −1: D~   
 2 : yHx ~:~
p
=  1
 3 : ( ) ( ) 211 ~~~: ′= xxxx l , sous la condition 0−+ ~ =′ x1  
La réitération de ce cycl t des scores xe produi ~  et des quantifications y~  qui, au bout d’un 
certain nombre d’itérations, e se difient plus de manière détectable. Ce couple de vecteurs 
tionnaires )~,~( ∗∗ yx  représe
 n  mo
sta  alors l’optimum recherché et la norme nte
21⎞⎛ ′
2
~~~ ⎟⎠⎜⎝=
∗∗∗ xxx du 
( 211DHHHy ′= −λH′ u
DZuZD 111
21
1 λλ =′
 
Commentaire [DD4] :  Cet 
algorithme connu également sous 
le terme de dual scaling est 
signalé par [Saporta, 1990] sous le 
terme de « méthode des moyennes 
réciproques » (cf.  p.215) 
Commentaire [DD5] :  
L’algorithme de la puissance 
itérée est souvent utilisé en 
pratique pour rechercher la valeur 
propre dominante . 
© Revue MODULAD, 2008 - 230 - Numéro 38 
vecteur ∗x~  stationnarisé nous fournit la plus grande valeur propre 1λ  et donc la valeur 
t
 
s non-linéaires 
 
ns non -li
Le concept d’homogénéité  est étendu aux transformations non linéaires ([De Leeuw & Van 
ij
ésentant la 
ulement si aprè transfor
minimale de la fonc ion de perte de pouvoir discriminant. 
A2.3 Transformation
A2.3.1 Extension aux transformatio néaires 
R ckevorsel, 1980]) de la manière suivante :  
le vecteur h  est dit homogène au score x , vecteur unitaire (de norme 1) reprj
variable synthé ue tiq ciblée, si et se s une mation jτ  de normalisation 
spécifique à chaque critère  j  (c.à.d. telle que [ ] 1  on obtient l’égalité
2
=jj hτ ) ,  [ ]jj hτ . x =
Le vecteur transformé [ ]jj hτ  constitu  une quantification du critère qualitatif   
Si le vecteur jh  n’est pas homogène à x , on définit la perte d’h mo e u
e j.
o généité comm ne 
rire vect iellement :fonction quadratique des écarts au score, que nous pouvons éc or
 ( ) [ ] [ ]( ) [ ]( )jjp jjp j hxhx ττ −′−= ∑2 11   [4] 
u teu l
jj pp == 121
 en utilisant le produit d  vec r-co onne 
jt hxx τσ −= ∑2 ,
[ ]jj hx τ−  par son transposé, le vecteur-ligne 
[ ]( )′− hx τ  jj
La minimisation de cette fonction objectif sur l’ensemble des critères analysés revient à 
rechercher un score  x   et des transformations non linéaires τ maximisant l’homogénéité de la 
batterie de critères proposés. 
 
A2.3.2 Indépendance vis à vis du codage 
Le passage des transformations linéaires aux transformations non linéaires s’effectue en fait 
par la recherche d’une quantification des catégories qui constitue une solution invariante, 
c’est à dire indépendante du codage utilisé initialement. Cette indépendance vis à vis du 
codage est obtenue en analyse d  l’homogénéité par l’intermédiaire des indicatrices de codage 
([Guttman, 1941]). 
Les indicatrices de codage sont des variables logiques indiquan aque catégorie d’un 
critère qualitatif quels sont les objets lui appartenant : 
si la variable vectorielle jh  codant le j 
e
t pour ch
critère à kj  comporte n ervations 
kj , on crée une matrice indicatrice jG  
t  jikg  définis
khsi ij =  appartient à la cat critère j » 
hsi ij ppartien  pas à la catégorie k du critère j » 
 
A2.3.3 Tableau disjonctif complet 
Afin de pouvoir opérer sur l’ensemble des critères qualitatifs, on concatène les matrices 
indicatrices jG  dans un tableau booléen, matrice globale des indicatrices, notée G  : 
ème 
codées par un ensemble de catégories variant de 1 à  
à n x kj  élémen s  par :  
kg jik ≠= 0   « l’objet i n’a t
g jik =1
catégories  obs
 « l’objet i
⎥⎦
⎤⎢⎣
⎡= pj GGGG LL1  
égorie k du 
Tableau A2 : tableau disjonctif complet c n l ies d n e 
retenus
G G1 G G5
i thread1 thread2 head1 head2 head3 head4 head5 t m2 l 1 leng ss a t
1 1 0 1 0 0 0 1 1 0
2 1 0 1 0 0 0 0 1 0
3 1 0 1 0 0 0 0 1 0
4 1 0 1 0 0 0 0 1 0
5 1 0 1 0 0 0 0 1 0
6 1 0 1 0 0 0 0 1 0
7 1 0 0 0 0 1 0 1 0
8 1 0 0 0 0 1 0 1 0
9 1 0 0 0 0 0 1 0
10 0 1 0 1 0 0 1 0 0 1 0
11 0 1 0 0 1 0 0 0 1 1 0
12 0 1 0 0 0 1 0 1 0 0 1 0
13 0 1 0 0 1 0 0 1 0 0 1 0
14 0 1 0 0 0 1 0 1 0 0 1 0
15 0 1 0 0 1 0 0 0 0 0 0 1 0
16 0 1 0 1 0 0 0 1 0 1 0
17 0 1 0 0 0 1 0 1 0 1 0
18 0 1 0 0 0 1 0 1 0 1 0
19 0 1 0 0 0 1 0 0 1 0
20 0 1 0 0 0 1 0 1 0 1 0
21 1 0 1 0 0 0 0 1 0 0 1
22 1 0 1 0 0 0 0 0 0 1
23 1 0 1 0 0 0 0 1 0 0 1
24 0 1 0 1 0 0 0 1 0 0 1
tot 12 12 9 3 3 6 18 10 2 20 144
ss2 to
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
 Revue MODULAD, 2008 - 231 - Numéro 38 
odant les  n=
G3
indhead1
0
0
0
0
0
0
0
0
0
11
24 ob
indhe
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
0
0
0
1
jets observés s
ad2 indhead3
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
1
1
1
0
12 1
elo
G4
bot
0
0
0
0
0
0
0
0
0
es catégor
om1 botto
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
0
0
0
0
6
e l’a
enght
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
1
1
1
1
alys pour les
ht2 leng
0
0
1
1
1
1
0
0
0
0
0
1
1
0
0
0
0
1 0
0
0
1 0
0
0
6
 p =
ht3
6 critères d’
lenght4 le
0 0
0 1
0 0
0 0
0 0
0 0
0 0
1 0
1 0
0 0
0
1
0
0
1
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
4
observ
G
nght5 b
2
ati
6
ra
0
0
0
0
0
0
1
0
0
1
0
0
0
0
ons 
1 br
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
3
©
© Revue MODULAD, 2008 - 232 - Numéro 38 
Ce tableau utilise un codage booléen de l’information qualifié de « disjonctif complet » dans 
itue dans une catégorie unique (la marge ligne de chaque 
rit ds de 
chacune des catégories. Le poids total du tableau est égal au produit  du nombre d’objets 
observés par le nombre de critères d’observation : pn
la mesure où chaque individu se s
matrice indicatrice jG  est égale à 1). La somme de chaque ligne du tableau G est égale au 
nombre de c ères qualitatifs p et la somme de chacune de ses colonnes donne le poi
× . 
 
A2.3.4 Tableau de Burt 
Le tableau de contingence généralisé GGC ′=  (ou encore tableau de Burt) croisant 
l’ensemble des critères  contient la structure des inter-relations entre les catégories des 
différents critères. Il est imprimé ci-contre sous une forme triangulaire car ce tableau est 
symétrique et possède donc ∑
=
p
j
jk
1
 lignes et ∑
=
p
j
jk
1
 colonnes. 
 Bien qu’il soit utilisé dans les calculs de la procédure HOMALS, le tableau de Burt C n’est 
pas fourni par le logiciel SPSS . 
 
A2.3.5 Blocs diagonaux : matrices de pondération 
Les blocs diagonaux du tableau de Burt sont constitués par des matrices jjj GGD ′= , issues 
du produit matriciel de jG  par son transposé jG ′ . Elles sont diagonales avec une diagonale 
constituée par la marge-colonne de jG  (donnant le nombre d’objets appartenant à chaque 
catégorie k du critère j). jD  est la matrice de pondération des effectifs marginaux des 
catégories du critère j. 
 
Tableau A3 : blocs diagonaux du tableau de Burt. 
 
D1 thread1 thread2
thread1 12 0
thread2 0 12
D2 head1 head2 head3 head4 head5
head1 9 0 0 0 0
head2 0 3 0 0 0
h 0 0 3 0 0
h 0 0 3 0
head5 0 0 0 6
ead3
ead4 0
0
D3  idh1 idh2 idh3 
idh1 11 0
idh2 0 0
idh3 0 0 1
0
12
ODULAD, 2008 - 233 - Numéro 38 
Tablea
C thread1 thread2 head1 head2 head3 head4 head5 indhead1 indhead2 indhead3 bottom1 bottom2 lenght1lenght2 lenght3 lenght4 lenght5brass1 brass2
12 0
0 12
9 0 9 0 0 0 0
0 3 0 3 0 0 0
0 3 0 0 3 0 0
3 0 0 0 0 3 0
0 6 0 0 0 0 6
0 11 0 2 3 0 6 11 0 0
12 0 9 0 0 3 0 0 12 0
0 1 0 1 0 0 0 0 0 1
0 6 0 1 1 0 4 6 0 0 6 0
12 6 9 2 2 3 2 5 12 1 0 18
4 6 4 2 0 0 4 6 4 0 5 5 10 0 0 0 0
4 2 4 0 1 0 1 2 4 0 0 6 0 6 0 0 0
2 0 0 0 0 2 0 0 2 0 0 2 0 0 2 0 0
1 3 1 0 2 0 1 3 1 0 1 3 0 0 0 4 0
1 1 0 1 0 1 0 0 1 1 0 2 0 0 0 0 2
9 11 6 2 3 3 6 10 9 1 6 14 6 6 2 4 2 20 0
3 1 3 1 0 0 0 1 3 0 0 4 4 0 0 0 0 0 4
thread1
thread2
head1
head2
head3
head4
head5
indhead1
indhead2
indhead3
bottom1
bottom2
lenght1
lenght2
lenght3
lenght4
lenght5
brass1
brass2
u A4 : tableau de contingence  généralisé ou tableau de Burt pour n=24 observations et p=6 critères.
© Revue M
 
En ne retenant que les blocs diagonaux jD du tableau de Burt C, on obtient une matrice de 
pondération D, possédant également ∑
= =
 
aux : tableaux de contingence simples 
rt sont 
p
j
jk
1
 lignes et ∑p jk
1
 colonnes, qui constitue la matrice 
de pon ation globale. 
j
dér
A2.3.6 Blocs non diagon
Les blocs non diagonaux du tableau de Bu constitués par des matrices jjjj ′′ ′= GGC , 
issues du produit matriciel de j ′G  par le transposé jG′ .  Le tableau de contingence non 
jj ′diagonal C  correspond au tri croisé des critères j et j′ . 
 
Tableau A5 : bloc non-diagonal et sommation  en ligne 
 
© Revue MODULAD, 2008 - 234 - Numéro 38 
 
La somme de la kème ligne d’une matrice non diagonale jj ′C  est égale au k
ème  élément 
diagonal de la matrice D  quelque soit le critère de croisement jj ′ . 
 
Tableau A6 : bloc non-diagonal et sommation  en colonne 
 
 
De façon symétrique, la somme ce non diagonale (tri croisé) 
jj ′C  est égale au k
ème  élém j′  quelque soit le critère de 
croisement j′ . 
 
42 head1 head2 head3 head4 head5 DC 4 bottom1 bottom2
bottom1 6 0
2 0
bottom1 0 1 1 0 4
bottom2 9 2 2 3 2 bottom 18
D1 thread1 thread2
thread1 12 0
thread2 0 12
C21 thread1 thread2
head1 9 0
head2 0 3
head3 0 3
head4 3 0
head5 0 6
 de la kème colonne d’une matri
ent diagonal de la matrice D
A2.3.7 Projecteurs orthogonaux 
Ultérieurement dans cet exposé, nous serons amené à utiliser le projecteur orthogonal jP  
qui permet de projeter les objets dans le sous-espace engendré par les variables indicatrices du 
codage du critère j que sont les kj vecteurs booléens de jG  : ( ) jjjjjjj DGGGGGP =′′= −− 11
qui est une matrice d’ordre nn
jG′  
× . 
Ce projecteur est un opérateur symétrique ( jj PP ′= ) et idempotent ( 2jj PP = ). 
On peut en dériver une notion de projecteur moye  , en effectuant l e n, noté 0P a moyenne d
© Revue MODULAD, 2008 - Numéro 38 - 235 
ces opérateurs sur l’ensemble des critères qualitatifs : ∑
=
=
p
j
jp 1
0
1 PP . 
  
A2.3.8 Discrétisation et scores induits 
L’opération de discrétisation des variables revient à remplace formé 
Commentaire [DD6] :  0P  
orthogonal suppose 
jjjj ′= ∀′ 0PP
, ce qui n’est pas le cas. 
≠
[ ]r le vecteur trans jj hτ  
par le produit matriciel jjyG  où jy  est le vecteur des quantifications  pour les kj  catégories 
du critère qualitatif j. 
La fonction de perte s’écrit alors : 
[ ]( ) ∑∑
==
−==
p
j
jj
p
j
j pp 1
2
2
1
2
2
2 11, yGxhyxσ  
Le vecteur jjj yGq =  à n éléments  contient les résultats numériques de la transformation  du 
e qualitatif j pour ch duits. Le 
eur x contient également n éléments caractérisant chacun des objets, appelés les scores 
viduels. La fonction de perte mesure alors le défaut d’ajustement entre les scores induits 
s scores 
 
.9 Optimisation globale  de la fonction de perte 
’analyse d’homogénéité peut être formulé selon deux points de vue distincts : 
une part, remplacer les p vecteurs jq  par un vecteur unique x , avec une perte 
homogénéité minimale. Idéalement, cela revient à choisir les vecteurs jy  tels que les 
cteurs jq  soient tous identiques. Dans ce cas, le vecteur x  des scores induits représente 
échelle unidimensionnelle commune dont les j critères qualitatifs constituent des 
résentations homogènes ; 
 autre part, en partant du vecteur x des scores i duels, l’objectif de minimisation de 
omogénéité sera atteint si nous choisissons ces scores de façon à ce qu ous
objets d’une même catégorie partagent le même score, ce qui implique que les p 
cteurs jq dentiques. 
i, le problème d’optimisation vu sous ces deux angles différents conduit à une solution où 
cores individuels contenus dans x  et les quantifications des catégories contenues dans les  
 parfaitement cohérentes, au sens suivant : 
ppjj yGyGyGx ===== LL11  
deux points de vue conduisent à deux formulations distinctes du problème 
timisation : emière formulation en termes de perte d’homogénéité, la seconde en 
es de perte de pouvoir discriminant. 
−x τ
acun des objets, ces éléments étant appelés les scores incritèr
vect
indi
et le
A2.3
Le but de l
• d’
d’
ve
une 
rep
• d’
la perte d’h
les 
ve
Ains
les s
jy  soient
Ces 
d’op
term
individuels. 
ndivi
e t  
 soient i
la pr
a perte d’homogénéité s’observe lorsqu’il n’existe pas de système de quantifications L
catégorielles  { }pj yyy LL1  tel que ppjj yGyGyx ===== LL11 . Cette formulation 
construisant un vecteur x  des scores i duels. 
La perte de pouvoir d
dividuels x  tel que  
G
suppose de partir ons catégo é en 
ndivi
iscriminant intervient lorsqu’il n’existe pas de vecteur de scores 
ppjj yGyGyx ===== LL11 . C mulation suppose de 
 avec un système de 
che tili
 et des quantifications catégorielles 
ée du problème de représentation passe 
erte d’homogénéité définie pour les p 
es écarts, la fonction de perte globale 
’écrit alors : 
( )
des quantificati rielles jy  et de tester leur homogénéit
G ette forin
partir des scores individuels x  et de tester leur pouvoir discriminant
quantifications catégorielles jy . 
Compte-tenu des différentes é lles de mesure u sées par l’ensemble des critères 
d’observation, un ajustement parfait des scores individuels
 pas réalisable en général : la résolution approchn’est
donc par la minimisation de la fonction globale de p
ritères. Si la norme est la somme des carrés dc
s
( ) ( )∑∑
==
−′−=−=
p
j
jjjj
p
j
jj pp 1
2
21
2 11, yGxyGxyGxyxσ  
 
2.3.10 Minimisatio
de
 d r ce qui précède à définir la 
erte minimale  de pouvoir discriminant par : 
A n de la perte de pouvoir discriminant 
Si l’on minimise la fonction de perte globale pour un vecteur  scores x  donné relativement 
à un système inconnu y e quantifications, on est conduit d’ap ès 
p
 ( ) ( ){ }yyxx
y
,min,* 22 σσ =  
En annulant la dérivée part ar rapport à y  et en résolvant vectoriellement le tème 
d’équations normales, on trouve la solution suivante à ce problème d’optimisation : 
 y j
que l’on peut récrire sous la forme : xGDy
ielle p sys
xGGG ′⎟⎠
⎞⎜⎝
⎛ ′=
−
jjj
1
 
′= − jjj 1   ce qui indique clairement que la 
quantification catégorielle optimale jy   constitue une moyenne pondérée (par l’inverse des 
éléments diagonale jjj GGDdiagonaux de la matrice ′=  ) des scores individuels pour les 
objets app espondantes du critère  j (sur la base des valeurs de  
xG ′j ). 
En substituant la valeur optimale jy  à l’  : 
( )
artenant aux catégories corr
inconnue y , on obtient
2
1
2 1,* ∑
=
′=
p
j
jjjp
xGGGxσ  
t rquant que ( ) jjjjj PGGGG =′′ −1 ,  on aboutit à : ( )
2
1− ′⎟⎠
⎞⎜⎝
⎛− jGx
22 1,* ∑e  en rema
 21=
−=
p
jp
xPxxσ . 
 
( )
j
Développons cette expression : 
( ) ( ) ∑
=
⎟⎠
⎞⎜⎝
⎛ ′−′′+′=−′−
p
j
jjjjj p 1
© Revue MODULAD, 2008 - 236 - Numéro 38 
∑
=
=
p
jp 1
211,* xPxxPPxxxxPxxPxx
 
2σ
et utilisons le fait que jP  est symétrique et idempotent, 
 nous en tirons :   ( ) ( ) ⎟⎠⎞⎜⎝⎛ ′
′−′=′−′=′−′= ∑
=  
Afin d’écarter les solutions triviales du problème d’optimisation 
 
( )
xx
xPxxxxPxxxxPxxxx 00
1
2 11,*
p
j
jp
σ
{ }xPxxx
x 0
2 min*,* ′−′=σ  
qui revient à maximiser  xPx 0′   
nous imposons des contraintes de normalisation, soit : 
0=′ x1  (pour éviter la solution triviale 1x =  et y = ) 
et 
1j
x (pour éviter la solution triviale 0x1=′ x  = ) 
 1  est le vecteur dont toutes les composantes sont et r dont toutes les 
x  so  co
 égales à 1 0  le vecteuoù
composantes sont nulles. 
La maximisation de Px 0′ us la ntrainte 1=′ xx  équivaut à maximiser le rapport xx
x
′ . 
L’ensemble des catég
xP′ 0
ories d’un critère qualitatif induisant une partition en kj groupes, la 
rés conduit à e des carrés inter-groupes 
de la somme des carrés in pes, comme suit : 
Somme des car tégories :  xPxGDGx jjjjBSC ′=′′= −1  
me des car
décomposition des sommes de car distinguer la somm
tra-grou
rés inter-ca x
Som rés intra-catégories :   ( ) ( )xPIxxGDGIx jjjjWSC −′=′−′= −1  
Somme des carrés totale :    xx′=TSC  
La maximisation de  xPx 0′  sous la contrainte de normalisation 1=′ xx  peut donc 
s’interpréter comme la maximisation du rapport de la variance inter-groupes à la variance 
totale. 
t xPx 0′La rec che d’un vecteur de scores individuels mher aximisan  correspond ainsi à un 
En 
e j. objectif de discrimination globale des groupes d’objets induits par les catégories du critèr
 
utilisant des opérateurs de centrage ⎟⎠
⎞⎜⎝
⎛
′
′−=
11
11IJM   et ⎟⎠
⎞⎜⎝
⎛
′
′−=
D11
D11IJ D , on montre 
© Revue MODUL 2008 - 237 - Numéro 38 AD, 
  
([M
2σ
e
eulman, 1982]) que la fonction de perte  de pouvoir discriminant peut s’écrire : ( ) xZZxx ′′−=1,*   
av c   21−= DJGJZ DMp    opérateur réalisant la projection du vecteur objet sur 
le sous-espaces engendrés par les indicatrices du codage de l’ensemble des critères 
ogonalement aux solutions triviales, que nous appellerons projecteur-objet. 
te reformulation montre que la recherche d’une solution au problème de maximisation du 
21− ′
s 
orth
Cet
ratio 
xx
xPx
′
′ 0  est équivalente au plan algébrique à un problème de recherche de vecteurs 
opres et de valeurs propres. pr
En eff de la matrice réelle symétrique ZZ ′ , la 
′
et, si VV ′Λ  est la décomposition spectrale 
solution x  maximisant  xVVx Λ′  est le vecteur propre correspondant à la plus grande valeur 
propre de la matrice ZZ ′ .  
Commentaire [DD7] : ( )11
11
′
′
 on reconnaît l’expression d’un 
projecteur I-orthogonal sur le 
vecteur 1 . 
Commentaire [DD8] : ( D1
11
′
′D
, projecteur D-orthogonal sur 1  
Commentaire [DD9] :  Il 
s’agit d’un problème de 
maximisation du quotient de deux 
formes quadratiques : le rapport 
xIx
xPx
′′
′ 0  est maximal pour le 
vecteur propre 1v  de 0
1 PI −  
associé à sa plus grande valeur 
propre 1λ (cf. [Saporta 90], 
p.484)
Le minimum d  ( ),*2 xσ  est donc égal à 11e  λ−  où 1λ  est la valeur propre maximum de la 
matrice ZZ ′ . 
 
A2.3.11 Minimisation de la perte d’homogénéité 
l’on mSi inimise maintenant la fonction de perte globale pour un système y  donné de 
quantificatio s catégorielles re a vement  s ores individuels x  inconnu, nous 
exprimons alors le programme de minimisation de la perte d’homogénéité comme suit : ( ) ( )
n l ti à un vecteur de c
{ }yxy ;min;2 σσ =∗ , x
En dérivant désormais la fonction de perte par rapport à  x  et en résolvant le système 
d’équations normales, on trouve la solution  à ce problème d’optimisation, y  étant  donné : 
yGGx
p
y
p
p
j
jj
11
1
== ∑
=
es.  
En substituant la solution optimale à l’équation dé de p te globale, nous 
en déduisons : 
 
Les scores objets optimaux sont constitués par la moyenne des quantifications des catégories 
correspondant
finissant la fonction er
( ) ⎟⎟⎞⎜⎜⎛ −⎟⎟⎞⎜⎜⎛ −=∗ ∑ jjjj yGGyyGGyy 111;2σ  
′
⎠⎝⎠⎝=
p
j ppp 1
∑
= ⎟⎠jj
n remplaçant GG′  par C  et jjG′  par jD , cela nous conduit à : 
⎟⎞⎜⎜⎝
⎛ ′′−′′−′′=
p
j
jjjj ppp 1 2
1211 yGGyyGGyGyGy  
e G
( ) ∑
= ⎟⎟⎠
⎞
⎜⎜⎝
⎛ ′′−′+′=∗
p
j
jjjjj ppp 1 2
2 1211; yGGyyDyCyyyσ  
© Revue MODULAD, 2008 - 238 - Numéro 38 
 yGGyyCyy ′′−′= 22 11 pp  Dy′+
12
p
yCy ′−=
2
11
pp
 yDy ′
⎟⎟⎠
⎞
⎜⎜⎝
⎛
′
′−′=
yDy
yCyyDy
pp
11  
Nous allons maintenant minimiser ( )y;2 ∗σ  s le des y  satisfaisant la condition 
p=′ yDy , ce qui revient à maximiser yCy′ . 
ob
ur l’ensemb
Ce pr lème d’optimisation peut être interprété en termes d’analyse de la variance. Nous 
ante •iq  inter-objets et une 
composante •− iji qq  intra-objets : 
Somme des carrés inter-objets :  Cyyq ′=•i 2  
rés intra-objets :  )
pouvons décomposer les scores induits jjj yGq =  en un compos
= ∑
=
n
i
B pSC
1
Somme des car ( yCDyq ⎟⎟⎠⎜⎜⎝
⎛ −′=− • pi j iijW
1
1 1
2
Somme tale :   Dyyq ′== ∑∑n
i
p
ijTSC
2  
q= ∑∑SC n p ⎞
= =
 
des carrés to
= =j1 1
Ainsi, nous maximison  le ratio de la somme des carréss  inter-objets sur la somme des carrés 
totale, sous la contrainte p=′Dyy . Cela s’interprète comme la recherche des quantifications 
max isent la  covariances pour les scores induits jq , tout en 
gardant la somme des variance Il s’agit de minimiser la somme des carrés intra-
aximiser l’homogénéité du 
système de quantifications donné a priori pour les catégories des critères retenus dans 
de normalisation p
catégorielles qui im somme des
s constante. 
objets, partant la perte d’homogénéité, et en conséquence de m
l’analyse. 
En utilisant la condition =′Dyy  et l’opérateur de projection orthogonale à 
( )
la solution triviale J , il vient : D
yCJy D′−=∗ 22 1;σ  Jy D′1p
© Revue MODUL - 239 - Numéro 38 AD, 2008 
Si l’on exprime la perte d’homogénéité en fonction de yDu 221−= p , on trouve alors : 
( )
1
uDCJJDuu 212 11; −′−=∗ Dσ 21−′ D
e perte 
p  
En utilisant le projecteur-objet Z , on montre ([Meulman, 1982]) que la fonction d
d’homogénéité peut s’écrire : ( ) uZZuu ′′−=1*,σ   
La valeur minimale de ( )a*,σ  est donc :  ( )ZZ′− 11 λ  
 où 1λ  est la plus grande valeur propre de la matrice ZZ′  ( également valeur propre 
maximale de la matrice ZZ ′ ) associé au vec t, teur propre  , ce qui nous condui à un facteur 
d’éche ologue du  minimisation de la rte 
de pou
thme du centrage réciproque  (Reciprocal Averaging - RA) déjà mentionné dans 
lle près, à une solution algébrique hom pe problème de
voir discriminant. 
 
A2.3.12 L’algorithme du centrage réciproque 
Plutôt que de calculer la décomposition en valeurs singulières, l’analyse d’homogénéité utilise 
l’algori
[Fisher, 1940]. Un tel algorithme, également appelé « moindres carrés alternés » (Alternating 
Least Squares – ALS), peut être vu comme un algorithme de la puissance itérée  pour calculer 
la décomposition aux valeurs singulières ([Nishisato 1980] en donne une preuve). 
ialement justifiée e comp
 de la valeur p maximale. 
ntificati
L’utilisation d’un tel algorithme était init par sa faibl lexité en taille 
mémoire et son efficacité dans la recherche ropre 
Pour minimiser la perte de pouvoir discriminant, il faut à chaque itération l calculer les 
qua ons catégorielles y~  comme moyenne des scores individuels appropriés 
is (arbitrairement chois en première instance), puis calculer les nouveaux scores x~  sur la base 
tifications catégorielles obte
: 
Commentaire [DD10] :  Cet 
algorithme connu également sous 
le terme de dual scaling est 
signalé par [Saporta, 1990] sous le 
terme de « méthode des moyennes 
réciproques » (cf.  p.215)
Commentaire [DD11] :  
L’algorithme de la puissance 
itérée est souvent utilisé en 
pratique pour rechercher la valeur 
propre dominante .
des quan nues, enfin normaliser ces scores individuels ce qui 
termine l’itération. 
Pour l’itération l, on a donc  les étapes suivantes : 
 1 : ( )lGxDy 1:~ −=   
 2 yGx ~1:~
p
=  
 3 : ( ) ( )1 ~~~ 21: xxx , sous la condition 0−+ ′=x l =′ x1  
La réitération de ce cycle produit des scores x~  et des quantifications y~  qui, au bout d’un 
certain nombre d’itératio s, odifient plus de manière détectable. Ce couple de vecteurs 
stationnaires ),( ∗∗ yx  représe
n  ne se m
ors l’optimum recherché et la norme nte al (~ ~ ) 21xx′ du vecteur x~  
stationnarisé nous t la plus grande v  propre 1 fourni aleur λ  et donc la valeur minimale de la 
fonction ertde p e de pouvoir discriminant. 
our la minimisation de la perte d’homogénéité, ce sont exactem es mêmes étapes qui P ent l
alternent :  
1 : ( )l
p
Gyx 1:~ =  
 2 : D ~~ 1−= xGy :   
 mais cette fois ci la norm ur y  au moyen de la transformation : alisation porte s
 3 : ( ) ( ) 211 ~~~: −+ ′= yDyyy l , sous la condition 0=′ y1  
 
Le schéma d’alter m s opérées yse d’homogénéité est 
issu de ces deux formulations distinctes du problème d’opt isation : la première formulation 
nance duale des transfor ation  d
im
ans l’anal
en termes de perte d’homogénéité ; la seconde en termes de perte de pouvoir discriminant. Il 
n’est cependant pas possible pour des raisons tenant à la géométrie de la solution d’obtenir Commentaire [DD12] : cf. 
les relations pseudo-une normalisation simultanée des deux vecteurs ),( ∗∗ yx  optimaux.  
 
A2.3.13 L’algorithm
barycentriques.
e des moindres carrés alternés 
L’algorithme itératif des moindres carrés alte és ( liqué dans la procédure 
HOMALS  utilise la première version de cette éthode pour converger vers une solution 
stationnaire ),( ∗∗ yx  qui minimise la perte globale d’homogénéité : 
 ( )
MCA) imprn
m
( ) ( )jjp jj yGxyGxyx −′−= ∑
jp =1
2 1,σ   
on 1x . 
Si la recherch rie sous la contrainte 1
 
sous la contrainte de normalisati =′x
′ =e des solutions doit s’effectuer en théo xx , elle 
s’effectue en pratique sous la contrainte n′ =xx , en utilisant la normalisation 
( )
© Revue MODULAD, 2008 - 240 - Numéro 38 
( ) xxx ~~~: 21−′= nl  .  
L’implantation de l’algorithme MCA   se déroule donc selon les étapes 
suivantes : 
0) initialisatio  : 
cteur ini
x~
sous HOMALS
n
(a. tirage aléatoire du ve tial )0~
 
x  dans une loi uniforme de moyenne nulle 
et de variance n  
b. calcul du vecteur initial  ( )0 :~ Dy =
   
( )01 ~xG′−  
1) calcul du vecteur des scores : ( )1~1~ −← ly
p
Gx  
2) no tion (de variance (rmalisa  n)   ( ) ) xxx ~~:~ 21−=l  
cation : 
x~′n
( ) ( )  3) calcul du vecteur des quantifi ll xGDy ~:~ 1 ′= −
4) test de convergence :   ( ) ( ) ε≤1  et/ou  − −~~ ll xx ( ) ( ) ε≤−1~ ly  
L’algo ationnaire ),( ∗∗ yx  : 
1vx =∗  
−~ ly
rithme converge vers la solution st
1
21
1 uDy
−∗ = λ
 
 
En raison de la transformation  ( ) ( )ll xGDy ~:~ 1 ′= −  appliquée à chaque itération, cette solution 
satisfait la « condition de statio rité » exprimée par l’équation xGyD jjj = . 
 
A3  Analyse multivariée de l’homogénéité 
 
A3.1 Représentation en plusieurs dimensions 
Compte-ten
nna
u de la complexité des inter-relations entre objets et variables catégorielles 
un aju
isé selon une seule dimension peut apparaître comme 
hénomènes observés. Une solution plus satisfaisante du 
problème de représentation peut alors être fournie par l’utilisation d’une image euclidienne à 
plusieurs dimensions. La recherche des différentes dimensions de la solution est réalisée en 
 en dimension  aux 
solutions trouvées précédemment. Ainsi, on recherchera une variable synthétique 2x , 
orthogonale à la première direction identifiée par la variable synthétique 1x , correspondant à 
intervenant dans certains phénomènes, stement des scores individuels et des 
quantifications catégorielles réal
insuffisant pour rendre compte des p
effectuant successivement l’analyse de l’homogénéité  1 orthogonalement
la plus grande valeur propre 1λ . Il convient alors de rechercher le minimum de la fonction de 
perte ( )22 ,yxσ  sous contra é inte d’orthogonalit
© Revue MODULAD, 2008 - 241 - Numéro 38 
021 =xx , ce que nous expri ons 
me 
 pe
m
vect ellement par 021 =xx . Pour rechercher une troisiè variable synthétique 3x , on 
minimisera la fonction de rte ( )33 ,yxσ  sous les contraintes 031 =
ori ′
′xx  et 032 =′xx . 
Cette recherche peut s’effectuer en utilisant les solutions fournies par la décomposition en 
valeurs singulières (DVS ce V  est une matrice ort pIV =hogonale ( V) car la matri ′  ). Les 
 d’orthogonalité et sont, co evecteurs-colonnes de V  satisfont les contraintes mme v cteurs 
propres, solutions successives des différentes étapes du programme d’optimisation  que l’on 
peut mener jusqu’au rang r de la matrice Z  (DVS « stricte ») ou jusqu’à p solutions incluant 
les vecteurs propres correspondant aux valeurs propres nulles ZZ ′  (DVS « étendue »).  
 
A3.2 Analyse de l’homogénéité selon plusieurs dimensions 
Cet ajustement, que nous avons effectué jusqu’ici en utilisant un sous-espace de dimension 1 
(le vecteur x  des scores), peut être réalisé sans perte de généralité dans un sous-espace à s 
dimensions, ds ≤<1 , en utilisant une matrice X  des scores individuels (coordonnées des n 
objets selon les s dimensions)  à sn×  coefficients et une matrice Y  à sp×  coefficients 
appelée matrice des quantifications catégorielles (coordonnées des p critères  selon les s 
dimensions). 
En formant à partir des s couples solutions 
j
( )jj yx ;  le couple de matrices ( )YX; , on définit 
une approximation d’ordre s de la matrice Z  par : 2121 DYXUVHDZ ′≅′Λ== . 21−
Les vecteurs-colonn  mes de la atrice V  une base orthonormée de l’espace v ct
ères ét iés, représentés par les vecteurs-colonnes de H . Les s premiers 
 de V  uant la matrice X  forment une base orthonormée de l’espace 
 par les vecte   issus des transformations linéaires 
 forment e oriel 
Commentaire [DD13] : Z =
. Cette approximation est-elle la 
meilleure approximation de rang s 
au sens du critère des moindres 
carrés (Théorème d’Eckart-
Young) ? 
 
généré par les cr ud
vecteurs-colonnes constit
it
vectoriel généré urs-colonnes de Y
appliquées aux critères étudiés. 
On réalise ainsi une analyse de l’homogénéité en dimension s. 
 
A3.3 Qualité de l’ajustement 
La qualité de l’ajustement réalisé par l’approximation d’ordre s peut être mesurée en utilisant 
22 ∑= ijF aA  ) comme norme matricielle. On peut alors comparer la norme de Frobenius (
, ji
21−D m= HZ  à son approxi ation d’ordre s, 21DYX ′ en formant le rapport de leurs normes 
respectives : 
( ) ( −= tr 12 HDZ ) ( ) ∑
=
=′=′=′
p
trtr λVΛVHZZ
j
jF
1
 
et 
( ) ( ) ∑s2
=
=′=′′=′
j
jF
trtr
1
21 λYDYXYDYXDYX . 
Les vecteurs-colonnes de 21−= HDZ  étant standardisés ( [ ]HHD ′= diag ), on en conclut que  
: p
p
j
jF
==∑
=
j j
ciellement
1
2 λZ .  
 
A3.4 Opérateurs de transformation  
La matrice jG , d’ordre jkn × , regroupant les vecteurs booléens associés aux indicatrices de 
codage du critère qualitatif  j  à  kj modalités, la transformation t  du vecteur h  peut se 
définir matri  par ( ) jjjj j sk j×  coefficients appelée 
matrice des q ications ca les (coordonnées des catégories  du critère j selon les s 
dimensions). 
pert variée, s’expri trices jY
t YGh =  où Y  est une matrice à 
uantif tégoriel
  
A3.5 Formulation globale de la fonction de perte multivariée 
L’analyse multivariée de l’homogénéité  consiste alors  minimiser une fonction globale de 
e multi mant en fonction des ma  et X  en étendant la définition de 
à
la fonction de perte globale univariée  (cf. supra partie I):  
( ) ( ) ( )∑
=
−′−=
j
jjjjp 1
2 yGxyGx   
En utilisan nsi
définit une no pour la matrice 
∑
=
−
j
jjp 1
yGx=
pp
2 11,yxσ
2
t la norme de Frobenius comme exte on de la norme euclidiennne à sn×ℜ , on 
rme quadratique ( ) d’ordre sn×jjYGX −  qui permet de 
ltiv iée comme une fonctiospécifier la fonction de perte globale mu ar n quadratique  
( ) ( ) ( ) ( )∑ ⎥⎦⎤⎢⎣⎡ ′−j jjjjF tracepp 1 YGXYGX  
à minim
s 
ritères 
L’opérateur trace utilisé pour la matrice ( )
∑ −=−= pp jj 22 11, YGXYXσ
==j 1
iser sous les contraintes 
d’ortho-normalisation sIn=′XX   
de centrage 0X1 =′ , 
permettant d’éviter les solutions triviales dans la r
correspondant à 0X =  et  0Y =j  pour chacun des c j. 
ésolution de ce système d’équation
( )jjjj GXYGX −′ Y−  d’ordre s s× , défini comme 
 somme des éléments diagonaux d’une matrice carré symétrique, à l’avantage d’être la
© Revue MODUL - 242 - Numéro 38AD, 2008   
invariant par changement de base e qui nous assure une solution indépendante 
du système de représentation choisi à un déplacement près (translatio ota
 orthonormée, c
n et/ou r tion). 
 A s 
ol  par l’algorithme des moindres carrés 
 consiste à minimiser alternativement la 
 
A3.6 lgorithme matriciel  des moindres carrés alterné
La s ution à ce problème d’optimisation est fournie
alternés (ALS – Alternating Least Squares) qui
fonction de perte conditionnellement aux matrices jY  et X  : 
Initialisation :  tirer une matrice aléatoire ( ) 00 :
~ XX ==l  telle que 0X1 =′ 0  et 
sn IXX =′ 00  
Itération l : 
Etape l.1 :  minimiser la fonction de perte conditionnellement à  jY  pour X  fixé. 
avec la transformation ( ) Jjljjj ∈′= − XGDY ~:~ 1   [5] 
où jjj GGD ′=  est la matrice diagonale d’ordre jj kk ×  contenant les 
effectifs marginaux des catégories du critère  j. 
Etape l.2 :  minimis r la fonction de perte conditionnellement à X  en ayant fixé les 
matrices  jY
~  avec la transforma
e
tion : ∑P
j
jjp 1
~1 YG  
=
=:Z
Etape l.3 :  centrer  la   avec la transforma
© Revue M AD, 2008 - 243 -  ODUL Numéro 38
matri tion ce Z ( )Z ′=
n
1:~ Z11Z −  
 
Etape l.4 :  orthonormaliser  la atrice Z~   avec la transforma Gram-
Schmidt  ( )
m tion de ( )ZX ~:~ 1 GRAMnl =+  
Test :    tester la stationnarité e la solution   ( ) ( ) d ε≤−+ ll XX ~~ 1  ? 
 
Les éta es 2 à 4 sont réitérées jusqu’à ce qu e au test de stationnarité, 
otat
nale par rotation R  (transformation telle que dIRRRR
e la matrice X  satisfassp
indiquant qu’un optimum est atteint. 
L’algorithme converge ainsi vers un couple de matrices stationnaires qui est la solution 
fournie par la procédure HOMALS pour le problème d’optimisation défini précédemment. 
 
A3.7 Invariance de la solution par r ion 
Si l’on utilise une base différente dans l’espace des colonnes de la matrice X  issue de la 
solution origi ′ ′= = ), alors les  
R  et  matrices issues de cette transformation XX =( XGDY (( jjj ′= −1  sont égale t optimales 
pour la fonction de perte d’homogénéité. 
men
  
A3.8 Normalisation 
Les colonnes de la matrice des scores individuels X  sont centrées : soustraction de la 
moyenne dx.  à chaque valeur x , soit matriciellem nt e ( )X11XX ′−= 1~ . Puis la matrice X~  id n
-Schmidt st
Modifi  Gram-Schmidt) ou par une factorisation QR. 
abilisée (MGS - est orthonormalisée par la procédure d’orthogonalisation de Gram
ed
 En raison de ces exigences modestes en ressources de calcul, la procédure 
cédure H
onstruction d
méthode Q erne » d
et R une 
En fixant 
d’orthogonalisation de Gram-Schmidt stabilisée est en règle générale utilisée par les 
programmes implantant la pro OMALS. Cependant, si l’orthonormalité est requise de 
manière critique dans la c e la solution optimale, il est préférable de recourir à la 
R de factorisation, méthode « mod e résolution des problèmes de moindres 
carrés  où Q désigne une matrice orthogonale matrice triangulaire inférieure.   
© Revue MODULAD, 2008 - 244 - Numéro 38 
XX ~n= , l’étape de normalisation conduit à sn IX =′∗ X ∗∗ . 
 
A3.9 Déco leurs singulières 
Le prob e d’optimisation de la fonction multivariée de perte d’ho
( )
mposition en va
lèm mogénéité : 
( ) ( )∑
= ⎥
⎤⎢⎣
⎡ −′−=
p
jjjjtr
2 1, YGXYGXYXσ  
ion dIn=′XX   
de centrage 0X1 =′ , 
 décomposition en valeurs singulières. En effet, on peut 
⎦jp 1
sous les contraintes 
malisatd’ortho-nor
peut être assimilé à un problème de
∗  est montrer que la matrice X constituée par les vecteurs singuliers à gauche de la matrice  
21DG11I −− ⎟⎠
⎞⎜⎝
⎛ −
n
p 21 , où 
′
⎥⎦
⎤⎢⎣
⎡= pj GGGG LL1  est le tableau disjonctif complet corrigé.  
La décomposition complète en valeurs singulières possède ∑
=
es ind ntifications catégorielles en utilisant 
e 
nsi l’effi que assez 
médiocre des m nés. 
 
Logo du Da versité de Leiden, Faculté des Sciences Sociales et du 
−=
p
j
j pkd
1
 dimensions. 
près extractiA on des scor ividuels, on calcule les qua
l’équation : XGDY jjj ′= −1 . 
L’avantage de l’algorithme des moindres carrés alternés est de pouvoir travailler seulement 
sur les s premières dimensions  requises en supposant qu s soit très petit devant d ( ds <<  ), 
minimisant les besoins en taille mémoire pour améliorer ai cacité algorithmi
oindres carrés alter
ta Theory Group (Uni
Comportement, Prof. Jacqueline Meulmann): 
 
 
Source ences.leidenuniv.nl/educationandchildstudies/datatheory/  : http://www.socialsci
