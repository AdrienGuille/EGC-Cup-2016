QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
Gilbert Ritschardâˆ—, Djamel A. Zighedâˆ—âˆ—
âˆ—DeÂ´partement dâ€™eÂ´conomeÂ´trie, UniversiteÂ´ de Gene`ve
gilbert.ritschard@themes.unige.ch http://mephisto.unige.ch
âˆ—âˆ—Laboratoire ERIC, UniversiteÂ´ Lumie`re Lyon 2
zighed@univ-lyon2.fr http://eric.univ-lyon2.fr
ReÂ´sumeÂ´. Cet article discute des possibiliteÂ´s de mesurer la qualiteÂ´ de
lâ€™ajustement dâ€™arbres dâ€™induction aux donneÂ´es comme cela se fait classi-
quement pour les mode`les statistiques. Nous montrons comment adapter
aux arbres dâ€™induction les statistiques du khi-2, notamment celle du rap-
port de vraisemblance utiliseÂ´e dans le cadre de la modeÂ´lisation de tables
de contingence. Cette statistique permet de tester lâ€™ajustement du mode`le,
mais aussi lâ€™ameÂ´lioration de lâ€™ajustement quâ€™apporte la complexification
de lâ€™arbre. Nous en deÂ´duisons eÂ´galement des formes adapteÂ´es des crite`res
dâ€™information AIC et BIC qui permettent de seÂ´lectionner le meilleur arbre
en termes de compromis entre ajustement et complexiteÂ´. Nous illustrons
la mise en Å“uvre pratique des statistiques et indicateurs proposeÂ´s avec un
exemple reÂ´el.
Mots cleÂ´s : arbre dâ€™induction, qualiteÂ´ dâ€™ajustement, tests du khi-2, com-
paraison dâ€™arbres
1 Introduction
Les arbres dâ€™induction (Kass, 1980; Breiman et al., 1984; Quinlan, 1993; Zighed et
Rakotomalala, 2000) sont lâ€™un des outils les plus populaires dâ€™apprentissage superviseÂ´.
Ils consistent a` rechercher par eÂ´clatements successifs de sommets, une partition de lâ€™en-
semble des combinaisons de valeurs des preÂ´dicteurs optimale pour preÂ´dire la variable
reÂ´ponse. La preÂ´diction se fait simplement en choisissant, dans chaque classe de la par-
tition obtenue, la modaliteÂ´ la plus freÂ´quente de la variable a` preÂ´dire. Bien que leur
utilisation premie`re soit la geÂ´neÂ´ration dâ€™arbres de deÂ´cisions pour la classification, les
arbres dâ€™induction fournissent une description de la facÂ¸on dont la distribution de la va-
riable a` preÂ´dire est conditionneÂ´e par les valeurs des preÂ´dicteurs. Ils nous indiquent par
exemple comment la reÂ´partition entre clients solvables et insolvables est influenceÂ´e par
les attributs aË†ge, sexe, niveau dâ€™eÂ´ducation, profession, etc. En ce sens les arbres dâ€™induc-
tion sont donc des outils de modeÂ´lisation de lâ€™influence des preÂ´dicteurs sur la variable
a` preÂ´dire au meË†me titre que par exemple la reÂ´gression lineÂ´aire, la reÂ´gression logistique
ou la modeÂ´lisation log-lineÂ´aire de tables de contingence multi-dimensionnelles. Câ€™est
essentiellement a` cet aspect de modeÂ´lisation descriptive, et en particulier a` lâ€™eÂ´valuation
de la qualiteÂ´ de la description fournie par un arbre induit que nous nous inteÂ´ressons
dans cet article.
En modeÂ´lisation statistique, quâ€™il sâ€™agisse de reÂ´gression lineÂ´aire, dâ€™analyse discri-
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
minante, de reÂ´gression logistique ou plus geÂ´neÂ´ralement de mode`le lineÂ´aire geÂ´neÂ´raliseÂ´
(GLM), il est dâ€™usage dâ€™eÂ´valuer la qualiteÂ´ dâ€™ajustement du mode`le, câ€™est-a`-dire la qua-
liteÂ´ de la description fournie par le mode`le, avec des mesures descriptives telles que le
coefficient de deÂ´termination R2 ou des pseudo R2, et avec des statistiques de test telles
que les khi-2 du score test, de Wald ou du rapport de vraisemblance. Parmi ces der-
nie`res, la statistique du rapport de vraisemblance jouit dâ€™une proprieÂ´teÂ´ dâ€™additiviteÂ´ qui
permet dâ€™eÂ´valuer eÂ´galement la pertinence statistique de la simplification dâ€™un mode`le
de reÂ´feÂ´rence par renforcement de contraintes sur ses parame`tres, ou, si lâ€™on regarde les
choses dans lâ€™autre sens, la significativiteÂ´ statistique de la complexification reÂ´sultant de
lâ€™ajout de parame`tres a` un mode`le donneÂ´.
Le cas particulier des tests de significativiteÂ´ globale de lâ€™explication, parfois appeleÂ´s
â€œtests omnibusâ€, ou` lâ€™on teste globalement lâ€™apport des facteurs explicatifs par rapport
a` un mode`le de reÂ´feÂ´rence naÂ¨Ä±f â€” le mode`le avec la constante comme seule variable ex-
plicative dans le cas de la reÂ´gression, le mode`le dâ€™indeÂ´pendance dans le cas des mode`les
log-lineÂ´aires â€” retiendra notre attention. Dans le cas des arbres dâ€™induction, le mode`le
de reÂ´feÂ´rence est naturellement lâ€™arbre de niveau 0 constitueÂ´ par le seul nÅ“ud initial.
Une des difficulteÂ´s principales a` laquelle on se heurte dans la pratique des arbres
ou graphes dâ€™induction est le fort degreÂ´ de complexiteÂ´ des arbres mis en eÂ´vidence. Il
nous parait alors souhaitable de pouvoir disposer aussi de crite`res tels que le crite`re
dâ€™information AIC dâ€™Akaike (1973) ou le crite`re dâ€™information bayeÂ´sien BIC (Schwarz,
1978; Kass et Raftery, 1995). Ces crite`res sont une combinaison de la qualiteÂ´ dâ€™ajuste-
ment (statistique du rapport de vraisemblance) et dâ€™une mesure de la complexiteÂ´. Ils
sâ€™ave`rent ainsi une aide preÂ´cieuse pour arbitrer entre complexiteÂ´ et qualiteÂ´ dâ€™ajustement
dans la seÂ´lection de mode`les.
Lâ€™article est organiseÂ´ comme suit. La section 2 situe la place des mesures de qualiteÂ´
dâ€™ajustement envisageÂ´s parmi les mesures classiques de qualiteÂ´ dâ€™un arbre dâ€™induc-
tion. La section 3 preÂ´cise le concept dâ€™ajustement consideÂ´reÂ´. La section 4 est consacreÂ´e
aux crite`res de qualiteÂ´ dâ€™ajustement. On montre comment les statistiques du khi-2 de
Pearson et du rapport de vraisemblance utiliseÂ´es dans le cadre de la modeÂ´lisation de
tables de contingence peuvent sâ€™adapter aux arbres dâ€™induction. Nous discutons ensuite
lâ€™ameÂ´lioration quâ€™apporte un mode`le par rapport a` un mode`le de reÂ´feÂ´rence. On montre
comment exploiter la diffeÂ´rence des statistiques G2 du rapport de vraisemblance pour
tester la significativiteÂ´ statistique du gain dâ€™information et proposons des indicateurs
de type R2. Nous montrons eÂ´galement comment appliquer les crite`res dâ€™information
dâ€™Akaike (AIC) et bayeÂ´sien (BIC) aux arbres dâ€™induction et discutons leur inteÂ´reË†t pour
guider le choix entre mode`les de complexiteÂ´ variable. La section 5 illustre lâ€™utilisation
des crite`res proposeÂ´s dans un cas concret et la section 6 valide empiriquement le calcul
des degreÂ´s de liberteÂ´ et la distribution des statistiques du khi-2 pour arbres. Fina-
lement, la conclusion fait lâ€™objet de la section 7 ou` nous mentionnons des pistes de
deÂ´veloppements futurs de la deÂ´marche initieÂ´e dans cet article.
2 Arbre dâ€™induction et mesures classiques de qualiteÂ´
Avant de nous concentrer sur la qualiteÂ´ dâ€™ajustement, nous rappelons brie`vement
le principe des arbres dâ€™induction et leurs crite`res usuels de qualiteÂ´. Ceci dans le but
RNTI - 2
G. Ritschard et D.A. Zighed
de pouvoir mieux situer le roË†le des mesures de qualiteÂ´ dâ€™ajustement proposeÂ´es dans cet
article. Le lecteur inteÂ´resseÂ´ trouvera par une discussion approfondie des arbres et de
leurs crite`res habituels de qualiteÂ´ par exemple dans Zighed et Rakotomalala (2000).
2.1 Rappel du principe des arbres dâ€™inductions
Lâ€™objectif est de construire une re`gle qui permette, a` partir de la connaissance dâ€™un
vecteur dâ€™attributs x = (x1, . . . , xp), de preÂ´dire une variable reÂ´ponse y, ou si lâ€™on preÂ´fe`re
de classer les cas selon les eÂ´tats de la variable y. La construction de la re`gle se fait en
deux temps. Dans un premier temps, on deÂ´termine une partition des valeurs possibles
de x telle que la distribution de la reÂ´ponse y soit la plus pure possible dans chaque
classe de la partition, ou de facÂ¸on plus ou moins eÂ´quivalente la plus diffeÂ´rente possible
dâ€™une classe a` lâ€™autre. La re`gle consiste ensuite a` attribuer a` chaque cas la valeur de y
la plus freÂ´quente dans sa classe.
Les arbres dâ€™induction deÂ´terminent la partition par eÂ´clatements successifs des som-
mets. En partant du sommet initial, ils recherchent lâ€™attribut qui permet le meilleur
eÂ´clatement selon un crite`re qui peut eË†tre par exemple le gain dâ€™entropie (C4.5, Sipina)
ou la significativiteÂ´ dâ€™un khi-2 (CHAID). Lâ€™opeÂ´ration est reÂ´peÂ´teÂ´e a` chaque nouveau
sommet jusquâ€™a` ce quâ€™un crite`re dâ€™arreË†t, une taille minimale du sommet par exemple,
soit atteint. Le reÂ´sultat est un arbre tel que celui preÂ´senteÂ´ a` la figure 1.
2.2 Taux dâ€™erreur
Le taux dâ€™erreur de classification, câ€™est-a`-dire le pourcentage de cas mal classeÂ´s est
peut-eË†tre le crite`re de qualiteÂ´ le plus utiliseÂ´. Il mesure la performance preÂ´dictive du
mode`le. Sâ€™agissant de classification, câ€™est eÂ´videmment la performance en geÂ´neÂ´ralisation
qui importe, câ€™est-a`-dire la performance pour des cas nâ€™ayant pas servi a` lâ€™apprentissage.
Câ€™est pourquoi il convient de calculer le taux dâ€™erreur sur un eÂ´chantillon de validation
diffeÂ´rent de lâ€™eÂ´chantillon dâ€™apprentissage. Pratiquement, ceci conduit a` partitionner
lâ€™ensemble de donneÂ´es en deux parties, lâ€™une servant a` lâ€™apprentissage, lâ€™autre a` la
validation.
Le taux dâ€™erreur est souvent eÂ´valueÂ´ par la validation croiseÂ´e qui donne eÂ´galement
des indications sur sa variabiliteÂ´. Cette meÂ´thode consiste a` partager les donneÂ´es en
 G46
 

 
 
	 
 
 
 

	 
	 
	 
Fig. 1 â€“ Arbre induit
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
par exemple 10 groupes de tailles approximativement eÂ´gales et a` reÂ´peÂ´ter ensuite lâ€™ap-
prentissage en reÂ´servant a` chaque fois un groupe diffeÂ´rent pour la validation. Le taux
dâ€™erreur en validation croiseÂ´e est la moyenne des taux obtenus. Notons que comme on
peut obtenir a` chaque fois un arbre diffeÂ´rent lâ€™on deÂ´termine ainsi le taux dâ€™erreur de la
meÂ´thode de construction de lâ€™arbre et non lâ€™erreur dâ€™un arbre induit particulier.
La deÂ´composition biais/variance/erreur reÂ´siduelle du taux dâ€™erreur (Geurts, 2002)
fournit des indications preÂ´cieuses sur la part de lâ€™erreur due a` la variabiliteÂ´ de lâ€™eÂ´chan-
tillon dâ€™apprentissage. Ces deÂ´compositions ne sont cependant quantifiables que numeÂ´-
riquement sur la base de simulations ou de meÂ´thodes de reÂ´eÂ´chantillonnage ce qui limite
leur utilisation systeÂ´matique.
2.3 QualiteÂ´ des partitions
La qualiteÂ´ dâ€™une partition est dâ€™autant meilleure que les sommets terminaux sont
purs, câ€™est-a`-dire quâ€™ils ont des distributions le plus proche possible de la distribution
deÂ´geÂ´neÂ´reÂ´e qui donne un poids de un a` lâ€™une des modaliteÂ´s et zeÂ´ro aux autres. Cette
configuration limite en effet lâ€™incertitude quant a` la valeur de la variable reÂ´ponse a`
lâ€™inteÂ´rieur de chaque sommet. Les distributions doivent eÂ´galement eË†tre diffeÂ´rentes dâ€™un
sommet a` lâ€™autre.
Il existe plusieurs facÂ¸ons de mesurer la qualiteÂ´ des partitions obtenues (voir Zighed
et Rakotomalala, 2000, chap. 9.) On peut mentionner :
â€“ Les mesures issues de la theÂ´orie de lâ€™information et qui consistent essentiellement
a` mesurer lâ€™entropie de la reÂ´ponse pour la partition consideÂ´reÂ´e.
â€“ Celles qui se fondent sur des distances entre distributions de probabiliteÂ´s. Le prin-
cipe consiste ici a` veÂ´rifier que les distributions diffe`rent le plus possible dâ€™une
classe a` lâ€™autre. La deÂ´marche est simple dans le cas de deux classes. Pour le cas
plus geÂ´neÂ´ral dâ€™un nombre quelconque de classes, les solutions proposeÂ´es restent
de porteÂ´e assez limiteÂ´e.
â€“ Enfin, les mesures qui sâ€™appuient sur des indices dâ€™association. Lâ€™ideÂ´e est ici que la
partition est dâ€™autant meilleure que lâ€™association entre les classes de la partition et
la variable reÂ´ponse est forte. Rakotomalala et Zighed (1998) proposent dâ€™utiliser
le degreÂ´ de signification du Ï„ de Goodman et Kruskal ce qui permet de tenir
compte eÂ´galement de la taille des eÂ´chantillons.
2.4 ComplexiteÂ´
Un des inteÂ´reË†ts majeurs souvent avanceÂ´ des arbres et graphes dâ€™induction est la
faciliteÂ´ de leur interpreÂ´tation. Ceci est vrai tant que la complexiteÂ´ de lâ€™arbre reste
limiteÂ´e, dâ€™ou` lâ€™inteÂ´reË†t de mesurer cette complexiteÂ´. Les indicateurs couramment utiliseÂ´s
sont :
â€“ Le nombre de sommets terminaux. Ce crite`re correspond au nombre de re`gles de
preÂ´diction ainsi quâ€™au nombre de classes de la partition finale.
â€“ La profondeur ou le nombre de niveaux de lâ€™arbre. Ce crite`re deÂ´pend de la pro-
ceÂ´dure de construction. Un arbre n-aire peut deÂ´finir une meË†me partition avec un
nombre plus petit de niveaux quâ€™un arbre binaire.
RNTI - 2
G. Ritschard et D.A. Zighed
â€“ Le nombre de nÅ“uds. Ce crite`re est eÂ´galement lieÂ´ a` la proceÂ´dure de construction. Il
sera en geÂ´neÂ´ral plus eÂ´leveÂ´ pour un arbre binaire qui tend a` multiplier les sommets
intermeÂ´diaires. Ce crite`re refle`te bien la complexiteÂ´ visuelle de lâ€™arbre induit.
â€“ La longueur des messages. Traduit la complexiteÂ´ des re`gles dâ€™affectation aux dif-
feÂ´rentes classes.
2.5 Place des mesures de qualiteÂ´ dâ€™ajustement envisageÂ´es
Les nouveaux crite`res de la qualiteÂ´ dâ€™ajustement proposeÂ´s dans cet article concernent
la capaciteÂ´ de lâ€™arbre a` reproduire la distribution de la reÂ´ponse y pour les individus
ayant un profil x donneÂ´. En dâ€™autres termes, on sâ€™inteÂ´resse a` la qualiteÂ´ de reproduction
de la table de contingence qui croise la variable reÂ´ponse y avec lâ€™ensemble des preÂ´dic-
teurs. Il sâ€™agit donc de la qualiteÂ´ descriptive de lâ€™arbre par opposition a` sa performance
en classification. On conside`re en particulier les deux aspects suivants :
â€“ lâ€™aptitude de lâ€™arbre induit a` deÂ´crire la distribution de la variable reÂ´ponse condi-
tionnellement aux valeurs prises par les preÂ´dicteurs,
â€“ le gain dâ€™information quâ€™apporte lâ€™arbre induit par rapport au nÅ“ud initial ou`
lâ€™on ne tient pas compte des preÂ´dicteurs.
On peut situer les mesures dâ€™ajustement qui nous occupent dans lâ€™optique Â«qualiteÂ´
des partitionsÂ» de la typologie preÂ´ceÂ´dente. Il sâ€™agit en effet de voir comment la partition
induite par lâ€™arbre ajuste la table cible. Les crite`res de qualiteÂ´ de partition mentionneÂ´s
en 2.3 ne mesurent pas explicitement cet ajustement, bien quâ€™elles sâ€™y reÂ´fe`rent implici-
tement. Lâ€™entropie de la reÂ´ponse pour la partition induite aussi bien que lâ€™association
entre la partition et la variable reÂ´ponse constituent en effet des mesures de la proxi-
miteÂ´ entre les distributions preÂ´dites par lâ€™arbre et les distributions cibles. Notre objectif
est cependant de proposer des mesures dâ€™ajustement qui sâ€™apparentent a` celles utili-
seÂ´es en modeÂ´lisation statistique et qui se preË†tent a` lâ€™infeÂ´rence statistique, le test de
significativiteÂ´ du deÂ´faut dâ€™ajustement ou le test de la diffeÂ´rence entre deux arbres en
particulier.
La qualiteÂ´ dâ€™ajustement de la table des donneÂ´es dâ€™apprentissage doit eË†tre mise en
paralle`le avec la stabiliteÂ´ de la description fournie, câ€™est-a`-dire la simpliciteÂ´ de lâ€™arbre.
En effet, en deÂ´veloppant trop loin un arbre, on a tendance a` deÂ´crire les speÂ´cificiteÂ´s de
lâ€™eÂ´chantillon plutoË†t que la structure du pheÂ´nome`ne sous-jacent. Un arbre trop complexe
deÂ´pend trop eÂ´troitement de lâ€™eÂ´chantillon pour fournir une description geÂ´neÂ´ralisable des
liens liant la variable reÂ´ponse aux preÂ´dicteurs. On cherchera ainsi par exemple lâ€™arbre le
plus simple qui ajuste la table de facÂ¸on satisfaisante. Alternativement, on peut sâ€™inteÂ´-
resser a` lâ€™arbre qui offre le meilleur compromis entre qualiteÂ´ dâ€™ajustement et complexiteÂ´.
Nous proposons a` cet effet des adaptations pour les arbres des crite`res dâ€™information
AIC et BIC, crite`res qui par construction rele`vent simultaneÂ´ment de la complexiteÂ´ et de
la qualiteÂ´ des partitions. Les adaptations proposeÂ´es reposent sur le mode`le parameÂ´treÂ´
de reconstruction des donneÂ´es a` partir de lâ€™arbre que nous introduisons a` la section 4.3
et qui permet de mesurer la complexiteÂ´ de lâ€™arbre en termes de nombre de parame`tres.
Cette dernie`re possibiliteÂ´ constitue en soi une contribution a` la panoplie des mesures
de complexiteÂ´.
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
3 Concept de qualiteÂ´ dâ€™ajustement dâ€™un arbre
3.1 Table cible et table preÂ´dite
De facÂ¸on geÂ´neÂ´rale, la qualiteÂ´ dâ€™ajustement dâ€™un mode`le se reÂ´fe`re a` sa capaciteÂ´ a`
reproduire les donneÂ´es. Dans le cas de la preÂ´diction quantitative dâ€™une variable Y , par
exemple dans le cas de la reÂ´gression lineÂ´aire, lâ€™objectif est clair. Il sâ€™agit dâ€™obtenir des
valeurs preÂ´dites yË†Î± qui sâ€™ajustent le mieux possible aux valeurs observeÂ´es yÎ±, pour Î± =
1, . . . , n, n eÂ´tant le nombre dâ€™observations. De meË†me, dans lâ€™optique de la classification,
les eÂ´tats preÂ´dits yË†Î± doivent correspondre le plus souvent possible aux vraies valeurs yÎ±.
Le taux dâ€™erreur est dans ce cas un indicateur naturel de qualiteÂ´ dâ€™ajustement.
Dans certaines situations, en particulier dans les sciences de comportement (socio-
logie, sciences politiques, marketing, ...), les arbres ou graphes dâ€™induction sont utiliseÂ´s
plus dans une optique descriptive que preÂ´dictive comme outil de mise en eÂ´vidence des
principaux deÂ´terminants de la variable a` preÂ´dire. Ils sont utiliseÂ´s comme outil dâ€™aide a`
la compreÂ´hension de pheÂ´nome`nes et non pas comme outil de classification.
Ce ne sont plus alors les eÂ´tats particuliers yÎ± que lâ€™on cherche a` reproduire. Pour
comprendre comment les preÂ´dicteurs interagissent sur la variable reÂ´ponse Y , il convient
en effet dâ€™examiner comment la distribution de Y change avec le profil x. Dans cette
optique, la qualiteÂ´ dâ€™ajustement consideÂ´reÂ´e ici se reÂ´fe`re a` la qualiteÂ´ de la reproduction
de lâ€™ensemble des distributions conditionnelles.
Formellement, dans lâ€™optique classification il sâ€™agit de caracteÂ´riser une fonction f(x)
qui preÂ´dit la valeur de y compte tenu de x. Avec les arbres, la construction de cette
fonction se fait en deux eÂ´tapes :
1. CaracteÂ´riser un mode`le descriptif p(Y |x) = (p1(x), . . . , p`(x)), ou` la notation
pi(x) deÂ´signe la probabiliteÂ´ conditionnelle p(Y = yi|x).
2. PreÂ´dire par la re`gle majoritaire f(x) = argmaxi pi(x).
Contrairement a` la reÂ´gression logistique par exemple, ou` p(Y |x) sâ€™exprime analytique-
ment en fonction de x, le mode`le descriptif prend ici la forme non parameÂ´trique dâ€™un
ensemble fini de distributions :{
p|j âˆˆ [0, 1]` | p|j = p(Y |xj), j = 1, . . . , c
}
.
Les preÂ´dicteurs eÂ´tant supposeÂ´s prendre un nombre fini de valeurs, lâ€™ensemble des dis-
tributions conditionnelles observeÂ´es est repreÂ´sentable sous forme dâ€™une table de contin-
gence T croisant y avec la variable composite deÂ´finie par le croisement de tous les
preÂ´dicteurs. Le tableau 1 est un exemple dâ€™une telle table dans le cas ou` la variable
a` preÂ´dire est le statut marital et les preÂ´dicteurs le genre et le secteur dâ€™activiteÂ´. Le
nombre de lignes de T est le nombre ` dâ€™eÂ´tats de la variable Y . Si chaque preÂ´dicteur
xÎ½ , Î½ = 1, . . . , p a cv valeurs diffeÂ´rentes, le nombre c de colonnes de T est au plus le
produit des cv, soit : c â‰¤
âˆ
Î½ cÎ½ , lâ€™ineÂ´galiteÂ´ eÂ´tant stricte lorsque certaines combinai-
sons de valeurs des attributs sont structurellement impossibles. On sâ€™inteÂ´resse donc a`
la capaciteÂ´ de lâ€™arbre a` reproduire cette table T.1
1Il nâ€™est pas sans inteÂ´reË†t de relever comme nous lâ€™avons fait dans Ritschard et Zighed (2003),
que dans cette optique de reconstitution de T , les arbres peuvent constituer un outil alternatif et
compleÂ´mentaire a` la modeÂ´lisation log-lineÂ´aire des tables de contingence multidimensionnelle.
RNTI - 2
G. Ritschard et D.A. Zighed
homme femme
marieÂ´ primaire secondaire tertiaire primaire secondaire tertiaire total
non 11 14 15 0 5 5 50
oui 8 8 9 10 7 8 50
total 19 22 24 10 12 13 100
Tab. 1 â€“ Exemple de table de contingence T
Un eÂ´leÂ´ment de la table T est noteÂ´ nij et repreÂ´sente le nombre de cas avec profil
xj qui dans les donneÂ´es prennent la valeur yi de la variable reÂ´ponse. On note TË† la
table preÂ´dite a` partir dâ€™un arbre et nË†ij deÂ´signe un eÂ´leÂ´ment geÂ´neÂ´rique de cette table.
Formellement, la qualiteÂ´ dâ€™ajustement se reÂ´fe`re ainsi a` la divergence entre les tables T
et TË†.
Il reste a` preÂ´ciser comment lâ€™on deÂ´duit la table estimeÂ´e TË† a` partir dâ€™un arbre. On
utilise pour cela le mode`le de reconstruction suivant ou` lâ€™on note Tj la j-e`me colonne
de T :
Tj = najp|j , j = 1, . . . , c (1)
Les parame`tres sont le nombre total n de cas, les proportions aj de cas par colonne j =
1, . . . , c, et les c vecteurs de probabiliteÂ´s p|j = p(Y |xj) correspondant a` la distribution
de Y dans chaque colonne j de la table. Nous verrons que chaque arbre donne lieu a` des
estimations pË†|j diffeÂ´rentes des vecteurs p|j et par suite a` une estimation TË† diffeÂ´rente.
Les aj seront estimeÂ´s par les proportions aË†j = nÂ·j/n observeÂ´es dans lâ€™eÂ´chantillon, nÂ·j
deÂ´signant le total de la j-e`me colonne de T. Contrairement aux pË†|j , les estimations aË†j
sont indeÂ´pendantes de lâ€™arbre induit.
3.2 Arbre satureÂ´ et arbre eÂ´tendu
En modeÂ´lisation statistique, on appelle mode`le satureÂ´ un mode`le avec le nombre
maximal de parame`tres libres qui peuvent eË†tre estimeÂ´s a` partir des donneÂ´es. En mo-
deÂ´lisation log-lineÂ´aire de tables de contingence multidimensionnelles, le mode`le satureÂ´
permet de reproduire exactement la table modeÂ´liseÂ´e. Par analogie, nous introduisons
le concept dâ€™arbre satureÂ´ qui permet de reproduire exactement la table T.
DeÂ´finition 1 (Arbre satureÂ´) Pour des variables preÂ´dictives cateÂ´gorielles, on appelle
arbre satureÂ´, un arbre qui reÂ´sulte de tous les eÂ´clatements successifs possibles selon les
modaliteÂ´s des variables preÂ´dictives.
Les distributions p|j conditionnelles aux feuilles de lâ€™arbre satureÂ´ sont estimeÂ´es par les
vecteurs de freÂ´quences relatives, soit les vecteurs dâ€™eÂ´leÂ´ments nij/nÂ·j , i = 1, . . . , `.
Lâ€™arbre satureÂ´ nâ€™est pas unique, des variantes eÂ´tant possibles selon lâ€™ordre dans
lequel les variables sont prises en compte. Tous les arbres satureÂ´s conduisent cependant
aux meË†mes feuilles (sommets terminaux). Ces feuilles correspondent aux colonnes de
la table de contingence T.
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
 G46
     
 
 
 
	
 

	
 


 



	

 
 
 
 
Fig. 2 â€“ Arbre satureÂ´
Par exemple, la figure 2 donne lâ€™arbre satureÂ´ correspondant au cas du tableau 1 ou`
lâ€™on cherche a` preÂ´dire le statut marital connaissant le genre (H = homme, F = femme)
et le secteur dâ€™activiteÂ´ (P = primaire, S = secondaire, T = tertiaire). Les distributions
conditionnelles sont :
pË†|HP =
(
11/19
8/19
)
, pË†|HS =
(
14/22
8/22
)
, pË†|HT =
(
15/24
9/24
)
,
pË†|FP =
(
0/10
10/10
)
, pË†|FS =
(
5/12
7/12
)
, pË†|FT =
(
5/13
8/13
)
Notons quâ€™un algorithme dâ€™induction dâ€™arbres ne peut en geÂ´neÂ´ral geÂ´neÂ´rer un arbre
satureÂ´ que si (i) toutes les cellules de la table de contingence des variables preÂ´dictives
sont non vides et si (ii) la distribution de la variable reÂ´ponse est diffeÂ´rente dans chacune
des cellules.
Pour comparer les distributions des feuilles de lâ€™arbre induit a` celles de lâ€™arbre
satureÂ´, on doit eÂ´tendre lâ€™arbre induit pour obtenir des feuilles de meË†me deÂ´finition et en
particulier en meË†me nombre que celles de lâ€™arbre satureÂ´.
DeÂ´finition 2 (Extension maximale dâ€™un arbre induit) Pour des variables preÂ´-
dictives cateÂ´gorielles, on appelle extension maximale de lâ€™arbre induit ou arbre induit
eÂ´tendu, lâ€™arbre obtenu a` partir de lâ€™arbre induit en proceÂ´dant a` tous les eÂ´clatements suc-
cessifs possibles de ses sommets terminaux et en appliquant aux feuilles de lâ€™extension
la distribution pË†a|k observeÂ´e dans le nÅ“ud terminal parent de lâ€™arbre initial.
Par exemple, si lâ€™arbre induit est lâ€™arbre avec les sommets blancs de la figure 3,
son extension maximale sâ€™obtient en ajoutant les sommets gris et en reÂ´partissant dans
ceux-ci lâ€™effectif selon la distribution du sommet dont ils sont issus. Les distributions
des six sommets terminaux de lâ€™extension se deÂ´duisent de celles des trois feuilles de
lâ€™arbre induit, soit pour notre exemple :
pË†|HP = pË†|HS = pË†|HT = pË†a|H =
(
40/65
25/65
)
pË†|FP = pË†a|FP =
(
0/10
10/10
)
pË†|FS = pË†|FT = pË†a|FPÂ¯ =
(
10/25
15/25
)
RNTI - 2
G. Ritschard et D.A. Zighed
 G46
    

 
   	
	  

 
  
  
   
  
 
 
 
 
 
 

 
 
 
  
	  
  
	  
Fig. 3 â€“ Arbre induit (sommets blancs) et son extension maximale
Les feuilles terminales de lâ€™extension de lâ€™arbre induit donnent lieu a` la table preÂ´dite
TË† repreÂ´senteÂ´e au tableau 2.
homme femme
marieÂ´ primaire secondaire tertiaire primaire secondaire tertiaire total
non 11.7 13.5 14.8 0 4.8 5.2 50
oui 7.3 8.5 9.2 10 7.2 7.8 50
total 19 22 24 10 12 13 100
Tab. 2 â€“ Exemple de table de contingence preÂ´dite TË†
4 QualiteÂ´ dâ€™ajustement dâ€™un arbre induit
Ayant preÂ´ciseÂ´ le concept dâ€™ajustement dâ€™un arbre qui nous occupe, et en particulier
les notions de tables cible et preÂ´dite, nous proposons dans cette section des statistiques
et indicateurs permettant dâ€™en eÂ´valuer la qualiteÂ´. Il sâ€™agit essentiellement dâ€™adaptations
des mesures classiquement utiliseÂ´es en modeÂ´lisation statistique pour juger de la qualiteÂ´
globale dâ€™un mode`le.
Nous proposons dâ€™utiliser des statistiques de test du khi-2 pour mesurer la diver-
gence entre la table preÂ´dite TË† et la table cible T. Comme en modeÂ´lisation multinomiale
log-lineÂ´aire (Agresti, 1990) ou` cette divergence sert dâ€™indicateur de lâ€™eÂ´cart entre mo-
de`le ajusteÂ´ et mode`le satureÂ´, elle renseigne ici sur lâ€™eÂ´cart entre lâ€™arbre induit et lâ€™arbre
satureÂ´.
indeÂ´pendance
nÅ“ud inital
mode`le satureÂ´
arbre satureÂ´
mode`le ajusteÂ´
arbre induit
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
PlutoË†t que de chercher a` savoir a` quelle distance lâ€™on se trouve du mode`le satureÂ´ cor-
respondant a` la partition la plus fine, il peut eË†tre utile de savoir ce que lâ€™on a gagneÂ´ par
rapport a` la situation dâ€™indeÂ´pendance ou` lâ€™on ne tient pas compte des preÂ´dicteurs. Cet
aspect correspond a` lâ€™eÂ´cart entre lâ€™arbre induit et le mode`le dâ€™indeÂ´pendance repreÂ´senteÂ´
par lâ€™arbre constitueÂ´ du seul nÅ“ud initial.
Nous commencÂ¸ons donc avec les tests dâ€™ajustement du khi-2, puis discutons de la
comparaison avec un mode`le de reÂ´feÂ´rence, et plus geÂ´neÂ´ralement de la diffeÂ´rence entre
deux mode`les, en introduisant les statistiques de test de lâ€™ameÂ´lioration de lâ€™ajustement.
Dans la meË†me optique nous proposons plusieurs indicateurs de type R2. Enfin, dans
la perspective de seÂ´lectionner lâ€™arbre offrant le meilleur compromis entre ajustement
et complexiteÂ´, nous compleÂ´tons la discussion en eÂ´tablissant des formes des crite`res
dâ€™information AIC et BIC applicables aux arbres.
4.1 Statistiques du khi-2 pour arbres dâ€™induction
Lâ€™objectif est de mesurer la divergence entre T et TË† avec une statistique permettant
dâ€™eÂ´valuer la significativiteÂ´ de lâ€™eÂ´cart observeÂ´. Rappelons que T est reproduit exactement
par lâ€™arbre satureÂ´. La divergence que lâ€™on se propose dâ€™eÂ´valuer ici sâ€™interpre`te donc
eÂ´galement comme lâ€™eÂ´cart entre lâ€™arbre induit et lâ€™arbre satureÂ´.
Les p|j eÂ´tant estimeÂ´s par le maximum de vraisemblance, on peut simplement appli-
quer les statistiques de divergence de Cressie et Read (1984), dont les cas particuliers
les plus connus sont les khi-2 de Pearson que lâ€™on note X2 et la statistique du rapport
de vraisemblance noteÂ´ G2 :
X2 =
âˆ‘`
i=1
câˆ‘
j=1
(nij âˆ’ nË†ij)2
nË†ij
(2)
G2 = 2
âˆ‘`
i=1
câˆ‘
j=1
nij ln
(
nij
nË†ij
)
(3)
Sous lâ€™hypothe`se que le mode`le est correct et sous certaines conditions de reÂ´gulariteÂ´, voir
par exemple Bishop et al. (1975, chap. 4), ces statistiques suivent une meË†me distribution
du Ï‡2 avec pour degreÂ´s de liberteÂ´ le nombre de cellules de la table de contingence moins
le nombre de parame`tres lineÂ´airement indeÂ´pendants du mode`le de preÂ´diction deT. Dans
notre cas T est preÂ´dit par le mode`le de reconstruction (1) dont il sâ€™agit alors de preÂ´ciser
le nombre de parame`tres lineÂ´airement indeÂ´pendants.
Un arbre induit non satureÂ´ deÂ´finit une partition de lâ€™ensemble X des profils x
possibles. Chacun de ses q sommets terminaux correspond donc a` un sous-ensemble
Xk âŠ† X , k = 1, . . . , q de profils xj pour lequel, on impose la contrainte
p|j = pa|k pour tout xj âˆˆ Xk k = 1, . . . , q (4)
ou` pa|k deÂ´signe la distribution dans le sommet terminal k de lâ€™arbre.
Chaque vecteur pa|k contient `âˆ’ 1 termes indeÂ´pendants et il y a q âˆ’ 1 distributions
conditionnelles pa|k indeÂ´pendantes. On en deÂ´duit le deÂ´compte du tableau 3 des para-
me`tres indeÂ´pendants pour un nombre q fixeÂ´ de sommets terminaux.2 En retranchant
2Bien que q soit induit des donneÂ´es, on raisonne ici conditionnellement a` q comme cela se fait en
RNTI - 2
G. Ritschard et D.A. Zighed
parame`tres nombre dont indeÂ´pendants
pi|j , i = 1, . . . , `, j = 1, . . . , c c` q(`âˆ’ 1)
aj , j = 1, . . . , c c câˆ’ 1
n 1 1
Total c(`+ 1) + 1 q(`âˆ’ 1) + c
Tab. 3 â€“ Calcul du nombre de parame`tres indeÂ´pendants
au nombre c` de cellules de T le nombre q(`âˆ’ 1) + c de parame`tres indeÂ´pendants, on
obtient les degreÂ´s de liberteÂ´ dM de lâ€™arbre induit, soit
degreÂ´s de liberteÂ´ = dM = (câˆ’ q)(`âˆ’ 1) .
Notons que ce nombre correspond au nombre de contraintes (4). Pour le mode`le dâ€™in-
deÂ´pendance, noteÂ´ I, on a q = 1 et lâ€™on retrouve la valeur usuelle des degreÂ´s de liberteÂ´
du test dâ€™indeÂ´pendance, soit dI = (câˆ’ 1)(`âˆ’ 1). De meË†me, pour lâ€™arbre satureÂ´, noteÂ´ S,
on a q = c et donc dS = 0.
Sous reÂ´serve des conditions de reÂ´gulariteÂ´, les statistiquesX2 etG2 de tables associeÂ´es
a` des arbres suivent donc, lorsque le mode`le est correct, une distribution du Ï‡2 avec
(câˆ’ q)(`âˆ’ 1) degreÂ´s de liberteÂ´.
Pour lâ€™arbre de la figure 1, on trouve par exemple, X2 = 0.1823 et G2 = 0.1836.
On a c = 6, q = 3 et ` = 2, et donc dM = (6 âˆ’ 3)(2 âˆ’ 1) = 3 degreÂ´s de liberteÂ´. Les
valeurs des statistiques sont tre`s petites et, avec un degreÂ´ de signification de lâ€™ordre de
98% dans les deux cas, indiquent clairement que TË† ajuste de facÂ¸on satisfaisante T. La
qualiteÂ´ de lâ€™ajustement de lâ€™arbre induit aux donneÂ´es est donc dans ce cas excellente.
TheÂ´oriquement, les statistiques X2 de Pearson (2) et G2 du rapport de vraisem-
blance (3) devraient permettre de tester si lâ€™arbre induit sâ€™ajuste de facÂ¸on satisfaisante
aux donneÂ´es. Il est bien connu cependant que la porteÂ´e du test reste limiteÂ´e lorsque
lâ€™eÂ´chantillon est grand, le moindre eÂ´cart devenant alors statistiquement significatif. Par
ailleurs, les conditions de reÂ´gulariteÂ´ requises pour que les statistiques soient distribueÂ´es
selon une loi du Ï‡2, et en particulier les conditions dâ€™inteÂ´rioriteÂ´ (pas dâ€™effectifs atten-
dus nuls), sont difficilement tenables lorsque lâ€™arbre satureÂ´ compte un grand nombre
de feuilles.
Plus inteÂ´ressante nous semble eË†tre lâ€™utilisation de la statistique G2 pour comparer
des mode`les imbriqueÂ´s, un mode`leM2 (restreint) eÂ´tant inclus dansM1 (non restreint) si
lâ€™espace de ses parame`tres est un sous-ensemble deM1, câ€™est-a`-dire, en dâ€™autres termes,
si les parame`tres de M2 sâ€™obtiennent en imposant des contraintes sur ceux du mode`le
M1. En effet dans ce cas la deÂ´viance entre les deux mode`les est (voir par exemple
Agresti, 1990, p. 211 ou Powers et Xie, 2000, p. 105) :
G2(M2|M1) = G2(M2)âˆ’G2(M1) (5)
modeÂ´lisation log-lineÂ´aire ou` lâ€™on suppose donneÂ´es les interactions prises en compte, alors quâ€™elles sont
en fait induites des donneÂ´es par le biais du processus de seÂ´lection du mode`le.
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
qui, sous lâ€™hypothe`se que M2 est correct, est approximativement distribueÂ´e selon une
loi du Ï‡2 avec pour degreÂ´s de liberteÂ´ la diffeÂ´rence d2 âˆ’ d1 des degreÂ´s de liberteÂ´ des
mode`les M2 et M1.
Cette dernie`re proprieÂ´teÂ´ permet en particulier de tester la significativiteÂ´ dâ€™un eÂ´cla-
tement. La deÂ´viance entre le mode`le apre`s lâ€™eÂ´clatement et celui avant lâ€™eÂ´clatement nous
renseigne en effet sur la pertinence statistique de cet eÂ´clatement. Par exemple, si M1
est lâ€™arbre induit de la figure 1 et M2 lâ€™arbre avant lâ€™eÂ´clatement du sommet Â« femmeÂ» .
On a G2(M1) = 0.18 avec 3 degreÂ´s de liberteÂ´ et G2(M2) = 8.41 avec 4 degreÂ´s de liberteÂ´.
La deÂ´viance est alors
G2(M2|M1) = 8.41âˆ’ 0.18 = 8.23 avec d2 âˆ’ d1 = 4âˆ’ 3 = 1
Son degreÂ´ de signification (p-valeur) est 0.4%, donc infeÂ´rieur au seuil geÂ´neÂ´ralement
admis de 5%, ce qui indique que lâ€™eÂ´clatement est statistiquement significatif.
4.2 Comparaison avec un mode`le de reÂ´feÂ´rence
Cette section est consacreÂ´e aux indicateurs de type R2 qui mesurent le gain relatif
de qualiteÂ´ dâ€™un arbre induitM par rapport a` lâ€™arbre trivial I constitueÂ´ par le seul nÅ“ud
initial. Nous discutons successivement le pourcentage dâ€™ameÂ´lioration du taux dâ€™erreur,
le pourcentage de reÂ´duction de lâ€™entropie, lâ€™ameÂ´lioration de lâ€™ajustement et les pseudo
R2. Les mesures consideÂ´reÂ´es ici pour la comparaison entre lâ€™arbre induit et le nÅ“ud
initial se geÂ´neÂ´ralisent aiseÂ´ment, bien que nous ne le traitions pas explicitement, au cas
geÂ´neÂ´ral de la comparaison de deux mode`les dont lâ€™un est inclus dans lâ€™autre.
4.2.1 Remarque sur le pourcentage de reÂ´duction du taux dâ€™erreur
Bien quâ€™on ne sâ€™inteÂ´resse pas ici a` la preÂ´diction de valeurs individuelles, nous aime-
rions souligner que lâ€™ideÂ´e de comparer la performance du mode`le avec le mode`le qui ne
tient pas compte des preÂ´dicteurs est eÂ´galement pertinente en terme de taux dâ€™erreur.
On peut noter que, sur lâ€™eÂ´chantillon dâ€™apprentissage, le pourcentage de reÂ´duction de
lâ€™erreur de classification correspond a` la mesure dâ€™association Î»y|partition de Guttman
(1941) et Goodman et Kruskal (1954) entre la variable reÂ´ponse y et la partition deÂ´finie
par le graphe induit. Il existe une forme analytique de la variance asymptotique de cette
mesure qui permet dâ€™en tester la significativiteÂ´ sous certaines conditions de reÂ´gulariteÂ´
(Goodman et Kruskal, 1972; Olszak et Ritschard, 1995). Nous nâ€™approfondissons pas
cet aspect ici, notre objectif eÂ´tant les qualiteÂ´s descriptives du graphe induit plutoË†t que
ses qualiteÂ´s preÂ´dictives.
4.2.2 Gain dâ€™information
Le gain dâ€™information peut eË†tre mesureÂ´ par la reÂ´duction de lâ€™entropie de la distribu-
tion de la variable reÂ´ponse que permet la connaissance des classes de la partition deÂ´finie
par lâ€™arbre induit. PreÂ´cisons que nous nous inteÂ´ressons a` la reÂ´duction globale dâ€™entro-
pie que permet lâ€™arbre par rapport a` la distribution marginale, câ€™est-a`-dire celle dans le
nÅ“ud initial. Le gain discuteÂ´ ici se distingue donc du gain partiel et conditionnel a` un
RNTI - 2
G. Ritschard et D.A. Zighed
nÅ“ud que certains algorithmes cherchent a` maximiser a` chaque eÂ´tape de construction
de lâ€™arbre.
Le gain dâ€™information relativement au nÅ“ud initial est mesureÂ´ par exemple par les
deux indicateurs suivants :
Ï„Ë†M |I =
n
âˆ‘
i
âˆ‘
j
nË†2ij
nÂ·j
âˆ’âˆ‘i n2iÂ·
n2 âˆ’âˆ‘i n2iÂ· (6)
uË†M |I =
âˆ‘
i
niÂ·
n log2
niÂ·
n âˆ’
âˆ‘
j
nÂ·j
n
âˆ‘
i
nË†ij
nÂ·j
log2
nË†ij
nÂ·jâˆ‘
i
niÂ·
n log2
niÂ·
n
(7)
Le Ï„Ë†M |I est la deuxie`me mesure dâ€™association nominale proposeÂ´e par Goodman et
Kruskal (1954). Il mesure la proportion de reÂ´duction de lâ€™entropie quadratiqueHQ(p) =âˆ‘
i pi(1âˆ’ pi), connue aussi comme lâ€™indice de variation de Gini. Le uË†M |I est connu en
statistique sous le nom de coefficient dâ€™incertitude de Theil (1967, 1970). Il mesure la
proportion de reÂ´duction de lâ€™entropie de Shannon HS(p) = âˆ’
âˆ‘
i pi log2 pi.
Pour lâ€™arbre M de la figure 1, on trouve par exemple Ï„Ë†M |I = 0.145 et uË†M |I = 0.132,
valeurs qui indiquent une reÂ´duction dâ€™entropie dâ€™environ 14%. Si lâ€™on compare ces
valeurs a` celles du mode`le satureÂ´, soit respectivement Ï„Ë†S|I = 0.146 et uË†S|I = 0.134,
il apparaË†Ä±t que lâ€™arbre capte a` peu pre`s toute lâ€™information que lâ€™on peut tirer des
attributs preÂ´dictifs retenus.
Pour tester la significativiteÂ´ statistique du gain dâ€™information, soit les hypothe`ses
H0 : Ï„M |I = 0 et H0 : uM |I = 0, une possibiliteÂ´ est dâ€™utiliser les variances asymp-
totiques que lâ€™on trouve dans la litteÂ´rature (voir par exemple Olszak et Ritschard,
1995) et une approximation par la loi normale. Il est preÂ´feÂ´rable cependant dâ€™utiliser les
transformations suivantes des indicateurs :
C(I|M) = (nâˆ’ 1)(`âˆ’ 1) Ï„Ë†M |I (8)
G2(I|M) =
(
âˆ’ 2
âˆ‘
i
niÂ· log(niÂ·/n)
)
uË†M |I (9)
La premie`re C est due a` Light et Margolin (1971) qui montrent que dans le cas ou` le
tableau preÂ´dit est le mode`le satureÂ´ (M = S), C(I|S) est, sous lâ€™hypothe`se dâ€™indeÂ´pen-
dance (Ï„S|I = 0), asymptotiquement distribueÂ´e comme un Ï‡2 a` dI degreÂ´s de liberteÂ´.
Dans le cas dâ€™un mode`leM plus restrictif que S, il suffit dâ€™adapter les degreÂ´s de liberteÂ´.
Ainsi, de facÂ¸on geÂ´neÂ´rale C(I|M) suit un Ï‡2 avec dI âˆ’ dM degreÂ´s de liberteÂ´.
La transformation du coefficient uË†M |I montre que tester la significativiteÂ´ de uM |I
est eÂ´quivalent a` tester la significativiteÂ´ de la diffeÂ´rence dâ€™ajustement entre I et M avec
G2(I|M) = G2(I) âˆ’ G2(M). Les deux transformations (8) et (9) suivent donc, sous
H0, asymptotiquement la meË†me loi du Ï‡2 a` dI âˆ’ dM degreÂ´s de liberteÂ´. Le test avec ces
statistiques est plus puissant quâ€™avec une approximation normale de Ï„Ë†I|M ou uË†I|M .
Pour notre exemple dâ€™arbre induit, on trouve C(I|M) = 14.32 et G2(I|M) = 18.36.
Ces valeurs sont tre`s grandes compte tenu des degreÂ´s de liberteÂ´ dI âˆ’ dM = 5âˆ’ 3 = 2.
Elles confirment donc la significativiteÂ´ statistique du gain dâ€™information de lâ€™arbre par
rapport au mode`le dâ€™indeÂ´pendance.
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
4.2.3 Pseudo R2
Dans une optique purement descriptive, on peut envisager, comme on le fait par
exemple dans la modeÂ´lisation log-lineÂ´aire, des pseudo R2 qui mesurent la proportion
de la deÂ´viance entre indeÂ´pendance I et arbre satureÂ´ que lâ€™arbre dâ€™induction reproduit.
On peut par exemple utiliser le pseudo R2
R2 = 1âˆ’ G
2(M)
G2(I)
ou sa version corrigeÂ´e des degreÂ´s de liberteÂ´
R2ajust = 1âˆ’
G2(M)/dM
G2(I)/dI
Pour notre exemple, on a G2(I) = 18.55, dI = 5, G2(M) = .18 et dM = 3, dâ€™ou`
R2 = .99 et R2ajust = .984. Ces valeurs confirment que lâ€™arbre capte presque le 100% de
lâ€™eÂ´cart entre lâ€™indeÂ´pendance et la table cible repreÂ´senteÂ´e par lâ€™arbre satureÂ´.
Dans une optique de reÂ´duction dâ€™entropie, nous proposons comme alternative au
pseudo R2 ci-dessus, de calculer la part de la proportion maximale de reÂ´duction dâ€™entro-
pie possible que lâ€™on atteint avec lâ€™arbre induit. La proportion maximale de reÂ´duction
dâ€™entropie est obtenue avec la partition la plus fine, câ€™est-a`-dire le mode`le satureÂ´. Elle
correspond a` Ï„Ë†S|I pour lâ€™entropie quadratique et uË†S|I pour lâ€™entropie de Shannon. Les
parts de ces valeurs atteintes avec lâ€™arbre induit sont donc :
R2Ï„ =
Ï„Ë†M |I
Ï„Ë†S|I
et R2u =
uË†M |I
uË†S|I
Pour notre exemple, on obtient respectivement R2Ï„ = .993 et R
2
u = .985.
4.3 Ajustement et complexiteÂ´
Dans le but de pouvoir arbitrer entre ajustement et complexiteÂ´, on peut recourir
aux crite`res dâ€™information AIC dâ€™Akaike (1973) ou au crite`re bayeÂ´sien BIC (Schwarz,
1978; Kass et Raftery, 1995).
Dans notre cas, ces crite`res peuvent par exemple sâ€™eÂ´crire :
AIC(M) = G2(M) + 2(q`âˆ’ q + c)
BIC(M) = G2(M) + (q`âˆ’ q + c) log(n)
Ici, la complexiteÂ´ est repreÂ´senteÂ´e par le nombre q`âˆ’ q+ c de parame`tres indeÂ´pendants.
Le crite`re BIC peÂ´nalise plus fortement la complexiteÂ´ que le crite`re AIC, la peÂ´nalisation
augmentant avec le nombre de donneÂ´es n. Notons quâ€™il existe des formes alternatives
du coefficient BIC. Raftery (1995), par exemple, propose BIC = G2 âˆ’ d log(n), ou` d
est le nombre de degreÂ´s de liberteÂ´, soit dans notre cas d = (c âˆ’ q)(` âˆ’ 1). Comme ce
nombre d diminue dâ€™une uniteÂ´ chaque fois que lâ€™on ajoute un parame`tre indeÂ´pendant,
la peÂ´nalisation reste eÂ´videmment la meË†me. Les deux formulations sont eÂ´quivalentes a`
une translation c` pre`s.
RNTI - 2
G. Ritschard et D.A. Zighed
Ces crite`res dâ€™information offrent une alternative aux tests statistiques pour la seÂ´-
lection de mode`les. Parmi plusieurs mode`les, celui qui minimise le crite`re reÂ´alise le
meilleur compromis entre ajustement et complexiteÂ´. Le mode`le qui minimise BIC en
particulier, est, dans une approche bayeÂ´sienne, optimal compte tenu de lâ€™incertitude
des mode`les.
Pour illustrer lâ€™utilisation de ces crite`res, on se propose de comparer notre arbre
induit M avec la variante Mâˆ— ou` lâ€™on eÂ´clate le sommet Â« femmeÂ» selon les trois
secteurs P, S, I au lieu du partage binaire entre primaire P et non primaire PÂ¯ . Dans
les deux cas on a n = 100, c = 6 et ` = 2. Pour M , on a q = 3 et donc (q` âˆ’ q +
c) = 9 et pour Mâˆ—, q = 4 et donc (q` âˆ’ q + c) = 10. Comme G2(M) = 0.18 et
G2(Mâˆ—) = .16, on obtient AIC(M) = 18.18 et AIC(Mâˆ—) = 20.16. De meË†me, on trouve
BIC(M) = 41.63 et BIC(Mâˆ—) = 46.21. Les deux crite`res indiquent que lâ€™arbre M plus
simple est preÂ´feÂ´rable a` lâ€™arbre Mâˆ—. Le gain en qualiteÂ´ dâ€™ajustement de Mâˆ— nâ€™est pas
assez important pour justifier lâ€™accroissement de la complexiteÂ´. Remarquons que du
point de vue de ces crite`res dâ€™information, lâ€™arbre induit est supeÂ´rieur tant au mode`le
dâ€™indeÂ´pendance (AIC(I) = 32.55, BIC(I) = 50.78) quâ€™au mode`le satureÂ´ (AIC(S) = 24,
BIC(S) = 55.26).
5 Illustration
Nous illustrons ici les enseignements apporteÂ´s par les crite`res de qualiteÂ´ dâ€™ajuste-
ment proposeÂ´s sur un exemple concret. On conside`re pour cela les donneÂ´es relatives aux
762 eÂ´tudiants qui ont commenceÂ´ leur premie`re anneÂ´e dâ€™eÂ´tudes a` la FaculteÂ´ des sciences
eÂ´conomiques et sociales de Gene`ve en 1998. Il sâ€™agit de donneÂ´es administratives reÂ´unies
par Petroff et al. (2001). On rapporte quelques reÂ´sultats dâ€™une analyse visant a` eÂ´valuer
les chances de respectivement reÂ´ussir, redoubler ou eË†tre eÂ´limineÂ´ a` la fin de la premie`re
anneÂ´e dâ€™eÂ´tudes selon les caracteÂ´ristiques personnelles portant notamment sur lâ€™origine
et le cursus scolaire. La figure 4 montre lâ€™arbre obtenu avec la proceÂ´dure CHAID (Kass,
1980) impleÂ´menteÂ´e dans Answer Tree (SPSS, 2001). Parmi une trentaine de preÂ´dicteurs
potentiels, CHAID en a seÂ´lectionneÂ´ 5 dont deux quantitatifs, lâ€™anneÂ´e dâ€™immatricula-
tion a` lâ€™universiteÂ´ et lâ€™aË†ge a` lâ€™obtention du diploË†me de lâ€™eÂ´cole secondaire. Les cinq
variables avec les discreÂ´tisations et regroupements de modaliteÂ´s proposeÂ´s par CHAID
sont le type de diploË†me secondaire (3 modaliteÂ´s), lâ€™aË†ge de son obtention (4), la date
dâ€™immatriculation (datimma, 2), le tronc commun choisi (troncom, 2) et la nationaliteÂ´
(nationa, 2). La table cible T deÂ´finie par ces variables contient 88 colonnes. Elle a 3
lignes correspondant aux 3 situations possibles de lâ€™eÂ´tudiant apre`s sa premie`re anneÂ´e
dâ€™eÂ´tude.
Le tableau 4 donne la taille de la partition, la deÂ´viance G2 avec ses degreÂ´s de liberteÂ´
et son degreÂ´ de signification et les crite`res AIC et BIC. Ces valeurs peuvent eË†tre compa-
reÂ´es a` celles de plusieurs variantes. CHAID2 est CHAID sans lâ€™eÂ´clatement du sommet
4 (nationa /âˆˆ {GE, hors Europe}) et CHAID3 sans lâ€™eÂ´clatement des sommets 4 et 5
(nationa âˆˆ {GE, hors Europe}). Le mode`le Sipina correspond au graphe de la figure 5
obtenu avec la proceÂ´dure Sipina (Sipina for Windows V2.5, 2000; Zighed et Rakotoma-
lala, 2000) qui, comme on peut le voir, autorise eÂ´galement des fusions de sommets. On
donne eÂ´galement les valeurs trouveÂ´es pour les partitions qui donnent respectivement
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
bilan oct.99
dipl. second.regroup.
Adj. P-value=0.0000, Chi-square=50.7197, df=2
Ã©conomique;moderne,<missing>
AGEDIP
Adj. P-value=0.0090, Chi-square=11.0157, df=1
>20,<missing><=20
classic .latine;scientifique
AGEDIP
Adj. P-value=0.0067, Chi-square=14.6248, df=2
>19(18,19]<=18
Ã©tranger,autre;dipl. ing.
nationalitÃ© regoup.
Adj. P-value=0.0011, Chi-square=16.2820, df=1
GenÃ¨ve;hors Europe
tronc commun
Adj. P-value=0.0188, Chi-square=5.5181, df=1
sc.socialessc.Ã©con. + HEC
ch-al.+Tessin;Europe;Suisse Romande
date d'immatriculation
Adj. P-value=0.0072, Chi-square=9.2069, df=1
>97<=97
Page 1, 1
Tree 01 - BIL_99
Category % n
echec 27.43 209
redouble 17.06 130
rÃ©ussi 55.51 423
Total (100.00) 762
Node 0
Category %
echec 22
redouble 19
rÃ©ussi 57
Total (32
Node 9
Category % n
echec 16.60 41
redouble 11.74 29
rÃ©ussi 71.66 177
Total (32.41) 247
Node 2
Category % n
echec 23.91 22
redouble 16.30 15
rÃ©ussi 59.78 55
Total (12.07) 92
Node 8
Category % n
echec 14.53 17
redouble 11.11 13
rÃ©ussi 74.36 87
Total (15.35) 117
Node 7
Category % n
echec 5.26 2
redouble 2.63 1
rÃ©ussi 92.11 35
Total (4.99) 38
Node 6
Category % n
echec 40.70 81
redouble 21.61 43
rÃ©ussi 37.69 75
Total (26.12) 199
Node 1
Category % n
echec 54.88 45
redouble 23.17 19
rÃ©ussi 21.95 18
Total (10.76) 82
Node 5
Category % n
echec 71.05 27
redouble 13.16 5
rÃ©ussi 15.79 6
Total (4.99) 38
Node 14
Category % n
echec 40.91 18
redouble 31.82 14
rÃ©ussi 27.27 12
Total (5.77) 44
Node 13
Category % n
echec 30.77 36
redouble 20.51 24
rÃ©ussi 48.72 57
Total (15.35) 117
Node 4
Category % n
echec 23.81 20
redouble 19.05 16
rÃ©ussi 57.14 48
Total (11.02) 84
Node 12
Category % n
echec 48.48 16
redouble 24.24 8
rÃ©ussi 27.27 9
Total (4.33) 33
Node 11
bilan oct.99
dipl. second.regroup.
Adj. P-value=0.0000, Chi-square=50.7197, df=2
Ã©c
Adj. P-val
<=20
classic .latine;scientifique
AGEDIP
Adj. P-value=0.0067, Chi-square=14.6248, df=2
>19(18,19]<=18
Ã©tranger,autre;dipl. ing.
nationalitÃ© regoup.
Adj. P-value=0.0011, Chi-square=16.2820, df=1
GenÃ¨ve;hors Europe
tronc commun
Adj. P-value=0.0188, Chi-square=5.5181, df=1
sc.socialessc.Ã©con. + HEC
ch-al.+Tessin;Europe;Suisse Romande
date d'immatriculation
Adj. P-value=0.0072, Chi-square=9.2069, df=1
>97<=97
Page 1, 1
Tree 01 - BIL_99
Fig. 4 â€“ Bilan apre`s une anneÂ´e en SES : arbre CHAID et deÂ´tail de la branche gauche
RNTI - 2
G. Ritschard et D.A. Zighed
Fig. 5 â€“ Graphe induit avec Sipina
les plus petits AIC et BIC. Enfin, le mode`le satureÂ´ correspond a` la partition la plus fine
et le mode`le dâ€™indeÂ´pendance au cas ou` tous les profils sont regroupeÂ´s en seul groupe.
On constate tout dâ€™abord quâ€™a` lâ€™exception du mode`le dâ€™indeÂ´pendance et de CHAID3
avec un degreÂ´ de signification leÂ´ge`rement infeÂ´rieur a` 5%, tous les mode`les reproduisent
de facÂ¸on satisfaisante la table T. La simplification de lâ€™arbre CHAID en CHAID2
ou CHAID3 se traduit comme attendu par une deÂ´teÂ´rioration du G2. Les eÂ´carts sont
G2(chaid2|chaid) = 9.5 et G2(chaid3|chaid) = 17.3 qui pour un gain de respective-
ment 2 et 4 degreÂ´s de liberteÂ´ sont clairement significatifs, ce qui valide statistiquement
lâ€™eÂ´clatement des nÅ“uds 4 et 5. Les diffeÂ´rences de G2 avec les autres mode`les qui ne
sont pas des sous graphes de lâ€™arbre CHAID ne peuvent eË†tre testeÂ´s. On peut par contre
comparer avec les AIC ou BIC de ces mode`les. On remarque tout dâ€™abord que CHAID
Mode`le q d G2 sig(G2) AIC BIC
SatureÂ´ 88 0 0 1 528 1751.9
Meilleur AIC 14 148 17.4 1 249.4 787.2
CHAID 9 158 177.9 0.133 390.0 881.3
CHAID2 8 160 187.4 0.068 395.4 877.5
CHAID3 7 162 195.2 0.038 399.2 872.1
Sipina 7 162 185.8 0.097 389.8 862.6
Meilleur BIC 6 164 75.2 1 275.2 738.8
IndeÂ´pendance 1 174 295.1 0.000 475.8 892.3
CHAID2 : CHAID sans eÂ´clatement datimma du sommet 4 (nationa 6= GE, hors Europe)
CHAID3 : CHAID2 sans eÂ´clatement troncom du sommet 5 (nationa= GE, hors Europe)
Tab. 4 â€“ SES 98 : qualiteÂ´s dâ€™ajustement dâ€™un choix de mode`les
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
proportion de reÂ´duction relativement
dâ€™entropie au mode`le satureÂ´ pseudo
Mode`le Ï„Ë†M|I uË†M|I Î»Ë†M|I Ï„Ë†M|I uË†M|I Î»Ë†M|I R
2
ajust
SatureÂ´ 0.193 0.197 0.183 1 1 1 1
Meilleur AIC 0.159 0.185 0.179 0.824 0.939 0.978 .941
CHAID 0.094 0.078 0.109 0.487 0.396 0.596 .336
CHAID2 0.086 0.072 0.088 0.446 0.365 0.481 .309
CHAID3 0.08 0.067 0.088 0.415 0.340 0.481 .289
Sipina 0.087 0.073 0.103 0.451 0.371 0.563 .324
Meilleur BIC 0.149 0.147 0.142 0.772 0.746 0.776 .745
IndeÂ´pendance 0 0 0 0 0 0 0
Tab. 5 â€“ SES 98 : mesures de type R2 pour un choix de mode`les
et ses deux variantes ont des AIC et BIC tre`s voisins, sensiblement meilleurs que ceux
du mode`le satureÂ´ et dans une moindre mesure que ceux du mode`le dâ€™indeÂ´pendance. La
partition geÂ´neÂ´reÂ´e par Sipina obtient un AIC eÂ´quivalent au mode`le CHAID, mais son
BIC est infeÂ´rieur a` celui des 3 mode`les CHAID. Lâ€™eÂ´cart supeÂ´rieur a` 10 traduit, selon
lâ€™eÂ´chelle postuleÂ´e par Raftery (1995), une supeÂ´rioriteÂ´ tre`s forte de cette partition. On
peut noter toutefois, que les valeurs des AIC et BIC obtenues pour le graphe Sipina
restent tre`s nettement supeÂ´rieures aux valeurs optimales possibles avec les attributs
retenus. Notons cependant que les partitions AIC et BIC optimales ne peuvent eË†tre
deÂ´crites par des arbres, les re`gles (non donneÂ´es ici) qui caracteÂ´risent les classes de ces
partitions consistant en des meÂ´langes de conjonctions (â€˜etâ€™) et dâ€™alternatives (â€˜ouâ€™) de
conditions. Les partitions AIC et BIC optimales ont eÂ´teÂ´ obtenues avec la proceÂ´dure
deÂ´crite dans Ritschard (2003).
Le tableau 5 reÂ´capitule les mesures de type R2. Le Î»Ë†M |I qui indique la proportion
de reÂ´duction du taux dâ€™erreur sur donneÂ´es dâ€™apprentissage est donneÂ´ pour comparaison
avec les proportions de reÂ´duction dâ€™entropie. Les proportions maximales de reÂ´duction
dâ€™entropie par rapport au mode`le dâ€™indeÂ´pendance sont eÂ´videmment obtenues avec la
partition la plus fine, câ€™est-a`-dire le mode`le satureÂ´. On note que ces maxima sont infeÂ´-
rieurs a` 1. La part de cette reÂ´duction maximale reÂ´aliseÂ´e par chaque mode`le est eÂ´galement
donneÂ´e. On voit que ces dernie`res valeurs sont tre`s similaires au pseudo R2 ajusteÂ´. Elles
nous indiquent par exemple, que les mode`les CHAID et Sipina, malgreÂ´ un ajustement
satisfaisant, ne captent quâ€™environ 1/3 du potentiel de reÂ´duction dâ€™entropie possible
avec les preÂ´dicteurs retenus. Les meilleures partitions du point de vue tant du BIC que
de lâ€™AIC font nettement mieux de ce point de vue.
6 EÂ´tude par simulations des statistiques X2 et G2
Nous rapportons ici les principaux enseignements de simulations meneÂ´es pour eÂ´tu-
dier empiriquement la distribution des statistiques du khi-2 consideÂ´reÂ´es a` la section preÂ´-
ceÂ´dente. Lâ€™objectif principal de ces analyses est de conforter empiriquement le nombre
de degreÂ´s de liberteÂ´ que nous avons calculeÂ´, a` savoir (câˆ’q)(`âˆ’1). Lebart et al. (2000, p.
362) mentionnent par exemple que des calculs similaires dans le cadre de lâ€™analyse des
RNTI - 2
G. Ritschard et D.A. Zighed
correspondance a donneÂ´ des reÂ´sultats faux. Rappelons que les degreÂ´s de liberteÂ´ dâ€™une
variable du Ï‡2 repreÂ´sentent son espeÂ´rance matheÂ´matique et la moitieÂ´ de sa variance.
Plusieurs seÂ´ries de simulations ont eÂ´teÂ´ reÂ´aliseÂ´es. Nous preÂ´sentons ici les reÂ´sultats
pour deux tailles de tables T, soit 2 Ã— 6 et 3 Ã— 88, cette dernie`re eÂ´tant la taille de la
table cible de lâ€™illustration de la section 5. Pour chacun des cas, nous avons consideÂ´reÂ´
plusieurs partitions des profils (colonnes des tables). Pour la petite table, le regroupe-
ment en une seule classe (indeÂ´pendance) et une partition en 3 geÂ´neÂ´reÂ´e par un arbre.
Pour la grande table, lâ€™indeÂ´pendance et les partitions correspondant a` lâ€™arbre CHAID
et au meilleur BIC de lâ€™illustration SES 98. A chaque fois, nous avons imposeÂ´ au niveau
de la population lâ€™eÂ´galiteÂ´ des distributions de la reÂ´ponse pour les profils dâ€™une meË†me
classe. Pour la grande table, ceci a eÂ´teÂ´ fait en modifiant la structure de la population
des eÂ´tudiants SES 98. Nous avons dâ€™autre part aussi eÂ´tudieÂ´ le cas dâ€™une population
reÂ´partie uniformeÂ´ment entre les 264 cases de la table (qui veÂ´rifie neÂ´cessairement toutes
les contraintes.) Dans chacune des populations ainsi deÂ´finies, nous avons tireÂ´s aleÂ´a-
toirement 200 eÂ´chantillons pour lesquels nous avons calculeÂ´ les statistiques X2 et G2
mesurant lâ€™eÂ´cart entre la table T eÂ´chantillonneÂ´e et la table TË† preÂ´dite en imposant la
partition.
Le tableau 6 donne la valeur moyenne et la demi-variance des 200 X2 et G2 obtenus
dans chaque cas ainsi que lâ€™erreur standard des moyennes. Pour chaque seÂ´rie de simu-
lations sont indiqueÂ´s la taille q de la partition, le nombre zs de zeÂ´ros structurels et le
nombre theÂ´orique de degreÂ´s de liberteÂ´ d = (câˆ’q)(`âˆ’1)âˆ’zs. Notons que si lâ€™on peut ici
deÂ´terminer le nombre de zeÂ´ros structurels, ceci nâ€™est pas le cas avec des donneÂ´es reÂ´elles
issues de populations qui restent inconnues.
moyenne variance/2
Mode`le q zs d X2 (err std) G2 (err std) X2 G2
table cible 2Ã— 6
indeÂ´pendance 1 0 5 4.98 (0.22) 4.99 (0.22) 4.69 4.75
arbre 3 0 3 3.05 (0.17) 3.06 (0.17) 2.70 2.70
table cible 3Ã— 88, population SES98, taille eÂ´chantillon 762
indeÂ´pendance 1 0 174 173.4 (1.26) 197.5 (1.46) 157.6 123.1
CHAID 9 0 158 142.9 (1.20) 156.7 (1.13) 142.0 126.0
BIC opt. 6 39 125 123.2 (1.24) 135.5 (1.22) 153.4 146.7
table cible 3Ã— 88, population SES98, taille eÂ´chantillon 100000
indeÂ´pendance 1 0 174 173.4 (1.45) 173.8 (1.45) 207.9 209.7
CHAID 9 0 158 159.0 (1.20) 159.4 (1.19) 144.2 142.1
BIC opt. 6 39 125 127.2 (1.17) 127.7 (1.18) 135.2 138.7
table cible 3Ã— 88, population uniforme, taille eÂ´chantillon 100000
indeÂ´pendance 1 0 174 173.0 (1.32) 173.1 (1.32) 173.7 173.1
CHAID 9 0 158 159.1 (1.24) 159.2 (1.24) 153.2 153.6
BIC opt. 6 0 164 165.2 (1.28) 165.2 (1.28) 163.0 163.2
Tab. 6 â€“ Moyennes et demi-variances observeÂ´es des statistiques X2 et G2. Les valeurs
encadreÂ´es indiquent les moyennes empiriques de X2 et G2 qui sâ€™eÂ´cartent significative-
ment de la valeur d theÂ´orique.
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
100
120
140
160
180
200
220
100 120 140 160 180 200 220
Q obs
Q
 c
hi
-2
80
100
120
140
160
180
200
80 100 120 140 160 180 200
Q obs
Q
 c
hi
-2
Fig. 6 â€“ QQ-plot : a` gauche partition CHAID en 9, eÂ´chantillons de taille 100000, d = 158
degreÂ´s de liberteÂ´ theÂ´oriques et a` droite partition BIC optimale en 6, eÂ´chantillons de
taille 762, d = 125
Les simulations reÂ´aliseÂ´es confirment de facÂ¸on geÂ´neÂ´rale le bien fondeÂ´ des reÂ´sultats
theÂ´oriques, en particulier lorsque lâ€™eÂ´chantillon est suffisamment grand pour assurer des
effectifs attendus conseÂ´quents dans chaque cellule. Dans le cas dâ€™eÂ´chantillons de taille
762 pour la table de 3Ã—88, on a en moyenne moins de 3 cas par case, ce qui est insuffisant
pour justifier la distribution du Ï‡2. Ceci se traduit par des eÂ´carts significatifs entre la
valeur moyenne et les degreÂ´s de liberteÂ´ theÂ´oriques. On remarque eÂ´galement que dans
cette situation le G2 exce`de la valeur de X2 de facÂ¸on importante, de lâ€™ordre de 10%.
Les demi-variances, qui devraient theÂ´oriquement aussi eË†tre eÂ´gales aux degreÂ´s de liberteÂ´,
sont moins convaincantes excepteÂ´ pour la population uniforme et, dans une moindre
mesure, les petites tables.
La figure 6 montre les qq-plots qui comparent les quantiles observeÂ´s (Q obs) des
G2 aux quantiles (Q chi-2) de la distribution du Ï‡2 theÂ´orique pour deux cas : a` gauche
pour la partition CHAID en 9 classes et des eÂ´chantillons de taille 100000, a` droite le cas
deÂ´favorable de la partition BIC optimale avec des eÂ´chantillons de taille 762. Le fait que
sur le graphique de gauche les points soient pratiquement aligneÂ´s sur la droite theÂ´orique
montre que, non seulement la moyenne des G2 correspond aux degreÂ´s de liberteÂ´, mais
que la distribution empirique est tre`s proche de celle du khi-2. Sur le graphique de
droite, on observe que les quantiles observeÂ´s exce`dent systeÂ´matiquement les quantiles
theÂ´oriques. Les points eÂ´tant cependant aligneÂ´s, la forme de Ï‡2 de la distribution ne
semble pas remise en cause. A titre indicatif, un test dâ€™ajustement de Kolmogorov-
Smirnov sur ces deux exemples, donne respectivement un degreÂ´ de signification de 98%
et de 0.0001%. Ce meË†me test de Kolmogorov-Smirnov pour le cas BIC optimal avec
eÂ´chantillons de taille 100000 donne un degreÂ´ de signification de 65%, bien que pour ce
meË†me cas lâ€™eÂ´cart entre la moyenne et les degreÂ´s de liberteÂ´ soit leÂ´ge`rement supeÂ´rieur a`
deux erreurs standards.
Ces analyses par simulations nous invitent a` une certaine prudence en ce qui
concerne les degreÂ´s de signification donneÂ´s dans le cadre de lâ€™illustration de la sec-
tion 5 au tableau 4. Les valeurs indiqueÂ´es donnent cependant clairement des ordres
RNTI - 2
G. Ritschard et D.A. Zighed
de grandeur raisonnables. Pour le G2, les degreÂ´s de liberteÂ´ observeÂ´s dans nos simula-
tions sont toujours supeÂ´rieurs ou eÂ´gaux aux degreÂ´s de liberteÂ´ theÂ´oriques. Les p-valeurs
calculeÂ´es semblent donc eË†tre des bornes infeÂ´rieures ce qui indiquerait que les tests de
significativiteÂ´ sont conservateurs. La preÂ´sence dâ€™eÂ´ventuels zeÂ´ros structurels non deÂ´celeÂ´s
agit cependant en sens inverse. Une borne supeÂ´rieure de leur nombre zs est donneÂ´e
par le nombre zssup de zeÂ´ros dans la table preÂ´dite TË†. En retenant les degreÂ´s de liberteÂ´
corrigeÂ´s (c âˆ’ q)(` âˆ’ 1) âˆ’ zssup, il est neÂ´anmoins possible de sâ€™assurer des degreÂ´s de
signification conservateurs.
7 Conclusion
Cet article aborde la question de la qualiteÂ´ de lâ€™ajustement des arbres dâ€™induction.
Il sâ€™agit dâ€™un aspect peu discuteÂ´ dans la litteÂ´rature sur lâ€™extraction de connaissances
alors meË†me que la qualiteÂ´ dâ€™ajustement fait partie des outils classiques dâ€™eÂ´valuation de
mode`les en statistique. La qualiteÂ´ dâ€™ajustement fournit des indications compleÂ´mentaires
aux indicateurs de qualiteÂ´ traditionnellement utiliseÂ´s pour les arbres dâ€™induction en
permettant, en particulier, dâ€™eÂ´valuer la pertinence statistique dâ€™un arbre induit.
Concre`tement, nous avons montreÂ´, en introduisant les notions dâ€™arbre satureÂ´ et
dâ€™arbre eÂ´tendu, comment adapter aux arbres dâ€™induction les statistiques du khi-2 de
Pearson et du rapport de vraisemblance utiliseÂ´s dans le cadre de la modeÂ´lisation de
tables de contingence. Nous avons eÂ´galement consideÂ´reÂ´ la question de la comparaison
de mode`les pour laquelle la diffeÂ´rence des statistiques G2 du rapport de vraisemblance
permet de tester la significativiteÂ´ statistique du gain dâ€™information dâ€™un mode`le par
rapport a` un mode`le de reÂ´feÂ´rence. Pour la comparaison avec le mode`le dâ€™indeÂ´pendance
ne tenant pas compte des preÂ´dicteurs, nous avons examineÂ´s divers indicateurs de type
R2. Enfin, nous avons vu que lâ€™on pouvait exploiter les crite`res dâ€™information AIC et
BIC pour guider le choix entre arbres de complexiteÂ´ variable.
Ce travail avait pour objectif de montrer comment appliquer des crite`res statistiques
bien eÂ´tablis aux arbres dâ€™induction. Il reste eÂ´videmment encore beaucoup a` faire. Dâ€™une
part, il convient de geÂ´neÂ´raliser la mise en Å“uvre des statistiques et indicateurs discuteÂ´s
dans des cas concrets, et en particulier de les impleÂ´menter dans une proceÂ´dure de
construction dâ€™arbres dâ€™induction.
Lâ€™approche retenue dans cet article qui sâ€™appuie notamment sur les concepts dâ€™arbre
satureÂ´ et dâ€™extension maximale de lâ€™arbre, sâ€™applique lorsque le croisement de toutes
les modaliteÂ´s des attributs donne lieu a` un nombre raisonnable de cateÂ´gories. Dans le
cas particulier de variables quantitatives continues, il y a lieu de discreÂ´tiser les valeurs.
La difficulteÂ´ tient ici au fait que, dans les arbres dâ€™induction, la discreÂ´tisation ne se
fait en re`gle geÂ´neÂ´rale pas a priori mais est deÂ´termineÂ´e en cours de processus de facÂ¸on a`
optimiser la discrimination entre classes.
Une approche possible est de retenir la discreÂ´tisation des variables continues deÂ´finie
par lâ€™ensemble des seuils utiliseÂ´s par le graphe induit. Les variables continues eÂ´tant
ainsi rendues cateÂ´gorielles, la construction de lâ€™arbre satureÂ´ et de lâ€™arbre eÂ´tendu de-
vient possible et la deÂ´marche preÂ´ceÂ´dente sâ€™applique. Câ€™est la deÂ´marche que nous avons
adopteÂ´e dans lâ€™illustration de la section 5. Les reÂ´sultats sont alors conditionnels a` la
discreÂ´tisation retenue, ce qui en limite eÂ´videmment la porteÂ´e. On peut songer a` dâ€™autres
RNTI - 2
QualiteÂ´ dâ€™ajustement dâ€™arbres dâ€™induction
approches prenant en particulier en compte le fait que les seuils de discreÂ´tisation sont
eÂ´galement des parame`tres du mode`le. Ceci meÂ´rite cependant une reÂ´flexion approfondie
qui deÂ´passe le cadre de cet article
Enfin, de facÂ¸on plus geÂ´neÂ´rale, il reste encore beaucoup de questions ouvertes sur la
pertinence statistique des arbres dâ€™induction. Par exemple, la mesure de la fiabiliteÂ´ des
estimations des parame`tres du mode`le de reconstruction (1) issu de lâ€™arbre et celle de
la stabiliteÂ´ de lâ€™arbre induit sont a` nos yeux essentielles pour appreÂ´cier la confiance a`
accorder a` un arbre.
ReÂ´feÂ´rences
Agresti, A. (1990). Categorical Data Analysis. New York : Wiley.
Akaike, H. (1973). Information theory and an extension of the maximum likelihood
principle. In B. N. Petrox et F. Caski (Eds.), Second International Symposium on
Information Theory, pp. 267. Budapest : Akademiai Kiado.
Bishop, Y. M. M., S. E. Fienberg, et P. W. Holland (1975). Discrete Multivariate
Analysis. Cambridge MA : MIT Press.
Breiman, L., J. H. Friedman, R. A. Olshen, et C. J. Stone (1984). Classification And
Regression Trees. New York : Chapman and Hall.
Cressie, N. et T. R. Read (1984). Multinomial goodness-of-fit tests. Journal of the
Royal Statistical Society 46, 440â€“464.
Geurts, P. (2002). Contributions to Decision Tree Induction : Bias/Variance Tradeoff
and Time Series Classification. Lie`ge : FaculteÂ´ des sciences appliqueÂ´es. PhD Thesis.
Goodman, L. A. et W. H. Kruskal (1954). Measures of association for cross classifica-
tions. Journal of the American Statistical Association 49, 732â€“764.
Goodman, L. A. et W. H. Kruskal (1972). Measures of association for cross classifica-
tions IV : simplification of asymptotic variances. Journal of the American Statistical
Association 67, 415â€“421.
Guttman, L. (1941). An outline of the statistical theory of prediction. In P. Horst
et others (Eds.), The Prediction of Personal Adjustment, Volume 8, New York, pp.
253â€“318. Social Science Research Council.
Kass, G. V. (1980). An exploratory technique for investigating large quantities of
categorical data. Applied Statistics 29 (2), 119â€“127.
Kass, R. E. et A. E. Raftery (1995). Bayes factors. Journal of the American Statistical
Association 90 (430), 773â€“795.
Lebart, L., A. Morineau, et M. Piron (2000). Statistique exploratoire multivarieÂ´e (Troi-
sie`me ed.). Paris : Dunod.
Light, R. J. et B. H. Margolin (1971). An analysis of variance for categorical data.
Journal of the American Statistical Association 66 (335), 534â€“544.
Olszak, M. et G. Ritschard (1995). The behaviour of nominal and ordinal partial
association measures. The Statistician 44 (2), 195â€“212.
RNTI - 2
G. Ritschard et D.A. Zighed
Petroff, C., A.-M. Bettex, et A. Korffy (2001, Juin). ItineÂ´raires dâ€™eÂ´tudiants a` la faculteÂ´
des sciences eÂ´conomiques et sociales : le premier cycle. Technical report, UniversiteÂ´
de Gene`ve, FaculteÂ´ SES.
Powers, D. A. et Y. Xie (2000). Statistical Methods for Categorical Data Analysis. San
Diego, CA : Academic Press.
Quinlan, J. R. (1993). C4.5 : Programs for Machine Learning. San Mateo : Morgan
Kaufmann.
Raftery, A. E. (1995). Bayesian model selection in social research. In P. Marsden (Ed.),
Sociological Methodology, pp. 111â€“163. Washington, DC : The American Sociological
Association.
Rakotomalala, R. et D. A. Zighed (1998). Mesures PRE dans les graphes dâ€™induc-
tion : une approche statistique de lâ€™arbitrage geÂ´neÂ´raliteÂ´-preÂ´cision. In G. Ritschard,
A. Berchtold, F. Duc, et D. A. Zighed (Eds.), Apprentissage : des principes naturels
aux meÂ´thodes artificielles, pp. 37â€“60. Paris : Hermes Science Publications.
Ritschard, G. (2003). Partition BIC optimale de lâ€™espace des preÂ´dicteurs. Revue des
nouvelles technologies de lâ€™information 1, 99â€“110.
Ritschard, G. et D. A. Zighed (2003). ModeÂ´lisation de tables de contingence par arbres
dâ€™induction. Revue des sciences et technologies de lâ€™information â€” ECA 17 (1â€“3),
381â€“392.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6,
461â€“464.
Sipina for Windows V2.5 (2000). http ://eric.univ-lyon2.fr. Logiciel.
SPSS (Ed.) (2001). Answer Tree 3.0 Userâ€™s Guide. Chicago : SPSS Inc.
Theil, H. (1967). Economics and Information Theory. Amsterdam : North-Holland.
Theil, H. (1970). On the estimation of relationships involving qualitative variables.
American Journal of Sociology 76, 103â€“154.
Zighed, D. A. et R. Rakotomalala (2000). Graphes dâ€™induction : apprentissage et data
mining. Paris : Hermes Science Publications.
Summary
This paper is concerned with the fit of induction trees. Namely, we explore the possi-
bility to measure the goodness-of-fit as it is classically done in statistical modeling. We
show how Chi-square statistics and especially the Log-likelihood Ratio statistic that is
abundantly used in the modeling of contingency tables can be adapted for induction
trees. Not only is the Log-likelihood Ratio statistic suited for testing the fit. It allows
us also to test the significance of the fit improvement provided by the complexifica-
tion of a tree. In addition, we derive from it adapted forms of the Akaike (AIC) and
Bayesian (BIC) information criteria that prove useful in selecting the best compro-
mise tree between fit and complexity. The practical use of the statistics and indicators
proposed is illustrated on an real example.
RNTI - 2
