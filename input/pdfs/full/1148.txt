EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS â€¢ March, 14-16, 2007 â€¢ Caserta, Italy
1Â© Revue MODULAD, 2007 NumÃ©ro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS â€¢ March, 14-16, 2007 â€¢ Caserta, Italy
2Â© Revue MODULAD, 2007 NumÃ©ro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS â€¢ March, 14-16, 2007 â€¢ Caserta, Italy
3Â© Revue MODULAD, 2007 NumÃ©ro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS â€¢ March, 14-16, 2007 â€¢ Caserta, Italy
4Â© Revue MODULAD, 2007 NumÃ©ro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS â€¢ March, 14-16, 2007 â€¢ Caserta, Italy
5Â© Revue MODULAD, 2007 NumÃ©ro 36
1Fundamentals of 
Analyzing and Mining 
Data Streams 
Graham Cormode
graham@research.att.com
Fundamentals of Analyzing and Mining Data Streams2
Outline
1. Streaming summaries, sketches and samples
â€“ Motivating examples, applications and models
â€“ Random sampling: reservoir and minwise
Â„ Application: Estimating entropy
â€“ Sketches: Count-Min, AMS, FM
2. Stream Data Mining Algorithms
â€“ Association Rule Mining
â€“ Change Detection
â€“ Clustering
2Fundamentals of Analyzing and Mining Data Streams3
Â„ Data is growing faster than our ability to store or 
index it
Â„ There are 3 Billion Telephone Calls in US each day, 
30 Billion emails daily, 1 Billion SMS, IMs. 
Â„ Scientific data: NASA's observation satellites 
generate billions of readings each per day.
Â„ IP Network Traffic: up to 1 Billion packets per hour 
per router.  Each ISP has many (hundreds) routers!
Â„ Whole genome sequences for many species now 
available: each megabytes to gigabytes in size
Data is Massive
Fundamentals of Analyzing and Mining Data Streams4
Massive Data Analysis
Must analyze this massive data:
Â„ Scientific research (monitor environment, species)
Â„ System management (spot faults, drops, failures)
Â„ Customer research (association rules, new offers) 
Â„ For revenue protection (phone fraud, service abuse)
Else, why even measure this data?
3Fundamentals of Analyzing and Mining Data Streams5
Example: Network Data
Â„ Networks are sources of massive data: the metadata per 
hour per router is gigabytes
Â„ Fundamental problem of data stream analysis: 
Too much information to store or transmit
Â„ So process data as it arrives: one pass, small space: the 
data stream approach.
Â„ Approximate answers to many questions are OK, if there 
are guarantees of result quality
Fundamentals of Analyzing and Mining Data Streams6
Streaming Data Questions
Â„ Network managers ask questions requiring us to 
analyze and mine the data: 
â€“ Find hosts with similar usage patterns (clusters)?
â€“ Which destinations or groups use most bandwidth?
â€“ Was there a change in traffic distribution overnight? 
Â„ Extra complexity comes from limited space and time
Â„ Will introduce solutions for these and other problems
4Fundamentals of Analyzing and Mining Data Streams7
Other Streaming Applications
Â„ Sensor networks
â€“ Monitor habitat and environmental parameters
â€“ Track many objects, intrusions, trend analysisâ€¦
Â„ Utility Companies
â€“ Monitor power grid, customer usage patterns etc.
â€“ Alerts and rapid response in case of problems
Fundamentals of Analyzing and Mining Data Streams8
Data Stream Models
Â„ We model data streams as sequences of simple tuples
Â„ Complexity arises from massive length of streams
Â„ Arrivals only streams:
â€“ Example: (x, 3), (y, 2), (x, 2) encodes
the arrival of 3 copies of item x, 
2 copies of y, then 2 copies of x.
â€“ Could represent eg. packets on a network; power usage
Â„ Arrivals and departures:
â€“ Example: (x, 3), (y,2), (x, -2) encodes
final state of (x, 1), (y, 2).
â€“ Can represent fluctuating quantities, or measure 
differences between two distributions
x
y
x
y
5Fundamentals of Analyzing and Mining Data Streams9
Approximation and Randomization
Â„ Many things are hard to compute exactly over a stream
â€“ Is the count of all items the same in two different streams?
â€“ Requires linear space to compute exactly
Â„ Approximation: find an answer correct within some factor
â€“ Find an answer that is within 10% of correct result
â€“ More generally, a (1Â± Îµ) factor approximation
Â„ Randomization: allow a small probability of failure
â€“ Answer is correct, except with probability 1 in 10,000
â€“ More generally, success probability (1-Î´)
Â„ Approximation and Randomization: (Îµ, Î´)-approximations
Fundamentals of Analyzing and Mining Data Streams10
Structure
1. Stream summaries, sketches and samples
â€“ Answer simple distribution agnostic questions about stream
â€“ Describe properties of the distribution
â€“ E.g. general shape, item frequencies, frequency moments
2. Data Mining Algorithms
â€“ Extend existing mining problems to the stream domain
â€“ Go beyond simple properties to deeper structure
â€“ Build on sketch, sampling ideas
Â„ Only a representative sample of each topic, many other 
problems, algorithms and techniques not covered
6Fundamentals of Analyzing and Mining Data Streams11
Outline
1. Streaming summaries, sketches and samples
â€“ Motivating examples, applications and models
â€“ Random sampling: reservoir and minwise
Â„ Application: Estimating entropy
â€“ Sketches: Count-Min, AMS, FM
2. Stream Data Mining Algorithms
â€“ Association Rule Mining
â€“ Change Detection
â€“ Clustering
Fundamentals of Analyzing and Mining Data Streams12
Sampling From a Data Stream
Â„ Fundamental prob: sample m items uniformly from stream
â€“ Useful: approximate costly computation on small sample
Â„ Challenge: donâ€™t know how long stream is  
â€“ So when/how often to sample?
Â„ Two solutions, apply to different situations:
â€“ Reservoir sampling (dates from 1980s?)
â€“ Min-wise sampling (dates from 1990s?)
7Fundamentals of Analyzing and Mining Data Streams13
Reservoir Sampling
Â„ Sample first m items
Â„ Choose to sample the iâ€™th item (i>m) with probability m/i
Â„ If sampled, randomly replace a previously sampled item
Â„ Optimization: when i gets large, compute which item will 
be sampled next, skip over intervening items. [Vitter 85]
Fundamentals of Analyzing and Mining Data Streams14
Reservoir Sampling - Analysis
Â„ Analyze simple case: sample size m = 1
Â„ Probability iâ€™th item is the sample from stream length n:
â€“ Prob. i is sampled on arrival Ã— prob. i survives to end
1 i  i+1 n-2 n-1
i i+1 i+2 n-1 n
Ã— Ã— â€¦ Ã—
= 1/n
Â„ Case for m > 1 is similar, easy to show uniform probability
Â„ Drawbacks of reservoir sampling: hard to parallelize
8Fundamentals of Analyzing and Mining Data Streams15
Min-wise Sampling
Â„ For each item, pick a random fraction between 0 and 1
Â„ Store item(s) with the smallest random tag [Nath et al.â€™04]
0.391 0.908 0.291 0.555 0.619 0.273
Â„ Each item has same chance of least tag, so uniform
Â„ Can run on multiple streams separately, then merge
Fundamentals of Analyzing and Mining Data Streams16
Application of Sampling: Entropy
Â„ Given a long sequence of characters
S = <a1, a2, a3â€¦ am> each aj âˆˆ {1â€¦ n}
Â„ Let fi = frequency of i in the sequence
Â„ Compute the empirical entropy:
H(S) = - âˆ‘i fi/m log fi/m = - âˆ‘i pi log pi
Â„ Example: S = < a, b, a, b, c, a, d, a>
â€“ pa = 1/2, pb = 1/4, pc = 1/8, pd = 1/8
â€“ H(S) = Â½ + Â¼ Ã— 2 + 1/8 Ã— 3 + 1/8 Ã— 3 = 7/4
Â„ Entropy promoted for anomaly detection in networks
9Fundamentals of Analyzing and Mining Data Streams17
Sampling Based Algorithm
Â„ Simple estimator: 
â€“ Randomly sample a position j in the stream 
â€“ Count how many times aj appears subsequently = r
â€“ Output X = -(r log r/m â€“ (r-1) log(r-1)/m)
Â„ Claim: Estimator is unbiased â€“ E[X] = H(S)
â€“ Proof: prob of picking j = 1/m, sum telescopes correctly
Â„ Variance is not too large â€“ Var[X] = O(log2 m)
â€“ Can be proven by bounding |X| â‰¤ log m
Fundamentals of Analyzing and Mining Data Streams18
Analysis of Basic Estimator
Â„ A general technique in data streams:
â€“ Repeat in parallel an unbiased estimator with bounded 
variance, take average of estimates to improve result
â€“ Apply Chebyshev bounds to guarantee accuracy
â€“ Number of repetitions depends on ratio Var[X]/E2[X] 
â€“ For entropy, this means space O(log2m/H2(S))
Â„ Problem for entropy: when H(S) is very small?
â€“ Space needed for an accurate approx goes as 1/H2!
10
Fundamentals of Analyzing and Mining Data Streams19
Outline of Improved Algorithm
Â„ Observation: only way to get H(S) =o(1) is to have only 
one character with pi close to 1
â€“ aaaaaaaaaaaaaaaaaaaaaaaaaaaaaabaaaaa
Â„ If we can identify this character, and make an estimator 
on stream without this token, can estimate H(S)
Â„ How to identify and remove all in one pass?
Â„ Can do some clever tricks with â€˜backup samplesâ€™ by 
adapting the min-wise sampling technique
Â„ Full details and analysis in [Chakrabarti, C, McGregor 07]  
â€“ Total space is O(Îµ-2 log m log 1/Î´) for (Îµ,Î´) approx
Fundamentals of Analyzing and Mining Data Streams20
Outline
1. Streaming summaries, sketches and samples
â€“ Motivating examples, applications and models
â€“ Random sampling: reservoir and minwise
Â„ Application: Estimating entropy
â€“ Sketches: Count-Min, AMS, FM
2. Stream Data Mining Algorithms
â€“ Association Rule Mining
â€“ Change Detection
â€“ Clustering
11
Fundamentals of Analyzing and Mining Data Streams21
Sketches
Â„ Not every problem can be solved with sampling
â€“ Example: counting how many distinct items in the stream
â€“ If a large fraction of items arenâ€™t sampled, donâ€™t know if 
they are all same or all different
Â„ Other techniques take advantage that the algorithm can 
â€œseeâ€ all the data even if it canâ€™t â€œrememberâ€ it all 
Â„ (To me) a sketch is a linear transform of the input
â€“ Model stream as defining a vector, sketch is result of 
multiplying stream vector by an (implicit) matrix
linear projection
Fundamentals of Analyzing and Mining Data Streams22
Trivial Example of a Sketch
Â„ Test if two (asynchronous) binary streams are equal 
d= (x,y) = 0 iff x=y, 1 otherwise
Â„ To test in small space: pick a random hash function h
Â„ Test h(x)=h(y) : small chance of false positive, no chance 
of false negative. 
Â„ Compute h(x), h(y) incrementally as new bits arrive 
(Karp-Rabin: h(x) = xi2i mod p) 
â€“ Exercise: extend to real valued vectors in update model
1 0 1 1 1 0 1 0 1 â€¦
1 0 1 1 0 0 1 0 1 â€¦
12
Fundamentals of Analyzing and Mining Data Streams23
Count-Min Sketch
Â„ Simple sketch idea, can be used for as the basis of many 
different stream mining tasks.
Â„ Model input stream as a vector x of dimension U
Â„ Creates a small summary as an array of w Ã— d in size
Â„ Use d hash function to map vector entries to [1..w]
Â„ Works on arrivals only and arrivals & departures streams
W
d
Array: 
CM[i,j]
Fundamentals of Analyzing and Mining Data Streams24
CM Sketch Structure
Â„ Each entry in vector x is mapped to one bucket per row.
Â„ Merge two sketches by entry-wise summation
Â„ Estimate x[j] by taking mink CM[k,hk(j)]
â€“ Guarantees error less than Îµ||x||1 in size O(1/Îµ log 1/Î´)
â€“ Probability of more error is less than 1-Î´
+c
+c
+c
+c
h1(j)
hd(j)
j,+c
d=log 1/Î´
w = 2/Îµ
[C, Muthukrishnan â€™04]
13
Fundamentals of Analyzing and Mining Data Streams25
Approximation
Approximate xâ€™[j] = mink CM[k,hk(j)]
Â„ Analysis: In k'th row, CM[k,hk(j)] = x[j] + Xk,j
â€“ Xk,j = Î£ x[i] | hk(i) = hk(j)
â€“ E(Xk,j) = Î£ x[k]*Pr[hk(i)=hk(j)] â‰¤ Pr[hk(i)=hk(k)] * Î£ a[i]
= Îµ ||x||1/2 by pairwise independence of h
â€“ Pr[Xk,j â‰¥ Îµ||x||1] = Pr[Xk,j â‰¥ 2E(Xk,j)] â‰¤ 1/2 by Markov inequality 
Â„ So, Pr[xâ€™[j]â‰¥ x[j] + Îµ ||x||1] = Pr[âˆ€ k. Xk,j>Îµ ||x||1] â‰¤1/2log 1/Î´ = Î´
Â„ Final result: with certainty x[j] â‰¤ xâ€™[j] and 
with probability at least 1-Î´,  xâ€™[j]< x[j] + Îµ ||x||1
Fundamentals of Analyzing and Mining Data Streams26
L2 distance
Â„ AMS sketch (for Alon-Matias-Szegedy) proposed in 1996
â€“ Allows estimation of L2 (Euclidean) distance between 
streaming vectors, || x - y ||2
â€“ Used at the heart of many streaming and non-streaming 
mining applications: achieves dimensionality reduction
Â„ Here, describe AMS sketch by generalizing CM sketch. 
Â„ Uses extra hash functions g1...glog 1/Î´ {1...U}Ã† {+1,-1}
Â„ Now, given update (j,+c), set CM[k,hk(i)] += c*gk(j)
linear 
projection
AMS sketch
14
Fundamentals of Analyzing and Mining Data Streams27
L2 analysis
Â„ Estimate ||x||22 = mediank âˆ‘i CM[k,i]2
Â„ Each rowâ€™s result is âˆ‘k g(i)2xi2 + âˆ‘h(i)=h(j) 2 g(i) g(j) xi xj
Â„ But g(i)2 = -12 = +12 = 1, and âˆ‘i xi2 = ||x||22
Â„ g(i)g(j) has 1/2 chance of  +1 or â€“1 : expectation is 0 â€¦
+c*g1(j)
+c*g2(j)
+c*g3(j)
+c*g4(j)
h1(j)
hd(j)
j,+c
d=log 1/Î´
w = 4/Îµ2
Fundamentals of Analyzing and Mining Data Streams28
L2 accuracy
Â„ Formally, one can show an (Îµ, Î´) approximation 
â€“ Expectation of each estimate is exactly ||x||22 and 
variance is bounded by Îµ2 times expectation squared. 
â€“ Using Chebyshevâ€™s inequality, show that probability that 
each estimate is within Â± Îµ ||x||22 is constant
â€“ Take median of log (1/Î´) estimates reduces probability 
of failure to Î´ (using Chernoff bounds)
Â„ Result: given sketches of size O(1/Îµ2 log 1/Î´) can 
estimate ||x||22 so that result is in (1Â±Îµ)||x||22 with 
probability at least 1-Î´ Â‰
â€“ Note: same analysis used many time in data streams
Â„ In Practice: Can be very fast, very accurate!  
â€“ Used in Sprint â€˜CMONâ€™ tool
15
Fundamentals of Analyzing and Mining Data Streams29
0
FM Sketch
Â„ Estimates number of distinct inputs (count distinct)
Â„ Uses hash function mapping input items to i with prob 2-i
â€“ i.e. Pr[h(x) = 1] = Â½, Pr[h(x) = 2] = Â¼, Pr[h(x)=3] = 1/8 â€¦
â€“ Easy to construct h() from a uniform hash function by 
counting trailing zeros  
Â„ Maintain FM Sketch =  bitmap array of L = log U bits 
â€“ Initialize bitmap to all 0s
â€“ For each incoming value x, set FM[h(x)] = 1
x = 5 h(x) = 3 0 0 0 001
FM BITMAP
6      5     4     3     2      1
Fundamentals of Analyzing and Mining Data Streams30
FM Analysis
Â„ If d distinct values, expect d/2 map to FM[1], d/4 to FM[2]â€¦
â€“ Let R = position of rightmost zero in FM, indicator of log(d)
â€“ Basic estimate d = c2R for scaling constant c â‰ˆ 1.3
â€“ Average many copies (different hash fns) improves accuracy
Â„ With O(1/Îµ2 log 1/Î´) copies, get (Îµ,Î´) approximation
â€“ 10 copies gets â‰ˆ 30% error, 100 copies < 10% error
fringe of 0/1s 
around  log(d)
0 0 0 00 1
FM BITMAP
0 00 111 1 11111
position â‰ˆ log(d)position â‰ˆ log(d)
1L R
16
Fundamentals of Analyzing and Mining Data Streams31
Sketching and Sampling Summary
Â„ Sampling and sketching ideas are at the heart of many 
stream mining algorithms
â€“ Entropy computation, association rule mining, clustering 
(still to come)
Â„ A sample is a quite general representative of the data 
set; sketches tend to be specific to a particular purpose
â€“ FM sketch for count distinct, AMS sketch for L2 estimation
Fundamentals of Analyzing and Mining Data Streams32
Practicality
Â„ Algorithms discussed here are quite simple and very fast
â€“ Sketches can easily process millions of updates per 
second on standard hardware
â€“ Limiting factor in practice is often I/O related
Â„ Implemented in several practical systems:
â€“ AT&Tâ€™s Gigascope system on live network streams
â€“ Sprintâ€™s CMON system on live streams
â€“ Googleâ€™s log analysis
Â„ Sample implementations available on the web
â€“ http://www.cs.rutgers.edu/~muthu/massdal-code-index.html
â€“ or web search for â€˜massdalâ€™
17
Fundamentals of Analyzing and Mining Data Streams33
Other Streaming Algorithms
Many fundamentals have been studied, not covered here:
Â„ Different streaming data types
â€“ Permutations, Graph Data, Geometric Data (Location 
Streams)
Â„ Different streaming processing models
â€“ Sliding Windows, Exponential and other decay, Duplicate 
sensitivity, Random order streams, Skewed streams
Â„ Different streaming scenarios
â€“ Distributed computations, sensor network computations
Fundamentals of Analyzing and Mining Data Streams34
Outline
1. Streaming summaries, sketches and samples
â€“ Motivating examples, applications and models
â€“ Random sampling: reservoir and minwise
Â„ Application: Estimating entropy
â€“ Sketches: Count-Min, AMS, FM
2. Stream Data Mining Algorithms
â€“ Association Rule Mining
â€“ Change Detection
â€“ Clustering
18
Fundamentals of Analyzing and Mining Data Streams35
Data Mining on Streams
Â„ Pattern finding: finding common patterns or features
â€“ Association rule mining, Clustering, Histograms, 
Wavelet & Fourier Representations
Â„ Data Quality Issues
â€“ Change Detection, Data Cleaning, Anomaly detection, 
Continuous Distributed Monitoring
Â„ Learning and Predicting
â€“ Building Decision Trees, Regression, Supervised Learning
Â„ Putting it all together: Systems Issues
â€“ Adaptive Load Shedding, Query Languages, Planning and 
Execution
Fundamentals of Analyzing and Mining Data Streams36
Association Rule Mining
Â„ Classic example: supermarket wants to discover 
correlations in buying patterns [Agrawal, Imielinski, Swami 93]
â€“ (bogus) result: diapers Ã† beer
Â„ Input: transactions t1 = {eggs, milk, bread}, t2 = {milk} ...tn
Â„ Output: rules of form {eggs, milk} Ã† bread
â€“ Support: proportion of input containing {eggs, milk, bread}
â€“ Confidence: proportion containing {eggs, milk, bread}
proportion containing {eggs, milk}
Â„ Goal: find rules with support, confidence above threshold
19
Fundamentals of Analyzing and Mining Data Streams37
Frequent Itemsets
Â„ Association Rule Mining (ARM) algorithms first find all 
frequent itemsets: subsets of items with support > Ï†
â€“ m-itemset: itemset with size m, i.e. |X| = m
Â„ Use these frequent itemsets to generate the rules
Â„ Start by finding all frequent 1-itemsets
â€“ Even this is a challenge in massive data streams
Fundamentals of Analyzing and Mining Data Streams38
Heavy Hitters Problem
Â„ The â€˜heavy hittersâ€™ are the frequent 1-itemsets
Â„ Many, many streaming algorithms proposed:
â€“ Random sampling
â€“ Lossy Counting [Manku, Motwani 02]
â€“ Frequent [Misra, Gries 82, Karp et al 02, Demaine et al 02]
â€“ Count-Min, Count Sketches [Charikar, Chen, Farach-Colton 02]
â€“ And many more...
Â„ 1-itemsets used to find, e.g heavy users in a network
â€“ The basis of general frequent itemset algorithms
â€“ A non-uniform kind of sampling
20
Fundamentals of Analyzing and Mining Data Streams39
Space Saving Algorithm
Â„ â€œSpaceSavingâ€ algorithm [Metwally, Agrawal, El Abaddi 05]
merges â€˜Lossy Countingâ€™ and â€˜Frequentâ€™ algorithms
â€“ Gets best space bound, very fast in practice
Â„ Finds all items with count â‰¥ Ï†n, none with count < (Ï†âˆ’Îµ)n
â€“ Error 0 <  Îµ < 1, e.g. Îµ = 1/1000
â€“ Equivalently, estimate each frequency with error Â±Îµn 
Â„ Simple data structure:
â€“ Keep k = 1/Îµ item names and counts, initially zero
â€“ Fill counters by counting first k distinct items exactly
Fundamentals of Analyzing and Mining Data Streams40
SpaceSaving Algorithm
Â„ On seeing new item:
â€“ If it has a counter, increment counter
â€“ If not, replace item with least count, increment count
7
5
123
21
Fundamentals of Analyzing and Mining Data Streams41
SpaceSaving Analysis
Â„ Smallest counter value, min, is at most Îµn
â€“ Counters sum to n by induction
â€“ 1/Îµ counters, so average is Îµn: smallest cannot be bigger
Â„ True count of an uncounted item is between 0 and min
â€“ Proof by induction, true initially, min increases monotonically
â€“ Hence, the count of any item stored is off by at most Îµn
Â„ Any item x whose true count > Îµn is stored 
â€“ By contradiction: x was evicted in past, with count â‰¤ mint
â€“ Every count is an overestimate, using above observation
â€“ So est. count of x > Îµn â‰¥ min â‰¥ mint, and would not be evicted
So: Find all items with count > Îµn, error in counts â‰¤ Îµn
Fundamentals of Analyzing and Mining Data Streams42
Extending to Frequent Itemsets
Â„ Use similar â€œapproximate countingâ€ ideas for finding 
frequent itemsets [Manku, Motwani 02]
â€“ From each new transaction, generate all subsets
â€“ Track potentially frequent itemsets, prune away infrequent
â€“ Similar guarantees: error in count at most Îµn
Â„ Efficiency concerns: 
â€“ Buffer as many transactions as possible, generate subsets 
together so can prune early
â€“ Need compact representation of itemsets
22
Fundamentals of Analyzing and Mining Data Streams43
Trie Representation of subsets
Compact representation of itemsets in lexicographic order.
50
40
30
31 29 32
45
42
50              40             30             31            29
45              32           42
Sets with frequency counts 
Adapted from slides by Gurmeet Manku
Use â€˜a prioriâ€™ rule: if a subset is infrequent, so are all 
of its supersets â€“ so whole subtrees can be pruned
Fundamentals of Analyzing and Mining Data Streams44
ARM Summary
Â„ [Manku, Motwani 02] gives details on when and how to prune
Â„ Final Result: can monitor and extract association rules 
from frequent item sets with high accuracy
Â„ Many extensions and variations to study:
â€“ Space required depends a lot on input, can be many 
potential frequent itemsets
â€“ How to mine when itemsets are observed over many sites 
(e.g. different routers; stores) and guarantee discovery?
â€“ Variant definitions: frequent subsequences, sequential 
patterns, maximal itemsets etc.
â€“ Sessions later in workshopâ€¦
23
Fundamentals of Analyzing and Mining Data Streams45
Outline
1. Streaming summaries, sketches and samples
â€“ Motivating examples, applications and models
â€“ Random sampling: reservoir and minwise
Â„ Application: Estimating entropy
â€“ Sketches: Count-Min, AMS, FM
2. Stream Data Mining Algorithms
â€“ Association Rule Mining
â€“ Change Detection
â€“ Clustering
Fundamentals of Analyzing and Mining Data Streams46
Change Detection
Basic question: monitor a stream of events (network, power 
grid, sensors etc.), detect â€œchangesâ€ for:
â€“ Anomaly detection â€“ trigger alerts/alarms
â€“ Data cleaning â€“ detect errors in data feeds
â€“ Data mining â€“ indicate when to learn a new model
Â„ What is â€œchangeâ€?
â€“ Change in behaviour of some subset of items
â€“ Change in patterns and rules detected
â€“ Change in underlying distribution of frequencies
24
Fundamentals of Analyzing and Mining Data Streams47
Approaches to Change Detection
General idea: compare a reference distribution to a current 
window of events
Â„ Item changes: individual items with big frequency change
â€“ Techniques based on sketches
Â„ Fix a distribution (eg. mixture of gaussians), fit parameters 
â€“ Not always clear which distribution to fix a priori
Â„ Non-parametric change detection
â€“ Few parameters to set, but must specify when to call a 
change significant
Fundamentals of Analyzing and Mining Data Streams48
Non-parametric Change Detection
Technique due to [Dasu et al 06]
Â„ Measure change using Kullback-Leibler divergence (KL)
â€“ Standard measure in statistics
â€“ Many desirable properties, generalizes t-test and Ï‡2
Â„ KL divergence = D(p||q) = Î£x p(x) log2 p(x)/q(x)
â€“ for probability distributions p, q
â€“ If p, q are distributions over high dimensional spaces, no 
intersection between samples â€“ need to capture density 
25
Fundamentals of Analyzing and Mining Data Streams49
Space Division Approach
Â„ Use a hierarchical space division (kd-tree) to define r
regions ri of (approximately equal) weight for the 
reference data
Â„ Compute discrete probability 
p over the regions
Â„ Apply same space division 
over a window of recent 
stream items to create q
Â„ Compute KL divergence D(p||q)
r1
r2
r3
r4
r5
r6
Fundamentals of Analyzing and Mining Data Streams50
Bootstrapping
How to tell if the KL divergence is significant?
Â„ Statistical bootstrapping approach: use the input data to 
compute a distribution of distances
Â„ Pool reference and first sliding window data, randomly 
split into two pieces, measure KL divergence
Â„ Repeat k times, find e.g. 0.99 quantile of divergences
Â„ If KL distance between reference and window > 0.99 
quantile of distances for several steps, declare â€œchangeâ€
26
Fundamentals of Analyzing and Mining Data Streams51
Streaming Computation
For each update:
Â„ Slide window, update region counts
Â„ Update KL divergence between 
reference p and window q, size w
Â„ Test for significance
Reference Sliding Window
Fundamentals of Analyzing and Mining Data Streams52
Efficient Implementation
Â„ Donâ€™t have to recompute KL divergence from scratch
â€“ Can write normalized KL divergence in terms of 
Î£i (p(ri) + 1/(2w)) log
â€“ Only two terms change per update
Â„ Total time cost per update: 
â€“ Update two regional counts in kd-tree, O(log w)
â€“ Update KL divergence, in time O(1)
â€“ Compare to stored divergence cut off for significance test
â€“ Overall, O(log w)
Â„ Space cost: store tree and counts, O(w)
p(ri) + 1/(2w)
q(ri) + 1/(2w)
27
Fundamentals of Analyzing and Mining Data Streams53
Change Detection Summary
Â„ Proposed technique is pretty efficient in practice
â€“ Competitive in accuracy with custom, application-aware 
change detection
â€“ Pretty fast â€“ tens of microseconds per update
â€“ Produces simple description of change based on regions
Â„ Extensions and open problems:
â€“ Other approaches â€“ histogram or kernel based?
â€“ Better bootstrapping: quantile approach is only first order 
accurateâ€¦
Fundamentals of Analyzing and Mining Data Streams54
Outline
1. Streaming summaries, sketches and samples
â€“ Motivating examples, applications and models
â€“ Random sampling: reservoir and minwise
Â„ Application: Estimating entropy
â€“ Sketches: Count-Min, AMS, FM
2. Stream Data Mining Algorithms
â€“ Association Rule Mining
â€“ Change Detection
â€“ Clustering
28
Fundamentals of Analyzing and Mining Data Streams55
Clustering Data Streams
Â„ We often talk informally about â€œclustersâ€: â€˜cancer 
clustersâ€™, â€˜disease clustersâ€™ or â€˜crime clustersâ€™
Â„ Clustering has an intuitive appeal.  We see a bunch of 
items... we want to discover the clusters...
Fundamentals of Analyzing and Mining Data Streams56
Stream Clustering Large Points
For clustering, need to compare the points.  What 
happens when the points are very high dimensional? 
â€“ Eg. trying to compare whole genome sequences
â€“ comparing yesterdayâ€™s network traffic with todayâ€™s 
â€“ clustering huge texts based on similarity
Â„ If each point is size d, d very large  cost is very high 
(at least O(d). O(d2) or worse for some metrics)
Â„ We can do better: create a sketch for each point
Â„ Do clustering using sketched approximate distances 
29
Fundamentals of Analyzing and Mining Data Streams57
Stream Clustering Many Points
Â„ What does it mean to cluster on the stream when there 
are too many points to store?
Â„ We see a sequence of points one after the other, and we 
want to output a clustering for this observed data. 
Â„ Moreover, since this clustering changes with time, for 
each update we maintain some summary information, 
and at any time can output a clustering. 
Â„ Data stream restriction: data is 
assumed too large to store, 
so we do not keep all the input, 
or any constant fraction of it. 
Fundamentals of Analyzing and Mining Data Streams58
Clustering for the stream
Â„ What should output of a stream clustering algorithm be?
Â„ Classification of every input point?  
Too large to be useful? 
Might this change as more input points arrive?
â€“ Two points which are initially put in different clusters might 
end up in the same one
Â„ An alternative is to output k cluster centers at end 
â€“ any point can be classified using these centers.
Input: Output:
30
Fundamentals of Analyzing and Mining Data Streams59
Approximation for k-centers
k-center: minimize diameter (max dist) of each cluster.
Â„ Pick some point from the data as the first center. 
Repeat:
â€“ For each data point, compute distance dmin from 
its closest center
â€“ Find the data point that maximizes dmin
â€“ Add this point to the set of centers
Until k centers are picked
Â„ If we store the current best center for each point, then 
each pass requires O(1) time to update this for the new 
center, else O(k) to compare to k centers.
Â„ So time cost is O(kn), but k passes [Gonzalez, 1985].
Fundamentals of Analyzing and Mining Data Streams60






ALG:
Select an arbitrary center c1
Repeat until have k centers
Select the next center ci+1 to  
be the one farthest from its 
closest center
Gonzalez Clustering k=4
Slide due to Nina Mishra
31
Fundamentals of Analyzing and Mining Data Streams61








Gonzalez Clustering k=4
Slide due to Nina Mishra
Fundamentals of Analyzing and Mining Data Streams62

	
  













Gonzalez Clustering k=4


 ! 

"#$


% $
 


 &
'!
$

($â‰¤ ) *(
+ ($
 

% 
,
"$
 

$ #

â‰¥&


â‰¥ # 



-
â‰¥ # -
.

/ $ 
32
Fundamentals of Analyzing and Mining Data Streams63
Gonzalez is 2-approximation
Â„ After picking k points to be centers, find next point that 
would be chosen. Let distance from closest center = dopt
Â„ We have k+1 points, every pair is separated by at least 
dopt. Any clustering into k sets must put some pair in 
same set, so any k-clustering must have diameter dopt
Â„ For any two points allocated to the same center, they are 
both at distance at most dopt from their closest center
Â„ Their distance is at most 2dopt, using triangle inequality.
Â„ Diameter of any clustering must be at least dopt, and is at 
most 2dopt â€“ so we have a 2 approximation.
Â„ Lower bound: NP-hard to guarantee better than 2
Fundamentals of Analyzing and Mining Data Streams64
Gonzalez Restated
Â„ Suppose we knew dopt (from Gonzalez algorithm for k-
centers) at the start
Â„ Do the following procedure:
Â„ Select the first point as the first center
Â„ For each point that arrives:
â€“ Compute dmin, the distance to the closest center
â€“ If dmin > dopt then set the new point to be a new 
center dopt
33
Fundamentals of Analyzing and Mining Data Streams65
Analysis Restated
Â„ dopt is given, so we know that there are k+1 points 
separated by â‰¥ dopt and dopt is as large as possible
Â„ So there are â‰¤ k points separated by > dopt
Â„ New algorithm outputs at most k centers: only include 
a center when its distance is > dopt from all others. 
If > k centers output, then > k points separated by > 
dopt, contradicting optimality of dopt. 
Â„ Every point not chosen as a center is < dopt from some 
center and so at most 2dopt from any point allocated to 
the same center (triangle inequality)
Â„ So: given dopt we find a clustering where every point is 
at most twice this distance from its closest center
Fundamentals of Analyzing and Mining Data Streams66
Guessing the optimal solution
Â„ Hence, a 2-approximation â€“ but, we arenâ€™t given dopt
â€“ If we knew d < dopt < 2d then we could run the algorithm.  If 
we find more than k centers, we guessed dopt too low
â€“ So, in parallel, guess dopt = 1, 2, 4, 8...
â€“ We reject everything < dopt, so best guess is < 2dopt:
our output will be < 2*2dopt/dopt = 4 approx
Â„ Need log2 (dmax/dsmallest) guesses, dsmallest is minimum 
distance between any pair of points, as dsmallest < dopt
Â„ O(k log(dmax / dsmallest) may be high, can we reduce more?
Â„ [Charikar et al 97]: doubling alg uses only O(k) space, gives 
8-approximation. Subsequent work studied other settings
34
Fundamentals of Analyzing and Mining Data Streams67
Clustering Summary
Â„ General techniques: keeping small subset (â€œcore-setâ€) of 
input; guessing a key value; combining subproblems
Â„ Many more complex solutions from computational 
geometry
Â„ Variations and extensions:
â€“ When few data points but data points are high 
dimensional, use sketching techniques to represent
â€“ Different objectives: k-median, k-means, etc.
â€“ Better approximations, different guarantees (e.g. outputs 
2k clusters, quality as good as that of best k-clustering)
Fundamentals of Analyzing and Mining Data Streams68
Summary
Â„ We have looked at
â€“ Sampling from streams and applications (entropy)
â€“ Sketch summaries for more advanced computations
â€“ Association Rule Mining to find interesting patterns
â€“ Change Detection for anomaly detection and alerts
â€“ Clustering to pick out significant clusters
Â„ Many other variations to solve the problems discussed 
here, many other problems to study on data streams
â€“ See more over the course of this workshop.
â€“ Other tutorials and surveys: [Muthukrishnan â€™05]
[Garofalakis, Gehrke, Rastogi â€™02]
35
Fundamentals of Analyzing and Mining Data Streams69
References
[Agrawal, Imielinski, Swami â€™93] R. Agrawal, T. Imielinski, A. Swami.  Mining 
Association Rules between Sets of Items in Large Databases. Proceedings 
of the ACM SIGMOD Conference on Management of Data, 1993.
[Alon, Matias, Szegedy â€™96] N. Alon, Y. Matias, and M. Szegedy. The space 
complexity of approximating the frequency moments. In Proceedings of the 
ACM Symposium on Theory of Computing, pages 20â€“29, 1996. 
[Chakrabarti, Cormode, McGregor â€™07] A. Chakrabarti, G. Cormode, and A. McGregor. 
A near-optimal algorithm for computing the entropy of a stream. In 
Proceedings of ACM-SIAM Symposium on Discrete Algorithms, 2007.
[Charikar, Chen, Farach-Colton â€™02] M. Charikar, K. Chen, and M. Farach-Colton. 
Finding frequent items in data streams. In Procedings of the International 
Colloquium on Automata, Languages and Programming (ICALP), 2002.
[Cormode, Muthukrishnan â€™04] G. Cormode and S. Muthukrishnan. An improved data 
stream summary: The count-min sketch and its applications.  Journal of 
Algorithms, 55(1):58â€“75, 2005.
Fundamentals of Analyzing and Mining Data Streams70
References
[ [Dasu et al â€™06] T. Dasu, S. Krishnan, S. Venkatasubramanian, K. Yi.  An 
Information Theoretic Approach to Detecting Changes in Multi-Dimensional 
Data Streams. Proceedings of the 38th Symposium on the Interface of 
Statistics, Computing Science, and Applications (Interface), 2006.
[Demaine et al â€™03] E. Demaine, A. LÃ³pez-Ortiz, and J. I. Munro. Frequency 
estimation of internet packet streams with limited space. In Proceedings of 
the 10th Annual European Symposium on Algorithms, volume 2461 of
Lecture Notes in Computer Science, pages 348â€“360, 2002.
[Garofalakis, Gehrke, Rastogi â€™02] M. Garofalakis and J. Gehrke and R. Rastogi. 
Querying and Mining Data Streams: You Only Get One Look. ACM 
SIGMOD Conference on Management of Data, 2002
[Gonzalez â€™85] T. F. Gonzalez. Clustering to minimize the maximum intercluster
distance. Theoretical Computer Science, 38(2-3):293â€“306, 1985.
[Karp, Papadimitriou, Shenker â€™03] R. Karp, C. Papadimitriou, and S. Shenker. A 
simple algorithm for finding frequent elements in sets and bags. ACM 
Transactions on Database Systems, 2003.
36
Fundamentals of Analyzing and Mining Data Streams71
References
[Manku, Motwani â€™02] G.S. Manku and R. Motwani. Approximate frequency counts 
over data streams. In Proceedings of International Conference on Very 
Large Data Bases, pages 346â€“357, 2002.
[Metwally, Agrawal, El Abbadi â€™05] A. Metwally, D. Agrawal, and A. El Abbadi. 
Efficient computation of frequent and top-k elements in data streams. In 
Proceedings of ICDT, 2005.
[Misra, Gries â€™82] J. Misra and D. Gries. Finding repeated elements. Science of 
Computer Programming, 2:143â€“152, 1982.
[Muthukrishnan â€™05] S. Muthukrishnan. Data Streams: Algorithms and Applications. 
Now Publishers, 2005.
[Nath et al.â€™04] S. Nath, P. B. Gibbons, S. Seshan, and Z. R. Anderson. Synopsis 
diffusion for robust aggrgation in sensor networks.  In ACM SenSys, 2004.
[Vitter â€™85] J. S. Vitter. Random Sampling with a Reservoir, ACM Transactions on 
Mathematical Software, 11(1), March 1985, 37-57.
