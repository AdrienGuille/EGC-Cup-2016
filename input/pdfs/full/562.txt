Une famille de matrices sparses pour une modÃ©lisation
multi-Ã©chelle par blocs
Camille Brunetâˆ—, Thomas Villmanâˆ—âˆ—
Vincent Vigneronâˆ—
âˆ—IBISC, UniversitÃ© dâ€™Evry Val dâ€™Essonnes
40 rue du Pelvoux - 91020 Evry Courcouronne - France
camille.brunet@ibisc.univ-evry.fr
âˆ—âˆ—University of applied sciences Mittweida
Technikumplatz 17 - 09648 Mittweida - Germany
RÃ©sumÃ©. La sÃ©riation est une technique dâ€™analyse de donnÃ©es qui ordonne les
observations directement Ã  partir de leur tableau de valeurs afin de rÃ©vÃ©ler une
structure intrinsÃ¨que Ã  ces donnÃ©es. Une telle approche prÃ©sente de nombreux
avantages de visualisation mais dÃ¨s lors que les donnÃ©es sont bruitÃ©es ou que les
groupes se superposent, la visualisation de toute structure devient difficile. Pour
faire face Ã  ces problÃ¨mes, nous introduisons de la parcimonie dans les donnÃ©es
Ã  travers une famille de matrices indicatrices de voisins communs. Celles-ci sont
ordonnÃ©es selon un algorithme de type branch and bound et la matrice rÃ©vÃ©lant
la meilleure structure au sens de "diagonale par blocs" est sÃ©lectionnÃ©e au moyen
dâ€™un critÃ¨re dÃ©rivÃ© des problÃ©matiques de compression de donnÃ©es. Cet outil de
partitionnement identifie des sous-ensembles de donnÃ©es relatifs aux clusters
tout en Ã©cartant celles qui sont bruitÃ©es ou extrÃªmes ce qui permet de visualiser
la structure globale intrinsÃ¨que aux donnÃ©es. Cependant, une trop grande spar-
sitÃ© des donnÃ©es amÃ¨ne parfois Ã  lâ€™Ã©viction de donnÃ©es sous-reprÃ©sentÃ©es; nous
proposons Ã  cet effet, une approche multi-Ã©chelle combinant diffÃ©rents niveaux
de sparsitÃ© dans une mÃªme visualisation.
1 Introduction
La notion de complexitÃ© des donnÃ©es peut sâ€™apprÃ©hender de diffÃ©rentes maniÃ¨res dans la
littÃ©rature. Elle peut en effet Ãªtre liÃ©e Ã  la nature et Ã  lâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© des donnÃ©es, Ã  leur di-
mension plus ou moins grande, Ã  leur qualitÃ© (donnÃ©es trÃ¨s bruitÃ©es ou non) ou encore Ã  leur
structure globale qui peut Ãªtre telle que la distinction de groupes dans les donnÃ©es est rendue
difficile. Les outils statistiques dâ€™analyse de donnÃ©es ont pour objectif dâ€™extraire de lâ€™infor-
mation. Ils cherchent Ã  explorer des donnÃ©es multidimensionnelles pour les synthÃ©tiser dâ€™une
part et permettre de structurer lâ€™information contenue dans ces donnÃ©es dâ€™autre part. Dans la
littÃ©rature, il existe plusieurs approches associÃ©es Ã  cette dÃ©marche telles les mÃ©thodes fac-
torielles (Pearson (1901); Jolliffe (1986)), les approches par classification non supervisÃ©e â€“
SparsitÃ© et sÃ©riation
ou clustering â€“ ou encore les mÃ©thodes de visualisation. Ces derniÃ¨res peuvent Ãªtre globales
puisquâ€™elles se basent sur les proximitÃ©s entre les diffÃ©rents groupes telles les cartes de Koho-
nen (Kohonen (1995)) ou les mÃ©thodes de graphes ; locales puisquâ€™elles Ã©valuent la proximitÃ©
entre les individus, comme les mÃ©thodes hiÃ©rarchiques ; ou encore elles peuvent combiner les
relations locales et globales, comme dans le cas de la sÃ©riation. Cette derniÃ¨re approche a la
particularitÃ© de travailler directement sur le tableau de valeurs que ce soit sur des matrices
symÃ©triques pour lesquelles les Ã©lÃ©ments en lignes et en colonnes se rÃ©fÃ¨rent Ã  la mÃªme en-
titÃ©, comme les tableaux de donnÃ©es relationnelles ou de dissimilaritÃ©s ; ou que ce soit sur des
matrices de type rectangulaire pour lesquelles les Ã©lÃ©ments en lignes diffÃ¨rent de ceux en co-
lonnes ; par exemple, les individus sont reprÃ©sentÃ©s en lignes tandis que les attributs le sont
sur les colonnes. Bien que la sÃ©riation soit un champ de la statistique trÃ¨s vaste du fait de ses
multiples origines (cf. I.Liiv (2010)) et de ses diverses Ã©volutions (cf. Marcotorchino (1987);
Mechelen et al. (2004)), une taxonomie des mÃ©thodes a Ã©tÃ© proposÃ©e par Carroll et Arabie
(1980) et associe Ã  chaque type de tableau un mode de sÃ©riation particulier. Lâ€™objectif de la
sÃ©riation est de rÃ©ordonner les lignes et les colonnes dâ€™un tableau de valeurs par des permu-
tations successives de sorte que les lignes adjacentes (respectivement les colonnes) soient les
plus semblables. Cette situation est illustrÃ©e par la figure 1 oÃ¹, Ã  partir dâ€™un tableau de donnÃ©es
relationnelles prÃ©sentÃ© par la figure 1 de gauche, les lignes et les colonnes ont permutÃ© pour
former une partition dans laquelle les Ã©lÃ©ments similaires ont Ã©tÃ© rassemblÃ©s entre eux, formant
ainsi des groupes (figure 1 du milieu), et afin de mieux apprÃ©cier la prÃ©sence de la structure
diagonale par blocs, cette matrice ordonnÃ©e est pixellisÃ©e (figure 1 de droite). Une telle ap-
proche pourrait sâ€™apparenter Ã  une technique locale de clustering ordonnÃ© dans la mesure oÃ¹
une information est apportÃ©e dâ€™une part sur les relations locales entre individus du fait dâ€™un
ordre dans les donnÃ©es et dâ€™autre part sur la structure globale des donnÃ©es. Cependant, la sÃ©-
riation prÃ©sente dâ€™autres avantages mis en avant par plusieurs auteurs tels Arabie et al. (1996),
comme lâ€™absence de connaissance a priori sur le nombre de clusters et la visualisation directe
de la structure sur le tableau de valeurs.
FIG. 1 â€“ Effets dâ€™un algorithme de rangement sur un jeu de donnÃ©es : matrice de relations
initiale (Ã  gauche), matrice ordonnÃ©e (au milieu) et reprÃ©sentation pixellisÃ©e de la matrice
ordonnÃ©e (Ã  droite).
Ces avantages se voient annulÃ©s dÃ¨s lors que les donnÃ©es sont bruitÃ©es ou que les
groupes de donnÃ©es se superposent. En effet, la prÃ©sence de donnÃ©es bruitÃ©es empÃªche une
visualisation claire des diffÃ©rents blocs et distinguer des clusters devient alors une tÃ¢che
difficile. Dans ce travail nous apprÃ©henderons la complexitÃ© des donnÃ©es Ã  travers la notion
C. Brunet et al.
de structure : des donnÃ©es seront complexes dÃ¨s lors que leur structuration, leur agencement
en groupes sera difficilement apprÃ©hendable. Cette complexitÃ© se traduira par des donnÃ©es
fortement bruitÃ©es ou par des recouvrements de classes rendant difficile la visualisation dâ€™une
structure. Dans ce contexte, notre travail tente dâ€™amÃ©liorer les procÃ©dures traditionnelles de
sÃ©riation dans la recherche de sous-ensembles de donnÃ©es caractÃ©ristiques de la structure.
Pour ce faire, nous introduisons une famille de matrices parcimonieuses relatives Ã  diffÃ©rents
niveaux de parcimonie des donnÃ©es, lesquelles sont ordonnÃ©es selon un algorithme de type
branch and bound pour faire Ã©merger une structure compacte par blocs. Un critÃ¨re dÃ©rivÃ©
de la problÃ©matique de compression des donnÃ©es sÃ©lectionne la matrice ordonnÃ©e la plus
compacte â€“au sens de diagonale par blocsâ€“ afin dâ€™obtenir la visualisation la plus informative
de la structure intrinsÃ¨que des donnÃ©es. Cependant, dans certaines situations, une trop grande
parcimonie engendre lâ€™Ã©viction de donnÃ©es sous-reprÃ©sentÃ©es formant de trÃ¨s petits clusters.
Pour pallier cette non-dÃ©tection, nous proposons une approche multi-Ã©chelle combinant
diffÃ©rents niveaux de sparsitÃ© des donnÃ©es.
Cet article est organisÃ© de la maniÃ¨re suivante : dans la Section 2, un Ã©tat de lâ€™art de la
sÃ©riation sera prÃ©sentÃ© selon deux points de vue diffÃ©rents, lâ€™un basÃ© sur les fondements
mathÃ©matiques du problÃ¨me de la sÃ©riation et lâ€™autre sur ses fondements algorithmiques.
La Section 3 dÃ©taillera notre approche parcimonieuse ainsi que lâ€™algorithme de rangement
Parcimonious Block-Clustering multi-Ã©chelle proposÃ©. Des expÃ©riences sur simulations et sur
des donnÃ©es de rÃ©fÃ©rence seront prÃ©sentÃ©es dans la Section 4.
2 La sÃ©riation
Liiv (I.Liiv (2010)) propose une dÃ©finition informelle de la sÃ©riation : [Seriation] is an
exploratory data analysis technique to reorder objects into a sequence along one dimensional
continuum so that it best reveals regularity and patterning among the whole series. Autrement
dit, la sÃ©riation cherche un ordre dans les donnÃ©es de sorte Ã  exprimer une localitÃ©/proximitÃ©
entre lignes ou colonnes adjacentes et faire ainsi Ã©merger une structure. Cet ordre est obtenu
par permutations successives des lignes et des colonnes ce qui permet dâ€™aborder le problÃ¨me
de la sÃ©riation Ã  travers deux angles diffÃ©rents : lâ€™un Ã©tant de fondement mathÃ©matique puisque
lâ€™enjeu est de dÃ©terminer lâ€™ensemble des meilleures permutations possibles, lâ€™autre Ã©tant de
fondement algorithmique du fait de la complexitÃ© de la solution (problÃ¨me np-complet).
2.1 ProblÃ¨me dâ€™optimisation
La problÃ©matique de la sÃ©riation se pose dans la dÃ©finition et lâ€™Ã©valuation de la meilleure
permutation possible, ce qui permet de la formuler comme un problÃ¨me dâ€™optimisation. Lâ€™ap-
proche par sÃ©riation sâ€™applique Ã  tout type de matrices, cependant, dans notre travail, nous
nous focaliserons sur des matrices de dissimilaritÃ©s. ConsidÃ©rons alors un ensemble de N Ã©lÃ©-
ments (x1, . . . , xN ) dÃ©crits par une matrice symÃ©trique de dissimilaritÃ© D = (dij)i,jâˆˆ(1,...,N)
de tailleNÃ—N oÃ¹ chaque Ã©lÃ©ment dij rend compte de la dissimilaritÃ© entre la paire dâ€™observa-
tions (xi, xj). On dÃ©finit Î¨ une fonction de permutation qui ordonne, selon un certain critÃ¨re
C, les Ã©lÃ©ments de la matrice D. Lâ€™objectif de la sÃ©riation est donc de trouver la fonction de
permutation optimale Î¨âˆ— qui optimise le critÃ¨re C de rangement, telle que :
SparsitÃ© et sÃ©riation
Î¨âˆ— = argmax
Î¨
C(Î¨(D)). (1)
Ces critÃ¨res prennent diffÃ©rentes formes dans la littÃ©rature mais une grande partie dâ€™entre eux
se base sur une mesure de similaritÃ© s(.) entre les Ã©lÃ©ments successifs de la matrice D et qui
vÃ©rifie
max
nâˆ’1âˆ‘
i=1
s(i, i+ 1).
Cette mesure de similaritÃ© se dÃ©cline de maniÃ¨re diffÃ©rente selon les auteurs comme on peut
lâ€™observer dans la table 1. McCormick et al. (1972) et Arabie et Hubert (1990) par exemple
cherchent Ã  maximiser une mesure dâ€™efficacitÃ© (measure of effectiveness) (cf. critÃ¨re C6 dans
la table 1) basÃ©e sur la somme des produits scalaires en lignes et en colonnes de la matrice
des donnÃ©es ; mesure qui a Ã©tÃ© par la suite gÃ©nÃ©ralisÃ©e par Climer et Zhang (2006). Dâ€™autres
auteurs, tels que Hubert et al. (2001) ou Chen (2002) se sont basÃ©s sur lâ€™optimisation dâ€™une
mesure de divergence entre la matrice de dissimilaritÃ© et une structure anti-Robinson cherchant
Ã  regrouper les valeurs de dissimilaritÃ©s les plus petites autour de la diagonale (cf. critÃ¨re C4).
A lâ€™inverse, certains auteurs tels Caraux et Pinloche (2005) (cf. critÃ¨res C1 et C2) ou Brusco et
Steinley (2006) (cf. critÃ¨re C3) cherchent plutÃ´t Ã  placer les plus petites dissimilaritÃ©s hors de
la diagonale (structure Robinson). Enfin, dans le cadre du problÃ¨me de compression de don-
nÃ©es, Johnson et al. (2004) ont proposÃ© de minimiser un critÃ¨re basÃ© sur le nombre de sÃ©quences
dâ€™Ã©lÃ©ments consÃ©cutifs (en ligne) diffÃ©rents de 0 (cf. critÃ¨re C5). Nombre dâ€™auteurs ont travaillÃ©
sur la proposition de nouveaux critÃ¨res de rangement comme Niermann (2005) qui cherche Ã 
comparer chaque observation Ã  ses voisins adjacents Ã  travers des critÃ¨res relatifs au voisinage
(cf. critÃ¨re C7) ou Batagelj (1997) et Doreian et al. (2004) qui proposent des critÃ¨res dâ€™Ã©quiva-
lence structurelle ou encore Dhillon et al. (2003) qui utilise lâ€™information mutuelle et un critÃ¨re
dâ€™entropie. Il existe une grande littÃ©rature sur ce sujet et une taxonomie de ces techniques est
disponible dans Hahsler et al. (2009). De plus, cette approche par sÃ©riation peut sâ€™inscrire dans
un domaine plus Ã©tendu quâ€™est la classification croisÃ©e, appelÃ©e aussi biclustering et une vue
dâ€™ensemble de ces diffÃ©rentes approches est dÃ©taillÃ©e dans lâ€™article de Mechelen et al. (2004).
En particulier, des auteurs tels Long et al. (2005) se sont orientÃ©s vers une dÃ©composition en
valeurs blocs de la matrice pour traiter des donnÃ©es de grandes dimensions ou encore Govaert
et Nadif (2008, 2010) ont abordÃ© de maniÃ¨re stochastique cette problÃ©matique de permutations
dans un modÃ¨le probabiliste en rÃ©duisant le rangement de la matrice Ã  un biclustering guidÃ©
par un modÃ¨le binomial. Cependant ces approches rÃ©centes nÃ©cessitent une connaissance a
priori du nombre de clusters formÃ©s par les individus et par les variables dont leur dÃ©termina-
tion nâ€™est pas toujours chose facile. Enfin, notons que la sÃ©riation vue comme un problÃ¨me de
maximisation est comparÃ©e par certains auteurs, tels Applegate et al. (2007), au problÃ¨me du
voyageur de commerce (TSP). En effet, une matrice de dissimilaritÃ©s peut Ãªtre perÃ§ue comme
un graphe pondÃ©rÃ© que lâ€™on parcourt. Ainsi, ordonner des lignes et colonnes de sorte que les
lignes (respectivement les colonnes) adjacentes soient les plus similaires possibles revient Ã 
chercher le parcours minimal pour â€œvisiterâ€ chaque observation une seule fois.
C. Brunet et al.
type CritÃ¨res Ã  optimiser selon la matrice de dissimilaritÃ© D = {dij}i,jâˆˆ{1,...,n}
critÃ¨res C1 =
nâˆ‘
i=1
nâˆ‘
j=1
dij |iâˆ’ j|2
structurels
C2 =
nâˆ‘
i=1
nâˆ‘
i=1
(dij âˆ’ Î±|iâˆ’ j|2)
C3 =
nâˆ’2âˆ‘
i=1
nâˆ’1âˆ‘
j=i+1
nâˆ‘
k=j+1
sign(dij âˆ’ dik) + sign(djk âˆ’ dik)
C4 =
âˆ‘
16i<j<k6n
f(dik, dij) +
âˆ‘
16i<j<k6n
f(dkj , dij) avec
â€”â€”â€”â€”- f(x, y) = sign(xâˆ’ y)
f(x, y) = |xâˆ’ y|sign(xâˆ’ y)
f(x, y) = I{x>y}
f(x, y) = |xâˆ’ y|I{x>y}
C5 =
nâˆ‘
i=1
nâˆ’1âˆ‘
j 6=i,j=1
|dij âˆ’ di,j+1|
critÃ¨res de C6 = 12
nâˆ‘
i,j=1
dij(di,jâˆ’1 + di,j+1 + diâˆ’1,j + di+1,j)
similaritÃ©s
C7 =
nâˆ‘
i,j=1
fij avec
â€”â€”â€”â€”- fij =
âˆ‘min(n,i+1)
k=max(1,iâˆ’1)
âˆ‘min(n,j+1)
`=max(i,jâˆ’1)(dij âˆ’ dk`)2
fij =
âˆ‘min(n,i+1)
k=max(1,iâˆ’1)(dij âˆ’ dkj)2 +
âˆ‘min(n,j+1)
`=max(i,jâˆ’1)(dij âˆ’ di`)2
1TAB. 1 â€“ Table des critÃ¨res de rangement utilisÃ©s dans le cadre du one-mode clustering.
SparsitÃ© et sÃ©riation
2.2 Principe algorithmique et coÃ»t calculatoire
Trouver la permutation optimale est un problÃ¨me np-complet. Il suppose le calcul de
toutes les combinaisons de permutations possibles de lignes et de colonnes, soit n!p! pour un
tableau rectangulaire ou n! dans le cas dâ€™une matrice de dissimilaritÃ© symÃ©trique. Le calcul
de toutes ces permutations possibles peut Ãªtre envisagÃ© lorsque le jeu de donnÃ©es est de petite
taille mais cette tÃ¢che devient difficile dÃ¨s lors que n > 30 (plus de 232 possibilitÃ©s). La
littÃ©rature propose un certain nombre dâ€™algorithmes de sÃ©riation. Un des algorithmes les plus
connus nommÃ© branch and bound suggÃ¨re de faire une recherche exhaustive des permutations
non pas sur lâ€™ensemble des donnÃ©es mais sur plusieurs sous-ensembles. Dans les annÃ©es
cinquantes, plusieurs auteurs tels Croes (1958) Eastman (1958) et Rossman et al. (1958) ont
proposÃ© de telles mÃ©thodes ; celles-ci ont Ã©tÃ© rÃ©exploitÃ©es plus rÃ©cemment par Chen (2002)
ou par Brusco et Stahl (2005). Un autre type de mÃ©thodes algorithmiques se base sur des
recherches heuristiques ; celles-ci ont Ã©tÃ© exploitÃ©es initialement par McCormick et al. (1972),
puis par Arabie et Hubert (1990) qui utilisent la notion de voisinage pour dÃ©finir une liste de
candidats potentiels considÃ©rÃ©s Ã  une itÃ©ration donnÃ©e. La colonne (ou la ligne) sÃ©lectionnÃ©e
est celle qui permet la plus grande augmentation de la mesure dâ€™efficacitÃ©. Dans cette mÃªme
optique, Kirkpatrick et al. (1983) ont introduit la notion, anglaise, du simulated annealing
paradigm dans la sÃ©riation qui permet Ã  lâ€™algorithme dâ€™accepter, avec une certaine probabilitÃ©,
un candidat pouvant Ãªtre pire que la solution courante. Ainsi, au dÃ©but de lâ€™algorithme, la
probabilitÃ© dâ€™acceptation dâ€™un candidat est Ã©levÃ©, mais dÃ©croit graduellement Ã  mesure du
dÃ©roulement de lâ€™algorithme. Enfin, comme la sÃ©riation est perÃ§ue comme un TSP, bon nombre
de chercheurs ont travaillÃ© sur des approches heuristiques qui sont rÃ©sumÃ©es dans Gutin et
Punnen (2002). La plupart de ces mÃ©thodes sont exposÃ©es dans Applegate et al. (2007) ou
dans Hahsler et al. (2009).
3 SÃ©riation par famille
Dans cette Section, nous proposons une nouvelle mÃ©thode de sÃ©riation pour faire face aux
problÃ¨mes relatifs aux donnÃ©es bruitÃ©es, au recouvrement de clusters ou Ã  la prÃ©sence de pe-
tits clusters. Celle-ci se base sur la recherche de sous-ensembles dâ€™individus caractÃ©risant la
structure sous-jacente de lâ€™ensemble des donnÃ©es. Pour cela, nous introduisons de la parcimo-
nie dans les donnÃ©es par lâ€™intermÃ©diaire dâ€™une famille de matrices binaires Ã  degrÃ©s de voisi-
nage commun diffÃ©rents, lesquelles sont ordonnÃ©es selon un algorithme nommÃ© Parsimonious
Block-Clustering. Cet algorithme ordonne la famille de matrices parcimonieuses et sÃ©lectionne
le niveau de parcimonie permettant de faire Ã©merger de la matrice une structure compacte par
blocs.
3.1 Une famille de matrices binaires imbriquÃ©es
Dans notre approche, le degrÃ© de voisinage est dÃ©fini comme une â€œvaleur-seuilâ€œ du nombre
de voisins communs entre paires dâ€™observations en deÃ§Ã  de laquelle les paires dâ€™observations
sont Ã©liminÃ©es. Plus le nombre de voisins communs imposÃ© est important et plus la matrice
sera parcimonieuse (remplie de zÃ©ros). Par consÃ©quent, nous associerons au degrÃ© de voisinage
C. Brunet et al.
commun, le degrÃ© de parcimonie. ConsidÃ©rons une matrice de donnÃ©es Y de dimension nÃ— p.
Soit Y d = (ydij)i,jâˆˆ{1,...,n} la matrice de distances associÃ©e Ã  Y , la nature de la fonction
de distance dÃ©pendant du type de donnÃ©es : elle peut Ãªtre une distance euclidienne entre les
individus i et j (et plus gÃ©nÃ©ralement des normes p), une corrÃ©lation, ou toute autre fonction
caractÃ©risant la notion de proximitÃ© entre des paires dâ€™observations. Soit A = (aij)i,jâˆˆ{1,...,n}
la matrice binaire de similaritÃ© contruite Ã  partir de Y d qui vÃ©rifie :
aij =
{
1 si ydij â‰¤ 
0 si ydij > .
(2)
oÃ¹  est un seuil de distance caractÃ©risant la proximitÃ© des paires dâ€™observations. Sa valeur peut
Ãªtre dÃ©terminÃ©e arbitrairement ; nous proposons de la fixer au premier quartile de la distribution
des distances entre paires dâ€™observations. Par ailleurs, la matrice de similaritÃ© est symÃ©trique
dâ€™oÃ¹ aij = aji. Soit la matrice de Gram B = ATA oÃ¹ chaque Ã©lÃ©ment bij est le nombre de
voisins aux deux points i et j. Cette matrice correspond Ã  une matrice de voisinage commun.
DÃ©finition 1
Une matrice binaire BÎ»m = (b
Î»m
ij )i,jâˆˆ{1,...,n} parcimonieuse de degrÃ© Î»m (avec m âˆˆ {1 â‰¤
m â‰¤M}) se caractÃ©rise par :
bÎ»mij =
{
1 si bij â‰¥ Î»m
0 si bij < Î»m.
. (3)
oÃ¹ bij reprÃ©sente les Ã©lÃ©ments de la matrice de Gram, B, dÃ©finie prÃ©cÃ©demment.
(BÎ»1 , . . . , BÎ»M ) forme une famille de matrices binaires parcimonieuses dont le degrÃ© de par-
cimonie est liÃ© au nombre de voisins communs.
Au regard de cette dÃ©finition, plus le seuil de voisinage imposÃ© Î»m est Ã©levÃ©, moins il y a
de paires dâ€™observations qui satisfont cette condition. La matrice associÃ©e contiendra un plus
grand nombre de zÃ©ros et sera donc plus parcimonieuse. La suite des degrÃ©s de parcimonie
(Î»m)mâˆˆ{1,...,M} qui vÃ©rifie Î»1 < Â· Â· Â· < Î»M permet dâ€™Ã©tablir une relation dâ€™ordre âŠ‚ entre les
M Ã©lÃ©ments de la famille (BÎ»m)mâˆˆ{1,...,M} :
BÎ»M âŠ‚ BÎ»Mâˆ’1 âŠ‚ . . . âŠ‚ BÎ»1 , (4)
dans laquelle la matrice la plus parcimonieuse est contenue dans toutes les autres matrices de
sa famille. Lâ€™un des avantages dâ€™une telle matrice est la disparition des valeurs extrÃªmes et du
bruit lorsque le seuil de parcimonie augmente ce qui facilite le rangement de la matrice ainsi
que lâ€™apparition dâ€™une structure diagonale par blocs. Cependant, au regard de cette famille de
matrices, se pose la question de la sÃ©lection du "meilleur" niveau de parcimonie câ€™est-Ã -dire
celui qui permettra dâ€™obtenir une visualisation claire de la structure des donnÃ©es. Nous intro-
duisons pour ce faire un critÃ¨re de sÃ©lection CÎ»m de la matrice centrale Bâˆ—Î», tel que :
DÃ©finition 2
La matrice centrale ordonnÃ©e (Bâˆ—Î»)ord contenue dans une famille de matrices ordonnÃ©es
(BÎ»m)ord = ((b
Î»m
i,j )ord)i,jâˆˆ{1,...,n},m âˆˆ I et I âˆˆ {1, . . . ,M}, vÃ©rifie :
SparsitÃ© et sÃ©riation
(Bâˆ—Î»)ord = argmin
mâˆˆI
CÎ»m = argmin
mâˆˆI
nâˆ‘
i=1
nâˆ’1âˆ‘
j=1
|(bÎ»mi,j )ord âˆ’ (bÎ»mi,j+1)ord|
|bÎ»mi,j âˆ’ bÎ»mi,j+1|
,
oÃ¹ BÎ»m = (b
Î»m
i,j )i,jâˆˆ{1,...,n} est la matrice binaire non ordonnÃ©e de degrÃ© Î»m.
Matrice
de voisins
communs
B =
2 0 0 3 0 0
0 1 0 0 0 0
0 0 3 0 3 3
2 0 0 3 0 0
0 0 3 0 3 3
0 0 3 0 3 3
DegrÃ© de
parcimonie
Î» â‰¥ 1 Î» â‰¥ 2 Î» â‰¥ 3
Matrices binaires
de voisins
communs
B1 =
1 0 0 1 0 0
0 1 0 0 0 0
0 0 1 0 1 1
1 0 0 1 0 0
0 0 1 0 1 1
0 0 1 0 1 1
B2 =
1 0 0 1 0 0
0 0 0 0 0 0
0 0 1 0 1 1
1 0 0 1 0 0
0 0 1 0 1 1
0 0 1 0 1 1
B3 =
0 0 0 0 0 0
0 0 0 0 0 0
0 0 1 0 1 1
0 0 0 0 0 0
0 0 1 0 1 1
0 0 1 0 1 1
6X
i=1
5X
j=1
Ë›Ë›Ë›
bÎ»mi,j âˆ’ bÎ»mi,j+1
Ë›Ë›Ë›
(calcul par ligne)
17
(3+2+3+3+3+3)
15
(3+0+3+3+3+3)
9
(0+0+3+0+3+3)
Matrices binaires
ordonnÃ©es
de voisins
communs
Bord1 =
1 1 0 0 0 0
1 1 0 0 0 0
0 0 1 1 1 0
0 0 1 1 1 0
0 0 1 1 1 0
0 0 0 0 0 1
Bord2 =
1 1 0 0 0 0
1 1 0 0 0 0
0 0 1 1 1 0
0 0 1 1 1 0
0 0 1 1 1 0
0 0 0 0 0 0
Bord3 =
1 1 1 0 0 0
1 1 1 0 0 0
1 1 1 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
6X
i=1
5X
j=1
Ë›Ë›Ë›â€œ
bÎ»mi,j
â€
ord
âˆ’
â€œ
bÎ»mi,j+1
â€
ord
Ë›Ë›Ë›
(calcul par ligne)
9
(1+1+2+2+2+1)
8
(1+1+2+2+2+0)
3
(1+1+1+0+0+0)
critÃ¨re
CÎ»m
CÎ»â‰¥1 = 917 = 1.89 CÎ»â‰¥2 =
8
15
= 1.88 CÎ»â‰¥3 = 39 = 3.03
1TAB. 2 â€“ Un exemple de calcul du critÃ¨re CÎ»m calculÃ© Ã  partir de la matrice de lâ€™exemple
introductif (cf. Figure 1 de gauche).
Ce critÃ¨re se base sur lâ€™idÃ©e que moins il y a dâ€™alternances entre les 0 et les 1 sur les
lignes de la matrice considÃ©rÃ©e plus celle-ci prÃ©sentera une structure compacte. En effet, dans
la table 2, si lâ€™on considÃ¨re la quantitÃ©
âˆ‘n
i=1
âˆ‘nâˆ’1
j=1 |(bÎ»mi,j )ord âˆ’ (bÎ»mi,j+1)ord| reprÃ©sentant le
nombre de changements entre les 0 et les 1 dâ€™une matrice ordonnÃ©e de degrÃ© Î»m et la quantitÃ©âˆ‘n
i=1
âˆ‘nâˆ’1
j=1 |bÎ»mi,j âˆ’ bÎ»mi,j+1| associÃ©e Ã  la matrice non rangÃ©e de mÃªme degrÃ©, il est notable que
le nombre de changements entre les 0 et les 1 demeure toujours plus petit dans le cas des ma-
trices ordonnÃ©es. Cependant, Ã  mesure que le degrÃ© de parcimonie augmente, le nombre dâ€™al-
ternances entre les 0 et les 1 diminue : dans lâ€™exemple, le numÃ©rateur
âˆ‘n
i=1
âˆ‘nâˆ’1
j=1 |(bÎ»mi,j )ordâˆ’
(bÎ»mi,j+1)ord| est Ã©gal Ã  9 pour un niveau Î» = 1 et Ã  3 lorsque le degrÃ© de parcimonie est de
3. Afin que le critÃ¨re de sÃ©lection ne soit pas biaisÃ© en faveur dâ€™une sparsitÃ© infinie, CÎ»m est
normalisÃ© par le nombre dâ€™alternances entre les 0 et les 1 de la matrice binaire non ordonnÃ©e
C. Brunet et al.
associÃ©e au mÃªme degrÃ© de parcimonie. Ainsi, dâ€™aprÃ¨s lâ€™exemple de la table 2, le niveau de
parcimonie retenu est Î» â‰¥ 2. Notons que ce niveau, une structure Ã  deux groupes est sÃ©lec-
tionnÃ©e et une donnÃ©e isolÃ©e pouvant Ãªtre considÃ©rÃ©e comme une donnÃ©e extrÃªme est exclue.
Ce critÃ¨re dÃ©rive de la notion de run utilisÃ© dans des problÃ¨mes de compression de donnÃ©es
(Johnson et al. (2004); Apaydin et al. (2008)) ; ce dernier caractÃ©risant une sÃ©quence maximale
de 1 consÃ©cutifs sur une ligne dâ€™une matrice boolÃ©enne. Le critÃ¨re que nous prÃ©sentons CÎ»m
est relatif au nombre total de changements prÃ©sents dans la matrice binaire non ordonnÃ©e de
mÃªme degrÃ© de parcimonie afin quâ€™il ne soit pas biaisÃ© en faveur dâ€™une parcimonie infinie ou
inversement, dâ€™une parcimonie trop faible.
3.2 Lâ€™algorithme PB-Clus basÃ© sur un critÃ¨re gÃ©omÃ©trique
Il existe un grand nombre de critÃ¨res pour rÃ©aliser la sÃ©riation dâ€™une matrice (Mechelen
et al. (2004)). Nous proposons dâ€™utiliser le produit scalaire comme critÃ¨re de rangement ; un
critÃ¨re dÃ©jÃ  empruntÃ© par McCormick et al. (1972) et par Arabie et Hubert (1990). Dans notre
approche cependant, lâ€™utilisation du produit scalaire normÃ© est appliquÃ©e uniquement en ligne,
du fait de la symÃ©trie de la matrice. Soit x
T
i xj
||xi||.||xj || le produit scalaire normÃ© entre le i
Ã¨me et le
jÃ¨me vecteur colonne dâ€™une matrice X . Dans le cadre de notre Ã©tude, le produit scalaire normÃ©
est calculÃ© sur les matrices binaires BÎ»m de degrÃ©s de parcimonie Î»m (âˆ€m âˆˆ {1, . . . ,M})
dÃ©finies dans la Section 3.1. La fonction de permutation Î¨ qui cherche Ã  optimiser la somme
des scalaires consÃ©cutifs peut sâ€™Ã©crire telle que :
Î¨âˆ— = argmax
Î¨
nâˆ’1âˆ‘
i=1
(bÎ»mÎ¨(i))
T bÎ»mÎ¨(i+1)
||bÎ»mÎ¨(i)||.||bÎ»mÎ¨(i+1)||
. (5)
Un tel critÃ¨re possÃ¨de des propriÃ©tÃ©s gÃ©omÃ©triques intÃ©ressantes pour dÃ©finir la notion de
groupe (ou de cluster) et pour interprÃ©ter sa formation.
DÃ©finition 3
Un groupe de points G se dÃ©finit comme lâ€™ensemble des observations dont les vecteurs de voi-
sinage commun sont corrÃ©lÃ©s entre eux.
Ainsi, si les groupes de donnÃ©es sont bien sÃ©parÃ©s, les vecteurs de voisinage de chacun de
ces groupes seront orthogonaux deux Ã  deux et les observations dâ€™un mÃªme groupe auront des
vecteurs de voisinage commun corrÃ©lÃ©s. En revanche, si les groupes ne sont pas suffisamment
sÃ©parÃ©s, aucune composante orthogonale ne sera dÃ©tectÃ©e et les groupes ne pourront appa-
raÃ®tre distinctement, dâ€™oÃ¹ lâ€™intÃ©rÃªt dâ€™une approche parcimonieuse des donnÃ©es. Lâ€™algorithme
que nous proposons, Parcimonious Block Clustering (PB-Clus), exploite donc cette dÃ©finition
Ã  travers une approche de type branch and bound oÃ¹ une recherche exhaustive du meilleur can-
didat potentiel dans chaque sous-ensemble de vecteurs sera faite. Initialement, une observation
i est choisie sur lâ€™ensemble des donnÃ©es. A partir du vecteur de voisinage commun associÃ© Ã 
i, lâ€™algorithme recherche dâ€™abord une liste de vecteurs indÃ©pendants entre eux, puis leurs com-
posantes connexes. Chaque sous-ensemble de la liste est rÃ©ordonnÃ© de sorte que la somme des
produits scalaires adjacents soit maximale par bloc. La forme gÃ©nÃ©rale de lâ€™algorithme PB-Clus
est prÃ©sentÃ©e ci-dessous et une implÃ©mentation plus dÃ©taillÃ©e est proposÃ©e dans la table 3 :
SparsitÃ© et sÃ©riation
1. Une matrice de produits scalaires est calculÃ©e Ã  partir de BÎ»m .
2. Une observation est sÃ©lectionnÃ©e et la composante connexe est recherchÃ©e i.e. lâ€™ensemble
des vecteurs corrÃ©lÃ©s. Puis un vecteur orthogonal Ã  ces premiers Ã©lÃ©ments est sÃ©lectionnÃ©
et ses vecteurs corrÃ©lÃ©s sont rÃ©cupÃ©rÃ©s Ã  leur tour. Cette Ã©tape est rÃ©itÃ©rÃ©e jusquâ€™Ã  ce quâ€™il
nâ€™y ait plus de vecteurs Ã  classer ou que les vecteurs restants soient nuls.
3. Dans chaque sous-groupe, les vecteurs sont rangÃ©s selon leur corrÃ©lation par rapport au
dernier Ã©lÃ©ment rangÃ©.
4. Lâ€™ordre obtenu dans chaque sous-matrice est appliquÃ© Ã  la matrice initiale BÎ».
PB-Clus est appliquÃ© pour diffÃ©rentes valeurs de Î» et la "matrice centrale" est sÃ©lectionnÃ©e
parmi la famille de matrices parcimonieuses ordonnÃ©es selon par le critÃ¨re CÎ»m dÃ©fini dans la
section 3.1. Cependant, une trop grande sparsitÃ© peut impliquer lâ€™Ã©vincement de petits clus-
ters initialement prÃ©sents dans la matrice des donnÃ©es. Comme nous disposons de plusieurs
matrices ordonnÃ©es avec diffÃ©rents seuils de parcimonie, une combinaison de ces derniers per-
mettrait de visualiser sur une mÃªme matrice la structure globale des donnÃ©es. Par ailleurs,
lâ€™algorithme de sÃ©riation PB-Clus traite les donnÃ©es, dans sa globalitÃ©, Ã  travers une approche
branch and bound. Cependant, localement, le rangement des vecteurs dans chacun des sous-
ensembles suit une procÃ©dure de type forward, ce qui implique que la place des vecteurs dÃ©jÃ 
ordonnÃ©s reste inchangÃ©e pour une permutation courante. Aussi, lâ€™algorithme propose une so-
lution approchÃ©e pour le rÃ©agencement dâ€™une matrice et ne prÃ©tend pas Ãªtre optimal compara-
tivement aux autres approches suggÃ©rÃ©es dans la littÃ©rature. Enfin, lâ€™algorithme PB-Clus a un
coÃ»t de calcul plus important que les autres mÃ©thodes de sÃ©riation dÃ¨s lors que le rangement est
effectuÃ© non pas sur une seule matrice mais surM matrices relatives Ã  des degrÃ©s de parcimo-
nie diffÃ©rents. Dans le cas dâ€™une matrice de taille nÃ— n ayantK groupes de mÃªme taille nK , il
y a au plus,K( nK !) calculs. Cependant Ã  mesure que le degrÃ© de parcimonie augmente, la ma-
trice se remplit de colonnes (lignes) de 0 ce qui diminue le nombre dâ€™Ã©lÃ©ments Ã  ranger, et par
consÃ©quent le temps de calcul. Aussi, le coÃ»t calculatoire resterait bien infÃ©rieur Ã M.K( nK !).
4 ExpÃ©riences sur donnÃ©es simulÃ©es
Nous proposons dans cette section dâ€™Ã©valuer notre mÃ©thode sur des simulations afin de
souligner ses principales caractÃ©ristiques face Ã  des donnÃ©es bruitÃ©es ou Ã  des groupes super-
posÃ©s. Pour ce faire, nous allons considÃ©rer dans une premiÃ¨re expÃ©rience le cas de groupes
de donnÃ©es superposÃ©s et dÃ©sÃ©quilibrÃ©s afin de caractÃ©riser le comportement de la famille
de matrices parcimonieuses selon le degrÃ© de superposition des clusters ; dans une seconde
expÃ©rience, nous analyserons lâ€™influence des donnÃ©es bruitÃ©es sur la visualisation des sous-
ensembles structurels dâ€™un jeu de donnÃ©es. Lâ€™ensemble des simulations Ã©valuant le comporte-
ment de PB-Clus en face de donnÃ©es fortement bruitÃ©es et alÃ©atoirement chevauchÃ©es se sont
inspirÃ©es des travaux de Prelic et al. (2006). Dans chacune des expÃ©riences proposÃ©es, la fa-
mille de matrices binaires de voisinage commun est construite par seuillage de la matrice des
distances calculÃ©e pour toutes les paires dâ€™observations, le seuil Ã©tant fixÃ© au premier quartile
de la distribution de ces distances.
C. Brunet et al.
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-Algorithme PB-Clus
â†’Initialisation de lâ€™algorithme PB-Clus:
â€”-seq = liste des niveaux de voisins communs dÃ©sirÃ©e
â€”-listeC = NA : intialisation de la liste contenant les critÃ¨res des matrices binaires ordonnÃ©es.
â†’Boucle principale de PB-Clus:
Pour i=1 Ã  taille (seq){
â€”-calculer BÎ»la matrice binaire de voisins communs de niveau Î» = seq (i)
â€”-calculer SÎ»la matrice de produits scalaires de BÎ»
â€”-OÎ»= ordre (SÎ») , calcule le rangement des individus
â€”-CÎ»= crit (BÎ», OÎ») , calcule le critÃ¨re de la dÃ©finition 2 de la section 3.1
â€”-listeC = merge (listeC, CÎ»)}
â†’RÃ©sultat de PB-Clus:
Bâˆ—Î» = min (listeC) , retourne le critÃ¨re minimale et le nombre de voisins communs associÃ©s.
OÎ»= ordre (Sâˆ—Î») , Ã©tablit le rangement des individus pour le niveau optimal de voisins communs.
Fonction ordre (SÎ») :
initialisation:
â€”-V = colinear (i.obs, SÎ»)
â€”-liste = liste des individus oÃ¹ le nombre de voisins communs est non nul (vc 6= 0).
â€”-i.max = lâ€™observation ayant vc maximum.
â€”-i.perm= 0 (liste qui donnera lâ€™ordre des individus selon leur scalaire ordonnÃ©.)
boucle principale:
â€”-tant que (taille (liste) > 1) {
â€”-si (taille (V.cor) â‰¥ 1) { i.perm =merge (i.perm,liste[V.col])
Sj = Sj [c(V.cor,V.ind),c(V.cor,V.ind)]
liste = liste[c(V.cor,V.ind)]
si (taille (V.cor) > 1) { V =colinear (1, BÎ»)} }
else { i.perm =merge (i.perm,liste[V.col])
Sj = Sj [V.ind,V.ind]
liste = liste[V.ind]
si (taille (V.cor) > 1) {
â€”â€”â€“i.max =observation ayant vc maximum dans Sj
â€”â€”â€“BÎ» = BÎ» [V.ind,V.ind]
â€”â€”â€“V =colinear (i.max, BÎ»)}}
Fonction colinear (i, S) retourne 3 listes diffÃ©rentes:
Soit si le vecteur de la ligne i de S, alors:
- V.col = liste des individus colinÃ©aires Ã  si.
- V.cor = liste des individus corrÃ©lÃ©s mais non colinÃ©aires Ã  si.
- V.ind = liste des individus indÃ©pendants Ã  si.
1TAB. 3 â€“ ImplÃ©mentation de lâ€™algorithme PB-Clus.
SparsitÃ© et sÃ©riation
4.1 SÃ©riation dans le cas de groupes superposÃ©s
SÃ©riation de groupes superposÃ©s et dÃ©sÃ©quilibrÃ©s : Dans cette expÃ©rience, les donnÃ©es sont
simulÃ©es Ã  partir dâ€™un mÃ©lange de trois lois gaussiennes de dimensions 2 dans lequel deux
groupes se superposent. Ces derniers sont de proportions diffÃ©rentes : le premier cluster est
formÃ© de 5% des donnÃ©es, soit 15 observations tandis que les deux autres reprÃ©sentent 32%
et 63% des donnÃ©es, soit respectivement 100 et 200 observations. Cette situation est illustrÃ©e
par la figure 2(a) dans laquelle les donnÃ©es sont reprÃ©sentÃ©es dans lâ€™espace des variables. La
partition centrale obtenue par notre mÃ©thode de sÃ©riation est illustrÃ©e par la Figure 2(b) et ce,
avec un seuil de sparsitÃ© de 16 voisins communs. Pour ce niveau de parcimonie, plus de 6% des
donnÃ©es ont Ã©tÃ© Ã©vincÃ©es ce qui a pour consÃ©quence lâ€™absence du troisiÃ¨me cluster, le plus petit,
qui de part sa taille (15 observations) au regard du seuil de parcimonie (16 voisins communs)
a Ã©tÃ© automatiquement Ã©vincÃ© des sous-ensembles de donnÃ©es qui forment la matrice centrale.
Afin dâ€™intÃ©grer la visualisation de ce cluster dans la matrice centrale, un seuil plus faible de
sparsitÃ© (8 voisins communs) a Ã©tÃ© appliquÃ© sur le sous-ensemble des donnÃ©es Ã©vincÃ©es dans le
but dâ€™en faire Ã©merger une sous-structure. La combinaison de la visualisation centrale obtenue
pour un niveau de parcimonie de 16 voisins communs avec celle du sous-ensemble de donnÃ©es
Ã©valuÃ© pour un seuil de 8 voisins communs, nous permet dâ€™afficher sur une mÃªme matrice les 3
groupes de donnÃ©es. La figure 2(c) rend compte de cette approche multi-Ã©chelle de lâ€™algorithme
PB-Clus. Enfin, parmi les donnÃ©es sÃ©riÃ©es, i.e. celles non Ã©vincÃ©es, 98% dâ€™entre elles ont Ã©tÃ©
bien classÃ©es.
a)
âˆ’10 âˆ’5 0 5 10 15
âˆ’
10
âˆ’
5
0
5
10
15
x
y
b) c)
1
FIG. 2 â€“ Projection des donnÃ©es dans leur espace (Ã  gauche), visualisations de la matrice
centrale ordonnÃ©e par PB-Clus avec un seuil de sparsitÃ© de 16 voisins communs (milieu) et de
la matrice ordonnÃ©e multi-Ã©chelle (Ã  droite).
Influence du niveau de superposition de clusters : Dans cette deuxiÃ¨me expÃ©rience, nous
cherchons Ã  Ã©valuer lâ€™influence du niveau de recouvrement de clusters dans la recherche dâ€™une
structuration des donnÃ©es. Pour ce faire, nous avons simulÃ© 3 gaussiennes dans un espace de di-
mension 2 telles que leurs moyennes respectives vÃ©rifient :m1 = (x, y),m2 = (x,âˆ’y),m3 =
(0,âˆ’y) avec x âˆˆ [0, 0.3] et y âˆˆ [0, 0.225]. Par consÃ©quent, la position relative des moyennes
varie et cette variation dÃ©termine le niveau de superposition des groupes. Ainsi, lorsque x = 0
et y = 0, les 3 groupes sont confondus et cela correspond Ã  un taux de recouvrement de 100%.
C. Brunet et al.
Dans le cas opposÃ© des groupes bien sÃ©parÃ©s oÃ¹ le taux de recouvrement est Ã©gale Ã  0, les
moyennes des clusters vÃ©rifient :m1 = (0.3, 0.225),m2 = (0.3,âˆ’0.225),m3 = (0,âˆ’0.225).
La table 4 prÃ©sente lâ€™Ã©volution du niveau des voisins communs et son taux dâ€™Ã©viction associÃ©,
en fonction du recouvrement des groupes. Tout dâ€™abord, on remarque que plus les clusters se
superposent, plus le critÃ¨re CÎ» sÃ©lectionne une reprÃ©sentation parcimonieuse des donnÃ©es. En
effet, lorsque la structure visible des donnÃ©es devient moins prononcÃ©e, cet effet est contre-
balancÃ© par une plus importante sparsitÃ© dans les donnÃ©es avec lâ€™imposition dâ€™un voisinage
commun plus Ã©levÃ©. De mÃªme, Ã  mesure que la structure des donnÃ©es devient de plus en plus
complexe, le taux de classification relatif aux sous-ensembles de donnÃ©es sÃ©riÃ©es diminue ainsi
que la qualitÃ© de la visualisation. Dans notre exemple, au-delÃ  dâ€™un taux de recouvrement des
donnÃ©es de 40%, le taux de classification devient faible (< 0.6) puisque lâ€™algorithme PB-Clus
ne dÃ©tecte plus de structure dans les donnÃ©es et ceci, quel que soit le niveau de parcimonie
imposÃ©.
taux (en %) de
recouvrement 0 6.7 13.3 20.0 26.6 33.3 40.0 46.6 53.30 73.3 100
x 0.30 0.28 0.26 0.24 0.22 0.20 0.18 0.16 0.14 0.08 0
y 0.225 0.21 0.195 0.18 0.165 0.15 0.135 0.120 0.09 0.06 0
degrÃ© de
parcimonie 5 6 9 35 33 34 35 35 35 35 34
pourcentage
de donnÃ©es
Ã©vincÃ©es
0.00 0.00 0.00 0.26 0.23 0.34 0.37 0.35 0.39 0.35 0.43
valeur de CÎ» 1.95 2.01 2.42 2.64 2.90 2.82 3.34 3.29 3.32 3.54 3.65
taux de
classification 0.99 0.99 0.99 0.99 0.95 0.90 0.86 0.60 0.49 0.44 0.39
1
TAB. 4 â€“ Influence du degrÃ© de recouvrement des clusters sur la dÃ©tection de structure.
4.2 DonnÃ©es bruitÃ©es
Comparaison entre des mÃ©thodes de sÃ©riation : Dans cette expÃ©rience, 30% des donnÃ©es
ont Ã©tÃ© remplacÃ©es par du bruit uniforme sur un hypercube [âˆ’1, 1] de dimension 4 et lâ€™autre
moitiÃ© a Ã©tÃ© simulÃ©e Ã  partir dâ€™un mÃ©lange de trois gaussiennes de dimension 4 ; elles sont
reprÃ©sentÃ©es sur leurs deux premiÃ¨res dimensions par la figure 3(a). La figure 3(b) prÃ©sente
lâ€™Ã©volution du critÃ¨re de compacitÃ© SÎ» en fonction des diffÃ©rents degrÃ©s de parcimonie, Ã  sa-
voir le nombre de voisins communs. La partition centrale (fig. 3(c)) sÃ©lectionnÃ©e est celle qui
a le critÃ¨re CÎ» minimal, ce qui correspond Ã  un voisinage commun de 59. Cette sparsitÃ© a
pour consÃ©quence une Ã©viction de 16% des donnÃ©es et ce sont 84% des donnÃ©es initiales qui
permettent dâ€™obtenir une reprÃ©sentation diagonale par blocs ; les sous-ensembles de donnÃ©es
exclues sont entiÃ¨rement formÃ©s de donnÃ©es bruitÃ©es. De plus, le taux de classification cor-
recte parmi les donnÃ©es sÃ©riÃ©es sâ€™Ã©lÃ¨ve Ã  99%, ce qui implique que ces sous-ensembles de
donnÃ©es sÃ©riÃ©es sont structurels des 3 clusters. Afin dâ€™Ã©valuer la performance de notre ap-
proche, trois mÃ©thodes de sÃ©riation basÃ©es sur des matrices de distances ont Ã©tÃ© appliquÃ©es : le
clustering hiÃ©rarchique pour la sÃ©riation (fig. 4(a)), lâ€™approche de Chen basÃ©e sur une structure
anti-Robinson (Chen (2002), fig. 4(b)) ou encore une autre mÃ©thode de sÃ©riation anti-Robinson
SparsitÃ© et sÃ©riation
(a) (b) (c)
FIG. 3 â€“ Visualisations du mÃ©lange de gaussiennes bruitÃ©es dans lâ€™espace de donnÃ©es (Ã 
gauche), de lâ€™Ã©volution du critÃ¨re CÎ» selon le degrÃ© de voisins communs (milieu) et de la
matrice centrale ordonnÃ©e par PB-Clus avec un seuil de sparsitÃ© de 67 voisins communs.
(a) (b) (c)
FIG. 4 â€“ Visualisations de la matrice des distances sÃ©riÃ©e par une mÃ©thode de clustering hiÃ©-
rarchique (gauche), une mÃ©thode de Chen (milieu) et une mÃ©thode basÃ©e sur une reprÃ©sentation
anti-Robinson (droite).
avec une approche par simulated annealing (Brusco et al. (2008)) illustrÃ©e par la figure 4(c).
Ces mÃ©thodes sont dÃ©taillÃ©es dans Hahsler et al. (2009) et programmÃ©es dans le package R
seriation. Parmi les mÃ©thodes de sÃ©riation utilisÃ©es, nous remarquons que seule la partition
centrale obtenue par lâ€™algorithme PB-Clus apporte une visualisation claire des trois clusters.
La reprÃ©sentation de cette structure en trois groupes distincts est possible grÃ¢ce Ã  la famille de
matrices binaires parcimonieuses. En effet, plus les matrices ont un degrÃ© de parcimonie Ã©levÃ©,
plus la quantitÃ© de donnÃ©es bruitÃ©es pris en compte diminue.
Influence du niveau de bruit : Cette deuxiÃ¨me expÃ©rience tend Ã  montrer le comportement
de lâ€™algorithme PB-Clus en face de donnÃ©es fortement bruitÃ©es. A cet effet, nous avons si-
mulÃ© trois gaussiennes de dimension 2 de 50 observations chacune et ayant pour moyenne
C. Brunet et al.
niveau de bruit 0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
degrÃ© de
parcimonie
4 12 6 8 12 18 24 21 35 31 35
pourcentage
de donnÃ©es
Ã©vincÃ©es
0.06 0.16 0.12 0.23 0.11 0.04 0.12 0.19 0.16 0.11 0.08
Taux de
classification
1 1 1 1 0.99 0.99 0.99 0.99 0.99 0.98 0.98
TAB. 5 â€“ Comportement de PB-Clus selon diffÃ©rents niveaux de bruit ajoutÃ©s aux donnÃ©es
initiales.
FIG. 5 â€“ Evolution de la visualisation de la structure des donnÃ©es en fonction de diffÃ©rents
niveaux de bruit ajoutÃ©s aux donnÃ©es initiales.
m1 = (âˆ’0.4,âˆ’0.3), m2 = (0.4,âˆ’0.3) et m3 = (0, 0.3) respectivement et pour matrice de
variances-covariances S = diag(0.1, 0.1). Ces groupes sont volontairement bien sÃ©parÃ©s afin
de pouvoir Ã©valuer la sensibilitÃ© de lâ€™algorithme au bruit. Les donnÃ©es bruitÃ©es ont Ã©tÃ© gÃ©nÃ©rÃ©es
selon une loi uniforme sur un cube [âˆ’1, 1] de dimension 2. Pour Ã©valuer lâ€™impact du bruit sur
la visualisation, nous avons fait varier le nombre de donnÃ©es bruitÃ©es ajoutÃ©es de 10% Ã  200%
du nombre des donnÃ©es de lâ€™Ã©chantillon initial. La figure 5 prÃ©sente lâ€™Ã©volution de la visua-
lisation des donnÃ©es Ã  mesure de lâ€™ajout de bruit tandis que la table 5 reprÃ©sente les niveaux
de parcimonie et le taux de donnÃ©es evincÃ©es associÃ©s au critÃ¨re CÎ» pour chaque niveau de
bruit. On remarque tout dâ€™abord que la visualisation de groupes se dÃ©grade peu avec lâ€™ajout
de bruit. En effet, sur la figure 5, la structure ne se dÃ©grade quâ€™Ã  partir du moment oÃ¹ les
donnÃ©es bruitÃ©es reprÃ©sentent plus de la moitiÃ© de lâ€™ensemble des donnÃ©es. La qualitÃ© de ces
reprÃ©sentations sâ€™explique par le seuil de parcimonie qui, augmentant avec lâ€™ajout de bruit (cf.
table 5), exclut un plus grand nombre de donnÃ©es. Lâ€™objectif de PB-Clus, basÃ© sur la sÃ©lection
de sous-ensembles de donnÃ©es structurelles est bien atteint dans cette expÃ©rience puisque la
visualisation met en exergue 3 groupes et ce, quel que soit le niveau de bruit.
5 Comparaison avec des mÃ©thodes classiques
Cette derniÃ¨re section est destinÃ©e Ã  Ã©valuer la qualitÃ© de la visualisation dâ€™une structure
dans les donnÃ©es ainsi que celle de la partition obtenue par lâ€™algorithme PB-Clus. Pour cela,
nous avons empruntÃ© deux types de jeux de donnÃ©es appartenant Ã  des communautÃ©s diffÃ©-
rentes : dâ€™une part, 5 bases de donnÃ©es rÃ©elles et de rÃ©fÃ©rences pour la sÃ©riation accessibles
SparsitÃ© et sÃ©riation
par le logiciel R et dâ€™autre part, 7 jeux de donnÃ©es simulÃ©es, les donnÃ©es FCPS, prÃ©sentant
des difficultÃ©s particuliÃ¨res pour la tÃ¢che de clustering. Les familles de matrices binaires de
voisinage commun ont Ã©tÃ© obtenues de la mÃªme maniÃ¨re que dans la Section 4.
5.1 DonnÃ©es de rÃ©fÃ©rence pour la sÃ©riation
Dans cette section, nous comparons les performances de lâ€™algorithme PB-Clus en termes
de visualisation dâ€™une part avec deux autres mÃ©thodes de sÃ©riation, lâ€™une utilisant la classifi-
cation hiÃ©rarchique (HC), lâ€™autre utilisant un critÃ¨re de divergence reposant sur une structure
anti-Robinson (CHEN) (Chen (2002)) dÃ©crites dans Hahsler et al. (2009) et en termes de classi-
fication dâ€™autre part, par lâ€™intermÃ©diaire dâ€™une mÃ©thode de classification non supervisÃ©e basÃ©e
sur les distances, le k-means. Cette comparaison a Ã©tÃ© faite sur 5 bases de donnÃ©es populaires
dans la communautÃ© statistique de clustering et de sÃ©riation qui sont disponibles dans le logi-
cielR. Les 5 jeux de donnÃ©es que nous avons choisis sont dÃ©crits ci-dessous :
â€“ La base de donnÃ©es des iris de Fisher a Ã©tÃ© construite par Anderson (1935) qui a collectÃ©
3 espÃ¨ces dâ€™iris diffÃ©rentes dans la pÃ©ninsule de GaspÃ© : les setosa, les virginica ainsi
que les versicolor. Chaque espÃ¨ce est reprÃ©sentÃ©e par 50 fleurs qui sont dÃ©crites par 4
caractÃ©ristiques morphomÃ©triques basÃ©es sur la largeur et la longueur de leurs sÃ©pales et
de leurs pÃ©tales. Cette base de donnÃ©es est extrÃªmement populaire dans la communautÃ©
statistique du fait de la difficultÃ© Ã  distinguer les virginica et les versicolor.
â€“ Les donnÃ©es ruspini proviennent des travaux de Ruspini (1970) sur le clustering : elles
sont formÃ©es de 75 points dÃ©crits sur 2 dimensions et rÃ©partis en 4 classes homogÃ¨nes et
Ã©quilibrÃ©es.
â€“ Les donnÃ©es townships sont des donnÃ©es binaires relatant la prÃ©sence ou lâ€™absence de 9
caractÃ©ristiques descriptives de 16 villes, telles la prÃ©sence ou lâ€™absence dâ€™universitÃ©s, de
coopÃ©ratives agricoles, de chemins de fer etc. Il nâ€™y a aucune information sur le nombre
de groupes structurant les donnÃ©es.
â€“ Les donnÃ©es faithful sont une base de donnÃ©es Ã©valuant le temps entre deux Ã©ruptions
de geysers du parc national de Yellowstone de Wyoming (USA) et leur durÃ©e. Elles sont
caractÃ©risÃ©es par 272 observations (Haerdle (1991)).
â€“ Les donnÃ©es geysers reprÃ©sentent une version complÃ¨te des donnÃ©es prÃ©cÃ©dentes et a Ã©tÃ©
collectÃ©e par Azzalini et Bowman (1990). Ce sont quelques 299 Ã©ruptions qui ont Ã©tÃ©
Ã©tudiÃ©es (mÃªmes types de mesures que prÃ©cÃ©demment) entre le 1er et le 15 aoÃ»t 1985.
Pour Ã©valuer la qualitÃ© de la visualisation de notre approche, nous avons utilisÃ© deux critÃ¨res de
Niermann (Neumann et Moore) prÃ©sentÃ©s dans la Section 2 ; la partition obtenue sera quand Ã 
elle Ã©valuÃ©e par validation croisÃ© avec le vrai label lorsque les donnÃ©es en sont dotÃ©es ou dans
le cas Ã©chÃ©ant, avec les labels estimÃ©s par les k-means. Comme ce dernier suppose une connais-
sance a priori du nombre de groupes du mÃ©lange, nous avons choisi dâ€™utiliser le nombre de
clusters dÃ©tectÃ©s par PB-Clus afin dâ€™obtenir des partitions comparables. La figure 6 de droite
C. Brunet et al.
reprÃ©sente les produits scalaires consÃ©cutifs des Ã©lÃ©ments i et i + 1 ordonnÃ©s sur les 5 bases
de donnÃ©es exposÃ©es prÃ©cÃ©demment. Ces courbes de produits scalaires consÃ©cutifs rendent
compte de la proximitÃ© entre deux observations adjacentes et de points de rupture pour le pas-
sage dâ€™un cluster Ã  un autre, ce qui permet de sÃ©lectionner le nombre de clusters du mÃ©lange et
dâ€™obtenir une partition des donnÃ©es. La colonne de gauche de la figure 6 reprÃ©sente la visualisa-
tion centrale de la matrice parcimonieuse ordonnÃ©e avec lâ€™algorithme PB-Clus. Dans le cas des
iris de Fisher, lâ€™observation de sa matrice centrale de degrÃ© de voisinage commun 8, prÃ©sente
une structure globale Ã  deux clusters dans laquelle il prÃ©figure localement, au niveau du premier
bloc, une sÃ©paration en deux parties diffÃ©rentes. On retrouve ici la structure particuliÃ¨re des iris
dans laquelle les versicolor et les virginica sont des espÃ¨ces peu distinctes. Par ailleurs, cette
partition en 3 groupes se voit Ãªtre confirmÃ©e par les 2 points de rupture prÃ©sents sur la courbes
de ses produits scalaires consÃ©cutifs. Lâ€™analyse simple de ces deux graphiques rend compte de
la performance de notre approche parcimonieuse pour la visualisation des donnÃ©es, dâ€™autant
plus que rares sont les mÃ©thodes de clustering qui sÃ©lectionnent un modÃ¨le optimal Ã  3 classes
dâ€™iris (cf. les modÃ¨les de mÃ©langes Raftery et Dean (2006)). Dans le cas des donnÃ©es Ruspini
et Faithful, les ruptures sur la courbe des produits scalaires consÃ©cutifs sont nettes et amples ce
qui rend compte de la dÃ©connexion totale des clusters entre eux ; cette mÃªme conclusion est vi-
sible sur leur matrice centrale ordonnÃ©e de degrÃ© 5 pour les donnÃ©es Ruspini et de 2 pour celles
de Faithful. A lâ€™inverse, les donnÃ©es Geysers et Townships prÃ©sentent des ruptures de faibles
ampleurs. On peut en effet observer sur les donnÃ©esGeysers deux petites ruptures sur la courbe
de ses produits scalaires consÃ©cutifs, lesquelles sâ€™expliquent par la proximitÃ© des clusters les
uns par rapport aux autres. Ensuite, dans le cas des donnÃ©es Townships, la courbe des scalaires
consÃ©cutifs nous montre que la premiÃ¨re ville est, certes, reliÃ©e aux 7 villes suivantes mais
moins fortement que ces 7 villes entre elles ce qui suppose un certain Ã©loignement de cette
donnÃ©e au reste du groupe. La visualisation centrale de la matrice parcimonieuse ordonnÃ©e de
degrÃ© 2 avec lâ€™algorithme PB-Clus apporte une meilleure comprÃ©hension des relations entre
les villes. En effet, on remarque que la premiÃ¨re donnÃ©e est fortement corrÃ©lÃ©e Ã  deux blocs
de villes distincts. Cette interprÃ©tation se voit Ãªtre confirmÃ©e par les travaux de Hahsler et al.
(2009) qui, par une approche de sÃ©riation sur le tableau initial rectangulaire des donnÃ©es, ont
montrÃ© lâ€™existence dâ€™une structure Ã  3 groupes : les villes urbaines, les villes de campagnes et
les villes intermÃ©diaires avec une ville en transition. Cette premiÃ¨re Ã©valuation basÃ©e sur notre
perception visuelle est complÃ©tÃ©e par une mesure de qualitÃ© basÃ©e sur des critÃ¨res de sÃ©riation
Ã©valuant la proximitÃ© du voisinage dans la matrice ordonnÃ©e. La table 6 Ã©value les perfor-
mances de 3 mÃ©thodes de sÃ©riation, dont PB-Clus au regard des deux mesures de Niermann
(2005) (Moore et Neumann), la meilleure mÃ©thode Ã©tant celle dont le critÃ¨re est minimum. On
remarque que les 2 critÃ¨res de Niermann sont minimum pour une approche parcimonieuse des
donnÃ©es et ceci sur lâ€™ensemble des bases de donnÃ©es ce qui rend compte de la performance de
PB-Clus relativement Ã  HC ou Ã  la mÃ©thode de Chen. Enfin, la derniÃ¨re table 7 prÃ©sente les ta-
bleaux de classification croisÃ©e avec le vrai label dans le cas des iris de Fisher et avec les labels
obtenus par k-means dans le cas des donnÃ©es Ruspini, Townships, Geysers et Faithful. Notons
que dans le cas des donnÃ©es iris et Geysers câ€™est-Ã -dire ceux oÃ¹ les ruptures sur la courbe des
scalaires consÃ©cutifs sont de faibles ampleurs, nous avons seuillÃ© les scalaires afin dâ€™obtenir
une labellisation pour chaque donnÃ©e. Sur les iris de Fisher, notre algorithme prÃ©sente un taux
de classification correcte de 89.0% lÃ©gÃ¨rement plus faible que est comparable Ã  celui obtenu
par les k-means (90.6%). Cette diffÃ©rence de taux est liÃ© aux donnÃ©es situÃ©es Ã  lâ€™intersection
SparsitÃ© et sÃ©riation
des virginica et des versicolor et Ã  lâ€™initialisation de notre algorithme. Pour les autres jeux de
donnÃ©es, on observe que les partitions obtenues par PB-Clus et par les k-means concordent
presque parfaitement, les taux de classification avoisinant les 98%.
MÃ©thodes PB-Clus HC Chen
CritÃ¨res Moore Neumann Moore Neumann Moore Neumann
DonnÃ©es
Iris 1 371.2 471.1 31 728.8 10 893.1 19 357.8 7 304.0
Townships 0244.5 091.8 01 109.9 00441.5 00849.0 0342.0
Ruspini 1 290.1 442.2 08 724.9 03 036.4 06 503.7 2 277.1
Faithful 2 634.1 889.4 34 045.5 11 503.5 23 390.0 9 894.2
Geysers 2 514.9 850.4 68 205.3 02 302.1 12 866.8 4 501.4
1TAB. 6 â€“ Comparaison de 3 mÃ©thodes de sÃ©riation, PB-Clus, HC et Chen selon les mesures
de Moore et de Neumann sur les donnÃ©es de rÃ©fÃ©rence.
Iris de Fisher Townships
clusters PB-Clus clusters Kmeans clusters PB-Clus
classes connues 1 2 3 classes connues 1 2 3 classes connues 1 2 3 4
Setosa 50 0 0 Setosa 50 0 0 Urbaines 8 0 0 0
Versicolor 0 50 0 Versicolor 0 49 1 IntermÃ©diaires 0 4 0 0
Virginica 0 17 33 Virginica 0 13 37 Campagnardes 0 0 2 0
Inclassables 0 1 0 1
Taux de classification = 0.88 Taux de classification = 0.90 Taux classification = 0.94
Ruspini Faithful Geysers
clusters PB-Clus clusters PB-Clus clusters PB-Clus
clusters Kmeans 1 2 3 4 clusters Kmeans 1 2 clusters Kmeans 1 2 3
groupe 1 13 0 0 0 groupe 1 168 4 groupe 1 88 2 7
groupe 2 0 35 0 0 groupe 2 0 100 groupe 2 0 105 0
groupe 3 0 0 15 0 groupe 3 0 0 97
groupe 4 0 0 0 20
Taux de classification = 1.00 Taux de classification = 0.98 Taux de classification = 0.97
1
TAB. 7 â€“ Tableaux de validation croisÃ©e des donnÃ©es benchmarks de sÃ©riation.
5.2 DonnÃ©es de rÃ©fÃ©rence pour le clustering
Dans cette derniÃ¨re expÃ©rience, nous considÃ©rons les jeux de donnÃ©es FCPS (Ultsch (2005))
disponibles sur le site http ://www.informatik.uni-marburg.de/databionics et qui sont caractÃ©-
risÃ©s par des donnÃ©es de petite dimension (p â‰¤ 3) divisÃ©es en plusieurs clusters. Chacune de
ces bases prÃ©sente un problÃ¨me particulier pour le clustering : certains groupes dâ€™un mÃªme
mÃ©lange ne suivent pas la mÃªme distribution telles que les donnÃ©es Lsun, Atom, TwoDiamonds
ou encore WingNut ; dâ€™autres sont caractÃ©risÃ©s par des variances intra diffÃ©rentes comme les
C. Brunet et al.
FIG. 6 â€“ Matrices de scalaires ordonnÃ©s (gauche) et reprÃ©sentation de scalaires consÃ©cutifs
ordonnÃ©s sur des donnÃ©es de rÃ©fÃ©rence (droite).
SparsitÃ© et sÃ©riation
donnÃ©es WingNut ou Lsun et dâ€™autres encore ne sont pas linÃ©airement sÃ©parables comme les
donnÃ©es Atom. Toutes ces spÃ©cificitÃ©s sont dÃ©taillÃ©es dans la table 8. Lâ€™objectif de cette ex-
pÃ©rience est dâ€™Ã©valuer la capacitÃ© de notre approche Ã  trouver des sous-ensembles structurels
caractÃ©risant chaque base de donnÃ©es et ce, quelles que soient la forme ou la taille des clusters.
Lâ€™algorithme PB-Clus est appliquÃ© sur lâ€™ensemble de ces donnÃ©es et la premiÃ¨re partie de la
table 9 prÃ©sente les degrÃ©s de parcimonie sÃ©lectionnÃ©s par le critÃ¨re CÎ», le taux dâ€™Ã©viction des
donnÃ©es associÃ©, le nombre de groupes dÃ©tectÃ©s ainsi que la visualisation de la matrice cen-
trale ordonnÃ©e. Notons que, comme prÃ©cÃ©demment, la partition obtenue ainsi que le nombre de
groupes prÃ©sents dans chaque jeu de donnÃ©es ont Ã©tÃ© obtenus par seuillage des scalaires consÃ©-
cutifs. On remarque tout dâ€™abord, que la visualisation de la structure intrinsÃ¨que de lâ€™ensemble
des donnÃ©es est relativement nette. Cette qualitÃ© de la reprÃ©sentation des structures sâ€™explique
principalement par un niveau de parcimonie relativement Ã©levÃ© comme le montre les niveaux
de voisinage commun sÃ©lectionnÃ©s de la table 9 mais aussi, par une analyse multi-Ã©chelle dans
le cas des donnÃ©es Atom et Target marquÃ©es dâ€™un astÃ©risque. Dans le cas des donnÃ©es Atom,
lâ€™algorithme PB-Clus avait sÃ©lectionnÃ© initialement une reprÃ©sentation parcimonieuse excluant
49% des donnÃ©es (soit le cluster de grande variance) avec un voisinage commun de 58. Pour
les donnÃ©es Target, 48% des donnÃ©es avaient Ã©tÃ© Ã©vincÃ©es pour un seuil de voisinage com-
mun sâ€™Ã©levant Ã  34, ce qui avait pour consÃ©quence dâ€™omettre le cluster en anneau ainsi que
les tous petits clusters. Lâ€™approche multi-Ã©chelle a donc permis dâ€™une part, de dÃ©tecter des
sous-groupes dans les donnÃ©es Ã©vincÃ©es (le niveau de parcimonie et le taux dâ€™Ã©viction ont Ã©tÃ©
renseignÃ©s entre des parenthÃ¨ses dans la table 9) et dâ€™autre part, de les visualiser. Dans le cas
des donnÃ©es Atom, on distingue bien deux groupes : lâ€™un compact et carrÃ©, relatif au cluster
dense et central, et lâ€™autre beaucoup plus effilÃ©, caractÃ©risant le second cluster de variance
beaucoup plus grande. De la mÃªme maniÃ¨re, lâ€™analyse multi-Ã©chelle appliquÃ©e sur les donnÃ©es
Target, permettent dâ€™introduire le cluster en forme dâ€™anneau dans la visualisation ainsi que les
4 petits clusters. Quelle que soit la base de donnÃ©es considÃ©rÃ©e, la structure intrinsÃ¨que a bien
Ã©tÃ© dÃ©tectÃ©e par lâ€™algorithme PB-Clus et ceci grÃ¢ce Ã  lâ€™introduction de parcimonie dans les
donnÃ©es. La deuxiÃ¨me partie de la table 9 prÃ©sente les taux de classification correcte sur les
donnÃ©es sÃ©riÃ©es (non Ã©vincÃ©es) issues de notre approche et ceux obtenus par Ultsch (2005) Ã 
partir de 3 mÃ©thodes non supervisÃ©es : les k-means et deux algorithmes de classification hiÃ©-
rarchiques (SingleLinkage et Ward). Certes, les taux de classification sont comparables entre
PB-Clus et les mÃ©thodes proposÃ©es, cependant, ces rÃ©sultats ont Ã©tÃ© obtenus en apportant une
connaissance a priori du nombre de groupes pour chaque jeu de donnÃ©es ce qui facilite le
partitionnement des diffÃ©rents jeux de donnÃ©es pour les algorithmes des k-means ou de classi-
fication hiÃ©rarchiques. De plus, dans notre approche, ce nâ€™est pas tant la comparaison des taux
de bonne classification entre les mÃ©thodes qui est intÃ©ressante mais plutÃ´t le fait que dâ€™une part,
la structure intrinsÃ¨que des donnÃ©es est toujours bien Ã©valuÃ©e quelle que soit leur difficultÃ© et
dâ€™autre part, sur les 7 jeux de donnÃ©es proposÃ©s plus de 93% des donnÃ©es structurelles sont
bien classÃ©es. Lâ€™ensemble de ces remarques rend compte de la performance de PB-Clus quant
Ã  la mise en exergue dâ€™une structuration et de donnÃ©es structurelles et ce, quelle que soit la
distribution des donnÃ©es, la taille des clusters ou leurs variances.
C. Brunet et al.
n K p DifficultÃ©s
Atom 800 2 3
diffÃ©rentes variances
non linÃ©airement sÃ©parables
Hepta 211 7 2 aucune difficultÃ©
Lsun 400 3 2
diffÃ©rentes variances
diffÃ©rentes distances inter
Target 770 6 2 outliers
Tetra 400 4 3 clusters trÃ¨s proches
TwoDiamands 800 2 2 frontiÃ¨re du cluster dÃ©finie par une densitÃ©
WingNut 1016 2 2 densitÃ© versus distances
TAB. 8 â€“ Nombre dâ€™observations (n), de clusters (K) et de dimensions (p) des bases de don-
nÃ©es FCPS.
PB-ClusDonnÃ©es : Atom Hepta Lsun Target Tetra TwoDiam WingNut
PB-Clus
Voisins Communs
Taux dâ€™Ã©viction
Nombre de groupes
34âˆ— (2)
0.49âˆ— (0)
2
10
0
7
68
0.17
3
58âˆ— (2)
0.48âˆ— (0)
6
53
0
4
32
0
2
110
0.087
2
Visualisation
Taux de classification :
PB-Clus
k-means
Ward
Single Linkage
0.99
0.71
1.00
0.66
1
0.97
0.97
0.97
0.93
0.76
0.99
0.72
0.96
0.63
0.99
0.65
0.98
0.99
0.26
0.98
0.99
1.00
0.50
1.00
0.94
0.96
1.00
0.88
1
TAB. 9 â€“ Tableau des rÃ©sultats de la sÃ©riation sur les bases de donnÃ©es FCPS.
SparsitÃ© et sÃ©riation
6 Conclusion
Lâ€™algorithme PB-Clus que nous proposons est une mÃ©thode de sÃ©riation qui permet de vi-
sualiser des sous-ensembles de donnÃ©es et leurs dÃ©pendances dans une base de donnÃ©es. Pour
ce faire, nous avons proposÃ© une famille de matrices parcimonieuses formÃ©es de matrices de
niveaux de parcimonie diffÃ©rents dont le niveau est directement dÃ©terminÃ© par le nombre de
voisins communs entre paires dâ€™observations. Cette mÃ©thode est un outil efficace pour lâ€™ana-
lyse de donnÃ©es qui offre visuellement de meilleurs rÃ©sultats que les mÃ©thodes traditionnelles
de sÃ©riation, en particulier lorsque les donnÃ©es sont bruitÃ©es ou que les groupes sont superposÃ©s.
De plus, cette approche parcimonieuse facilite lâ€™interprÃ©tation des donnÃ©es et offre une qualitÃ©
de partitionnement comparable aux k-means avec lâ€™avantage de ne poser aucune hypothÃ¨se
sur le nombre de clusters. Par ailleurs, choisir un niveau de parcimonie dans les donnÃ©es re-
vient Ã  chercher des sous-ensembles de donnÃ©es explicatifs dâ€™une structure qui leur est propre.
Ce nouveau point de vue peut sâ€™apparenter Ã  une approche par niveaux de densitÃ© communÃ©-
ment appelÃ© levels set qui a Ã©tÃ© initialement abordÃ©e par Hartigan (1987) puis repris par Nolan
(1991). Une comparaison de ces deux approches et la recherche dâ€™un lien thÃ©orique font partis
de nos travaux de recherche en cours.
Remerciements
Nous tenons Ã  remercier le ComitÃ© de Lecture du numÃ©ro spÃ©cial RNTI â€Fouille de Don-
nÃ©es Complexes : donnÃ©es multiplesâ€ qui a contribuÃ© Ã  lâ€™amÃ©lioration de la qualitÃ© de lâ€™article
prÃ©sentÃ©.
RÃ©fÃ©rences
Anderson, E. (1935). The irises of the gaspe peninsula. Bulletin of the American Iris Society 59,
2â€“5.
Apaydin, T., A. Tosun, et H. Ferhatosmanoglu (2008). Analysis of basic data reordering tech-
niques. SSDBM, 517â€“524.
Applegate, D., R. Bixby, V. ChvÃ tal, et W. Cook (2007). The Traveling Salesman Problem.
Princeton University Press.
Arabie, P. et L. Hubert (1990). The bond energy algorithm revisited. IEEE transactions on
systems, man, and cybernetics 20(1), 268â€“274.
Arabie, P., L. J. Hubert, et G. D. Soete (1996). Clustering and Classification, Chapter An
overview of combinatorial data analysis, pp. 5â€“63. World Scientific.
Azzalini, A. et A. Bowman (1990). A look at some data on the old faithful geyser. Applied
Statistics 39, 357â€“365.
Batagelj, V. (1997). Notes on blockmodeling. Social Networks 7, 143â€“155.
Brusco, M., H. F. Kohn, et S. Stahl (2008). Heuristic implementation of dynamic programming
for matrix permutation problems in combinatorial data analysis. Psychometrika.
C. Brunet et al.
Brusco, M. et S. Stahl (2005). Branch and Bound applications in combinatorial data analysis.
Springer.
Brusco, M. et D. Steinley (2006). Inducing a blockmodel structure of two-mode binary data
using seriation procedures. Journal of Mathematical Psychology 50, 468â€“477.
Caraux, G. et S. Pinloche (2005). Permutmatrix : a graphical environment to arrange gene
expression profiles in optimal linear order. Bioinformatics 21(7).
Carroll, D. et P. Arabie (1980). Multidimensional scaling. Annu Rev Psychol 31, 607â€“649.
Chen, C. (2002). Generalized association plots : Information visualization via iteratively ge-
nerated correlation matrices. Statistica Sinica 12, 7â€“29.
Climer, S. et W. Zhang (2006). Rearrangement clustering : Pitfalls, remedies and applications.
Journal of Machine Learning Research 7, 919â€“943.
Croes, G. (1958). A method for solving the traveling-salesman problems. Operations Re-
search 6, 791â€“812.
Dhillon, I., S. Mallela, et D. Modha (2003). Information theoric co-clustering. Ninth ACM
SIGKDD, Int. Conf. Knowledge Discovery and Data Mining KDDâ€™03, 89â€“98.
Doreian, P., V. Batagelj, et A. Ferligoj (2004). Generalized blockmodeling of two-mode net-
work data. Social Networks 26, 29â€“53.
Eastman, W. (1958). Linear programming with pattern constraints. Masterâ€™s thesis, Ph.D.
Dissertation, Harvard University, Cambridge, Mass.
Govaert, G. et N. Nadif (2008). Algorithms for model-based block gaussian clustering. The
4th international Conference on datamining, 536â€“272.
Govaert, G. et N. Nadif (2010). Latent block model for contingency table. Communication in
Statistics : Theory and Methods 39, 416â€“425.
Gutin, G. et A. Punnen (2002). The traveling salesman problem and its variations, Volume 12.
Springer.
Haerdle, W. (1991). Smoothing Techniques with Implementation in S. Springer, New York.
Hahsler, M., K. Hornik, et C. Buchta (2009). Getting things in order : an introduction to the r
package seriation. Technical Report 58, R.
Hartigan, J. (1987). Estimation of a convex density contour in two dimensions. Journal of the
American Statistical Association 82(397), 267â€“270.
Hubert, L., P. Arabie, et J. Meulman (2001). Combinatorial data analysis : Optimization by
dynamic programming. Society ofr industrial and Applied Mathematics.
I.Liiv (2010). Seriation and matrix reordering methods : an historical overview. Wiley inter-
science 3(2), 70â€“91.
Johnson, D., S. Krishnan, et J. Chhugani (2004). Compressing large boolean matrices using
reordering techniques. Proceedings of the Thirtieth international conference on Very large
data bases 30, 13â€“23.
Jolliffe, I. (1986). Principal Component Analysis. Springer.
Kirkpatrick, S., C. Gelatt, et M. Vecchi (1983). Optimization by simulated annealing.
Science 220, 681â€“690.
SparsitÃ© et sÃ©riation
Kohonen, T. (1995). Self Organizing Maps (Heidleberg ed.). Berlin : Springer.
Long, B., Z. Zhang, et P. Yu (2005). Co-clustering by block value decomposition. Proceedings
of the eleventh ACM SIGKDD international conference on Knowledge discovery in data
mining, 635â€“640.
Marcotorchino, F. (1987). Block seriation problems : a unified approach. Applied Stochastic
Models and Data Analysis 3, 73â€“91.
McCormick, W., P. Schweitzer, et T. White (1972). Problem decomposition and data reorga-
nization by a clustering technique. Operations Research 20, 993 â€“1009.
Mechelen, I. V., H.-H. Bock, et P. D. Boeck (2004). Two-mode clustering methods : a structu-
red overview. Statistical Methods in Medical Research 13, 363â€“394.
Niermann, S. (2005). Optimizing the ordering of tables with evolutionary computations. The
American Statistician 59(1), 41â€“46.
Nolan, D. (1991). The excess-mass. Journal of Multivariate Analysis 39, 348â€“371.
Pearson, K. (1901). On lines and planes of closest fit to systems of points is space. Philoso-
phical magazine series 6 2, 157â€“175.
Prelic, A., S. Bleuler, P. Zimmermann, P. B. A. Wille, W. Gruissem, L. Hennig, L. Thiele, et
E. Zitzler (2006). A systematic comparison and evaluation of biclustering methods for gene
expression data. Bioinformatics 22(9), 1122â€“1129.
Raftery, A. et N. Dean (2006). Variable selection for model-based clustering. journal of the
American Statistical Association 101(473), 168â€“178.
Rossman, M., R. Twery, et F. Stone (1958). A Solution to the Traveling Salesman Problem by
combinatorial programming. Peat, Marwick, Mitchell and Co., Chicago.
Ruspini, H. (1970). Numerical methods for fuzzy clustering. Information Sciences 2(3), 319â€“
350.
Ultsch, A. (2005). Clustering with som : U*C. In Proc. Workshop on Self-Organizing Maps,
Paris, France, 75â€“82.
Summary
Seriation is a useful statistical method to visualize clusters in a dataset but since the data
are noisy, the visualization of such a structure becomes difficult. To deal with this problem, a
metric based on common neighborhood is proposed to introduce several degrees of sparsity in
the dataset. A family of sparse matrices are ordered by a branch and bound algorithm and the
matrix which has the best block diagonal form is selected by a compressing criterion. This tool
enables to identify clusters without taking into account noisy data or outliers and reveals the
intrinsic structure of data. However, if the metric introduces too much sparsity in the data, then
under-sampled groups of data could be ousted ; we propose then a multi-scale approach which
combines different levels of sparsity in a same visualization.
