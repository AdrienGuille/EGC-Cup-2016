Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
Bilan du Premier Défi Francophone de Fouille de
Textes
Jérôme Azé
∗
, Mathieu Roche
∗∗
,
Érick Alphonse
∗∗∗
, Ahmed Amrani
∗,∗∗∗∗
Thomas Heitz
∗
, Amar-Djalil Mezaour
∗∗∗∗∗
∗
LRI  Université Paris-Sud
Bât. 490, 91405 Orsay Cedex
{aze,amrani,heitz}@lri.fr
∗∗
LIRMM  Université de Montpellier 2
161 rue Ada, 34392 Montpellier Cedex 5
mroche@lirmm.fr
∗∗∗
LIPN  Université de Paris Nord
99, avenue Jean-Baptiste Clément, 93430 Villetaneuse
Erick.Alphonse@lipn.univ-paris13.fr
∗∗∗∗
ESIEA Recherche
9 rue Vésale, 75005 Paris
amrani@esiea.fr
∗∗∗∗∗
Exalead
10 place de la Madeleine, 75019 Paris
Amar-Djalil.Mezaour@exalead.com
Résumé. Le DÉfi Fouille de Textes (DEFT) a consisté à supprimer les
phrases non pertinentes dans un corpus de discours politiques en français.
Il a eu lieu en 2005 et réuni onze équipes, totalisant une trentaine de
participants. Cet article décrit les prétraitements effectués sur les corpus
de F. Mitterrand et de J. Chirac dans le cadre de ce défi. Notamment,
la conversion au format texte, le découpage en phrases, le classement des
discours, l'introduction de phrases de F. Mitterrand dans les discours de
J. Chirac et l'identification des dates et noms de personnes. Les résultats
obtenus par les onze équipes participantes sont aussi présentés.
1 Introduction
Le but du défi proposé consiste à supprimer les phrases non pertinentes dans
un corpus de discours politiques en français. Ce défi porte le nom de DEFT pour
DÉfi Fouille de Textes. Ce défi, proche de la tâche Novelty du challenge TREC
1
[Soboroff, Harman, 2003, Amrani et al., 2004], est motivé par le besoin de mettre en
place des techniques de fouille de textes permettant soit d'identifier des phrases non
pertinentes dans des textes, soit d'identifier des phrases particulièrement singulières
dans des textes apparemment sans réel intérêt. Cette étape est préliminaire à tout
processus d'extraction d'informations.
1
Text REtrieval Conferences : http ://trec.nist.gov
1 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Par exemple, dans les corpus spécialisés (biologie, médecine, etc.) un travail consé-
quent est dédié à l'identification des phrases pertinentes pour ensuite y rechercher des
informations spécifiques. Ce type de tâche consistant à effectuer un premier filtrage des
textes est une étape préliminaire essentielle à effectuer pour la constitution de corpus
pertinents et homogènes. Ce type de prétraitements est aussi utilisé dans des tâches
type questions/réponses (voir TREC).
Nous proposons ici une liste non exhaustive de tâches similaires à celle proposée
dans DEFT'05 et pour lesquelles il devrait être possible de réutiliser avec peu de mo-
difications les approches mises en ÷uvre pour répondre à DEFT'05.
 détection des passages les plus singuliers dans des textes quelconques (rupture de
style, changement de contexte) ;
 détection de plagiats possibles dans des textes ;
 détection des informations générales dans des corpus techniques.
Le cadre d'étude de l'attribution d'auteur pour un segment de texte ne semble
pas très normalisé malgré les nombreuses études qui existent [Rudman, 1997]. C'est en
partie pour donner un cadre d'étude standardisé que ce défi a été mis en place. Nous
partons de l'hypothèse que les auteurs laissent une empreinte dans les textes décelable
à l'aide de méthodes d'analyse statistique [Baayen et al., 2002].
Un corpus de textes, issus de la Présidence de J. Chirac (1995-2005), a été fourni aux
participants de DEFT'05. Ce corpus obtenu à partir du site http ://elysee.fr/, d'une
taille de 14 Mo de texte brut, est composé d'allocutions officielles du Président. Dans
ce corpus, des passages issus d'un corpus d'allocutions du Président de la République
F. Mitterrand (1981-1995) sont insérés. Les passages d'allocutions de F. Mitterrand in-
sérés sont composés d'au moins deux phrases successives. Chaque discours de J. Chirac
contient zéro ou un passage extrait d'une allocution de F. Mitterrand. Le corpus de
discours de F. Mitterrand représentant 17 Mo de texte brut a été obtenu à partir du
site http ://discours-publics.ladocumentationfrancaise.fr/.
Les passages de F. Mitterrand introduits traitent d'une thématique différente. Par
exemple, dans les allocutions de J. Chirac évoquant la politique internationale, les
phrases de F. Mitterrand introduites sont issues de discours traitant de politique na-
tionale. Ainsi, la rupture thématique peut être une des manières de détecter les phrases
issues du corpus de F. Mitterrand.
Le défi DEFT'05 se présente sous la forme de trois tâches distinctes à résoudre :
 Tâche 1 : Identifier les phrases de F. Mitterrand dans le corpus ne comportant
ni années, ni noms de personnes.
 Tâche 2 : Identifier les phrases de F. Mitterrand dans le corpus ne comportant
pas d'années.
 Tâche 3 : Identifier les phrases de F. Mitterrand dans le corpus avec la présence
des années et des noms de personnes.
Ces trois tâches sont données dans un ordre décroissant de difficulté car l'année
et/ou les noms de personnes peuvent être de bons indicateurs des périodes de prési-
dence.
RNTI - 1 2
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
L'article présente toute la chaîne de traitements du défi (de la préparation des
données jusqu'à l'analyse des résultats de DEFT'05).
Une large part de cet article traite de la préparation des données de DEFT (section
2) effectuée par les auteurs. Le but était non seulement de normaliser les corpus pour les
trois tâches mais également d'introduire les phrases de F. Mitterrand dans les discours
de J. Chirac de manière pertinente afin de rendre le défi abordable sans être trivial.
La section 3 présente une comparaison succincte des approches utilisées par les par-
ticipants de DEFT'05 (méthodes de classification, apports linguistiques, etc.). L'objet
de cet article n'est pas de décrire de manière détaillée les méthodologies choisies par
les participants. Les approches des participants sont présentées dans les autres articles
de cette revue.
La section 4 analyse les résultats obtenus par les participants. Cette analyse s'appuie
sur le calcul du Fscore, critère d'évaluation fixé pour le défi et pour la plupart des
compétitions internationales de fouille de textes. De plus, une discussion fondée sur
l'utilisation du front de Pareto complète l'analyse des résultats.
Enfin, pour conclure, nous présentons une discussion qui nous a permis de motiver
la mise en ÷uvre du défi DEFT'06 en septembre 2006.
2 Préparation des données
La figure 1 illustre l'ensemble de la chaîne de préparation des données du défi.
Toutes les étapes de cette chaîne sont décrites dans les sections suivantes. Ainsi, après
l'acquisition des corpus utilisés pour de défi, les différents traitements présentés dans
cette section sont relatifs à la normalisation des corpus, leur expertise, la méthode
utilisée pour introduire les phrases de F. Mitterrand dans le corpus de J. Chirac, la
préparation des corpus pour les trois tâches du défi, les derniers traitements nécessaires
en fin de chaîne. Une dernière section (section 2.6) présente une comparaison entre les
corpus de test et d'apprentissage constitués.
2.1 Normalisation des corpus
Les corpus d'allocutions ont demandé un nombre de prétraitements important.
Après avoir supprimé les commentaires et les balises HTML, les en-têtes des allo-
cutions ont été enlevées (dates, lieux, etc.). Puis les entités au format SGML ont été
transformées en caractères ISO8859-1. Par exemple, les entités  &eacute ;  ont été
remplacées par le caractère  é .
Chacune des lignes des corpus fournis aux participants est composée d'une seule
phrase. Pour identifier les phrases, il est nécessaire de repérer les ponctuations de fin
de phrases (point final, point d'exclamation, point d'interrogation). Notons que comme
dans les travaux de [Smadja, 1993], cette tâche nécessite le fait de ne pas considérer
tous les points comme des ponctuations de fin de phrases (par exemple, les abréviations
telles que R.M.I. pour Revenu Minimum d'Insertion ou M. pour Monsieur). De manière
similaire aux travaux de [Rudolf, widzi«ski, 2004], nous pouvons considérer que les
3 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Fig. 1  Chaîne globale de traitements de DEFT'05.
RNTI - 1 4
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
points peuvent avoir des rôles spécifiques et sont utilisés dans différentes situations :
abréviations, adresses internet, numéros de sections, etc.
Chaque locuteur peut utiliser régulièrement des phrases types du domaine qui pour-
raient permettre d'identifier les allocuteurs. À titre d'exemple, nous avons supprimé
l'expression  Vive la République  qui est plus fréquente dans le corpus de F. Mit-
terrand (216 fois dans le corpus de F. Mitterrand contre 48 fois dans le corpus de J.
Chirac).
Enfin, chaque phrase a été indexée grâce à une numérotation spécifique.
2.2 Expertise des corpus
Une étape d'expertise manuelle a alors été effectuée à partir des corpus normalisés.
Le but de cette expertise a consisté à associer une catégorie à chacun des textes du
corpus. Trois catégories ont été déterminées par le comité d'organisation (les auteurs
de cet article) :
 Catégorie nationale
 Catégorie internationale
 Catégorie mixte ou ambiguë
Un discours traitant à 80% (estimation) d'une thématique déterminée sera associée à
cette catégorie. Les discours contenant moins de 80% d'une thématique ont été associés
à la catégorie mixte et ont été supprimés des données utilisées pour créer les corpus
fournis aux participants.
Au total, 2523 textes ont été expertisés par les six organisateurs et auteurs de
cet article : 1200 allocutions de J. Chirac et 1323 allocutions de F. Mitterrand. Sur
ces 2523 textes, 36.6% des textes ont été associés à la catégorie Nationale, 47.2% à
la catégorie Internationale et 16,2% à la catégorie Mixte (voir tableau 1). Le détail
complet des résultats donnés dans le tableau 1 montre notamment que les discours de
F. Mitterrand ont davantage été associés à la catégorie Nationale que les allocutions
de J. Chirac.
F. Mitterrand J. Chirac Global
Nationale 40.8% 31.9% 36.6%
Internationale 45.0% 49.7% 47.2%
Mixte 14.1% 18.4% 16.2%
Tab. 1  Répartition des expertises par allocuteur.
Précisons que d'une période à l'autre, la répartition peut différer significativement.
À titre d'exemples, les allocutions officielles de J. Chirac en 2002, année de l'élection
présidentielle ont davantage été associées à la catégorie nationale. Sur les 125 allocu-
tions de J. Chirac en 2002, 63 (50%) appartiennent à la catégorie Nationale, 46 (36.7%)
appartiennent à la catégorie Internationale et 16 (12.8%) ont été associées à la catégorie
Mixte.
5 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
L'expertise présentée dans cette section sera utilisée pour introduire les phrases de
F. Mitterrand dans le corpus de J. Chirac (voir section suivante).
2.3 Introduction des phrases de F. Mitterrand dans le corpus
de J. Chirac
L'introduction des extraits de discours de F. Mitterrand dans les discours de J.
Chirac a été réalisée en respectant les points suivants :
 croisement des thématiques identifiées (politique nationale vs politique interna-
tionale) ;
 sélection des extraits de discours de F. Mitterrand les plus proches des discours
de J. Chirac pour l'introduction ;
 introduction d'au plus un passage de F. Mitterrand dans chaque discours de J.
Chirac.
Le croisement des thématiques est lié à l'analyse présentée dans le tableau 1. Nous
présentons le point 2 et le point 3 respectivement dans les deux paragraphes suivants.
2.3.1 Sélection des passages à insérer
La sélection des passages se fait sur la base d'un score de similarité entre un extrait
de F. Mitterrand et un discours de J. Chirac. Ce calcul est fonction des n-grammes
2
de
caractères et n-grammes de mots (n caractères ou mots consécutifs présents dans les
textes permettant ainsi de les caractériser).
Calcul du score.
Nous avons calculé de manière systématique les n-grammes de caractères et de mots
(pour n=1, 2 et 3) des discours de J. Chirac et des parties de discours de F. Mitterrand
candidates à l'insertion (c'est-à-dire toutes les parties de discours sauf la première et la
dernière). Puis, nous avons comparé les discours de J. Chirac et parties de discours de F.
Mitterrand (en tenant compte du croisement thématique) sur la base de ces n-grammes.
Ces éléments sont comparés sur la base du score suivant :
score(dcatC , p
cat
M ) = scorecar(d
cat
C , p
cat
M ) + scoremot(d
cat
C , p
cat
M ) (1)
avec 
cat international ou national
cat cate´gorie oppose´e a` cat
dcatC discours de J. Chirac appartenant a` cat
pcatM partie de discours de F. Mitterrand appartenant a` cat
scorecar(dcatC , p
cat
M ) =
3∑
n=1
(
2
n
)
× communn(d
cat
C , p
cat
M )
|dcatC |n + |pcatM |n
(2)
2
L'outil nsp-v0.71 a été utilisé pour calculer les n-grammes
(http ://www.d.umn.edu/∼tpederse/nsp.html).
RNTI - 1 6
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
où
|x|n nombre de n mots ou n caracte`res conse´cutifs de x
communn(dcatC , p
cat
M ) nombre de n mots ou n caracte`res conse´cutifs
communs entre dcatC et p
cat
M
scoremot(dcatC , p
cat
M ) est calculé selon la même formule mais sur la base des n-
grammes entre mots et non pas entre caractères.
Prenons un exemple de trois fragments de phrases d1, d2 et d3 et calculons les
scores obtenus. Dans cet exemple, nous considérons ici que la phrase d1 appartient à
un discours de François Mitterrand, catégorie Nationale et que les phrases d2 et d3
appartiennent à un discours de Jacques Chirac, catégorie Internationale.
Exemples
d1 : Le ministre Jospin
a
d2 : Le premier ministre Juppé
b
d3 : Le premier ministre Jospin
c
a
Ministre de l'Éducation Nationale (1988-1992) sous la Présidence de F. Mitterrand
b
Premier Ministre (1995-1997) sous la Présidence de J. Chirac
c
Premier Ministre (1997-2002) sous la Présidence de J. Chirac
Nous calculons ci-dessous les scores entre les phrases d2 et d1 puis entre d3 et d1
qui respectent le croisement thématique et des locuteurs. Le détail des n-grammes est
donné en annexe dans le tableau 10.
Calcul du score fondé sur les n-grammes de caractères  formule (2)
communn=1(d2, d1) = 11, communn=1(d3, d1) = 12
communn=2(d2, d1) = 11, communn=2(d3, d1) = 15
communn=3(d2, d1) = 10, communn=3(d3, d1) = 15
|d1|n=1 = 12, |d2|n=1 = 13, |d3|n=1 = 12
|d1|n=2 = 15, |d2|n=2 = 21, |d3|n=2 = 21
|d1|n=3 = 16, |d2|n=3 = 23, |d3|n=3 = 24
scorecar(d2, d1) =
(
2
1
)× 1113+12 + ( 22)× 1121+15 + ( 23)× 1023+16 = 1.36
scorecar(d3, d1) =
(
2
1
)× 1212+12 + ( 22)× 1521+15 + ( 23)× 1524+16 = 1.67
Calcul du score fondé sur les n-grammes de mots  formule (2)
communn=1(d2, d1) = 1, communn=1(d3, d1) = 2
communn=2(d2, d1) = 0, communn=2(d3, d1) = 1
communn=3(d2, d1) = 0, communn=3(d3, d1) = 0
|d1|n=1 = 2, |d2|n=1 = 3, |d3|n=1 = 3
|d1|n=2 = 2, |d2|n=2 = 3, |d3|n=2 = 3
|d1|n=3 = 1, |d2|n=3 = 2, |d3|n=3 = 2
scoremot(d2, d1) =
(
2
1
)× 13+2 + ( 22)× 03+2 + ( 23)× 02+1 = 0.4
scoremot(d3, d1) =
(
2
1
)× 23+2 + ( 22)× 13+2 + ( 23)× 02+1 = 1
7 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Calcul du score fondé sur les n-grammes de caractères et de mots  formule (1)
score(d2, d1) = 1.36 + 0.4 = 1.76
score(d3, d1) = 1.67 + 1 = 2.67
Le résultat précédent montre que les phrases d1 et d3 respectant le croisement thé-
matique et de locuteurs différents sont, d'après le score établi, plus proches que les
phrases d1 et d2.
Choix des passages à insérer fondé sur les scores calculés.
Afin que les parties des discours de F. Mitterrand soient intégrées dans le corpus
de J. Chirac sans qu'il y ait de ruptures trop évidentes, nous avons utilisé les scores
calculés avec la formule 1 pour choisir les passages à insérer. Le but était alors d'intro-
duire les passages des discours de F. Mitterrand dans les discours de J. Chirac ayant
des scores élevés. Avec l'exemple précédent, les discours d1 et d3 sont alors sélectionnés.
Plus formellement et plus généralement la méthode de sélection des passages à
insérer respecte le principe suivant. Ayant calculé le score pour tous les couples possibles
(dcatC , p
cat
M ), nous retenons pour chaque d
cat
C les vingt meilleurs p
cat
M (c'est-à-dire tels
que score(dcatC , p
cat
M ) soient les plus élevés). Ces vingt candidats à l'insertion sont triés
par valeurs décroissantes du score.
Puis, les discours de J. Chirac sont parcourus aléatoirement et les insertions de
passages de discours de F. Mitterrand sont réalisées de la manière suivante :
Soient dcatC le discours de J. Chirac étudié et Ld
cat
C
pcatM
la liste des passages candidats à
l'insertion. Soit EpcatM l'ensemble des passages de discours de F. Mitterrand déjà intro-
duits dans des discours de J. Chirac.
La liste ordonnée LdcatC
pcatM
est parcourue depuis le premier passage vers le dernier
jusqu'à trouver un passage qui soit absent de EpcatM . Si un tel passage existe, il est
introduit dans dcatC , puis dans EpcatM . Par contre, si aucun passage n'est trouvé alors le
discours de J. Chirac étudié n'est pas modifié (c'est-à-dire que le discours est donc non
bruité).
2.3.2 Insertion d'un passage de F. Mitterrand dans un discours de J. Chi-
rac
La position d'un passage à insérer est déterminée en respectant les contraintes
suivantes :
 ni avant le premier, ni après le dernier paragraphe
3
du discours de J. Chirac ;
 aléatoirement dans le reste du discours et entre deux paragraphes.
Le corpus ainsi constitué a été divisé en deux sous-ensembles : le corpus d'appren-
tissage et le corpus de test. Nous avons utilisé 70% des discours pour constituer le
corpus d'apprentissage et les 30% restant pour le test. Les discours ont été choisis de
manière aléatoire et stratifiée : nous avons garanti par construction que les proportions
3
Un paragraphe correspond à un bloc de texte entre balises HTML <p> ou séparé par deux balises
<br>.
RNTI - 1 8
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
de discours bruités et non bruités, dans les corpus de test et d'apprentissage, sont
identiques à celles observées dans le corpus initial.
Il peut arriver que deux thématiques identiques (Nationale ou Internationale) soient
insérées dans un même texte. Ceci peut s'expliquer par le fait qu'un texte de F. Mitter-
rand associé à une catégorie Nationale (resp. Internationale) peut comporter des pas-
sages d'une catégorie Internationale (resp. Nationale). Ces passages de F. Mitterrand
de la catégorie Internationale (resp. Nationale) bien que minoritaires dans l'allocution
associée à la catégorie Nationale (resp. Internationale) pourraient alors être introduits
dans une allocution de J. Chirac de la catégorie Internationale (resp. Nationale).
2.4 Constitution des trois corpus avec et sans informations re-
latives aux noms de personnes et aux années
Nous rappelons que le défi DEFT'05 comporte trois tâches distinctes :
 Tâche 1 : Identifier les phrases de F. Mitterrand dans le corpus de test ne
comportant ni années, ni noms de personnes.
 Tâche 2 : Identifier les phrases de F. Mitterrand dans le corpus de test ne
comportant pas d'années.
 Tâche 3 : Identifier les phrases de F. Mitterrand dans le corpus de test avec la
présence des années et des noms de personnes.
Pour constituer les corpus associés aux tâches 1 et 2, nous avons du identifier les
dates (années) ainsi que les noms de personnes. Ces identifications sont détaillées ci-
dessous.
2.4.1 Identification des dates
Seules les années situées dans l'intervalle [1900 : 2099] ont été identifiées. Ces an-
nées pourraient en effet faciliter l'identification des phrases issues du corpus de F.
Mitterrand.
Ainsi les années de la forme 19xx et 20xx où  x  est un chiffre quelconque ont été
identifiées et remplacées par une balise <date>. De même, les intervalles entre années
ont été reconnus : 19xx-19xx, 19xx-20xx, 20xx-20xx et xx-xx.
Chacune des dates de ces intervalles ont également été remplacées par une balise
<date>.
Les dates au format 1er février 2004 n'ont pas été identifiées et peuvent donc
figurer dans les corpus, sous la forme 1er février <date>
Ce traitement a permis de constituer les corpus utiles pour les tâches 1 et 2.
2.4.2 Identification des noms de personnes
Pour constituer le corpus de la tâche 1, chaque nom de personne repéré dans une
liste a été remplacé dans le document par une balise <nom>. Cette liste a été établie
semi-automatiquement.
9 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Tout d'abord, un dictionnaire de noms de personnes composés d'un seul mot a été
constitué (par exemple, Picasso, Dali, etc.). Ensuite, les membres du Comité d'Organi-
sation de DEFT'05 ont analysé les suites de mots suivants dans l'ensemble des corpus
afin d'identifier les noms de personnes :
 couples de mots commençant par une majuscule ;
 couples de mots commençant par une majuscule avec une particule intercalée
entre les deux mots ;
 particule suivi d'un mot en majuscules.
Les particules utilisées (avec et sans majuscules) sont les suivantes : Abd, Al, Ap,
Ben, Bin, D', Da, Dalle, Dall', Dell', De, De La, De Los, Del, Dela, Della, Delle, Den,
Der, Di, Du, El, Ibn, La, Le, Li, Lo, Mac, Mc, O', Of, Saint, San, Van, Van Den, Van
Der, Von, Von Der, y.
2.5 Traitement final des corpus
Le dernier traitement a consisté à maintenir en majuscule seulement la première
lettre des noms de personnes. En effet, dans le corpus de J. Chirac la plupart des noms
de personnes sont écrits en majuscules (Jacques CHIRAC, François MITTERRAND,
etc.). Ainsi, l'identification des noms en majuscules aurait pu être une règle simple
mais efficace pour reconnaître les phrases issues du corpus de F. Mitterrand et de J.
Chirac des tâches 1 et 2. Pour corriger cette situation, nous avons uniformisé l'écriture
des noms de personnes en écrivant seulement en majuscule la première lettre du nom
de personne : MITTERRAND → Mitterrand. Bien entendu les acronymes (PS, RPR,
EDF, etc.) sont maintenus en majuscules.
Lors du passage majuscules/minuscules, il est très fréquent que les accents soient à
restituer (par exemple  JUPPE  correspond en lettres minuscules à  Juppé ).
Une manière semi-automatique de procéder consiste à relever la présence des noms de
personnes (nom commençant par une majuscule) que l'on trouve dans le texte avec
des accents. Dans ce cas, nous pouvons décider d'apposer par défaut l'accent omis. Si
aucun mot similaire (avec accents) n'est repéré dans le corpus, et sans utiliser de res-
sources extérieures, il est nécessaire d'expertiser ces noms et d'y apposer manuellement
les accents manquants.
2.6 Similarités entre les corpus d'apprentissage et de test
Les corpus d'apprentissage et de test ont été constitués simultanément, c'est la
raison pour laquelle ils ont des caractéristiques similaires. Ainsi, les méthodes mises en
÷uvre sur les corpus d'apprentissage peuvent être appliquées sur les corpus de test sans
nécessiter d'adaptations spécifiques. Nous donnons dans le tableau 2 les caractéristiques
essentielles des corpus d'apprentissage et de test.
Remarquons que le pourcentage d'étiquettes <nom> dans les corpus de test est
plus élevé que dans les corpus d'apprentissage (voir tableau 2). Cela peut s'expliquer
par le fait que nous avons apporté une attention toute particulière à la préparation des
corpus de test pour lesquels les participants avaient seulement deux à quatre jours de
RNTI - 1 10
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
Corpus Corpus
d'apprentissage de test
Taille moyenne des
phrases successives 18.8 19.1
de F. Mitterrand
Pourcentage d'allocutions sans 31.9% 32.3%
phrases de F. Mitterrand insérées (187/587) (95/294)
Nombre d'étiquettes <nom> 0.18% 0.21%
par rapport au nombre de mots du corpus (2511/1420833) (1331/616584)
Nombre d'étiquettes <date> 0.13% 0.12%
par rapport au nombre de mots du corpus (1846/1420833) (774/616584)
Tab. 2  Comparaison des corpus d'apprentissage et de test.
traitements possibles.
Après avoir décrit les traitements effectués pour la préparation du corpus de DEFT-
'05, la section suivante résume les différentes méthodes utilisées par les participants.
Des descriptions plus précises des approches utilisées sont développées dans les autres
articles de cette revue.
3 Approches utilisées pour la résolution des tâches de
DEFT'05
Onze équipes ont participé à DEFT'05. Ces onze équipes sont issues de neuf labo-
ratoires différents et représentent une trentaine de participants.
Le tableau 3 présente les caractéristiques des traitements effectués par les diffé-
rentes équipes. Le prétraitement le plus efficace semble être les n-grammes de mots
compris entre 1 et 4 (équipes 1, 3, 4 et 7-9), tandis que la suppression des mots (2, 4,
8 et 10), tels les mots vides, ne semble pas avoir une influence déterminante sur le Fscore.
Les méthodes de classification les plus employées ont été les chaînes de Markov
(équipes 1, 2, 4, 6 et 7-9) et l'apprentissage bayésien (1, 3, 4 et 8). Les meilleurs Fscores
ont été obtenus grâce à la combinaison de ces deux méthodes (1 et 4) bien que les
chaînes de Markov seules semblent déjà très efficaces (2).
L'apport linguistique le plus utilisé est l'étiquetage grammatical (équipes 1, 6, 7-9,
8 et 10) même si les résultats sont très différents d'une équipe à l'autre. L'utilisation
d'entités nommées (1 et 6) semble avoir été efficace pour l'équipe 1 grâce à l'utilisation
d'un dictionnaire conséquent. Les approches à base de vecteurs de termes ne semblent
pas les plus adaptées (8 et 10).
Les participants à DEFT'05 ont travaillé durant près de deux mois sur les corpus
11 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Numéro d'équipe 1 2 3 4 5 6 7-9 8 10
Types de prétraitements
Suppression de mots X X X X
N-lettres 7
N-mots 1-2 4 1-3 4
Méthodes de classification
Chaînes de Markov X X X X X
Viterbi X X X X
Bayes X X X X
SVM X X
Apports linguistiques
Vecteurs de termes X X
Étiquetage grammatical X X X X X
Relations syntaxiques X X
Entités nommées X X
Tab. 3  Comparaison des traitements effectués par les différentes équipes. Les équipes
qui ont un article dans cette revue sont les équipes numérotées 1, 2, 4, 5, 7-9 et
10.
d'apprentissage (étiquetés selon les locuteurs). Les corpus de test (sans l'information sur
les locuteurs) ont ensuite été mis à disposition. Les résultats obtenus avec les approches
résumées dans cette section ont alors été envoyés au comité d'organisation. L'analyse
de ces résultats à partir des corpus de test est présentée dans la section suivante.
4 Analyse des résultats de DEFT'05
Les résultats obtenus sont assez variés (en termes de précision, rappel et Fscore) et
tendent donc à montrer que les tâches à réaliser étaient non triviales, tout en restant
parfaitement abordables.
4.1 Analyse des résultats fondée sur le Rappel, la Précision et
le Fscore
Toutes les exécutions du défi ont été évaluées en calculant le Fscore (avec β = 1) :
Fscore(β) =
(β2 + 1)× Pre´cision×Rappel
β2 × Pre´cision+Rappel (3)
Sachant que la précision et le rappel sont fournies par les mesures suivantes :
Pre´cision =
nb phrases correctement extraites
nb phrases extraites
(4)
Rappel =
nb phrases correctement extraites
nb phrases pertinentes
(5)
RNTI - 1 12
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
tâche 1 tâche 2 tâche 3
équipe 1 0.870 (1) 0.884 (1) 0.880 (1)
équipe 2 0.860 (2) 0.852 (2) 0.866 (2)
équipe 3 0.820 (3) 0.821 (3) 0.819 (3)
équipe 4 0.760 (4) 0.742 (6) 0.746 (6)
équipe 5 0.751 (5) 0.755 (5) 0.755 (5)
équipe 6 0.732 (6) 0.794 (4) 0.788 (4)
équipe 7 0.562 (7) 0.559 (8) 0.573 (7)
équipe 8 0.494 (8) 0.521 (9) 0.507 (9)
équipe 9 0.493 (9) 0.560 (7) 0.563 (8)
équipe 10 0.325 (10) 0.307 (10) 0.305 (11)
équipe 11 0.177 (11) 0.177 (11) 0.417 (10)
Tab. 4  Meilleurs Fscore des différentes équipes pour chaque tâche. Le rang corres-
pondant pour chaque tâche est indiqué entre parenthèses. Ces résultats sont triés par
Fscore décroissant sur la base de la première tâche.
Le Fscore pour β = 1 se réécrit de la manière suivante :
Fscore =
2× nb phrases correctement extraites
nb phrases pertinentes+ nb phrases extraites
(6)
Le tableau 4 et les figures 2, 3 et 4 présentent les meilleurs Fscore obtenus par les
différentes équipes pour chaque tâche. L'analyse des résultats détaillés (par exécution)
sur la base du Fscore avec β = 1 permet de voir que la plupart des équipes ont amélioré
leurs résultats au fur et à mesure des tâches. Ainsi, l'ajout d'informations (noms de
personnes, puis années) représente une aide réelle pour les différents systèmes repré-
sentés dans ce défi.
Le tableau 5 présente les meilleures mesures de précision et et de rappel pour chaque
équipe. Par exemple, pour la première tâche, ce tableau montre que les équipes ayant
un meilleur score de précision (équipe 5) et de rappel (équipe 9) n'ont pas obtenu les
meilleurs scores en terme de Fscore (voir tableau 4). Ceci s'explique par le fait que
ces équipes ayant une précision (resp. rappel) de bonne qualité ont un rappel (resp.
précision) décevant. Ainsi, pour avoir un Fscore ayant une valeur importante, il est
nécessaire d'avoir un bon compromis entre le rappel et la précision. Les deux équipes
les mieux placées en terme de Fscore sont les équipes 1 et 2 (voir tableau 4). Ainsi, ces
dernières ont obtenu des valeurs élevées aussi bien en terme de précision que de rappel
(voir tableau 5).
La figure 2 confirme que les résultats en terme de précision et de rappel sont singu-
lièrement différents d'une équipe à l'autre qui peuvent avoir des Fscore assez faibles. Par
exemple, l'équipe 10 possède une précision très correcte alors que le rappel est faible
et inversement le rappel de l'équipe 9 est très élevé et la précision est faible. Selon la
tâche à effectuer, une équipe ayant une excellente précision (resp. rappel) peut être
particulièrement recherchée. Ainsi, une manière différente d'évaluer le résultat global
13 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Précision Rappel
tâche 1 tâche 2 tâche 3 tâche 1 tâche 2 tâche 3
éq 5 0.941(1) 0.925 (1) 0.927 (1) éq 9 0.915(1) 0.623 (9) 0.630 (9)
éq 1 0.883(2) 0.911 (2) 0.890 (3) éq 7 0.900(2) 0.904 (1) 0.906 (1)
éq 2 0.880(3) 0.884 (3) 0.875 (4) éq 1 0.858(3) 0.861 (2) 0.872 (3)
éq 3 0.868(4) 0.866 (4) 0.891 (2) éq 2 0.856(4) 0.857 (4) 0.884 (2)
éq 4 0.801(5) 0.802 (6) 0.754 (7) éq 4 0.849(5) 0.859 (3) 0.861 (4)
éq 10 0.778(6) 0.790 (7) 0.791 (6) éq 6 0.812(6) 0.781 (6) 0.736 (8)
éq 6 0.666(7) 0.808 (5) 0.848 (5) éq 8 0.783(7) 0.852 (5) 0.838 (5)
éq 9 0.532(8) 0.520 (8) 0.546 (8) éq 3 0.777(8) 0.780 (7) 0.758 (6)
éq 7 0.413(9) 0.472 (9) 0.505 (9) éq 5 0.755(9) 0.756 (8) 0.757 (7)
éq 8 0.365(10) 0.384 (10) 0.364 (11) éq 10 0.207(10) 0.190 (10) 0.189 (11)
éq 11 0.226(11) 0.226 (11) 0.402 (10) éq 11 0.145(11) 0.145 (11) 0.434 (10)
Tab. 5  Meilleures mesures de précision et de rappel des différents équipes pour chaque
tâche. Le rang correspondant pour chaque tâche est indiqué entre parenthèses.
des équipes consiste à utiliser le front de Pareto qui sera développé dans la section
suivante.
 0
 0.2
 0.4
 0.6
 0.8
 1
éq
ui
pe
 1
éq
ui
pe
 2
éq
ui
pe
 3
éq
ui
pe
 4
éq
ui
pe
 5
éq
ui
pe
 6
éq
ui
pe
 7
éq
ui
pe
 8
éq
ui
pe
 9
éq
ui
pe
 1
0
éq
ui
pe
 1
1
F s
co
re
 
(β=
1)
Meilleure exécution de chaque équipe
Tâche 1
Fscore
Rappel
Précision
Fig. 2  Fscores (β = 1) pour les meilleures exécutions - Tâche 1.
4.2 Analyse des résultats fondée sur le front de Pareto
Le front de Pareto est défini par l'ensemble des approches qui sont telles qu'aucune
autre approche ne présente de meilleurs résultats pour tous les critères étudiés (ici
précision et rappel). Les approches qui ne sont pas sur le front de Pareto sont dites
dominées.
RNTI - 1 14
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
 0
 0.2
 0.4
 0.6
 0.8
 1
éq
ui
pe
 1
éq
ui
pe
 2
éq
ui
pe
 3
éq
ui
pe
 6
éq
ui
pe
 5
éq
ui
pe
 4
éq
ui
pe
 9
éq
ui
pe
 7
éq
ui
pe
 8
éq
ui
pe
 1
0
éq
ui
pe
 1
1
F s
co
re
 
(β=
1)
Meilleure exécution de chaque équipe
Tâche 2
Fscore
Rappel
Précision
Fig. 3  Fscores (β = 1) pour les meilleures exécutions - Tâche 2.
 0
 0.2
 0.4
 0.6
 0.8
 1
éq
ui
pe
 1
éq
ui
pe
 2
éq
ui
pe
 3
éq
ui
pe
 6
éq
ui
pe
 5
éq
ui
pe
 4
éq
ui
pe
 7
éq
ui
pe
 9
éq
ui
pe
 8
éq
ui
pe
 1
1
éq
ui
pe
 1
0
F s
co
re
 
(β=
1)
Meilleure exécution de chaque équipe
Tâche 3
Fscore
Rappel
Précision
Fig. 4  Fscores (β = 1) pour les meilleures exécutions - Tâche 3.
15 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Par exemple, deux des approches soumises par l'équipe numéro 5 pour la tâche 1
sont telles qu'elles présentent une précision optimale (par rapport aux autres résultats)
pour un rappel faible (voir Figure 5). Mais il n'existe pas d'autres approches ayant à
la fois une meilleure précision et un meilleur rappel.
L'analyse du front de Pareto associé à la tâche 1 (voir Figure 5) montre que plu-
sieurs équipes se trouvent sur le front de Pareto et donc qu'en fonction de la valeur de
β choisie, l'ordre des approches en fonction du Fscore peut être modifié. Nous obtenons
les mêmes résultats pour les tâches 2 et 3 (voir Figures 6 et 7).
Notons que l'équipe ayant le meilleur résultat en terme de Fscore avec β = 1 est
obtenu avec l'équipe se situant au plus près du point de coordonnées (1,1).
La courbe 5 présente l'ensemble des exécutions des équipes. Cette courbe montre
que cinq équipes sont présentes sur le front de Pareto : les équipes 1, 2, 5, 7 et 9. Nous
avons noté dans la section précédente que l'équipe 9 avait un rappel très élevé, ce qui
explique la présence de cette équipe sur le front de Pareto.
Notons enfin que sur cette même courbe (Figure 5), les deux points relatifs à l'équipe
5 (évoqués précédemment) situés sur le front de Pareto ne représentent pas le Fscore
sélectionné pour le classement du défi. En effet, ces deux exécutions ont un Fscore re-
lativement faible (0.580 et 0.416 - voir tableau 7 de l'Annexe) comparativement à celui
de la troisième exécution (0.751 - voir tableau 4 et tableau 7 de l'Annexe). Pourtant,
ces deux exécutions appartiennent au front de Pareto, contrairement à la troisième qui
présente la meilleure valeur de Fscore.
Pour cette même équipe, une valeur de β de 0.2 permet d'obtenir un Fscore = 0.886
pour l'approche présentant une précision de 0.926 et un rappel de 0.422. Cette valeur
de β indique que la précision a un poids supérieur à celui du rappel. De plus, avec une
telle valeur de β, cette équipe se retrouverait en première position pour le critère du
Fscore.
5 Conclusion
La problématique abordée dans DEFT'05 est relative à une tâche importante dans
tout processus de fouille de données et constitue une étape préliminaire aux phases
d'extraction d'informations.
L'implication de nombreuses équipes de recherche dans ce défi montre l'intérêt réel
de la communauté pour ce problème et notamment pour la comparaison et l'évaluation
de différentes méthodes de prétraitement des données et d'extraction d'informations.
La diversité des résultats obtenus par les équipes ayant participé montre que cette
tâche représente une réelle difficulté pour la communauté et l'un des avantages de
DEFT'05 est lié à la nature artificielle du corpus qui permet ainsi une évaluation plus
RNTI - 1 16
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ap
pe
l
Précision
11
1010
10
9
9
9
8
8
8
7 7
6
5
5
5
4
4
4 3
2
2
11
1
Fig. 5  Front de Pareto pour la tâche 1.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ap
pe
l
Précision
11
10
10
9
9
9
88
8
7
7
7
6
5
5
5
4
4
4 3
2
2
1
1
1
Fig. 6  Front de Pareto pour la tâche 2.
17 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ap
pe
l
Précision
11
10
10
9
9
9
888
7
7
7
6
5
5
5
4
4
4
3
2
2
1
11
Fig. 7  Front de Pareto pour la tâche 3.
objective des résultats obtenus par les différentes équipes.
L'engouement de la communauté pour ce défi et les différentes propositions d'ex-
tensions de DEFT vont permettre la poursuite de ce défi en 2006. Comme l'a montré
l'atelier consacré à DEFT'05 dans le cadre de la conférence TALN'05, DEFT'05 a réussi
à fédérer la communauté francophone de fouille de textes. Lors de cet atelier, l'ensemble
des participants a souhaité poursuivre l'initiative amorcée avec DEFT'05. Cependant,
la majorité des chercheurs présents a souhaité que DEFT'06 mette plutôt l'accent sur
des données réelles pour pouvoir ainsi mieux évaluer les différentes approches.
Les organisateurs de DEFT'05 se sont donc portés volontaires pour organiser la
deuxième édition de DEFT. Trois corpus différents sont mis à la disposition des parti-
cipants dans le cadre du défi DEFT'06. La nature du défi a également évolué puisqu'il
s'agit de se focaliser sur la détection de segments thématiques clairement identifiés
dans les corpus utilisés. L'identification de ces segments thématiques a été réalisée in-
dépendamment de DEFT'06 soit par les auteurs des corpus, soit par la nature même
des corpus ou enfin par des experts des domaines concernés. Cette expertise extérieure
au défi permet de répondre à l'une des questions soulevées dans le cadre de DEFT'05
qui concernait la compétence du comité d'organisation dans l'analyse des discours po-
litiques et leurs classements dans les catégories nationales ou internationales.
RNTI - 1 18
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
Références
[Amrani et al., 2004] Amrani. A, Azé. J, Heitz. T, Kodratoff. Y and Roche. M (2004),
From the texts to the concepts they contain : a chain of linguistic treatments, Pro-
ceedings of TREC'04 (Text REtrieval Conference), National Institute of Standards
and Technology, Gaithersburg Maryland USA, pages 712-722.
[Baayen et al., 2002] Baayen, R. H., H. Van Halteren, A. Neijt, and F. Tweedie (2002),
An experiment in authorship attribution. In : Proceedings of JADT 2002. St. Malo,
pp. 29-37.
[Rudman, 1997] Joseph Rudman (1997), The State of Authorship Attribution Studies :
Some Problems and Solutions, Computers and the Humanities, Volume 31, Issue
4, Jul 1997, pages 351-365.
[Rudolf, widzi«ski, 2004] Rudolf M., M. widzi«ski (2004), Automatic utterance
boundaries recognition in large Polish text corpora, Proceedings of IIPWM'04
(Intelligent Information Processing and Web Mining), Springer Verlag series Ad-
vances in Soft Computing, 247-256.
[Smadja, 1993] Smadja F. (1993), Retrieving collocations from text : Xtract, Compu-
tational Linguistics, Vol. 191, p143-177.
[Soboroff, Harman, 2003] Soboroff I., Harman D. (2003), Overview of the TREC 2003
Novelty Track, NIST Special Publication : SP 500-255 The Twelfth Text Retrieval
Conference (TREC 2003).
Summary
The text-mining challenge (DEFT) consisted of removing non relevant sentences
from French corpora of political speeches. It took place in 2005 and brought together
about thirty participants from eleven teams. This paper describes the preprocessings
carried out on the corpora of F. Mitterrand and J. Chirac within the framework of this
challenge. In particular, conversion to text format, sentence segmentation, classification
of the speeches, introduction of F. Mitterrand's sentences into J. Chirac's speeches
and identification of dates and people's names. The results obtained by the eleven
participating teams are also analysed in terms of their fscore and their place on the
Pareto front.
Annexe
19 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Numéro d'équipe Laboratoire Correspondant
1 LIA Marc Elbèze
Grégoire Moreau-de-Montcheuil
Patrice Bellot
Juan-Manuel Torres-Moreno
2 ENST Loïs Rigouste
Olivier Cappé
François Yvon
3 LORIA Laurent Pierron
UHP Coskun Durkal
ESIAL Sébastien Freydiger
4 LIA Alexandre Labadié
Yann Romero
Laurianne Sitbon
5 CLIPS-Imag Loïc Maisonnasse
Caroline Tambellini
6 LIP6 Frédéric Kerloch
Ludovic DENOYER
Patrick Gallinari
7 LIMSI Nicolas Hernandez
Gabriel Illouz
Benoit Habert
8 LGI2P Michel Plantié
Gérard Dray
Alexandre Meimouni
Pascal Poncelet
Jacky Montmain
9 LIMSI Martine Hurault-Plantet
Michèle Jardino
10 LIRMM Jacques Chauché
11 LaBRI Richard Moot
Patrick Henry
Renaud Marley
Maxime Amblard
Tab. 6  Équipes ayant participées à DEFT'05 (les contacts de chaque équipe sont
indiqués en gras).
RNTI - 1 20
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
Équipe Exécution Pre´cision Rappel Fscore
1 3 0.883 0.858 0.870
1 1 0.880 0.854 0.867
2 1 0.865 0.856 0.860
3 1 0.868 0.777 0.820
2 2 0.880 0.739 0.803
1 2 0.826 0.764 0.794
4 1 0.688 0.849 0.760
5 1 0.748 0.755 0.751
6 1 0.666 0.812 0.732
4 2 0.801 0.666 0.728
4 3 0.590 0.773 0.669
5 2 0.926 0.422 0.580
7 1 0.413 0.880 0.562
7 2 0.372 0.900 0.526
8 3 0.361 0.783 0.494
9 3 0.337 0.915 0.492
8 2 0.365 0.736 0.488
9 1 0.532 0.449 0.487
9 2 0.404 0.580 0.476
5 3 0.941 0.267 0.416
10 3 0.755 0.207 0.325
10 2 0.778 0.191 0.306
10 1 0.764 0.153 0.255
8 1 0.147 0.400 0.215
11 1 0.226 0.145 0.177
Tab. 7  Résultats détaillés pour la tâche 1.
21 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
Équipe Exécution Pre´cision Rappel Fscore
1 1 0.909 0.861 0.884
1 3 0.911 0.858 0.884
2 1 0.846 0.857 0.852
3 1 0.866 0.780 0.821
2 2 0.884 0.739 0.805
1 2 0.827 0.775 0.800
6 1 0.808 0.781 0.794
5 1 0.753 0.756 0.755
4 1 0.653 0.859 0.742
4 2 0.802 0.673 0.732
5 3 0.794 0.672 0.728
4 3 0.571 0.774 0.657
5 2 0.925 0.430 0.587
9 1 0.508 0.623 0.560
7 2 0.409 0.883 0.559
7 3 0.472 0.665 0.552
9 2 0.520 0.547 0.533
7 1 0.369 0.904 0.524
8 3 0.384 0.810 0.521
8 1 0.374 0.795 0.508
8 2 0.350 0.852 0.496
9 3 0.520 0.461 0.489
10 2 0.790 0.190 0.307
10 1 0.777 0.153 0.256
11 1 0.226 0.145 0.177
Tab. 8  Résultats détaillés pour la tâche 2.
RNTI - 1 22
Bilan du Premier Défi Francophone de Fouille de Textes Azé et al.
Équipe Exécution Pre´cision Rappel Fscore
1 3 0.890 0.871 0.880
1 1 0.887 0.872 0.879
2 1 0.849 0.884 0.866
3 1 0.891 0.758 0.819
1 2 0.829 0.776 0.801
2 2 0.875 0.730 0.796
6 1 0.848 0.736 0.788
5 1 0.753 0.757 0.755
4 1 0.658 0.861 0.746
4 2 0.754 0.730 0.742
5 3 0.792 0.672 0.727
4 3 0.574 0.775 0.660
5 2 0.927 0.434 0.591
7 3 0.505 0.662 0.573
9 3 0.509 0.630 0.563
7 2 0.406 0.883 0.557
9 1 0.520 0.549 0.535
7 1 0.367 0.906 0.523
9 2 0.546 0.481 0.511
8 3 0.364 0.835 0.507
8 2 0.357 0.837 0.501
8 1 0.357 0.838 0.501
11 1 0.402 0.434 0.417
10 2 0.791 0.189 0.305
10 1 0.775 0.152 0.255
Tab. 9  Résultats détaillés pour la tâche 3.
23 RNTI - 1
Bilan du Premier Défi Francophone de Fouille de Textes
d1
i
n
e
s
p
L
t
m
r
o
J
i<>n
e<>
o<>s
t<>r
<>J
i<>s
p<>i
s<>t
<>m
L<>e
J<>o
m<>i
s<>p
r<>e
n<>i
r<>e<>
m<>i<>n
i<>s<>t
J<>o<>s
n<>i<>s
<>m<>i
<>J<>o
e<> <>m
i<>n<>i
e<> <>J
t<>r<>e
s<>p<>i
L<>e<>
s<>t<>r
o<>s<>p
p<>i<>n
Jospin
ministre
ministre<>Jospin
Le<>ministre<>
Le<>ministre<>Jospin<>
d2
e
p
i
r
m
L
n
t
é
u
J
s
m<>i
e<>
r<>e
p<>p
u<>p
t<>r
i<>s
s<>t
<>m
L<>e
J<>u
i<>n
<>p
i<>e
n<>i
p<>r
<>J
r<>
e<>m
e<>r
p<>é
r<>e<>
<>p<>r
u<>p<>p
m<>i<>n
e<>r<>
i<>s<>t
e<>m<>i
n<>i<>s
<>m<>i
m<>i<>e
r<>e<>m
i<>n<>i
e<> <>J
i<>e<>r
p<>p<>é
<>J<>u
r<> <>m
t<>r<>e
e<> <>p
L<>e<>
J<>u<>p
s<>t<>r
p<>r<>e
Juppé
premier
ministre
Le<>premier
premier<>ministre
ministre<>Juppé
Le<>premier<>ministre
premier<>ministre<>Juppé
d3
i
e
r
p
n
m
s
L
t
o
J
m<>i
i<>n
e<>
r<>e
o<>s
t<>r
i<>s
s<>t
<>m
L<>e
J<>o
<>p
i<>e
n<>i
p<>r
<>J
p<>i
r<>
e<>m
e<>r
s<>p
r<>e<>
<>p<>r
m<>i<>n
e<>r<>
i<>s<>t
J<>o<>s
e<>m<>i
n<>i<>s
<>m<>i
<>J<>o
m<>i<>e
r<>e<>m
i<>n<>i
e<> <>J
i<>e<>r
r<> <>m
t<>r<>e
s<>p<>i
e<> <>p
L<>e<>
s<>t<>r
p<>i<>n
o<>s<>p
p<>r<>e
Jospin
premier
ministre
Le<>premier
ministre<>Jospin
premier<>ministre
premier<>ministre<>Jospin
Le<>premier<>ministre
Tab. 10  Détail des n-grammes de l'exemple donné en section 2.3.1, page 6, le symbole
<> correspond au séparateur des éléments des n-grammes.
RNTI - 1 24
