© Revue MODULAD, 2005 - 63 - Numéro 33 
PLANIFICATION  
D’EXPERIENCES NUMERIQUES 
 
Astrid Jourdan 
Département de Mathématiques 
E.I.S.T.I. 
26, avenue des Lilas 
64062 Pau cedex 9 
jourdanastrid@yahoo.fr 
 
 
 
Résumé 
Malgré les progrès des outils informatiques, l’étude numérique de certains 
phénomènes physiques complexes reste très coûteuse en temps de calcul. Il est donc 
nécessaire de faire appel à des méthodes de planification d’expériences. 
Cet article présente et analyse les deux approches des expériences numériques les 
plus utilisées : la méthode classique des plans d’expériences et une méthode par 
krigeage. L’objectif est donner un aperçu de ce qui ce fait dans le domaine et de 
guider le lecteur vers une bibliographie détaillée. 
Mots clefs : plan d’expériences, surface de réponse, krigeage, hypercube latin 
 
Abstract 
Physical phenomena are now investigated by computer models which are often 
computationally expensive to run. Scientists have been in the forefront of 
developing designs and analysis of computer experiments.  
This paper gives an overview of two usual approaches : the standard response 
surface methodology and the Kriging method. The goal is to provide a general idea 
of this problem and guide towards a detailed bibliography. 
Key words : experimental designs, response surface, kriging, latin hypercube 
 
 
1. INTRODUCTION 
 
Pour des raisons de coût, de nombreux phénomènes scientifiques sont étudiés, non plus 
via l’expérimentation physique, mais à l’aide de modèles numériques. En effet, les 
progrès de ces dernières années concernant la modélisation mathématique de ces 
phénomènes ainsi que leur traitement numérique, permettent une étude extrêmement 
réaliste des phénomènes en question. A titre d’exemples, nous pouvons citer l’étude de 
la production de pétrole brut en industrie pétrolière (Dejean et Blanc 1999, Jourdan et 
Zabalza 2004), ou de la performance de circuits intégrés (Welch et al. 1988), ou encore 
la propagation d’un feu en milieu fermé (Santner et al. 2003). 
De façon schématique, un modèle numérique peut être considéré comme un 
programme. Le simulateur prend des variables en entrée et donne une ou plusieurs 
réponses en sortie. Une expérience numérique consiste alors à fixer un vecteur x de 
valeurs pour les variables d’entrée du simulateur puis à récolter la ou les réponses de 
celui-ci. On étudie ensuite le comportement de cette réponse en fonction des variations 
des variables d’entrée. 
Cependant, malgré les progrès des outils informatiques, le temps de calcul pour une 
expérience numérique reste très long. L’objectif est donc de remplacer la réponse du 
© Revue MODULAD, 2005 - 64 - Numéro 33 
simulateur par un modèle statistique simple représentant au mieux cette réponse. Nous 
sommes alors confrontés à deux questions : 
- Comment choisir les valeurs des variables d’entrée, x1,...,xn, de façon à récolter 
un maximum d’informations sur le comportement de la réponse en un minimum 
de simulations? 
- Quel modèle statistique est approprié aux réponses du simulateur y(x1),..., y(xn) ? 
Le modèle statistique est ensuite utilisé pour répondre à différentes questions telles :  
- prédire la réponse du simulateur en des points non testés par des simulations, 
- déterminer une valeur d’une variable d’entrée optimisant le phénomène étudié, 
- ajuster un modèle numérique à des données expérimentales, 
-  sélectionner les variables d’entrée influant réellement sur la réponse 
(« screening »), 
- etc. 
Ces problématiques sont proches de celles rencontrées en expérimentation physique 
mais certaines particularités des expériences numériques doivent être prises en compte 
pour le traitement. En effet : 
- les expériences sont déterministes, c’est-à-dire que deux simulations avec deux 
jeux de variables d’entrée identiques donnent la même réponse, 
- les variables d’entrée sont très nombreuses. En effet, aux variables liées au 
phénomène physique, viennent s’ajouter des variables dues au modèle numérique 
(par exemple une taille de maillage).  De plus aucune contrainte expérimentale 
ne permet de réduire la plage de variation des variables qui peut être alors très 
grande. C’est pourquoi une phase initiale de « screening » pour trier les 
variables influant réellement sur la réponse est primordiale. 
- la réponse du simulateur est souvent irrégulière du fait de la complexité des 
modèles mathématiques et cette irrégularité n’est pas masquée par une erreur 
expérimentale. 
Dans cet article nous allons présenter et commenter les deux approches des expériences 
numériques les plus couramment utilisées. La méthode classique des plans 
d’expériences et une approche plus actuelle par krigeage. Chacune est articulée autour 
des deux questions précédentes, à savoir le choix d’un modèle statistique et les plans 
d’expériences adaptés à ce modèle. L’objectif est de donner un aperçu de ce qui se fait 
dans le domaine et de guider le lecteur vers une bibliographie détaillée. 
 
2. APPROCHE CLASSIQUE DES PLANS D’EXPERIENCES 
 
2.1. Rappels sur le modèle polynomial 
 
Les surfaces de réponse sont couramment utilisées pour les expériences numériques, 
nous pouvons par exemple citer Iman et Helton (1988) ou Dejean et Blanc (1999). 
Cette technique bien connue consiste à utiliser un plan d’expérience classique tel qu’un 
plan Box-Behnken ou composite (voir par exemple Box et Draper (1987)) pour 
sélectionner les valeurs des variables d’entrée. Les réponses du simulateur en ces 
points permettent d’ajuster le modèle polynomial d’ordre deux, 
 
                                 ε+β+β+β+β= ∑∑∑
< i
2
ii
ji
jiij
i
ii0 xxxxy ,                                   (1) 
où ε~N(0,σ2) est un terme d’erreur. Le model (1) inclut certaines interactions xixj et 
termes quadratiques xi2 selon les choix spécifiés par l’utilisateur. 
© Revue MODULAD, 2005 - 65 - Numéro 33 
La principale différence entre expériences physique et numérique est la réponse 
déterministe du simulateur. Le terme d’erreur, ε, prend alors en compte un biais dû au 
modèle et non plus une erreur expérimentale. On peut cependant considérer ε comme 
une erreur de mesure. En effet, l’objet de l’étude n’est pas la réponse du simulateur 
mais le phénomène simulé qui peut être considéré comme la réponse du simulateur plus 
une erreur aléatoire due aux simplifications du modèle mathématique ou aux erreurs 
systématiques du schéma numérique.  
Considérons un plan d’expériences D={x1,…xn}. Les réponses du simulateur aux points 
du plan, YT=[y(x1),…,y(xn)], sont utilisées pour estimer les paramètres inconnus du 
modèle (1), 
                                                         β= ˆ)x(X)x(yˆ ,                                                (2) 
où YX)XX(ˆ T1T −=β  est l’estimateur des moindres carrés de β et X=[X(x1),…,X(xn)]T 
est la matrice du modèle. 
Les outils statistiques traditionnels (étude des résidus, sélection de variables, ...) sont 
utilisés pour déterminer le meilleur modèle statistique. 
REMARQUE : Les outils statistiques tels que la validation croisée ne semblent pas 
adaptés aux plans d’expériences classiques. En effet, supprimer un point à un plan, par 
exemple factoriel, peut détruire sa structure, et il n’a alors plus les propriétés requises 
pour donner une bonne estimation des paramètres du modèle polynomial. On préfèrera 
valider le modèle par des expériences supplémentaires. 
Les résultats, la méthodologie ainsi que les propriétés des plans d’expériences 
classiques sont exposés dans de multiples ouvrages tels que Benoist et al. (1995) ou 
Droesbeke et al. (1997). 
 
2.2. Quelques éléments sur les plans d’expériences usuels 
 
Pour ajuster le modèle (1) on utilise, soit des plans classiques tabulés tels que les plans 
composites, Box-Behnken, soit des plans optimaux construits numériquement. Il est 
cependant nécessaire de prendre quelques précautions dues au caractère déterministe 
des expériences : 
- pas de répétition d’expériences puisque celles-ci donnent la même réponse, 
- pas de randomisation des expériences car les conditions expérimentales 
n’influent pas sur la réponse. 
On peut envisager des plans en blocs si certaines des variables d’entrée discrètes 
prennent en compte un contexte ou différents scénarios (Jourdan et Zabalza 2004). 
Quel que soit le plan utilisé, il peut présenter des inconvénients importants pour les 
expériences numériques (Fig 3a) : 
- le nombre de niveaux testés pour chaque variable d’entrée est en général faible, 
- les points se situent souvent sur les bords du domaine expérimental. 
Cette répartition des points, qui est optimale pour ajuster un modèle polynomial, ne 
permet pas de capter des éventuelles irrégularités à l’intérieur du domaine 
expérimental. Or la complexité des modèles mathématiques laisse supposer que la 
réponse du simulateur sera irrégulière et que le modèle polynomial ne sera pas adapté à 
cette réponse. 
REMARQUE : Nous pouvons noter que dans un cas particulier, Carraro et al. (2005) ont 
établi un critère d’optimalité sélectionnant des points à l’intérieur du domaine 
expérimental (cas où les variables d’entrée sont incertaines mais connues avec une loi 
de probabilité) .  
 
© Revue MODULAD, 2005 - 66 - Numéro 33 
 
2.3. Pourquoi utilise-t-on encore des plans d’expériences classiques en 
simulation? 
 
Les plans d’expériences classiques ne sont pas remis en cause. Si on les utilise dans le 
domaine où ils sont applicables, ils sont de très bonne qualité. Mais si les hypothèses 
initiales ne sont pas vérifiées, comme cela semble être souvent le cas en simulation, 
leur utilisation n’est pas appropriée. Cependant, malgré la complexité des modèles 
numériques, un simple modèle polynomial peut être suffisant pour approcher la réponse 
du simulateur et ce pour diverses raisons. 
- La réponse du simulateur peut-être lisse même si le modèle mathématique 
décrivant le phénomène simulé est très complexe. Par exemple l’industrie 
pétrolière utilise des simulateurs d’écoulement pour prédire des productions 
cumulées d’huile. On constate que ce type de réponse est suffisamment régulière 
pour être résumée par une surface de réponse polynomiale (Dejean et Blanc 
1999). 
- Dans de nombreuses applications, le domaine d’étude peut être plus petit que le 
domaine de variation des variables d’entrée. Sur un domaine restreint il est alors 
possible de représenter la réponse du simulateur par un simple modèle 
polynomial même si globalement la réponse a un comportement plus complexe. 
Dans l’exemple précédent, les variables d’entrée du simulateur sont connues 
avec une plage d’incertitude due aux erreurs de mesure de ces variables lors de 
la phase de caractérisation du réservoir sur le terrain. Le domaine expérimental 
représente alors ces incertitudes sur les variables d’entrée, il est donc 
suffisamment petit pour justifier l’utilisation d’un modèle polynomial même 
pour des réponses complexes telles que la production de gaz ou d’eau (Jourdan 
et Zabalza 2004). 
- Le modèle polynomial peut être très utile pour faire une analyse de sensibilité. 
Iman et Helton (1988) ont montré au travers de nombreux exemples que, même 
si la surface de réponse n’est pas de qualité, elle permet tout de même de trier 
par ordre de contribution les variables d’entrée influant sur la réponse. 
Enfin une dernière raison pour laquelle la méthode des plans d’expériences classique 
est souvent utilisée vient du fait qu’elle est bien connue des ingénieurs et 
l’interprétation physique du modèle polynomial est simple, de plus elle disponible en 
logiciel. Cependant « l’automatisation » de cette méthode amène quelquefois à une 
mauvaise utilisation, le logiciel étant alors utilisé en boite noire sans s’assurer que 
l’approche est bien adaptée au problème à traiter. 
 
2.4. Avantages et inconvénients 
 
Les deux principaux avantages de cette méthode sont sa simplicité et le faible coût de 
simulation. En effet, si une surface de réponse polynomiale est adaptée à la réponse 
étudiée, alors cette approche est celle qui nécessite le moins de simulations. 
Cependant, il se peut que ce modèle statistique soit trop simple au regard de la 
complexité de la réponse du simulateur. Dans de nombreux cas il est nécessaire de faire 
appel à des modèles statistiques plus sophistiqués tels que le krigeage. De plus, comme 
nous l’avons signalé précédemment, les points des plans classiques sont positionnées 
sur les bords du domaine expérimental. Ils ne sont donc pas adaptés pour détecter les 
irrégularités à l’intérieur du domaine (Fig. 3a), ni d’ailleurs pour ajuster un modèle 
statistique autre que polynomial. D’autres types de plans sont alors envisagés. 
 
© Revue MODULAD, 2005 - 67 - Numéro 33 
3. APPROCHE PAR KRIGEAGE 
 
Des travaux plus récents (Sacks et al. 1989, Bates et al. 1996, Koehler et Owen 1996) 
suggèrent que les modèles polynomiaux ne sont pas adaptés aux expériences 
numériques. Par exemple une surface de réponse d’ordre deux n’est pas assez flexible 
pour modéliser une surface admettant plusieurs extrema. C’est pourquoi, un modèle 
spatial a été adapté du modèle de krigeage habituellement utilisé en géostatistiques. 
 
3.1. Description du modèle 
 
Dans cette approche, la réponse déterministe du simulateur est considérée comme la 
réalisation de la fonction aléatoire suivante 
                                                     )x()x(X)x(Y Γ+β= ,                                           (3) 
où x=(x1,…,xd) représente les variables d’entrée, X(x) est le vecteur des fonctions de 
base de la régression, β est le vecteur des paramètres inconnus du modèle et Γ est un 
processus gaussien d’espérance nulle et de fonction de covariance donnée par  
cov(Γ(x),Γ(w))=σ2R(x,w), 
où σ2 est la variance et R(x,w) est la fonction de corrélation, 
⎟⎠
⎞⎜⎝
⎛ −= ∑
=
d
1i
2
ii )wx(-θexpR(x,w) , ∀x∈R d, ∀w∈R d, 
avec θ le paramètre inconnu de corrélation du modèle. 
On remarque que la corrélation entre observations (réponses du simulateur) dépend 
- du paramètre de corrélation θ : la corrélation augmente quand θ diminue, 
- de la distance entre les observations : la corrélation diminue quand la distance 
augmente. 
On considère qu’au delà d’une certaine distance, deux observations ne sont plus 
corrélées. Le paramètre θ permet de déterminer cette distance de corrélation. 
REMARQUE : Différentes fonctions de corrélations autre que la gaussienne peuvent être 
considérées suivant la régularité de la réponse étudiée (exponentielle, sphérique, 
Matèrn,...). Chritensen (1990) ou Koehler et Owen (1996) présentent un descriptif de 
ces fonctions. Le paramètre θ peut aussi se présenter sous la forme d’un vecteur de 
façon à prendre en compte différentes distances de corrélation suivant les axes. Ces 
solutions peuvent sembler plus adaptées qu’un simple processus gaussien. Il est 
cependant à noter qu’une structure de covariance complexe nécessite plus de 
simulations pour estimer ses paramètres. 
Soit D={x1,…xn} un plan d’expériences. Les réponses du simulateur aux points du 
plan, YT=[y(x1),…,y(xn)], sont utilisées pour estimer les paramètres inconnus β, σ2 et θ 
du modèle (3). Supposons dans un premier temps que θ est connu et introduisons les 
notations suivantes : X=[X(x1),…,X(xn)]T est la matrice du plan, R=(R(xi,xj))i,j=1,…,n, 
est la matrice des corrélations entre les points du plan et r(x)=[R(x1,x),…,R(xn,x)]T est 
le vecteur des corrélations entre x et les points du plan. Le meilleur prédicteur linéaire 
sans biais (BLUP) est donné par (Sacks et al. 1989, Christensen 1990, Koehler et Owen 
1996) 
                                        ]ˆXY[R)x(rˆ)x(X)x(Yˆ 1T β−+β= − ,                                       (4) 
où YRX)XRX(ˆ 1T11T −−−=β  est l’estimateur des moindres carrés généralisé de β. Le 
prédicteur (4) minimise l’erreur quadratique moyenne 
                  ( ))x(K)XRX()x(K)x(rR)x(r1)]x(Yˆ)x(Y[E)x(MSE 11TT1T22 −−− +−σ=−= ,     (5) 
où T1T ]XR)x(r)x(X[)x(K −−= . La variance σ2 est estimée par 
© Revue MODULAD, 2005 - 68 - Numéro 33 
                                      )ˆXY(R)ˆXY(
n
1ˆ 1T2 β−β−=σ − .                                             (6) 
Dans les expressions précédentes (4), (5) et (6), le paramètre θ doit être spécifié. Sous 
des hypothèses gaussiennes, θ peut être estimé par maximum de vraisemblance (Mardia 
et Marshall 1984). Cependant l’optimisation numérique du critère de vraisemblance est 
très coûteuse en temps de calcul et aboutit bien souvent à un maximum local (Warnes 
et Ripley 1987). C’est pourquoi dans cet article, nous sélectionnons le paramètre θ qui 
minimise l’erreur quadratique moyenne intégrée empirique, 
                                           IMSE= ∑
=
σ
G
1k
2
k /)x(MSEG
1 ,                                            (7) 
où xk, k=1,…,G sont les points d’une grille G du domaine expérimental et MSE(xk) est 
donné par (5). La grille G peut être fine puisque l’IMSE (7) ne nécessite pas la réponse 
du simulateur aux points xk. De plus, pour une grille suffisamment grande, on observe 
que le θ optimal est indépendant de la taille de la grille. Cette technique présente 
l’avantage de ne pas faire appel aux hypothèses gaussiennes. 
 
3.2. Modèle d’interpolation 
 
Les réponses du simulateur sont observées sans erreur de mesure. Le processus 
gaussien Γ représente alors l’écart entre la régression linéaire présumée et la réponse 
du simulateur. Il permet ainsi d’interpoler les simulations, )x(Y)x(Yˆ ii = . Les deux 
termes dans l’expression du prédicteur (4) sont non corrélés. On remarque que le 
second terme, ]ˆXY[R)x(r 1T β−− , permet de combler l’écart entre la régression linéaire 
prédite, βˆ)x(X , et les réponses du simulateur aux points du plan (Fig. 1). 
Cette particularité du modèle est intéressante dans le cadre des expériences numériques 
puisque les réponses sont déterministes. De plus, l’interpolation permet d’appréhender 
d’éventuelles irrégularités de la surface de réponse. 
 
x
y(x)
Computer
responses(x)Yˆ
co
rr
ec
tio
n βX(x) ˆ
co
rr
ec
tio
n
 
FIGURE 1. Comportement de la surface de réponse issue d’un 
krigeage dans le cas d’une régression linéaire constante. 
 
REMARQUE : Sacks et al. (1989) et Jourdan (2002) suggèrent d’ajouter dans le modèle 
(3) un terme d’erreur appelé effet pépite en géostatistiques. Ce terme permet de 
s’affranchir de la contrainte d’interpolation et ainsi lisser la surface obtenue. Cet effet 
pépite peut prendre en compte une erreur de mesure dans le cas d’expériences 
physiques. 
 
3.3. Choix du modèle de régression 
 
En pratique la régression est bien souvent choisie constante (Welch et al. 1992). Elle 
représente alors la réponse moyenne des simulations (Fig. 1). Ce choix peut se justifier 
© Revue MODULAD, 2005 - 69 - Numéro 33 
par le fait que dans l’expression du prédicteur (4) le terme de correction, 
]ˆXY[R)x(r 1T β−− , permet d’interpoler les réponses quelle que soit la régression fixée. 
Cependant, on remarque que dans ce terme intervient essentiellement la structure de 
covariance du modèle au travers les quantités R et r(x). Dans le cas d’une régression 
constante les prédictions obtenues par le prédicteur (4) sont donc déterminées par : 
- la fonction de corrélation choisie a priori par l’utilisateur 
- l’estimation du paramètre θ qui, comme nous l’avons vu précédemment, n’est 
pas précise. 
La Figure 2 illustre le fait qu’une régression constante ne permet pas d’obtenir une 
surface de réponse robuste aux variations du paramètre θ. Il est donc nécessaire de 
choisir une régression plus consistante telle qu’une régression polynomiale (Sacks et 
al. 1989) ou une régression trigonométrique (Bates et al. 1996, Jourdan 2002). En 
effet, plus l’ajustement en moyenne sera précis est moins le terme de correction, 
]ˆXY[R)x(r 1T β−− , et au travers lui la structure de covariance, interviendra dans la 
prédiction. 
1
8
6
4
2
0
-
-
2
1
1
5
0
-
1
8
6
4
2
0
-
9
7
5
3
1
-
1
8
6
4
2
0
-
9
7
5
3
1
-
θ=1 θ=20 θ=50 
Trigonometric Regression 
Constant Regression 
 
FIGURE 2. Variations de la surface de réponse krigée en fonction de différentes valeurs 
deθ pour une régression constante (en bas) et une régression trigonométrique (en haut) 
 
3.4. Plans d’expériences pour krigeage 
 
Les plans d’expériences classiques (factoriels, composite, Box-Benhken,...) ne sont pas 
adaptés au krigeage. Dans ces plans, associés à des modèles polynomiaux du premier 
ou second degré, les points sont placés aux bords du domaine expérimental (Fig. 3a) ce 
qui ne permet pas : 
- de détecter les possibles irrégularités de la réponse du simulateur à l’intérieur du 
domaine expérimental 
- d’obtenir une bonne estimation du paramètre de corrélation θ puisque seules les 
grandes distances sont testées par le plan. 
Initialement les plans développés pour le krigeage étaient des plans optimaux pour 
certains critères statisques tels que : 
- l’erreur quadratique moyenne intégrée, IMSE, (Sacks et al. 1989) qui va sélectionner 
le plan qui minimise l’écart entre la réponse du simulateur et sa prédiction, 
© Revue MODULAD, 2005 - 70 - Numéro 33 
[ ][ ]∫ −σ d1,0 22D dx)x(Yˆ)x(YE1min . 
- le critère d’entropie (Schwery et Wynn 1987, Currin et al. 1991) qui permet de 
mesurer la quantité d’information fournie par la simulation, ( )Rdetmax
D
. 
On constate cependant que ce type de plans présente deux inconvénients majeurs. 
D’une part leur construction numérique coûte très cher puisqu’elle nécessite 
l'optimisation numérique d'une fonction à n×d paramètres (Sacks et al. 1989). D’autre 
part, les plans ne sont optimaux que pour une structure de covariance donnée et 
notamment pour un paramètre de corrélation θ fixé a priori. De plus, Koehler et Owen 
(1996) ont remarqué que les critères ci-dessus ont tendance a repoussé les points du 
plan sur les bords du domaine. 
REMARQUE : Les critères statistiques sont détaillés par exemple dans les articles de  
Bates et al. (1996) et Koehler et Owen (1996). 
Par la suite, les plans développés pour le krigeage ont été construits de façon à ce que 
leurs points représentent au mieux le domaine expérimental, d’où leur nom de « space 
filling designs ». On trouve essentiellement deux types de ces plans dans la 
bibliographie : 
- Les plans à marges uniformes tels que les hypercubes latins (Fig 3b) ou les 
tableaux orthogonaux (Mc Kay et al. 1979, Stein 1987, Owen 1992). En 
projection sur un axe ou une face du domaine la répartition des points est 
uniforme. 
- Les suites de faible discrépance telles que les lattices (réseaux) à un générateur 
(Niederreiter 1992, Fang et Wang 1994). La distribution empirique des points 
doit être proche de la distribution uniforme. 
Ces plans sont construits indépendamment du modèle notamment de la structure de 
covariance choisie. Ils ne répondent pas nécessairement aux critères statistiques cités 
ci-dessus.  
Les plans qui sont donc actuellement utilisés en krigeage sont ceux qui, à l’intérieur 
d’une classe de « space filling designs », optimisent un critère statistique. Cette 
technique permet ainsi : 
- de s’assurer une bonne répartition spatiale des points du plan, 
- d’avoir un plan optimal pour le modèle, 
- de diminuer le temps de calcul de l’optimisation puisque le champ 
d’investigation est réduit à la classe de plans. 
En pratique les plans les plus utilisés sont les hypercubes latins. Chaque arête du 
domaine expérimental est divisé en n segments de même longueur de façon à obtenir un 
maillage du domaine. Un hypercube latin sélectionne alors n points parmi les nd points 
de la grille de façon à ce que les n niveaux des variables d’entrée soient testés une fois 
par les simulations (Fig. 3b). Les hypercubes latins présentent beaucoup d’avantages. 
- Ils sont simples à construire. En effet, chaque colonne d’une hypercube est une 
permutation de {1,...,n} ou de n symboles quelconques. 
- Les points sont uniformément distribués sur chaque axe du domaine. 
La distribution uniforme sur chaque axe n’assure pas l’uniformité sur le domaine 
expérimental. Cependant, pour n fixé, ils existent (n !)d hypercubes latins possibles. Il 
est donc possible de sélectionner le plan optimisant un critère d’uniformité 
(discrépance,...) ou bien un critère statistique (IMSE, entropie,...). Park (1994) propose 
un algorithme d’échange pour déterminer un hypercube optimal (localement) pour un 
critère donné. Collombier et Jourdan (2001) ont montré qu’un hypercube optimal est 
© Revue MODULAD, 2005 - 71 - Numéro 33 
robuste aux variations du paramètre de corrélation θ qui, rapellons-le, n’est pas connu 
lors de la construction du plan. 
 
 
 
⎟⎟
⎟⎟
⎟⎟
⎟⎟
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜⎜
⎜⎜
⎜⎜
⎜⎜
⎜
⎝
⎛
α−
α
α−
α
++
+−
−+
−−
0
0
0
0
00
11
11
11
11
 
Fig. 3a  Fig.3b  
⎟⎟
⎟⎟
⎟⎟
⎟⎟
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜⎜
⎜⎜
⎜⎜
⎜⎜
⎜
⎝
⎛
79
48
17
86
55
24
93
62
31
 
FIGURE 3.Deux plans à 2 facteurs de taille 9 : (a) Plan composite standard (b) Hypercube latin 
 
3.5. Avantages et inconvénients 
 
L’approche par krigeage semble plus appropriée aux expériences numériques car elle 
permet d’appréhender des surfaces de réponse complexes. Cependant, le modèle 
statistique étant plus sophistiqué, il demande plus de simulations pour estimer ses 
paramètres, notamment pour obtenir une estimation précise de θ. La méthode 
statistique choisie dépendra donc de l’objectif recherché, par exemple pour une 
première étape de screening, il sera préférable d’utiliser des plans d’expériences  
classiques. Notons aussi que la taille d’un hypercube latin est très flexible et en 
pratique il n’est pas aisé de la déterminer a priori. 
Une approche séquentielle a été proposée par Jourdan (2003) afin de diminuer le coût 
du krigeage. L’idée est d’ajuster un modèle polynomial à l’aide d’un hypercube latin 
présentant des propriétés d’orthogonalité puis d’ajouter des simulations uniquement si 
la surface de réponse polynomiale demande à être corrigée par un krigeage.  
 
4. CONCLUSION 
 
La méthode par krigeage semble être l’alternative aux plans d’expériences la plus 
communément utilisée, notamment parce qu’elle propose un modèle statistique plus 
flexible. Elle n’est cependant pas la seule et d’autres types de modélisation peuvent 
être envisagés. Par exemple, les réseaux de neurones sont connus pour leur capacité à 
modéliser des surfaces irrégulières. La construction de plans d’expériences optimaux 
adaptés à ce type de modèle a fait l’objet des articles de S. Issanchou et J.P. Gauchi 
(2005) et Haines (1998). 
Le but de cet article n’est pas de préférer une méthode à une autre, chacune présentant 
les avantages et les inconvénients cités précédemment. L’objectif est de guider le 
lecteur afin qu’il choisisse la méthode la plus adaptée au problème à traiter.  
 
BIBLIOGRAPHIE 
 
Bates R.A., Buck R.J., Riccomagno E., Wynn H.P. (1996). Experimental Design and Observation 
for large Systems. J. R. Statist. Soc. B, 58, p. 77-94. 
Benoist D., Tourbier Y., Germain-Tourbier S. (1995). Plans d’expériences : construction et 
analyse. Tec et Doc, Paris. 
© Revue MODULAD, 2005 - 72 - Numéro 33 
Box G.E.P., Draper N.R. (1987). Empirical Model Building and Responses Surfaces. Wiley. New 
York. 
Carraro L., Corre B., Helbert C., Roustant O. (2005). Construction d’un critère d’optimalité pour la 
quantification d’incertitudes d’une sortie de simulateur. A paraître dans la Revue de Statistique 
Appliquée. 
Christensen R. (1990). Linear Models for Multivariate, time series, and spatial Data, Springer-
Verlag. 
Collombier D., Jourdan A. (2001). Régression trigonométrique et plans d'expériences pour 
expériences simulées. Revue de Statistique Appliquée, 49 (2), 5-26.  
Currin C.T., Mitchell M., Morris M., Ylvisaker D. (1991). Bayesian prediction of deterministic 
functions, with applications to the design ans the analysis of computer experiments. J. Amer. Statist. 
Assoc. 86, 953-963. 
Dejean J.-P., Blanc G. (1999). Managing Uncertainties on Production Prediction Using Integrated 
Statistical Methods. Paper SPE 56696 presented at the 1999 SPE Annual Technical Conference and 
Exhibition, Houston, U.S.A., October 3-6. 
Droesbeke J.-J, Fine J., Saporta G. (1997) (Editeurs scientifiques). Plans d’expériences, application 
à l’entreprise. Technip, Paris. 
Fang K.T., Wang Y. (1993). Number-theoretic methods in statistics. Chapman and Hall, London. 
Haines L. M. (1998). Optimal Design for Neural Networks. New Developments and Applications in 
Experimental Design. Editors: N. Flournoy, W.F. Rosenberger and W.K. Wong, Institute of 
Mathematical Statistics, Hayward, CA., 152-162. 
Iman R., Helton J.C. (1988). An Investigation of Uncertainty and Sensitivity Analysis Techniques 
for Computer Models. Risk Analysis, 8, 71-90. 
Issanchou S., Gauchi J.-P. (2005). Computer-aided Optimal Designs for improving the neural 
network generalization. Soumis à Technometrics. 
Johnson M.E., Moore L.M., Ylvisaker D. (1990). Minimax and maximin distance designs. J. of 
Statist. Planning and Inference  26, 131-148. 
Jourdan A. (2002). Approches statistiques des expériences simulées. Revue de Statistique 
Appliquée, 50 (1), 49-64. 
Jourdan A. (2003). Approche séquentielle du krigeage. Présentation séminaire « plans 
d’expériences numériques ». Ecole Nationale des Mines - Saint Etienne 
Jourdan A., Zabalza-Mezghani I. (2004). Response surface designs for scenario management and 
uncertainty quantification in reservoir production. Mathematical Geology 36 (8), 965-985. 
Koehler J.R., Owen A.B. (1996) Computer Experiments. In Ghosh, S., Rao, C.R., (Eds.), Handbook 
of Statistics, 13 : Designs and Analysis of Experiments, North- Holland, Amsterdam, p. 261-308.  
Mardia K.V., Marshall R.J. (1984). Maximum Likelihood Estimation of Models for Residual 
Covariance in Spatial Regression. Biometrika, 71, 135-146. 
McKay M.D., Beckman R.J., Conover W.J. (1979). Comparison of three methods for selecting 
values input variables in the analysis of output from a computer code. Technometrics,  21, 239-245. 
Niederreiter H. (1992). Random number generation and quasi-Monte Carlo methods. CBMS-NSF, 
SIAM, Philadelphia. Technical Report n°4, Illinois, Dept. of Statistics. 
Owen A.B. (1992). Orthogonal arrays for computer experiments, integration and visualization. 
Statist. Sinica 2, 439-452. 
© Revue MODULAD, 2005 - 73 - Numéro 33 
Park J.S. (1994). Optimal Latin hypercube designs for computer experiments. J. of Statist. Planning 
and Inference 39, 95-111. 
Sacks J., Welch W.J., Mitchell T.J., Wynn H.P. (1989) Design and analysis of Computer 
Experiments. Statistical Science, 4, p. 409-435.  
Sacks J., Schiller S.B., Welch W.J. (1989). Designs for Computer Experiments. Technometrics, 31, 
41-47. 
Santner T.J., Williams B.J., Notz W. I. (2003) The Design and Analysis of Computer Experiments, 
Springer Series in Statistics, Springer-Verlag New York. 
Schwery M.C., Wynn H.P. (1987). Maximum entropy sampling. J. of Appl. Statist. 14, 165-170. 
Stein M. (1987). Large sample properties of simulations using latin hypercube sampling. 
Technometrics, 29, p. 143-151 
Warnes J.J., Ripley B.D. (1987). Problem with Likelihood Estimation of Covariance Functions of 
Spatial Gaussian Processes. Biometrika, 74, 640-642. 
Welch W.J, Yu T.K., Kang S.M., Sacks J. (1988). Computer experiments for quality control by 
parameter design. Rapport technique 4, université de l’Illinois, dept. de statistics. 
