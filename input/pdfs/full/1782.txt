Se´lection de variables et agre´gation d’opinions
Gae¨lle Legrand et Nicolas Nicoloyannis∗
∗Laboratoire ERIC
Universite´ Lumie`re Lyon 2
Batiment L
5 av. Pierre Mende`s-France
69 676 BRON cedex FRANCE
glegrand@eric.univ-lyon2.fr ; nicolas.nicoloyannis@univ-lyon2.fr
Re´sume´. La taille des bases de donne´es e´tant de plus en plus importante,
le processus de se´lection de variables devient essentiel. Nous proposons une
me´thode de se´lection, pour les variables qualitatives, base´e sur l’agre´gation
d’opinion. Le re´sultat, sous forme d’un pre´ordre de variables, est fourni
par l’agre´gation des re´sultats obtenus par plusieurs me´thodes myopes de
se´lection de variables.
1 Introduction
La taille des bases de donne´es e´tant de plus en plus importante, l’ame´lioration de
la qualite´ de repre´sentation des donne´es est devenue un proble`me majeur de l’extrac-
tion des connaissances a` partir des donne´es. L’une des difficulte´s principales lie´e a` la
repre´sentation des donne´es est la dimension des donne´es.
Le proble`me de la dimension des donne´es concerne le nombre de variables descrip-
tives caracte´risant chacun des individus. Parmi ces variables, certaines peuvent eˆtre non
pertinentes, inutiles et/ou redondantes. Donc, si l’on de´sire extraire de l’information
utile et compre´hensible a` partir de nos donne´es, il convient en premier lieu de retirer
les parties non pertinentes.
La se´lection de variables permet de re´soudre ce proble`me. C’est un processus choi-
sissant un sous-ensemble optimal de variables selon un crite`re particulier. Il permet
l’e´limination de variables inutiles, non pertinentes et redondantes ainsi que l’e´limination
du bruit ge´ne´re´ par certaines variables. Le processus d’apprentissage est acce´le´re´ et la
pre´cision pre´dictive des algorithmes d’induction peut eˆtre ame´liore´e.
Il existe deux familles d’algorithmes de se´lection de variables : les me´thodes ”enveloppe”
[John et al., 1994] et les me´thodes ”filtre”[Kira et Rendell, 1992a]. La diffe´rence fonda-
mentale entre ces deux familles re´side dans le fait que la premie`re est lie´e a` l’algorithme
d’induction utilise´e alors que la seconde est totalement inde´pendante.
1.1 Approches Enveloppe
Les me´thodes de type enveloppe prennent en compte l’influence du sous-ensemble
de variables se´lectionne´ sur les performances de l’algorithme d’induction. Elles utilisent
l’algorithme d’apprentissage comme fonction d’e´valuation pour tester les diffe´rents
sous-ensembles de variables ge´ne´re´s. Cependant, leur couˆt calculatoire est bien sou-
vent trop important.
Se´lection de variables et agre´gation d’opinions
1.2 Approches Filtre
Les approches filtre sont de 5 types :
– Les me´thodes exhaustives testent tous les sous-ensembles possibles de P va-
riables parmi M variables existantes. Nous pouvons citer les algorithmes MDLM
[Sheinvald et al., 1990] ou FOCUS [Almuallim et G., 1992]. La complexite´ de FO-
CUS est de l’ordre de O(NM ), avec N le nombre d’individus. Ces algorithmes
sont impossibles a` appliquer du fait de leur couˆt calculatoire trop e´leve´.
– Les me´thodes heuristiques sont tre`s nombreuses. La me´thode la plus connue est
RELIEF [Kira et Rendell, 1992b] dont la complexite´ est de l’ordre de O(INM)
avec I le nombre d’ite´rations effectue´es et fixe´es par l’utilisateur. Il existe e´galement
des me´thodes du type Branch and bound telles que ABB [Liu et al., 1998]. Ces
me´thodes requie`rent plusieurs acce`s a` la base de donne´es.
– Les me´thodes probabilistes sont repre´sente´es par LVF [Liu et Setiono, 1996]. Sa
complexite´ est de l’ordre de O(INM) avec I le nombre d’ite´rations effectue´es
et fixe´es par l’utilisateur. Cependant, du fait de sa caracte´ristique probabiliste,
le nombre de variables se´lectionne´es tend vers la moitie´ du nombre de variables
initiales. Comme les me´thodes pre´ce´dentes, ces me´thodes requie`rent plusieurs
acce`s a` la base de donne´es.
– Les me´thodes de se´lection en un seul parcours de base sont des processus ite´ratifs
qui, comme leur nom l’indique, ne ne´cessitent qu’un seul scan de la base e´tudie´e.
Le processus de se´lection s’effectue de la manie`re suivante : lors de la premie`re
e´tape, la variable la plus corre´le´e avec la variable endoge`ne est se´lectionne´e. Lors
de la deuxie`me e´tape, la variable qui est la plus partiellement corre´le´e avec la va-
riable endoge`ne est se´lectionne´e et ainsi de suite. Afin de n’avoir qu’un seul scan
de base, les mesures rapides de corre´lation sont utilise´es (coefficient de corre´lation
line´aire de Pearson, coefficient de corre´lation de rangs de Kendall, etc). Ce type de
me´thode est repre´sente´ par MIFS [Battiti, 1994], CFS [Hall, 2000], et la me´thode
propose´e par Lallich et Rakotomalala, [Lallich et Rakotomalala, 2000]. Ces me´thodes,
qui sont les plus rapides et qui sont relativement efficaces, paraissent les plus
inte´ressantes.
– Les me´thodes myopes ou pas a` pas utilisent des crite`res de se´lection myopes pour
se´lectionner les variables. Ces me´thodes ne prennent pas en compte l’interaction
entre variables et permettent de classer les variables en fonction de leur pouvoir
discriminant. Ce type de me´thodes est efficace et tre`s rapide en particulier sur
des proble`mes comportant a` la fois beaucoup de variables et d’individus. La
complexite´ de ce type de me´thodes est de l’ordre de O(NlogN).
2 Point de de´part
Nous sommes partis du constat suivant : les me´thodes pas a` pas utilisant des crite`res
de se´lection myopes tels que l’entropie de Shannon sont rapides, peu couˆteuses et
pre´sentent des re´sultats plutoˆt encourageants. Il existe 4 cate´gories de crite`res permet-
tant de mesurer diffe´rentes caracte´ristiques des variables :
– Les crite`res d’information : c’est la quantite´ d’information apporte´e par une va-
RNTI - C - 1
Legrand et al.
riable sur la variable a` pre´dire. La variable, ayant le gain d’information le plus
e´leve´, sera pre´fe´re´e aux autres variables : l’entropie de Shannon [Shannon, 1948],
le ratio du gain [Quinlan, 1986], le gain normalise´ [Jun et al., 1997].
– Les crite`res de distance : ces mesures s’inte´ressent au pouvoir discriminant d’une
variable. Elles e´valuent la se´parabilite´ des classes en se basant sur les distribu-
tions de probabilite´s des classes : la distance de Mantaras [De Mantaras, 1991],
le crite`re de Gini [Breiman et al., 1984].
– Les crite`res d’inde´pendance regroupent toutes les mesures de corre´lation ou d’as-
sociation. Elles permettent de calculer le degre´ avec lequel une variable exoge`ne
est associe´e a` une variable endoge`ne. Le calcul de l’e´cart a` l’inde´pendance de deux
variables d’un tableau de contingence est effectue´ : le chi2, le crite`re de Tschuprow
[Hart, 1984] et [Mingers, 1987], le coefficient de Cramer.
– Les crite`res de consistance : ils recherchent l’ensemble de variables le plus petit qui
satisfait un pourcentage d’inconsistance minimum de´fini par l’utilisateur. Deux
objets sont inconsistants si leurs modalite´s sont identiques et s’ils appartiennent
a` deux classes diffe´rentes. Ces mesures permettent de de´tecter les variables re-
dondantes : le τ de Zhou [Zhou et Dillon, 1991].
Cependant, l’utilisation d’une me´thode myope ge´ne`re trois proble`mes :
1. Le choix du crite`re est de´licat : quel crite`re est le plus efficace?
2. La forme du re´sultat (une liste de variables trie´es) ne nous permet pas de de´terminer
le sous-ensemble optimal de variables.
3. Ce type de me´thodes ne prend pas en compte l’interaction existante entre les
variables exoge`nes. Nous verrons, lors des expe´rimentations, que cela n’a pas
d’impact sur la qualite´ des re´sultats.
La me´thode que nous proposons permet de re´soudre les deux premiers proble`mes sou-
leve´s de la manie`re suivante :
1. Il n’existe pas de crite`re meilleur ou plus efficace que les autres. Chaque crite`re
met en avant certaines qualite´s spe´cifiques a` chaque variable. Il semble inte´ressant
d’obtenir un re´sultat tenant compte de l’avis de plusieurs crite`res diffe´rents. Afin
d’obtenir ce type de re´sultats, nous utilisons une me´thode d’agre´gation d’opinions.
2. L’obtention d’une liste trie´e de variables limite l’inte´reˆt de la se´lection de va-
riables. En effet, comment peut on de´terminer la taille optimale du sous-ensemble?
Lorsque l’on est en pre´sence d’une liste trie´e de variables, l’une des me´thodes qui
semble efficace pour obtenir un sous-ensemble optimal de variables est d’utiliser
une approche enveloppe qui ajoute ou oˆte ite´rativement les e´le´ments de la liste
trie´e. A chaque ite´ration, la me´thode d’apprentissage est applique´e pour tester
si l’ajout ou la suppression d’une variable entraˆıne une ame´lioration du taux
d’apprentissage. Toutefois, ce processus est bien trop couˆteux et long pour eˆtre
applique´. Pour cette raison, nous parame´trons la me´thode d’agre´gation utilise´e
pour qu’elle nous fournisse non pas un ordre sur les variables mais un pre´ordre to-
tal. Aussi, nous n’ajouterons pas les variables une par une mais par sous-ensemble
de variables.
RNTI - C - 1
Se´lection de variables et agre´gation d’opinions
3 Pre´sentation de la me´thode
La me´thode de se´lection propose´e est une me´thode hybride a` l’intersection des ap-
proches filtre et enveloppe. Elle est de type Forward Selection et agre`ge les classements
des variables obtenus a` l’aide de plusieurs crite`res de se´lection myopes. Elle ne traite que
les variables qualitatives. Lorsque des variables quantitatives sont pre´sentes, elles sont
discre´tise´es de manie`re supervise´e a` l’aide de la me´thode Fusinter [Zighed et al., 1996].
Le re´sultat fourni est une liste trie´e de sous-ensembles disjoints de variables. Cette
me´thode peut se de´composer en 3 e´tapes :
– Le calcul et la discre´tisation des diffe´rents crite`res pour chaque variable,
– L’application de la me´thode d’agre´gation d’opinions sur les re´sultats obtenus a`
l’e´tape pre´ce´dente,
– La recherche du sous-ensemble optimal.
3.1 Calcul et discre´tisation des crite`res
Nous avons se´lectionne´ un ensemble de 10 crite`res myopes de se´lection : l’entropie
de Shannon, le gain d’information, le ratio du gain, le gain normalise´, la distance de
Mantaras, le crite`re de Gini, le chi2, le crite`re de Tschuprow, le coefficient de Cramer
et le τ de Zhou.
Les calculs de chaque crite`re pour la totalite´ des variables s’effectuent en pa-
ralle`le. Le re´sultat obtenu est un ensemble constitue´ de 10 listes ordonne´es dans l’ordre
de´croissant de l’importance des variables.
Deux variables pouvant eˆtre aussi pertinentes l’une que l’autre vis-a`-vis de la variable
endoge`ne meˆme si elles n’apportent pas le meˆme type d’information, nous introduisons
la notion d’e´quivalence de variables. Afin de de´finir cette notion, nous conside´rons un
proble`me d’apprentissage caracte´rise´ par un ensemble d’individus O = {o1,...,oj ,...,on}
de´crits par un ensemble de variables X = {x1,...,xi,...,xp} nomme´ l’ensemble initial des
variables.
Soit CR = {cr1,...,crk,...,cr10} l’ensemble des 10 crite`res myopes de se´lection, choisis
avec crk = {crk1,...,crki,...,crkp}, l’ensemble des valeurs du crite`re k pour les p variables
de X.
Les valeurs crki de chaque crite`re sont normalise´es a` l’aide de la transformation
suivante : pour une variable xi ∈ X et un crite`re crk ∈ CR, la valeur normalise´e du
crite`re est :
crki,N =
crki−Min({crk})
Max({crk})−Min({crk}) .
Apre`s leur normalisation, ces valeurs sont discre´tise´es en de´ciles. La discre´tisation
permet d’affecter a` chaque variable xi un rang pour chaque crite`re crk de la manie`re
suivante :
– Pour les crite`res qui doivent eˆtre minimise´s :
Si crki,N ∈ [0; 0.1[ alors
Rki = 1
Si crki,N ∈ [0.1; 0.2[ alors
Rki = 2
...
RNTI - C - 1
Legrand et al.
Si crki,N ∈ [0.9; 1] alors
Rki = 10
– Pour les crite`res qui doivent eˆtre maximise´s :
Si crki,N ∈ [0; 0.1[ alors
Rki = 10
Si crki,N ∈ [0.1; 0.2[ alors
Rki = 9
...
Si crki,N ∈ [0.9; 1] alors
Rki = 1
Rki est le rang affecte´ a` la variable xi ∈ X pour le crite`re crk ∈ CR. La variable la
plus pertinente est celle posse´dant le rang le plus faible.
Ainsi la notion d’e´quivalence se de´finit de la manie`re suivante : deux variables sont
e´quivalentes du point de vue d’un crite`re particulier si et seulement si pour ce crite`re,
elles ont le meˆme rang : xi ⇀↽ xj ⇔ Rki = Rkj .
3.2 Agre´gation des re´sultats des crite`res
Pour toutes me´thodes d’agre´gation [Marcotorchino, 1981], il convient de de´finir
l’ensemble des juges et l’ensemble des individus. Dans notre cas, les individus sont les
variables et les juges sont l’ensemble des crite`res. Nous utilisons la me´thode d’agre´gation
d’opinions de´veloppe´e dans [Nicoloyannis et al., 1999] et [Nicoloyannis et al., 1998] et
base´e sur [Marcotorchino et Michaud, 1981] et [Michaud, 1982]. Nous n’allons pas de´crire
en de´tails cette technique mais en pre´senter le principe sous-jacent.
Pour tout couple d’individus (xi,xj), chaque juge e´met un avis Ak(i,j). Ak, l’opi-
nion du juge k est une application de X ×X dans {Pref,NPref,EQ}. Ainsi,
Ak(i,j) = Pref ⇔ le juge k pre´fe`re xi a xj ⇔ Rki < Rkj
Ak(i,j) = NPref ⇔ le juge k pre´fe`re xj a xi ⇔ Rki > Rkj
Ak(i,j) = EQ⇔le juge k conside`re xi et xj comme e´quivalentes ⇔ Rki = Rkj
Le re´sultat que nous de´sirons obtenir est une opinionOP dite opinion de type pre´fe´rence
large et qui engendre une relation de pre´ordre total sur X. OP est une application de
X ×X dans {Pref,NPref,EQ}.
De´finition 1 : Le degre´ d’accord ρij(OP,Ak) entre les avis OP (i,j) et Ak(i,j) est
de´fini comme dans le tableau 1.
De´finition 2 : Le degre´ d’accord entre les opinions OP et Ak est DA(OP,Ak) =∑
(xi,xj)∈X ρij(OP,Ak).
De´finition 3 : Le degre´ d’accord entre l’opinion OP et l’opinion de tous les juges
est DA(OP ) =
∑10
k=1DA(OP,Ak).
Notre proble`me consiste donc a` construire une opinion OP qui engendre un pre´ordre
total sur X et qui maximise DA(OP ). Pour la maximisation, la me´thode du recuit
simule´ [Kirkpatrick et al., 1983] est utilise´e. Ce proble`me de programmation line´aire
RNTI - C - 1
Se´lection de variables et agre´gation d’opinions
OP / Ak Pref NPref EQ
Pref 1 0 1/2
NPref 0 1 1/2
EQ 1/2 1/2 1
Tab. 1 – Degre´s d’accord ρij.
NP-difficile peut eˆtre re´solu par diffe´rentes me´ta-heuristiques ; nous avons se´lectionne´
la me´thode du recuit-simule´ car elle est facile a` mettre en oeuvre et conduit a` des temps
de calcul de quelques secondes.
Apre`s l’application de cette technique d’agre´gation, nous obtenons une liste ordonne´e
L = {l1,...,lm,...,lM} de sous-ensembles disjoints de variables.
3.3 De´couverte du sous-ensemble optimal de variables
Jusqu’a` pre´sent, nous e´tions dans une optique d’approche filtre. Lors de cette e´tape,
nous sommes dans une optique d’approche enveloppe. L’avantage d’utiliser une ap-
proche enveloppe est lie´ au fait que l’influence du sous-ensemble de variables sur les
performances de l’algorithme d’apprentissage est prise en compte. Le processus de
de´termination du sous-ensemble optimal s’effectue de la manie`re suivante : a` la mieme
ite´ration, le sous ensemble de variable lm ∈ L est ajoute´ au sous-ensemble optimal de
variables. Le crite`re d’arreˆt est double : il y a arreˆt du processus soit lorsque le taux d’er-
reur est constant sur deux ite´rations soit lorsque que l’on assiste a` une augmentation
du taux d’erreur.
Le processus est le suivant :
X∗ = {}
Pour m = 1 a` M faire
X∗ = X∗ ∪ lm
Application de l’algorithme d’apprentissage
Si taux d’erreur a` l’ite´ration m ≥ taux d’erreur a` l’ite´ration m− 1 Alors
Arreˆt
X∗ = X∗ − lm
Fin Pour
Le re´sultat sera le sous-ensemble optimal X∗.
4 Expe´rimentations
Les variables quantitatives ont e´te´ discre´tise´es avec la me´thode Fusinter. La se´lection
de variables s’est effectue´e sur 30% des individus tout en gardant la re´partition initiale
des classes. Les tests avec MIFS et ReliefF ont e´galement e´te´ effectue´s sur ces meˆmes
30%. Les 70% restant sont utilise´s pour la phase d’apprentissage. Pour cela, nous avons
choisi une 10-cross validation et les algorithmes d’apprentissage sont : l’arbre d’induc-
tion ID3 et les baye´siens na¨ıfs. Les tests avant se´lection ont e´galement e´te´ effectue´s sur
RNTI - C - 1
Legrand et al.
ces meˆmes 70% de la base. Les bases de test utilise´es sont issues de la collection de
l’UCI Irvine, (Blake, 98).
Les tableaux 2 et 3 montrent les taux d’erreur et les e´carts-type associe´s obtenus
avant et apre`s la se´lection de variables respectivement avec ID3 et les baye´siens na¨ıfs.
Les re´sultats obtenus avec ID3 sont inte´ressants. En effet, excepte´ pour la base Iono,
on assiste a` une diminution du taux d’erreur et/ou a` une stabilisation des re´sultats (di-
minution de l’e´cart-type). Pour les baye´siens na¨ıfs, les re´sultats avant et apre`s se´lection
sont sensiblement identiques. Cependant, le nombre de variables diminue alors que les
taux d’erreur ne connaissent pas d’augmentation.
Bases Avant Se´lection Apre`s Se´lection
Taux d’erreur Ecart Type Taux d’erreur Ecart Type
Tic Tac Toe 28,44 7,53 26,78 2,65
Breast 5,9 2,64 4,47 2,84
CRX 14,46 5,44 15,7 3,1
Diabetes 24,3 3,97 24,3 4,55
Pima 25,24 5,76 26,73 5,24
Vehicle 30,59 5,54 33,1 5,32
Austra 17,16 6,21 17,56 6,87
Cleve 27,1 9,18 21,9 8,67
Heart 31,05 6,42 26,32 11,04
Iono 10,92 4,37 11,75 4,65
German 29,57 6,13 29,86 4,76
Tab. 2 – Evaluation de notre me´thode de se´lection avec ID3.
Bases Avant Se´lection Apre`s Se´lection
Taux d’erreur Ecart Type Taux d’erreur Ecart Type
Tic Tac Toe 28,72 4,1 28,15 6,5
Breast 2,86 1,87 2,65 2,05
CRX 14,65 7,24 15,3 3,75
Diabetes 24,67 2,96 24,48 7,87
Pima 21,52 4,54 22,84 5,95
Vehicle 33,61 5,27 33,95 4,18
Austra 14,65 3,42 15,27 3,61
Cleve 20,04 5,31 17,77 6,14
Heart 16,84 5,16 18,95 7,88
Iono 6,9 5,21 10,58 5,03
German 23,86 2,93 24,43 7,63
Tab. 3 – Evaluation de notre me´thode de se´lection avec les Baye´siens Na¨ıfs.
Les tableaux 4 et 5 indiquent le nombre de variables se´lectionne´es respectivement
avec ID3 et les baye´siens na¨ıfs. A l’exception de la base Tic Tac Toe, le nombre de
RNTI - C - 1
Se´lection de variables et agre´gation d’opinions
variables se´lectionne´es par notre me´thode est infe´rieur a` celui se´lectionne´ par ReliefF
et/ou MIFS. Pour les baye´siens na¨ıfs, nos re´sultats se situent entre ceux de ReliefF et
ceux de MIFS.
Bases Sans Se´lection Notre me´thode ReliefF MIFS
Tic Tac Toe 9 7 5 3
Breast 9 3 6 9
CRX 15 3 2 7
Diabetes 8 2 4 5
Pima 8 2 7 4
Vehicle 18 14 18 6
Austra 14 1 2 13
Cleve 13 7 6 8
Heart 13 2 2 13
Iono 34 2 25 8
German 20 5 14 3
Tab. 4 – Nombre de variables se´lectionne´es avec ID3.
Bases Sans Se´lection Notre me´thode ReliefF MIFS
Tic Tac Toe 9 7 5 3
Breast 9 7 6 9
CRX 15 5 2 7
Diabetes 8 6 4 5
Pima 8 5 7 4
Vehicle 18 12 18 6
Austra 14 2 2 13
Cleve 13 5 6 8
Heart 13 8 2 13
Iono 34 26 25 8
German 20 9 14 3
Tab. 5 – Nombre de variables se´lectionne´es avec les Baye´siens Na¨ıfs.
Le tableau 6 montre le nombre d’ite´rations effectue´es lors de la phase enveloppe
de notre me´thode. Le nombre maximal d’ite´rations est de l’ordre de 9 (pour la base
Vehicle). Le nombre d’appels a` l’algorithme d’apprentissage est donc bien plus faible
que pour les me´thodes enveloppe pures.
Les tables 7 et 8 compare´es aux tables 2 et 3 montrent que notre me´thode est
comparable du point de vue des taux d’erreur a` MIFS et ReliefF.
RNTI - C - 1
Legrand et al.
Bases ID3 BN
Tic Tac Toe 4 4
Breast 3 5
CRX 2 3
Diabetes 3 4
Pima 3 4
Vehicle 9 7
Austra 2 3
Cleve 5 4
Heart 2 4
Iono 3 6
German 5 7
Tab. 6 – Nombre d’ite´rations ne´cessaires a` notre me´thode.
5 Conclusion
Dans cet article, nous avons pre´sente´ une me´thode de se´lection de variables pour
variables qualitatives, base´e sur l’agre´gation d’opinion.
C’est une me´thode hybride entre approche filtre et enveloppe qui posse`de les avan-
tages de chaque approche et qui permet de re´duire leurs inconve´nients :
– L’influence des variables se´lectionne´es sur l’algorithme d’apprentissage utilise´ est
pris en compte. Ainsi, les variables se´lectionne´es sont diffe´rentes suivant l’algo-
rithme utilise´.
– Les temps de calcul sont largement infe´rieurs a` ceux des me´thodes enveloppe
pures graˆce a` l’utilisation du pre´ordre et a` l’obtention d’une liste trie´e de sous-
ensembles de variables.
Du point de vue du nombre de variables se´lectionne´es, les re´sultats que nous obtenons
sont dans la plupart des cas comparables voire meilleurs a` ceux obtenus par ReliefF et
MIFS. Du point de vue de la qualite´ d’apprentissage, nous assistons a` une diminution
des taux d’erreur apre`s la se´lection. Les comparaisons en terme de taux d’erreur entre
notre me´thode et ReliefF ou MIFS sont probants.
Nous envisageons d’ame´liorer la me´thode propose´e suivant divers aspects. La me´thode
de discre´tisation utilise´e pour les valeurs des crite`res doit eˆtre plus approprie´e. Nous
voudrions, e´galement, que le re´sultat de la me´thode d’agre´gation ne soit plus une liste
de sous-ensembles de variables, mais le sous-ensemble optimal de variables.
Il serait e´galement inte´ressant de ne plus travailler uniquement sur des variables quali-
tatives mais sur des variables de tous types (nume´riques, qualitatives, quantitatives,...).
RNTI - C - 1
Se´lection de variables et agre´gation d’opinions
Bases ReliefF MIFS
Taux d’erreur Ecart type Taux d’erreur Ecart type
Tic Tac Toe 30,51 5,9 30,81 7,11
Breast 5,29 3,16 5,9 2,64
CRX 17,54 5,88 16,12 6,7
Diabetes 25,78 4,05 23,19 4,9
Pima 25,05 7,69 24,87 4,83
Vehicle 42,25 6,52 40,62 7,39
Austra 15,31 5,23 17,17 4,12
Cleve 40,54 7,77 24,68 10,27
Heart 27,38 9,06 28,42 9,76
Iono 11,78 3,94 15,75 8,71
German 30,14 6,01 27,43 5,06
Tab. 7 – MIFS et ReliefF avec ID3.
Bases ReliefF MIFS
Taux d’erreur Ecart type Taux d’erreur Ecart type
Tic Tac Toe 27,97 4,19 28,87 5,42
Breast 3,45 2,56 2,86 1,87
CRX 16,53 2,8 14,66 5,7
Diabetes 25,81 6,13 21,32 4,43
Pima 25,04 3,41 21,33 4,3
Vehicle 45,82 8,78 39,85 8,01
Austra 15,28 5,15 14,28 3,08
Cleve 40,67 4,33 20,52 11,34
Heart 21,05 10,53 17,89 10,04
Iono 9,32 6,22 5,22 4,4
German 30,71 4,96 26,29 3,63
Tab. 8 – MIFS et ReliefF avec les Baye´siens na¨ıfs.
RNTI - C - 1
Legrand et al.
Re´fe´rences
[Almuallim et G., 1992] H. Almuallim et Dietterich T. G. Efficient algorithms for iden-
tifying relevant features. Technical Report 92-30-03, 1992.
[Battiti, 1994] R. Battiti. Using mutual information for selecting features in supervised
neural net learning. IEEE Trans. on Neural Networks, 5:537–550, July 1994.
[Breiman et al., 1984] L. Breiman, J. H. Friedman, R. A. Olshen, et C. J. Stone. Clas-
sification and Regression trees, The Wadsworth Statistics/Probability Series, Wad-
sworth, Belmont, CA. 1984.
[De Mantaras, 1991] R.L. De Mantaras. A distance-based attribute selection measure
for decision tree induction. In Machine Learning, volume 6, pages 81–92, 6-9 1991.
[Hall, 2000] Mark A. Hall. Correlation-based feature selection for discrete and numeric
class machine learning. In Proc. 17th International Conf. on Machine Learning, pages
359–366. Morgan Kaufmann, San Francisco, CA, 2000.
[Hart, 1984] A. Hart. Experience in the use of an inductive system in knoowledge eng.
In M. Bramer, editor, Research and Development in Expert Systems. Cambridge
Univ. Press, Cambridge, MA,, 1984.
[John et al., 1994] George H. John, Ron Kohavi, et Karl Pfleger. Irrelevant fea-
tures and the subset selection problem. In International Conference on Ma-
chine Learning, pages 121–129, 1994. Journal version in AIJ, available at
http://citeseer.nj.nec.com/13663.html.
[Jun et al., 1997] B.H. Jun, C.S. Kim, H.Y. Song, et J. Kim. A new criterion in se-
lection and discretization of attributes for the generation of decision trees. IEEE
Transactions on Pattern Analysis and Machine Intelligence (PAMI), 1997.
[Kira et Rendell, 1992a] K. Kira et L. A. Rendell. The feature selection problem: Tra-
ditional methods and a new algorithm. In MIT Press, editor, Tenth National Confe-
rence on Artificial Intelligence, pages 129–134, 1992.
[Kira et Rendell, 1992b] K. Kira et L.A. Rendell. A practical approach to feature
selection. In Proceedings of the Tenth International Conference on Machine Learning,
pages 500–512, 1992.
[Kirkpatrick et al., 1983] S. Kirkpatrick, C. D. Gelatt, et M. P. Vecchi. Optimisation
by simulated annealing. Science, 220:671–674, 1983.
[Lallich et Rakotomalala, 2000] S. Lallich et R. Rakotomalala. Fast feature selection
using partial correlation for multi-valued attributes. In Proc. of the 4th European
Conf. on Knowledge Discovery in Databases, PKDD 2000, pages 221–231, 2000.
[Liu et al., 1998] Huan Liu, Hiroshi Motoda, et Manoranjan Dash. A monotonic mea-
sure for optimal feature selection. In European Conference on Machine Learning,
pages 101–106, 1998.
[Liu et Setiono, 1996] Huan Liu et Rudy Setiono. A probabilistic approach to feature
selection - a filter solution. In Int. Conf. on Machine Learning, pages 319–327, 1996.
[Marcotorchino et Michaud, 1981] F. Marcotorchino et P. Michaud. Heuristic ap-
proach to the similarity aggregation problem. Methods of Operations Research,
43:395–404, 1981.
RNTI - C - 1
Se´lection de variables et agre´gation d’opinions
[Marcotorchino, 1981] J. F. Marcotorchino. Agre´gation de similarite´s en classification
automatique. The`se de Doctorat d’Etat, Universite´ Paris 6, 1981.
[Michaud, 1982] P. Michaud. Agre´gation a` la majorite´ 1 : Hommage a` condorcet, etude
n◦f-051. Technical report, Centre Scientifique IBM-France, 1982.
[Mingers, 1987] J. Mingers. Expert systems – rule induction with statistical data. In
Journal of the Operational Research Society, 1987.
[Nicoloyannis et al., 1998] N. Nicoloyannis, M. Terrenoire, et D. Tounissoux. An opti-
misation model for aggregating preferences : A simulated annealing approach. Health
and System Science, 2(1-2):33–44, 1998.
[Nicoloyannis et al., 1999] N. Nicoloyannis, M. Terrenoire, et D. Tounissoux. Perti-
nence d’une classification. Revue Electronique sur l’Apprentissage par les Donne´es,
3(1):39–49, 1999.
[Quinlan, 1986] J.R. Quinlan. Introduction of decision trees. In Machine Learning,
volume 1, pages 81–106, 1986.
[Shannon, 1948] C.E. Shannon. A mathematical theory of communication. In Bell
System Technical Journal, 1948.
[Sheinvald et al., 1990] Sheinvald, Dom, Niblack, et Rendell. A modeling approach to
feature selection. In 10th Int. Conf. on Pattern Recognition, 1990.
[Zhou et Dillon, 1991] X. Zhou et T.S. Dillon. A statistical–heuristic feature selection
criterion for decision tree induction. In IEEE Transactions on Pattern Analysis and
Machine Intelligence, volume 13, pages 834–841, 1991.
[Zighed et al., 1996] D. A. Zighed, R. Rakotomalala, et S. Rabase´da. A discretization
method of continous attributes in induction graphs. In 13th European Meetings on
Cybernetics and System Research, pages 997–1002, 1996.
Summary
Feature selection enables to reduce the representation space of the data. This process
gets more and more important because of databases size increase. Therefore we propose
a method for discrete attributes based on preferences aggregation. The result, in form
of an ordering of features, is furnished by the aggregation of the results obtained by
several short-sighted methods of feature selection.
RNTI - C - 1
