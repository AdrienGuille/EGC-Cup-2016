Qualite´ d’ajustement d’arbres d’induction
Gilbert Ritschard∗, Djamel A. Zighed∗∗
∗De´partement d’e´conome´trie, Universite´ de Gene`ve
gilbert.ritschard@themes.unige.ch http://mephisto.unige.ch
∗∗Laboratoire ERIC, Universite´ Lumie`re Lyon 2
zighed@univ-lyon2.fr http://eric.univ-lyon2.fr
Re´sume´. Cet article discute des possibilite´s de mesurer la qualite´ de
l’ajustement d’arbres d’induction aux donne´es comme cela se fait classi-
quement pour les mode`les statistiques. Nous montrons comment adapter
aux arbres d’induction les statistiques du khi-2, notamment celle du rap-
port de vraisemblance utilise´e dans le cadre de la mode´lisation de tables
de contingence. Cette statistique permet de tester l’ajustement du mode`le,
mais aussi l’ame´lioration de l’ajustement qu’apporte la complexification
de l’arbre. Nous en de´duisons e´galement des formes adapte´es des crite`res
d’information AIC et BIC qui permettent de se´lectionner le meilleur arbre
en termes de compromis entre ajustement et complexite´. Nous illustrons
la mise en œuvre pratique des statistiques et indicateurs propose´s avec un
exemple re´el.
Mots cle´s : arbre d’induction, qualite´ d’ajustement, tests du khi-2, com-
paraison d’arbres
1 Introduction
Les arbres d’induction (Kass, 1980; Breiman et al., 1984; Quinlan, 1993; Zighed et
Rakotomalala, 2000) sont l’un des outils les plus populaires d’apprentissage supervise´.
Ils consistent a` rechercher par e´clatements successifs de sommets, une partition de l’en-
semble des combinaisons de valeurs des pre´dicteurs optimale pour pre´dire la variable
re´ponse. La pre´diction se fait simplement en choisissant, dans chaque classe de la par-
tition obtenue, la modalite´ la plus fre´quente de la variable a` pre´dire. Bien que leur
utilisation premie`re soit la ge´ne´ration d’arbres de de´cisions pour la classification, les
arbres d’induction fournissent une description de la fac¸on dont la distribution de la va-
riable a` pre´dire est conditionne´e par les valeurs des pre´dicteurs. Ils nous indiquent par
exemple comment la re´partition entre clients solvables et insolvables est influence´e par
les attributs aˆge, sexe, niveau d’e´ducation, profession, etc. En ce sens les arbres d’induc-
tion sont donc des outils de mode´lisation de l’influence des pre´dicteurs sur la variable
a` pre´dire au meˆme titre que par exemple la re´gression line´aire, la re´gression logistique
ou la mode´lisation log-line´aire de tables de contingence multi-dimensionnelles. C’est
essentiellement a` cet aspect de mode´lisation descriptive, et en particulier a` l’e´valuation
de la qualite´ de la description fournie par un arbre induit que nous nous inte´ressons
dans cet article.
En mode´lisation statistique, qu’il s’agisse de re´gression line´aire, d’analyse discri-
Qualite´ d’ajustement d’arbres d’induction
minante, de re´gression logistique ou plus ge´ne´ralement de mode`le line´aire ge´ne´ralise´
(GLM), il est d’usage d’e´valuer la qualite´ d’ajustement du mode`le, c’est-a`-dire la qua-
lite´ de la description fournie par le mode`le, avec des mesures descriptives telles que le
coefficient de de´termination R2 ou des pseudo R2, et avec des statistiques de test telles
que les khi-2 du score test, de Wald ou du rapport de vraisemblance. Parmi ces der-
nie`res, la statistique du rapport de vraisemblance jouit d’une proprie´te´ d’additivite´ qui
permet d’e´valuer e´galement la pertinence statistique de la simplification d’un mode`le
de re´fe´rence par renforcement de contraintes sur ses parame`tres, ou, si l’on regarde les
choses dans l’autre sens, la significativite´ statistique de la complexification re´sultant de
l’ajout de parame`tres a` un mode`le donne´.
Le cas particulier des tests de significativite´ globale de l’explication, parfois appele´s
“tests omnibus”, ou` l’on teste globalement l’apport des facteurs explicatifs par rapport
a` un mode`le de re´fe´rence na¨ıf — le mode`le avec la constante comme seule variable ex-
plicative dans le cas de la re´gression, le mode`le d’inde´pendance dans le cas des mode`les
log-line´aires — retiendra notre attention. Dans le cas des arbres d’induction, le mode`le
de re´fe´rence est naturellement l’arbre de niveau 0 constitue´ par le seul nœud initial.
Une des difficulte´s principales a` laquelle on se heurte dans la pratique des arbres
ou graphes d’induction est le fort degre´ de complexite´ des arbres mis en e´vidence. Il
nous parait alors souhaitable de pouvoir disposer aussi de crite`res tels que le crite`re
d’information AIC d’Akaike (1973) ou le crite`re d’information baye´sien BIC (Schwarz,
1978; Kass et Raftery, 1995). Ces crite`res sont une combinaison de la qualite´ d’ajuste-
ment (statistique du rapport de vraisemblance) et d’une mesure de la complexite´. Ils
s’ave`rent ainsi une aide pre´cieuse pour arbitrer entre complexite´ et qualite´ d’ajustement
dans la se´lection de mode`les.
L’article est organise´ comme suit. La section 2 situe la place des mesures de qualite´
d’ajustement envisage´s parmi les mesures classiques de qualite´ d’un arbre d’induc-
tion. La section 3 pre´cise le concept d’ajustement conside´re´. La section 4 est consacre´e
aux crite`res de qualite´ d’ajustement. On montre comment les statistiques du khi-2 de
Pearson et du rapport de vraisemblance utilise´es dans le cadre de la mode´lisation de
tables de contingence peuvent s’adapter aux arbres d’induction. Nous discutons ensuite
l’ame´lioration qu’apporte un mode`le par rapport a` un mode`le de re´fe´rence. On montre
comment exploiter la diffe´rence des statistiques G2 du rapport de vraisemblance pour
tester la significativite´ statistique du gain d’information et proposons des indicateurs
de type R2. Nous montrons e´galement comment appliquer les crite`res d’information
d’Akaike (AIC) et baye´sien (BIC) aux arbres d’induction et discutons leur inte´reˆt pour
guider le choix entre mode`les de complexite´ variable. La section 5 illustre l’utilisation
des crite`res propose´s dans un cas concret et la section 6 valide empiriquement le calcul
des degre´s de liberte´ et la distribution des statistiques du khi-2 pour arbres. Fina-
lement, la conclusion fait l’objet de la section 7 ou` nous mentionnons des pistes de
de´veloppements futurs de la de´marche initie´e dans cet article.
2 Arbre d’induction et mesures classiques de qualite´
Avant de nous concentrer sur la qualite´ d’ajustement, nous rappelons brie`vement
le principe des arbres d’induction et leurs crite`res usuels de qualite´. Ceci dans le but
RNTI - 2
G. Ritschard et D.A. Zighed
de pouvoir mieux situer le roˆle des mesures de qualite´ d’ajustement propose´es dans cet
article. Le lecteur inte´resse´ trouvera par une discussion approfondie des arbres et de
leurs crite`res habituels de qualite´ par exemple dans Zighed et Rakotomalala (2000).
2.1 Rappel du principe des arbres d’inductions
L’objectif est de construire une re`gle qui permette, a` partir de la connaissance d’un
vecteur d’attributs x = (x1, . . . , xp), de pre´dire une variable re´ponse y, ou si l’on pre´fe`re
de classer les cas selon les e´tats de la variable y. La construction de la re`gle se fait en
deux temps. Dans un premier temps, on de´termine une partition des valeurs possibles
de x telle que la distribution de la re´ponse y soit la plus pure possible dans chaque
classe de la partition, ou de fac¸on plus ou moins e´quivalente la plus diffe´rente possible
d’une classe a` l’autre. La re`gle consiste ensuite a` attribuer a` chaque cas la valeur de y
la plus fre´quente dans sa classe.
Les arbres d’induction de´terminent la partition par e´clatements successifs des som-
mets. En partant du sommet initial, ils recherchent l’attribut qui permet le meilleur
e´clatement selon un crite`re qui peut eˆtre par exemple le gain d’entropie (C4.5, Sipina)
ou la significativite´ d’un khi-2 (CHAID). L’ope´ration est re´pe´te´e a` chaque nouveau
sommet jusqu’a` ce qu’un crite`re d’arreˆt, une taille minimale du sommet par exemple,
soit atteint. Le re´sultat est un arbre tel que celui pre´sente´ a` la figure 1.
2.2 Taux d’erreur
Le taux d’erreur de classification, c’est-a`-dire le pourcentage de cas mal classe´s est
peut-eˆtre le crite`re de qualite´ le plus utilise´. Il mesure la performance pre´dictive du
mode`le. S’agissant de classification, c’est e´videmment la performance en ge´ne´ralisation
qui importe, c’est-a`-dire la performance pour des cas n’ayant pas servi a` l’apprentissage.
C’est pourquoi il convient de calculer le taux d’erreur sur un e´chantillon de validation
diffe´rent de l’e´chantillon d’apprentissage. Pratiquement, ceci conduit a` partitionner
l’ensemble de donne´es en deux parties, l’une servant a` l’apprentissage, l’autre a` la
validation.
Le taux d’erreur est souvent e´value´ par la validation croise´e qui donne e´galement
des indications sur sa variabilite´. Cette me´thode consiste a` partager les donne´es en
 G46
 

 
 
	 
 
 
 

	 
	 
	 
Fig. 1 – Arbre induit
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
par exemple 10 groupes de tailles approximativement e´gales et a` re´pe´ter ensuite l’ap-
prentissage en re´servant a` chaque fois un groupe diffe´rent pour la validation. Le taux
d’erreur en validation croise´e est la moyenne des taux obtenus. Notons que comme on
peut obtenir a` chaque fois un arbre diffe´rent l’on de´termine ainsi le taux d’erreur de la
me´thode de construction de l’arbre et non l’erreur d’un arbre induit particulier.
La de´composition biais/variance/erreur re´siduelle du taux d’erreur (Geurts, 2002)
fournit des indications pre´cieuses sur la part de l’erreur due a` la variabilite´ de l’e´chan-
tillon d’apprentissage. Ces de´compositions ne sont cependant quantifiables que nume´-
riquement sur la base de simulations ou de me´thodes de re´e´chantillonnage ce qui limite
leur utilisation syste´matique.
2.3 Qualite´ des partitions
La qualite´ d’une partition est d’autant meilleure que les sommets terminaux sont
purs, c’est-a`-dire qu’ils ont des distributions le plus proche possible de la distribution
de´ge´ne´re´e qui donne un poids de un a` l’une des modalite´s et ze´ro aux autres. Cette
configuration limite en effet l’incertitude quant a` la valeur de la variable re´ponse a`
l’inte´rieur de chaque sommet. Les distributions doivent e´galement eˆtre diffe´rentes d’un
sommet a` l’autre.
Il existe plusieurs fac¸ons de mesurer la qualite´ des partitions obtenues (voir Zighed
et Rakotomalala, 2000, chap. 9.) On peut mentionner :
– Les mesures issues de la the´orie de l’information et qui consistent essentiellement
a` mesurer l’entropie de la re´ponse pour la partition conside´re´e.
– Celles qui se fondent sur des distances entre distributions de probabilite´s. Le prin-
cipe consiste ici a` ve´rifier que les distributions diffe`rent le plus possible d’une
classe a` l’autre. La de´marche est simple dans le cas de deux classes. Pour le cas
plus ge´ne´ral d’un nombre quelconque de classes, les solutions propose´es restent
de porte´e assez limite´e.
– Enfin, les mesures qui s’appuient sur des indices d’association. L’ide´e est ici que la
partition est d’autant meilleure que l’association entre les classes de la partition et
la variable re´ponse est forte. Rakotomalala et Zighed (1998) proposent d’utiliser
le degre´ de signification du τ de Goodman et Kruskal ce qui permet de tenir
compte e´galement de la taille des e´chantillons.
2.4 Complexite´
Un des inte´reˆts majeurs souvent avance´ des arbres et graphes d’induction est la
facilite´ de leur interpre´tation. Ceci est vrai tant que la complexite´ de l’arbre reste
limite´e, d’ou` l’inte´reˆt de mesurer cette complexite´. Les indicateurs couramment utilise´s
sont :
– Le nombre de sommets terminaux. Ce crite`re correspond au nombre de re`gles de
pre´diction ainsi qu’au nombre de classes de la partition finale.
– La profondeur ou le nombre de niveaux de l’arbre. Ce crite`re de´pend de la pro-
ce´dure de construction. Un arbre n-aire peut de´finir une meˆme partition avec un
nombre plus petit de niveaux qu’un arbre binaire.
RNTI - 2
G. Ritschard et D.A. Zighed
– Le nombre de nœuds. Ce crite`re est e´galement lie´ a` la proce´dure de construction. Il
sera en ge´ne´ral plus e´leve´ pour un arbre binaire qui tend a` multiplier les sommets
interme´diaires. Ce crite`re refle`te bien la complexite´ visuelle de l’arbre induit.
– La longueur des messages. Traduit la complexite´ des re`gles d’affectation aux dif-
fe´rentes classes.
2.5 Place des mesures de qualite´ d’ajustement envisage´es
Les nouveaux crite`res de la qualite´ d’ajustement propose´s dans cet article concernent
la capacite´ de l’arbre a` reproduire la distribution de la re´ponse y pour les individus
ayant un profil x donne´. En d’autres termes, on s’inte´resse a` la qualite´ de reproduction
de la table de contingence qui croise la variable re´ponse y avec l’ensemble des pre´dic-
teurs. Il s’agit donc de la qualite´ descriptive de l’arbre par opposition a` sa performance
en classification. On conside`re en particulier les deux aspects suivants :
– l’aptitude de l’arbre induit a` de´crire la distribution de la variable re´ponse condi-
tionnellement aux valeurs prises par les pre´dicteurs,
– le gain d’information qu’apporte l’arbre induit par rapport au nœud initial ou`
l’on ne tient pas compte des pre´dicteurs.
On peut situer les mesures d’ajustement qui nous occupent dans l’optique «qualite´
des partitions» de la typologie pre´ce´dente. Il s’agit en effet de voir comment la partition
induite par l’arbre ajuste la table cible. Les crite`res de qualite´ de partition mentionne´s
en 2.3 ne mesurent pas explicitement cet ajustement, bien qu’elles s’y re´fe`rent implici-
tement. L’entropie de la re´ponse pour la partition induite aussi bien que l’association
entre la partition et la variable re´ponse constituent en effet des mesures de la proxi-
mite´ entre les distributions pre´dites par l’arbre et les distributions cibles. Notre objectif
est cependant de proposer des mesures d’ajustement qui s’apparentent a` celles utili-
se´es en mode´lisation statistique et qui se preˆtent a` l’infe´rence statistique, le test de
significativite´ du de´faut d’ajustement ou le test de la diffe´rence entre deux arbres en
particulier.
La qualite´ d’ajustement de la table des donne´es d’apprentissage doit eˆtre mise en
paralle`le avec la stabilite´ de la description fournie, c’est-a`-dire la simplicite´ de l’arbre.
En effet, en de´veloppant trop loin un arbre, on a tendance a` de´crire les spe´cificite´s de
l’e´chantillon plutoˆt que la structure du phe´nome`ne sous-jacent. Un arbre trop complexe
de´pend trop e´troitement de l’e´chantillon pour fournir une description ge´ne´ralisable des
liens liant la variable re´ponse aux pre´dicteurs. On cherchera ainsi par exemple l’arbre le
plus simple qui ajuste la table de fac¸on satisfaisante. Alternativement, on peut s’inte´-
resser a` l’arbre qui offre le meilleur compromis entre qualite´ d’ajustement et complexite´.
Nous proposons a` cet effet des adaptations pour les arbres des crite`res d’information
AIC et BIC, crite`res qui par construction rele`vent simultane´ment de la complexite´ et de
la qualite´ des partitions. Les adaptations propose´es reposent sur le mode`le parame´tre´
de reconstruction des donne´es a` partir de l’arbre que nous introduisons a` la section 4.3
et qui permet de mesurer la complexite´ de l’arbre en termes de nombre de parame`tres.
Cette dernie`re possibilite´ constitue en soi une contribution a` la panoplie des mesures
de complexite´.
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
3 Concept de qualite´ d’ajustement d’un arbre
3.1 Table cible et table pre´dite
De fac¸on ge´ne´rale, la qualite´ d’ajustement d’un mode`le se re´fe`re a` sa capacite´ a`
reproduire les donne´es. Dans le cas de la pre´diction quantitative d’une variable Y , par
exemple dans le cas de la re´gression line´aire, l’objectif est clair. Il s’agit d’obtenir des
valeurs pre´dites yˆα qui s’ajustent le mieux possible aux valeurs observe´es yα, pour α =
1, . . . , n, n e´tant le nombre d’observations. De meˆme, dans l’optique de la classification,
les e´tats pre´dits yˆα doivent correspondre le plus souvent possible aux vraies valeurs yα.
Le taux d’erreur est dans ce cas un indicateur naturel de qualite´ d’ajustement.
Dans certaines situations, en particulier dans les sciences de comportement (socio-
logie, sciences politiques, marketing, ...), les arbres ou graphes d’induction sont utilise´s
plus dans une optique descriptive que pre´dictive comme outil de mise en e´vidence des
principaux de´terminants de la variable a` pre´dire. Ils sont utilise´s comme outil d’aide a`
la compre´hension de phe´nome`nes et non pas comme outil de classification.
Ce ne sont plus alors les e´tats particuliers yα que l’on cherche a` reproduire. Pour
comprendre comment les pre´dicteurs interagissent sur la variable re´ponse Y , il convient
en effet d’examiner comment la distribution de Y change avec le profil x. Dans cette
optique, la qualite´ d’ajustement conside´re´e ici se re´fe`re a` la qualite´ de la reproduction
de l’ensemble des distributions conditionnelles.
Formellement, dans l’optique classification il s’agit de caracte´riser une fonction f(x)
qui pre´dit la valeur de y compte tenu de x. Avec les arbres, la construction de cette
fonction se fait en deux e´tapes :
1. Caracte´riser un mode`le descriptif p(Y |x) = (p1(x), . . . , p`(x)), ou` la notation
pi(x) de´signe la probabilite´ conditionnelle p(Y = yi|x).
2. Pre´dire par la re`gle majoritaire f(x) = argmaxi pi(x).
Contrairement a` la re´gression logistique par exemple, ou` p(Y |x) s’exprime analytique-
ment en fonction de x, le mode`le descriptif prend ici la forme non parame´trique d’un
ensemble fini de distributions :{
p|j ∈ [0, 1]` | p|j = p(Y |xj), j = 1, . . . , c
}
.
Les pre´dicteurs e´tant suppose´s prendre un nombre fini de valeurs, l’ensemble des dis-
tributions conditionnelles observe´es est repre´sentable sous forme d’une table de contin-
gence T croisant y avec la variable composite de´finie par le croisement de tous les
pre´dicteurs. Le tableau 1 est un exemple d’une telle table dans le cas ou` la variable
a` pre´dire est le statut marital et les pre´dicteurs le genre et le secteur d’activite´. Le
nombre de lignes de T est le nombre ` d’e´tats de la variable Y . Si chaque pre´dicteur
xν , ν = 1, . . . , p a cv valeurs diffe´rentes, le nombre c de colonnes de T est au plus le
produit des cv, soit : c ≤
∏
ν cν , l’ine´galite´ e´tant stricte lorsque certaines combinai-
sons de valeurs des attributs sont structurellement impossibles. On s’inte´resse donc a`
la capacite´ de l’arbre a` reproduire cette table T.1
1Il n’est pas sans inte´reˆt de relever comme nous l’avons fait dans Ritschard et Zighed (2003),
que dans cette optique de reconstitution de T , les arbres peuvent constituer un outil alternatif et
comple´mentaire a` la mode´lisation log-line´aire des tables de contingence multidimensionnelle.
RNTI - 2
G. Ritschard et D.A. Zighed
homme femme
marie´ primaire secondaire tertiaire primaire secondaire tertiaire total
non 11 14 15 0 5 5 50
oui 8 8 9 10 7 8 50
total 19 22 24 10 12 13 100
Tab. 1 – Exemple de table de contingence T
Un e´le´ment de la table T est note´ nij et repre´sente le nombre de cas avec profil
xj qui dans les donne´es prennent la valeur yi de la variable re´ponse. On note Tˆ la
table pre´dite a` partir d’un arbre et nˆij de´signe un e´le´ment ge´ne´rique de cette table.
Formellement, la qualite´ d’ajustement se re´fe`re ainsi a` la divergence entre les tables T
et Tˆ.
Il reste a` pre´ciser comment l’on de´duit la table estime´e Tˆ a` partir d’un arbre. On
utilise pour cela le mode`le de reconstruction suivant ou` l’on note Tj la j-e`me colonne
de T :
Tj = najp|j , j = 1, . . . , c (1)
Les parame`tres sont le nombre total n de cas, les proportions aj de cas par colonne j =
1, . . . , c, et les c vecteurs de probabilite´s p|j = p(Y |xj) correspondant a` la distribution
de Y dans chaque colonne j de la table. Nous verrons que chaque arbre donne lieu a` des
estimations pˆ|j diffe´rentes des vecteurs p|j et par suite a` une estimation Tˆ diffe´rente.
Les aj seront estime´s par les proportions aˆj = n·j/n observe´es dans l’e´chantillon, n·j
de´signant le total de la j-e`me colonne de T. Contrairement aux pˆ|j , les estimations aˆj
sont inde´pendantes de l’arbre induit.
3.2 Arbre sature´ et arbre e´tendu
En mode´lisation statistique, on appelle mode`le sature´ un mode`le avec le nombre
maximal de parame`tres libres qui peuvent eˆtre estime´s a` partir des donne´es. En mo-
de´lisation log-line´aire de tables de contingence multidimensionnelles, le mode`le sature´
permet de reproduire exactement la table mode´lise´e. Par analogie, nous introduisons
le concept d’arbre sature´ qui permet de reproduire exactement la table T.
De´finition 1 (Arbre sature´) Pour des variables pre´dictives cate´gorielles, on appelle
arbre sature´, un arbre qui re´sulte de tous les e´clatements successifs possibles selon les
modalite´s des variables pre´dictives.
Les distributions p|j conditionnelles aux feuilles de l’arbre sature´ sont estime´es par les
vecteurs de fre´quences relatives, soit les vecteurs d’e´le´ments nij/n·j , i = 1, . . . , `.
L’arbre sature´ n’est pas unique, des variantes e´tant possibles selon l’ordre dans
lequel les variables sont prises en compte. Tous les arbres sature´s conduisent cependant
aux meˆmes feuilles (sommets terminaux). Ces feuilles correspondent aux colonnes de
la table de contingence T.
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
 G46
     
 
 
 
	
 

	
 


 



	

 
 
 
 
Fig. 2 – Arbre sature´
Par exemple, la figure 2 donne l’arbre sature´ correspondant au cas du tableau 1 ou`
l’on cherche a` pre´dire le statut marital connaissant le genre (H = homme, F = femme)
et le secteur d’activite´ (P = primaire, S = secondaire, T = tertiaire). Les distributions
conditionnelles sont :
pˆ|HP =
(
11/19
8/19
)
, pˆ|HS =
(
14/22
8/22
)
, pˆ|HT =
(
15/24
9/24
)
,
pˆ|FP =
(
0/10
10/10
)
, pˆ|FS =
(
5/12
7/12
)
, pˆ|FT =
(
5/13
8/13
)
Notons qu’un algorithme d’induction d’arbres ne peut en ge´ne´ral ge´ne´rer un arbre
sature´ que si (i) toutes les cellules de la table de contingence des variables pre´dictives
sont non vides et si (ii) la distribution de la variable re´ponse est diffe´rente dans chacune
des cellules.
Pour comparer les distributions des feuilles de l’arbre induit a` celles de l’arbre
sature´, on doit e´tendre l’arbre induit pour obtenir des feuilles de meˆme de´finition et en
particulier en meˆme nombre que celles de l’arbre sature´.
De´finition 2 (Extension maximale d’un arbre induit) Pour des variables pre´-
dictives cate´gorielles, on appelle extension maximale de l’arbre induit ou arbre induit
e´tendu, l’arbre obtenu a` partir de l’arbre induit en proce´dant a` tous les e´clatements suc-
cessifs possibles de ses sommets terminaux et en appliquant aux feuilles de l’extension
la distribution pˆa|k observe´e dans le nœud terminal parent de l’arbre initial.
Par exemple, si l’arbre induit est l’arbre avec les sommets blancs de la figure 3,
son extension maximale s’obtient en ajoutant les sommets gris et en re´partissant dans
ceux-ci l’effectif selon la distribution du sommet dont ils sont issus. Les distributions
des six sommets terminaux de l’extension se de´duisent de celles des trois feuilles de
l’arbre induit, soit pour notre exemple :
pˆ|HP = pˆ|HS = pˆ|HT = pˆa|H =
(
40/65
25/65
)
pˆ|FP = pˆa|FP =
(
0/10
10/10
)
pˆ|FS = pˆ|FT = pˆa|FP¯ =
(
10/25
15/25
)
RNTI - 2
G. Ritschard et D.A. Zighed
 G46
    

 
   	
	  

 
  
  
   
  
 
 
 
 
 
 

 
 
 
  
	  
  
	  
Fig. 3 – Arbre induit (sommets blancs) et son extension maximale
Les feuilles terminales de l’extension de l’arbre induit donnent lieu a` la table pre´dite
Tˆ repre´sente´e au tableau 2.
homme femme
marie´ primaire secondaire tertiaire primaire secondaire tertiaire total
non 11.7 13.5 14.8 0 4.8 5.2 50
oui 7.3 8.5 9.2 10 7.2 7.8 50
total 19 22 24 10 12 13 100
Tab. 2 – Exemple de table de contingence pre´dite Tˆ
4 Qualite´ d’ajustement d’un arbre induit
Ayant pre´cise´ le concept d’ajustement d’un arbre qui nous occupe, et en particulier
les notions de tables cible et pre´dite, nous proposons dans cette section des statistiques
et indicateurs permettant d’en e´valuer la qualite´. Il s’agit essentiellement d’adaptations
des mesures classiquement utilise´es en mode´lisation statistique pour juger de la qualite´
globale d’un mode`le.
Nous proposons d’utiliser des statistiques de test du khi-2 pour mesurer la diver-
gence entre la table pre´dite Tˆ et la table cible T. Comme en mode´lisation multinomiale
log-line´aire (Agresti, 1990) ou` cette divergence sert d’indicateur de l’e´cart entre mo-
de`le ajuste´ et mode`le sature´, elle renseigne ici sur l’e´cart entre l’arbre induit et l’arbre
sature´.
inde´pendance
nœud inital
mode`le sature´
arbre sature´
mode`le ajuste´
arbre induit
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
Plutoˆt que de chercher a` savoir a` quelle distance l’on se trouve du mode`le sature´ cor-
respondant a` la partition la plus fine, il peut eˆtre utile de savoir ce que l’on a gagne´ par
rapport a` la situation d’inde´pendance ou` l’on ne tient pas compte des pre´dicteurs. Cet
aspect correspond a` l’e´cart entre l’arbre induit et le mode`le d’inde´pendance repre´sente´
par l’arbre constitue´ du seul nœud initial.
Nous commenc¸ons donc avec les tests d’ajustement du khi-2, puis discutons de la
comparaison avec un mode`le de re´fe´rence, et plus ge´ne´ralement de la diffe´rence entre
deux mode`les, en introduisant les statistiques de test de l’ame´lioration de l’ajustement.
Dans la meˆme optique nous proposons plusieurs indicateurs de type R2. Enfin, dans
la perspective de se´lectionner l’arbre offrant le meilleur compromis entre ajustement
et complexite´, nous comple´tons la discussion en e´tablissant des formes des crite`res
d’information AIC et BIC applicables aux arbres.
4.1 Statistiques du khi-2 pour arbres d’induction
L’objectif est de mesurer la divergence entre T et Tˆ avec une statistique permettant
d’e´valuer la significativite´ de l’e´cart observe´. Rappelons que T est reproduit exactement
par l’arbre sature´. La divergence que l’on se propose d’e´valuer ici s’interpre`te donc
e´galement comme l’e´cart entre l’arbre induit et l’arbre sature´.
Les p|j e´tant estime´s par le maximum de vraisemblance, on peut simplement appli-
quer les statistiques de divergence de Cressie et Read (1984), dont les cas particuliers
les plus connus sont les khi-2 de Pearson que l’on note X2 et la statistique du rapport
de vraisemblance note´ G2 :
X2 =
∑`
i=1
c∑
j=1
(nij − nˆij)2
nˆij
(2)
G2 = 2
∑`
i=1
c∑
j=1
nij ln
(
nij
nˆij
)
(3)
Sous l’hypothe`se que le mode`le est correct et sous certaines conditions de re´gularite´, voir
par exemple Bishop et al. (1975, chap. 4), ces statistiques suivent une meˆme distribution
du χ2 avec pour degre´s de liberte´ le nombre de cellules de la table de contingence moins
le nombre de parame`tres line´airement inde´pendants du mode`le de pre´diction deT. Dans
notre cas T est pre´dit par le mode`le de reconstruction (1) dont il s’agit alors de pre´ciser
le nombre de parame`tres line´airement inde´pendants.
Un arbre induit non sature´ de´finit une partition de l’ensemble X des profils x
possibles. Chacun de ses q sommets terminaux correspond donc a` un sous-ensemble
Xk ⊆ X , k = 1, . . . , q de profils xj pour lequel, on impose la contrainte
p|j = pa|k pour tout xj ∈ Xk k = 1, . . . , q (4)
ou` pa|k de´signe la distribution dans le sommet terminal k de l’arbre.
Chaque vecteur pa|k contient `− 1 termes inde´pendants et il y a q − 1 distributions
conditionnelles pa|k inde´pendantes. On en de´duit le de´compte du tableau 3 des para-
me`tres inde´pendants pour un nombre q fixe´ de sommets terminaux.2 En retranchant
2Bien que q soit induit des donne´es, on raisonne ici conditionnellement a` q comme cela se fait en
RNTI - 2
G. Ritschard et D.A. Zighed
parame`tres nombre dont inde´pendants
pi|j , i = 1, . . . , `, j = 1, . . . , c c` q(`− 1)
aj , j = 1, . . . , c c c− 1
n 1 1
Total c(`+ 1) + 1 q(`− 1) + c
Tab. 3 – Calcul du nombre de parame`tres inde´pendants
au nombre c` de cellules de T le nombre q(`− 1) + c de parame`tres inde´pendants, on
obtient les degre´s de liberte´ dM de l’arbre induit, soit
degre´s de liberte´ = dM = (c− q)(`− 1) .
Notons que ce nombre correspond au nombre de contraintes (4). Pour le mode`le d’in-
de´pendance, note´ I, on a q = 1 et l’on retrouve la valeur usuelle des degre´s de liberte´
du test d’inde´pendance, soit dI = (c− 1)(`− 1). De meˆme, pour l’arbre sature´, note´ S,
on a q = c et donc dS = 0.
Sous re´serve des conditions de re´gularite´, les statistiquesX2 etG2 de tables associe´es
a` des arbres suivent donc, lorsque le mode`le est correct, une distribution du χ2 avec
(c− q)(`− 1) degre´s de liberte´.
Pour l’arbre de la figure 1, on trouve par exemple, X2 = 0.1823 et G2 = 0.1836.
On a c = 6, q = 3 et ` = 2, et donc dM = (6 − 3)(2 − 1) = 3 degre´s de liberte´. Les
valeurs des statistiques sont tre`s petites et, avec un degre´ de signification de l’ordre de
98% dans les deux cas, indiquent clairement que Tˆ ajuste de fac¸on satisfaisante T. La
qualite´ de l’ajustement de l’arbre induit aux donne´es est donc dans ce cas excellente.
The´oriquement, les statistiques X2 de Pearson (2) et G2 du rapport de vraisem-
blance (3) devraient permettre de tester si l’arbre induit s’ajuste de fac¸on satisfaisante
aux donne´es. Il est bien connu cependant que la porte´e du test reste limite´e lorsque
l’e´chantillon est grand, le moindre e´cart devenant alors statistiquement significatif. Par
ailleurs, les conditions de re´gularite´ requises pour que les statistiques soient distribue´es
selon une loi du χ2, et en particulier les conditions d’inte´riorite´ (pas d’effectifs atten-
dus nuls), sont difficilement tenables lorsque l’arbre sature´ compte un grand nombre
de feuilles.
Plus inte´ressante nous semble eˆtre l’utilisation de la statistique G2 pour comparer
des mode`les imbrique´s, un mode`leM2 (restreint) e´tant inclus dansM1 (non restreint) si
l’espace de ses parame`tres est un sous-ensemble deM1, c’est-a`-dire, en d’autres termes,
si les parame`tres de M2 s’obtiennent en imposant des contraintes sur ceux du mode`le
M1. En effet dans ce cas la de´viance entre les deux mode`les est (voir par exemple
Agresti, 1990, p. 211 ou Powers et Xie, 2000, p. 105) :
G2(M2|M1) = G2(M2)−G2(M1) (5)
mode´lisation log-line´aire ou` l’on suppose donne´es les interactions prises en compte, alors qu’elles sont
en fait induites des donne´es par le biais du processus de se´lection du mode`le.
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
qui, sous l’hypothe`se que M2 est correct, est approximativement distribue´e selon une
loi du χ2 avec pour degre´s de liberte´ la diffe´rence d2 − d1 des degre´s de liberte´ des
mode`les M2 et M1.
Cette dernie`re proprie´te´ permet en particulier de tester la significativite´ d’un e´cla-
tement. La de´viance entre le mode`le apre`s l’e´clatement et celui avant l’e´clatement nous
renseigne en effet sur la pertinence statistique de cet e´clatement. Par exemple, si M1
est l’arbre induit de la figure 1 et M2 l’arbre avant l’e´clatement du sommet « femme» .
On a G2(M1) = 0.18 avec 3 degre´s de liberte´ et G2(M2) = 8.41 avec 4 degre´s de liberte´.
La de´viance est alors
G2(M2|M1) = 8.41− 0.18 = 8.23 avec d2 − d1 = 4− 3 = 1
Son degre´ de signification (p-valeur) est 0.4%, donc infe´rieur au seuil ge´ne´ralement
admis de 5%, ce qui indique que l’e´clatement est statistiquement significatif.
4.2 Comparaison avec un mode`le de re´fe´rence
Cette section est consacre´e aux indicateurs de type R2 qui mesurent le gain relatif
de qualite´ d’un arbre induitM par rapport a` l’arbre trivial I constitue´ par le seul nœud
initial. Nous discutons successivement le pourcentage d’ame´lioration du taux d’erreur,
le pourcentage de re´duction de l’entropie, l’ame´lioration de l’ajustement et les pseudo
R2. Les mesures conside´re´es ici pour la comparaison entre l’arbre induit et le nœud
initial se ge´ne´ralisent aise´ment, bien que nous ne le traitions pas explicitement, au cas
ge´ne´ral de la comparaison de deux mode`les dont l’un est inclus dans l’autre.
4.2.1 Remarque sur le pourcentage de re´duction du taux d’erreur
Bien qu’on ne s’inte´resse pas ici a` la pre´diction de valeurs individuelles, nous aime-
rions souligner que l’ide´e de comparer la performance du mode`le avec le mode`le qui ne
tient pas compte des pre´dicteurs est e´galement pertinente en terme de taux d’erreur.
On peut noter que, sur l’e´chantillon d’apprentissage, le pourcentage de re´duction de
l’erreur de classification correspond a` la mesure d’association λy|partition de Guttman
(1941) et Goodman et Kruskal (1954) entre la variable re´ponse y et la partition de´finie
par le graphe induit. Il existe une forme analytique de la variance asymptotique de cette
mesure qui permet d’en tester la significativite´ sous certaines conditions de re´gularite´
(Goodman et Kruskal, 1972; Olszak et Ritschard, 1995). Nous n’approfondissons pas
cet aspect ici, notre objectif e´tant les qualite´s descriptives du graphe induit plutoˆt que
ses qualite´s pre´dictives.
4.2.2 Gain d’information
Le gain d’information peut eˆtre mesure´ par la re´duction de l’entropie de la distribu-
tion de la variable re´ponse que permet la connaissance des classes de la partition de´finie
par l’arbre induit. Pre´cisons que nous nous inte´ressons a` la re´duction globale d’entro-
pie que permet l’arbre par rapport a` la distribution marginale, c’est-a`-dire celle dans le
nœud initial. Le gain discute´ ici se distingue donc du gain partiel et conditionnel a` un
RNTI - 2
G. Ritschard et D.A. Zighed
nœud que certains algorithmes cherchent a` maximiser a` chaque e´tape de construction
de l’arbre.
Le gain d’information relativement au nœud initial est mesure´ par exemple par les
deux indicateurs suivants :
τˆM |I =
n
∑
i
∑
j
nˆ2ij
n·j
−∑i n2i·
n2 −∑i n2i· (6)
uˆM |I =
∑
i
ni·
n log2
ni·
n −
∑
j
n·j
n
∑
i
nˆij
n·j
log2
nˆij
n·j∑
i
ni·
n log2
ni·
n
(7)
Le τˆM |I est la deuxie`me mesure d’association nominale propose´e par Goodman et
Kruskal (1954). Il mesure la proportion de re´duction de l’entropie quadratiqueHQ(p) =∑
i pi(1− pi), connue aussi comme l’indice de variation de Gini. Le uˆM |I est connu en
statistique sous le nom de coefficient d’incertitude de Theil (1967, 1970). Il mesure la
proportion de re´duction de l’entropie de Shannon HS(p) = −
∑
i pi log2 pi.
Pour l’arbre M de la figure 1, on trouve par exemple τˆM |I = 0.145 et uˆM |I = 0.132,
valeurs qui indiquent une re´duction d’entropie d’environ 14%. Si l’on compare ces
valeurs a` celles du mode`le sature´, soit respectivement τˆS|I = 0.146 et uˆS|I = 0.134,
il apparaˆıt que l’arbre capte a` peu pre`s toute l’information que l’on peut tirer des
attributs pre´dictifs retenus.
Pour tester la significativite´ statistique du gain d’information, soit les hypothe`ses
H0 : τM |I = 0 et H0 : uM |I = 0, une possibilite´ est d’utiliser les variances asymp-
totiques que l’on trouve dans la litte´rature (voir par exemple Olszak et Ritschard,
1995) et une approximation par la loi normale. Il est pre´fe´rable cependant d’utiliser les
transformations suivantes des indicateurs :
C(I|M) = (n− 1)(`− 1) τˆM |I (8)
G2(I|M) =
(
− 2
∑
i
ni· log(ni·/n)
)
uˆM |I (9)
La premie`re C est due a` Light et Margolin (1971) qui montrent que dans le cas ou` le
tableau pre´dit est le mode`le sature´ (M = S), C(I|S) est, sous l’hypothe`se d’inde´pen-
dance (τS|I = 0), asymptotiquement distribue´e comme un χ2 a` dI degre´s de liberte´.
Dans le cas d’un mode`leM plus restrictif que S, il suffit d’adapter les degre´s de liberte´.
Ainsi, de fac¸on ge´ne´rale C(I|M) suit un χ2 avec dI − dM degre´s de liberte´.
La transformation du coefficient uˆM |I montre que tester la significativite´ de uM |I
est e´quivalent a` tester la significativite´ de la diffe´rence d’ajustement entre I et M avec
G2(I|M) = G2(I) − G2(M). Les deux transformations (8) et (9) suivent donc, sous
H0, asymptotiquement la meˆme loi du χ2 a` dI − dM degre´s de liberte´. Le test avec ces
statistiques est plus puissant qu’avec une approximation normale de τˆI|M ou uˆI|M .
Pour notre exemple d’arbre induit, on trouve C(I|M) = 14.32 et G2(I|M) = 18.36.
Ces valeurs sont tre`s grandes compte tenu des degre´s de liberte´ dI − dM = 5− 3 = 2.
Elles confirment donc la significativite´ statistique du gain d’information de l’arbre par
rapport au mode`le d’inde´pendance.
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
4.2.3 Pseudo R2
Dans une optique purement descriptive, on peut envisager, comme on le fait par
exemple dans la mode´lisation log-line´aire, des pseudo R2 qui mesurent la proportion
de la de´viance entre inde´pendance I et arbre sature´ que l’arbre d’induction reproduit.
On peut par exemple utiliser le pseudo R2
R2 = 1− G
2(M)
G2(I)
ou sa version corrige´e des degre´s de liberte´
R2ajust = 1−
G2(M)/dM
G2(I)/dI
Pour notre exemple, on a G2(I) = 18.55, dI = 5, G2(M) = .18 et dM = 3, d’ou`
R2 = .99 et R2ajust = .984. Ces valeurs confirment que l’arbre capte presque le 100% de
l’e´cart entre l’inde´pendance et la table cible repre´sente´e par l’arbre sature´.
Dans une optique de re´duction d’entropie, nous proposons comme alternative au
pseudo R2 ci-dessus, de calculer la part de la proportion maximale de re´duction d’entro-
pie possible que l’on atteint avec l’arbre induit. La proportion maximale de re´duction
d’entropie est obtenue avec la partition la plus fine, c’est-a`-dire le mode`le sature´. Elle
correspond a` τˆS|I pour l’entropie quadratique et uˆS|I pour l’entropie de Shannon. Les
parts de ces valeurs atteintes avec l’arbre induit sont donc :
R2τ =
τˆM |I
τˆS|I
et R2u =
uˆM |I
uˆS|I
Pour notre exemple, on obtient respectivement R2τ = .993 et R
2
u = .985.
4.3 Ajustement et complexite´
Dans le but de pouvoir arbitrer entre ajustement et complexite´, on peut recourir
aux crite`res d’information AIC d’Akaike (1973) ou au crite`re baye´sien BIC (Schwarz,
1978; Kass et Raftery, 1995).
Dans notre cas, ces crite`res peuvent par exemple s’e´crire :
AIC(M) = G2(M) + 2(q`− q + c)
BIC(M) = G2(M) + (q`− q + c) log(n)
Ici, la complexite´ est repre´sente´e par le nombre q`− q+ c de parame`tres inde´pendants.
Le crite`re BIC pe´nalise plus fortement la complexite´ que le crite`re AIC, la pe´nalisation
augmentant avec le nombre de donne´es n. Notons qu’il existe des formes alternatives
du coefficient BIC. Raftery (1995), par exemple, propose BIC = G2 − d log(n), ou` d
est le nombre de degre´s de liberte´, soit dans notre cas d = (c − q)(` − 1). Comme ce
nombre d diminue d’une unite´ chaque fois que l’on ajoute un parame`tre inde´pendant,
la pe´nalisation reste e´videmment la meˆme. Les deux formulations sont e´quivalentes a`
une translation c` pre`s.
RNTI - 2
G. Ritschard et D.A. Zighed
Ces crite`res d’information offrent une alternative aux tests statistiques pour la se´-
lection de mode`les. Parmi plusieurs mode`les, celui qui minimise le crite`re re´alise le
meilleur compromis entre ajustement et complexite´. Le mode`le qui minimise BIC en
particulier, est, dans une approche baye´sienne, optimal compte tenu de l’incertitude
des mode`les.
Pour illustrer l’utilisation de ces crite`res, on se propose de comparer notre arbre
induit M avec la variante M∗ ou` l’on e´clate le sommet « femme» selon les trois
secteurs P, S, I au lieu du partage binaire entre primaire P et non primaire P¯ . Dans
les deux cas on a n = 100, c = 6 et ` = 2. Pour M , on a q = 3 et donc (q` − q +
c) = 9 et pour M∗, q = 4 et donc (q` − q + c) = 10. Comme G2(M) = 0.18 et
G2(M∗) = .16, on obtient AIC(M) = 18.18 et AIC(M∗) = 20.16. De meˆme, on trouve
BIC(M) = 41.63 et BIC(M∗) = 46.21. Les deux crite`res indiquent que l’arbre M plus
simple est pre´fe´rable a` l’arbre M∗. Le gain en qualite´ d’ajustement de M∗ n’est pas
assez important pour justifier l’accroissement de la complexite´. Remarquons que du
point de vue de ces crite`res d’information, l’arbre induit est supe´rieur tant au mode`le
d’inde´pendance (AIC(I) = 32.55, BIC(I) = 50.78) qu’au mode`le sature´ (AIC(S) = 24,
BIC(S) = 55.26).
5 Illustration
Nous illustrons ici les enseignements apporte´s par les crite`res de qualite´ d’ajuste-
ment propose´s sur un exemple concret. On conside`re pour cela les donne´es relatives aux
762 e´tudiants qui ont commence´ leur premie`re anne´e d’e´tudes a` la Faculte´ des sciences
e´conomiques et sociales de Gene`ve en 1998. Il s’agit de donne´es administratives re´unies
par Petroff et al. (2001). On rapporte quelques re´sultats d’une analyse visant a` e´valuer
les chances de respectivement re´ussir, redoubler ou eˆtre e´limine´ a` la fin de la premie`re
anne´e d’e´tudes selon les caracte´ristiques personnelles portant notamment sur l’origine
et le cursus scolaire. La figure 4 montre l’arbre obtenu avec la proce´dure CHAID (Kass,
1980) imple´mente´e dans Answer Tree (SPSS, 2001). Parmi une trentaine de pre´dicteurs
potentiels, CHAID en a se´lectionne´ 5 dont deux quantitatifs, l’anne´e d’immatricula-
tion a` l’universite´ et l’aˆge a` l’obtention du diploˆme de l’e´cole secondaire. Les cinq
variables avec les discre´tisations et regroupements de modalite´s propose´s par CHAID
sont le type de diploˆme secondaire (3 modalite´s), l’aˆge de son obtention (4), la date
d’immatriculation (datimma, 2), le tronc commun choisi (troncom, 2) et la nationalite´
(nationa, 2). La table cible T de´finie par ces variables contient 88 colonnes. Elle a 3
lignes correspondant aux 3 situations possibles de l’e´tudiant apre`s sa premie`re anne´e
d’e´tude.
Le tableau 4 donne la taille de la partition, la de´viance G2 avec ses degre´s de liberte´
et son degre´ de signification et les crite`res AIC et BIC. Ces valeurs peuvent eˆtre compa-
re´es a` celles de plusieurs variantes. CHAID2 est CHAID sans l’e´clatement du sommet
4 (nationa /∈ {GE, hors Europe}) et CHAID3 sans l’e´clatement des sommets 4 et 5
(nationa ∈ {GE, hors Europe}). Le mode`le Sipina correspond au graphe de la figure 5
obtenu avec la proce´dure Sipina (Sipina for Windows V2.5, 2000; Zighed et Rakotoma-
lala, 2000) qui, comme on peut le voir, autorise e´galement des fusions de sommets. On
donne e´galement les valeurs trouve´es pour les partitions qui donnent respectivement
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
bilan oct.99
dipl. second.regroup.
Adj. P-value=0.0000, Chi-square=50.7197, df=2
économique;moderne,<missing>
AGEDIP
Adj. P-value=0.0090, Chi-square=11.0157, df=1
>20,<missing><=20
classic .latine;scientifique
AGEDIP
Adj. P-value=0.0067, Chi-square=14.6248, df=2
>19(18,19]<=18
étranger,autre;dipl. ing.
nationalité regoup.
Adj. P-value=0.0011, Chi-square=16.2820, df=1
Genève;hors Europe
tronc commun
Adj. P-value=0.0188, Chi-square=5.5181, df=1
sc.socialessc.écon. + HEC
ch-al.+Tessin;Europe;Suisse Romande
date d'immatriculation
Adj. P-value=0.0072, Chi-square=9.2069, df=1
>97<=97
Page 1, 1
Tree 01 - BIL_99
Category % n
echec 27.43 209
redouble 17.06 130
réussi 55.51 423
Total (100.00) 762
Node 0
Category %
echec 22
redouble 19
réussi 57
Total (32
Node 9
Category % n
echec 16.60 41
redouble 11.74 29
réussi 71.66 177
Total (32.41) 247
Node 2
Category % n
echec 23.91 22
redouble 16.30 15
réussi 59.78 55
Total (12.07) 92
Node 8
Category % n
echec 14.53 17
redouble 11.11 13
réussi 74.36 87
Total (15.35) 117
Node 7
Category % n
echec 5.26 2
redouble 2.63 1
réussi 92.11 35
Total (4.99) 38
Node 6
Category % n
echec 40.70 81
redouble 21.61 43
réussi 37.69 75
Total (26.12) 199
Node 1
Category % n
echec 54.88 45
redouble 23.17 19
réussi 21.95 18
Total (10.76) 82
Node 5
Category % n
echec 71.05 27
redouble 13.16 5
réussi 15.79 6
Total (4.99) 38
Node 14
Category % n
echec 40.91 18
redouble 31.82 14
réussi 27.27 12
Total (5.77) 44
Node 13
Category % n
echec 30.77 36
redouble 20.51 24
réussi 48.72 57
Total (15.35) 117
Node 4
Category % n
echec 23.81 20
redouble 19.05 16
réussi 57.14 48
Total (11.02) 84
Node 12
Category % n
echec 48.48 16
redouble 24.24 8
réussi 27.27 9
Total (4.33) 33
Node 11
bilan oct.99
dipl. second.regroup.
Adj. P-value=0.0000, Chi-square=50.7197, df=2
éc
Adj. P-val
<=20
classic .latine;scientifique
AGEDIP
Adj. P-value=0.0067, Chi-square=14.6248, df=2
>19(18,19]<=18
étranger,autre;dipl. ing.
nationalité regoup.
Adj. P-value=0.0011, Chi-square=16.2820, df=1
Genève;hors Europe
tronc commun
Adj. P-value=0.0188, Chi-square=5.5181, df=1
sc.socialessc.écon. + HEC
ch-al.+Tessin;Europe;Suisse Romande
date d'immatriculation
Adj. P-value=0.0072, Chi-square=9.2069, df=1
>97<=97
Page 1, 1
Tree 01 - BIL_99
Fig. 4 – Bilan apre`s une anne´e en SES : arbre CHAID et de´tail de la branche gauche
RNTI - 2
G. Ritschard et D.A. Zighed
Fig. 5 – Graphe induit avec Sipina
les plus petits AIC et BIC. Enfin, le mode`le sature´ correspond a` la partition la plus fine
et le mode`le d’inde´pendance au cas ou` tous les profils sont regroupe´s en seul groupe.
On constate tout d’abord qu’a` l’exception du mode`le d’inde´pendance et de CHAID3
avec un degre´ de signification le´ge`rement infe´rieur a` 5%, tous les mode`les reproduisent
de fac¸on satisfaisante la table T. La simplification de l’arbre CHAID en CHAID2
ou CHAID3 se traduit comme attendu par une de´te´rioration du G2. Les e´carts sont
G2(chaid2|chaid) = 9.5 et G2(chaid3|chaid) = 17.3 qui pour un gain de respective-
ment 2 et 4 degre´s de liberte´ sont clairement significatifs, ce qui valide statistiquement
l’e´clatement des nœuds 4 et 5. Les diffe´rences de G2 avec les autres mode`les qui ne
sont pas des sous graphes de l’arbre CHAID ne peuvent eˆtre teste´s. On peut par contre
comparer avec les AIC ou BIC de ces mode`les. On remarque tout d’abord que CHAID
Mode`le q d G2 sig(G2) AIC BIC
Sature´ 88 0 0 1 528 1751.9
Meilleur AIC 14 148 17.4 1 249.4 787.2
CHAID 9 158 177.9 0.133 390.0 881.3
CHAID2 8 160 187.4 0.068 395.4 877.5
CHAID3 7 162 195.2 0.038 399.2 872.1
Sipina 7 162 185.8 0.097 389.8 862.6
Meilleur BIC 6 164 75.2 1 275.2 738.8
Inde´pendance 1 174 295.1 0.000 475.8 892.3
CHAID2 : CHAID sans e´clatement datimma du sommet 4 (nationa 6= GE, hors Europe)
CHAID3 : CHAID2 sans e´clatement troncom du sommet 5 (nationa= GE, hors Europe)
Tab. 4 – SES 98 : qualite´s d’ajustement d’un choix de mode`les
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
proportion de re´duction relativement
d’entropie au mode`le sature´ pseudo
Mode`le τˆM|I uˆM|I λˆM|I τˆM|I uˆM|I λˆM|I R
2
ajust
Sature´ 0.193 0.197 0.183 1 1 1 1
Meilleur AIC 0.159 0.185 0.179 0.824 0.939 0.978 .941
CHAID 0.094 0.078 0.109 0.487 0.396 0.596 .336
CHAID2 0.086 0.072 0.088 0.446 0.365 0.481 .309
CHAID3 0.08 0.067 0.088 0.415 0.340 0.481 .289
Sipina 0.087 0.073 0.103 0.451 0.371 0.563 .324
Meilleur BIC 0.149 0.147 0.142 0.772 0.746 0.776 .745
Inde´pendance 0 0 0 0 0 0 0
Tab. 5 – SES 98 : mesures de type R2 pour un choix de mode`les
et ses deux variantes ont des AIC et BIC tre`s voisins, sensiblement meilleurs que ceux
du mode`le sature´ et dans une moindre mesure que ceux du mode`le d’inde´pendance. La
partition ge´ne´re´e par Sipina obtient un AIC e´quivalent au mode`le CHAID, mais son
BIC est infe´rieur a` celui des 3 mode`les CHAID. L’e´cart supe´rieur a` 10 traduit, selon
l’e´chelle postule´e par Raftery (1995), une supe´riorite´ tre`s forte de cette partition. On
peut noter toutefois, que les valeurs des AIC et BIC obtenues pour le graphe Sipina
restent tre`s nettement supe´rieures aux valeurs optimales possibles avec les attributs
retenus. Notons cependant que les partitions AIC et BIC optimales ne peuvent eˆtre
de´crites par des arbres, les re`gles (non donne´es ici) qui caracte´risent les classes de ces
partitions consistant en des me´langes de conjonctions (‘et’) et d’alternatives (‘ou’) de
conditions. Les partitions AIC et BIC optimales ont e´te´ obtenues avec la proce´dure
de´crite dans Ritschard (2003).
Le tableau 5 re´capitule les mesures de type R2. Le λˆM |I qui indique la proportion
de re´duction du taux d’erreur sur donne´es d’apprentissage est donne´ pour comparaison
avec les proportions de re´duction d’entropie. Les proportions maximales de re´duction
d’entropie par rapport au mode`le d’inde´pendance sont e´videmment obtenues avec la
partition la plus fine, c’est-a`-dire le mode`le sature´. On note que ces maxima sont infe´-
rieurs a` 1. La part de cette re´duction maximale re´alise´e par chaque mode`le est e´galement
donne´e. On voit que ces dernie`res valeurs sont tre`s similaires au pseudo R2 ajuste´. Elles
nous indiquent par exemple, que les mode`les CHAID et Sipina, malgre´ un ajustement
satisfaisant, ne captent qu’environ 1/3 du potentiel de re´duction d’entropie possible
avec les pre´dicteurs retenus. Les meilleures partitions du point de vue tant du BIC que
de l’AIC font nettement mieux de ce point de vue.
6 E´tude par simulations des statistiques X2 et G2
Nous rapportons ici les principaux enseignements de simulations mene´es pour e´tu-
dier empiriquement la distribution des statistiques du khi-2 conside´re´es a` la section pre´-
ce´dente. L’objectif principal de ces analyses est de conforter empiriquement le nombre
de degre´s de liberte´ que nous avons calcule´, a` savoir (c−q)(`−1). Lebart et al. (2000, p.
362) mentionnent par exemple que des calculs similaires dans le cadre de l’analyse des
RNTI - 2
G. Ritschard et D.A. Zighed
correspondance a donne´ des re´sultats faux. Rappelons que les degre´s de liberte´ d’une
variable du χ2 repre´sentent son espe´rance mathe´matique et la moitie´ de sa variance.
Plusieurs se´ries de simulations ont e´te´ re´alise´es. Nous pre´sentons ici les re´sultats
pour deux tailles de tables T, soit 2 × 6 et 3 × 88, cette dernie`re e´tant la taille de la
table cible de l’illustration de la section 5. Pour chacun des cas, nous avons conside´re´
plusieurs partitions des profils (colonnes des tables). Pour la petite table, le regroupe-
ment en une seule classe (inde´pendance) et une partition en 3 ge´ne´re´e par un arbre.
Pour la grande table, l’inde´pendance et les partitions correspondant a` l’arbre CHAID
et au meilleur BIC de l’illustration SES 98. A chaque fois, nous avons impose´ au niveau
de la population l’e´galite´ des distributions de la re´ponse pour les profils d’une meˆme
classe. Pour la grande table, ceci a e´te´ fait en modifiant la structure de la population
des e´tudiants SES 98. Nous avons d’autre part aussi e´tudie´ le cas d’une population
re´partie uniforme´ment entre les 264 cases de la table (qui ve´rifie ne´cessairement toutes
les contraintes.) Dans chacune des populations ainsi de´finies, nous avons tire´s ale´a-
toirement 200 e´chantillons pour lesquels nous avons calcule´ les statistiques X2 et G2
mesurant l’e´cart entre la table T e´chantillonne´e et la table Tˆ pre´dite en imposant la
partition.
Le tableau 6 donne la valeur moyenne et la demi-variance des 200 X2 et G2 obtenus
dans chaque cas ainsi que l’erreur standard des moyennes. Pour chaque se´rie de simu-
lations sont indique´s la taille q de la partition, le nombre zs de ze´ros structurels et le
nombre the´orique de degre´s de liberte´ d = (c−q)(`−1)−zs. Notons que si l’on peut ici
de´terminer le nombre de ze´ros structurels, ceci n’est pas le cas avec des donne´es re´elles
issues de populations qui restent inconnues.
moyenne variance/2
Mode`le q zs d X2 (err std) G2 (err std) X2 G2
table cible 2× 6
inde´pendance 1 0 5 4.98 (0.22) 4.99 (0.22) 4.69 4.75
arbre 3 0 3 3.05 (0.17) 3.06 (0.17) 2.70 2.70
table cible 3× 88, population SES98, taille e´chantillon 762
inde´pendance 1 0 174 173.4 (1.26) 197.5 (1.46) 157.6 123.1
CHAID 9 0 158 142.9 (1.20) 156.7 (1.13) 142.0 126.0
BIC opt. 6 39 125 123.2 (1.24) 135.5 (1.22) 153.4 146.7
table cible 3× 88, population SES98, taille e´chantillon 100000
inde´pendance 1 0 174 173.4 (1.45) 173.8 (1.45) 207.9 209.7
CHAID 9 0 158 159.0 (1.20) 159.4 (1.19) 144.2 142.1
BIC opt. 6 39 125 127.2 (1.17) 127.7 (1.18) 135.2 138.7
table cible 3× 88, population uniforme, taille e´chantillon 100000
inde´pendance 1 0 174 173.0 (1.32) 173.1 (1.32) 173.7 173.1
CHAID 9 0 158 159.1 (1.24) 159.2 (1.24) 153.2 153.6
BIC opt. 6 0 164 165.2 (1.28) 165.2 (1.28) 163.0 163.2
Tab. 6 – Moyennes et demi-variances observe´es des statistiques X2 et G2. Les valeurs
encadre´es indiquent les moyennes empiriques de X2 et G2 qui s’e´cartent significative-
ment de la valeur d the´orique.
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
100
120
140
160
180
200
220
100 120 140 160 180 200 220
Q obs
Q
 c
hi
-2
80
100
120
140
160
180
200
80 100 120 140 160 180 200
Q obs
Q
 c
hi
-2
Fig. 6 – QQ-plot : a` gauche partition CHAID en 9, e´chantillons de taille 100000, d = 158
degre´s de liberte´ the´oriques et a` droite partition BIC optimale en 6, e´chantillons de
taille 762, d = 125
Les simulations re´alise´es confirment de fac¸on ge´ne´rale le bien fonde´ des re´sultats
the´oriques, en particulier lorsque l’e´chantillon est suffisamment grand pour assurer des
effectifs attendus conse´quents dans chaque cellule. Dans le cas d’e´chantillons de taille
762 pour la table de 3×88, on a en moyenne moins de 3 cas par case, ce qui est insuffisant
pour justifier la distribution du χ2. Ceci se traduit par des e´carts significatifs entre la
valeur moyenne et les degre´s de liberte´ the´oriques. On remarque e´galement que dans
cette situation le G2 exce`de la valeur de X2 de fac¸on importante, de l’ordre de 10%.
Les demi-variances, qui devraient the´oriquement aussi eˆtre e´gales aux degre´s de liberte´,
sont moins convaincantes excepte´ pour la population uniforme et, dans une moindre
mesure, les petites tables.
La figure 6 montre les qq-plots qui comparent les quantiles observe´s (Q obs) des
G2 aux quantiles (Q chi-2) de la distribution du χ2 the´orique pour deux cas : a` gauche
pour la partition CHAID en 9 classes et des e´chantillons de taille 100000, a` droite le cas
de´favorable de la partition BIC optimale avec des e´chantillons de taille 762. Le fait que
sur le graphique de gauche les points soient pratiquement aligne´s sur la droite the´orique
montre que, non seulement la moyenne des G2 correspond aux degre´s de liberte´, mais
que la distribution empirique est tre`s proche de celle du khi-2. Sur le graphique de
droite, on observe que les quantiles observe´s exce`dent syste´matiquement les quantiles
the´oriques. Les points e´tant cependant aligne´s, la forme de χ2 de la distribution ne
semble pas remise en cause. A titre indicatif, un test d’ajustement de Kolmogorov-
Smirnov sur ces deux exemples, donne respectivement un degre´ de signification de 98%
et de 0.0001%. Ce meˆme test de Kolmogorov-Smirnov pour le cas BIC optimal avec
e´chantillons de taille 100000 donne un degre´ de signification de 65%, bien que pour ce
meˆme cas l’e´cart entre la moyenne et les degre´s de liberte´ soit le´ge`rement supe´rieur a`
deux erreurs standards.
Ces analyses par simulations nous invitent a` une certaine prudence en ce qui
concerne les degre´s de signification donne´s dans le cadre de l’illustration de la sec-
tion 5 au tableau 4. Les valeurs indique´es donnent cependant clairement des ordres
RNTI - 2
G. Ritschard et D.A. Zighed
de grandeur raisonnables. Pour le G2, les degre´s de liberte´ observe´s dans nos simula-
tions sont toujours supe´rieurs ou e´gaux aux degre´s de liberte´ the´oriques. Les p-valeurs
calcule´es semblent donc eˆtre des bornes infe´rieures ce qui indiquerait que les tests de
significativite´ sont conservateurs. La pre´sence d’e´ventuels ze´ros structurels non de´cele´s
agit cependant en sens inverse. Une borne supe´rieure de leur nombre zs est donne´e
par le nombre zssup de ze´ros dans la table pre´dite Tˆ. En retenant les degre´s de liberte´
corrige´s (c − q)(` − 1) − zssup, il est ne´anmoins possible de s’assurer des degre´s de
signification conservateurs.
7 Conclusion
Cet article aborde la question de la qualite´ de l’ajustement des arbres d’induction.
Il s’agit d’un aspect peu discute´ dans la litte´rature sur l’extraction de connaissances
alors meˆme que la qualite´ d’ajustement fait partie des outils classiques d’e´valuation de
mode`les en statistique. La qualite´ d’ajustement fournit des indications comple´mentaires
aux indicateurs de qualite´ traditionnellement utilise´s pour les arbres d’induction en
permettant, en particulier, d’e´valuer la pertinence statistique d’un arbre induit.
Concre`tement, nous avons montre´, en introduisant les notions d’arbre sature´ et
d’arbre e´tendu, comment adapter aux arbres d’induction les statistiques du khi-2 de
Pearson et du rapport de vraisemblance utilise´s dans le cadre de la mode´lisation de
tables de contingence. Nous avons e´galement conside´re´ la question de la comparaison
de mode`les pour laquelle la diffe´rence des statistiques G2 du rapport de vraisemblance
permet de tester la significativite´ statistique du gain d’information d’un mode`le par
rapport a` un mode`le de re´fe´rence. Pour la comparaison avec le mode`le d’inde´pendance
ne tenant pas compte des pre´dicteurs, nous avons examine´s divers indicateurs de type
R2. Enfin, nous avons vu que l’on pouvait exploiter les crite`res d’information AIC et
BIC pour guider le choix entre arbres de complexite´ variable.
Ce travail avait pour objectif de montrer comment appliquer des crite`res statistiques
bien e´tablis aux arbres d’induction. Il reste e´videmment encore beaucoup a` faire. D’une
part, il convient de ge´ne´raliser la mise en œuvre des statistiques et indicateurs discute´s
dans des cas concrets, et en particulier de les imple´menter dans une proce´dure de
construction d’arbres d’induction.
L’approche retenue dans cet article qui s’appuie notamment sur les concepts d’arbre
sature´ et d’extension maximale de l’arbre, s’applique lorsque le croisement de toutes
les modalite´s des attributs donne lieu a` un nombre raisonnable de cate´gories. Dans le
cas particulier de variables quantitatives continues, il y a lieu de discre´tiser les valeurs.
La difficulte´ tient ici au fait que, dans les arbres d’induction, la discre´tisation ne se
fait en re`gle ge´ne´rale pas a priori mais est de´termine´e en cours de processus de fac¸on a`
optimiser la discrimination entre classes.
Une approche possible est de retenir la discre´tisation des variables continues de´finie
par l’ensemble des seuils utilise´s par le graphe induit. Les variables continues e´tant
ainsi rendues cate´gorielles, la construction de l’arbre sature´ et de l’arbre e´tendu de-
vient possible et la de´marche pre´ce´dente s’applique. C’est la de´marche que nous avons
adopte´e dans l’illustration de la section 5. Les re´sultats sont alors conditionnels a` la
discre´tisation retenue, ce qui en limite e´videmment la porte´e. On peut songer a` d’autres
RNTI - 2
Qualite´ d’ajustement d’arbres d’induction
approches prenant en particulier en compte le fait que les seuils de discre´tisation sont
e´galement des parame`tres du mode`le. Ceci me´rite cependant une re´flexion approfondie
qui de´passe le cadre de cet article
Enfin, de fac¸on plus ge´ne´rale, il reste encore beaucoup de questions ouvertes sur la
pertinence statistique des arbres d’induction. Par exemple, la mesure de la fiabilite´ des
estimations des parame`tres du mode`le de reconstruction (1) issu de l’arbre et celle de
la stabilite´ de l’arbre induit sont a` nos yeux essentielles pour appre´cier la confiance a`
accorder a` un arbre.
Re´fe´rences
Agresti, A. (1990). Categorical Data Analysis. New York : Wiley.
Akaike, H. (1973). Information theory and an extension of the maximum likelihood
principle. In B. N. Petrox et F. Caski (Eds.), Second International Symposium on
Information Theory, pp. 267. Budapest : Akademiai Kiado.
Bishop, Y. M. M., S. E. Fienberg, et P. W. Holland (1975). Discrete Multivariate
Analysis. Cambridge MA : MIT Press.
Breiman, L., J. H. Friedman, R. A. Olshen, et C. J. Stone (1984). Classification And
Regression Trees. New York : Chapman and Hall.
Cressie, N. et T. R. Read (1984). Multinomial goodness-of-fit tests. Journal of the
Royal Statistical Society 46, 440–464.
Geurts, P. (2002). Contributions to Decision Tree Induction : Bias/Variance Tradeoff
and Time Series Classification. Lie`ge : Faculte´ des sciences applique´es. PhD Thesis.
Goodman, L. A. et W. H. Kruskal (1954). Measures of association for cross classifica-
tions. Journal of the American Statistical Association 49, 732–764.
Goodman, L. A. et W. H. Kruskal (1972). Measures of association for cross classifica-
tions IV : simplification of asymptotic variances. Journal of the American Statistical
Association 67, 415–421.
Guttman, L. (1941). An outline of the statistical theory of prediction. In P. Horst
et others (Eds.), The Prediction of Personal Adjustment, Volume 8, New York, pp.
253–318. Social Science Research Council.
Kass, G. V. (1980). An exploratory technique for investigating large quantities of
categorical data. Applied Statistics 29 (2), 119–127.
Kass, R. E. et A. E. Raftery (1995). Bayes factors. Journal of the American Statistical
Association 90 (430), 773–795.
Lebart, L., A. Morineau, et M. Piron (2000). Statistique exploratoire multivarie´e (Troi-
sie`me ed.). Paris : Dunod.
Light, R. J. et B. H. Margolin (1971). An analysis of variance for categorical data.
Journal of the American Statistical Association 66 (335), 534–544.
Olszak, M. et G. Ritschard (1995). The behaviour of nominal and ordinal partial
association measures. The Statistician 44 (2), 195–212.
RNTI - 2
G. Ritschard et D.A. Zighed
Petroff, C., A.-M. Bettex, et A. Korffy (2001, Juin). Itine´raires d’e´tudiants a` la faculte´
des sciences e´conomiques et sociales : le premier cycle. Technical report, Universite´
de Gene`ve, Faculte´ SES.
Powers, D. A. et Y. Xie (2000). Statistical Methods for Categorical Data Analysis. San
Diego, CA : Academic Press.
Quinlan, J. R. (1993). C4.5 : Programs for Machine Learning. San Mateo : Morgan
Kaufmann.
Raftery, A. E. (1995). Bayesian model selection in social research. In P. Marsden (Ed.),
Sociological Methodology, pp. 111–163. Washington, DC : The American Sociological
Association.
Rakotomalala, R. et D. A. Zighed (1998). Mesures PRE dans les graphes d’induc-
tion : une approche statistique de l’arbitrage ge´ne´ralite´-pre´cision. In G. Ritschard,
A. Berchtold, F. Duc, et D. A. Zighed (Eds.), Apprentissage : des principes naturels
aux me´thodes artificielles, pp. 37–60. Paris : Hermes Science Publications.
Ritschard, G. (2003). Partition BIC optimale de l’espace des pre´dicteurs. Revue des
nouvelles technologies de l’information 1, 99–110.
Ritschard, G. et D. A. Zighed (2003). Mode´lisation de tables de contingence par arbres
d’induction. Revue des sciences et technologies de l’information — ECA 17 (1–3),
381–392.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6,
461–464.
Sipina for Windows V2.5 (2000). http ://eric.univ-lyon2.fr. Logiciel.
SPSS (Ed.) (2001). Answer Tree 3.0 User’s Guide. Chicago : SPSS Inc.
Theil, H. (1967). Economics and Information Theory. Amsterdam : North-Holland.
Theil, H. (1970). On the estimation of relationships involving qualitative variables.
American Journal of Sociology 76, 103–154.
Zighed, D. A. et R. Rakotomalala (2000). Graphes d’induction : apprentissage et data
mining. Paris : Hermes Science Publications.
Summary
This paper is concerned with the fit of induction trees. Namely, we explore the possi-
bility to measure the goodness-of-fit as it is classically done in statistical modeling. We
show how Chi-square statistics and especially the Log-likelihood Ratio statistic that is
abundantly used in the modeling of contingency tables can be adapted for induction
trees. Not only is the Log-likelihood Ratio statistic suited for testing the fit. It allows
us also to test the significance of the fit improvement provided by the complexifica-
tion of a tree. In addition, we derive from it adapted forms of the Akaike (AIC) and
Bayesian (BIC) information criteria that prove useful in selecting the best compro-
mise tree between fit and complexity. The practical use of the statistics and indicators
proposed is illustrated on an real example.
RNTI - 2
