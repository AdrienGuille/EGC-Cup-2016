Lâ€™apprentissage statistique a` grande eÂ´chelle
LeÂ´on Bottou1, Olivier Bousquet2
1 NEC Labs America, Princeton, USA
2 Google, Zurich, Suisse
ReÂ´sumeÂ´ Depuis une dizaine dâ€™anneÂ´es, la taille des donneÂ´es croit plus vite que la
puissance des processeurs. Lorsque les donneÂ´es disponibles sont pratiquement infi-
nies, câ€™est le temps de calcul qui limite les possibiliteÂ´s de lâ€™apprentissage statistique.
Ce document montre que ce changement dâ€™eÂ´chelle nous conduit vers un compromis
qualitativement diffeÂ´rent dont les conseÂ´quences ne sont pas eÂ´videntes. En particu-
lier, bien que la descente de gradient stochastique soit un algorithme dâ€™optimisation
meÂ´diocre, on montrera, en theÂ´orie et en pratique, que sa performance est excellente
pour lâ€™apprentissage statistique a` grande eÂ´chelle.
1 Introduction
La theÂ´orie de lâ€™apprentissage statistique prend rarement en compte le couË†t des algo-
rithmes dâ€™apprentissage. Vapnik [1] ne sâ€™y inteÂ´resse pas. Valiant [2] exclue les algorithmes
dâ€™apprentissage dont le couË†t croit exponentiellement. Cependant, malgreÂ´ de nombreux
progre`s sur les aspects statistiques [3, 4], peu de reÂ´sultats concerne la complexiteÂ´ des
algorithmes dâ€™apprentissage (e.g., [5].)
Ce document deÂ´veloppe une ideÂ´e simple : une optimisation approximative est souvent
suffisante pour les besoins de lâ€™apprentissage. La premie`re partie reprend la deÂ´composition
de lâ€™erreur de preÂ´vision proposeÂ´e dans [6] dans laquelle un terme suppleÂ´mentaire qui deÂ´crit
les conseÂ´quences de lâ€™optimisation approximative. Dans le cas de lâ€™apprentissage a` petite
eÂ´chelle, cette deÂ´composition deÂ´crit le compromis habituel entre approximation et esti-
mation. Dans le cas de lâ€™apprentissage a` grande eÂ´chelle, elle deÂ´crit une situation plus
complexe qui deÂ´pend en particulier du couË†t de calcul associeÂ´ a` lâ€™algorithme dâ€™apprentis-
sage. La seconde partie explore les proprieÂ´teÂ´s asymptotiques de lâ€™apprentissage a` grande
eÂ´chelle lorsque lâ€™on utilise diverses meÂ´thodes dâ€™optimisation. Ces reÂ´sultats montrent clai-
rement que le meilleur algorithme dâ€™optimisation nâ€™est pas neÂ´cessairement le meilleur
algorithme dâ€™apprentissage. Finalement, cette analyse est confirmeÂ´e par quelques compa-
raisons expeÂ´rimentales.
2 Optimisation approximative
Comme [7, 1], consideÂ´rons un espace de paires entreÂ´es-sorties (x, y) âˆˆ X Ã— Y eÂ´quippeÂ´
dâ€™une loi jointe de probabiliteÂ´ P (x, y). La loi conditionelle P (y|x) repreÂ´sente la relation
inconnue qui lie entreÂ´es et sorties. Une fonction de perte `(yË†, y) mesure lâ€™eÂ´cart entre la
sortie preÂ´dite yË† et la sortie observeÂ´e y. Notre objectif est la fonction f âˆ— qui minimise le
risque moyen
E(f) =
âˆ«
`(f(x), y) dP (x, y) = E [`(f(x), y)],
cÂ© Revue MODULAD, 2010 -61- NumeÂ´ro 42
câ€™est a` dire,
f âˆ—(x) = argmin
yË†
E [`(yË†, y)|x].
Bien que la distribution P (x, y) soit inconnue, on nous donne un eÂ´chantillon S composeÂ´ de
n exemples dâ€™apprentissage (xi, yi), i = 1 . . . n tireÂ´s indeÂ´pendemment de cette loi inconnue.
Nous pouvons alors deÂ´finir le risque empirique
En(f) =
1
n
nâˆ‘
i=1
`(f(xi), yi) = En[`(f(x), y)].
Notre principe dâ€™apprentissage consiste ensuite a` choisir une famille F de fonctions de
preÂ´vision, et a` rechercher celle qui minimise le risque empirique : fn = argminfâˆˆF En(f).
Lorsque la famille F est suffisamment restrictive cette approche est justifieÂ´e par des
reÂ´sultats combinatoires bien connus [1]. Comme il nâ€™est pas vraisemblable que la famille
de fonction F contienne la fonction optimale f âˆ—, nous appelons f âˆ—F = argminfâˆˆF E(f)
la meilleure fonction au sein de la famille F . Afin de simplifier lâ€™analyse, nous suppose-
rons que f âˆ—, f âˆ—F et fn sont bien deÂ´finies et uniques.
Nous pouvons alors deÂ´composer lâ€™exceÂ´dent dâ€™erreur
E [E(fn)âˆ’ E(fâˆ—)] = E [E(fâˆ—F )âˆ’ E(fâˆ—)]ï¸¸ ï¸·ï¸· ï¸¸ + E [E(fn)âˆ’ E(fâˆ—F )]ï¸¸ ï¸·ï¸· ï¸¸
= Eapp + Eest
, (1)
ou` les espeÂ´rances sâ€™entendent par rapport au tirage aleÂ´atoire des exemples dâ€™apprentis-
sage. Lâ€™erreur dâ€™approximation Eapp mesure comment la solution optimale f âˆ— peut eË†tre
approcheÂ´e par la famille de fonction F . Lâ€™erreur dâ€™estimation Eest mesure ce que lâ€™on perd
en minimisant le risque empirique En(f) a` la place du risque moyen E(f). Lâ€™erreur dâ€™esti-
mation est deÂ´termineÂ´e par le nombre n dâ€™exemples dâ€™apprentissage et par la capaciteÂ´ de la
famille de fonctions [1]. Choisir une famille plus grande3 reÂ´duit lâ€™erreur dâ€™approximation
mais augmente lâ€™erreur dâ€™estimation. Ce compromis a eÂ´teÂ´ eÂ´tudieÂ´ en deÂ´tail [1, 3, 8, 9].
2.1 Erreur dâ€™optimization
Il est souvent couË†teux de calculer fn en minimisant le risque empirique En(f). Comme
le risque empirique En(f) est deÂ´ja` une approximation du risque moyen E(f), il ne devrait
pas eË†tre neÂ´cessaire dâ€™accomplir cette minimisation avec une tre`s grande preÂ´cision. Il est
souvent tentant de stopper un algorithme iteÂ´ratif dâ€™optimisation avant quâ€™il ne converge.
Supposons quâ€™un algorithme approximatif dâ€™optimisation calcule une solution ap-
procheÂ´e fËœn qui minimise la fonction de couË†t avec une toleÂ´rance Ï > 0 preÂ´deÂ´finie.
En(fËœn) â‰¤ En(fn) + Ï
On peut alors deÂ´composer lâ€™exceÂ´dent dâ€™erreur E = E[E(fËœn)âˆ’ E(f âˆ—)] en trois termes :
E = E [E(fâˆ—F )âˆ’ E(fâˆ—)]ï¸¸ ï¸·ï¸· ï¸¸ + E [E(fn)âˆ’ E(fâˆ—F )]ï¸¸ ï¸·ï¸· ï¸¸ + E[E(fËœn)âˆ’ E(fn)]ï¸¸ ï¸·ï¸· ï¸¸
= Eapp + Eest + Eopt
. (2)
3On conside`re souvent une seÂ´rie de familles de fonctions de la forme FB = {f âˆˆ H, â„¦(f) â‰¤ B}.
Pour chaque valeur de lâ€™hyperparame`tre B, la fonction fn est obtenue en minimisant le risque empirique
reÂ´gulariseÂ´ En(f)+ Î»â„¦(f) avec une valeur approprieÂ´e du coefficient de Lagrange Î». On peut alors ajuster
le compromis estimationâ€“approximation en choisissant Î» au lieu de B.
cÂ© Revue MODULAD, 2010 -62- NumeÂ´ro 42
Nous appelons erreur dâ€™optimisation le terme suppleÂ´mentaire Eopt qui mesure lâ€™impact de
lâ€™optimization approximative sur lâ€™erreur de preÂ´vision. Lâ€™ordre de grandeur de ce terme
est bien suË†r comparable a` celui de la toleÂ´rance Ï (see section 3.1.)
2.2 Le compromis approximationâ€“estimationâ€“optimisation
Cette deÂ´composition deÂ´crit un compromis plus complexe car il concerne trois variables
et deux contraintes. Les contraintes sont le nombre maximal dâ€™exemples et le temps calcul
maximal disponibles pour lâ€™apprentissage. Les variables sont la taille de la famille de
fonction F , la toleÂ´rance de lâ€™optimisation Ï, et le nombre n dâ€™exemples effectivement
utiliseÂ´s pour lâ€™apprentissage. Cela conduit au programme suivant :
min
F ,Ï,n
E = Eapp + Eest + Eopt sous les contraintes
{
n â‰¤ nmax
T (F , Ï, n) â‰¤ Tmax (3)
Le nombre effectif dâ€™exemples dâ€™apprentissage est une variable parce quâ€™il peut eË†tre im-
possible de mener a` bien lâ€™optimisation sur tous les exemples dans le temps imparti. Cela
se produit souvent en pratique. La table 1 reÂ´sume comment les termes du programme (3)
varient habituellement lorsque les variables F , n, ou Ï augmentent.
Tab. 1 â€“ Variations des termes de (3) lorsque F , n, ou Ï augmentent.
F n Ï
Eapp (approximation error) â†˜
Eest (estimation error) â†— â†˜
Eopt (optimization error) Â· Â· Â· Â· Â· Â· â†—
T (computation time) â†— â†— â†˜
La solution du programme (3) change consideÂ´rablement selon que la contrainte ac-
tive est celle concernant le nombre dâ€™exemples n < nmax ou celle concernant le temps
dâ€™apprentissage T < Tmax.
â€“ Nous parlerons dâ€™apprentissage a` petite eÂ´chelle lorsque le programme (3) est
contraint par le nombre maximal dâ€™exemples nmax. Comme le temps de calcul nâ€™est
pas limiteÂ´, nous pouvons choisir Ï tres petit et reÂ´duire lâ€™erreur dâ€™optimisation Eopt a`
un niveau insignifiant. Lâ€™exceÂ´dent dâ€™erreur est alors deÂ´termineÂ´ par les erreurs dâ€™ap-
proximation et dâ€™estimation. Il suffit alors de prendre n = nmax pour retrouver le
compromis approximationâ€“estimation habituel.
â€“ Nous parlerons dâ€™apprentissage a` grande eÂ´chelle lorsque le programme (3) est
contraint par le temps de calcul maximal Tmax. Il est alors possible dâ€™atteindre
une meilleure erreur de preÂ´vision avec une optimisation approximative â€“ câ€™est a` dire
en choisissant Ï > 0 â€“ car le temps de calcul ainsi eÂ´conomiseÂ´ nous permet de trai-
ter un plus grand nombre dâ€™exemples dâ€™apprentissage dans le temps imparti Tmax.
Cela deÂ´pend bien suË†r de la forme exacte de la fonction T (F , Ï, n) pour lâ€™algorithme
dâ€™optimisation retenu.
cÂ© Revue MODULAD, 2010 -63- NumeÂ´ro 42
3 Analyse asymptotique
Dans la section preÂ´ceÂ´dente, nous avons eÂ´tendu le compromis statistique habituel afin de
tenir compte de lâ€™erreur dâ€™optimisation. Nous avons deÂ´fini preÂ´ciseÂ´ment la diffeÂ´rence entre
apprentissage a` petite et grande eÂ´chelle. Ce dernier cas est substantiellement diffeÂ´rent
parce quâ€™il faut tenir compte du couË†t de calcul de lâ€™algorithme utiliseÂ´. Afin de clarifier les
compromis de lâ€™apprentissage a` grande eÂ´chelle, nous allons faire plusieurs simplifications.
Il est possible de donner des analyses plus preÂ´cises dans des cas particuliers [10].
â€“ Nous travaillons avec des bornes supeÂ´rieures des erreurs dâ€™approximation et dâ€™es-
timation (2). Ces bornes donnent en fait une bonne ideÂ´e des vitesses de conver-
gence [11, 12, 13, 14], a` un facteur constant pre`s.
â€“ Nous eÂ´tudions seulement les proprieÂ´teÂ´s asymptotiques de la deÂ´composition (2). Pour
reÂ´soudre le programme (3) il nous suffira alors de faire en sorte que les trois erreurs
deÂ´croissent avec des vitesses asymptotiques comparables.
â€“ Nous consideÂ´rons une famille de fonction F fixeÂ´e et nous ignorons donc lâ€™erreur
dâ€™approximation Eapp. Cette partie du compromis recouvre des reÂ´aliteÂ´s pratiques
aussi diverses que choisir les mode`les ou choisir les variables explicatives. Discuter
ces pratiques deÂ´passe les objectifs de ce document.
â€“ Finalement, toujours pour simplifier lâ€™analyse, nous supposons que la famille de
fonctions F est parameÂ´treÂ´e lineÂ´airement par un vecteur w âˆˆ Rd. Nous supposons
eÂ´galement que x, y et w sont borneÂ´s. Il existe alors une constante B telle que
0 â‰¤ `(fw(x), y) â‰¤ B et `(Â·, y) sont des fonctions Lipschitziennes.
Apre`s avoir montreÂ´ comment la convergence uniforme des bornes traditionelles permet
de prendre lâ€™erreur dâ€™optimisation en compte, nous comparerons les proprieÂ´teÂ´s asympto-
tiques de quelques algorithmes dâ€™apprentissage.
3.1 Convergence des erreurs dâ€™estimation et dâ€™optimisation
Lâ€™erreur dâ€™optimization Eopt depend directement de la toleÂ´rance dâ€™optimisation Ï. Ce-
pendant, cette toleÂ´rance borne la quantiteÂ´ empirique En(fËœn) âˆ’ En(fn) alors que lâ€™erreur
dâ€™optimisation concerne sa contre-partie espeÂ´reÂ´e E(fËœn)âˆ’E(fn). Cette section discute lâ€™im-
pact de lâ€™erreur dâ€™optimisation Eopt et de la toleÂ´rance dâ€™optimisation Ï sur les bornes qui
sâ€™appuient sur les concepts de convergence uniforme inventeÂ´s par Vapnik et Chervonen-
kis [1]. Dans la suite, nous utiliserons la lettre c pour deÂ´signer toute constante positive. En
particulier, deux occurences successives de la lettre c peuvent repreÂ´senter des constantes
de valeur diffeÂ´rentes.
3.1.1 Le cas geÂ´neÂ´ral
Comme la dimension de Vapnik-Chervonenkis [1] dâ€™une famille de fonction parameÂ´treÂ´e
lineÂ´airement par w âˆˆ Rd est d+ 1, nous pouvons directement eÂ´crire
E
[
sup
fâˆˆF
|E(f)âˆ’ En(f)|
]
â‰¤ c
âˆš
d
n
,
cÂ© Revue MODULAD, 2010 -64- NumeÂ´ro 42
ou` lâ€™espeÂ´rance sâ€™entend vis-a`-vis du tirage aleÂ´atoire de lâ€™ensemble dâ€™apprentissage.4 Ce
reÂ´sultat donne immeÂ´diatement une borne sur lâ€™erreur dâ€™estimation :
Eest = E
[ (
E(fn)âˆ’ En(fn)
)
+
(
En(fn)âˆ’ En(fâˆ—F )
)
+
(
En(fâˆ—F )âˆ’ E(fâˆ—F )
) ]
â‰¤ 2 E
[
sup
fâˆˆF
|E(f)âˆ’ En(f)|
]
â‰¤ c
âˆš
d
n
.
On peut aussi obtenir une borne sur les erreurs combineÂ´es dâ€™estimation et dâ€™optimisation :
Eest + Eopt = E
[
E(fËœn)âˆ’ En(fËœn)
]
+ E
[
En(fËœn)âˆ’ En(fn)
]
+ E [En(fn)âˆ’ En(fâˆ—F )] + E [En(fâˆ—F )âˆ’ E(fâˆ—F )]
â‰¤ c
âˆš
d
n
+ Ï+ 0 + c
âˆš
d
n
= c
(
Ï+
âˆš
d
n
)
.
Malheureusement, il est bien connu que cette vitesse de convergence est trop pessimiste
pour un grand nombre de cas. Des bornes plus raffineÂ´es sont donc neÂ´cessaires.
3.1.2 Le cas reÂ´alisable
Quand la fonction de perte `(yË†, y) est positive, pour tout Ï„ > 0, les bornes relatives
de convergence uniforme [1] affirment avec probabiliteÂ´ 1âˆ’ eâˆ’Ï„ que
sup
fâˆˆF
E(f)âˆ’ En(f)âˆš
E(f)
â‰¤ c
âˆš
d
n
log
n
d
+
Ï„
n
.
Ce reÂ´sultat est tre`s utile car il deÂ´crit une convergence acceÂ´leÂ´reÂ´e O(log n/n) dans le cas
reÂ´alisable, câ€™est a` dire lorsque `(fn(xi), yi) = 0 pour tous les exemples dâ€™apprentissage.
Nous avons alors En(fn) = 0, En(fËœn) â‰¤ Ï et pouvons eÂ´crire
E(fËœn)âˆ’ Ï â‰¤ c
âˆš
E(fËœn)
âˆš
d
n
log
n
d
+
Ï„
n
.
En interpreÂ´tant ce reÂ´sultat comme une ineÂ´galite polynomiale du second degreÂ´,
E(fËœn) â‰¤ c
(
Ï+
d
n
log
n
d
+
Ï„
n
)
.
En inteÂ´grant cette ineÂ´galite avec les technique ordinaires (voir [15] par exemple), on ob-
tient une vitesse de convergence acceÂ´leÂ´reÂ´e pour les erreurs combineÂ´es dâ€™estimation et
dâ€™optimisation :
Eest + Eopt = E
[
E(fËœn)âˆ’ E(fâˆ—F )
]
â‰¤ E
[
E(fËœn)
]
= c
(
Ï+
d
n
log
n
d
)
.
4Bien que les bornes originales de Vapnik et Chervonenkis aient la forme c
âˆš
d
n log
n
d , on peut eÂ´liminer
le terme logarithmique avec la technique de â€œchainingâ€ (e.g., [12].)
cÂ© Revue MODULAD, 2010 -65- NumeÂ´ro 42
3.1.3 Convergence acceÂ´leÂ´reÂ´e
De nombreux auteurs (e.g., [12, 4, 14]) proposent des bornes avec convergence acceÂ´leÂ´reÂ´e
dans des conditions plus geÂ´neÂ´rales. On peut en effet montrer que
Eapp + Eest â‰¤ c
(
Eapp +
(
d
n
log
n
d
)Î± )
for
1
2
â‰¤ Î± â‰¤ 1 . (4)
si la variance de la fonction de perte satisfait
âˆ€f âˆˆ F E
[(
`(f(X), Y )âˆ’ `(f âˆ—F(X), Y )
)2] â‰¤ c ( E(f)âˆ’ E(f âˆ—F) )2âˆ’ 1Î± . (5)
La vitesse asymptotique de convergence de (4) est alors donneÂ´e par lâ€™exposant Î± qui
apparaË†Ä±t dans la majoration de la variance (5). Il y a deux facÂ¸ons importantes dâ€™eÂ´tablir
une telle majoration :
â€“ La premie`re facÂ¸on exploite la convexiteÂ´ stricte de certaines fonctions de perte [14,
theÂ´ore`me 12]. Par exemple, Lee et al. [16] eÂ´tablissent une vitesse O(log n/n) quand
on utilise une fonction de perte quadratique `(yË†, y) = (yË† âˆ’ y)2.
â€“ La deuxie`me facÂ¸on consite a` faire des hypothe`ses sur la distribution des donneÂ´es.
Par exemple, dans le cas de proble`mes de reconnaissance de formes, la â€œcondition
de Tsybakovâ€ deÂ´crit comment les distribution conditionnelles P (y|x) se croisent
au voisinage de la frontie`re de deÂ´cision optimale [13, 14]. Le cas reÂ´alisable discuteÂ´
section 3.1.2 en est un cas particulier.
Les techniques illustreÂ´es sections 3.1.1 et 3.1.2 permettent dâ€™englober lâ€™erreur dâ€™opti-
misation dans ces bornes acceÂ´leÂ´reÂ´es malgreÂ´ leur complexiteÂ´ accrue. On obtient alors une
borne combineÂ´e de la forme
E = Eapp + Eest + Eopt = E
[
E(fËœn)âˆ’ E(fâˆ—)
]
â‰¤ c
(
Eapp +
(
d
n
log
n
d
)Î±
+ Ï
)
. (6)
Par exemple, Massart [15, theÂ´ore`me 4.2] donne un reÂ´sultat geÂ´neÂ´ral avec Î± = 1. En
combinant ce reÂ´sultat avec des bornes connues sur la capaciteÂ´ des classes de fonctions
lineÂ´airement parameÂ´treÂ´es (e.g., [12]), on obtient
E = Eapp + Eest + Eopt = E
[
E(fËœn)âˆ’ E(fâˆ—)
]
â‰¤ c
(
Eapp + d
n
log
n
d
+ Ï
)
. (7)
Le lecteur inteÂ´reÂ´sseÂ´ trouvera eÂ´galement des bornes comparables dans [17, 4].
3.2 Algorithmes dâ€™optimisation par descente de gradient
Nous sommes maintenant en mesure de comparer les proprieÂ´teÂ´s asymptotiques pour
lâ€™apprentissage de trois versions de lâ€™optimisation par descente de gradient. Rappelons
que la famille de fonction F est parameÂ´triseÂ´e lineÂ´airement par w âˆˆ Rd. Soient wâˆ—F et
wn les parame`tres correspondant aux fonctions f
âˆ—
F et fn deÂ´finies section 2. Nous faisons
lâ€™hypothe`se que la fonction w 7â†’ `(fw(x), y) est convexe et deux fois diffeÂ´rentiable avec
des deÂ´riveÂ´es secondes continues. Grace a` cette convexiteÂ´, le fonction de couË†t empirique
C(w) = En(fw) posse`de un minimum unique.
cÂ© Revue MODULAD, 2010 -66- NumeÂ´ro 42
Deux matrices jouent un roË†le important dans cette analyse : la matrice Hessienne H
et la matrice G de covariance des gradients a` lâ€™optimum empirique.
H =
âˆ‚2C
âˆ‚w2
(wn) = En
[
âˆ‚2`(fwn(x), y)
âˆ‚w2
]
, (8)
G = En
[(
âˆ‚`(fwn(x), y)
âˆ‚w
)(
âˆ‚`(fwn(x), y)
âˆ‚w
)â€² ]
. (9)
Afin de reÂ´sumer lâ€™information contenue dans ces deux matrices, nous supposons quâ€™il
existe des constantes Î»max â‰¥ Î»min > 0 et Î½ > 0 telles que lâ€™on puisse choisir, pour tout
Î· > 0, un nombre dâ€™exemples suffisament grand pour faire en sorte que lâ€™assertion suivante
soit vraie avec une probabiliteÂ´ plus grande que 1âˆ’ Î· :
tr(GHâˆ’1) â‰¤ Î½ et Spectre(H) âŠ‚ [Î»min , Î»max ] (10)
Le rapport de conditionnement Îº = Î»max/Î»min est un bon indicateur de la difficulteÂ´
du proble`me dâ€™optimisation [18]. Lâ€™hypothe`se Î»min > 0 permet dâ€™eÂ´viter des complica-
tions dans le cas de lâ€™algorithme de gradient stochastique. Cette condition nâ€™implique une
convexiteÂ´ stricte que dans un voisinage de lâ€™optimum. Si la fonction de couË†t eÂ´tait partout
convexe, lâ€™argument de [14, theÂ´ore`me 12] donnerait une convergence acceÂ´leÂ´reÂ´e avec Î± â‰ˆ 1
pour (4) et (6). Ce nâ€™est pas le cas, par exemple, lorsque la fonction de perte ` est obtenue
en lissant localement une fonction `(z, y) = max{0, 1âˆ’ yz}. La fonction de couË†t est alors
lineÂ´aire par morceaux avec des coins et des areË†tes lisseÂ´s. Bien que cette fonction ne soit
pas partout strictement convexe, son optimum est vraisemblablement atteint sur un coin
lisseÂ´ avec une matrice Hessienne non singulie`re.
Les trois algorithmes que nous eÂ´tudions dans cette section utilisent de lâ€™information
sur le gradient de la fonction de couË†t C(w) pour mettre a` jour iteÂ´rativement leur estimeÂ´e
courante w(t) du vecteur de parame`tres optimaux.
â€“ La Descente de Gradient (GD) consiste a` iteÂ´rer
w(t+ 1) = w(t)âˆ’ Î·âˆ‚C
âˆ‚w
(w(t)) = w(t)âˆ’ Î· 1
n
nâˆ‘
i=1
âˆ‚
âˆ‚w
`
(
fw(t)(xi), yi
)
ou` le gain Î· > 0 est suffisament faible. Lâ€™algorithm GD posse`de une vitesse de conver-
gence â€œlineÂ´aireâ€ [18]. Lorsque Î· = 1/Î»max, cet algorithme atteint une preÂ´cision Ï
apre`s O(Îº log(1/Ï)) iteÂ´rations. Le nombre exact dâ€™iteÂ´rations depend bien suË†r du
choix des parame`tres initiaux w(0).
â€“ La Descent de Gradient de Second Ordre (2GD) consiste a` iteÂ´rer
w(t+ 1) = w(t)âˆ’Hâˆ’1âˆ‚C
âˆ‚w
(w(t)) = w(t)âˆ’ 1
n
Hâˆ’1
nâˆ‘
i=1
âˆ‚
âˆ‚w
`
(
fw(t)(xi), yi
)
ou` la matrice Hâˆ’1 est lâ€™inverse de la matrice Hessienne (8). Câ€™est un cas plus favo-
rable que lâ€™algorithme de Newton parce nous nâ€™eÂ´valuons pas la matrice Hessienne
locale a` chaque iteÂ´ration, mais nous supposons simplement que la matrice Hes-
sienne optimale nous est donneÂ´e par avance. Lâ€™algorithme 2GD posse`de une vitesse
cÂ© Revue MODULAD, 2010 -67- NumeÂ´ro 42
de convergence â€œsuperlineÂ´aireâ€ et meË†me â€œquadratiqueâ€ [18] : une seule iteÂ´ration suf-
fit lorsque le couË†t est quadratique. Dans le cas geÂ´neÂ´ral, cet algorithme atteint une
preÂ´cision Ï apre`s au plus O(log log(1/Ï)) iteÂ´rations.
â€“ La Descente Stochastique de Gradient (SGD) consiste, pour chaque iteÂ´ration,
a` tirer un example dâ€™apprentissage (xt, yt) au hasard, et a` mettre a` jour w(t) sur la
base du gradient de la fonction de perte pour cet exemple seulement.
w(t+ 1) = w(t)âˆ’ Î·
t
âˆ‚
âˆ‚w
`
(
fw(t)(xt), yt
)
.
Les w(t) forment donc un processus stochastique induit par le tirage aleÂ´atoire
dâ€™un nouvel exemple a` chaque iteÂ´ration. Murata [19, section 2.2] en a calculeÂ´ la
moyenne ES[w(t)] et la variance VarS[w(t)]. En appliquant ce reÂ´sultat a` la distribu-
tion discre`te engendreÂ´e par lâ€™ensemble dâ€™apprentissage, en prenant Î· = 1/Î»min, et
en deÂ´finissant Î´w(t) = w(t)âˆ’ wn, on obtient Î´w(t)2 = O(1/t).
Nous pouvons alors eÂ´crire
ES [C(w(t))âˆ’ inf C ] = ES
[
tr
(
H Î´w(t) Î´w(t)â€²
)]
+ o
(
1
t
)
= tr
(
H ES [Î´w(t)]ES [Î´w(t)]â€² +H VarS [w(t)]
)
+ o
(
1
t
)
â‰¤ tr(GH)t + o
(
1
t
) â‰¤ Î½Îº2t + o(1t ) .
(11)
Lâ€™algorithme SGD atteint donc une preÂ´cision moyenne Ï apre`s au plus Î½Îº2/Ï+o(1/Ï)
iteÂ´rations. Sa convergence est en fait limiteÂ´e par le bruit stochastique induit par le
choix aleÂ´atoire dâ€™un exemple unique a` chaque iteÂ´ration. Ni la valeur initiale w(0) du
parame`tre, ni le nombre total dâ€™exemples dâ€™apprentissage nâ€™apparaissent dans cette
borne. Lorsque lâ€™ensemble dâ€™apprentissage est grand, il est possible dâ€™atteindre la
preÂ´cision Ï viseÂ´e sans meË†me avoir visiteÂ´ tous les exemples dâ€™apprentissage. Câ€™est en
fait une forme de borne sur la geÂ´neÂ´ralisation.
Les trois premie`res colonnes de la table 2 donnent, pour chaque algorithme, le temps
requis pour une iteÂ´ration, le nombre dâ€™iteÂ´rations requises pour atteindre une preÂ´cision
dâ€™optimisation preÂ´deÂ´finie Ï, et leur produit, câ€™est a` dire le temps requis pour atteindre
cette preÂ´cision. Ces reÂ´sultats asymptotiques sont valides avec une probabiliteÂ´ 1 parce que
la probabiliteÂ´ de leur neÂ´gation est infeÂ´rieure a` Î· pour tout Î· > 0.
La dernie`re colonne majore le temps neÂ´cessaire pour ramener lâ€™exceÂ´dent dâ€™erreur de
preÂ´vision en dessous de la valeur c (Eapp + Îµ) ou` c est la constante qui apparaË†Ä±t dans la
borne (6). On calcule cela en observant que choisir Ï âˆ¼ ( dn log nd )Î± dans (6) donne une
vitesse asymptotique de convergence optimale pour Îµ avec un temps de calcul minimal.
On utilise ensuite les eÂ´quivalences asymptotiques Ï âˆ¼ Îµ et n âˆ¼ d
Îµ1/Î±
log 1Îµ .
ReÂ´ciproquement, on peut consideÂ´rer la valeur de  pour laquelle cette dernie`re colonne
est eÂ´gale a` Tmax. Cette valeur est alors le meilleur exceÂ´dent dâ€™erreur de preÂ´vision que
chaque algorithme peut atteindre dans la limite de temps de calcul Tmax . Câ€™est donc la
solution du programme (3) pour un proble`me dâ€™apprentissage a` grande eÂ´chelle satisfaisant
nos hypothe`ses simplificatrices.
Ces reÂ´sultats asymptotiques montrent clairement que la performance de preÂ´vision des
syste`mes dâ€™apprentissage a` grande eÂ´chelle deÂ´pend a` la fois des proprieÂ´teÂ´s statistiques du
mode`le et des proprieÂ´teÂ´s calculatoires de lâ€™algorithme dâ€™optimisation retenu. Cette double
deÂ´pendence conduit a` des reÂ´sultats surprenants.
cÂ© Revue MODULAD, 2010 -68- NumeÂ´ro 42
Tab. 2 â€“ Comportement asymptotique (avec probabiliteÂ´ 1) des trois algorithmes
de descente de gradient. Il est inteÂ´ressant de comparer les deux dernie`res colonnes
(temps requis pour optimiser avec une toleÂ´rance Ï, et temps requis pour atteindre
un exceÂ´dent dâ€™erreur infeÂ´rieur a` ). LeÂ´gende : n nombre dâ€™exemples ; d dimension du
vecteur de parame`tres ; Îº, Î½ voir equation (10).
Algo. CouË†t dâ€™une IteÂ´rations pour Temps pour Temps pour atteindre
iteÂ´ration atteindre Ï atteindre Ï E â‰¤ c (Eapp + Îµ)
GD O(nd) O
(
Îº log 1Ï
)
O
(
ndÎº log 1Ï
)
O
(
d2 Îº
Îµ1/Î±
log2 1Îµ
)
2GD O(d2 + nd) O(log log 1Ï) O((d2 + nd) log log 1Ï) O( d2Îµ1/Î± log 1Îµ log log 1Îµ)
SGD O(d) Î½Îº2Ï + o
(
1
Ï
)
O
(
dÎ½Îº2
Ï
)
O
(
d Î½ Îº2
Îµ
)
â€“ Le reÂ´sultat pour SGD ne deÂ´pend pas du coefficient Î± de la vitesse asymptotique
dâ€™estimation statistique. Lorsque ce coefficient est meÂ´diocre, il est moins neÂ´cessaire
dâ€™optimiser avec preÂ´cision, ce qui laisse le temps de traiter plus dâ€™exemples.
â€“ Utiliser un algorithme dâ€™optimisation superlineÂ´aire nâ€™ameÂ´liore pas sensiblement la
vitesse de convergence asymptotique de lâ€™exceÂ´dent dâ€™erreur de preÂ´vision Îµ. Bien
que lâ€™algorithme superlineÂ´aire 2GD ameÂ´liore le terme logarithmique, la vitesse dâ€™ap-
prentissage est domineÂ´e par le terme polynomial en (1/Îµ). Utiliser un algorithme
dâ€™optimisation sophistiqueÂ´ est souvent moins efficace que travailler les constantes
d, Îº et Î½ avec des meÂ´thodes simples de preÂ´conditionnement ou avec une meilleure
impleÂ´mentation [20].
â€“ Lâ€™algorithme SGD offre a` la fois la pire vitesse dâ€™optimisation et la meilleure vitesse
dâ€™apprentissage. Cela avait eÂ´teÂ´ preÂ´dit theÂ´oriquement et observeÂ´ expeÂ´rimentalement
dans le cas dâ€™un algorithme de gradient stochastique de second ordre [21].
Au contraire, dans le cas de lâ€™apprentissage a` petite eÂ´chelle, on peut reÂ´duire lâ€™erreur
dâ€™optimisation Eopt a` des niveaux insignifiants. La performance en preÂ´vision est alors
deÂ´termineÂ´e uniquement par les proprieÂ´teÂ´s statistiques du mode`le.
4 ExpeÂ´riences
Cette section vise a` confirmer les reÂ´sultats preÂ´ceÂ´dents avec quelques expeÂ´riences simples
sur une taË†che bien connue de cateÂ´gorisation de documents. Cette taË†che consiste a` identifier
les documents appartenant a` la cateÂ´gorie CCAT dans le corpus RCV1-v2 [22] a` lâ€™aide
dâ€™un seÂ´parateur a` vaste marge (SVM). Les programmes utiliseÂ´s et quelques reÂ´sultats
suppleÂ´mentaires sont disponibles sur le site web http://leon.bottou.org/projects/sgd.
Afin dâ€™augmenter le nombre dâ€™exemples dâ€™apprentissage, nous avons renverseÂ´ les roË†les
des ensembles dâ€™apprentissage et de test speÂ´cifieÂ´s par [22]. Nous avons ainsi 781265
exemples dâ€™apprentissage et 23149 exemples reÂ´serveÂ´s pour tester la performance en
preÂ´diction. Chaque document est repreÂ´senteÂ´ par 47152 variables explicatives que nous
avons recalculeÂ´es afin que leur normalisation ne deÂ´pende que de nos exemples dâ€™appren-
cÂ© Revue MODULAD, 2010 -69- NumeÂ´ro 42
50
100
0.1 0.01 0.001 0.0001 1eâˆ’05 1eâˆ’07 1eâˆ’08 1eâˆ’09
Training time (secs)
1eâˆ’06
Optimization accuracy (trainingCostâˆ’optimalTrainingCost) 
  TRON
SGD
0.25 Testing loss
0.20
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.001 0.01 0.1 10 100 1000
Testing loss
1
n=30000
n=100000
n=300000
n=781265n=10000
Training time (secs)
SGD
CONJUGATE GRADIENTS
Fig. 1 â€“ Temps dâ€™apprentissage et
couË†t de preÂ´vision en fonction de la
preÂ´cision Ï de lâ€™optimisation pour SGD
et TRON [23].
Fig. 2 â€“ CouË†t de preÂ´vision en fonction
du temps dâ€™apprentissage pour SGD et
pour lâ€™algorithme CG appliqueÂ´ a` une
fraction des exemples.
Tab. 3 â€“ ReÂ´sultats obtenus avec une SVM lineÂ´aire sur le corpus RCV1.
Model Algo. Temps CouË†t Erreur de
dâ€™apprentissage preÂ´vision
CouË†t SVM, Î» = 10âˆ’4
Voir [24, 25].
SVMLight 23,642 secs 0.2275 6.02%
SVMPerf 66 secs 0.2278 6.03%
SGD 1.4 secs 0.2275 6.02%
CouË†t logistique, Î» = 10âˆ’5
Voir [23].
TRON (-e .01) 30 secs 0.18907 5.68%
TRON (-e .001) 44 secs 0.18890 5.70%
SGD 2.3 secs 0.18893 5.66%
tissage. Nous utilisons une fonction de discrimination lineÂ´aire avec la fonction de perte
â€œcharnie`reâ€ traditionnellement utiliseÂ´e pour les SVMs.
min
w
C(w, b) =
Î»
2
+
1
n
nâˆ‘
i=1
`(yt(wxt + b)) avec `(z) = max{0, 1âˆ’ z} .
Les deux premie`re lignes de la table 3 reproduisent des reÂ´sultats anteÂ´rieurs [24] obtenus
sur les meË†mes donneÂ´es avec la meË†me valeur de lâ€™hyperparame`tre Î». La troisie`me ligne
donne les reÂ´sultats obtenus avec un algorithme SGD dont chaque iteÂ´ration consiste a`
tirer un example dâ€™apprentissage aleÂ´atoire (xt, yt) âˆˆ R47152 Ã— {Â±1} et a` mettre a` jour le
parame`tre w de la fonction discriminante :
wt+1 = wt âˆ’ Î·t
(
Î»w +
âˆ‚`(yt(wxt + b))
âˆ‚w
)
avec Î·t =
1
Î»(t+ t0)
.
Le biais b est mis a` jour de facÂ¸on identique. Le gain Î·t retenu est une approximation du
gain optimal (voir section 3.2 ) dans laquelle nous remplacÂ¸ons la plus petite valeur propre
de la matrice Hessienne par son minorant Î». Le deÂ´calage t0 est choisi de facÂ¸on a` faire
en sorte que le gain initial soit comparable avec la taille attendue du parame`tre optimal.
cÂ© Revue MODULAD, 2010 -70- NumeÂ´ro 42
Les reÂ´sultats montrent sans aucun doute que lâ€™algorithme SGD surclasse les algorithmes
classiques dâ€™optimisation des SVM. Des reÂ´sultats comparables [25] ont eÂ´teÂ´ obtenus avec
un algorithme SGD corrigeÂ´ par une opeÂ´ration de projection. Notre reÂ´sultat montre que
cette opeÂ´ration nâ€™est pas neÂ´cessaire.
La table 3 donne eÂ´galement les reÂ´sultats que lâ€™on obtient avec la fonction de perte lo-
gistique `(z) = log(1 + eâˆ’z). Comme cette fonction de perte est infiniment deÂ´rivable, nous
pouvons comparer lâ€™algorithme SGD avec un algorithme dâ€™optimisation superlineÂ´aire.
Nous avons utiliseÂ´ lâ€™algorithme TRON [23] avec deux crite`res dâ€™arreË†t diffeÂ´rents. Ces
reÂ´sultats apparaissent eÂ´galement dans la figure 1. Lâ€™algorithme TRON prend moins dâ€™une
minute pour calculer lâ€™optimum avec 10 chiffres significatifs. Bien que lâ€™algorithme SGD
soit incapable dâ€™une telle performance dâ€™optimisation, il est initialement plus rapide que
TRON. La partie supeÂ´rieure de la figure 1 montre que lâ€™erreur en preÂ´vision cesse de dimi-
nuer bien avant que TRON ne commence a` surclasser SGD.
La figure 2 montre comment lâ€™erreur en preÂ´vision eÂ´volue en fonction du temps dâ€™ap-
prentissage. Nous avons cette fois compareÂ´ lâ€™algorithme SGD avec un algorithme de gra-
dients conjugueÂ´s (CG) appliqueÂ´ a` divers sous-ensembles des exemples dâ€™apprentissage.5
Supposons, par exemple, que nous ne disposons que dâ€™une seconde de temps de calcul.
Lancer lâ€™algorithme CG sur seulement 30000 exemples est alors bien plus efficace que le
lancer sur tout lâ€™ensemble dâ€™apprentissage. En revanche il est difficile de savoir a` lâ€™avance
quel est le meilleur nombre dâ€™exemples a` consideÂ´rer, et lâ€™algorithme SGD appliqueÂ´ a` tous
les exemples reste geÂ´neÂ´ralement plus rapide.
5 Conclusion
En prenant a` la fois en compte les contraintes sur le temps de calcul et sur le nombre
maximal dâ€™exemples, nous avons mis en eÂ´vidence des diffeÂ´rences qualitatives entre les
performances des syste`mes dâ€™apprentissage a` petite eÂ´chelle et a` grande eÂ´chelle. La per-
formance en preÂ´vision dâ€™un syste`me dâ€™apprentissage a` grande eÂ´chelle deÂ´pend a` la fois des
proprieÂ´teÂ´s statistiques du mode`le et des proprieÂ´teÂ´s calculatoires de lâ€™algorithme dâ€™opti-
misation retenu. Des reÂ´sultats theÂ´oriques asymptotiques et des reÂ´sultats expeÂ´rimentaux
montrent alors quâ€™un algorithme dâ€™optimisation meÂ´diocre, lâ€™algorithme SGD, se reÂ´veÂ´le un
excellent algorithme dâ€™apprentissage a` grande eÂ´chelle.
Cette analyse peut donner lieu a` de nombreux raffinements. Shalev-Shwartz et Sre-
bro [10] lâ€™ont eÂ´tendu aux risques reÂ´gulariseÂ´s. On peut eÂ´galement attendre quelques effets
inteÂ´ressants relatifs au choix de fonctions de perte surrogeÂ´es [8, 14].
ReÂ´feÂ´rences
[1] Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer
Series in Statistics. Springer-Verlag, Berlin, 1982.
5Cette expeÂ´rience, suggeÂ´reÂ´e par Olivier Chapelle, utilise une variante de lâ€™algorithme superlineÂ´aire CG
optimiseÂ´e pour offrir une bonne vitesse initiale de convergence.
cÂ© Revue MODULAD, 2010 -71- NumeÂ´ro 42
[2] Leslie G. Valiant. A theory of learnable. Proc. of the 1984 STOC, pages 436â€“445,
1984.
[3] SteÂ´phane Boucheron, Olivier Bousquet, and GaÂ´bor Lugosi. Theory of classification :
a survey of recent advances. ESAIM : Probability and Statistics, 9 :323â€“375, 2005.
[4] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory
and Related Fields, 135(3) :311â€“334, 2006.
[5] J. Stephen Judd. On the complexity of loading shallow neural networks. Journal of
Complexity, 4(3) :177â€“192, 1988.
[6] LeÂ´on Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances
in Neural Information Processing Systems (NIPS 2007), volume 20. MIT Press, 2008.
[7] Richard O. Duda and Peter E. Hart. Pattern Classification And Scene Analysis.
Wiley and Son, 1973.
[8] Tong Zhang. Statistical behavior and consistency of classification methods based on
convex risk minimization. The Annals of Statistics, 32 :56â€“85, 2004.
[9] Clint Scovel and Ingo Steinwart. Fast rates for support vector machines. In Peter
Auer and Ron Meir, editors, Proceedings of the 18th Conference on Learning Theory
(COLT 2005), volume 3559 of Lecture Notes in Computer Science, pages 279â€“294,
Bertinoro, Italy, June 2005. Springer-Verlag.
[10] Shai Shalev-Shwartz and Nathan Srebro. SVM optimization : inverse dependence on
training set size. In Proceedings of the 25th International Machine Learning Confe-
rence (ICML 2008), pages 928â€“935. ACM, 2008.
[11] Vladimir N. Vapnik, Esther Levin, and Yann LeCun. Measuring the VC-dimension
of a learning machine. Neural Computation, 6(5) :851â€“876, 1994.
[12] Olivier Bousquet. Concentration Inequalities and Empirical Processes Theory Applied
to the Analysis of Learning Algorithms. PhD thesis, Ecole Polytechnique, 2002.
[13] Alexandre B. Tsybakov. Optimal aggregation of classifiers in statistical learning.
Annals of Statististics, 32(1), 2004.
[14] Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification
and risk bounds. Journal of the American Statistical Association, 101(473) :138â€“156,
March 2006.
[15] Pascal Massart. Some applications of concentration inequalities to statistics. Annales
de la FaculteÂ´ des Sciences de Toulouse, series 6, 9(2) :245â€“303, 2000.
[16] Wee S. Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of
convexity in learning with squared loss. IEEE Transactions on Information Theory,
44(5) :1974â€“1980, 1998.
[17] Shahar Mendelson. A few notes on statistical learning theory. In Shahar Mendelson
and Alexander J. Smola, editors, Advanced Lectures in Machine Learning, volume
2600 of Lecture Notes in Computer Science, pages 1â€“40. Springer-Verlag, Berlin,
2003.
[18] John E. Dennis, Jr. and Robert B. Schnabel. Numerical Methods For Unconstrained
Optimization and Nonlinear Equations. Prentice-Hall, Inc., Englewood Cliffs, New
Jersey, 1983.
cÂ© Revue MODULAD, 2010 -72- NumeÂ´ro 42
[19] Noboru Murata. A statistical study of on-line learning. In David Saad, editor, Online
Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.
[20] Yann Le Cun, LeÂ´on Bottou, Genevieve B. Orr, and Klaus-Robert MuÂ¨ller. Efficient
backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer
Science LNCS 1524. Springer Verlag, 1998.
[21] LeÂ´on Bottou and Yann Le Cun. Large scale online learning. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard SchoÂ¨lkopf, editors, Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA, 2004.
[22] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1 : A new benchmark
collection for text categorization research. Journal of Machine Learning Research,
5 :361â€“397, 2004.
[23] Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi. Trust region Newton methods
for large-scale logistic regression. In Zoubin Ghahramani, editor, Proceedings of the
24th International Machine Learning Conference, pages 561â€“568, Corvallis, OR, June
2007. ACM.
[24] Thorsten Joachims. Training linear SVMs in linear time. In Proceedings of the
12th ACM SIGKDD International Conference, Philadelphia, PA, August 2006. ACM
Press.
[25] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos : Primal estima-
ted subgradient solver for SVM. In Zoubin Ghahramani, editor, Proceedings of the
24th International Machine Learning Conference, pages 807â€“814, Corvallis, OR, June
2007. ACM.
cÂ© Revue MODULAD, 2010 -73- NumeÂ´ro 42
