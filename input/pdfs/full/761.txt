Un modÃ¨le gÃ©nÃ©ratif pour lâ€™Apprentissage de la Topologie
MichaÃ«l Aupetitâˆ—, Pierre Gaillardâˆ—âˆ—, GÃ©rard Govaertâˆ—âˆ—âˆ—
âˆ— Commissariat Ã  lâ€™Energie Atomique
LIST, Laboratoire Intelligence Multi-capteurs et Apprentissage
F-91191 Gif-sur-Yvette
michael.aupetit@cea.fr
âˆ—âˆ— Commissariat Ã  lâ€™Energie Atomique
Centre DAM - Ile de France
BruyÃ¨res-le-ChÃ¢tel - 91297 Arpajon cedex
pierre.gaillard@cea.fr
âˆ—âˆ—âˆ— UTC - Heudiasyc
CompiÃ¨gne - France
gerard.govaert@hds.utc.fr
RÃ©sumÃ©. Un nuage de points est plus quâ€™un ensemble de points isolÃ©s. La dis-
tribution des points peut Ãªtre gouvernÃ©e par une structure topologique cachÃ©e, et
du point de vue de la fouille de donnÃ©es, modÃ©liser et extraire cette structure est
au moins aussi important que dâ€™estimer la seule densitÃ© de probabilitÃ© du nuage.
Dans cet article, nous proposons un modÃ¨le gÃ©nÃ©ratif basÃ© sur le graphe de De-
launay dâ€™un ensemble de prototypes reprÃ©sentant le nuage de points, et suppo-
sant un bruit gaussien. Nous dÃ©rivons les Ã©quations de lâ€™algorithme Expectation-
Maximisation de maximisation de la vraisemblance, et nous utilisons le critÃ¨re
dâ€™information bayÃ©sien (BIC) pour sÃ©lectionner le modÃ¨le de complexitÃ© opti-
male. Ce modÃ¨le ne nÃ©cessite aucun rÃ©glage manuel arbitraire de paramÃ¨tres.
Les expÃ©riences que nous menons sur des donnÃ©es jouets et des bases dâ€™images
montrent que la connexitÃ© du graphe reproduit correctement celle du nuage de
points. Nous montrons aussi que ce modÃ¨le peut Ãªtre utilisÃ© en tant quâ€™outil de
prÃ©traitement en classification supervisÃ©e de caractÃ¨res manuscrits. Ce travail a
pour objectif de poser les premiÃ¨res pierres dâ€™un cadre thÃ©orique basÃ© sur les mo-
dÃ¨les gÃ©nÃ©ratifs statistiques, permettant la construction automatique de modÃ¨les
topologiques dâ€™un nuage de points.
1 Introduction
En apprentissage statistique, on suppose que les donnÃ©es sont gÃ©nÃ©rÃ©es par une fonction
densitÃ© de probabilitÃ© (pdf) p(.) ayant Ã©ventuellement beaucoup moins de degrÃ©s de libertÃ©
que lâ€™espace ambiant (Belkin et Niyogi, 2004). ConsidÃ©rant des donnÃ©es de type vecteurs de
rÃ©els, lâ€™ensemble de donnÃ©es forme un nuage de points dans RD que lâ€™on suppose situÃ© au
voisinage dâ€™un ensemble de variÃ©tÃ©s, appelÃ©es "variÃ©tÃ©s principales" (Tibshirani, 1992), plon-
gÃ©es dans lâ€™espace ambiant, et images de certaines variÃ©tÃ©s latentes au travers dâ€™un processus
Apprentissage Automatique de la Topologie
dâ€™observation dâ€™un phÃ©nomÃ¨ne physique. Lâ€™Apprentissage de la Topologie est un domaine rÃ©-
cent en Apprentissage Automatique (Aupetit et al., 2007), dont lâ€™objectif est de dÃ©velopper des
mÃ©thodes basÃ©es sur les statistiques pour retrouver les invariants topologiques de ces variÃ©tÃ©s
latentes Ã  partir du nuage de points (Figure 2). La connexitÃ© ou la dimension intrinsÃ¨que sont
de tels invariants topologiques.
Etant donnÃ© un ensemble x de M points observÃ©s, dans un espace ambiant euclidien Ã 
D dimensions, les mÃ©thodes statistiques permettent de rÃ©soudre des problÃ¨mes trÃ¨s gÃ©nÃ©raux
de discrimination, classification ou rÃ©gression, en estimant la densitÃ© de probabilitÃ© de cet
ensemble (Bishop, 2006). Cependant, la fonction densitÃ© de probabilitÃ© p (fonction de RD
dans R+) bien quâ€™elle contienne la totalitÃ© de lâ€™information extractible de la population dont
le nuage de points est un Ã©chantillon, est habituellement estimÃ©e par des mÃ©thodes (fenÃªtres
de Parzen par exemple) qui ne rendent pas explicite lâ€™information gÃ©omÃ©trique et topologique
relatives Ã  son support.
En effet, si lâ€™on suppose que la population est une sous-variÃ©tÃ© de lâ€™espace ambiant, il reste
des informations Ã  extraire de ces variÃ©tÃ©s et donc des variÃ©tÃ©s latentes correspondantes, que
nous nâ€™extrayons pas avec les estimateur de densitÃ© actuels. Pourtant nous pourrions apprendre
de leur gÃ©omÃ©trie (position relative des variÃ©tÃ©s, courbure) et mÃªme plus encore de leur topo-
logie (connexitÃ©, dimension intrinsÃ¨que, invariants topologiques comme les nombres de Betti
(Munkres, 1993)). De plus, la topologie des variÃ©tÃ©s latentes est plus susceptible dâ€™Ãªtre prÃ©-
servÃ©e par le processus dâ€™observation que leur gÃ©omÃ©trie, parce que la gÃ©omÃ©trie, Ã  un facteur
dâ€™Ã©chelle prÃ¨s, est prÃ©servÃ©e seulement par similaritÃ©, tandis que la topologie lâ€™est par homÃ©o-
morphisme, une classe de transformations bien plus grande contenant les similaritÃ©s.
Cette connaissance vaut-elle la peine dâ€™Ãªtre extraite ? Nous le pensons. Par exemple, en Re-
connaissance de Formes, la forme des nuages de points est considÃ©rÃ©e comme lâ€™Ã©lÃ©ment perti-
nent pour les tÃ¢ches de discrimination (Carlsson et al., 2004). En classification non-supervisÃ©e
ou semi-supervisÃ©e, il est proposÃ© dâ€™attribuer Ã  la mÃªme classe les points qui appartiennent Ã  la
mÃªme composante connexe (Belkin et al., 2006). Dans le contexte de lâ€™Analyse Exploratoire de
DonnÃ©es (Aupetit et Catz, 2005; Aupetit, 2007; Gaillard et al., 2008), les caractÃ©ristiques topo-
logiques sont dâ€™un intÃ©rÃªt primordial particuliÃ¨rement en dimension supÃ©rieure Ã  trois, afin de
dÃ©tecter des formes non visualisables directement. Extraire les invariants topologiques en mo-
dÃ©lisant les variÃ©tÃ©s principales fournit aussi un moyen de mesurer les distances gÃ©odÃ©siques le
long de ces variÃ©tÃ©s, ce qui a des applications dans la planification optimale de trajectoire et le
calcul de cinÃ©matique inverse en robotique (Zeller et al., 1996), ou encore lors de projections
non-linÃ©aires pour faciliter le dÃ©pliage des variÃ©tÃ©s (Lee et al., 2002; de Silva et Tenenbaum,
2003).
Enfin, si nous sommes en mesure dâ€™extraire des invariants topologiques dâ€™un nuage de
points, ces invariants sont susceptibles dâ€™Ãªtre plus robustes au bruit et Ã  diffÃ©rentes conditions
dâ€™observation que la gÃ©omÃ©trie de ce nuage. Comme nous le montrons en perspective de ce
travail, cela permet de gÃ©nÃ©rer un ensemble de donnÃ©es structurÃ©es (un ensemble de graphes)
complÃ©mentaire des nuages de points de dÃ©part.
Dans ce travail, nous nous focalisons sur lâ€™extraction de la connexitÃ© des variÃ©tÃ©s princi-
pales dâ€™un nuage de points.
M. Aupetit, P. Gaillard et G. Govaert
2 Etat de lâ€™art
Les techniques de Quantification Vectorielle (Ahalt et al., 1990) comme les Â«K-moyennesÂ»
(Queen, 1967) ou le Neural-Gas (Martinetz et al., 1993), et leur version gÃ©nÃ©rative (Celeux et
Govaert, 1992), tels les ModÃ¨les de MÃ©langes Gaussiens (McLachlan et Peel, 2000) tendent Ã 
reprÃ©senter les variÃ©tÃ©s principales par un ensemble fini de N points w = {wi âˆˆ RD}Ni=1, que
nous appelons prototypes 1.
Le modÃ¨le gÃ©nÃ©ratif est un modÃ¨le du processus de gÃ©nÃ©ration des donnÃ©es en deux temps :
tirage alÃ©atoire dâ€™un composant parmi les N , suivant une loi multinomiale, puis tirage des
donnÃ©es suivant la loi gaussienne assignÃ©e Ã  ce composant. Ainsi dans ce modÃ¨le, les variÃ©tÃ©s
principales sont supposÃ©es Ãªtre un ensemble de points w (les prototypes ou moyennes des
composants gaussiens), ayant Ã©tÃ© corrompues par un bruit additif Gaussien qui a menÃ© aux
donnÃ©es finalement observÃ©es.
Ces deux techniques nâ€™impliquent aucune structure topologique pour modÃ©liser les variÃ©tÃ©s
principales (aucune connexitÃ©, seulement un ensemble de sources ponctuelles isolÃ©es - les pro-
totypes). Cependant, elles sont Ã  lâ€™origine dâ€™une large famille de techniques que nous passons
briÃ¨vement en revue maintenant.
Il y a deux familles dâ€™approches en Apprentissage Automatique, qui impliquent des notions
de topologie. Dâ€™une part, les approches Â«Apprentissage de VariÃ©tÃ©sÂ» (Â«Manifold LearningÂ» en
anglais) sont basÃ©es sur la projection non-linÃ©aire des donnÃ©es dans un espace de plus faible
dimension dont la topologie est dans une large mesure fixÃ©e a priori. Leur objectif principal
est de visualiser les donnÃ©es comme pour les Carte Auto-OrganisÃ©es (Kohonen, 2001; Bishop
et al., 1998), leurs versions Ã  topologie adaptative (Fritzke, 1992; Alahakoon et al., 1998) ou les
versions continues appelÃ©es courbes et surfaces principales (Hastie et Stuetzle, 1989; Chang
et Ghosh, 2001) et leur version gÃ©nÃ©rative (Tibshirani, 1992), ou de rÃ©duire leur dimension
comme prÃ©traitement de mÃ©thodes de classification (Tipping et Bishop, 1999; de Silva et Te-
nenbaum, 2003; Lee et al., 2002).
Dâ€™autre part, ce travail traite des approches dâ€™Â«Apprentissage de la TopologieÂ» (Â«Topo-
logy LearningÂ» en anglais), basÃ©es sur la construction dâ€™un espace dont la topologie nâ€™est pas
contrainte a priori mais au contraire est apprise des donnÃ©es, cela au prix de la visualisabi-
litÃ© (possibilitÃ© de structures non connexes et de dimensions intrinsÃ¨ques non homogÃ¨nes non
prÃ©servables par projection). Ainsi, suivant quelques hypothÃ¨ses gÃ©nÃ©rales sur les variÃ©tÃ©s gÃ©-
nÃ©ratrices, on souhaite retrouver la topologie et la gÃ©omÃ©trie de celles-ci Ã  partir des donnÃ©es.
Certains travaux se basent sur un graphe dont les sommets sont les donnÃ©es, par exemple le
graphe des K-plus-proches-voisins (KPPV) utilisÃ© pour estimer les distances gÃ©odÃ©siques pour
rÃ©duire la dimension par projection (de Silva et Tenenbaum, 2003). Cependant il est difficile de
rÃ©gler K, ces mÃ©thodes sont sensibles au bruit (Carreira-PerpiÃ±Ã¡n et Zemel, 2005) et elles ne
permettent pas de traiter les donnÃ©es incomplÃ¨tes. Des approches issues de la GÃ©omÃ©trie Al-
gorithmique ont aussi Ã©tÃ© proposÃ©es (Niyogi et al., 2006; Bubenik et Kim, 2007; Chazal et al.,
2007), mais souffrent de ces mÃªmes limites. Dâ€™autres travaux (Martinetz et Schulten, 1994;
Aupetit, 2003; de Silva et Carlsson, 2004) se sont basÃ©s sur la construction dâ€™un graphe ayant
pour sommets des prototypes, et dont la connexitÃ© tendait Ã  reproduire celle de la structure
sous-jacente aux donnÃ©es.
1Dans le cas des modÃ¨les de mÃ©lange, les prototypes sont aussi appelÃ©s Â«composantsÂ».
Apprentissage Automatique de la Topologie
(a) (b)
FIG. 1 â€“ Limite du Competitive Hebbian Learning : Un nuage de points (points bleus), un ensemble
de prototypes (ronds verts) et la triangulation induite de Delaunay (traits gras). Les ROI des arcs (traits
fins) sont coloriÃ©es avec un niveau de gris proportionnel au nombre de tÃ©moins quâ€™elles contiennent
(blanc nul, noir maximum). (a) SensibilitÃ© au bruit : un seul tÃ©moin (flÃ¨che rouge) suffit Ã  crÃ©er un arc
(flÃ¨che verte). (b) Forme non pertinente des ROI : les ROI (flÃ¨ches rouges) ne sont pas simplement
gÃ©omÃ©triquement reliÃ©es Ã  leurs arcs (flÃ¨ches vertes). Absence de self-consistance : certains arcs (flÃ¨che
pointillÃ©e) ne coupent pas leur propre ROI, et la taille de ces ROI peut Ãªtre si petite (flÃ¨che rouge poin-
tillÃ©e) que mÃªme une trÃ©s forte densitÃ© de points au voisinage de lâ€™arc ne suffirait pas Ã  gÃ©nÃ©rer un tÃ©moin
de cette arc.
Martinetz et Schulten (Martinetz et Schulten, 1994) ont proposÃ© un algorithme de construc-
tion dâ€™un graphe appelÃ© Triangulation de Delaunay Induite, qui approche le Graphe de Delau-
nay Restreint dÃ©fini dans (Edelsbrunner et Shah, 1997). Cet algorithme appelÃ© Competitive
Hebbian Learning (CHL), consiste Ã  connecter deux prototypes wi et wj sâ€™il existe un point
x âˆˆ x du nuage dont ils sont les premier et deuxiÃ¨me plus proches voisins. Un tel point est
appelÃ© Â«tÃ©moinÂ» de lâ€™arc {i, j} (de Silva et Carlsson, 2004), et cet arc fait partie du graphe
de Delaunay2 DG(w) des prototypes. La rÃ©gion de RD qui contient tous les tÃ©moins dâ€™un arc
{i, j}, est appelÃ©e "RÃ©gion dâ€™Influence" (ROI) de cette arc.
Dans (Fritzke, 1995), une variante du CHL est prÃ©sentÃ©e. Elle est basÃ©e sur un processus
dâ€™ajout dynamique de prototypes et de liens appelÃ© Growing Neural Gas (GNG). Du point de
vue de lâ€™Apprentissage Automatique, le Competitive Hebbian Learning et le graphe rÃ©sultant
(appelÃ© IDT pour Â«Induced Delaunay TriangulationÂ») ont certaines limites :
1. SensibilitÃ© au bruit (Figure 1 (a)) Un seul point tÃ©moin suffit Ã  crÃ©er un arc de lâ€™IDT.
2. Forme des ROI non pertinente (Figure 1 (b)) Un seuil sur le nombre de tÃ©moins mi-
nimal de chaque arc a Ã©tÃ© proposÃ© pour filtrer le bruit (Martinetz et al., 1993; Fritzke,
1995) mais le choix de ce seuil ne repose sur aucun critÃ¨re objectif. Les ROI sont des
polytopes de RD rendant difficile le calcul de leur volume pour baser ce seuil sur des
probabilitÃ©s. Elles peuvent aussi Ãªtre de taille trÃ¨s rÃ©duite, pouvant ne contenir aucun
2Le graphe de Delaunay des prototypes connecte deux prototypes si leurs cellules de VoronoÃ¯ sont adjacentes. La
cellule de VoronoÃ¯ dâ€™un point s dâ€™un ensemble S dans un espace vectoriel E suivant une mesure de distance d, est le
lieu des points de E dont s est le plus proche suivant d parmi les points S.
M. Aupetit, P. Gaillard et G. Govaert
tÃ©moin alors que la distribution locale de lâ€™Ã©chantillon lÃ©gitimerait pourtant lâ€™existence
de lâ€™arc correspondant.
3. Absence de parcimonie et mauvaise topologie des sources ponctuelles Comme tout
point du nuage a un premier et un second plus proches prototypes, tout prototype ayant
des tÃ©moins est nÃ©cessairement connectÃ© Ã  un autre prototype. Donc une source pon-
tuelle est reprÃ©sentÃ©e par deux prototypes interconnectÃ©s au lieu dâ€™un, et une dimension
intrinsÃ¨que de 1 au lieu de 0 est faussement induite de lâ€™arc les reliant.
4. Absence de self-consistance3 (Figure 1 (b)) La rÃ©alisation gÃ©omÃ©trique RIDT de lâ€™IDT
dans lâ€™espace ambiant RD, nâ€™est pas une variÃ©tÃ© self-consistante : mÃªme un Ã©chantillon-
nage aussi dense que voulu de RIDT ne garantit pas de gÃ©nÃ©rer des points tÃ©moins pour
toutes les arcs de cette IDT, car il peut exister des arcs {i, j} de lâ€™IDT dont la ROI ne
coupe pas leur rÃ©alisation gÃ©omÃ©trique [wi, wj ].
5. Absence de mesure de qualitÃ© Il nâ€™a pas Ã©tÃ© proposÃ© de critÃ¨re objectif pour mesurer la
qualitÃ© du graphe obtenu, particuliÃ¨rement en dimension supÃ©rieure Ã  3 oÃ¹ lâ€™inspection
visuelle nâ€™est plus possible, ce qui fait obstacle Ã  lâ€™automatisation du processus sur une
base statistique objective permettant la sÃ©lection dâ€™un modÃ¨le optimal.
En rÃ©sumÃ©, le CHL est avant tout un moyen pratique et peu complexe en termes dâ€™algo-
rithmie et dâ€™implÃ©mentation car basÃ© sur une recherche de plus proches voisins, de gÃ©nÃ©rer
des arcs du graphe de Delaunay dâ€™un ensemble de prototypes. Il se trouve que le CHL a pour
effet de bord de gÃ©nÃ©rer des variÃ©tÃ©s dont la connexitÃ© semble visuellement proche de ce que
seraient les variÃ©tÃ©s principales du nuage de points. Au mieux sait-on (Martinetz et Schulten,
1994) que le CHL reproduit cette connexitÃ© sous certaines conditions dâ€™Ã©chantillonnage des
variÃ©tÃ©s principales. Mais aucune mesure ne permet de quantifier le respect de ces conditions
sans connaÃ®tre a priori les variÃ©tÃ©s et leur Ã©chantillonnage. Il apparaÃ®t donc que cet effet de
bord nâ€™est pas suffisament contrÃ´lable pour Ãªtre exploitable dans le contexte de lâ€™apprentissage
automatique.
2.1 Notre contribution
Afin de dÃ©passer les limites du CHL, nous avons changÃ© de point de vue. Si nous considÃ©-
rons la densitÃ© de probabilitÃ© de la population dont le nuage de points est un Ã©chantillon, nous
souhaitons dÃ©tecter les rÃ©gions de faible densitÃ© qui sÃ©parent les rÃ©gions de forte densitÃ©, et
surtout rendre explicite le rÃ©sultat de cette sÃ©paration en termes de connexitÃ©. Il nous faut donc
un modÃ¨le de densitÃ© particulier en ce quâ€™il rend extractible (calculable) lâ€™information sur la
connexitÃ©. Pour cela, nous nous plaÃ§ons dans le cadre des modÃ¨les gÃ©nÃ©ratifs.
Les modÃ¨les de mÃ©langes classiques peuvent Ãªtre vus comme le pendant gÃ©nÃ©ratif des
techniques de quantification vectorielle Ã  lâ€™origine du CHL (le Â«Neural-GasÂ» (Martinetz et al.,
1993)), aussi nous proposons un modÃ¨le gÃ©nÃ©ratif pour remplacer le CHL4. Ce modÃ¨le gÃ©nÃ©ra-
tif nous fournit naturellement un critÃ¨re de qualitÃ© : la vraisemblance, et permet dâ€™exprimer le
3La Â«self-consistanceÂ» a Ã©tÃ© dÃ©finie pour les courbes principales par (Hastie et Stuetzle, 1989). Tout point dâ€™une
courbe principale est au centre de gravitÃ© des points qui se projettent sur lui. Les points de la courbe se projettent
sur eux-mÃªmes. Une courbe principale est donc toujours courbe principale dâ€™elle-mÃªme, dâ€™oÃ¹ le terme de "self-
consistance"
4Une nuance toutefois, notre modÃ¨le nâ€™est thÃ©oriquement Ã©quivalent au CHL pour aucune valeur de ses paramÃ¨tres,
alors que câ€™est le cas entre les K-moyennes et les modÃ¨les de mÃ©lange gaussiens Ã  variance nulle et proportions Ã©gales
Apprentissage Automatique de la Topologie
problÃ¨me dâ€™apprentissage de la topologie dans le cadre thÃ©orique clairement dÃ©fini des modÃ¨les
de mÃ©lange. Dans ce modÃ¨le, nous supposons que la rÃ©alisation gÃ©omÃ©trique dâ€™un graphe5 est
la source gÃ©nÃ©ratrice du nuage de points. Dans un modÃ¨le de mÃ©lange classique, seuls les som-
mets sont pondÃ©rÃ©s, ici les arcs de ce graphes sont pondÃ©rÃ©s en plus des sommets. La clef de
lâ€™apprentissage de la connexitÃ© rÃ©side dans lâ€™utilisation dâ€™un graphe comme variÃ©tÃ© source car
on sait calculer, donc extraire, la connexitÃ© dâ€™un graphe, ainsi que dans lâ€™attibution de propor-
tions Ã  chacuns de ses arcs, i.e. leur propension Ã  gÃ©nÃ©rer des donnÃ©es. Lâ€™idÃ©e est que si un
arc Ã  une proportion nulle, il peut Ãªtre supprimÃ© du graphe et donc du modÃ¨le gÃ©nÃ©ratif sans
en modifier la vraisemblance. Ainsi Ã  vraisemblance Ã©gale, le graphe le plus parcimonieux est
celui dont les arcs inutiles ont Ã©tÃ© Ã©laguÃ©s, et sa connexitÃ© est alors proposÃ©e comme estimateur
de la connexitÃ© des variÃ©tÃ©s principales du nuage de points.
Nous utilisons le critÃ¨re dâ€™information BayÃ©sien (BIC) pour rÃ©gler le compromis entre
vraisemblance et parcimonie, et nous supposons que les observations issues des variÃ©tÃ©s gÃ©-
nÃ©ratrices sont perturbÃ©es suivant une loi de densitÃ© gaussienne isovariÃ©e. Nous nommons ce
modÃ¨le gÃ©nÃ©ratif le "Graphe GÃ©nÃ©ratif Gaussien" (GGG).
Nous avons dÃ©jÃ  introduit ce modÃ¨le dans (Aupetit, 2006) et une version supervisÃ©e dans
(Gaillard et al., 2008). Ici nous proposons pour la premiÃ¨re fois dâ€™utiliser le critÃ¨re BIC pour
le rÃ©glage des paramÃ¨tres et mÃ©ta-paramÃ¨tres. Nous proposons un cadre gÃ©nÃ©ratif gÃ©nÃ©ral pour
lâ€™apprentissage de la topologie puis nous nous focalisons sur le problÃ¨me de lâ€™apprentissage
de la connexitÃ©, et comparons notre approcheÃ  celles de lâ€™Ã©tat de lâ€™art sur des donnÃ©es rÃ©elles
multi-dimensionnelles.
3 Le Graphe GÃ©nÃ©ratif Gaussien
3.1 Vers un modÃ¨le dâ€™apprentissage automatique de la topologie
Nous dÃ©crivons les fondements de notre approche du problÃ¨me de lâ€™Apprentissage de la
Topologie.
De notre Ã©tude de lâ€™Ã©tat de lâ€™art, nous dÃ©duisons les propriÃ©tÃ©s quâ€™un modÃ¨le devrait pos-
sÃ©der : (1) le modÃ¨le devrait Ãªtre dÃ©gagÃ© au maximum de toute contrainte topologique a priori,
i.e. le modÃ¨le devrait Ãªtre le plus flexible possible pour pouvoir modÃ©liser la topologie de
nâ€™importe quel variÃ©tÃ© aussi compliquÃ©e soit-elle ; (2) la topologie de ce modÃ¨le devrait Ãªtre
extractible, i.e. calculable ; ce modÃ¨le devrait Ãªtre (3) robuste au bruit ; (4) parcimonieux ; (5)
self-consistant ; (6) fournir un moyen de mesure de sa qualitÃ© et (7) ne pas nÃ©cessiter de rÃ©-
glages arbitraires de ces mÃ©ta-paramÃ¨tres ; enfin, (8) sa complexitÃ© en temps de calcul devrait
Ãªtre raisonnable.
Dans le cadre de lâ€™approximation de fonction, on estime une fonction complexe en com-
binant des fonctions Ã©lÃ©mentaires de base issues dâ€™une famille de fonctions suffisament riche
(approximation universelle). Pour modÃ©liser des variÃ©tÃ©s inconnues, nous suivons la mÃªme ap-
proche en considÃ©rant un modÃ¨le obtenu par lâ€™assemblage de variÃ©tÃ©s issues dâ€™une famille suf-
fisament riche pour que lâ€™on puisse obtenir la complexitÃ© nÃ©cessaire. Afin de rendre calculable
lâ€™extraction des caractÃ©ristiques topologiques de cette variÃ©tÃ© modÃ¨le, nous devons utiliser des
5Dans la suite, nous ne faisons plus la distinction entre un graphe et sa rÃ©alisation gÃ©omÃ©trique, câ€™est-Ã -dire son
plongement dans un espace vectoriel, ici lâ€™espace des donnÃ©es, que lâ€™on obtient en donnant Ã  chaque sommet une
position dans cet espace.
M. Aupetit, P. Gaillard et G. Govaert
variÃ©tÃ©s Ã©lÃ©mentaires Â«discrÃ¨teÂ» et en nombre fini. Ainsi la famille des d-boules (point (d = 0),
segment (d = 1), disque (d = 2), boule (d = 3)...) ne convient pas car elle permet certes dâ€™ex-
traire la dimension intrinsÃ¨que locale, mais la connexitÃ© est difficile Ã  obtenir (intersection de
d-boules). La famille des d-pavÃ©s (point (d = 0), segment (d = 1), carrÃ© plein (d = 2), cube
plein (d = 3)...) permet de retrouver la dimension intrinsÃ¨que locale (la dimension d du d-
pavÃ© qui localement explique le nuage de point), la connexitÃ© (on peut assembler les d-pavÃ©s
par leurs sommets, arcs, faces... et reprÃ©senter cet assemblage par une structure de graphe qui
permet de calculer rapidement la connexitÃ©), mais câ€™est la parcimonie qui nâ€™est pas optimale
car il faut 2d sommets pour reprÃ©senter un objet de dimension d. Câ€™est pourquoi il est plus intÃ©-
ressant dâ€™utiliser un complexe simplicial (Munkres, 1993), i.e. un assemblage de k-simplexes
acollÃ©s les uns aux autres par leurs facettes. La rÃ©alisation gÃ©omÃ©trique dâ€™un k-simplexe dans
RD est lâ€™enveloppe convexe dâ€™un ensemble de k + 1 points de RD (ses sommets) : point
(d = 0), segment (d = 1), triangle plein (d = 2), tÃ©trahÃ¨dre plein (d = 3)... Un exemple
de complexe simplicial est le complexe de Delaunay dÃ©fini comme le dual du complexe formÃ©
par les cellules de VoronoÃ¯ 6. Du fait de sa nature discrÃ¨te, de nombreuses caractÃ©ristiques to-
pologiques dâ€™un complexe simplicial sont calculables, donc extractibles (de Silva et Carlsson,
2004; de Silva, 2003). En particulier, la dimension intrinsÃ¨que locale est donnÃ©e par la dimen-
sion des simplexes principaux (ceux qui ne sont les facettes dâ€™aucun autre simplexe dans le
complexe), et lâ€™arc-connexitÃ© est donnÃ©e par celle du 1-squelette du complexe, i.e. le graphe
sous-jacent au complexe, dÃ©fini par ses sommets (0-simplexes) et arcs (1-simplexe). Enfin le
complexe simplicial est le plus parcimonieux car d + 1 sommets (un d-simplexe) suffisent Ã 
reprÃ©senter un objet de dimension d.
Dans le prÃ©sent travail, nous nous focalisons sur lâ€™extraction de la connexitÃ© deMprin,
nous proposons de modÃ©liser cette variÃ©tÃ© avec un sous-graphe du graphe de Delaunay de
quelques prototypes localisÃ©s au voisinage de celle-ci. GrÃ¢ce Ã  ce modÃ¨le, nous remplissons
les propriÃ©tÃ©s dÃ©sirables (1) et (2) dÃ©crites ci-dessus. La parcimonie est en partie rÃ©alisÃ©e en
terme du nombre de paramÃ¨tres requis pour modÃ©liser un objet de dimension donnÃ©e, mais pas
pour le moment en terme du nombre total de variÃ©tÃ©s Ã©lÃ©mentaires requises dans lâ€™assemblage.
3.2 Lâ€™apprentissage de la topologie dans le cadre gÃ©nÃ©ratif
Nous posons le problÃ¨me de lâ€™Apprentissage de la Topologie comme un problÃ¨me gÃ©nÃ©ratif
(Figure 2abc) : soit M un espace topologique latent et RD lâ€™espace Euclidien (espace des
observations ou espace ambient). Les variÃ©tÃ©s principalesMprin sont dÃ©finies comme lâ€™image
deM par une fonction f : f(M) = Mprin. Cependant, en pratique, nous ne connaissons
niM ni f , et nous devons nous contenter dâ€™un ensemble fini de points x reprÃ©sentant les M
donnÃ©es observÃ©es, au lieu dâ€™un ensemble de variÃ©tÃ©sMprin. Les points x âˆˆ x sont les images
par lâ€™application f de points "cachÃ©s" z âŠ‚ M, issus deMprin suivant une certaine fonction
densitÃ© de probabilitÃ© pprin, et potentiellement corrompus par un bruit de nature inconnue  :
x = f(z) + . Nous souhaitons extraire la connexitÃ© deM Ã  partir de lâ€™observation du nuage
de points x. Lâ€™application f peut modifier la gÃ©omÃ©trie deM et Ã©ventuellement sa topologie.
Cependant, nous supposons que f est un homeomorphisme, donc que la topologie deM est la
mÃªme que celle deMprin, et donc que nous nâ€™avons pas besoin dâ€™estimer f . Il suffit de nous
6Dans le plan, Ã  chaque sommet des cellules de VoronoÃ¯ correspond un triangle dans le complexe de Delaunay
(aussi appelÃ© "triangulation"), Ã  chaque arc de VoronoÃ¯ , un arc de Delaunay, et Ã  chaque cellule de VoronoÃ¯ un sommet
de Delaunay. De maniÃ¨re gÃ©nÃ©rale, Ã  chaque k-cellule de VoronoÃ¯ correspond un (D âˆ’ k)-simplexe de Delaunay.
Apprentissage Automatique de la Topologie
FIG. 2 â€“ Lâ€™Apprentissage de la Topologie vu sous lâ€™angle gÃ©nÃ©ratif : (a) Un Ã©chantillon z (ronds
rouges) de lâ€™espace topologiqueM (un segment de droite et un point). (b)M et z sont plongÃ©s dans
lâ€™espace des observations suivant f . Lâ€™image par ce plongement dÃ©finit les variÃ©tÃ©s principalesMprin
(courbe gris foncÃ© et point isolÃ©) et une distribution de points f(z) (ronds rouges). (c) Ces points sont
perturbÃ©s par un bruit , menant au nuage de point x finalement observÃ© (ronds blancs). Lâ€™objectif de
lâ€™Apprentissage de la Topologie est de retrouver la topologie deM (ici un point et un segment de droite) Ã 
partir de la seule observation du nuage de points x (M, f et les distributions de z et de  sont inconnus).
focaliser sur lâ€™extraction de la topologie deMprin. Pour cela, nous proposons de modÃ©liser
Mprin avec un modÃ¨le statistique basÃ© sur un graphe permettant lâ€™extraction de cette topologie
(en particulier la connexitÃ©) avec des mÃ©thodes de lâ€™Ã©tat de lâ€™art (de Silva et Carlsson, 2004)).
Nous introduisons le modÃ¨le gÃ©nÃ©ratif en simplifiant les hypothÃ¨ses gÃ©nÃ©rales ci-dessus
(Figure 2). Au lieu de considÃ©rer tout type de variÃ©tÃ© pourMprin, et du fait des propriÃ©tÃ©s
dÃ©sirables des complexes simpliciaux, nous supposons queMprin est une sous-graphe G du
graphe de Delaunay de quelques prototypes positionnÃ©s dans lâ€™espace ambient. Au lieu de sup-
poser toute densitÃ© de probabilitÃ© pprin surMprin, nous supposons que pprin est uniforme le
long de chaque arc du graphe G. Et au lieu de supposer nâ€™importe quel type de bruit , nous
supposons un bruit gaussien additif isovariÃ© de moyenne nulle et de variance Ïƒ. Ce dernier
point satisfait la propriÃ©tÃ© (3) de prise en compte du bruit (Un modÃ¨le de bruit plus sophistiquÃ©
peut Ãªtre envisagÃ©). Le modÃ¨le gÃ©nÃ©ratif obtenu est donc une somme pondÃ©rÃ©e de fonctions de
densitÃ© basÃ©es sur les composantes Ã©lÃ©mentaires du graphe (ses arcs et ses sommets), convo-
luÃ©es avec un bruit gaussien. En tant que modÃ¨le gÃ©nÃ©ratif, et en considÃ©rant un rÃ©glage des
paramÃ¨tres et de la complexitÃ© du modÃ¨le par le critÃ¨re BIC (parcimonie et mesure de la qua-
litÃ©), ce modÃ¨le est par construction le meilleur modÃ¨le de densitÃ© des points quâ€™il gÃ©nÃ¨re
(self-consistance)7. Cela satisfait les propriÃ©tÃ©s (4), (5) et (6). De plus, le cadre statistique four-
nit des critÃ¨res objectifs comme le critÃ¨re BIC pour sÃ©lÃ©ctionner la complexitÃ© du modÃ¨le, si
bien quâ€™aucun mÃ©ta-paramÃ¨tre ne nÃ©cessite de rÃ©glage manuel arbitraire.
Cependant, les algorithmes (propriÃ©tÃ© (8)) pour lâ€™apprentissage des paramÃ¨tres de ce mo-
dÃ¨le sont relativement complexes (voir section 3.6).
7Cela ne signifie pas que le modÃ¨le est identifiable, il pourrait exister des modÃ¨les ayant mÃªme densitÃ© et mÃªme
complexitÃ© donc mÃªme score BIC mais ayant une topologie diffÃ©rente. Nous discuterons de cela en fin dâ€™article
M. Aupetit, P. Gaillard et G. Govaert
3.3 DÃ©finition formelle du modÃ¨le
Etant donnÃ© un ensemble de prototypes w positionnÃ©s au voisinage dâ€™un nuage de points
(les donnÃ©es) avec un modÃ¨le de mÃ©lange gaussien isovariÃ©, le graphe de Delaunay (DG) des
prototypes est construit8. Chaque arc et chaque sommet du graphe est la base dâ€™un modÃ¨le
gÃ©nÃ©ratif, de sorte que le graphe gÃ©nÃ¨re un mÃ©lange de densitÃ©s gaussiennes. Le modÃ¨le de
mÃ©lange rÃ©sultant reprÃ©sente les donnÃ©es Ã  partir de ces Ã©lÃ©ments gÃ©nÃ©ratifs que nous appe-
lons "points Gaussiens" et "segments Gaussiens", constituant le "Graphe GÃ©nÃ©ratif Gaussien"
(GGG).
La valeur de la densitÃ© dâ€™un point Gaussien centrÃ© sur un prototype wj âˆˆ w et de variance
Ïƒ2, calculÃ©e en un point xi âˆˆ x est dÃ©finie par :
g0j (xi;Ïƒ) = g
0(xi|wj ;Ïƒ) = (2piÏƒ2)âˆ’D/2 exp(âˆ’ (xi âˆ’ wj)
2
2Ïƒ2
) (1)
Un segment gaussien normalisÃ© est dÃ©fini comme la somme dâ€™un nombre infini de points
gaussiens rÃ©guliÃ¨rement rÃ©partis le long dâ€™un segment de droite. Il sâ€™agit donc de lâ€™intÃ©grale
dâ€™un point gaussien le long dâ€™un segment de droite (Figure 3). La valeur en un point xi du
segment gaussien [wajwbj ] associÃ© au j
e arc {aj , bj} de longueur Lj du graphe de Delaunay,
de variance Ïƒ2 est donnÃ©e par :
g1j (xi;Ïƒ) = g
1(xi|{waj , wbj};Ïƒ) = 1
(2piÏƒ2)
D
2 Lj
âˆ« wbj
waj
exp
(
âˆ’ (xiâˆ’t)
2
2Ïƒ2
)
dt
=
exp
â€
âˆ’ (xiâˆ’q
i
j)
2
2Ïƒ2
Â«
(2piÏƒ2)
Dâˆ’1
2
Â·
erf
â€
Qij
Ïƒ
âˆš
2
Â«
âˆ’erf
â€
Qijâˆ’Lj
Ïƒ
âˆš
2
Â«
2Lj
(2)
oÃ¹ Lj = â€–wbjâˆ’wajâ€–, Qij =
(xiâˆ’waj )â€²(wbjâˆ’waj )
Lj
et qij =waj+(wbjâˆ’waj )
Qij
Lj
est la projection
orthogonale de xi sur la droite passant par waj et wbj . Dans le cas oÃ¹ waj = wbj , nous posons
g1j (xi;Ïƒ) = g
0(xi|waj ;Ïƒ).
Dans lâ€™Ã©quation (2), la partie gauche du produit reprÃ©sente le bruit gaussien orthogonal
au segment, et la partie droite le bruit gaussien intÃ©grÃ© le long du segment (convolution). Les
fonctions g0 et g1 sont positives et lâ€™on dÃ©montre que leur intÃ©grale sur RD vaut 1, donc que
ce sont des fonctions densitÃ©s de probabilitÃ©.
Un point gaussien est associÃ© Ã  chaque prototype de w et un segment gaussien Ã  chaque arc
du graphe de Delaunay (DG). Le mÃ©lange de gaussiennes est obtenu par une somme pondÃ©rÃ©e
des N0 points gaussiens et N1 segments gaussiens, de telle sorte que la somme des poids pi
vaut 1 et quâ€™ils soient positifs ou nuls :
p(xi; Î˜) =
1âˆ‘
d=0
Ndâˆ‘
i=1
pidj g
d
j (xi;Ïƒ) (3)
avec
âˆ‘1
d=0
âˆ‘Nd
j=1 pi
d
j = 1 et pi
d
j â‰¥ 0 âˆ€j, d et oÃ¹Î˜ = {pi,w, Ïƒ,DG} reprÃ©sente lâ€™ensemble
des paramÃ¨tres du modÃ¨le. Le poids pi0j (resp.pi
1
j ) est la probabilitÃ© a priori quâ€™une donnÃ©e x
soit tirÃ©e du point gaussien associÃ© Ã  wj (resp. du segment gaussien associÃ© au je arc de DG).
8Les algorithmes pour construire le graphe de Delaunay sont fournis dans (Barber et al., 1996; Agrell, 1993)
Apprentissage Automatique de la Topologie
FIG. 3 â€“ Du point gaussien au segment gaussien : segments gaussiens dÃ©finis sur [0; k], avec k = 0
(trait plein), k = 0.5 (trait point-tiret), k = 1 (trait tiretÃ©), k = 2 (trait pointillÃ©).
Ainsi lâ€™espace latent est un ensemble de points et de segments, qui sont plongÃ©s dans lâ€™es-
pace ambient par la rÃ©alisation gÃ©omÃ©trique du graphe de Delaunay. De plus, en toute gÃ©nÃ©ralitÃ©
, les segments latents sont dÃ©finis de longueur 1, ce qui permet de dÃ©finir la distribution a priori
dâ€™une variable cachÃ©e t pour chaque point et chaque segment. Dans notre cas, la distribution a
priori sur t pour un point est la distribution de Dirac, et pour un segment, la distribution uni-
forme. Si nous introduisons une variable latente discrÃ¨te z indiquant quel Ã©lÃ©ment gÃ©nÃ©ratif a
gÃ©nÃ©rÃ© les donnÃ©es, le processus de gÃ©nÃ©ration est le suivant :
â€“ (i) tirage du je Ã©lÃ©ment gÃ©nÃ©ratif de dimension d avec une probabilitÃ© pidj , i.e. la variable
latente z suit une distribution multinomiale de paramÃ¨tre pi
â€“ (ii) tirage de la variable latente t suivant une distribution de Dirac ou uniforme fonction
de la nature point ou segment du je Ã©lÃ©ment ;
â€“ (iii) tirage de la donnÃ©e xi suivant une distribution gaussienne de variance Ïƒ2 et de
moyenne w avec w = wj si le je Ã©lÃ©ment est un point, et w = waj +
(wbjâˆ’waj )
Lj
t sinon
(oÃ¹ aj et bj sont les extrÃ©mitÃ©s du segment je Ã©lÃ©ment).
Le GGG en tant que modÃ¨le gÃ©nÃ©ratif est prÃ©sentÃ© sur la figure 4.
3.4 La vraisemblance comme mesure de qualitÃ© et sa maximisation avec
lâ€™algorihthme EM
La fonction p(xi;pi,w, Ïƒ,DG) est la densitÃ© de probabilitÃ© au point xi sachant les para-
mÃ¨tres du modÃ¨le. Nous mesurons la vraisemblance P des donnÃ©es x par rapport aux para-
mÃ¨tres Î˜ = {pi,w, Ïƒ,DG} du modÃ¨le GGG :
P (Î˜;x) =
Mâˆ
i=1
p(xi;pi,w, Ïƒ,DG) (4)
Afin de maximiser la vraisemblance P par rapport Ã  pi et Ïƒ, nous utilisons le cadre EM
(Dempster et al., 1977). Lâ€™idÃ©e clef de lâ€™algorithme EM est de considÃ©rer que les donnÃ©es
observÃ©es xi ne sont quâ€™une partie des donnÃ©es dites complÃ¨tes, et que la maximisation de la
M. Aupetit, P. Gaillard et G. Govaert
FIG. 4 â€“ GÃ©nÃ©ration des donnÃ©es avec le GGG : Pour rÃ©soudre le problÃ¨me dâ€™apprentissage de la
topologie (figure 2), nous dÃ©finissons un espace latent qui correspond Ã  un ensemble de points et de
segments (a), plongÃ© dans lâ€™espace dâ€™observation par la rÃ©alisation gÃ©omÃ©trique du graphe de Delaunay
(b).(a) Une donnÃ©e observÃ©e x est gÃ©nÃ©rÃ©e en sÃ©lectionnant un composant z suivant la distribution a
priori p(z) puis sachant ce composant, en tirant une valeur t (disque rouge) suivant la distribution a
priori p(t). (c) Enfin, en tirant le vecteur x (disque blanc) dâ€™une distribution gaussienne isotrope (cercles
magenta).
vraisemblance associÃ©e Ã  ces donnÃ©es complÃ¨tes est facile. On dÃ©finit les donnÃ©es complÃ¨tes
(xi, zi, ti), oÃ¹ zi et ti sont des variables cachÃ©es.
Lâ€™algorithme EM effectue itÃ©rativement une Ã©tape de calcul de lâ€™espÃ©rance (Ã©tape E) puis
une Ã©tape de maximisation (Ã©tapeM), qui garantie la convergence vers un maximum local de la
vraisemblance (Boyles, 1983). Durant lâ€™Ã©tape E, les donnÃ©es manquantes sont estimÃ©es Ã  partir
des donnÃ©es observÃ©es et de lâ€™estimation courante des paramÃ¨tres du modÃ¨le. Durant lâ€™Ã©tapeM,
la vraisemblance est maximisÃ©e sous lâ€™hypothÃ¨se que les donnÃ©es manquantes sont connues.
Lâ€™estimation des donnÃ©es manquantes par lâ€™Ã©tape E est utilisÃ©e Ã  la place des vÃ©ritables valeurs
que lâ€™on ne connaÃ®t pas. Les rÃ¨gles dâ€™adaptation prennent en compte les contraintes de positivitÃ©
et de somme unitaire des paramÃ¨tres :
pi
d[new]
j =
1
M
âˆ‘M
i=1 zËœ
d
ij
Ïƒ2[new] = 1DM
âˆ‘M
i=1
[âˆ‘N0
j=1 zËœ
0
ij(xj âˆ’ wi)2
+
âˆ‘N1
j=1 zËœ
1
ij
g0(xi|qij ;Ïƒ)(I1[(xiâˆ’qij)2+Ïƒ2]+I2)
Lj Â·g1j (xi,Ïƒ)
] (5)
oÃ¹ zËœdij = p(d, j|xi; Î˜) =
pidj g
d
j (xi;Ïƒ)P1
d=0
PNd
j=1 pi
d
j g
d
j (xi;Ïƒ)
est la probabilitÃ© a posteriori que la donnÃ©e
xi soit gÃ©nÃ©rÃ©e par le jme Ã©lÃ©ment gÃ©nÃ©ratif de dimension d (Ã©crit (d, j)), et avec :
I1 = Ïƒ
âˆš
pi
2 (erf(
Qij
Ïƒ
âˆš
2
)âˆ’ erf(Q
i
jâˆ’Lj
Ïƒ
âˆš
2
))
I2 = Ïƒ2
(
(Qijâˆ’Lj) exp(âˆ’
(Qijâˆ’Lj)2
2Ïƒ2 )âˆ’Qij exp(âˆ’
(Qij)
2
2Ïƒ2 )
) (6)
Apprentissage Automatique de la Topologie
Jusquâ€™Ã  prÃ©sent, la position des prototypes qui sont les sommets du graphe de Delaunay,
est figÃ©e une fois que le graphe est construit. Afin dâ€™accroÃ®tre la vraisemblance du modÃ¨le par
rapport aux donnÃ©es, il serait intÃ©ressant de mettre Ã  jour la position des prototypes. Cependant,
lâ€™Ã©tape M impliquant les prototypes nâ€™est pas triviale, donc nous proposons une approximation
de lâ€™Ã©tape M. Nous observons empiriquement que la rÃ¨gle suivante accroÃ®t la plupart du temps
la vraisemblance. Si ce nâ€™est pas le cas pour un prototype, la rÃ¨gle nâ€™est pas appliquÃ©e et
la position de ce prototype nâ€™est pas modifiÃ©e (on passe au suivant). Lâ€™Ã©tape M approchÃ©e
prend en compte la probabilitÃ© que les donnÃ©es soient gÃ©nÃ©rÃ©es par un prototype (wk , k âˆˆ
[1, 2, ..., N0]) et par les arcs ayant ce prototype comme extrÃ©mitÃ© :
w
[new]
k =
PM
i=1
[
zËœ0ikxi+
P
jâˆˆWk zËœ
1
ij
g0(xi|qij ;Ïƒ)
Lj Â·g1j (xi;Ïƒ)
(âˆ’E2wbj+E3xi)
]
PM
i=1
[
zËœ0ik+
P
jâˆˆWk zËœ
1
ijE1
] (7)
oÃ¹Wk reprÃ©sente lâ€™ensemble des arcs [waj , wbj ] ayant wk = waj comme extrÃ©mitÃ©, et oÃ¹
E1 = Ïƒ
2
L2j
[e
âˆ’(Qj)2
2Ïƒ2 (Qj âˆ’ 2Lj)âˆ’ e
âˆ’(Qjâˆ’Lj)2
2Ïƒ2 (Qj âˆ’ Lj)] + 1L2j ((Lj âˆ’Qj)
2 + Ïƒ)I1
E2 = Ïƒ
2
L2j
[eâˆ’
(Qjâˆ’Lj)2
2Ïƒ2 Qj âˆ’ eâˆ’
(Qj)
2
2Ïƒ2 (Qj âˆ’ Lj)]âˆ’ 1L2j (Q
2
j âˆ’ LjQj + Ïƒ2)I1
E3 = 1Lj [e
âˆ’(Qjâˆ’Lj)2
Ïƒ2 âˆ’ e
âˆ’Q2j
Ïƒ2 + (Qj âˆ’ Lj)I1]
(8)
La rÃ¨gle de mise Ã  jour des positions des prototypes peut Ãªtre intÃ©grÃ©e dans un schÃ©ma
EM, appelÃ© algorithme EM gÃ©nÃ©ralisÃ© (GEM) (Dempster et al., 1977) (voir (Gaillard, 2008)
pour le dÃ©veloppement des Ã©quations). Cet algorithme correspond dans notre cas Ã  dâ€™abord
effectuer lâ€™Ã©tape E puis Ã  mettre Ã  jour durant lâ€™Ã©tape M, les paramÃ¨tres pi plus lâ€™un des deux
paramÃ¨tres Ïƒ ou wk âˆˆ w. Le dÃ©veloppement in extenso des rÃ¨gles de mise Ã  jour sont fournis
dans (Gaillard, 2008).
Le principe du GGG est prÃ©sentÃ© sur la figure 5.
3.5 Emergence de la topologie et sÃ©lection de modÃ¨le par le critÃ¨re BIC
Pour obtenir le graphe reprÃ©sentant la topologie (TRG) Ã  partir du modÃ¨le gÃ©nÃ©ratif, lâ€™idÃ©e
clef est de supprimer du graphe DG des prototypes, les Ã©lÃ©ments gaussiens qui nâ€™ont aucune
chance dâ€™avoir gÃ©nÃ©rÃ© des donnÃ©es, i.e les Ã©lÃ©ments associÃ©s Ã  un poids faible : pidj < Î³N0 .
Soit G = {Ïƒ,w,E, pi} le graphe gÃ©nÃ©ratif dÃ©fini par sa variance du bruit Ïƒ2, ses N0 som-
mets w, son ensemble dâ€™arcs E et ses proportions pi. Et soit GN0,Î³N0 = {Ïƒ,w,E, pi |pidj â‰¥
Î³N0}, le graphe gÃ©nÃ©ratif qui contient seulement les Ã©lÃ©ments gÃ©nÃ©ratifs ayant une proportion
plus Ã©levÃ©e que le seuil Î³N0 : pi
d
j â‰¥ Î³N0 , âˆ€d âˆˆ {0, 1}.
En rÃ©glant le paramÃ¨tre Î³N0 de 1 Ã  0, on obtient une sÃ©quence de graphes gÃ©nÃ©ratifs em-
boÃ®tÃ©s allant de lâ€™ensemble vide au graphe DG complet :
G1 = âˆ… âŠ† . . . âŠ† GÎ³N0 âŠ† . . . âŠ† G0 = DG (9)
Supposons que lâ€™un des graphes de la sÃ©quence ait la Â«bonneÂ» topologie, i.e. celle inconnue
des variÃ©tÃ© principales, nous utilisons le CritÃ¨re dâ€™Information BayÃ©sien (BIC) pour sÃ©lection-
ner ce graphe. Le critÃ¨re BIC (Schwartz, 1978) est adaptÃ© et satisfaisant en pratique pour la
M. Aupetit, P. Gaillard et G. Govaert
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0
0.2
0.4
0.6
0.8
1
(a) (b) (c) (d)
FIG. 5 â€“ Principe du Graphe GÃ©nÃ©ratif Gaussien : (a) 300 donnÃ©es (points noirs) sont tirÃ©s dâ€™un
ensemble de variÃ©tÃ©s du planMprin : un segment oblique, un segment horizontal et un point isolÃ©, de
proportions respectives {0.25; 0.5; 0.25}, perturbÃ©es par un bruit gaussien isotrope de moyenne nulle
et de variance Ïƒ2 = 0.001. Les prototypes (disques blancs) sont positionnÃ©s Ã  lâ€™aide dâ€™un modÃ¨le de
mÃ©lange gaussien. (b) Les prototypes sont connectÃ©s avec les arcs du graphe de Delaunay. (c) La den-
sitÃ© de probabilitÃ©gÃ©nÃ©rÃ©e par le graphe gÃ©nÃ©ratif gaussien initial. (d) La densitÃ© gÃ©nÃ©rÃ©e par le GGG
optimal obtenu par maximisation de la vraisemblance en fonction de Ïƒ, pi et w. Noter comment les arcs
se dÃ©placent pour mieux expliquer les donnÃ©es gÃ©nÃ©rÃ©es par les segments, et comment les proportions
diminues lorsquâ€™il nâ€™y a pas de donnÃ©es sous-jacente Ã  expliquer.
sÃ©lection du nombre de composants dâ€™un modÃ¨le de mÃ©lange classique (Roeder et Wasserman,
1997; Fraley et Raftery, 2002). Ici nous lâ€™utilisons pour rÃ©gler le compromis entre la vraisem-
blance P (x;GÎ³N0 ) et la complexitÃ© du graphe gÃ©nÃ©ratif GÎ³N0 . Donc nous supposons que la
topologie du modÃ¨le gÃ©nÃ©ratif dâ€™un ensemble de points est un estimateur de la topologie des
variÃ©tÃ©s principales de cet ensemble, dont la qualitÃ© est mesurÃ©e par le critÃ¨re BIC de vraisem-
blance pÃ©nalisÃ©e :
BIC(N0, GÎ³N0 ) = âˆ’log(P (x;GÎ³N0 )) +
vÎ³N0
2
log(M) (10)
oÃ¹ vÎ³N0 est le nombre de paramÃ¨tres libres du modÃ¨le GÎ³N0 etM le nombre de donnÃ©es.
La vraisemblance de chaque graphe gÃ©nÃ©ratif GÎ³N0 de la sÃ©quence est optimisÃ©e en fonc-
tion des proportions9 pi de telle sorte que tous les graphes gÃ©nÃ©ratifs sont Ã  leur maximum de
vraisemblance. A la fin, le graphe reprÃ©sentant la topologie pour un nombre de prototypes N0
donnÃ©, est celui dÃ©fini par le graphe gÃ©nÃ©ratif GÎ³âˆ—N0 minimisant le critÃ¨re BIC Ã  N0 fixÃ© (eq.
(10)). On recommence pour chaque valeur N0 et lâ€™on retient finalement le TRG optimal asso-
ciÃ© au couple de paramÃ¨tres (Nâˆ—0 , Î³
âˆ—
Nâˆ—0
) qui minimise BIC. Lâ€™algorithme complet est donnÃ© sur
la figure 6 et est illustrÃ© sur la figure 7.
3.6 ComplexitÃ© des temps de calcul
La complexitÃ© en temps de calcul du GGG est O(D(N0 + N1)Mtmax) plus le temps
O(DN30 ) requis pour construire le graphe de Delaunay (Agrell, 1993) qui domine le temps
total au pire cas. Dâ€™autres graphes non-paramÃ©triques moins gourmands en temps de calcul
9Pour des raisons de temps de calcul, seuls les proportions pi sont optimisÃ©es durant cette Ã©tape. En commen-
Ã§ant par les proportions normalisÃ©s obtenus avec le GGG complet, le problÃ¨me est convexe et lâ€™algorithme converge
rapidement. La variance du bruit est aussi supposÃ©e peu variable.
Apprentissage Automatique de la Topologie
Algorithme : Graphe GÃ©nÃ©ratif Gaussien
EntrÃ©e Choisir Nmax0 , le nombre de prototypes maximal
POUR chaque N0 âˆˆ {1, ..., Nmax0 }
Initialisation Positionner les prototypes w avec un GM isotropique
Construire le DG (ou lâ€™IDT) des prototypes
Initialiser pi Ã  1/(N0 +N1)
Initialiser Ïƒ Ã  la valeur trouvÃ©e par le GM
EM Utiliser la rÃ¨gle de mise Ã  jour (5) pour trouver Ïƒâˆ—,
piâˆ—, wâˆ— maximisant la vraisemblance P .
Elagage Construire une sÃ©quence emboÃ®tÃ©e de GGG par rapport Ã  piâˆ—
BIC Calculer le critÃ¨re BIC(GN0,Î³âˆ—N0 ) = minÎ³N0 (BIC(N0, GÎ³N0 )) (10)
FIN POUR
Sortie Retourner le TRG minimisant BIC pour tout N0 âˆˆ {1, ..., Nmax0 }
et ce critÃ¨re BIC minimal
FIG. 6 â€“ Algorithme du Graphe GÃ©nÃ©ratif GaussienGM : modÃ¨le de mÃ©lange gaussien ; DG : graphe
de Delaunay ; IDT : Triangulation Induite de Delaunay ; TRG : graphe reprÃ©sentant la topologie ; BIC :
critÃ¨re dâ€™information bayÃ©sien.
Î³Nâˆ—0 = 0.48 Î³Nâˆ—0 = 0.25 Î³Nâˆ—0 = 0.23 Î³Nâˆ—0 = 0.0022 Î³Nâˆ—0 = 1
âˆ’100
log(P ) = âˆ’9871 log(P ) = âˆ’8310 log(P ) = 527 log(P ) = 529 log(P ) = 529
BIC = 19783 BIC = 16662 BIC = âˆ’1009 BIC = âˆ’1013 BIC = âˆ’1007
(a) (b) (c) (d) (e)
FIG. 7 â€“ Emergence de la topologie avec le critÃ¨re dâ€™information bayÃ©sien (BIC) : (a-e) 5 graphes
gÃ©nÃ©ratifs GÎ³Nâˆ—0
issus de la sÃ©quence de graphes emboÃ®tÃ©s basÃ©es sur Î³Nâˆ—0 (ici N
âˆ—
0 = 4). Pour chaque
graphe gÃ©nÃ©ratif prÃ©sentÃ©, la valeur du seuil Î³Nâˆ—0 , la log-vraisemblance et la valeur de BIC associÃ©e
sont fournies au-dessus de chaque tracÃ©. (d) Le graphe gÃ©nÃ©ratif de la sÃ©quence emboÃ®tÃ©e qui minimise
BIC. Ce graphe est le graphe reprÃ©sentant la topologie (TRG) dont la connexitÃ© estime celle des variÃ©tÃ©s
principales.
M. Aupetit, P. Gaillard et G. Govaert
peuvent Ãªtre envisagÃ©s Ã  la place du DG, comme le graphe des plus-proche-voisins (NNG)
O(DN20 ), lâ€™arbre recouvrant minimal (MST) O(DN
2
0 ), le graphe des voisins relatifs (RNG)
O(DN30 ), les graphe de Gabriel (GG) O(DN
3
0 ) ou la triangulation induite de Delaunay (IDT)
O(DN0M). Cependant, ces graphes portent moins dâ€™information que le graphe de Delaunay
dont ils sont des sous-graphes10.
Quand la dimension des donnÃ©es augmente, nous proposons de considÃ©rer lâ€™IDT (obtenu
avec lâ€™algorithme CHL) au lieu du graphe de Delaunay comme graphe initial du GGG. En
effet, lâ€™IDT contient en gÃ©nÃ©rale plus dâ€™arcs que nÃ©cessaire pour modÃ©liser la connexitÃ©. Nous
utilisons lâ€™IDT lorsque la dimension D est supÃ©rieure Ã  4.
4 ExpÃ©riences
4.1 ProblÃ¨me jouet
Dans ces expÃ©riences, nous souhaitons vÃ©rifier la pertinence du GGG pour apprendre la
connexitÃ© dâ€™un ensemble de donnÃ©es jouets. Lâ€™ensemble (Figure 8) consiste en 300 points du
plan, tirÃ©s dâ€™une spirale et 200 dâ€™un point isolÃ©. Les points sont perturbÃ©s par un bruit gaussien
additif de moyenne 0 et de variance 0.0025.
Sur la figure 8, nous comparons le TRG obtenu avec le GGG, Ã  ceux obtenus avec le CHL,
le CHL filtrÃ© pour lequel les arcs qui ont un nombre de tÃ©moins infÃ©rieur Ã  un seuil T sont
supprimÃ©s, et le GNG. Le GGG est optimisÃ© avec lâ€™algorithme dÃ©crit en section 3.4 afin de
retrouver les variÃ©tÃ©s principales. Nous utilisons le critÃ¨re BIC pour sÃ©lectionner la complexitÃ©
du modÃ¨le avec diffÃ©rentes valeurs pour le nombre de prototypesN0. A la fin, le graphe gÃ©nÃ©ra-
tif optimal comprend 13 prototypes. Pour le CHL et le CHL filtrÃ©, nous utilisons les prototypes
trouvÃ©s pour le GGG optimal. Pour le CHL filtrÃ©, nous fixons le seuil T de telle sorte que le
graphe obtenu corresponde visuellement au mieux Ã  la solution attendue. Cependant, rappe-
lons que rÃ©gler le seuil T requiert un contrÃ´le visuel et ne correspond Ã  lâ€™optimum dâ€™aucune
fonction dâ€™Ã©nergie, ce qui rend totalement arbitraire lâ€™utilisation de ce modÃ¨le en dimension
supÃ©rieure Ã  3. Pour le GNG, nous dÃ©finissons le nombre maximum de prototypes comme le
nombre de prototypes trouvÃ©s pour le GGG optimal. Tous les autres paramÃ¨tres du GNG sont
rÃ©glÃ©s aux valeurs indiquÃ©es dans lâ€™article original (Fritzke, 1995). En particulier, le paramÃ¨tre
dâ€™Ã¢ge11 est fixÃ© arbitrairement Ã  50. Dans toutes les expÃ©riences suivantes, la mÃ©thodologie est
la mÃªme.
Le CHL filtrÃ© ou non, et le GNG ne sont pas en mesure de retrouver la vraie connexitÃ©
de lâ€™ensemble de points. En particulier, un groupe de points isolÃ© (Figure 8 (b)) si T = 0. En
remarquant que T1 < T2 â‡’ IDT (T2) âŠ† IDT (T1), on peut voir sur la figure 8c quâ€™aucun
seuil T ne permet de retrouver la vraie connexitÃ© de lâ€™ensemble de points.
4.2 ConnexitÃ© des donnÃ©es Teapot
Les donnÃ©es Teapot originales ont Ã©tÃ© crÃ©Ã©es Ã  partir de la vue sous diffÃ©rents angles, dâ€™une
thÃ©iÃ¨re en rotation autour dâ€™un axe vertical (Weinberger et Saul, 2006) (Figure 9). Dans cette
10En particulierNNG âŠ‚MST âŠ‚ RNG âŠ‚ GG âŠ‚ DG (Veltkamp, 1991)
11Notons quâ€™aucune mÃ©thode objective nâ€™est fournie dans (Fritzke, 1995) pour rÃ©gler ces paramÃ¨tres.
Apprentissage Automatique de la Topologie
âˆ’0.6 âˆ’0.4 âˆ’0.2 0 0.2 0.4 0.6 0.8 1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
âˆ’0.6 âˆ’0.4 âˆ’0.2 0 0.2 0.4 0.6 0.8 1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
âˆ’0.6 âˆ’0.4 âˆ’0.2 0 0.2 0.4 0.6 0.8 1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
âˆ’0.6 âˆ’0.4 âˆ’0.2 0 0.2 0.4 0.6 0.8 1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
(a) GGG (b) CHL : T = 0 (c) CHL : T âˆ— = 19 (d) GNG
FIG. 8 â€“ Une spirale et un point isolÃ© : le TRG dÃ©fini par (a) le GGG, (b) le CHL, (c) le CHL filtrÃ©, (d)
le GNG. Le GGG est le seul modÃ¨le capable de retrouver la vraie connexitÃ©.
FIG. 9 â€“ Huit images de la base de donnÃ©es Â«teapotÂ» originale : la couleur des boÃ®tesencode le degrÃ©
de rotation de la thÃ©iÃ¨re. Quelques images ont Ã©tÃ© retirÃ©es entre les boÃ®tes rouges et bleues foncÃ©es, et
entre les boÃ®tes bleue-vertes et vertes clair, pour crÃ©er artificiellement un ensemble composÃ© de deux
composantes connexes.
expÃ©rience nous utilisons les donnÃ©es fournies par Zhu et Lafferty (2005) 12 oÃ¹ les images sont
converties en Ã©chelle de gris et rÃ©duites Ã  une taille de 12Ã— 16 pixels. Ce nouvel ensemble de
donnÃ©es a Ã©tÃ© conÃ§u dans le cadre de la reconnaissance de formes, pour tester des algorithmes
devant dÃ©terminer automatiquement si lâ€™anse se trouve Ã  droite ou Ã  gauche. Donc les images
dans lesquelles lâ€™anse se trouve dans lâ€™axe de la prise de vue sont supprimÃ©es. Finalement, 365
images sont disponibles. Bien quâ€™en dimension 192, les donnÃ©es se trouvent au voisinage dâ€™une
variÃ©tÃ© de dimension 1 paramÃ©trÃ©e par lâ€™angle de rotation de la prise de vue, et cette variÃ©tÃ©
est sÃ©parÃ©e en deux composantes connexes, lâ€™une contenant les images avec lâ€™anse Ã  droite,
lâ€™autre celles avec lâ€™anse Ã  gauche. Nous voulons retrouver ces caractÃ©ristiques topologique (2
composantes connexes de dimension intrinsÃ¨que 1).
Afin dâ€™analyser la structure sous-jacente aux donnÃ©es, les techniques de rÃ©duction de di-
mension sont largement utilisÃ©es. Cependant, du fait de la perte dâ€™information quâ€™elles en-
gendrent, la plupart des distances visualisÃ©es sont soit comprimÃ©es soit Ã©tirÃ©es, et il est donc
difficile de savoir si les formes observÃ©es existent ou non dans lâ€™espace ambient (Aupetit,
2007). Dans les expÃ©riences suivantes, nous montrons que le GGG est une mÃ©thode complÃ©-
mentaire aux techniques de projection "classiques" pour analyser un ensemble de donnÃ©es.
Nous utilisons lâ€™Analyse en Composantes Principales (Bishop, 1995) (ACP), le Generative
Topographic Mapping (Bishop et al., 1998) (GTM)13 et ISOMAP (de Silva et Tenenbaum,
2003)14 pour visualiser les donnÃ©es Teapot (figure 10).
12DonnÃ©es fournies sur le site internet http ://pages.cs.wisc.edu/âˆ¼jerryzhu
13Une implÃ©mentation Matlab du GTM est fournie Ã  lâ€™adresse internet http ://www.ncrg.aston.ac.uk/GTM/
14Une implÃ©mentation Matlab dâ€™ISOMAP est fournie Ã  lâ€™adresse internet http ://web.mit.edu/cocosci/isomap/code/
M. Aupetit, P. Gaillard et G. Govaert
PCA GTM ISOMAP 10-NN ISOMAP 15-NN ISOMAP 20-NN
(a) (b) (c) (d) (e)
FIG. 10 â€“ Cinq projections des donnÃ©es Â«teapotÂ» : la couleur des points correspond Ã  la couleur
utilisÃ©e sur la figure 9. Projection des donnÃ©es Â«teapotÂ» suivant les deux premiers axes principaux (ACP)
(a), par le GTM dÃ©fini par une grille 20 Ã— 20 et une transformation non linÃ©aire obtenue par une grille
10Ã—10 de fonctions gaussiennes (b), et par ISOMAP utilisant un graphe desK-plus-proches voisins avec
K = {10, 15, 20} (c-e). Aucune de ces projections nâ€™est en mesure de montrer la structure dÃ©connectÃ©e
originelle.
Nous optimisons le GGG avec lâ€™algorithme dÃ©crit en section 3.4 avec un N0 candidat
compris entre 40 et 80, afin de retrouver la connexitÃ© des variÃ©tÃ©s principales de cet ensemble.
Le graphe gÃ©nÃ©ratif optimal est finalement dÃ©fini par 67 prototypes.
Le TRG rÃ©sultant nous informe sur lâ€™existence de 2 composantes connexes en dÃ©pit de ce
que montrent les techniques de projection classiques (figure 10). Lâ€™analyse des degrÃ©s des som-
mets15 du TRG (les degrÃ©s valent 2, sauf pour les 4 sommets extrÃ©mitÃ©s des deux composantes
connexes, dont le degrÃ© vaut 1) montre que chaque composante est une chaÃ®ne de sommets,
donc une variÃ©tÃ© homÃ©omorphe Ã  un segment, montrant que la dimension intrinsÃ¨que de ces
deux variÃ©tÃ©s vaut 1.
De plus, le modÃ¨le Ã©tant gÃ©nÃ©ratif nous savons aussi que les deux variÃ©tÃ©s ont Ã  peu prÃ¨s
la mÃªme probabilitÃ© a priori : 0.507 et 0.493, et que le long des variÃ©tÃ©s, les donnÃ©es sont Ã 
peu prÃ¨s uniformÃ©ment distribuÃ©es, puisque la moyenne et la variance de la quantitÃ©
pi1j
Lj
sont
respectivement : 4.5966eâˆ’ 005 et 1.5720eâˆ’ 010.
Enfin, nous comparons le TRG obtenu par le GGG avec celui obtenu par le CHL et le
GNG. Pour le CHL, nous utilisons les prototypes obtenus par le GGG optimal. Pour le GNG,
le processus de croissance est maintenu jusquâ€™Ã  obtenir 67 prototypes.
La figure 11 montre que le CHL ne permet pas de retrouver la connexitÃ© des donnÃ©es.
En particulier, le TRG obtenu par le CHL ne possÃ¨de quâ€™une seule composante connexe qui
contient des sommets de degrÃ© 3 ou plus. Le GNG permet de retrouver la bonne connexitÃ©
mais ses mÃ©ta-paramÃ¨tres doivent Ãªtre rÃ©glÃ©s manuellement sans critÃ¨re objectif.
4.3 Classification non supervisÃ©e dâ€™images avec le TRG
Dans cette expÃ©rience, nous avons sÃ©lectionnÃ© les images de 5 objets (figure 12) de la base
dâ€™objets fournie par lâ€™Amsterdam Library of Object Images (ALOI) Geusebroek et al. (2005).
Pour chaque objet, un camera a enregistrÃ© 72 images en faisant tourner lâ€™objet autour dâ€™un axe
vertical avec un pas angulaire de 5 degrÃ©s. A nouveau, la taille des images est rÃ©duite Ã  12Ã—16
15Dans un graphe, le degrÃ© dâ€™un sommet est le nombre dâ€™arcs qui ont ce sommet comme extrÃ©mitÃ©
Apprentissage Automatique de la Topologie
(a) (b) (c)
FIG. 11 â€“ Projections de diffÃ©rent Graphes ReprÃ©sentant la Topologie (TRG) : le code de couleurs
est celui dÃ©fini sur la figure 9. (a-c) Les donnÃ©es et les prototypes sont projetÃ©s par ISOMAP Ã  partir
du graphe des 15-PPV. Les projections diffÃ¨rent de celles des figures 10 (d-f) car les prototypes sont
inclus dans lâ€™ensemble projetÃ©. Projection du TRG obtenu avec le GGG (a), le CHL (b) et le GNG (c).
lâ€™Ã©paisseur des arcs est proportionnelle aux proportions pi1 du GGG (a), au nombre de points tÃ©moins
des arcs du CHL (b) et Ã  lâ€™Ã¢ge final des arcs du GNG (c). Le GGG et le GNG permettent de retrouver la
vraie connexitÃ©, mais les mÃ©ta-paramÃ¨tres du GNG ont Ãªtre rÃ©glÃ©s Ã  la main donc arbitrairement.
FIG. 12 â€“ Cinq objets de la base ALOI. De gauche Ã  droite : une carte Ã  jouer, un kiwi, une balle, une
alarme, un tube.
pixels. Nous nous attendons Ã  trouver dans lâ€™espace des pixels Ã  192 dimensions, 5 groupes de
72 points chacuns correspondant aux images dâ€™un objet particulier.
Nous utilisons lâ€™ACP, le GTM et ISOMAP pour visualiser lâ€™ensemble de points. Les figures
13(a-c) montrent que les techniques classiques de projection ne permettent pas de dÃ©tecter
visuellement 5 groupes de points.
Nous optimisons le GGG avec lâ€™algorithme de la section 3.4 avec un N0 candidat compris
entre 60 and 90, pour retrouver la connexitÃ© des variÃ©tÃ©s principales de cet ensemble de points.
Enfin, nous assimilons chaque composante connexe du TRG obtenu, Ã  un groupe de points.
Comme le TRG correspond Ã  un modÃ¨le gÃ©nÃ©ratif, chaque donnÃ©e peut Ãªtre assignÃ©e Ã  un
groupe par la rÃ¨gle du maximum a posteriori. Nous comparonscette classification non super-
visÃ©e avec celle obtenue avec le CHL et le GNG. Pour ces deux modÃ¨les, les donnÃ©es appar-
tiennent Ã  la composante connexe contenant leur prototype le plus proche. Pour comparer la
qualitÃ© des diffÃ©rentes classifications, nous utilisons la fonction dâ€™erreur suivante :
E =M(M âˆ’1)/2âˆ‘Mâˆ’1i=1 âˆ‘Mj=i+1 Î´ij oÃ¹ Î´ij vaut 1 si les donnÃ©es i et j appartiennent par
erreur au mÃªme groupe ou Ã  un groupe diffÃ©rent, et 0 sinon.
Nous rÃ©pÃ©tons 10 fois la procÃ©dure (lâ€™intialisation alÃ©atoire des paramÃ¨tre initiaux et la
M. Aupetit, P. Gaillard et G. Govaert
convergence vers un optimum local de la fonction optimisÃ©e mÃ¨nent Ã  des paramÃ¨tres otpi-
maux et donc des classifications diffÃ©rentes.). Nous reportons sur la figure 14 la moyenne
de lâ€™erreur de classification et la moyenne du nombre de composantes connexes trouvÃ©es par
chaque algorithme. Les rÃ©sultats expÃ©rimentaux montrent que le GGG est meilleur que les
autres mÃ©thodes suivant ces deux indices de qualitÃ© : il fournit des estimations plus prÃ©cises et
plus stables de la vraie connexitÃ©.
Sur la figure 13(d), nous traÃ§ons la meilleure classification obtenue par chaque algorithme
aprÃ¨s 10 essais. Le graphe gÃ©nÃ©ratif optimal est dÃ©fini par 80 prototypes. Le TRG rÃ©sultant nous
montre quâ€™il existe 5 variÃ©tÃ©s sÃ©parÃ©es, et la classification ne fait aucune erreur. Remarquons
que les classifications basÃ©es sur des prototypes (K-Means ou MÃ©langes de gaussiennes) nous
auraient fourni autant de groupes que de prototypes. En comparaison, le CHL fournit seulement
3 composantes connexes, et le GNG en trouve 7 (il sÃ©pare un groupe lÃ©gitime en 4 groupes, et
connecte deux groupes normalement sÃ©parÃ©s).
4.4 Des donnÃ©es vectorielles aux donnÃ©es de type graphe
Dans cette expÃ©rience, nous montrons comment un TRG peut Ãªtre utilisÃ© pour coder lâ€™infor-
mation topologique contenue dans des donnÃ©es de type nuages de points. Nous utilisons la base
de donnÃ©es MNIST qui regroupe des images de chiffres manuscrits. Les 100 premiÃ¨res images
de chaque chiffre (soit 1000 images de 28 Ã— 28 pixels) sont transformÃ©es en images binaires
(noir et blanc) 16, et pour chaque image, on positionne une donnÃ©es Ã  la place de chaque pixel
blanc dans le plan image. Lâ€™ensemble constitue une base de 1000 nuages de points, chacun de
ces nuages reprÃ©sentant un chiffre et pouvant contenir jusquâ€™Ã  784 points. Nous construisons
le TRG Ã  partir du GGG sur chacun de ces 1000 nuages de points (Quelques exemple de TRG
obtenus sont prÃ©sentÃ©s sur la figure 15).
Nous classifions manuellement chaque TRG en fonction de sa topologie, et nous obtenons
7 classes principales dâ€™homotopie representÃ©es sur la premiÃ¨re ligne du tableau 16. Chaque
classe dâ€™homotopie peut Ãªtre reprÃ©sentÃ©e par une structure de graphe, et donc chaque image
dâ€™un chiffre initialement vue comme un nuage de points, est transformÃ©e en un donnÃ©e de type
graphe, contenant lâ€™essentiel de lâ€™information topologique.
En quoi ces classes peuvent-elles Ãªtre utiles ? La plupart des chiffres appartiennent en ma-
joritÃ© Ã  une ou deux classes diffÃ©rentes. Donc pour une image dâ€™un chiffre inconnu, le calcul de
sa classe dâ€™homotopie avec un TRG fournit un a priori sur la probabilitÃ© que ce caractÃ¨re ma-
nuscrit soit tel ou tel chiffre. Aussi il nous semble prometteur de combiner un classifieur basÃ©
sur les pixels avec un classifieur basÃ© sur la classe dâ€™homotopie extraite du TRG obtenu par un
GGG. Pour lâ€™instant, ce travail reste Ã  effectuer pour Ã©valuer lâ€™apport de cette approche. Notons
aussi que les classes dâ€™homotopie ont Ã©tÃ© dÃ©terminÃ©es manuellement, il faudrait concevoir une
mÃ©thode de calcul automatique de ces classes.
16La base MNIST est un sous-ensemble dâ€™une base plus grande du NIST, dans laquelle les images sont en noir et
blanc.
Apprentissage Automatique de la Topologie
(a) (b) (c)
(d) (e) (f)
FIG. 13 â€“ Analyse des donnÃ©es ALOI : (a-c) Projections des donnÃ©es ALOI avec (a) lâ€™ACP, (b) le
GTM (grille 40 Ã— 40 et transformation non linÃ©aire sur une grille de 10 Ã— 10 fonctions gaussiennes,
(c) ISOMAP (70-PPV). (d-f) Projection des donnÃ©es par ISOMAP des graphes obtenus par les diffÃ©rents
algorithmes. La couleur indique les classes en termes de composantes connexes des graphes GGG (d),
CHL (e), et GNG (f). Sur le tracÃ© (d), oÃ¹ les couleurs sont aussi en accord avec la classe rÃ©elle, les cartes
Ã  jouer sont reprÃ©sentÃ©es par un âˆ—, les kiwis par un âˆ‡, les balles par un âˆ†, lâ€™alarme par un O et le
tube par un . Le GGG est le seul capable de retrouver les vraies classes. Par ailleurs, on peut noter que
sans connaÃ®tre les classes Ã  lâ€™avance, les diffÃ©rentes projections (a-c) sont trompeuses quant au nombre
original de classes Ã  trouver. Lâ€™ACP (a) semble montrer quâ€™il existe deux classes distinctes, le GTM (b)
fournit un trÃ©s grand nombre de classes, et ISOMAP montre 3 classes distinctes, alors que les points de
la classe de gauche appartiennent en fait Ã  une unique classe comme on le voit en vert sur le tracÃ© (d).
GGG CHL GNG
E (%) 0.1Â± 0.2 13.0Â± 8.3 4.4Â± 2.1
# connected components 5.4Â± 0.9 3.0Â± 0.7 7.1Â± 0.8
FIG. 14 â€“ Robustesse de la classification par composantes connexes des images ALOI
M. Aupetit, P. Gaillard et G. Govaert
FIG. 15 â€“ GGG sur les images MNIST : Quelques exemples de TRG obtenus avec le GGG travaillant
dans le plan image sur les images MNIST.
a b c d e f g
Other
0 95 3 2
1 98 1 1
2 7 52 19 11 11
3 42 18 31 9
4 2 2 16 52 10 18
5 64 26 3 7
6 25 63 12
7 6 84 2 4 2
8 4 8 18 9 45 16
9 2 2 85 4 7
FIG. 16 â€“ Topologie des images MNIST : la topologie du TRG rÃ©sultant de lâ€™application du GGG sur
chacun des 1000 chiffres, a Ã©tÃ© classÃ©e manuellement en 7 classes principales dâ€™homotopie reprÃ©sentÃ©es
sur la premiÃ¨re ligne. Par exemple, on peut lire que 19% des TRG modÃ©lisant le chiffre 2 ont la mÃªme
connexitÃ© quâ€™un cercle connectÃ© Ã  un segment (colonne d).
Apprentissage Automatique de la Topologie
4.5 Discussion
4.5.1 Avantages et limites du GGG
Le Graphe GÃ©nÃ©ratif Gaussien (GGG) permet de contourner les limites de lâ€™algorithme
Competitive Hebbian Learning (CHL) pour modÃ©liser la connexitÃ©. En particulier, il per-
met de prendre en compte le bruit, et de mesurer la qualitÃ© du modÃ¨le, mÃªme lorsquâ€™aucune
visualisation nâ€™est possible. Cependant, la complexitÃ© en temps de calcul est plus Ã©levÃ©e :
O(D(N0 + N1)Mtmax) plus le temps requis pour construire le graphe initial pour le GGG,
tandis quâ€™il faut un temps O(DN0M) pour le CHL.
4.5.2 ModÃ¨le de mÃ©lange gÃ©nÃ©ralisÃ©
Le GGG peut Ãªtre vu comme une gÃ©nÃ©ralisation des modÃ¨les de mÃ©lange classiques, Ã  des
points et des segments : un mÃ©lange de gaussiennes est un GGG sans arcs. Le GGG fournit
une estimation de la densitÃ© de probabilitÃ© des donnÃ©es plus prÃ©cise que celle dâ€™un mÃ©lange
de gaussiennes basÃ© sur le mÃªme ensemble de prototypes et la mÃªme hypothÃ¨se de bruit gaus-
sien isovariÃ© (parce que le GGG ajoute les segments gaussiens Ã  lâ€™ensemble des points gaus-
siens). Le GGG et surtout son extension aux complexes simpliciaux (non traitÃ© ici mais dont
le principe reste identique basÃ©s sur des k-simplexes gÃ©nÃ©ratifs) permettent naturellement de
modÃ©liser des densitÃ©s uniformes par morceaux plus prÃ©cisÃ©ment quâ€™un modÃ¨le de mÃ©lange
de gaussiennes classique. Le GGG fournit aussi intrinsÃ¨quement par sa structure gÃ©nÃ©ratrice
un modÃ¨le explicite de la connexitÃ© des variÃ©tÃ©s principales. Par contraste, les autres modÃ¨les
gÃ©nÃ©ratifs ne fournissent aucun indice sur cette connexitÃ©, exceptÃ© le Generative Topographic
Mapping (Bishop et al., 1998) et les courbe principales probabilistes (Tibshirani, 1992). Ce-
pendant, dans ces deux cas, la connexitÃ© du modÃ¨le est contrainte a priori et non apprise des
donnÃ©es.
4.5.3 RÃ©partition des degrÃ©s de libertÃ©
On pourrait envisager un modÃ¨le de bruit plus flexible, en individualisant les variances pour
chaque prototype, et en considÃ©rant la matrice de covariance complÃ¨te au lieu dâ€™une matrice
identitÃ© comme câ€™est le cas pour le GGG. Cependant, nous considÃ©rons que les modÃ¨les de
mÃ©lange gaussiens classiques donnent trop de flexibilitÃ© au modÃ¨le au mauvais endroit (sauf
dans le cas oÃ¹ lâ€™on a une connaissance prÃ©cise du processus de gÃ©nÃ©ration des donnÃ©es qui
justifie cette rÃ©partition des degrÃ©s de libertÃ©). En effet, ils tentent dâ€™expliquer la topologie
non triviale des variÃ©tÃ©s principales par une structure trop simpliste (un ensemble de points
gÃ©nÃ©ratifs) et un modÃ¨le de bruit trop complexe (des gaussiennes ellipsoÃ¯dales indÃ©pendantes
les unes des autres). Au contraire, on reporte dans le GGG les degrÃ©s de libertÃ© sur le mo-
dÃ¨le structurel plutÃ´t que sur le modÃ¨le de bruit : une structure complexe (un graphe voire un
complexe simplicial) et un bruit simple (isovariÃ©). Donc si lâ€™on complexifiait le modÃ¨le GGG
en libÃ©rant les variances de chaque prototypes (et en interpolant linÃ©airement ces variances le
long des arcs par exemple), ou mÃªme en supposant des densitÃ©s non uniformes (linÃ©aires par
exemple) le long des arc, on aurait un modÃ¨le plus flexible et probablement identifiable, mais il
faudrait un nombre de donnÃ©es beaucoup plus important pour estimer les paramÃ¨tres et surtout
Ã©viter que deux modÃ¨les radicalement diffÃ©rents en termes de topologie (structure complexe et
M. Aupetit, P. Gaillard et G. Govaert
bruit simple, ou structure simple et bruit complexe) soient tout aussi vraisemblables (au sens
de BIC) lâ€™un que lâ€™autre.
4.5.4 Topologie, identifiabilitÃ© et critÃ¨re BIC
Dans ce travail nous faisons explicitement lâ€™hypothÃ¨se que la connexitÃ© dâ€™un graphe gÃ©-
nÃ©ratif optimal au sens du critÃ¨re BIC par rapport Ã  des donnÃ©es issues de certaines variÃ©tÃ©s
principales, est proche de la connexitÃ© de ces variÃ©tÃ©s. Nous avons montrÃ© que câ€™Ã©tait le cas sur
quelques exemples, mais il reste Ã  le dÃ©montrer thÃ©oriquement. Mesurer la "proximitÃ©" topolo-
gique de deux variÃ©tÃ©s nâ€™est pas trivial : un sphÃ¨re percÃ©e dâ€™un trou a la topologie dâ€™un disque
aussi petit que soit ce trou tant quâ€™il existe. Donc si lâ€™on considÃ¨re deux sphÃ¨res superposÃ©es,
lâ€™une ayant un trou et lâ€™autre pas, on peut avoir une distance (de Hausdorff par exemple) aussi
petite que lâ€™on veut entre ces deux objets en rÃ©duisant la taille du trou sans pour autant rendre
identique la topologie de ces deux variÃ©tÃ©s (elles ne sont pas homÃ©omorphes). Ainsi deux
objets gÃ©omÃ©triquement trÃ¨s proches peuvent avoir une topologie radicalement diffÃ©rente. Ce-
pendant, si lâ€™on considÃ¨re un Ã©chantillon fini de telles variÃ©tÃ©s, on ne connaitra leur topologie
quâ€™au travers de cet Ã©chantillon et la topologie trouvÃ©e dÃ©pendra des hypothÃ¨ses dÃ©finissant le
modÃ¨le. Vu sous lâ€™angle gÃ©nÃ©ratif, le problÃ¨me se pose alors ainsi : Ã©tant donnÃ©es deux variÃ©-
tÃ©s gÃ©nÃ©ratrices qui expliquent tout aussi bien le nuage de points en termes de vraisemblance
(proximitÃ© gÃ©omÃ©trique), mais avec une topologie diffÃ©rente, lequel choisir ?
Le principe du rasoir dâ€™Occam nous incite Ã  choisir le plus parcimonieux, celui ayant le
moins de paramÃ¨tres. Le critÃ¨re BIC est un critÃ¨re qui pÃ©nalise la complexitÃ© et donc qui joue le
rÃ´le du rasoir dâ€™Occam. Cependant nous devons dÃ©montrer quâ€™il ne peut y avoir deux modÃ¨les
GGG ayant la mÃªme vraisemblance et la mÃªme complexitÃ© mais avec une topologie diffÃ©rente,
ou au moins que cela est peu probable. Ce problÃ¨me est liÃ© Ã  lâ€™identifiabilitÃ© du GGG et nous
pensons que la clÃ© se situe dans le pouvoir explicatif dâ€™un segment gaussien : un segment
gaussien Ã  le pouvoir explicatif dâ€™un mÃ©lange dâ€™une infinitÃ© de gaussiennes uniformÃ©ment
rÃ©parties le long du segment, donc Ã  vraisemblance Ã©gale, la complexitÃ© dâ€™un mÃ©lange de K
gaussiennes rÃ©parties uniformÃ©ment sur un segment (KD + 1) est toujours plus grande que
celle dâ€™un segment gaussien (2D+ 1). Il est donc toujours plus avantageux du point de vue de
la complexitÃ© et donc du critÃ¨re BIC dâ€™expliquer localement des donnÃ©es issues dâ€™une variÃ©tÃ©
linÃ©ique uniforme perturbÃ©e par un bruit gaussien, avec un segment gaussien (mÃªme topologie)
quâ€™avec un mÃ©lange de K points gaussiens (K variÃ©tÃ©s de dimension 0). De mÃªme, expliquer
ces donnÃ©es avec un d-simplexe gaussien (d > 1) dont les sommets seraient alignÃ©s (donc une
variÃ©tÃ© Ã©lÃ©mentaire dont la dimension est trop grande par rapport Ã  celle de la variÃ©tÃ© principale)
augmenterait la complexitÃ© du modÃ¨le ((d + 1)D + 1) sans amÃ©liorer la vraisemblance, donc
dÃ©graderait aussi le score BIC. BIC serait donc un bon critÃ¨re dâ€™adÃ©quation topologique du
modÃ¨le GGG aux donnÃ©es. Cette piste reste encore Ã  dÃ©fricher et formaliser.
5 Conclusion
5.1 RÃ©sumÃ©
Nous avons proposÃ© un cadre dans lequel le problÃ¨me de lâ€™apprentissage de la topologie
dâ€™un nuage de points peut Ãªtre posÃ© comme un problÃ¨me dâ€™apprentissage statistique. Nous
Apprentissage Automatique de la Topologie
avons dÃ©fini un modÃ¨le gÃ©nÃ©ratif basÃ© sur le graphe de Delaunay de prototypes, permettant
dâ€™apprendre la connexitÃ© des variÃ©tÃ©s principales dâ€™un nuage de points. Ce modÃ¨le est flexible,
parcimonieux, self-consistent, robuste au bruit, ne nÃ©cessite pas de rÃ©glage manuel arbitraire
des mÃ©ta-paramÃ¨tres, fournit une mesure objective de qualitÃ© par la vraisemblance pÃ©nalisÃ©e
au sens de BIC, et dont la connexitÃ© est calculable. Nous avons montrÃ© sur des exemples jouets
et des donnÃ©es rÃ©elles de grande dimension que ce modÃ¨le fournit effectivement une bonne
estimation de la connexitÃ© des variÃ©tÃ©s principales, avec de meilleurs score que les mÃ©thodes
de lâ€™Ã©tat de lâ€™art. Nous lâ€™avons utilisÃ© en classification non supervisÃ©, et montrÃ© quâ€™il Ã©tait
utile pour lâ€™analyse exploratoire de donnÃ©es en fournissant une vue des donnÃ©es plus juste
et complÃ©mentaire des mÃ©thodes de visualisation par projection. Nous avons aussi proposÃ©
de lâ€™utiliser pour extraire dâ€™images de caractÃ¨res manuscrits des caractÃ©ristiques topologiques
utilisables comme entrÃ©es supplÃ©mentaires de mÃ©thodes de reconnaissance de formes.
5.2 Perspectives
La triangulation induite de Delaunay a Ã©tÃ© Ã©tendue (de Silva et Carlsson, 2004) pour gÃ©nÃ©rer
des simplexes de dimension supÃ©rieure Ã  1, qui forment un complexe simplicial appelÃ© "witness
complex". Une piste Ã  suivre consiste Ã  Ã©tendre le modÃ¨le de graphe gÃ©nÃ©ratif dÃ©crit ici, au
cas dâ€™un complexe simplicial, ce qui permettrait dâ€™accÃ©der Ã  des caractÃ©ristiques topologiques
plus riches (nombre de Betti, dimension intrinsÃ¨que supÃ©rieure Ã  1, au lieu de la seule arc-
connexitÃ©).
Nous Ã©tudions aussi lâ€™utilisation de ce modÃ¨le comme support dâ€™un apprentissage semi-
supervisÃ© oÃ¹ la structure des donnÃ©es non Ã©tiquetÃ©es joue un rÃ´le dans la construction dâ€™un
classifieur (Belkin et Niyogi, 2004; Belkin et al., 2006). Nous montrons17 que la propagation
des Ã©tiquettes le long des arcs dâ€™un GGG en tenant compte de la densitÃ© de ces arcs (propaga-
tion dâ€™autant plus forte que la densitÃ© est forte) est aussi efficace que les autres approches de
lâ€™Ã©tat de lâ€™art gÃ©nÃ©ralement basÃ©es sur le graphe desK plus proches voisins, mais ne nÃ©cessite
aucun rÃ©glage arbitraire de mÃ©ta-paramÃ¨tres (K par exemple).
Nous envisageons dâ€™Ã©tudier lâ€™impact des caractÃ©ristiques topologiques additionnelles sur
un classifieur en reconnaissance de formes. Nous poursuivons lâ€™Ã©tude du lien entre critÃ¨re
BIC et topologie du modÃ¨le. Ces rÃ©sultats seront aussi Ã  comparer Ã  ceux obtenus en GÃ©omÃ©-
trie Algorithmique avec la mesure de "persistance topologique" (Edelsbrunner et al., 2000) et
lâ€™inscription de cette mesure dans un cadre statistique (Bubenik et Kim, 2007).
Concernant la famille de graphes dans laquelle la solution est recherchÃ©e, nous avons consi-
dÃ©rÃ© les graphes de Delaunay. Cependant la famille la plus riche est reprÃ©sentÃ©e par le graphe
complet. Peut-on Ã©viter le choix a priori arbitraire de Delaunay, en partant du graphe complet
pour obtenir un graphe reprÃ©sentant la topologie suivant les principes dÃ©crits ici ? Ou bien peut-
on justifier thÃ©oriquement le choix de Delaunay comme bon candidat pour le graphe initial ?
Comment pourrait se dÃ©cliner ce modÃ¨le dans un cadre parcimonieux basÃ© sur les donnÃ©es,
comme celui des Machines Ã  Vecteurs Supports (SVM), lâ€™Ã©mergence des arcs dÃ©finissant le
TRG dans le modÃ¨le prÃ©sentÃ© ici faisant penser Ã  lâ€™Ã©mergence des vecteurs supports qui seuls
suffisent Ã  dÃ©finir la frontiÃ¨re de dÃ©cision dans les SVM.
17Soumission en cours
M. Aupetit, P. Gaillard et G. Govaert
Dâ€™un point de vue plus gÃ©nÃ©ral, ce travail se veut une contribution au rapprochement des
domaines de lâ€™Apprentissage Statistique et de la Topologie Algorithmique, Ã  la frontiÃ¨re des-
quels nous pensons quâ€™il ouvre de nombreuses perspectives.
RÃ©fÃ©rences
Agrell, E. (1993). A method for examining vector quantizer structures. Proceedings of IEEE
International Symposium on Information Theory, 394â€“394.
Ahalt, A., A. Krishnamurthy, et D. M. P. Chen (1990). Competitive learning algorithms for
vector quantization. Neural Networks 3.
Alahakoon, D., S. Halgamuge, et B. Srinivasan (1998). A structure adapting feature map for
optimal cluster representation. In S. Usui et T. Omori (Eds.), ICONIP, pp. 809â€“812. IOA
Press.
Aupetit, M. (2003). Robust topology representing networks. In Proceedings of the European
Symposium on Artificial Neural Networks, Bruges (Belgium), pp. 45â€“50. d-side.
Aupetit, M. (2006). Learning topology with the generative gaussian graph and the em al-
gorithm. In Y. Weiss, B. SchÃ¶lkopf, et J. Platt (Eds.), Advances in Neural Information
Processing Systems 18, pp. 83â€“90. Cambridge, MA : MIT Press.
Aupetit, M. (2007). Visualizing distortions and recovering topology in continuous projection
techniques. Neurocomputing, Elsevier 70, 1304â€“1330.
Aupetit, M. et T. Catz (2005). High-dimensional labeled data analysis with topology represen-
ting graphs. Neurocomputing, Elsevier 63, 139â€“169.
Aupetit, M., F. Chazal, G. Gasso, D. Cohen-Steiner, et P. Gaillard (2007). Topology learning :
New challenges at the crossing of machine learning, computational geometry and topology.
Barber, C., D. Dobkin, et H. Huhdanpaa (1996). The quickhull algorithm for convex hulls.
ACM Transactions on Mathematical Software 22, 469â€“483.
Belkin, M. et P. Niyogi (2004). Semi-supervised learning on riemannian manifolds. Journal
of Machine Learning Special Issue on Clustering 56, 209â€“239.
Belkin, M., P. Niyogi, et V. Sindhwani (2006). Manifold regularization : A geometric frame-
work for learning from examples. Journal of Machine Learning Research 7, 2399â€“2434.
Bishop, C. (1995). Neural Networks for Pattern Recognition. New York : Oxford Univ. Press.
Bishop, C. (2006). Pattern Recognition and Machine Learning. Springer.
Bishop, C., M. SvensÃ©n, et C. Williams (1998). Gtm : the generative topographic mapping.
Neural Computation, MIT Press 10(1), 215â€“234.
Boyles, R. (1983). On the convergence of the EM algorithm. Journal of the Royal Statistical
Society, Series B 45, 47â€“50.
Bubenik, P. et P. Kim (2007). A statistical approach to persistent homology. Homology, homo-
topy and Applications 9(2), 337â€“362.
Carlsson, G., A. Zomorodian, A. Collins, et L. Guibas (2004). Persistence barcodes for shapes.
In SGP â€™04 : Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geo-
metry processing, New York, NY, USA, pp. 124â€“135. ACM Press.
Apprentissage Automatique de la Topologie
Carreira-PerpiÃ±Ã¡n, M. A. et R. S. Zemel (2005). Proximity graphs for clustering and manifold
learning. In L. K. Saul, Y. Weiss, et L. Bottou (Eds.), Advances in Neural Information
Processing Systems 17, pp. 225â€“232. Cambridge, MA : MIT Press.
Celeux, G. et G. Govaert (1992). A classification em algorithm for clustering and two stochas-
tic versions. Computational Statistics and Data Analysis 14(3), 315â€“332.
Chang, K. et J. Ghosh (2001). A unified model for probabilistic principal surfaces. IEEE
Transactions on Pattern Analysis and Machine Intelligence 23, 22 â€“ 41.
Chazal, F., D. Cohen-Steiner, et A. Lieutier (2007). A sampling theory for compact sets in
euclidean spaces. Discrete and Computational Geometry.
de Silva, V. (2003). Plex : Simplicial complexes in matlab.
de Silva, V. et G. Carlsson (2004). Topological estimation using witness complexes. In
M. Alexa et S. Rusinkiewicz (Eds.), Eurographics Symposium on Point-Based Graphics,
pp. 157â€“166.
de Silva, V. et J. B. Tenenbaum (2003). Global versus local methods in nonlinear dimensiona-
lity reduction. In S. T. S. Becker et K. Obermayer (Eds.), Advances in Neural Information
Processing Systems 15, pp. 705â€“712. Cambridge, MA : MIT Press.
Dempster, A., N. Laird, et D. Rubin (1977). Maximum likelihood from incomplete data via
the em algorithm. Journal of the Royal Statistical Society, Series B 39(1), 1â€“38.
Edelsbrunner, H., D. Letscher, et A. Zomorodian (2000). Topological persistence and simpli-
fication. IEEE Symp. on Found. of Comp. Sci., 454â€“463.
Edelsbrunner, H. et N. Shah (1997). Triangulating topological spaces. International Journal
on Computational Geometry and Applications 7, 365â€“378.
Fraley, C. et A. Raftery (2002). Model-based clustering, discriminant analysis, and density
estimation. Journal of the American Statistical Association 97, 611â€“631.
Fritzke, B. (1992). Growing cell structures-a self-organizing network in k dimensions. In
I. Aleksander et J. Taylor (Eds.), Artificial Neural Networks, Volume 2, Amsterdam, Nether-
lands, pp. 1051â€“1056. North-Holland.
Fritzke, B. (1995). A growing neural gas network learns topologies. In G. Tesauro, D. Tou-
retzky, et T. Leen (Eds.), Advances in Neural Information Processing Systems 7, Cambridge,
MA. MIT Press.
Gaillard, P. (2008). Apprentissage de la connexitÃ© dÅ un nuage de points par modÃ¨le gÃ©nÃ©ra-
tif. applications Ã  lÅ analyse exploratoire de donnÃ©es et Ã  la classification semi-supervisÃ©e.
UniversitÃ© de Technologie de CompiÃ¨gne - Commissariat Ã  lâ€™Energie Atomique.
Gaillard, P., M. Aupetit, et G. Govaert (2008). Learning topology of a labeled data set with the
supervised generative gaussian graph. Neurocomputing 71(7-9), 1283â€“1299.
Geusebroek, J., G. Burghouts, et A. Smeulders (2005). The Amsterdam library of object
images. International Journal of Computer Vision 61(1), 103â€“112.
Hastie, T. et W. Stuetzle (1989). Principal curves. Journal of the American Statistical Asso-
ciation 84, 502â€“516.
Kohonen, T. (2001). Self-Organizing Maps. Berlin, Heidelberg, New York : Springer Series in
Information Sciences.
M. Aupetit, P. Gaillard et G. Govaert
Lee, J., A. Lendasse, et M. Verleysen (2002). Curvilinear distance analysis versus isomap. In
Proceedings of the European Symposium on Artificial Neural Networks, Bruges (Belgium),
pp. 185â€“192. d-side.
Martinetz, T., S. Berkovitch, et K. Schulten (1993). Neural-gas network for vector quantization
and its application to time-series prediction. IEEE Transactions on Neural Networks 4(4),
558â€“569.
Martinetz, T. et K. Schulten (1994). Topology representing networks. Neural Networks, Else-
vier London 7, 507â€“522.
McLachlan, G. et D. Peel (2000). Finite Mixture Models. New York : John Wiley & Sons.
Munkres, J. (1993). Elements of Algebraic Topology. Westview Press.
Niyogi, P., S. Smale, et S. Weinberger (2006). Finding the homology of submanifolds with
high confidence from random samples. Discrete and Computational Geometry.
Queen, J. M. (1967). Some methods of classification and analysis of multivariate observations.
In Proceedings of the Fifth Berkeley Symposium on Mathemtical Statistics and Probability,
pp. 281â€“297.
Roeder, K. et L. Wasserman (1997). Practical Bayesian density estimation using mixtures of
normals. Journal of the American Statistical Association 92(439), 894â€“902.
Schwartz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6, 461â€“
464.
Tibshirani, R. (1992). Principal curves revisited. Statistics and Computing 2, 183â€“190.
Tipping, M. et C. Bishop (1999). Probabilistic principal component analysis. Journal of the
Royal Statistical Society Series B 21(3), 611â€“622.
Veltkamp, R. (1991). The gamma-neighborhood graph. Computational Geometry 1, 227â€“246.
Weinberger, K. et L. Saul (2006). Unsupervised learning of image manifolds by semidefinite
programming. International Journal of Computer Vision 70(1), 77â€“90.
Zeller, M., R. Sharma, et K. Schulten (1996). Topology representing network for sensor-based
robot motion planning. World Congress on Neural Networks, INNS Press, 100â€“103.
Zhu, X. et J. Lafferty (2005). Harmonic mixtures : combining mixture models and graph-based
methods for inductive and scalable semi-supervised learning. In ICML â€™05 : Proceedings of
the 22nd International Conference on Machine learning, New York, USA, pp. 1052â€“1059.
ACM.
Summary
A point set is more than a set of points. Some hidden topological structure may govern
the point distribution, and from a data mining perspective, catching this structure is at least as
important as estimating the sole spatial point density. In this work, we propose a generative
model based on the Delaunay graph of a set of prototypes representative of the point set,
assuming a Gaussian noise. We determine the Expectation-Maximization equations and we
use the Bayesian Information Criterion to select the best model. Moreover, it does not need
any hand-tuning of the meta-parameters. Empirical experiments on toys and real image data
Apprentissage Automatique de la Topologie
show that the connectedness of the proposed graph accounts correctly for that of the point
set. This model provides a principled way to cluster a point set regarding its connectedness.
We also show how it could be used as a pre-processing step in classification of point sets, to
complete the point sets with their topological invariants coded as graph structures. At last, this
work is an attempt to lay foundation stones towards the construction of a topological model of
a point set, grounded on statistical generative models.
