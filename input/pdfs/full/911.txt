M√©lange de distributions comme fonction d‚Äôimportance dans
l‚Äô√©chantillonnage pr√©f√©rentiel combin√© avec l‚Äôalgorithme de
Monte Carlo par Cha√Æne de Markov
Dorota Gajda‚àó, Chantal Guihenneuc-Jouyaux‚àó‚àó
Judith Rousseau‚àó‚àó‚àó
‚àóBiostatistiques, CESP Centre de recherche en Epid√©miologie et Sant√© des Populations,
U1018, Inserm, F-94807, Villejuif, France
Universit√© Paris Sud, UMRS1018, Villejuif, F-94807, France
dorota.gajda@inserm.fr,
‚àó‚àó EA 4064 (√©pid√©miologie environnementale : impact sanitaire des pollutions),
Facult√© des Sciences Pharmaceutiques et Biologiques, Universit√© Paris Descartes,
4, av de l‚ÄôObservatoire, 75006 Paris, France
‚àó‚àó‚àóUniversit√© Paris Dauphine, Paris, France
R√©sum√©. Les algorithmes de Monte Carlo par Cha√Æne de Markov (MCMC) sont
tr√®s souvent utilis√©s pour estimer les lois a posteriori ainsi que leurs moments
dans le cadre d‚Äôun mod√®le Bay√©sien. En effet, selon les mod√®les, les lois a pos-
teriori ou leurs moments peuvent ne pas avoir d‚Äôexpression analytique et le re-
cours a des m√©thodes d‚Äôapproximation est donc indispensable. Lors de l‚Äô√©tude
empirique d‚Äôestimateurs, diff√©rents jeux donn√©s sont simul√©s sous le m√™me mo-
d√®le (et avec les m√™mes valeurs de param√®tres) et pour chaque jeu de donn√©es,
les estimations a posteriori des param√®tres sont obtenus via MCMC. Cette proc√©-
dure est r√©it√©r√©e pour d‚Äôautres valeurs des param√®tres. Globalement, les temps de
calcul peuvent √™tre tr√®s importants. L‚Äô√©chantillonnage pr√©f√©rentiel (Importance
Sampling en anglais, IS) combin√© avec les algorithmes MCMC est une solution
permettant de r√©duire ce temps de calcul. En effet, l‚ÄôIS n√©cessite le choix d‚Äôune
fonction d‚Äôimportance que nous proposons construite comme m√©lange de lois
a posteriori pr√©s√©lectionn√©es sur quelques jeux donn√©es simul√©s, lois a poste-
riori d√©j√† estim√©es via MCMC. Les autres calculs ne n√©cessitent plus le recours
aux algorithmes MCMC. Les approches √©voqu√©es ici sont illustr√©es sur deux
exemples de mod√®les de Poisson.
1 Introduction
L‚Äô√©chantillonnage pr√©f√©rentiel (Importance Sampling en anglais, IS) est pr√©sent√© ici comme
une m√©thode d‚Äôoptimisation algorithmique dans le cas de l‚Äô√©tude empirique d‚Äôun estimateur. En
effet, m√™me s‚Äôil est possible d‚Äôavoir des propri√©t√©s asymptotiques des estimateurs, les √©tudes
empiriques sont n√©cessaires afin d‚Äô√©valuer leurs comportements dans un cadre non asymptotique.
La d√©marche consiste alors √† d√©finir ces situations caract√©ristiques (taille d‚Äô√©chantillon, valeurs
M√©lange de distributions et √©chantillonnage pr√©f√©rentiel combin√© avec MCMC
des param√®tres) et pour chacune d‚Äôentre elles, √† simuler plusieurs jeux de donn√©es sur lesquels,
les param√®tres sont estim√©s. Cette d√©marche permet de caract√©riser les performances des
estimateurs selon diff√©rentes situations en contr√¥lant les fluctuations al√©atoires. Notre travail
se place dans le cadre d‚Äôune mod√©lisation param√©trique bay√©sienne o√π de mani√®re g√©n√©rique,
les donn√©es sont not√©es X et les param√®tres Œ∏. Dans le contexte Bay√©sien, des lois a priori
pi(Œ∏) sont sp√©cifi√©es sur les param√®tres et sont suppos√©es les m√™mes dans toutes les situations
√©tudi√©es. La d√©marche bay√©sienne, comme abord√©e dans la vaste litt√©rature (Cf. par exemple
Robert (2007)), consiste √† combiner l‚Äôinformation a priori des param√®tres repr√©sent√©e par les
lois a priori avec l‚Äôinformation provenant des donn√©es √† travers la vraisemblance pi(X|Œ∏) pour
obtenir la loi a posteriori des param√®tres conditionnelle aux donn√©es, pi(Œ∏|X). Cette loi, d‚Äôapr√®s
le Th√©or√®me de Bayes, s‚Äô√©crit comme
pi(Œ∏|X) = pi(X|Œ∏)pi(Œ∏)
pi(X)
(1)
o√π pi(X) est la loi marginale de X . Quand la loi a posteriori ou quand les moments de cette
loi ne sont pas explicites, une approximation est obtenue par les algorithmes stochastiques dits
de Monte Carlo par Cha√Ænes de Markov (MCMC) comme pr√©sent√©s par Hastings (1970) ou
par Geman et Geman (1984). Ces algorithmes it√©ratifs permettent d‚Äôobtenir des r√©alisations
Markoviennes sous la loi a posteriori recherch√©e et, via la th√©orie ergodique, d‚Äôobtenir ainsi
des estimations de ses moments. Dans le cas d‚Äô√©tudes empiriques, l‚Äôalgorithme it√©ratif MCMC
doit √™tre utilis√© pour chaque jeu de donn√©es simul√©. L‚Äôutilisation r√©p√©t√©e de ces algorithmes
peut √™tre tr√®s co√ªteuse en temps de calcul. Pour r√©duire ce temps de calcul, nous proposons
une alternative consistant en l‚Äôutilisation combin√©e d‚Äô√©chantillonnage pr√©f√©rentiel et de MCMC.
L‚Äôid√©e d‚Äôutilisation simultan√©e de l‚ÄôIS a √©t√© d√©j√† propos√©e entre autre par Geyer et Thompson
(1992) pour l‚Äô√©valuation de vraisemblances, par Gelfand et al. (1992) pour des crit√®res de
validation crois√©e mais pas dans le contexte d‚Äô√©tudes empiriques d‚Äôestimateurs. La partie 2
pr√©sente le principe de l‚Äô√©chantillonnage pr√©f√©rentiel combin√© avec MCMC dans ce contexte
ainsi que les propri√©t√©s des estimateurs obtenus. Le choix de la fonction d‚Äôimportance est
√©galement discut√©. Lors d‚Äôune premi√®re √©tude (Gajda et al. (2010)), nous avons propos√© certains
choix pour cette fonction amenant √† soit des r√©ductions importantes de temps de calcul mais
avec parfois de mauvaises estimations, soit de tr√®s bonnes estimations mais avec une r√©duction
moins nette du temps de calcul. Afin d‚Äôam√©liorer cette proc√©dure, nous proposons ici une
fonction d‚Äôimportance comme m√©lange de distributions. Enfin, dans la partie 3, ces approches
sont illustr√©es dans le cadre de mod√®les poissonniens.
2 M√©thodes
2.1 G√©n√©ralit√©s
Soit le mod√®le param√©trique MŒ∏ o√π Œ∏ est le vecteur des param√®tres. Pour une situation
particuli√®re en Œ∏ = Œ∏0,K jeux de donn√©es, (X(1), . . . , X(K)) sont simul√©s sous le mod√®leMŒ∏0 .
Pour un jeu de donn√©es i ainsi obtenu, l‚Äôalgorithme MCMC permet d‚Äôobtenir la r√©alisation
d‚Äôune cha√Æne de Markov Œ∏1, ¬∑ ¬∑ ¬∑ , Œ∏N admettant comme loi stationnaire la loi a posteriori
pi(Œ∏|X(i)). Par le th√©or√®me ergodique, la moyenne empirique 1N
‚àëN
j=1 g(Œ∏j) converge
presque s√ªrement vers EŒ∏|X(i)(g(Œ∏)), esp√©rance de g(Œ∏) par rapport √† la loi a posteriori, et
D. Gajda et al.
ceci pour toute fonction int√©grable g. Le choix de la fonction g permet ainsi d‚Äôobtenir des
estimations des diff√©rents moments de la distribution a posteriori. Malgr√© son efficacit√©
incontestable dans les probl√®mes o√π les vraies lois ou les vraies valeurs des esp√©rances
a posteriori ne sont pas accessibles, l‚Äôutilisation des algorithmes MCMC dans le cadre
bay√©sien d‚Äôune √©tude de simulations peut devenir extr√™mement longue, car doit √™tre r√©p√©t√©e
autant de fois que de nombre de jeux de donn√©es simul√©s. Pour K = 100 jeux de donn√©es,
la d√©marche classique pour estimer (Epi(Œ∏|Xi)(g(Œ∏)), i = 1, ¬∑ ¬∑ ¬∑ , 100) est illustr√©e sur la figure 1.
X(1)
X(2)
X(100)
-MCMC
-MCMC
-MCMC
Œ∏
(1)
1 ...Œ∏
(1)
N ‚àº pi(Œ∏|X(1))
Œ∏
(2)
1 ...Œ∏
(2)
N ‚àº pi(Œ∏|X(2))
Œ∏
(100)
1 ...Œ∏
(100)
N ‚àº pi(Œ∏|X(100))
-Th.erg
-Th.erg
-Th.erg
1
N
‚àëN
i=1 g(Œ∏
(1)
i )
1
N
‚àëN
i=1 g(Œ∏
(2)
i )
1
N
‚àëN
i=1 g(Œ∏
(100)
i )
...
...
...
FIG. 1 ‚Äì D√©marche classique via MCMC
Le principe de base de l‚Äô√©chantillonnage pr√©f√©rentiel est d‚Äôestimer une esp√©rance selon une
densit√© f gr√¢ce √† des r√©alisations obtenues sous une autre densit√© h. Par exemple, si on d√©sire
estimer Ef (g(Œ∏)) =
‚à´
g(Œ∏)f(Œ∏)dŒ∏, il est simple de montrer que Ef (g(Œ∏)) =
‚à´ g(Œ∏)f(Œ∏)
h(Œ∏) h(Œ∏)dŒ∏
pourvu que le support de f soit inclus dans le support de h. Cette simple remarque permet
de fournir deux estimateurs : - l‚Äôestimateur classique, 1N
‚àëN
j=1 g(Œ∏j) o√π (Œ∏j)j=1,...,N ‚àº f - et
l‚Äôestimateur IS, 1N
‚àëN
j=1
g(Œ∏j)f(Œ∏j)
h(Œ∏j)
o√π (Œ∏j)j=1,...,N ‚àº h. La fonction h est appel√©e fonction
d‚Äôimportance et en g√©n√©ral, est choisie comme √©tant simple, rapide √† simuler et respectant la
condition sur les supports.
Dans le cas particulier de l‚Äô√©tude empirique d‚Äôestimateurs, le choix de la fonction d‚Äôimpor-
tance est guid√© par d‚Äôautres consid√©rations. Le principe de cette m√©thode peut √™tre pr√©sent√© pour
deux jeux de donn√©esX(m) etX(k). Nous supposons disposer d√©j√† des r√©sultats de l‚Äôalgorithme
MCMC relatifs au premier jeu de donn√©es X(m) et donc en particulier, des r√©alisations Mar-
koviennes de Œ∏ sous la loi stationnaire pi(Œ∏|X(m)) obtenues par MCMC et des approximations
ergodiques correspondantes de Epi(Œ∏|X(m))g(Œ∏). On d√©sire estimer Epi(Œ∏|X(k))(g(Œ∏)) mais cette
fois-ci sans utiliser d‚Äôalgorithmes MCMC. D‚Äôapr√®s le r√©sultat suivant :
Epi(Œ∏|X(k))(g(Œ∏)) = Epi(Œ∏|X(m))(g(Œ∏)
pi(Œ∏|X(k))
pi(Œ∏|X(m)) ) (2)
M√©lange de distributions et √©chantillonnage pr√©f√©rentiel combin√© avec MCMC
Une r√©alisation Markovienne {Œ∏1, . . . , Œ∏N} sous la loi stationnaire pi(Œ∏|X(m)) permet alors
d‚Äôestimer Epi(Œ∏|X(k))(g(Œ∏)) par la moyenne empirique
1
N
‚àëN
i=1
g(Œ∏i)pi(Œ∏i|X(k))
pi(Œ∏i|X(m)) ou bien par sa
version "normalis√©e" (self-normalised Importance Sampling estimator) :‚àëN
i=1 g(Œ∏i)
pi(X(k)|Œ∏i)
pi(X(m)|Œ∏i)‚àëN
i=1
pi(X(k)|Œ∏i)
pi(X(m)|Œ∏i)
‚Üí Epi(Œ∏|X(k))[g(Œ∏)] p.s.. (3)
Cette version normalis√©e fait intervenir uniquement les vraisemblances et non plus les lois a
posteriori √©vitant ainsi le calcul des lois marginales. En effet il est possible de d√©montrer que :
1
N
N‚àë
i=1
pi(X(k)|Œ∏i)
pi(X(m)|Œ∏i) ‚Üí
pi(X(k))
pi(X(m))
p.s.. (4)
La loi a posteriori pi(Œ∏|X(m)) est ici la fonction d‚Äôimportance. D‚Äôapr√®s la loi forte des
grands nombres, les estimateurs ainsi obtenus par √©chantillonnage pr√©ferentiel dans leur version
normalis√©e (formule 3) sont des estimateurs consistants de Epi(Œ∏|X(k))(g(Œ∏)). Dans le cas
ind√©pendant (et non avec d√©pendance Markovienne), Geweke (1989) montre sous certaines
conditions que les estimateurs IS suivent asymptotiquement une loi normale, r√©sultat √©tendu au
cas markovien par Doss (1994) dans la discussion de l‚Äôarticle de Tierney (1994). Les d√©tails des
propri√©t√©s asymptotiques des estimateurs se trouvent dans l‚Äôarticle Gajda et al. (2010).
2.2 Choix de la fonction d‚Äôimportance
La m√©thode d‚Äô√©chantillonnage pr√©f√©rentiel n√©cessite le choix d‚Äôune fonction d‚Äôimportance,
choix souvent d√©licat √† faire. En effet, dans notre contexte, la fonction d‚Äôimportance est une loi
a posteriori et pour obtenir une bonne estimation, le support de la fonction d‚Äôimportance doit
couvrir le support de la fonction d‚Äôint√©r√™t. Plus pr√©cisemment, sauf dans le cas particulier de
mod√®les o√π un param√®tre d√©finirait le support, le probl√®me est plut√¥t de choisir une fonction
d‚Äôimportance qui ne tende pas vers 0 plus rapidement que la loi a posteriori d‚Äôorigine. Ceci
n‚Äôest pas √©vident √† savoir quand les densit√©s a posteriori n‚Äôont pas d‚Äôexpressions explicites.
Lors d‚Äôune pr√©c√©dente √©tude (Gajda et al. (2010)), nous avons propos√© deux strat√©gies de choix
de la fonction d‚Äôimportance appel√©e ici loi a posteriori de " r√©f√©rence " pour le calcul de l‚ÄôIS. La
premi√®re strat√©gie (appel√©e par la suite strat√©gie 1) consiste √† choisir la fonction d‚Äôimportance
par simple tirage au hasard (√©quiprobable) d‚Äôune seule loi a posteriori. Cette loi a posteriori
(appel√©e "r√©f√©rence fixe") est ensuite utilis√©e comme unique fonction d‚Äôimportance pour toutes
les estimations. Cette premi√®re strat√©gie est illustr√©e sur la figure 2 pour une s√©rie deK = 100
r√©plications de jeux de donn√©es avec pi(Œ∏|X(1)) comme r√©f√©rence fixe.
L‚Äôid√©e d‚Äôutiliser la m√™me distribution a posteriori comme fonction d‚Äôimportance pour
l‚Äôensemble de toutes les estimations peut para√Ætre trop restrictive. Choisir une fonction
d‚Äôimportance parmi un sous-ensemble de distributions pour chaque estimation est une
alternative s√©duisante. La premi√®re √©tape de la deuxi√®me strat√©gie (strat√©gie 2) consiste
en la pr√©s√©lection d‚Äôun petit nombre de jeux de donn√©es (au hasard ou par une proc√©dure
automatique). La seconde √©tape consiste √† choisir pour chaque nouvelle estimation (et donc
chaque nouveau jeu de donn√©es) une loi a posteriori de r√©f√©rence (appel√©e "r√©f√©rence choisie")
D. Gajda et al.
X(1)
X(2)
X(100)
-MCMC
-IS
-IS
Œ∏
(1)
1 ...Œ∏
(1)
N
‚àºpi(Œ∏|X(1))
 
 
 	










‚àëN
i=1 g(Œ∏
(1)
i )
pi(x(2)|Œ∏(1)
i
)
pi(x(1)|Œ∏(1)
i
)‚àëN
i=1
pi(x(2)|Œ∏(1)
i
)
pi(x(1)|Œ∏(1)
i
)
‚àëN
i=1 g(Œ∏
(1)
i )
pi(x(100)|Œ∏(1)
i
)
pi(x(1)|Œ∏(1)
i
)‚àëN
i=1
pi(x(100)|Œ∏(1)
i
)
pi(x(1)|Œ∏(1)
i
)
-Th.erg 1N
‚àëN
i=1 g(Œ∏
(1)
i )
...
...
...
.
.
.
FIG. 2 ‚Äì D√©marche via IS avec r√©f√©rence fixe
parmi les distributions pr√©s√©lectionn√©es. Pour un total de 100 jeux de donn√©es et 10 jeux de
donn√©es pr√©s√©lectionn√©s, cette d√©marche est pr√©sent√©e sur la figure 3. Cette deuxi√®me strat√©gie
n√©cessite donc de savoir choisir une loi a posteriori de r√©f√©rence (parmi les distributions
pr√©s√©lectionn√©es) "adapt√©e" au jeu de donn√©es sur lequel les estimations sont faites. Pour
cela, plusieurs crit√®res de choix ont √©t√© propos√©s : Le premier est fond√© sur la minimisation
de la norme L1 de la diff√©rence entre deux lois a posteriori, le deuxi√®me sur la minimisation
de la divergence de Kullback-Leibler et le troisi√®me sur la minimisation de la variance de
l‚Äôestimation MCMC.
L‚Äôavantage de la premi√®re strat√©gie est la rapidit√© car une seule fonction d‚Äôimportance est
utilis√©e pour toutes les estimations mais parfois au prix d‚Äôune estimation moins performante
alors que la deuxi√®me strat√©gie am√®ne √† de meilleures estimations mais moins rapidement.
Dans ce travail, nous proposons une troisi√®me strat√©gie (strat√©gie 3) o√π la fonction d‚Äôim-
portance est une densit√© de m√©lange des lois pr√©s√©lectionn√©es. Ainsi, cette d√©marche offre
l‚Äôavantage √† nouveau d‚Äô√™tre fond√©e sur une seule fonction d‚Äôimportance pour l‚Äôensemble des
estimations avec, de plus, un support plus large que la fonction d‚Äôimportance de la premi√®re
strat√©gie. Si J est le nombre de lois a posteriori pr√©s√©lectionn√©s (dans notre exemple, J=10),
alors la fonction d‚Äôimportance pimix est de la forme
pimix(Œ∏) =
J‚àë
j=1
nj
|n|pi(Œ∏|X
(j)) =
J‚àë
j=1
nj
|n|cjhj(Œ∏)
o√π pi(Œ∏|X(j)) = cjhj(Œ∏) (version normalis√©e ou non de la ji√®me densit√© a posteriori), nj
est le nombre de r√©alisations Markoviennes obtenues via MCMC sous la loi pi(Œ∏|X(j)) et
|n| = n1 + ¬∑ ¬∑ ¬∑+ nJ .
M√©lange de distributions et √©chantillonnage pr√©f√©rentiel combin√© avec MCMC
X(1)
X(10)
X(11)
X(100)
-MCMC
-MCMC
-IS
-IS
Œ∏
(1)
1 ...Œ∏
(1)
N
‚àºpi(Œ∏|X(1))
Œ∏
(10)
1 ...Œ∏
(10)
N
‚àºpi(Œ∏|X(10))


















‚àëN
i=1 g(Œ∏
(m11)
i )
pi(x(11)|Œ∏(m11)
i
)
pi(x(m11)|Œ∏(m11)
i
)‚àëN
i=1
pi(x(11)|Œ∏(m11)
i
)
pi(x(m11)|Œ∏(m11)
i
)
‚àëN
i=1 g(Œ∏
(m100)
i )
pi(x(100)|Œ∏(m100)
i
)
pi(x(m100)|Œ∏(m100)
i
)‚àëN
i=1
pi(x(100)|Œ∏(m100)
i
)
pi(x(m100)|Œ∏(m100)
i
)
m11‚àà{1,...,10}
m100‚àà{1,...,10}
-Th.erg
-Th.erg
1
N
‚àëN
i=1 g(Œ∏
(1)
i )
1
N
‚àëN
i=1 g(Œ∏
(10)
i )
...
...
...
...
...
.
.
.
FIG. 3 ‚Äì D√©marche via IS avec r√©f√©rence choisie
Si (Œ∏(j)1 , ¬∑ ¬∑ ¬∑ , Œ∏(j)nj ) est l‚Äôensemble des nj r√©alisations sous la loi pi(Œ∏|X(j)) alors, pour k > J , :
1
|n|
J‚àë
j=1
nj‚àë
i=1
g(Œ∏
(j)
i )
pi(Œ∏
(j)
i |X(k))
pimix(Œ∏
(j)
i )
‚Üí Epi(Œ∏|X(k))(g(Œ∏))
Les coefficients du m√©lange d√©pendent de constantes de normalisation (c1, ¬∑ ¬∑ ¬∑ cJ ) qui ne
sont pas explicites. Nous proposons de les estimer par deux m√©thodes d√©crites dans la partie 2.3.
Les constantes de normalisations estim√©es sont not√©es (cÀÜ1, ¬∑ ¬∑ ¬∑ cÀÜJ ). L‚Äôestimation de
Epi(Œ∏|X(k))(g(Œ∏)) par √©chantillonnage pr√©f√©rentiel est :
J‚àë
j=1
nj‚àë
i=1
w(Œ∏
(j)
i )g(Œ∏
(j)
i )
o√π
w(Œ∏
(j)
i ) =
pi(X(k)|Œ∏(j)i )/pimix(Œ∏(j)i )‚àëJ
j=1
‚àënj
i=1 pi(X
(k)|Œ∏(j)i )/pimix(Œ∏(j)i )
et
pimix(Œ∏) =
J‚àë
j=1
nj
|n| cÀÜjhj(Œ∏).
D. Gajda et al.
Pour √©valuer et comparer les qualit√©s des estimateurs obtenus par les trois strat√©gies, nous
avons calcul√© les racines carr√©es des erreurs quadratiques moyennes, D et DÀú. D peut √™tre
interpr√©t√©e comme une distance entre les estimations et les vraies valeurs a posteriori et DÀú
comme une distance entre les estimations via IS et les estimations obtenues par l‚Äôapproche
classique MCMC. En effet, si les expressions des esp√©rances sous la loi a posteriori ne sont
pas explicites alors la distance D n‚Äôest pas calculable et sera donc remplac√©e par DÀú. Plus
pr√©cisemment, pour lesK‚àíJ jeux de donn√©es (en dehors des jeux de donn√©es pr√©s√©lectionn√©s),
les expressions de D et de DÀú sont les suivantes :
D =
[
1
K ‚àí J
K‚àë
k=J+1
(ISk ‚àí Epi(Œ∏|X(k))(g(Œ∏)))2
]1/2
(5)
et
DÀú =
[
1
K ‚àí J
K‚àë
k=J+1
(ISk ‚àíMCMCk)2
]1/2
(6)
o√π ISk est l‚Äôestimation de Epi(Œ∏|X(k))(g(Œ∏)) obtenue par √©chantillonnage pr√©f√©rentiel et
MCMCk celle obtenue par l‚Äôapproche "classique" via MCMC. Le crit√®re de comparaison
choisi est l‚Äôerreur quadratique moyenne comme lors de l‚Äô√©tude pr√©c√©dente (Gajda et al. (2010)).
D‚Äôautres crit√®res pourraient √™tre utilis√©s comme par exemple le risque bay√©sien int√©gr√© (Robert
(2007)) dont l‚Äôadmissibilit√© et l‚Äôoptimalit√© pourraient √™tre ensuite jug√©es par rapport √† la borne
de Cram√©r-Rao bay√©sienne (Gill et Levit (1995)).
Un premier exemple pr√©sente les r√©sultats dans le cas d‚Äôun mod√®le o√π toutes les expressions
a posteriori sont explicites. Les estimations seront donc compar√©es aux vraies valeurs en utilisant
D. Dans un deuxi√®me exemple, les calculs analytiques n‚Äô√©tant plus possibles, DÀú sera √©valu√©e.
2.3 Constantes de normalisation
Nous proposons deux m√©thodes pour estimer les coefficients du m√©lange (c1, ¬∑ ¬∑ ¬∑ cJ ) inter-
venant dans pimix.
La premi√®re m√©thode dite de "Reverse Logistic Regression" a √©t√© propos√©e par Geyer (1993).
En utilisant la reparam√©trisation suivante :
Œ∑j = log cj + log
nj
|n|
alors la probabilit√© que Œ∏ appartienne √† la j√®me composante du m√©lange est :
pj(Œ∏, Œ∑) =
hj(Œ∏)e
Œ∑j‚àëJ
i=1 hi(Œ∏)e
Œ∑i
o√π Œ∑ = (Œ∑1, ¬∑ ¬∑ ¬∑ , Œ∑J). Geyer (1993) propose d‚Äôestimer les coefficients Œ∑j et donc les coefficients
cj par maximisation de la log-vraisemblance l(Œ∑) =
‚àëJ
j=1
‚àënj
i=1 log pj(Œ∏
(j)
i , Œ∑). Les d√©tails
des proc√©dures it√©ratives de maximisation se trouvent dans Geyer (1993).
M√©lange de distributions et √©chantillonnage pr√©f√©rentiel combin√© avec MCMC
La seconde m√©thode utilise √† nouveau la th√©orie ergodique et l‚Äô√©chantillonnage pr√©f√©rentiel.
En effet, d‚Äôapr√®s l‚Äô√©galit√© pi(Œ∏|X(j)) = cjhj(Œ∏) = cjpi(X(j)|Œ∏)pi(Œ∏) alors
cj =
pi(Œ∏|X(j))
pi(X(j)|Œ∏)pi(Œ∏) .
Si f est une densit√© de probabilit√© dont le support est inclus dans le support de pi(Œ∏|X(j)) alors
cj =
‚à´
f(Œ∏)
pi(X(j)|Œ∏)pi(Œ∏)pi(Œ∏|X
(j))dŒ∏.
Ainsi, d‚Äôapr√®s la th√©orie ergodique, si {Œ∏(j)1 , . . . , Œ∏(j)nj } sont des r√©alisations Markoviennes
sous la loi stationnaire pi(Œ∏|X(j)) alors
cÀÜj =
1
nj
nj‚àë
i=1
f(Œ∏
(j)
i )
pi(X(j)|Œ∏(j)i )pi(Œ∏(j)i )
‚Üí cj p.s.
Pour ce calcul, f(Œ∏(j)i ) a √©t√© choisie comme la densit√© d‚Äôune loi normale de moyenne
¬µÀÜj=
‚àënj
i=1
Œ∏
(j)
i
nj
(estimation de l‚Äôesp√©rance a posteriori de Œ∏ sous pi(Œ∏|X(j))) et de variance
œÉÀÜ2j=
‚àënj
i=1
Œ∏
(j)2
i
nj
‚àí (¬µÀÜj)2 (estimation de la variance a posteriori de Œ∏ sous pi(Œ∏|X(j))).
3 Exemples
Dans cette partie, deux exemples sont pr√©sent√©s dans le cadre poissonnien. Le premier
exemple est un mod√®le simple o√π les lois a posteriori sont explicites et le second exemple est un
mod√®le lin√©aire g√©n√©ralis√© offrant l‚Äôavantage d‚Äô√™tre plus r√©aliste mais o√π cette fois-ci les lois
a posteriori ne sont plus explicites. Les approches pr√©sent√©es dans ce papier reposent sur la
convergence des algorithmes MCMC. Il est donc essentiel de la v√©rifier. Comme cela est sugg√©r√©
dans les articles de Brooks et Roberts (1998) et Mengersen et al. (1999), plusieurs diagnostics de
convergence doivent √™tre utilis√©s (et non un seul). Dans notre √©tude, la convergence a √©t√© √©tudi√©e
en v√©rifiant que les erreurs de Monte Carlo sont inf√©rieures √† 5% des √©carts types a posteriori des
param√®tres, et en utilisant les diagnostics de Gelman et Rubin ainsi que des outils graphiques
(traces des param√®tres avec bonne m√©langeance, autocorrelations des param√®tres). L‚Äôensemble
de ces outils est accessible sous R dans la librairie BRuGS (R Development Core Team (2008)).
Concernant les r√©sultats pr√©sent√©s ici, 50000 it√©rations avec un "temps de chauffe" (Burn-in) de
5000 it√©rations ont √©t√© utilis√©es dans les algorithmes MCMC.
3.1 Mod√®le de Poisson
Le premier exemple d‚Äôapplication des m√©thodes pr√©sent√©es ci-dessus est un mod√®le simple
de Poisson avec le param√®tre de moyenne Œª. L‚Äôavantage de cet exemple est qu‚Äôen choisissant
comme loi a priori la loi conjug√©e Gamma, la loi a posteriori de Œª est explicite. Ainsi, la
comparaison avec les vraies valeurs a posteriori est √©galement possible. En effet, pour un jeu de
donn√©es X(k) = (X(k)1 , . . . , X
(k)
n ) o√π les X
(k)
i sont ind√©pendantes et de m√™me loi de Poisson
D. Gajda et al.
P(Œª) et pour la loi a priori Œª ‚àº G(Œ±, Œ≤), la densit√© a posteriori de Œª sachant X(k) est aussi une
loi Gamma G(‚àëni=1X(k)i + Œ±, Œ≤ + n). La vraie moyenne et la vraie variance a posteriori sont
√©gales √† (
‚àën
i=1X
(k)
i + Œ±)/(Œ≤ + n) et (
‚àën
i=1X
(k)
i + Œ±)/(Œ≤ + n)
2 respectivement.
Nous avons simul√© un total de 100 jeux de donn√©es de taille n = 20 selon la loi de Poisson
avec la param√®tre Œª = 20. Ensuite, pour chaque simulation, la moyenne et la variance de la
loi a posteriori du param√®tre ont √©t√© estim√©es de deux mani√®res diff√©rentes : classiquement via
MCMC et via Importance Sampling combin√© avec MCMC comme pr√©sent√© pr√©c√©demment
selon les trois strat√©gies.
La premi√®re strat√©gie consiste √† utiliser une seule loi a posteriori de r√©f√©rence choisie
au hasard pour l‚Äôensemble des autres jeux de donn√©es. Afin d‚Äôenvisager toutes les possibili-
t√©s, nous avons √©tudi√© les r√©sultats pour toutes les lois a posteriori possibles de r√©f√©rence :
pi(Œª|X(1)), . . . , pi(Œª|X(100)). Les r√©sultats pr√©sent√©s dans le tableau 1 sur les lignes intitul√©es
"la pire r√©f. fixe" et "la meilleur r√©f. fixe" correspondent respectivement aux r√©sultats associ√©s au
choix des lois a posteriori de r√©f√©rence qui donnaient les plus grandes valeurs et les plus petites
valeurs du crit√®re D par rapport aux vraies valeurs a posteriori. Pour la deuxi√®me strat√©gie, les
dix lois a posteriori pr√©sectionn√©es correspondent simplement aux dix premi√®res.
Ces dix premi√®res lois a posteriori pr√©selectionn√©es ont √©t√© aussi utilis√©es pour la construction
de la loi m√©lange comme fonction d‚Äôimportance selon la troisi√®me strat√©gie.
Le tableau 1 pr√©sente les valeurs de D (voir formule 5) par rapport aux vraies valeurs des
moyennes a posteriori selon les diff√©rentes stat√©gies.
Stat√©gie D
mcmc 9.5
1 la pire r√©f. fixe 346.8
1 la meilleure r√©f. fixe 33.9
2 crit√®re 1 6.5
2 crit√®re 2 11.2
2 crit√®re 3 6.7
3 m√©lange 6.3
TAB. 1 ‚Äì Valeurs deD (multipli√©es par 103), racine carr√©e des erreurs quadratiques moyennes
de la moyenne a posteriori dans le mod√®le de Poisson avec Œª = 20.
L‚Äôordre de grandeur des erreurs quadratiques reste tr√®s petit sauf dans le cas de la premi√®re
strat√©gie, en particulier dans le cas de la mauvaise r√©f√©rence fixe. Les deux crit√®res et la technique
du m√©lange ont donn√© des erreurs quadratiques nettement inf√©rieures √† celles de la r√©f√©rence fixe
(premi√®re strat√©gie). De plus, les deuxi√®me et troisi√®me strat√©gies pr√©sentent l‚Äôavantage d‚Äô√©viter
une fonction d‚Äôimportance choisie par hasard parmi les 100. L‚Äôapproche classique par MCMC
√©tant elle-m√™me une approximation, il est int√©ressant de voir ici que les r√©sultats obtenus avec IS
sont l√©g√®rement meilleurs qu‚Äôavec l‚Äôapproche classique. Concernant les variances a posteriori
(r√©sultats non montr√©s), les conclusions sont √† peu pr√®s identiques.
M√©lange de distributions et √©chantillonnage pr√©f√©rentiel combin√© avec MCMC
3.2 Mod√®le lin√©aire g√©n√©ralis√©
Le second exemple concerne un exemple de mod√®le lin√©aire g√©n√©ralis√©. Afin d‚Äô√©tudier les
avantages des approches combin√©es avec l‚Äô√©chantillonnage pr√©f√©rentiel en terme de pr√©cisions
d‚Äôestimation et de temps de calcul, cet exemple concerne une grande taille d‚Äô√©chantillon
(n=1000) avec 10 covariables. Le mod√®le est le suivant :
Xi|Œª ‚àº P(Œªi)
log(Œªi) = a+
‚àë10
j=1 bjZij
Des lois a priori vagues N (0, 105) ont √©t√© choisies pour les coefficients a et
{bj , j = 1, . . . , 10}. Pour les simulations des 100 jeux de donn√©es, les valeurs choisies sont :
a = 0, bj = 0.05 pour j = 1, . . . , 10. Les valeurs de DÀú (voir formule 6) ont √©t√© calcul√©es
dans ce cas car les expressions analytiques des lois a posteriori et de leurs moments ne sont
plus explicites. Concernant les r√©sultats des estimations des coefficients {bj , j = 1, . . . , 10},
la moyenne des 10 valeurs de DÀú a √©t√© √©valu√©e pour chaque strat√©gie. Pour la strat√©gie 2, les
trois crit√®res ont donn√© des r√©sultats tr√®s proches (0.8 √ó 10‚àí2, 1.6 √ó 10‚àí2 and 1.1 √ó 10‚àí2
pour les crit√®res 1, 2 and 3 respectivement). Concernant la strat√©gie 1, la moyenne de DÀú est
de 3.8 √ó 10‚àí2 en choisissant comme r√©f√©rence fixe la premi√®re distribution a posteriori. La
moyenne de DÀú pour la strat√©gie 3 est 0.6 √ó 10‚àí2. Ces r√©sultats confirment les r√©sultats du
premier exemple simple √† savoir de meilleures pr√©cisions pour les strat√©gies 2 et 3 que pour la
strat√©gie 1 avec ici une l√©g√®re meilleure performance de la strat√©gie 3.
Dans ce deuxi√®me exemple, les performances en terme de temps de calcul ont √©t√© √©valu√©es.
La table 2 montre les rapports des temps entre les diff√©rentes strat√©gies et le temps de calcul de
l‚Äôapproche "classique" MCMC. Tous ces calculs ont √©t√© faits sur le m√™me ordinateur et avec le
m√™me nombre d‚Äôit√©rations. La strat√©gie 1 qui ne n√©cessite qu‚Äôune fonction d‚Äôimportance pour
l‚Äôensemble des estimations r√©duit nettement le temps de calcul par rapport √† l‚Äôapproche MCMC
car demande environ que 6% du temps de l‚Äôapproche MCMC classique. Par contre, les gains en
temps pour la strat√©gie 2 sont beaucoup moins nets voir sup√©rieurs pour le crit√®re 3. Rappelons
que pour ce crit√®re, une fonction d‚Äôimportance diff√©rente est choisie pour chaque nouveau jeu
de donn√©es mais √©galement pour chaque fonction g intervenant dans l‚Äôesp√©rance. Le gain de la
strat√©gie 3 reste moins bon que celui de la strat√©gie 1 mais nettement meilleur que celui de la
strat√©gie 2. En effet, la strat√©gie 3 demande moins de la moiti√© du temps (42%) que l‚Äôapproche
classique MCMC pour des qualit√©s d‚Äôestimation proches de celles obtenues par la strat√©gie 2.
TAB. 2 ‚Äì Rapport des temps de calculs entre les m√©thodes combin√©es avec l‚Äô√©chantillonnage
pr√©f√©rentiel et la m√©thode classique MCMC concernant la r√©gression de Poisson avec n =
1, 000 et 10 covariables.
Strat√©gie 1 Strat√©gie 2 Strat√©gie 2 Strat√©gie 2 Strat√©gie 3
crit√®re 1 (L1) crit√®re 2 (KL) crit√®re 3 M√©lange
0.057 0.957 0.771 1.156 0.423
D. Gajda et al.
4 Conclusions
Les estimations par √©chantillonnage pr√©f√©rentiel sont, en g√©n√©ral, proches des estimations
obtenues classiquement via MCMC en √©tant √©galement proches des vraies valeurs a posteriori.
Concernant la comparaison des diff√©rentes strat√©gies propos√©es avec IS, les erreurs quadratiques
calcul√©es avec les lois a posteriori de r√©f√©rence choisies par les crit√®res (strat√©gie 2) ou avec
la loi m√©lange (strat√©gie 3), ont toujours √©t√© plus petites que les erreurs quadratiques obtenues
avec la strat√©gie 1 "r√©f√©rence fixe", en particulier quand la fonction d‚Äôimportance correspondait
au pire choix comme r√©f√©rence fixe.
Les exemples pr√©sent√©s ci-dessus montrent que dans le cadre des estimations r√©p√©t√©es, le
choix de la fonction d‚Äôimportance n‚Äôest pas √©vident √† faire m√™me pour un mod√®le simple. Parmi
toutes les r√©f√©rences fixes possibles, il existe des cas qui peuvent amener √† de tr√®s mauvais
r√©sultats. Il est impossible d‚Äôidentifier la loi de r√©f√©rence fixe qui aboutirait √† des r√©sultats
convenables. Il est donc utile de mettre en place d‚Äôautres strat√©gies. L‚Äôid√©e d‚Äôune loi de m√©lange
comme forme de loi de r√©f√©rence unique a donn√© des erreurs quadratiques qui se sont r√©v√©l√©es
√™tre toujours plus petites que celles avec la strat√©gie "r√©f√©rence fixe" et de tr√®s bons r√©sultats en
terme de gain de temps de calculs. Ainsi, cette approche offre le double avantage d‚Äôavoir une
seule fonction d‚Äôimportance pour l‚Äôensemble des estimations (donc d‚Äô√™tre rapide) et de donner
de bons r√©sultats en terme de qualit√©s d‚Äôestimations.
Concernant les algorithmes MCMC, le nombre d‚Äôit√©rations utilis√© dans les estimations est
bien souvent inf√©rieur au nombre d‚Äôit√©rations r√©ellement effectu√© et ceci pour plusieurs raisons.
En premier lieu, une partie des it√©rations initiales (dite "p√©riode de chauffe") est √©cart√©e afin
de garantir que la cha√Æne de Markov soit sous le r√©gime stationnaire. En second lieu, souvent
seul un sous-√©chantillonnage des it√©rations est conserv√©, par exemple une it√©ration sur dix ou
vingt. M√™me s‚Äôil est toujours avantageux en terme de pr√©cision d‚Äôestimateur de garder toutes les
it√©rations (MacEachern et Berliner (1994), Geyer (1992)), ce sous-√©chantillonnage peut √™tre
int√©ressant pour des raisons d‚Äôam√©lioration de la m√©langeance de la cha√Æne ou bien simplement
de stockage. Ainsi, une proportion parfois non n√©glileable des it√©rations est rejet√©e. Enfin,
lors de chaque it√©ration, des simulations sont n√©cessaires. Par exemple, l‚Äôalgorithme de Gibbs
requiert des simulations sous la loi conditionnelle compl√®te de chaque param√®tre. Si ces lois
conditionnelles ne sont pas connues, une √©tape de Hasting Metropolis est souvent introduite
n√©cessitant une simulation sous une loi dite instrumentale. Ainsi, il est souvent plus co√ªteux
en temps d‚Äôobtenir les r√©alisations de la loi a posteriori via un algorithme MCMC que de les
manipuler ensuite comme dans le calcul de l‚Äô√©chantillonnage pr√©f√©rentiel, et ceci m√™me quand
la proportion des it√©rations rejet√©es est faible. Typiquement, la troisi√®me strat√©gie montre un
gain en temps de calcul alors qu‚Äôelle requiert un nombre d‚Äô√©valuations des vraisemblances
plus important que par une proc√©dure MCMC classique. En effet, ces √©valuations doivent √™tre
faites sur l‚Äôensemble des r√©alisations markoviennes obtenues par les J proc√©dures MCMC.
Cette troisi√®me strat√©gie est donc particuli√®rement adapt√©e quand la vraisemblance est explicite
et non approch√©e num√©riquement (par √† nouveau une proc√©dure it√©rative). En effet, si dans
le d√©roulement de l‚Äôalgorithme MCMC, l‚Äô√©valuation de la vraisemblance (ou d‚Äôune de ses
composantes) est non n√©gligeable par rapport √† l‚Äôutilisation de g√©n√©rateurs de nombre pseudo
al√©atoire, elle le sera aussi dans le calcul d‚Äô√©chantillonnage pr√©f√©rentiel. La comparaison exacte
de la complexit√© num√©rique entre MCMC et les strat√©gies combin√©es utilisant l‚Äô√©chantillonnage
pr√©f√©rentiel est difficile √† faire de mani√®re g√©n√©rale car d√©pend du mod√®le √©tudi√© et du type
d‚Äôalgorithme MCMC utilis√©, ceci reste n√©anmoins une piste de recherche √† approfondir.
M√©lange de distributions et √©chantillonnage pr√©f√©rentiel combin√© avec MCMC
Afin de poursuivre ce travail, il serait int√©ressant d‚Äô√©tudier les performances de ces diff√©rentes
approches dans le cadre des mod√®les lin√©aires g√©n√©ralis√©s mixtes qui demandent souvent des
temps de calculs importants.
R√©f√©rences
Brooks, S. P. et G. O. Roberts (1998). Convergence assessment techniques for Markov chain
Monte Carlo. Statistics and Computing 8(4), 319‚Äì335.
Doss, H. (1994). Discussion of the paper "Markov chains for exploring posterior distributions"
by luke tierney. Ann. Statist. 22(4), 1728‚Äì1734.
Gajda, D., C. Guihenneuc-Jouyaux, J. Rousseau, K. Mengersen, et D. Nur (2010). Use in
practice of importance sampling for repeated MCMC for Poisson models. Electron. J.
Statist. 4, 361‚Äì383.
Gelfand, A., D. Dey, et H. Chang (1992). Model determination using predictive distributions
with implementation via sampling-based methods. In B. J. D. A. Bernardo, J.M. et A. Smith
(Eds.), Bayesian Statistics, Volume 4, pp. 147‚Äì167. Oxford University Press.
Geman, S. et D. Geman (1984). Stochastic Relaxation, Gibbs Distributions, and the Bayesian
Restoration of Images. IEEE Transactions on Pattern Analysis and Machine Intelligence 6,
721‚Äì740.
Geweke, J. (1989). Bayesian inference in econometric models using Monte Carlo integration.
Econometrica 57(6), 1317‚Äì1339.
Geyer, C. et E. Thompson (1992). Constrained Monte Carlo Maximum Likelihood for De-
pendent Data (with discussion). Journal of the Royal Statistical Society. Series B (Methodo-
logical) 54(3), 657‚Äì699.
Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Stat. Sci. 7(4), 473‚Äì511.
Geyer, C. J. (1993). Estimating Normalizing Constants and Reweighting Mixtures in Markov
Chain Monte Carlo (revision). Technical Report No. 568 R(4), School of Statistics, University
of Minnesota, http ://www.stat.umn.edu/PAPERS/tr568r.html.
Gill, R. et B. Levit (1995). Application of the van Trees inequality : a Bayesian Cram√©r-Rao
bound. Bernoulli 1, 59‚Äì79.
Hastings, W. K. (1970). Monte Carlo Sampling Methods Using Markov Chains and Their
Applications. Biometrika 57(1), 97‚Äì109.
MacEachern, S. N. et M. L. Berliner (1994). Subsampling the Gibbs Sampler. Am. Stat. 48(3),
188‚Äì190.
Mengersen, K. L., C. P. Robert, et C. Guihenneuc-Jouyaux (1999). MCMC convergence
diagnostics : a reviewww. In Bayesian statistics, 6 (Alcoceber, 1998), New York, pp. 415‚Äì440.
Oxford Univ. Press.
R Development Core Team (2008). R : A Language and Environment for Statistical Computing.
Vienna, Austria : R Foundation for Statistical Computing. ISBN 3-900051-07-0.
Robert, C. (2007). The Bayesian Choice. From Decision-Theoretic Foundations to Computatio-
nal Implementation (2 ed.). Springer Texts in Statistics. Springer.
D. Gajda et al.
Tierney, L. (1994). Markov chains for exploring posterior distributions. Ann. Statist. 22(4),
1701‚Äì1762. With discussion and a rejoinder by the author.
Summary
The Importance Sampling method is used in combination with MCMC in Bayesian simula-
tion study. In the particular context of numerous simulated data sets, MCMC algorithms have
to be called several times which may become computationally expensive. We propose to use
MCMC on a preselected set of the simulated data in order to obtain Markovian realisations
of each corresponding posterior distribution. The estimates for the other simulated data are
computed via IS based on this preselected data set. Since Importance Sampling requires the
choice of an importance function, we propose a strategy for this choice based on a mixture
of the preselected posterior distributions. The featured methods are illustrated in simulations
studies under two different Poisson models.
