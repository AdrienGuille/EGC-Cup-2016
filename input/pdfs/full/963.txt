Nouvelle m√©thode de classification adapt√©e aux donn√©es de
grande dimension : application aux donn√©es de biopuces
Doulaye Demb√©l√©
IGBMC, CNRS-IMSERM-ULP, 1 rue Laurent Fries, BP 10142
Parc d‚ÄôInnovation, 67404 Illkirch Cedex, France
Doulaye.Dembele@igbmc.u-strasbg.fr,
http://www-microarrays.u-strasbg.fr
R√©sum√©. Nous proposons une nouvelle m√©thode de classification adapt√©e aux
donn√©es de grande dimension. Pour ces donn√©es la distance de Chebyshev semble
int√©ressante, car elle n√©cessite moins de temps de calcul compar√©e √† la distance
Euclidienne, plus utilis√©e en raison de ses bonnes propri√©t√©s g√©om√©triques. La
m√©thode propos√©e combine les m√©thodes de regroupement hi√©rarchique et par
partition pour obtenir le nombre de classes dans les donn√©es. Des donn√©es is-
sues d‚Äôexp√©riences de biopuces sont utilis√©es pour illustrer les performances de
la m√©thode propos√©e.
1 Introduction
La classification permet de repr√©senter des donn√©es sous une forme plus ais√©e √† interpr√©ter,
√† visualiser ou √† manipuler. Nous proposons une nouvelle m√©thode de classification adapt√©e
aux donn√©es de grande dimension. Pour ces donn√©es le probl√®me d‚Äôespace vide est connu
(Dohono, 2000). D‚Äôautres faiblesses sont li√©es √† l‚Äôutilisation de la distance Euclidienne : sous
certaines hypoth√®ses sur la distribution des √©chantillons, les distances entre toutes les paires de
points des donn√©es sont identiques quand la dimension augmente (Beyer et al., 1998). Dans ces
conditions il est impossible de discriminer les classes, s‚Äôil y en a, dans les donn√©es. Il est aussi
montr√© dans (Herault et al., 2002) qu‚Äôen augmentant l‚Äôordre de la m√©trique de Minkowski, il
est possible d‚Äôaugmenter le rang de la matrice des distances des donn√©es prises deux √† deux. Le
maximum de dimension pour la matrice des distances sera obtenu pour un ordre infini, c‚Äôest-
√†-dire en utilisant la distance de Chebyshev. Notons que le rang de la matrice des distances
d√©finit le degr√© de contraste permettant d‚Äôobtenir des classes dans les donn√©es, c‚Äôest-√†-dire le
degr√© de redondance dans les donn√©es. La distance de Chebyshev semble alors int√©ressante. De
plus, elle n√©cessite moins de temps de calcul compar√© √† la distance Euclidienne g√©n√©ralement
utilis√©e en raison de ses bonnes propri√©t√©s g√©om√©triques. Le gain en temps de calcul n‚Äôest pas
n√©gligeable pour les donn√©es de grande dimension comme celles g√©n√©r√©es par les biologistes
dans le cadre de l‚Äô√©tude de l‚Äôexpression des g√®nes √† l‚Äôaide de la technologie des biopuces.
Nous nous int√©ressons ici aux m√©thodes de classification heuristiques automatiques et non
supervis√©es √©galement appel√©es clustering. Ces m√©thodes se divisent en deux grandes familles :
les m√©thodes hi√©rarchiques et les m√©thodes par partition. Les m√©thodes hi√©rarchiques ne n√©-
cessitent pas la connaissance a priori du nombre de classes dans les donn√©es. Leur r√©sultat
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
est repr√©sent√© sous forme d‚Äôun arbre ou dendrogramme dans lequel les branches contiennent
les √©chantillons similaires du point de vue du crit√®re utilis√© pour les construire (Jain et Dubes,
1988; Everitt, 1993; Jain et al., 2000). Cette m√©thode est int√©ressante mais elle ne permet pas de
r√©examiner un √©chantillon d√©j√† plac√© dans une branche. Les m√©thodes par partition consistent
√† trouver le meilleur regroupement des N √©chantillons des donn√©es en K classes de mani√®re
√† optimiser un crit√®re de qualit√© d√©fini a priori. Pour r√©soudre ce probl√®me combinatoire, on
utilise habituellement une heuristique pour obtenir un r√©sultat en un temps raisonnable. Dans
cette heuristique, les √©chantillons sont initialement repartis enK classes puis it√©rativement, on
recherche la meilleure combinaison locale qui am√©liore la qualit√© du crit√®re pr√©d√©fini en chan-
geant la classe de certains √©chantillons. Cette √©tape prend fin quand il n‚Äôy a plus d‚Äôam√©lioration
possible du crit√®re (Jain et Dubes, 1988; Everitt, 1993; Jain et al., 2000). Le principal incon-
v√©nient des m√©thodes par partition est la n√©cessit√© de conna√Ætre a priori le nombre de classes
√† former. Dans cet article, nous proposons une nouvelle strat√©gie qui combine les m√©thodes
hi√©rarchiques et les m√©thodes par partition. Dans un premier temps la matrice des distances
des donn√©es est calcul√©e et utilis√©e pour former un nombre maximum de classes (√©tape par
partition sans connaissance a priori du nombre des classes), puis un regroupement est effectu√©
pour r√©duire le nombre des classes (√©tape hi√©rarchique ascendante avec crit√®re d‚Äôarr√™t). L‚Äôid√©e
de combiner les m√©thodes de classification hi√©rarchique et par partition n‚Äôest pas nouvelle
(Wong, 1982). Toutefois la proc√©dure pr√©sent√©e dans cet article est originale.
Dans la suite la nouvelle m√©thode de classification est d√©crite puis des r√©sultats obtenus
avec des donn√©es de biopuces sont ensuite pr√©sent√©s.
2 Donn√©es de grande dimension
On assiste de nos jours √† la g√©n√©ration de grandes masses de donn√©es. Ceci est particuli√®re-
ment vrai dans le domaine de la biologie mol√©culaire o√π les techniques de biopuces permettent
par exemple d‚Äô√©tudier simultan√©ment la transcription de plusieurs milliers (e.g. 50 000) de
g√®nes. Si une telle exp√©rience est r√©p√©t√©e pour des conditions (e.g. 100) diff√©rentes, on se
retrouve avec un tableau contenant des millions de valeurs ou composantes. En plus de l‚Äôas-
pect stockage, il y a un besoin d‚Äôoutils efficaces pour la gestion de ces donn√©es et l‚Äôaccession
√† l‚Äôinformation qu‚Äôelles contiennent. C‚Äôest pour cette raison que les outils de classification
et de fouille de donn√©es sont utiles. Ces m√©thodes permettent de rechercher des √©chantillons
similaires √† une r√©f√©rence ou de regrouper un ensemble d‚Äô√©chantillons en des sous-ensembles
coh√©rents du point de vue d‚Äôun crit√®re pr√©d√©fini. Le mot √©chantillon est utilis√© ici pour d√©signer
un g√®ne dans une exp√©rience de biopuces.
La plupart des m√©thodes de classification sont bas√©es sur la distance Euclidienne. Quand
la dimension n des donn√©es augmente, le ph√©nom√®ne d‚Äôespace vide appara√Æt (Dohono, 2000).
Ceci se caract√©rise par des propri√©t√©s qui sont r√©sum√©s dans (Jimenez et Landgrebe, 1995; He-
rault et al., 2002). Une solution √† ce probl√®me consiste √† rechercher les classes dans un espace
de dimension faible compar√© √† l‚Äôespace initial. La r√©duction de la dimension des donn√©es et
la recherche des classes peuvent √™tre faites simultan√©ment ou dans des √©tapes s√©par√©es. Les
donn√©es de dimension r√©duite sont obtenues apr√®s projection dans un espace de dimension
plus faible que l‚Äôespace original. Un outil tr√®s utilis√© pour r√©duire la dimension des donn√©es
est l‚ÄôAnalyse en Composantes Principales (ACP) (Golub et Loan, 1996; Holter et al., 2000;
Bonnet et al., 2002; Horn et Axel, 2003). Dans certaines m√©thodes, une projection non lin√©aire
D. Demb√©l√©
(curviligne) est utilis√©e simultan√©ment avec la recherche des classes (Demartines, 1994; He-
rault et al., 2002). Bien que la r√©duction de la dimension soit particuli√®rement utile pour la
visualisation, il est montr√© dans (Yeung et Ruzzo, 2001) que les premi√®res composantes d‚Äôune
analyse ACP ne capturent pas n√©cessairement l‚Äôinformation sur les groupes pr√©sents dans les
donn√©es. Il est alors souvent utile de rechercher les groupes dans l‚Äôespace initial.
2.1 Normes et distances
Il est montr√© dans (Demartines, 1994) que pour des √©chantillons de composantes ind√©pen-
dantes et identiquement distribu√©es, leur d√©viation standard tend √† devenir constante quand la
dimension n augmente. Il est aussi montr√© dans (Beyer et al., 1998) pour ces m√™mes donn√©es
que si l‚Äô√©cart type des distances entre toutes les paires de points tend vers z√©ro, ces distances
ont tendance √† √™tre identiques quand la dimension n augmente. Ces deux r√©sultats combin√©s
expliquent en partie la d√©t√©rioration des performances des algorithmes de classification pour
les donn√©es de grande dimension. Notons toutefois que dans la pratique, l‚Äôhypoth√®se d‚Äôind√©-
pendance des √©chantillons n‚Äôest pas toujours satisfaite.
Soient xi et xj deux √©chantillons des donn√©es ayant chacun n valeurs. La distance Eucli-
dienne est obtenue en prenant p = 2 dans la distance de Minkowski d√©finie par :
d(xi,xj) =
[
n‚àë
k=1
(xik ‚àí xjk)p
] 1
p
(1)
La norme Lp ou de Minkowski de l‚Äô√©chantillon xi est d√©finie par :
Lp(xi) =
[
n‚àë
k=1
(xik)
p
] 1
p
(2)
SoitMp la matrice form√©e avec les distances de tous les √©chantillons et d√©finie par :
Mp = [dp(xi,xj)], i, j = 1, . . . , N (3)
o√π N est le nombre total des √©chantillons dans les donn√©es.
La matrice Mp est carr√©e d‚Äôordre N et la borne sup√©rieure de son rang est donn√©e par
r = n(p‚àí1)+2 (Herault et al., 2002). Plus r sera √©lev√©, plus grand sera le nombre de colonnes
ind√©pendantes dans la matriceMp, i.e. moins il y a de redondance dans lesN √©chantillons xi.
La relation donnant r montre que le plus grand rang sera obtenu pour p infini, c‚Äôest-√†-dire en
utilisant la distance de Chebyshev d√©finie par :
d(xi,xj) = max
k
|xik ‚àí xjk| (4)
En comparaison avec la distance Euclidienne, la distance de Chebyshev n√©cessite moins de
charge de calcul. Ceci peut s‚Äôav√©rer utile quand la dimension des donn√©es est √©lev√©e. La faible
charge de calcul combin√©e avec la possibilit√© d‚Äôavoir un maximum de contraste dans les don-
n√©es nous ont conduit √† utiliser cette distance dans la nouvelle m√©thode que nous proposons
dans le paragraphe suivant. Pour les donn√©es de biopuces exprim√©es en rapport d‚Äôexpression
ou ratios, au lieu d‚Äôutiliser la norme L2 comme c‚Äôest le cas habituellement, nous avons opt√©
pour une autre normalisation qui consiste √† transformer les valeurs de fa√ßon √† ce qu‚Äôelles soient
toutes comprises entre 0 et 1.
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
3 Nouvelle m√©thode de classification
La m√©thode heuristique de clustering par partition la plus utilis√©e est la m√©thode K-Means.
Soient K le nombre des classes √† trouver, ck le centre de la classe k (k = 1, . . . ,K) et xi
l‚Äô√©chantillon i (i = 1, . . . , N ) des donn√©es. La m√©thode K-Means permet d‚Äôobtenir la r√©parti-
tion des donn√©es apr√®s la minimisation de la fonction suivante :
J(ck) =
K‚àë
k=1
N‚àë
i=1
uikd(xi, ck) (5)
o√π d(xi, ck) d√©signe la distance entre l‚Äô√©chantillon xi et le centre ck de la classe k alors que
uik vaut 1 si l‚Äô√©chantillon xi appartient √† la classe k et 0 sinon.
La minimisation de la fonction (5) peut √™tre obtenue avec l‚Äôalgorithme suivant (Jain et al.,
2000) :
1. S√©lectionnerK √©chantillons pour repr√©senter les centres des groupes √† former,
2. Repartir les √©chantillons entre les groupes suivant leur proximit√© par rapport aux
centres des groupes,
3. Calculer les centres ck des groupes (moyennes des √©chantillons des groupes),
4. R√©p√©ter les √©tapes 2 et 3 jusqu‚Äô√† convergence.
Dans la relation (5), il y a deux param√®tres √† choisir avant le d√©but des calculs, la distance
d(., .) et le nombre K des classes. La distance Euclidienne est la plus utilis√©e du fait de ses
bonnes propri√©t√©s g√©om√©triques. Elle d√©finit toutefois implicitement une forme sph√©rique pour
les classes √† trouver. Pour obtenir des classes de forme ellipso√Ødale, la distance de Mahalanobis
est utilis√©e. Pour les donn√©es de grande dimension la distance de Chebyshev qui est √©quivalente
√† la m√©trique de Minkowski √† l‚Äôordre infini offre une matrice de distances de rang maximum
(Herault et al., 2002). Un rang √©lev√© pour la matrice des distances indiquera l‚Äôabsence de forte
redondance dans les √©chantillons et par cons√©quent la possibilit√© d‚Äôobtenir un grand nombre
de classes. C‚Äôest pour cette raison que notre choix s‚Äôest port√© sur la distance de Chebyshev.
La distance de Chebyshev est utilis√©e avec succ√®s dans l‚Äôalgorithme K-Means (Estlick et al.,
2001).
√âtant donn√© qu‚Äôil est souvent difficile de conna√Ætre a priori le nombre des classes, nous
proposons de le d√©terminer directement √† partir des donn√©es. L‚Äôid√©e est de former dans la
fonction (5) des classes telles que les √©chantillons membres d‚Äôune classe soient plus proches
entre eux que par rapport √† ceux des autres classes. Cette proximit√© peut √™tre d√©finie par un seuil
sur les distances (Lukashin et Fuchs, 2001). Soit dseuil cette distance seuil, toutes les distances
des √©chantillons d‚Äôune classe doivent √™tre au plus √©gales √† dseuil . Le probl√®me consiste alors √†
d√©terminer ce seuil √† partir des distances des donn√©es. La recherche du seuil est examin√©e plus
loin. √Ä partir d‚Äôun seuil appropri√© sur les distances, les donn√©es sont r√©parties pour obtenir une
valeur maximale pourK , puis un regroupement de certaines classes est effectu√©.
3.1 Algorithme
La proc√©dure de la nouvelle m√©thode de classification se r√©sume comme suit :
D. Demb√©l√©
1. Calculer toutes les distances entre les √©chantillons des donn√©es,
2. Rechercher un seuil pour les distances des √©chantillons d‚Äôune classe,
3. Repartir les donn√©es en utilisant le seuil trouv√©,
4. D√©terminer le nombre maximum Kmax de classes sans prendre en compte les
classes singletons,
5. Calculer lesKmax centres des classes et les utiliser pour avoir une partition initiale
des donn√©es,
6. Utiliser une m√©thode de regroupement et un crit√®re de validation pour obtenir K
classes.
Soit D le nombre total des distances de toutes les paires d‚Äô√©chantillons. La premi√®re
√©tape consiste √† calculer les D = N(N‚àí1)2 distances. La m√©diane de ces distances (voir plus
loin), permet d‚Äôobtenir le seuil recherch√© (√©tape 2). Ce seuil est utilis√© dans la troisi√®me √©tape
conjointement avec les distances pour affecter un index √† chaque √©chantillon. Cela est fait en
comparant le premier √©chantillon aux autres, puis le second √©chantillon non index√© est compar√©
aux autres, et ceci est poursuivi jusqu‚Äô√† l‚Äôavant dernier √©chantillon. Le m√™me index est associ√©
aux √©chantillons de distances inf√©rieures ou √©gales au seuil. La figure 1 illustre les d√©tails de
l‚Äô√©tape 3. L‚Äôexamen des diff√©rents index permet enfin d‚ÄôobtenirKmax (√©tape 4). La cinqui√®me
√©tape consiste √† calculer les centres des classes retenues puis √† r√©partir les √©chantillons entre
celles-ci. Dans la derni√®re √©tape, une m√©thode hi√©rarchique ascendante peut √™tre utilis√©e. Il est
√©galement possible d‚Äôutiliser l‚Äôalgorithme K-Means dans lequel on fait varier le nombre des
classes, k = 2, . . . ,Kmax, puis on retient la valeur de k pour laquelle un crit√®re de validation
est optimal (Milligan et Cooper, 1985).
3.2 Conditionnement des donn√©es
Avant d‚Äôutiliser un algorithme de classification, les donn√©es sont souvent standardis√©es.
Cela consiste en g√©n√©ral √† transformer les donn√©es pour avoir une moyenne nulle et un √©cart
type √©gal √† un pour chaque √©chantillon (normalisation L2). La standardisation est particuli√®-
rement utile pour les donn√©es d‚Äôexpression absolues de biopuces, celles-ci peuvent avoir des
amplitudes tr√®s diff√©rentes alors que l‚Äôon est int√©ress√© par la variation des profils. La norma-
lisation L2 rend toutefois les donn√©es sph√©riques. Une autre transformation peut consister √†
ramener toutes les valeurs des donn√©es dans un intervalle compris entre 0 et 1. Cela est obtenu
en otant la valeur minimale des donn√©es de chaque composante et en divisant le r√©sultat par
l‚Äô√©tendue, c‚Äôest-√†-dire, la diff√©rence entre les valeurs maximale et minimale. Un inconv√©nient
de cette transformation est sa sensibilit√© aux √©chantillons aberrants, c‚Äôest-√†-dire ceux qui ont
des valeurs √©loign√©es compar√©es √† la majorit√©. Les √©chantillons aberrants (ayant des valeurs
tr√®s faibles ou tr√®s √©lev√©es) conduiront √† l‚Äôobtention d‚Äôune √©tendue grande et par cons√©quent
√† un tassement de la majorit√© des valeurs des donn√©es. Ceci doit √™tre pris en compte par un
pr√©traitement o√π les √©chantillons ayant des valeurs aberrantes sont corrig√©s ou √©cart√©s de l‚Äôana-
lyse. Un exemple de pr√©traitement pour des donn√©es de biopuces consiste √† √©liminer toutes les
valeurs inf√©rieures √† un seuil pour tous les √©chantillon (Lukashin et Fuchs, 2001).
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
oui
Fin
non
oui
j <= N
k = k+1
id = 0
i <= N
?
?
id = 0
?
non
non
oui
cIdx[j] = k
id = 1
d(xi,xj) <= dseuil
& cIdx[j] = 0
?
non
oui
j = i+1, ..., N
j = j+1
i = 1, ..., N
i = i+1
id = 0;  k = 1;  cIdx[1] = k
cIdx[i]=0 pour i=1,...,N
D√©but
FIG. 1 ‚Äì R√©partition des donn√©es en utilisant le seuil dseuil (√©tape 3). Initialement l‚Äôindex
du groupe de chaque √©chantillon est mis √† z√©ro sauf le premier qui est suppos√© appartenir au
groupe 1 (k=1). √âtant donn√© un √©chantillon xi appartenant au groupe k, nous pla√ßons dans le
m√™me groupe tous les √©chantillons xj non encore class√©s et pour lesquels d(xj ,xi) ‚â§ dseuil .
Un indicateur (id) permet de contr√¥ler l‚Äôincr√©mentation du nombre de groupes.
3.3 D√©termination du seuil des distances
Le nombre maximal de classesKmax dans les donn√©es d√©pend de la valeur du seuil dseuil
utilis√©. Si cette valeur est √©lev√©e, Kmax sera faible et inappropri√©, inversement si la valeur de
dseuil est faible,Kmax sera √©lev√© et conduira √† la g√©n√©ration de beaucoup de classes singletons.
Nous avons fait des tests sur des donn√©es synth√©tiques et r√©elles pour pouvoir choisir la valeur
de dseuil .
Consid√©rons des donn√©es contenant deux groupes dans l‚Äôespace de dimension 2. Supposons
que les √©chantillons dans chaque groupe ont une distribution normale de moyennes ¬µ1 et ¬µ2 et
de m√™me variance œÉI (I = matrice identit√© de dimension 2). Soient a, b, et  respectivement
la plus grande distance entre deux √©chantillons d‚Äôun m√™me groupe (√©tendue du groupe), la dis-
tance entre les centres des deux groupes et la plus petite distance entre un √©chantillon du groupe
1 et un autre du groupe 2 (distance entre les groupes). Si b >  > a, alors l‚Äôhistogramme des
D distances de Chebyshev des donn√©es poss√®de deux pics, figure 2.A. Quand la distance entre
D. Demb√©l√©
les groupes diminue ( ‚Üí 0), on observe un rapprochement des deux pics de l‚Äôhistogramme
des distances, figures 2.B, 2.C. Notons que le nombre de pics n‚Äôindique pas le nombre de
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
500
1000
1500
2000
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
500
1000
1500
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
500
1000
A
B
C
FIG. 2 ‚Äì Histogrammes des distances de donn√©es synth√©tiques contenant 2 groupes dans l‚Äôes-
pace de dimension 2. Les √©chantillons dans chaque groupe ont une distribution normale de
moyennes ¬µ1 = [0 0] et ¬µ2 = [4 0]. La variance est la m√™me pour les 2 groupes et a pour
valeur œÉ1 = 0.1, œÉ2 = 0.5 et œÉ3 = 0.7 respectivement pour les donn√©es de (A), (B) et (C).
groupes dans les donn√©es. Cela est montr√© dans la figure 3.A pour des donn√©es de dimensions
10 contenant 14 groupes bien s√©par√©s. Les figures 3.B, 3.C et 3.D montrent les histogrammes
des distances de Chebyshev pour les donn√©es de l‚Äôiris, du serum et de la levure respectivement.
Les donn√©es de l‚Äôiris sont des mesures de la longueur et de la largeur des s√©pales et des p√©tales
de trois esp√®ces d‚Äôiris : Setosa, Virginica et Versicolor. Elles peuvent √™tre r√©cup√©r√©es √† l‚Äôadresse
suivante : ftp ://ftp.ics.uci.edu/pub/machine-learning-databases/. Les donn√©es du s√©rum et de
la levure sont pr√©sent√©es dans le paragraphe 4.
Les figures 2 et 3 montrent que pour des donn√©es contenant des groupes tels que b >  > a,
la valeur de dseuil peut √™tre choisie directement √† partir de l‚Äôhistogramme. Par exemple dseuil
vaut respectivement 0.4 et 0.3 pour les donn√©es des figures 2.A et 3.A. Ceci n‚Äôest pas possible
quand la plus petite distance entre les groupes est plus petite que la plus grande √©tendue des
groupes ( ‚â§ a). Cette situation semble le cas pour les donn√©es de biopuces. D√®s que la dis-
tance  entre les groupes devient plus petite que la plus grande √©tendue a des groupes, la simple
lecture de l‚Äôhistogramme des distances ne permet pas d‚Äôavoir dseuil . La pr√©sence de plus d‚Äôun
pic dans l‚Äôhistogramme indique la pr√©sence de groupes dans les donn√©es. Toutefois, quand
un seuil pic est pr√©sent, sa position par rapport √† l‚Äôorigine peut servir √† tester la pr√©sence de
groupes dans les donn√©es, voir le test bas√© sur la moyenne des distances dans (Bock, 1985). De
fa√ßon g√©n√©rale, le premier pic de l‚Äôhistogramme correspond aux faibles distances c‚Äôest-√†-dire
des distances qui proviennent des √©chantillons proches des centres de leurs groupes respec-
tifs. Ainsi en supposant que ces √©chantillons correspondent √† œÑ% des donn√©es, nous pouvons
estimer le nombre de distances donnant le premier pic de l‚Äôhistogramme. Cela a √©t√© fait pour
diff√©rentes valeurs de œÑ et pour diff√©rentes donn√©es et repr√©sent√© dans le tableau 1. Le choix
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
0 0.2 0.4 0.6 0.8 1
0
0.5
1
1.5
2
2.5
3
x 104
0 0.2 0.4 0.6 0.8
0
500
1000
1500
0 0.2 0.4 0.6 0.8 1
0
2000
4000
6000
8000
10000
12000
14000
0 0.2 0.4 0.6 0.8 1
0
0.5
1
1.5
2
x 106
A B
C D
FIG. 3 ‚Äì Histogrammes des distances de donn√©es (A) synth√©tiques contenant 14 groupes bien
s√©par√©s et de dimension 10, (B) des donn√©es de l‚Äôiris, (C) du s√©rum et de (D) la levure, voir
texte.
retenu consiste √† choisir œÑ de mani√®re √† avoir un seuil dseuil se situant √† la moiti√© des D dis-
tances ordonn√©es. Ceci est obtenu apr√®s r√©solution de l‚Äô√©quation œÑN(œÑN‚àí1)2 =
N(N‚àí1)
4 =
D
2
o√π œÑ ‚àà]0 1[. On obtient ainsi œÑ = 1+
‚àö
1+2N(N‚àí1)
2N , cette expression est voisine de 0.707 d√®s
que N d√©passe 500. Ainsi le seuil dseuil est estim√© par la m√©diane des D distances. Les r√©-
sultats du tableau 1 montrent que le premier pic de l‚Äôhistogramme des distances des donn√©es
synth√©tiques y14C correspond √† un plus faible pourcentage d‚Äô√©chantillons. Les donn√©es de
l‚Äôiris contiennent 3 groupes dont 2 ayant des √©chantillons qui se chevauchent. Quant aux don-
n√©es de biopuces du s√©rum et de la levure les nombres de 10 et 30 groupes ont √©t√© utilis√© dans
la litt√©rature, voir (Iyer et al., 1999) et (Tavazoie et al., 1999). Les r√©sultats du tableau 1 pour
les donn√©es r√©elles montrent que la m√©diane des distances fournit un bon compromis si les
donn√©es sont transform√©es pour avoir des valeurs comprises entre 0 et 1. Nous utilisons cette
solution heuristique pour avoir une valeur du seuil dseuil .
3.4 Crit√®re d‚Äôarr√™t
Les performances d‚Äôune trentaine de proc√©dures permettant de d√©terminer le nombre de
classes dans des donn√©es sont compar√©es dans (Milligan et Cooper, 1985). Pour appr√©cier la
qualit√© de l‚Äôaffectation de l‚Äô√©chantillon xi dans la classe Ck, Rousseeuw a propos√© une me-
sure appel√©e silhouette (Rousseeuw, 1987; Kaufman et Rousseeuw, 1990). La valeur de la
silhouette est comprise entre ‚àí1 et 1. Une valeur n√©gative correspond √† une mauvaise affecta-
tion de l‚Äô√©chantillon consid√©r√©. Le nombre de classes est √©gal √† la valeur deK pour laquelle la
moyenne des silhouettes de tous les √©chantillons des donn√©es est maximale. R√©cemment, une
nouvelle m√©thode bas√©e sur une statistique appel√©e ‚ÄúGap‚Äù est propos√©e dans (Tibshirani et al.,
D. Demb√©l√©
œÑ% 50% 70.7% 75% 80% 90%
y14C dseuil 0.4987 0.7974 0.8058 0.8158 0.8397
(480,10) Kmax 3 1 1 1 1
Iris dseuil 0.1025 0.2307 0.2948 0.3461 0.4615
(150,4) Kmax 10 5 5 5 5
Serum dseuil 0.1444 0.2233 0.2428 0.2663 0.3247
(517,13) Kmax 45 23 14 12 7
Yeast dseuil 0.0256 0.0548 0.0667 0.0873 0.1653
(2945,15) Kmax 86 50 46 35 13
TAB. 1 ‚Äì Valeurs estim√©es de dseuil et de Kmax en fonction du pourcentage œÑ% des donn√©es
suppos√©es utilis√©es dans le premier pic de l‚Äôhistogramme. Les donn√©es synth√©tiques y14C et
r√©elles de l‚Äôiris, du s√©rum et de la levure sont utilis√©es.
2001). Toutes ces m√©thodes ont √©t√© propos√©es pour des algorithmes de classification bas√©s sur
la distance Euclidienne.
En utilisant la transformation [0, 1] des donn√©es, des √©chantillons de profils identiques
peuvent √™tre affect√©s √† des classes diff√©rentes en fonction de leur valeur absolue moyenne.
Pour regrouper les classes, nous utilisons l‚Äôinformation de leur forme. Ceci permet d‚Äôiden-
tifier les classes de profils voisins. Nous d√©finissons alors la co-variation des profils comme
le coefficient de corr√©lation des √©carts non centr√©s observ√©s pour les valeurs successives de
chaque profil. Le mot coefficient de co-expression (ce) est utilis√© dans (Moller-Levet et Yin,
2005) pour des donn√©es temporelles de biopuces d‚Äôexpression. Pour ce type de donn√©es en
temps continu, la variation des profils est obtenue en prenant la d√©riv√©e. En temps discr√©tis√©,
le vecteur‚àÜci obtenu avec les √©carts associ√©s au profil ci est :
‚àÜci = cik ‚àí ci(k‚àí1) ; k = 2, . . . , n avec ‚àÜck1 = 0 (6)
Pour deux profils ci et cj , le coefficient de co-variation (ccv) est d√©fini par :
ccv(ci, cj) =
‚àën
k=1‚àÜcik‚àÜcjk(‚àën
k=1‚àÜc
2
ik
‚àën
k=1‚àÜc
2
jk
) 1
2
(7)
Les ccv ont √©t√© convertis en distances qui sont ensuite utilis√©es dans la m√©thode de classifica-
tion hi√©rarchique ascendante. La distance de saut minimal (single linkage) a √©t√© utilis√©e pour
aggr√©ger les classes.
4 R√©sultats
Pour illustrer les performances de la m√©thode propos√©e, nous avons utilis√© des donn√©es is-
sues de la technologie des biopuces pour l‚Äô√©tude de l‚Äôexpression des g√®nes. Les biopuces sont
des petits supports (lames de verre) sur lesquels plusieurs milliers de s√©quences d‚ÄôADN (Acide
D√©soxyriboNucl√©ique) correspondant chacune √† un g√®ne sont attach√©es √† des adresses connues
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
(spots). L‚ÄôARN (Aide RiboNucl√©ique) des √©chantillons √† analyser est marqu√© avec une mol√©-
cule fluorescente puis hybrid√© (par appariement entre s√©quences d‚ÄôADN compl√©mentaires) sur
les biopuces. Les biopuces sont ensuite scann√©es. Le niveau d‚Äôexpression des g√®nes est repr√©-
sent√© par une intensit√© de fluorescence. La quantification de l‚Äôimage (mesure de l‚Äôintensit√© de
fluorescence pour chacun des spots) fournit des donn√©es num√©riques qui servent √† l‚Äôanalyse.
4.1 Donn√©es utilis√©es
4.1.1 Donn√©es de s√©rum
Ces donn√©es sont d√©crites et utilis√©es dans (Iyer et al., 1999). Elles proviennent d‚Äôune
√©tude sur la r√©ponse de fibroblastes humains au s√©rum au cours du temps (n = 13 condi-
tions). Nous avons utilis√© les donn√©es correspondant √† une s√©lection de N = 517 g√®nes.
Ces donn√©es peuvent √™tre r√©cup√©r√©es √† l‚Äôadresse suivante : http ://www.sciencemag.org/ fea-
ture/data/984559.shl.
4.1.2 Donn√©es de la levure
Nous nous sommes aussi servi des donn√©es issues de l‚Äô√©tude du profil d‚Äôexpression de 6200
g√®nes chez la levure (Cho et al., 1998) r√©alis√© sur deux cycles mitotiques au cours desquels les
valeurs ont √©t√© mesur√©es toutes les 10 minutes (soit 17 hybridations diff√©rentes). Nous avons
utilis√© les donn√©es d‚Äôune s√©lection de N = 2945 g√®nes, c‚Äôest-√†-dire la s√©lection faite dans
(Tavazoie et al., 1999). Dans cette s√©lection les valeurs qui correspondent aux instants 90 et
100 minutes sont exclues, soit n = 15.
4.2 R√©sultats obtenus
Avant de pr√©senter les r√©sultats obtenus avec la proc√©dure propos√©e, nous avons d√©termin√©
le rang de la matrice des distances pour les donn√©es du s√©rum et de la levure. Le rang de la
matrice des distances est donn√© par le nombre de ses valeurs singuli√®res sup√©rieures √† 0.001.
Le tableau 2 montre le rang de la matrice des distances de Minkowski et de Chebyshev pour
les donn√©es du s√©rum et de la levure. Le rang obtenu est syst√©matiquement plus grand pour la
distance de Chebyshev.
Minkowski Chebyshev
Norme L2 L4 L8 L20 [0,1]
S√©rum 13 (15) 38 (41) 90 (93) 217 (249) 514
Levure 15 (17) 44 (47) 104 (107) 269 (287) 2943
TAB. 2 ‚Äì Rang de la matrice des distances pour la distance de Chebyshev et pour diff√©rentes
normes de la distance de Minkowski. La valeur maximale th√©orique du rang est indiqu√©e entre
parenth√®ses pour les donn√©es Lp norm√©es.
D. Demb√©l√©
4.2.1 Donn√©es de s√©rum
La valeur de distance seuil obtenue pour ces donn√©es est dseuil = 0.223. En utilisant cette
valeur, nous avons obtenu un nombre maximum de classes Kmax = 23. Les profils de ces
classes sont repr√©sent√©s sur la figure 4.
0 5 10 15
‚àí2
‚àí1
0
C1, N=22
0 5 10 15
‚àí1
‚àí0.5
0
C2, N=136
0 5 10 15
‚àí1
0
1
C3, N=58
0 5 10 15
‚àí2
‚àí1
0
1
C4, N=53
0 5 10 15
‚àí2
0
2
C5, N=5
0 5 10 15
‚àí2
‚àí1
0
1
C6, N=13
0 5 10 15
‚àí1
0
1
2
C7, N=17
0 5 10 15
‚àí1
0
1
2
3
C8, N=5
0 5 10 15
‚àí2
‚àí1
0
C9, N=34
0 5 10 15
0
0.5
1
C10, N=21
0 5 10 15
‚àí1
0
1
2
C11, N=17
0 5 10 15
‚àí1
0
1
2
C12, N=12
0 5 10 15
‚àí1
0
1
2
3
C13, N=11
0 5 10 15
0
1
2
3
C14, N=9
0 5 10 15
‚àí1
0
1
2
C15, N=18
0 5 10 15
‚àí1
0
1
2
C16, N=30
0 5 10 15
‚àí1
0
1
2
C17, N=13
0 5 10 15
0
1
2
3
C18, N=11
0 5 10 15
‚àí1
0
1
2
3
C19, N=6
0 5 10 15
‚àí1
0
1
2
3
C20, N=6
0 5 10 15
‚àí1
0
1
2
3
C21, N=12
0 5 10 15
0
5
C22, N=3
0 5 10 15
0
5
C23, N=5
FIG. 4 ‚Äì Donn√©es de s√©rum, les 23 classes obtenues dans la phase initiale. Chaque profil est
identifi√© par un num√©ro et le nombre des √©chantillons qui la forme, e.g. la premi√®re classe C1
contient 22 √©chantillons (N = 22).
Un extrait de la matrice des ccv des 23 profils initiaux des donn√©es de s√©rum est donn√©e
dans les tableaux 3 et 4. La similitude des profils des classes 13, 14 et 20 est indiqu√©e dans
le tableau 4 par des ccv √©lev√©s, ‚â• 0.75. La valeur du ccv est la plus petite, ‚àí0.87, pour les
classes 6 et 12 montrant une opposition de profils pour ces classes. La figure 4 contient tous
les profils obtenus dans (Iyer et al., 1999) avec des redondances. Avec la m√©thode hi√©rarchique
ascendante les classes ont √©t√© regroup√©es.
4.2.2 Donn√©es de la levure
En appliquant l‚Äôalgorithme propos√© aux donn√©es des 2945 g√®nes, nous avons obtenu une
distance seuil dseuil = 0.0549 et un nombre maximum de classes Kmax = 50. Parmi les 50
profils obtenus, certains sont connus pour ces donn√©es (Tavazoie et al., 1999). Les profils de
certaines classes sont repr√©sent√©s sur les figures 5 et 6.
Nous avons calcul√© la matrice des ccv des 50 classes obtenues puis cette matrice est utilis√©e
dans la m√©thode hi√©rarchique ascendante √† saut minimal. Les classes 14 et 31 ont √©t√© fusionn√©es
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
1 2 3 4 5 6 7 8 9 10
2 0.84 0
3 -0.20 0.13 0
4 0.84 0.95 0.11 0
5 0.31 -0.05 -0.06 0.13 0
6 -0.35 -0.23 0.48 -0.14 0.26 0
7 -0.44 -0.60 -0.03 -0.56 0.39 0.45 0
8 -0.28 -0.05 0.48 0.07 -0.06 0.51 0.30 0
9 0.75 0.91 0.36 0.92 0.18 0.11 -0.41 0.11 0
10 -0.64 -0.72 -0.16 -0.69 0.03 0.35 0.62 0.07 -0.62 0
11 0.61 0.59 -0.45 0.56 -0.34 -0.74 -0.65 -0.22 0.28 -0.44
12 0.41 0.29 -0.31 0.28 0.02 -0.87 -0.48 -0.39 0.02 -0.38
13 0.68 0.75 -0.25 0.77 0.02 -0.33 -0.47 -0.08 0.57 -0.60
14 0.17 0.36 -0.25 0.42 -0.26 -0.17 -0.44 0.19 0.21 -0.27
15 -0.22 -0.22 0.14 -0.27 -0.04 -0.47 -0.17 -0.18 -0.35 0.04
TAB. 3 ‚Äì Extrait de la matrice des ccv des 23 classes initialement obtenues pour les donn√©es
de s√©rum.
0 5 10 15
500
1000
1500
2000
C5, N=34
0 5 10 15
110
120
130
140
C7, N=1610
0 5 10 15
280
300
320
C8, N=395
0 5 10 15
400
600
C10, N=40
0 5 10 15
600
800
1000
1200
C11, N=52
0 5 10 15
200
400
600
800
1000
1200
C12, N=13
0 5 10 15
400
500
600
C13, N=48
0 5 10 15
200
400
600
800
C14, N=39
0 5 10 15
400
600
800
1000
1200
1400
C15, N=38
0 5 10 15
600
700
800
C16, N=28
0 5 10 15
400
600
800
1000
1200
1400
C18, N=10
0 5 10 15
500
1000
C20, N=21
FIG. 5 ‚Äì Donn√©es de la levure.
les premi√®res, √† cette nouvelle classe, la classe 32 a √©t√© ajout√©e √† la huiti√®me √©tape puis la classe
12 √† la douzi√®me √©tape de la m√©thode hi√©rarchique.
4.2.3 Discussion
Avec les donn√©es de la levure, nous avons obtenu une classe, C7, contenant plus de la
moiti√© des √©chantillons dans les donn√©es. En observant les niveaux d‚Äôexpression des g√®nes de
cette classe, on note que le profil d‚Äôexpression moyen varie entre 110 et 120. Cette valeur est
D. Demb√©l√©
12 13 14 15 16 17 18 19
13 0.43 0
14 0.23 0.75 0
15 0.66 -0.24 -0.22 0
16 0.05 -0.47 -0.25 0.44 0
17 -0.08 -0.42 -0.29 0.38 0.67 0
18 -0.44 -0.12 -0.08 -0.20 0.08 0.55 0
19 -0.06 -0.56 -0.38 0.42 0.76 0.59 0.08 0
20 0.38 0.90 0.87 -0.14 -0.42 -0.38 -0.08 -0.62
21 0.31 -0.44 -0.27 0.69 0.67 0.36 -0.15 0.78
22 0.19 -0.42 -0.28 0.49 0.73 0.33 0.08 0.68
23 0.43 -0.27 -0.13 0.78 0.78 0.60 0.10 0.47
TAB. 4 ‚Äì Extrait de la matrice des ccv des 23 classes initialement obtenues pour les donn√©es
de s√©rum (suite).
0 5 10 15
500
1000
C21, N=18
0 5 10 15
0
500
1000
C22, N=5
0 5 10 15
0
500
1000
C23, N=10
0 5 10 15
500
1000
1500
2000
C27, N=7
0 5 10 15
0
500
1000
C28, N=4
0 5 10 15
0
1000
C31, N=11
0 5 10 15
500
1000
1500
C32, N=8
0 5 10 15
1000
2000
3000
4000
C33, N=8
0 5 10 15
500
1000
1500
C36, N=7
0 5 10 15
1000
2000
C37, N=13
0 5 10 15
0
1000
2000
C39, N=4
0 5 10 15
0
1000
2000
C42, N=5
FIG. 6 ‚Äì Donn√©es de la levure (suite), s√©lection de 24 classes obtenues dans la phase initiale.
Chaque profil est identifi√© par un num√©ro et le nombre des √©chantillons qui la forme, e.g. la
premi√®re classe C14 contient 39 √©chantillons (N = 39).
tr√®s faible compar√©e √† la variation globale (0‚àí9500). Ce type de r√©sultat n‚Äôest pas obtenu pour
les donn√©es du s√©rum qui sont des ratios d‚Äôexpression dont les valeurs varient entre‚àí3 et 3. La
classe C7 contient tous les √©chantillons de niveau d‚Äôexpression moyen faible. La normalisation
[0, 1] a ramen√© ces valeurs au voisinage de z√©ro. Nous avons extrait les 1610 √©chantillons de la
classe C7. La proc√©dure propos√©e est ensuite appliqu√©e √† cette nouvelle s√©lection. Ceci nous a
permis d‚Äôobtenir des profils similaires √† certains pr√©sents dans les figures 5 et 6.
La normalisation [0, 1] permet d‚Äôobtenir des classes contenant des √©chantillons de profil
moyen similaire. Nous pouvons ainsi s√©parer des g√®nes fortement exprim√©s de ceux faiblement
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
exprim√©s. Ceci est masqu√© par la normalisation L2. Le ccv permet ensuite de regrouper les
classes de profils similaires ind√©pendamment des niveaux des profils moyens observ√©s. Le ccv
n‚Äôest pas parfait, voir sa valeur (cas des donn√©es du s√©rum) pour les classes 3 et 6 qui sont
assez similaires puis pour les classes 4 et 13 qui ne sont pas similaires. Cependant il a donn√©
des r√©sultats semblables √† ceux pr√©sent√©s dans (Iyer et al., 1999) o√π le nombre de classes a
√©t√© fix√© a priori √† 10. La proc√©dure propos√©e est tr√®s rapide. L‚Äô√©tape de calcul des distances, la
plus co√ªteuse en temps est 6 fois plus rapide avec la distance de Chebyshev qu‚Äôavec la distance
Euclidienne. Pour les donn√©es avec N tr√®s √©lev√©, une s√©lection al√©atoire de quelques milliers
d‚Äô√©chantillons peut √™tre suffisant pour obtenir une estimation de la valeur du seuil dseuil.
5 Conclusion
Une nouvellem√©thode de classification de donn√©es est pr√©sent√©e. La distance de Chebyshev
est utilis√©e. Cette distance semble plus appropri√©e pour les donn√©es de grande dimension. La
strat√©gie propos√©e consiste dans un premier temps √† rechercher un nombremaximumde classes
dans les donn√©es. Ceci est fait apr√®s examen de la matrice des distances des donn√©es. Puis une
r√©duction du nombre de classes est effectu√©e. Pour cela une m√©thode hi√©rarchique ascendante
peut √™tre utilis√©e. La m√©thode K-Means qui offre la possibilit√© de r√©affecter un √©chantillon √†
une autre classe peut √™tre aussi utilis√©e. Pour la standardisation des donn√©es, la m√©thode qui
permet de ramener toutes les valeurs entre 0 et 1 a √©t√© utilis√©e. Un travail futur consistera √†
√©tudier le coefficient de co-variation plus en d√©tail. Une interface conviviale pourra √©galement
faciliter l‚Äôexploitation des r√©sultats de classification.
Remerciements
Merci √† Philippe Kastner, Bernard Jost et Christelle Thibault qui ont pris le temps de lire
cet article. Ce travail a b√©n√©fici√© du soutien du Centre National de la Recherche Scientifique,
de l‚ÄôInstitut National de la Recherche M√©dicale, de l‚ÄôH√¥pital Universitaire de Strasbourg et du
Centre National de Recherche en G√©nomique.
R√©f√©rences
Beyer, K., J. Goldstein, R. Ramakrishnan, et U. Shaft (1998). When Is ‚ÄúNearest Neighbor"
Meaningful ? In P. B. Catriel Beeri (Ed.), Proc. of the ICDT‚Äô99, Volume LNCS 1540, pp.
217‚Äì235. Springer-Verlag Berlin Heidelberg.
Bock, H. H. (1985). On Some Significance Tests in Cluster Analysis. Journal of Classifica-
tion 2, 77‚Äì108.
Bonnet, N., M. Herbin, J. Cutrona, et J.-M. Zahm (2002). A New Clustering Approach, Based
on the Estimation of the Probability Density Function, for Gene Expression Data. In IFCS
symposium, july 15-20, Krak√∂w, Poland, pp. pp‚Äìpp+7.
Cho, R. J., M. J. Campbell, E. A. Winzeler, L. Steinmetz, A. Conway, L. Wodicka, T. G. Wolf-
sberg, A. E. Gabrielian, D. Landsman, D. J. Lockhart, et R. W. Davis (1998). A Genome-
Wide Transcriptional Analysis of the Mitotic Cell Cycle. Molecular Cell 2, 65‚Äì73.
D. Demb√©l√©
Demartines, P. (1994). Analyse de donn√©es par r√©seaux de neurones auto-organis√©s. Ph. D.
thesis, TIRF, INPG, Grenoble, France.
Dohono, D. L. (2000). High-Dimensional Data Analysis : The Curses and Blessings of Di-
mensionality. In Am. Math. Soc. Conf. "Math Challenges of the 21st Century", Los Angeles,
www-stat.stanford.edu/Àúdonoho.
Estlick, M., M. Leeser, J. Theiler, et J. J. Szymanski (2001). Algorithmic Transformations in
the Implementation of K- Means Clustering on Reconfigurable Hardware. In FPGA, pp.
103‚Äì110.
Everitt, B. S. (1993). Cluster Analysis (3rd ed.). Arnold, London.
Golub, G. H. et C. F. V. Loan (1996). Matrix Computations. The Johns Hopkins University
Press, Third Edition.
Herault, J., A. Gu√©rin-Dugu√©, et P. Villemain (2002). Searching for the Embedded Manifolds
in High-Dimensional Data, Problems and Unsolved Questions. In SANN‚Äô2002 Proceedings
- European Symposium on Artificial Neural Networks 24-26 April, Bruges, Belgium, pp.
173‚Äì184.
Holter, N. S., M. Mitra, A. Maritan, M. Cieplak, J. R. Banavar, et N. V. Fedoroff (2000).
Fundanmental Patterns Underying Gene Expression Profiles : Simplicity from Complexity.
PNAS 97(15), 8409‚Äì8414.
Horn, D. et I. Axel (2003). Novel Clustering Algorithm for Microarray Expression Data in a
Truncated SVD Space. Bioinformatics 19(9), 1110‚Äì1115.
Iyer, V. R., M. B. Eisen, D. T. Ross, G. Schuler, T. Moore, J. C. F. Lee, J. M. Trent, L. M.
Staudt, J. H. Jr, M. S. Bogoski, D. Lashkari, D. Shalon, D. Botstein, et P. O. Brown (1999).
The Transcriptional Program in the Response of Human Fibroblast to Serum. Science 283,
83‚Äì87.
Jain, A. K. et R. C. Dubes (1988). Algorithms for Clustering Data. Prentice Hall, Englewood
Cliff, New Jersey.
Jain, A. K., R. P. W. Duin, et J. Mao (2000). Statistical Pattern Recognition : A Review. IEEE
trans. PAMI 22(1), 4‚Äì37.
Jimenez, J. O. et D. Landgrebe (1995). High Dimension Feature Reduction Via Projection
Pursuit. Technical Report TR-ECE 96-5, School of Electical and Computer Engineering,
Purdue University.
Kaufman, L. et P. Rousseeuw (1990). Finding Group in Data : an Introduction to Cluster
Analysis. Wiley, New York.
Lukashin, A. V. et R. Fuchs (2001). Analysis of Temporal Gene Expression Profiles : Cluste-
ring by Simulated Annealing and Determining the Optimal Number of Clusters. Bioinfor-
matics 17(5), 405‚Äì414.
Milligan, G. W. et M. C. Cooper (1985). An Examination of Procedures for Determining the
Number of Clusters in a Data Set. Psychometrika 50(2), 159‚Äì179.
Moller-Levet, C. S. et H. Yin (2005). Modeling and Analysis of Gene Expression Time-Series
Based on Co-Expression. Int J of Neural Syst. 15(4), 311‚Äì322.
Une m√©thode de classification adapt√©e aux donn√©es de grande dimension
Rousseeuw, J. P. (1987). Silhouettes : a Graphical Aid to the Interpration and Validation of
Cluster Analysis. J. Comp. Appl. Math. 20, 53‚Äì65.
Tavazoie, S., J. D. Hughes, M. J. Campbell, R. I. Cho, et G. M. Church (1999). Systematic
Determination of Genetic Network Architecture. Nature Genetic 22, 281‚Äì285.
Tibshirani, R., G. Walther, et T. Hastie (2001). Estimating the number of clusters in a dataset
via the gap statistic. Journal Royal Stat Society (B) 63(2), 441‚Äì423.
Wong, M. A. (1982). A hybrid clustering method for identifying high-density clusters. Journal
of American Statistical Association 77(380), 841‚Äì847.
Yeung, K. Y. et W. L. Ruzzo (2001). Principal Component Analysis for Clustering Gene
Expression Data. Bioinformatics 17(9), 763‚Äì774.
Summary
In this paper, we proposed a new clustering method especially suitable for high dimensional
data. This method combines partitioning and hierarchical clustering algorithms for getting
groups in a given data set. We also used Chebyshev distance which seems interesting for
high dimensional. Indeed, less computationally load is required in comparison with Euclidean
distance frequently used because of its nice geometrical properties. Microarray data are used
for illustrating performances of our findings.
