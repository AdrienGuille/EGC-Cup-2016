Carte auto-organisatrice probabiliste sur donn√©es binaires
Rodolphe Priam, Mohamed Nadif
LITA, Universit√© de Metz
Ile du Saulcy, 57045 Metz
R√©sum√©. Les m√©thodes factorielles d‚Äôanalyse exploratoire statistique d√©finissent
des directions orthogonales informatives √† partir d‚Äôun ensemble de donn√©es.
Elles conduisent par exemple √† expliquer les proximit√©s entre individus √† l‚Äôaide
d‚Äôun groupe de variables caract√©ristiques. Dans le contexte du datamining lorsque
les tableaux de donn√©es sont de grande taille, une m√©thode de cartographie syn-
th√©tique s‚Äôav√®re int√©ressante. Ainsi une carte auto-organisatrice (SOM) est une
m√©thode de partitionnement munie d‚Äôune structure de graphe de voisinage -sur
les classes- le plus souvent planaire. Des travaux r√©cents sont d√©velopp√©s pour
√©tendre le SOM probabiliste Generative Topographic Mapping (GTM) aux mo-
d√®les de m√©langes classiques pour donn√©es discr√®tes. Dans ce papier nous pr√©-
sentons et √©tudions un mod√®le g√©n√©ratif sym√©trique de carte auto-organisatrice
pour donn√©es binaires que nous appelons Bernoulli Aspect Topological Model
(BATM). Nous introduisons un nouveau lissage et acc√©l√©rons la convergence de
l‚Äôestimation par une initialisation originale des probabilit√©s en jeu.
1 Introduction
La visualisation des corr√©lations et similarit√©s principales dans un √©chantillon de donn√©es
est l‚Äôobjectif des m√©thodes factorielles (Lebart et al., 1984). Ces m√©thodes cherchent souvent
des directions informatives orthogonales dans un nuage de donn√©es. Ces directions concentrent
l‚Äôessentiel de la variance projet√©e car l‚Äôinertie est porteuse de sens. Une d√©composition perti-
nente de l‚Äôinertie sur des plans de projection r√©v√®le quels individus sont similaires et quelles
variables sont d√©pendantes. Bien que ces m√©thodes soient tr√®s pertinentes, les grands √©chan-
tillons de donn√©es demandent de nouvelles m√©thodes efficaces pour leur analyse. Dans ce
contexte, les cartes de Kohonen (1997) sont connues dans le domaine de l‚Äôanalyse explora-
toire des donn√©es pour g√©n√©raliser les m√©thodes factorielles telles que la m√©thode d‚ÄôAnalyse
en Composantes Principales ou ACP (Lebart et al., 1984) pour les donn√©es continues. Plus
g√©n√©ralement, les cartes auto-organisatrices ou SOM (Kohonen, 1997) sont des m√©thodes de
classification avec une contrainte de voisinage sur les classes conf√©rant un sens topologique √†
la partition finale. Le GTM ou Generative Topographic Mapping (Bishop et al., 1998) est une
carte auto-organisatrice probabiliste avec des contraintes sur les moyennes d‚Äôun m√©lange gaus-
sien pour donn√©es continues, mais ce mod√®le est inop√©rant pour des donn√©es cat√©gorielles ou
binaires. Des mod√®les r√©cents (Girolami, 2001; Kab√°n et Girolami, 2001; Tipping, 1999) ont
√©t√© propos√©s pour √©tendre le GTM aux mod√®les de m√©langes classiques pour donn√©es discr√®tes.
Hofmann et Puzicha (1998) ont par contre propos√© l‚Äôapproche du mod√®le sym√©trique √† aspects
- 445 - RNTI-E-6
Carte auto-organisatrice probabiliste sur donn√©es binaires
qui traite de la classification simultan√©e des lignes et colonnes d‚Äôun tableau de contingence.
Cette approche est b√©n√©fique dans plusieurs domaines tels que le textmining et la segmentation
d‚Äôimage. Dans ce papier, nous nous int√©ressons aux donn√©es binaires et nous √©tudions un mo-
d√®le original en pr√©sentant un nouveau lissage de carte auto-organisatrice et une initialisation
adapt√©e pour acc√©l√©rer la convergence des algorithmes d‚Äôestimation des param√®tres du mod√®le.
Les probabilit√©s sont param√©tr√©es de fa√ßon ad√©quate comme le GTM afin d‚Äôinduire une auto-
organisation des facteurs latents, ce qui nous am√®ne √† une nouvelle m√©thode pour visualiser
les donn√©es discr√®tes ou vecteurs multidimensionnels de composantes 1/0.
Ce papier est organis√© comme suit. Dans la section 2 nous d√©crivons notre mod√®le et abor-
dons le probl√®me d‚Äôestimation des param√®tres par la maximisation de la vraisemblance. Dans
la section 3 nous r√©alisons des exp√©riences num√©riques pour valider notre mod√®le. Enfin la
section 4 r√©sume les points principaux et pr√©sente les travaux en cours.
2 Le mod√®le BATM
Le mod√®le propos√© repose sur l‚Äôhypoth√®se d‚Äôind√©pendance des I √ó J cellules xij ‚àà {0, 1}
d‚Äôun tableau binaire en mod√©lisant chaque probabilit√© unidimensionnelle d‚Äôobserverxij comme
un m√©lange deK lois de Bernoulli : Pr(xij = 1) = E(xij) =
‚àë
k pikiajk avec piki les propor-
tions des K composants telles que
‚àë
k piki = 1. Ce mod√®le g√©n√©ratif correspond √† s√©lectionner
pour chaque ligne xi = (xi1, xi2, ¬∑ ¬∑ ¬∑ , xiJ ) fix√©e, une distribution discr√®te pii de composantes
piki, puis pour chaque j-i√®me composante xij , s√©lectionner un √©tat k avec la probabilit√© piki afin
de lui attribuer une valeur binaire selon la loi de Bernoulli de param√®tre ajk . Un mod√®le com-
parable pour la classification sans contrainte a √©t√© r√©cemment propos√©. La log-vraisemblance
de D = {xi}i=Ii=1 s‚Äô√©crit alors :
L(Œ∏|D) =
‚àë
i,j
log
[‚àë
k
pikia
xij
jk (1‚àí ajk)
1‚àíxij
]
.
Afin d‚Äôinduire une auto-organisation topologique des probabilit√©s, nous consid√©rons lesK co-
ordonn√©es {sk}k=Kk=1 d‚Äôune grille bidimensionnelle r√©guli√®re qui mod√©lise un plan discr√©tis√©
sur lequel l‚Äôensemble des donn√©es est dispos√© par le BATM. La grille est projet√©e non lin√©aire-
ment dans un espace de plus grande dimension L, par une transformation non lin√©aire consti-
tu√©e de L bases fonctionnelles œÜ`, et telle que Œæk = (œÜ1(sk), œÜ2(sk), ¬∑ ¬∑ ¬∑ , œÜL(sk))T ; on note
la matrice Œ¶ = [Œæ1|Œæ2| ¬∑ ¬∑ ¬∑ |ŒæK ]T . Les ajk forment alors les noeuds d‚Äôune surface non lin√©aire
discr√®te : les probabilit√©s de Bernoulli sont param√©tr√©es par des fonctions ajk = œÉ(wTj Œæk) o√π
œÉ(u) = eu/(1 + eu) est une fonction sigmo√Øde et wj un param√®tre inconnu appartenant √† RL.
La log-vraisemblance devient :
L(Œ∏|D) =
‚àë
i,j
log
[‚àë
k
pikiœÉ(w
T
j Œæk)
xij (1 ‚àí œÉ(wTj Œæk))
1‚àíxij
]
.
Ce mod√®le s‚Äôinterpr√®te comme une version binaire cartographique du LSA probabiliste ou
pLSA de Hofmann et Puzicha (1998). Les param√®tres inconnus sont estim√©s dans la section
suivante.
- 446 -RNTI-E-6
R. Priam et M. Nadif
2.1 Estimation par GEM
L‚Äôinf√©rence de notre mod√®le se r√©alise en maximisant la log-vraisemblance par une m√©-
thode it√©rative ; une solution analytique exacte n‚Äôexiste pas en raison des non lin√©arit√©s du
m√©lange et des fonctions sigmo√Ødes. Donc nous √©tudions l‚Äôapproche de l‚Äôalgorithme de mon-
t√©e de gradient par EM (Dempster et al., 1977) g√©n√©ralis√© (GEM) de McLachlan et Peel
(2000). Cette approche suppose la vraisemblance compl√©t√©e par la connaissance de la parti-
tion Z = {Z1,Z2, ¬∑ ¬∑ ¬∑ ,ZK} :
L(Œ∏, Z|D) =
‚àë
i,j
log
[
pizii a
xij
jzi
(1 ‚àí ajzi)
1‚àíxij
]
,
avec zi les variables latentes ayant pour support {1, 2 ¬∑ ¬∑ ¬∑ ,K}. L‚ÄôalgorithmeEMou Expectation-
Maximisation repose sur la maximisation de l‚Äôesp√©rance conditionnelle sachant les donn√©es et
les param√®tres de l‚Äôit√©ration pr√©c√©dente. Ayant P (t)(Z|D) du pas t pr√©c√©dent, nous maximi-
sons au pas t+ 1 :
Q(Œ∏|Œ∏(t)) = EP (t)(Z|D)[L(Œ∏, Z|D)]
=
‚àë
i,j,k
P
(t)
k|i,j,xij
{
log piki + xijŒæ
T
k wj ‚àí log(1 + exp(Œæ
T
k wj))
}
,
avec Pk|i,j,xij ‚àù pikia
xij
jk (1 ‚àí ajk)
1‚àíxij la probabilit√© a posteriori que xij soit g√©n√©r√©e pas
le composant k. Un calcul direct donne pi(t+1)ki = argmaxpikiQ(Œ∏|Œ∏(t)) =
‚àë
j P
(t)
k|i,j,xij
/J .
Pour r√©soudre w(t+1) = argmaxwQ(Œ∏|Œ∏(t)), nous effectuons des d√©rivations √©l√©mentaires du
crit√®re qui aboutissent au gradient Q(t)j et au bloc de la hessienne H
(t)
j . Le pas de Newton-
Raphson suivant augmente alors localement la log-vraisemblance :
w
(t+1)
j = w
(t)
j ‚àíH
(t)
j
‚àí1
Q
(t)
j .
A la convergence du GEM, nous obtenons un estimateur au maximum de vraisemblance not√© Œ∏ÀÜ.
Pour √©viter un surapprentissage et une instabilit√© num√©rique, nous ajoutons √† la fonctionQ un
param√®tre de r√©gularisation bay√©sien (MacKay, 1992) : ‚àíŒ±‚àëj wTj wj . Cette correction ajoute
‚àíŒ±wj au gradientQj et ‚àíŒ± √† la diagonale de la hessienneHj . La valeur de l‚Äôhyperparam√®tre
Œ± est choisie manuellement comme la plupart du temps dans la litt√©rature, ici nous avons pris
Œ± = 0.01.
2.2 Formulation IRLS
Nous √©crivons l‚Äôalgorithme de Newton sous une forme matricielle qui est proche d‚Äôun
pas d‚ÄôIteratively Reweighted Least Squares ou IRLS (McCullagh et Nelder, 1983) pour la
r√©gression logistique. Pour j de 1 √† J :
Q
(t)
j = Œ¶
T
[
R
(t)
j Aj ‚àíG
(t)
j a
(t)
j
]
‚àí 0.01w
(t)
j ,
H
(t)
j = ‚àíŒ¶
TG
(t)
j F
(t)
j Œ¶‚àí 0.01IL.
- 447 - RNTI-E-6
Carte auto-organisatrice probabiliste sur donn√©es binaires
Nous avonsR(t)j la matrice de tailleK√óI qui compte pour cellules les probabilit√©s a posteriori
P
(t)
k|i,j,xij
, la matrice diagonale G(t)j a pour √©l√©ments non nuls
‚àë
i P
(t)
k|i,j,xij
, Aj est le vecteur
de i-i√®me composante aij , a(t)j est un vecteur colonne avec a
(t)
jk pour k-i√®me composante, F
(t)
j
est la matrice diagonale avec a(t)jk (1 ‚àí a
(t)
jk ) sur sa diagonale et enfin, IL est la matrice identit√©
de taille L.
Pour acc√©l√©rer num√©riquement l‚Äôalgorithme, l‚Äôapproche de Bohning (1993) remplace la
matrice exacte, relativement lourde √† calculer en pratique, par une matrice alternative fixe. Par
exemple, la matriceB = ‚àí I4Œ¶
TŒ¶‚àí 0.01IL, qui est telle queH(t)j  B, i.e.H
(t)
j ‚àíB est non
n√©gative, sym√©trique, ce qui permet de maximiser la vraisemblance. Comme la convergence
est lente, nous proposons un algorithme de type variationnel alternatif.
2.3 Estimation variationnelle
En suivant la borne1 (Saul et al., 1996) sur log(1+ exp(ŒæTk wj)), nous obtenons le nouveau
crit√®re √† optimiser :
QÀú(Œ∏|Œ∏(t)) =
‚àë
i,j,k
P
(t)
k|i,j,xij
{
log piki + (xij ‚àí 0.5)Œæ
T
k wj
+ Œª(j)[(Œæ
T
k wj)
2 ‚àí 2j ] + 0.5j ‚àí log(1 + exp(j))
}
.
Avec Œª(j) = ‚àítanh(0.5j)/(4j) tel que Q(Œ∏|Œ∏(t)) ‚â• QÀú(Œ∏|Œ∏(t)) o√π j est un param√®tre
variationnel √† estimer en maximisant QÀú. En d√©rivant ce nouveau crit√®re, nous obtenons le pas
de maximisation :

(t)
j =
‚àö
w
(t)
j
T
Œ¶TG
(t)
j Œ¶w
(t)
j
I
,
w
(t+1)
j =
[
‚àí 2Œª(
(t)
j )Œ¶
TG
(t)
j Œ¶‚àí 0.01IL
]‚àí1
Œ¶TR
(t)
j A
‚Ä≤
j ,
o√π A‚Ä≤j est le vecteur colonne ayant xij ‚àí 0.5 pour i-√®me composante. Finalement, trois algo-
rithmes, et un quatri√®me d√©crit ci-apr√®s, sont pr√©sent√©s pour estimer les param√®tres du mod√®le.
Ayant √©limin√© la solution du gradient simple mais inefficace, on constate que l‚Äôalgorithme
IRLS donne la meilleure vraisemblance dans notre cas comme le montre les exp√©riences dans
la section suivante.
3 Simulations
Dans cette section, nous abordons tout d‚Äôabord deux √©l√©ments compl√©mentaires √† la m√©-
thode propos√©e, l‚Äôinitialisation des param√®tres du mod√®le et l‚Äôauto-organisation des probabi-
lit√©s sur les lignes afin d‚Äôobtenir la meilleure carte projective finale possible. Nous d√©crivons
alors les r√©sultats num√©riques de nos simulations sur des donn√©es binaires r√©elles.
1log œÉ(u) ‚â• u/2 + Œª()(u2 ‚àí 2) + log œÉ() ‚àí /2 pour des raisons de concavit√©.
- 448 -RNTI-E-6
R. Priam et M. Nadif
3.1 Initialisation du mod√®le
Des tirages al√©atoires r√©p√©t√©s des param√®tres initiaux sont une solution aux minima locaux
que rencontrent les algorithmes bas√©s sur le gradient. Pour obtenir la meilleure convergence
possible on proc√®de √† une ‚Äùbonne initialisation‚Äù. Puisque les cartes de Kohonen sont des g√©-
n√©ralisations de l‚ÄôACP, le premier plan de cette m√©thode fournit une int√©ressante premi√®re
position (Elemento, 1999) des centres de classes de la carte. Notons (Xci , Y ci ) les coordonn√©es
sur le premier plan factoriel de l‚ÄôACP (Jolliffe, 2002), AFC (Benz√©cri, 1992), LSA (Deer-
wester et al., 1990) ou m√™me celles obtenues suite √† une projection non lin√©aire telle qu‚Äôun
MDS (Sammon, 1969). Alors une grille r√©guli√®re est dessin√©e sur cette premi√®re projection
et chaque cellule de la grille correspond √† un facteur du mod√®le BATM : xi est affect√©e √† la
z
(0)
i -i√®me classe correspondant √† la cellule dans laquelle ses coordonn√©es (Xci , Y ci ) de pro-
jection tombent -sur le plan d‚Äôinitialisation-. Nous initialisons les probabilit√©s de m√©lange par
pi
(0)
ki ‚àù h(k, z
(0)
i ) pour une fonction de lissage telle que celle de voisinage des cartes de Koho-
nen, i.e. h(k, z(0)i ) ‚àù exp(‚àí||sk ‚àí sz(0)
i
||2/œÉ) pour œÉ bien choisi. Alors on pose :
a
(0)
jk =
‚àë
i pi
(0)
ki xij + Œ±‚àë
i pi
(0)
ki + IŒ±
,
o√π Œ± > 0, bien choisi, a pour r√¥le de r√©gulariser les param√®tres ajk qui correspondent √† des
cellules vides. Finalement, LP (0)J est la matrice de taille K √ó J dont les cellules ont pour
valeurs log[a(0)jk /(1‚àí a
(0)
jk )]. Cette matrice nous permet d‚Äô√©valuer une matriceW
(0)
J qui a pour
colonnes les param√®tres initiaux w(0)j des fonctions logistiques. La solution au probl√®me de
r√©gression associ√© s‚Äô√©crit alors :
W
(0)
J = [w
(0)
1 |w
(0)
2 | ¬∑ ¬∑ ¬∑ |w
(0)
J ] = (Œ¶
TŒ¶)‚àí1Œ¶TLP
(0)
J .
Nous construisons les centres en effectuant un pas de l‚Äôalgorithme non s√©quentiel de Koho-
nen pour les affectations √©valu√©es sur le plan de projection initiale : l‚Äôaffectation s‚Äôeffectue
en attribuant le label z(0)i du noeud le plus proche de (Xci , Y ci ) pour un treillis r√©gulier des-
sin√© sur le nuage bidimensionnel des projet√©s afin de discr√©tiser celui-ci. Cette approche doit
√©galement induire par ailleurs un niveau d‚Äôentropie plus √©lev√© de la classification initiale des
donn√©es -comparativement √† la simple classification dure sur le plan initial- et facilite ainsi une
convergence des param√®tres vers une solution encore meilleure.
3.2 Lissage des param√®tres sur les lignes
Il peut √™tre int√©ressant d‚Äôajouter une contrainte topologique sur les param√®tres partitionnant
les lignes afin d‚Äôacc√©l√©rer la convergence de l‚Äôalgorithme et am√©liorer la carte finale. Comme
une solution par un soft-max (Bishop, 1995) nous appara√Æt relativement lourde, nous proposons
une solution alternative en ajoutant un simple lissage par un terme de p√©nalisation issu de
l‚Äôapproche du TNEM (Priam, 2003). Bri√®vement, il s‚Äôagit de classer les vecteurs de donn√©es
avec un lissage spatial sur les composantes piki du mod√®le BATM, √† la mani√®re d‚Äôun champ de
Markov cach√© (Zhang, 1992; Celeux et al., 2003; Ambroise et Govaert, 1998). On pose :
QŒ≤(Œ∏|Œ∏
(t)) = Q(Œ∏|Œ∏(t)) +
Œ≤
2
‚àë
i
piTi Vpii,
- 449 - RNTI-E-6
Carte auto-organisatrice probabiliste sur donn√©es binaires
o√π pii est le vecteur de composantes piki, et V est -soit la matrice des fonctions de voisinage
de la carte auto-organisatrice, i.e. Vk` = h(k, `), -soit la matrice binaire d‚Äôadjacence du treillis
correspondant √† notre carte probabiliste, i.e. Vk` = 1 ssi le k-i√®me noeud est voisin du `-i√®me.
Le pas compl√©mentaire associ√© s‚Äô√©crit :
pi
(t+1)
ki =
‚àë
j P
(t)
k|i,j,xij
+ Œ≤ pi
(t+1)
ki
‚àë
` Vk` pi
(t+1)
`i
J + Œ≤ pi
(t+1)
i
T
Vpi
(t+1)
i
,
et se r√©sout en it√©rant l‚Äô√©galit√© et en r√©injectant dans le membre de droite les anciennes valeurs
courantes des pi(t+1)ki jusqu‚Äô√† ce que la stabilisation de leurs valeurs soit atteinte. Nous retrou-
vons √©videmment le pas d‚Äôestimation non contrainte en annulant Œ≤. Nous avons √©limin√© le
terme additif du TNEM qui porte sur les entropies des pii, donc nous obtenons un nouvel algo-
rithme appel√© TNEM2, plus g√©n√©ral que le TNEMoriginal puisqu‚Äôil s‚Äôapplique √† des probabili-
t√©s non forc√©ment a posteriori. Une alternative au TNEM est une estimation des piki param√©tr√©s
comme le GTM. La fonction √† optimiser s‚Äô√©crit alors QI(Œ∏|Œ∏(t)) =
‚àë
i,j,k P
(t)
k|i,j,xij
{
ŒæTk wi ‚àí
log
‚àë
` exp(Œæ
T
` wi)
}
o√π les wi sont les inconnues √† d√©terminer. Leur estimation s‚Äôeffectue
comme pr√©c√©demment, par une mont√©e de gradient en r√©alisant une boucle sur l‚Äôindice i des
lignes, de 1 √† I , et en calculant les gradientsQ(t)i et les matrices hessiennesH
(t)
i . Enfin, les pa-
ram√®tres w(0)i s‚Äôinitialisent √† l‚Äôaide d‚Äôune r√©gression sur la nouvelle matrice LP
(0)
I qui a pour
cellules les logarithmes des pi(0)ki . Nous proposons finalement le quatri√®me algorithme d‚Äôesti-
mation not√© IRLS+TNEM2 qui associe une maximisation sur les param√®tres des colonnes par
l‚ÄôIRLS √† un lissage des probabilit√©s des lignes par le TNEM2. Nous expliquons dans la suite
comment ce lissage se comporte en pratique.
3.3 Post-processing de la carte finale
La carte finale montre une grille de centres de classes bien organis√©es ; √† chacune on affecte
les donn√©es dont elle est le plus proche. Pour les cartes auto-organisatrices classiques on utilise
la distance euclidienne entre le vecteur centre et le vecteur donn√©e. Ici le mod√®le permet une
alternative probabiliste puisque nous avons la probabilit√© de g√©n√©ration d‚Äôune donn√©e par un
composant k du m√©lange. Donc chacun des vecteurs xi est affect√© √† un centre par un maximum
a posteriori (MAP), i.e. zÀÜi = argmaxk pÀÜiki. De la m√™memani√®re, la j-i√®me variable est affect√©e
au centre de label zÀÜj = argmaxk aÀÜjk. Le MAP aboutit aux positions bidimensionnelles pi =
szÀÜi et pj = szÀÜj pour les lignes et colonnes de la matrice projet√©e. Une seconde mani√®re de
projeter chaque donn√©e est par sa position moyenne (Bishop et al., 1998) au lieu du MAP
pr√©c√©dent, i.e. les positions pÀúi =
‚àë
k pÀÜikisk et pÀúj =
‚àë
k(aÀÜjk/
‚àë
` aÀÜj`)sk.
3.4 Exp√©riences
Nous exp√©rimentons notre mod√®le sur plusieurs √©chantillons de donn√©es pour valider notre
approche, par exemple, sur l‚Äô√©chantillon Zoo2, qui compte 101 animaux avec sept classes et
21 caract√©ristiques binaires. Notre m√©thode BATM converge vers une carte bien organis√©e o√π
2ftp ://ftp.ics.uci.edu/pub/machine-learning-databases/zoo/zoo.names
- 450 -RNTI-E-6
R. Priam et M. Nadif
l‚Äôon reconna√Æt les sept classes que l‚Äôalgorithme a projet√©es. La segmentation de la grille sur
la figure 1 s‚Äôeffectue √† l‚Äôaide d‚Äôune proc√©dure automatique consistant (Vesanto et Alhoniemi,
2000) en une classification ascendante hi√©rarchique avec agr√©gation par le diam√®tre (complete-
linkage) associ√©e √† une distance du œá2 sur la matrice des pÀÜiki, qui donne les meilleurs r√©sultats
en pratique. En remarque, la classe contenant les reptiles est peu homog√®ne d‚Äôapr√®s nos exp√©-
riences, car ces animaux se regroupent mal. L‚Äô√©volution de la log-vraisemblance par les quatre
algorithmes est pr√©sent√©e √† la figure 2 pour le tableau du Zoo, d√©montrant la sup√©riorit√© de
l‚Äôalgorithme IRLS comparativement √† des algorithmes plus r√©cents alternatifs. Cependant, un
surapprentissage peut amener √† une solution non suffisamment liss√©e, et nous pr√©f√©rons l‚Äôap-
proche IRLS+TNEM2 √† cause de son efficacit√© malgr√© sa vraisemblance moins √©lev√©e. Cette
valeur plus faible s‚Äôexplique par le terme de p√©nalisation qui aide √† une plus rapide et meilleure
auto-organisation des lignes comme v√©rifi√©e ici puisque cet algorithme s‚Äôarr√™te bien plus t√¥t
que les trois autres, pour un crit√®re d‚Äôarr√™t identique (log-vraisemblance relative inf√©rieure au
seuil 10e-5). Notre initialisation originale par une r√©gression ad√©quate se base sur le premier
plan principal d‚Äôune Analyse des Correspondances (AFC). Celle-ci s‚Äôillustre sur la figure 3,
d√©montrant l‚Äôint√©r√™t d‚Äôune carte auto-organisatrice : alors que la m√©thode factorielle lin√©aire
n‚Äôest pas capable de montrer les sept classes sur ce premier plan, notre carte par BATM extrait
ces sept classes et trouve leur lien statistique gr√¢ce √† la propri√©t√© de voisinage. Un ensemble
de donn√©es textuelles est √©galement projet√©. Cette base est un √©chantillon du fichier Classic3
(Dhillon et al., 2003) qui compte trois classes (MEDLINE, CISI, CRANFIELD). Nous avons
tir√© al√©atoirement (tirage √©quiprobable sans remise) 450 documents de ce fichier en prenant
150 documents dans chaque classe. Nous avons s√©lectionn√© les termes dont la fr√©quence to-
tale est sup√©rieure √† 30 sur le corpus entier et pour l‚Äôensemble du vocabulaire de 4303 termes.
Nous aboutissons, en √©liminant les textes vides, √† une matrice de taille al√©atoire : 450 par 170
environ. Nous montrons les positions moyennes des labels des documents correspondants et
projet√©s sur la figure 4 pour l‚Äôune de ces matrices. Nous sommes en mesure de visualiser assez
distinctement les trois classes s√©par√©es par notre projection non lin√©aire.
4 Conclusion et discussion
Nous avons pr√©sent√© une nouvelle m√©thode de carte auto-organisatrice -r√©capitul√©e sur la
figure 5- pour donn√©es binaires, comme on peut en trouver dans le domaine du traitement de
l‚Äôimage et du texte. De nouveaux r√©sultats pour l‚Äôinitialisation d‚Äôune m√©thode de projection
probabiliste de donn√©es qualitatives a √©galement √©t√© introduit.
Une perspective de nos travaux est la construction de biplots non lin√©aires par carte topolo-
gique. Nous travaillons actuellement √† la projection de matrices de taille plus importante ainsi
que sur des ensembles d‚Äôimages binaires qui donnent des r√©sultats encourageants. Ensuite des
variantes au mod√®le BATM se posent en rempla√ßant E(xij) par une hypoth√®se alternative, i.e.
un m√©lange de lois diff√©rent tel que par exemple E(xij) =
‚àë
k pikpii|kb
xij
jk (1 ‚àí bjk)
(1‚àíxij)
ou bien E(xij) =
‚àë
k pika
xij
ik (1 ‚àí aik)
(1‚àíxij)b
xij
jk (1 ‚àí bjk)
(1‚àíxij)
. Le mod√®le BATM s‚Äô√©tend
√©galement √† d‚Äôautres types de donn√©es comme il est propos√© dans le paragraphe suivant. L‚Äôesti-
mation peut encore √™tre am√©lior√©e en d√©terminant notamment le meilleur hyperparam√®treŒ≤. En
conclusion, le r√©cent mod√®le du Block Clustering (Govaert et Nadif, 2003, 2005) effectue une
classification simultan√©e des lignes et colonnes d‚Äôun tableau num√©rique en √©tendant le mod√®le
- 451 - RNTI-E-6
Carte auto-organisatrice probabiliste sur donn√©es binaires
stingray bass
catfish
chub
dogfish
herring
pike
piranha
tuna
seawasp carp dolphin seal
haddock porpoise
seahorse
sole
octopus clam pitviper sealion
starfish seasnake
slowworm
crayfish crab frog platypus mink
lobster scorpion frog
slug newt
worm toad
tuatara aardvark boar
bear cheetah
mole leopard
opossum lion
lynx
mongoose
polecat
puma
raccoon
wolf
flea tortoise hare antelope
gnat vole buffalo
honeybee deer
housefly elephant
ladybird giraffe
moth oryx
termite
wasp
crow kiwi ostrich gorilla cavy calf
duck penguin rhea squirrel goat
gull swan wallaby hamster
hawk pony
skimmer pussycat
skua reindeer
chicken fruitbat girl
dove vampire
flamingo
lark
parakeet
pheasant
sparrow
vulture
wren
FIG. 1 ‚Äì Une carte de taille 8√ó8, pour l‚Äô√©chantillon Zoo par BATM, segment√©e en sept macro-
classes repr√©sent√©es visuellement par sept niveaux de gris au niveau des cellules agr√©g√©es.
- 452 -RNTI-E-6
R. Priam et M. Nadif
0 100 200 300 400 500 600
‚àí1100
‚àí1000
‚àí900
‚àí800
‚àí700
‚àí600
‚àí500
‚àí400
‚àí300
‚àí200
IRLS
IRLS+TNEM2
Bohning
Variationnel
FIG. 2 ‚Äì Les courbes de la log-vraisemblance du BATM obtenues par les quatre algorithmes
pr√©sent√©s (IRLS, IRLS+TNEM2, Bohning (incompl√®te) et variationnel) pour les 101 animaux.
aardvark
antelope
bass
bearbo
buffalo
calf
carp
c tfish
cavy
ch etah
chicken
hub
clam
crab
crayfish
crow
deer
dogfis
dolphin
dove
duck
elephant
flamingo
flea
frog
frog
fruitbat
giraffe
girl
gnat
goat
gorilla
gull
haddock
hamster
re
hawk
herring
honeybeehousefly
kiwi
ladybird
lark
leopardi n
lobster
ynx
mink
molem ngoose
moth
newt
octopus
op sum
oryx
ostrich
parakeet
penguin
pheasant
pike piran a
pitviper
platypus
p lec t
p ny
p r oise
um
pussyc t
r cc n
rei deer
rhea
scorpion
seah rse
seal
sealion
seasnake seawasp
skimmerua
slowworm
slug
ole
sparrow
squirrel
starfish
stingray
swan
termite
toad
tortoise
tuatara
tuna
vampire
v l
vulture
wallaby
wasp
w lf
worm
w n
FIG. 3 ‚Äì Initialisation de la carte BATM pour les 101 animaux.
0
0
0
0
0 0
0
0
0
0 0
00
00
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 0
0
0
00
0
0
0
0
0
0 0
0
00
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
111
111
1
1
111
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
11
1
1 1 1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1 1
1
11
1
1
1
11
1
1
1
1
1
11
22
2
2
2
2
22
2
2
2
2
2
2
2
2
2 222
2
22
2
2
2
2
22 22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22 2
22 2 2
22
2
2
2
2 2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2 2
2
2
2 2
2
2 22
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
FIG. 4 ‚Äì Projections moyennes de l‚Äô√©chantillon des 450 documents dans Classic3.
- 453 - RNTI-E-6
Carte auto-organisatrice probabiliste sur donn√©es binaires
Initialisation de Œ∏(0) = ({a(0)
jk
}, {pi
(0)
ki
}) pour LP (0)I LP
(0)
J
Pas Estimation deQ(Œ∏|Œ∏(t))
‚áí √âvaluation des P (t)
k|i,j,xij
pour Œ∏(t) = ({a(t)
jk
}, {pi
(t)
ki
})
Pas Maximisation deQ(Œ∏|Œ∏(t))
‚áí √âvaluation GEM/IRLS ou variationnelle des param√®tres a(t+1)
jk
,
‚áí √âvaluation TNEM2 ou GEM des param√®tres pi(t+1)
ki
.
Post-processing final de la carte de param√®tres Œ∏ÀÜ
‚áí Tableau segment√© par CAH, projections moyennes, biplot.
FIG. 5 ‚Äì Sch√©ma r√©capitulatif de la m√©thode BATM.
de m√©lange classique √† un mod√®le de m√©lange crois√©. Celui-ci est un mod√®le g√©n√©ratif flexible
qui s‚Äôav√®re une alternative efficace et prometteuse au mod√®le √† aspects. Il serait int√©ressant de
l‚Äô√©tendre en lui ajoutant une propri√©t√© d‚Äôauto-organisation.
Annexe : param√©trisation probabiliste alternative au soft-max
Lorsque la matrice de donn√©es est un tableau de contingence, la loi de Bernoulli n‚Äôest
plus valable, et une hypoth√®se de loi multinomiale est g√©n√©ralement suppos√©e. Un param√©trage
soft-max est alors introduit pour le cas de probabilit√©s contraintes en r√©gression et classification
notamment. On √©crit dans notre cas pj|k = ew
T
j Œæk/
‚àë
j‚Ä≤ e
wT
j‚Ä≤
Œæk avec
‚àë
j pj|k = 1. Donc cette
param√©trisation implique l‚Äôinversion d‚Äôune matrice hessienne pleine pour proc√©der √† l‚Äôoptimi-
sation. Nous proposons un moyen alternatif plus efficace. L‚Äôid√©e principale est d‚Äôaboutir √† de
nouveaux param√®tres -sans la contrainte de somme √† l‚Äôunit√© classique pour une multinomiale-
en √©crivant pj|k comme une loi jointe de variables de loi de Bernoulli de param√®tres inconnus.
Il s‚Äôagit d‚Äô√©crire la loi jointe de la j-i√®me colonne associ√©e en mettant √† l‚Äôunit√© la compo-
sante qui nous int√©resse, et √† z√©ro les autres, puis en supposant des lois de Bernoulli sur l‚Äôen-
semble des composantes prises ind√©pendantes pour le vecteur binaire r√©sultant. L‚Äôexpression
pj|k = pjk
‚àè
j‚Ä≤ 6=j(1 ‚àí pj‚Ä≤k) ' pjk avec pjk ‚àà [0, 1] donne une solution valide au maximum
de vraisemblance d‚Äôune loi multinomiale, pour des probabilit√©s assez petites (√©ventuellement
par l‚Äôajout de composantes artificielles suppl√©mentaires pour diminuer les valeurs), i.e. p(t+1)jk
(optimum global) annule la d√©riv√©e -sans contrainte donc sans lagrangien- de :‚àë
i
‚àë
j
‚àë
k
p
(t)
kijxij log[pj|k] =
‚àë
i
‚àë
j
‚àë
k
p
(t)
kijxij log
[
pjk
‚àè
j‚Ä≤ 6=j
(1 ‚àí pj‚Ä≤k)
]
.
Nous retrouvons l‚Äôexpression classique de l‚Äôestimation des param√®tres de la loi multinomiale,
p
(t+1)
jk =
P
i
p
(t)
kij
xij
P
i
P
j‚Ä≤ p
(t)
kij‚Ä≤
xij‚Ä≤
, pour des probabilit√©s a posteriori p(t)kij et induisant leur normalisation
- 454 -RNTI-E-6
R. Priam et M. Nadif
automatique. Comme aucune contrainte ne devient n√©cessaire sur les pjk, la param√©trisation
par des sigmo√Ødes pjk = œÉ(ŒæTk wj) est licite pour une auto-organisation des valeurs recher-
ch√©es, sans param√®tre soft-max. Nous aboutissons √† une formulation IRLS (ou variationnelle)
tr√®s g√©n√©rale pour la loi multinomiale, d‚Äôo√π une nouvelle expression du vecteur gradient et de
la matrice hessienne pour l‚Äôestimation du mod√®le BATM sur donn√©es cat√©gorielles.
R√©f√©rences
Ambroise, C. et G. Govaert (1998). Convergenceof an em-type algorithm for spatial clustering.
Pattern Recogn. Lett. 19(10), 919‚Äì927.
Benz√©cri, J. P. (1992). Correspondence Analysis Handbook. New-York : Dekker.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Clarendon Press.
Bishop, C. M., M. Svens√©n, et C. K. I. Williams (1998). Developpements of generative topo-
graphic mapping. Neurocomputing 21, 203‚Äì224.
Bohning, D. (1993). Construction of reliable maximum likelihood algorithms with application
to logistic and cox regression. Handbook of Statistics 9, 409‚Äì422.
Celeux, G., F. Forbes, et N. Peyrard (2003). Em procedures using mean field-like approxima-
tions for markov model-based image segmentation. Pattern Recognition 36, 131‚Äì144.
Deerwester, S., S. T. Dumais, G. W. Furnas, T. Landauer, et R. Harshman (1990). Indexing
by latent semantic analysis. Journal of the American Society for Information Science 41(6),
391‚Äì407.
Dempster, A., N. Laird, et D. Rubin (1977). Maximum-likelihood from incomplete data via
the EM algorithm (with discussion). Journal of the Royal Statistical Society, Series B 39,
1‚Äì38.
Dhillon, I. S., S. Mallela, et D. S. Modha (2003). Information-theoretic co-clustering. In
Proceedings of The Ninth ACM SIGKDD InternationalConference on Knowledge Discovery
and Data Mining(KDD-2003), pp. 89‚Äì98.
Elemento, O. (1999). Initialisation, convergence, et validation de cartes topologiques de koho-
nen (in french). Master‚Äôs thesis, Rapport de DEA (INRIA, Yves Lechevallier).
Girolami, M. (2001). Document representation based on generative multivariate bernoulli
latent topics models. In U. of Cambridge (Ed.), BCS-IRSG 22nd Annual Colloquium on
Information Retrieval Research, pp. 194‚Äì201.
Govaert, G. et M. Nadif (2003). Clustering with block mixture models. Pattern Recogni-
tion 36(2), 463‚Äì473.
Govaert, G. et M. Nadif (2005). An EM algorithm for the block mixture model. IEEE Tran-
sactions on Pattern Analysis and Machine Intelligence 27(4), 643‚Äì647.
Hofmann, T. et J. Puzicha (1998). Statistical models for co-occurrence data. Technical Report
AIM-1625, MIT.
Jolliffe, I. (2002). Principal Component Analysis. Springer Verlag.
Kab√°n, A. et M. Girolami (2001). A combined latent class and trait model for analysis and
visualisation of discrete data. IEEE Transactions on Pattern Analysis and Machine Intelli-
- 455 - RNTI-E-6
Carte auto-organisatrice probabiliste sur donn√©es binaires
gence 23(8), 859‚Äì872.
Kohonen, T. (1997). Self-organizing maps. Springer.
Lebart, L., A. Morineau, et K. Warwick (1984). Multivariate Descriptive Statistical Analysis.
J. Wiley.
MacKay, D. J. C. (1992). Bayesian interpolation. Neural Computation 4(3), 415‚Äì447.
McCullagh, P. et J. Nelder (1983). Generalized linear models. London : Chapman and Hall.
McLachlan, G. J. et D. Peel (2000). Finite Mixture Models. New York : John Wiley and Sons.
Priam, R. (2003). M√©thodes de carte auto organisatrice par m√©lange de lois contraintes.
Application √† l‚Äôexploration dans les tableaux de contingence textuels (in french). Ph. D.
thesis, Universit√© de Rennes 1.
Sammon, J. (1969). A nonlinear mapping for data structure analysis. IEEE Transactions on
Computers 5(18C), 401‚Äì409.
Saul, L. K., T. Jaakkola, et M. I. Jordan (1996). Mean field theory for sigmoid belief networks.
Journal of Artificial Intelligence Research 4, 61‚Äì76.
Tipping, M. E. (1999). Probabilistic visualisation of high-dimensional binary data. Advances
in Neural Information Processing Systems, 592‚Äì598.
Vesanto, J. et E. Alhoniemi (2000). Clustering of the self-organizing map. IEEE Trans. on
Neural Networks 11(3), 586‚Äì600.
Zhang, J. (1992). The mean field theory in EM procedures for markov random fields. IEEE
Transactions on Signal Processing 10(40), 2570‚Äì2583.
Summary
The mixture models behave very well to cluster large samples of continuous or categorical
data. Adding a vicinity constraint permits them to project data like factorial methods but in
a nonlinear way. In this paper we present a new model called Bernoulli Aspect Topological
Mapping (BATM) : a generative self-organizing map to deal with binary data by an automatic
map smoothing and an original initialization.
- 456 -RNTI-E-6
