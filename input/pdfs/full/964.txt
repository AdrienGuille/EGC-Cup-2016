Segmentation d‚Äôimages par la Classification floue bas√©e
sur la programmation DC et DCA
Le Thi Hoai An‚àó, Le Hoai Minh‚àó,
Nguyen Trong Phuc‚àó, Pham Dinh Tao‚àó‚àó
‚àó LITA ‚Äì UFR MIM
Universit√© Paul Verlaine ‚Äì Metz
Ile du Saulcy
57045 Metz Cedex 01, France
{lethi,lehoai,nguyen}@univ-metz.fr
‚àó‚àó Equipe Mod√©lisation, Optimisation et Recherche Op√©rationnelle,
INSA de Rouen
Place Emile Blondel BP 08
76131 Mont-Saint-Aignan C√©dex
pham@insa-rouen.fr
R√©sum√©. Dans ce papier, nous proposons une nouvelle m√©thode de seg-
mentation d‚Äôimage via la classification floue, et bas√©e sur la programma-
tion DC (Difference of Convex functions) et DCA (DC Algorithm). En
exploitant un sch√©ma de DCA simple et robuste pour la classification
floue, nous proposons un nouveau mod√®le par l‚Äôintroduction des infor-
mations spatiales au mod√®le FCM standard. DCA appliqu√© au nouveau
mod√®le est de la m√™me forme que DCA pour le mod√®le FCM standard,
et avec les informations spatiales, notre algorithme am√©liore nettement la
segmentation des images bruit√©es. Pour trouver un bon point initial de
l‚Äôalgorithme, nous proposons une proc√©dure alternative de DCA et FCM.
Les simulations num√©riques sur plusieurs imageries m√©dicales, qui sont
en tr√®s haute r√©solution issue d‚Äôun signal, montrent l‚Äôefficacit√© de notre
approche par rapport aux m√©thodes standards sur le temps de calcul et
la qualit√© des solutions.
1 Introduction
La segmentation d‚Äôimage joue un r√¥le important dans une vari√©t√© d‚Äôapplications
telles que la m√©decine, la g√©ologie, la biom√©trie et la bureautique. Le premier champs
d‚Äôapplication est le traitement de l‚Äôimagerie m√©dicale qui est fond√© sur des images
de l‚Äôint√©rieur du corps d‚Äôun patient (√©chographies, radiographies,...). En effet, on peut
rep√©rer sur ces images la pr√©sence d‚Äôanomalies ce qui permet de d√©tecter certaines
maladies. Par exemple, la d√©tection de micro-calcifications dans une mammographie
peut relever la pr√©sence d‚Äôun cancer du sein. La g√©ologie applique des cartes dont les
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
couleurs peuvent repr√©senter la densit√© de population, le climat. En plus, le champs
d‚Äôapplications de la segmentation d‚Äôimage peut s‚Äô√©largir dans bien d‚Äôautres domaines
tels que : le suivi de forme dans des documents vid√©o, la d√©tection d‚Äôobjets stellaires
dans des images astronomiques, la d√©tection de fronts dans les images satellites pour
l‚Äôassimilation de donn√©es en m√©t√©ologie.
La segmentation d‚Äôimage est une √©tape primaire dans la plupart des applications
de la vision d‚Äôordinateur √† l‚Äôanalyse d‚Äôimage. C‚Äôest une des t√¢ches les plus impor-
tantes de la phase de pr√©-traitement. L‚Äôidentification d‚Äôobjets r√©els, de pseudo-objets
et d‚Äôombres, ou la recherche de tout √©l√©ment d‚Äôint√©r√™t pr√©sent dans l‚Äôimage, tous n√©-
cessitent une forme de segmentation. La segmentation se d√©finit comme un processus
qui consiste √† d√©couper une image en r√©gions connexes pr√©sentant une homog√©n√©it√©
selon un certain crit√®re, comme par exemple les crit√®res de texture et/ou de couleur.
Ce processus est connu par sa complexit√© en raison de la subjectivit√© de la d√©finition
de r√©gions connexes, et de l‚Äôobjectif qu‚Äôon veut atteindre de notre image segment√©e.
Par exemple, dans un mur de brique, doit-on consid√©rer que chacune des briques
forme une r√©gion autonome ?
Dans ce domaine, de nombreuses m√©thodes bas√©es sur diff√©rentes approches,
bas√©es sur le contour, la r√©gion ou la texture, ont √©t√© d√©velopp√©es au cours de ces
derni√®res ann√©es (Pal et Pal, 1993). On peut confirmer qu‚Äôaucune m√©thode ne semble
pr√©valoir sur les autres, chacune ayant son domaine de pr√©dilection. En absence de
m√©thode universelle, il est classique de retrouver les diff√©rentes approches classifi√©es
en quatre th√®mes : clustering, approches contours, approches r√©gions et m√©thodes hy-
brides. Rajapakse et al. (2004) a classifi√© les diff√©rentesm√©thodes en quatre cat√©gories :
‚Äì Les m√©thodes classiques telles que le seuillage, la croissance de r√©gion, la seg-
mentation bas√©e sur les contours. La premi√®re m√©thode consiste √† d√©terminer
le seuil √† appliquer √† l‚Äôimage : le seuillage permet de s√©lectionner les parties de
l‚Äôimage qui int√©ressent l‚Äôop√©rateur. La deuxi√®me m√©thode propose √† faire cro√Ætre
chaque r√©gion autour d‚Äôun pixel de d√©part dont l‚Äôagglom√©ration n‚Äôexploite au-
cune connaissance a priori de l‚Äôimage ou du bruit et la d√©cision d‚Äôint√©grer √† la
r√©gion un pixel voisin repose seulement sur un crit√®re d‚Äôhomog√©n√©it√© impos√©
√† la zone en croissance. Et la troisi√®me m√©thode s‚Äôint√©resse aux contours des
objets extraits de l‚Äôimage. La plupart du temps, ces contours sont morcel√©s et
peu pr√©cis, il faut donc utiliser des techniques de reconstruction de contours par
interpolation ou conna√Ætre a priori la forme de l‚Äôobjet recherch√©.
‚Äì Les m√©thodes statistiques telles que la segmentation bay√©sienne ou la segmen-
tation au sens du maximum de vraisemblance sont bas√©es sur les d√©veloppe-
ments des cha√Ænes de Markov. Dans Bouman et Liu (1998), on peut trouver un
algorithme de segmentation non supervis√© "Multiple Resolution Segmentation"
(MRS), formul√© dans le contexte bay√©sien. La m√©thode utilise un mod√®le AR-
2D causal (Gaussian Autoregressive) : l‚Äôimage observ√©e est consid√©r√©e comme le
m√©lange de champs al√©atoires statistiquement homog√®nes. Le champ des classes
est mod√©lis√© par un champ al√©atoire markovien. La segmentation optimale est
d√©finie au sens du Maximum a Posteriori (MAP) et estim√©e gr√¢ce √† un algo-
rithme de minimisation locale (Iterated Conditional Mode).
‚Äì Les m√©thodes de r√©seaux de neurones : Une des strat√©gies possibles pour la seg-
H. A. Le Thi et al.
mentation est celle de la classification de pixels. Quelques exemples de segmen-
tation d‚Äôimages color√©es par r√©seaux de neurones ont √©t√© publi√©s r√©cemment.
Dans Campadelli et al. (1997), les r√©seaux utilis√©s √©taient des r√©seaux de Hop-
field, configur√©s √† l‚Äôaide de l‚Äôhistogramme des couleurs. Dans Littman et Ritter
(1997), les auteurs utilisent les r√©seaux auto-organis√©s du type Kohonen pour la
segmentation.
‚Äì Les m√©thodes de clustering flou : ces techniques permettent d‚Äôobtenir une parti-
tion floue de l‚Äôimage en donnant √† chaque pixel de l‚Äôimage un degr√© d‚Äôapparte-
nance √† une r√©gion donn√©e.
Un inconv√©nient du mod√®le standard de FCM dans la segmentation d‚Äôimage est
de ne pas tenir compte de l‚Äôinformation spatiale qui est une relation entre le pixel et
ses voisinages. Pourtant, cette information rend l‚Äôalgorithme tr√®s sensible au bruit et
√† d‚Äôautres objets fa√ßonn√©s dans l‚Äôimage. En fait, cette relation est une des caract√©ris-
tiques importantes d‚Äôune image car les voisinages poss√®dent souvent les valeurs sem-
blables, et la probabilit√© qu‚Äôils appartiennent √† la m√™me partition est tr√®s √©lev√©e. Par
ailleurs, si nous consid√©rons une image bruit√©e, le FCM n‚Äôest pas une m√©thode adap-
t√©e pour surmonter ce probl√®me. R√©cemment, de nombreux chercheurs ont ajout√©
l‚Äôinformation spatiale √† l‚Äôalgorithme original de FCM pour am√©liorer l‚Äôefficacit√© de
la segmentation d‚Äôimage (Pham, 2002; Zhang et Chen, 2004; Hung et al., 2006).
Lemod√®le de la classification dans la segmentation d‚Äôimage est, en g√©n√©ral, un pro-
bl√®me de tr√®s grande dimension pour lequel, la recherche des m√©thodes efficaces est
toujours d‚Äôactualit√©. Le but de notre travail est double. Premi√®rement, nous proposons
une nouvelle m√©thode de segmentation d‚Äôimage via le mod√®le de FCM bas√©e sur la
programmation DC et DCA. Deuxi√®mement, pour la segmentation d‚Äôimage bruit√©e,
nous consid√©rons un mod√®le adaptatif de FCM (appel√© FCM-Spatial) qui incorpore
l‚Äôinformation spatiale √† la fonction de clustering.
La programmation DC et DCA ont √©t√© appliqu√©es avec succ√®s √† de nombreux pro-
bl√®mes d‚Äôoptimisation non convexe diff√©rentiable ou non de grande dimension dans
diff√©rents domaines des sciences appliqu√©es, en particulier aux probl√®mes du data
mining (voir par exemple LeThi et al. (2006, 2005); Liu et al. (2003); Neumann et al.
(2004); Weber et al. (2005)). Nous √©tudions dans ce travail, en premier lieu, un sch√©ma
DCA appliqu√© au mod√®le FCM qui est simple et efficace. Nous utilisons ensuite cet al-
gorithme pour la r√©solution du nouveau mod√®le FCM-Spatial qui est de m√™me forme
que FCM standard (mais le nombre de variables est double). Pour calculer le bon point
initial et acc√©l√©rer la convergence de DCA, nous proposons une proc√©dure alternative
de FCM-DCA qui combine le DCA avec l‚Äôalgorithme classique de FCM. Les r√©sultats
exp√©rimentaux sur plusieurs images bruit√©es ont illustr√© l‚Äôefficacit√© de l‚Äôalgorithme
propos√© et sa sup√©riorit√© par rapport √† l‚Äôalgorithme standard de FCM sur le temps de
calculs et la qualit√© des solutions. Par ailleurs, avec le mod√®le spatial, notre algorithme
r√©duit consid√©rablement l‚Äôeffet du bruit.
Le papier est organis√© de la fa√ßon suivante. Dans la deuxi√®me section, nous pr√©-
sentons la formulation du probl√®me FCM. La r√©solution de ce probm√®me par la pro-
grammation DC et DCA est √©tudi√©e dans la troisi√®me section. Enfin, la segmentation
d‚Äôimage par la classification floue est pr√©sent√©e dans la derni√®re section.
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
2 Fomulation du probl√®me FCM
Soit X := {x1, x2, ..., xn} l‚Äôensemble de n points √† classer. Chaque point xi est un
vecteur dans l‚Äôespace IRp. Nous avons √† classer ces n points dans c (2 ‚â§ c ‚â§ n) classes
diff√©rentes.
Consid√©rons une matrice de pourcentages U de taille (c√ó n) dont chaque √©l√©ment
ui,k d√©finit le pourcentage d‚Äôappartenance d‚Äôun point xk √† la classe Ci. Il est clair que
ui,k ‚àà [0, 1] pour i = 1...c, k = 1...n ;
c‚àë
i=1
ui,k = 1, pour k = 1...n. (1)
Si la matrice de pourcentages U est d√©termin√©e, on en d√©duit la classification selon
la r√®gle suivante : le point xk (pour k = 1, . . . , n) est class√© dans la classe Ci (pour
i = 1, . . . , c) si et seulement si
ui,k = max{uj,k : j ‚àà {1, . . . , c}}.
Consid√©rons la fonction Jm d√©finie par :
Jm(U, V ) =
n‚àë
k=1
c‚àë
i=1
umi,k||xk ‚àí vi||
2, (2)
o√π V est une (c √ó p) - matrice dont chaque ligne vi correspond au centre de la classe
Ci.m ‚â• 2 est un param√®tre entier qui d√©finit le degr√© de flou du mod√®le.
La t√¢che de chercher une classification revient ainsi √† celle de trouver la matrice de
pourcentages U et les centres vi. Le mod√®le math√©matique de FCM s‚Äô√©crit ainsi :Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥
min Jm(U, V )
s.t ui,k ‚àà [0, 1] for i = 1, . . . , c, k = 1, . . . , n
c‚àë
i=1
ui,k = 1, k = 1, . . . , n.
(3)
Soient xk,j la j e`me composante, j = 1, ..., p, du vecteur xk et
Œ±j := min
k=1,...,n
xk,j , Œ≤i := max
k=1,...,n
xk,j .
Il est clair que vi ‚àà Œ†
p
j=1 [Œ±j , Œ≤j ] .
Pour chaque k ‚àà {1, ..., n}, soit ‚àÜk le (c‚àí 1)-simplexe dans IRc d√©fini par
‚àÜk :=
{
U (k) := (ui,k)i ‚àà IR
c
+ :
c‚àë
i=1
ui,k = 1
}
.
Le probl√®me FCM est reformul√© comme :{
min Jm(U, V )
s.t U ‚àà ‚àÜ := Œ†nk=1 ‚àÜk, V ‚àà T := Œ†
p
j=1 [Œ±j , Œ≤j]√ó ...√óŒ†
p
j=1 [Œ±j , Œ≤j ]
. (4)
H. A. Le Thi et al.
L‚Äôalgorithme FCM propos√© par Bezdek (1981) se d√©crit comme suit :
Algorithme 1 : FCM
Initialisation :
‚Äì Choisir le nombre de classes c.
‚Äì Initialise la matrice des pourcentages U ansi que les centres vi al√©atoirement.
R√©peter
‚Äì Calculer les centres vi suivant l‚Äô√©quation :
vi =
n‚àë
k=1
umikxk/
n‚àë
k=1
umik ‚àÄi = 1, 2, ..., c.
‚Äì Calculer la matrice des pourcentages U :
uik =
[
c‚àë
k=1
‚Äñxk ‚àí vi‚Äñ2/(m‚àí1)
‚Äñxk ‚àí vj‚Äñ2/(m‚àí1)
]‚àí1
‚àÄi = 1, 2, . . . , c et ‚àÄk = 1, 2, . . . , n.
Jusqu‚Äô√† [la modification de solution (U, V ) entre les deux it√©rations successives
est suffisament petite].
On voit que FCM est un probl√®me d‚Äôoptimisation non convexe dont la r√©solution
sera d√©taill√©e dans la suite.
3 La programmation DC et DCA pour la r√©solution de
FCM
Pour faciliter la compr√©hension de notre m√©thode, nous pr√©sentons, en premier
lieu de cette section, une br√®ve description de la programmation DC et DCA.
3.1 Introduction √† la programmation DC et DCA
La programmation DC joue un r√¥le central en programmation non convexe (dif-
f√©rentiable ou non) car la quasi totalit√© des probl√®mes d‚Äôoptimisation de la vie cou-
rante est de nature DC. Elle conna√Æt des d√©veloppements spectaculaires au cours de
cette derni√®re d√©cennie. DCA est une m√©thode de descente (de type primal-dual sans
recherche lin√©aire) pour la r√©solution d‚Äôun programme DC de la forme
Œ± := inf{f(x) := g(x)‚àí h(x) : x ‚àà IRp}, (5)
o√π g, h sont les fonctions convexes semi-continues inf√©rieurement et propres sur IRp.
Une telle fonction f est appel√©e fonction DC, et les fonctions convexes g et h les com-
posantes DC de f . Il est √† noter que la minimisation d‚Äôune fonction DC sur un en-
semble convexe ferm√© C de IRp se ram√®ne √† un probl√®me de type (5) car la contrainte
x ‚àà C peut √™tre incorpor√©e dans la fonction objective √† l‚Äôaide de la fonction indicatrice
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
œáC d√©finie par œáC = 0 si x ‚àà C,+‚àû sinon. Lorsqu‚Äôune de ses composantes DC est po-
ly√©drale la fonction f est dite DC poly√©drale et le programme DC correspondant DC
poly√©dral. La programmation DC poly√©drale joue un r√¥le crucial en programmation
non convexe.
La congugaison d‚Äôune fonction convexe g, not√©e g‚àó est d√©finie par
g‚àó(y) := sup{„Äàx, y„Äâ ‚àí g(x) : x ‚àà IRp}.
La dualit√© DC est d√©finie via la conjugaison des composantes DC et le programme
dual de (5) est donn√© par (ici l‚Äôespace dual de IRp est identifi√© √† lui-m√™me) :
Œ±D := inf{h
‚àó(y)‚àí g‚àó(y) : y ‚àà IRp}. (6)
Puisque chaque fonction h ‚àà Œì0(IRp) est caract√©ris√©e comme le supremum d‚Äôune
famille finie des fonctions affines, c.√†.d.
h(x) := sup{„Äàx, y„Äâ ‚àí h‚àó(y) : y ‚àà IRp},
on a
Œ± = inf{g(x)‚àí sup{„Äàx, y„Äâ ‚àí h‚àó(y) : y ‚àà IRp} : x ‚àà IRp} = inf{Œ±(y) : y ‚àà IRp},
o√π
Œ±(y) := inf{g(x)‚àí [„Äàx, y„Äâ ‚àí h‚àó(y)] : x ‚àà IRp} (Py).
Il est clair que (Py) est un programme convexe et
Œ±(y) = h‚àó(y)‚àí g‚àó(y) si y ‚àà dom h‚àó, et +‚àû sinon. (7)
Par suite
Œ± = inf{h‚àó(y)‚àí g‚àó(y) : y ‚àà dom h‚àó}.
Finallement on obtient, avec la convention naturelle +‚àû‚àí (+‚àû) = +‚àû :
Œ± = Œ±D := inf{h
‚àó(y)‚àí g‚àó(y) : y ‚àà IRp}.
On observe ainsi la sym√©trie parfaite entre les programmes DC primal et dual : le dual
de (6) est exactement (5).
Le transport des solutions optimales globales entre l‚Äôensemble des solutions op-
timales P de (5) et celui de (6) not√© D s‚Äôexprime de la mani√®re suivante (LeThi et
PhamDinh, 2005; PhamDinh et LeThi, 1997) :
‚à™{‚àÇh(x‚àó) : x‚àó ‚àà P} ‚äÇ D et ‚à™ {‚àÇg‚àó(y‚àó) : y‚àó ‚àà D} ‚äÇ P. (8)
La relation (8) indique que la r√©solution d‚Äôun programme DC implique celle de son
dual. D‚Äôautre part, ce transport reste valable entre les ensembles des solutions locales
de (5) et (6) sous certaines hypoth√®ses techniques.
En analyse convexe,
‚àÇh(x0) := {y ‚àà IRp : h(x) ‚â• h(x0) + „Äàx ‚àí x0, y„Äâ, ‚àÄx ‚àà IRp}
H. A. Le Thi et al.
est appel√© le sous-diff√©rentiel de h au point x0. Tout √©l√©ment de ‚àÇh(x0) est appel√©
gradient de h en x0. Le sous-diff√©rentiel ‚àÇh(x0) est une partie convexe ferm√©e qui
coincide avec le gradient ‚àáh(x0) si et seulement h est diff√©rentiable en x0. Pour un
 > 0, le ‚àí sous-diff√©rentiel de h est d√©fini par
‚àÇh(x
0) := {y ‚àà IRp : h(x) ‚â• h(x0) + „Äàx‚àí x0, y„Äâ ‚àí , ‚àÄx ‚àà IRp}.
L‚Äô√©galit√© des valeurs optimales des programmes primal et dual (5) et (6) peut √™tre
traduite de mani√®re √©quivalente par
P = {x‚àó : ‚àÇh(x
‚àó) ‚äÇ ‚àÇg(x
‚àó), ‚àÄ > 0} .
Mais sauf des cas tr√®s rares, cette condition d‚Äôoptimalit√© globale est impraticable.
Nous nous int√©ressons d√®s lors aux conditions d‚Äôoptimalit√© locale pour les
programmes DC (voir LeThi (1997); LeThi et PhamDinh (2005); PhamDinh et LeThi
(1997, 1998) et r√©f√©rences inclues) :
‚àÇh(x‚àó) ‚äÇ ‚àÇg(x‚àó), (9)
et
‚àÇh(x‚àó) ‚à© ‚àÇg(x‚àó) 6= ‚àÖ. (10)
(Un tel point x‚àó v√©rifiant (10) est appel√© point critique de g ‚àí h).
La condition n√©cessaire d‚Äôoptimalit√© locale (9) est √©galement suffisante dans
plusieurs cas rencontr√©s en pratique - par exemple, quand la fonction objectif f :=
g ‚àí h est DC poly√©drale avec h poly√©drale, ou quand f est localement convexe en x‚àó.
Bas√© sur les conditions d‚Äôoptimalit√© locale et la dualit√© DC, DCA consiste en la
construction de deux suites {xk} et {yk}, candidates respectives aux solutions des pro-
bl√®mes primal et dual que l‚Äôon am√©liore √† chaque it√©ration (les deux suites {g(xk) ‚àí
h(xk)} et {h‚àó(yk) ‚àí g‚àó(yk)} sont d√©croissantes) et qui convergent vers des solutions
primale et duale x‚àó et y‚àó v√©rifiant des conditions d‚Äôoptimalit√© locale. Le sch√©ma g√©n√©-
ral de DCA prend la forme :
yk ‚àà ‚àÇh(xk); xk+1 ‚àà ‚àÇg‚àó(yk). (11)
La premi√®re interpr√©tation de DCA est simple : √† chaque it√©ration on remplace dans
le programme DC primal la deuxi√®me composante DC h par sa minorante affine
hk(x) := h(x
k) + „Äàx‚àí xk, yk„Äâ au voisinage de xk pour obtenir le programme convexe
suivant
inf{g(x)‚àí hk(x) : x ‚àà IR
p} (12)
dont l‚Äôensemble des solutions optimales n‚Äôest autre que ‚àÇg‚àó(yk).
De mani√®re analogue, la deuxi√®me composante DC g‚àó du programme DC dual (6)
est remplac√©e par sa minorante affine (g‚àó)k(y) := g‚àó(yk) + „Äày‚àí yk, xk+1„Äâ au voisinage
de yk pour donner naissance au programme convexe
inf{h‚àó(y)‚àí (g‚àó)k(y) : y ‚àà IR
p} (13)
dont ‚àÇh(xk+1) est l‚Äôensemble des solutions optimales. DCA op√®re ainsi une double
lin√©arisation √† l‚Äôaide des sous-gradients de h et g‚àó. Il est √† noter que DCA travaille
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
avec les composantes DC g et h et non pas avec la fonction f elle-m√™me. Chaque
d√©composition DC de f donne naissance √† un DCA. Pour un programme DC donn√©,
la question de d√©composition DC optimale reste ouverte, en pratique on cherche des
d√©compositions DC bien adapt√©es √† la structure sp√©cifique du programme DC √©tudi√©
pour lesquelles les suites {xk} et {yk} sont faciles √† calculer, (si possible) explicites
pour que les DCA correspondants soient moins co√ªteux en temps et par cons√©quent
capables de supporter de tr√®s grandes dimensions.
Soient C (resp. D) l‚Äôensemble convexe qui contient la suite {xk} (resp. {yk}) et
œÅ(g, C) (ou œÅ(g) si C = IRp) d√©fini par
œÅ(g, C) = sup
{
œÅ ‚â• 0 : g ‚àí
œÅ
2
‚Äñ ¬∑ ‚Äñ2 soit convexe sur C
}
.
DCA est unem√©thode de descente sans recherche lin√©aire, qui poss√®de les propri√©-
t√©s suivantes :
i) Les suites {g(xk)‚àí h(xk)} et {h‚àó(yk)‚àí g‚àó(yk)} sont d√©croisantes et
‚Ä¢ g(xk+1)‚àí h(xk+1) = g(xk)‚àí h(xk) ssi
yk ‚àà ‚àÇg(xk)‚à©‚àÇh(xk), yk ‚àà ‚àÇg(xk+1)‚à©‚àÇh(xk+1) et [œÅ(g, C)+œÅ(h,C)]‚Äñxk+1‚àíxk‚Äñ =
0. De plus, si g ou h est strictement convexe sur C alors xk = xk+1.
Dans ce cas DCA se termine √† l‚Äôit√©ration k (convergence finie de DCA).
‚Ä¢ h‚àó(yk+1) ‚àí g‚àó(yk+1) = h‚àó(yk) ‚àí g‚àó(yk) ssi xk+1 ‚àà ‚àÇg‚àó(yk) ‚à© ‚àÇh‚àó(yk), xk+1 ‚àà
‚àÇg‚àó(yk+1) ‚à© ‚àÇh‚àó(yk+1) et [œÅ(g‚àó, D) + œÅ(h‚àó, D)]‚Äñyk+1 ‚àí yk‚Äñ = 0. De plus, si g‚àó
ou h‚àó est strictement convexe sur D alors yk+1 = yk.
Dans ce cas DCA se termine √† l‚Äôit√©ration k (convergence finie de DCA).
ii) Si œÅ(g, C) + œÅ(h,C) > 0 (resp. œÅ(g‚àó, D) + œÅ(h‚àó, D) > 0)) alors la s√©rie {‚Äñxk+1 ‚àí xk‚Äñ2
(resp. {‚Äñyk+1 ‚àí yk‚Äñ2} converge.
iii) Si la valeur optimale Œ± du probl√®me (5) est finie et deux suites {xk} et {yk} sont
born√©es alors toute valeur d‚Äôadh√©rence xÀú (resp. yÀú) de la suite {xk} (resp. {yk}) est le
point critique de g ‚àí h (resp. h‚àó ‚àí g‚àó).
iv) DCA a la convergence lin√©aire pour les programmes DC g√©n√©raux.
v) DCA a la convergence finie pour les programmes DC poly√©draux.
Pour une √©tude compl√®te de la programmation DC et DCA, se r√©f√©rer √† LeThi
(1997); LeThi et PhamDinh (2005); PhamDinh et LeThi (1997, 1998) et r√©f√©rences in-
cluses. Il est √† noter que la recherche d‚Äôune d√©composition DC ad√©quate et celle d‚Äôun
bon point initial sont deux t√¢ches importantes dans la r√©solution d‚Äôun programme
non convexe par DCA car elles conditionnent la r√©ussite du r√©sultant DCA.
3.2 R√©solution du probl√®me FCM
Dans toute la suite nous utilisons la pr√©sentation matricielle qui nous semble plus
commode, sachant que l‚Äôon peut identifier une matrice et un vecteur (par ligne ou par
colonne).
Nous cherchons tout d‚Äôabord une d√©composition DC de la fonction objectif de (4). En
appliquant la formule :
2f1f2 = (f1 + f2)
2 ‚àí (f21 + f
2
2 )
H. A. Le Thi et al.
nous obtenons :
Jm(U, V ) =
n‚àë
k=1
c‚àë
i=1
umi,k‚Äñxk ‚àí vi‚Äñ
2
= 12
n‚àë
k=1
c‚àë
i=1
(umi,k + ‚Äñxk ‚àí vi‚Äñ
2)2 ‚àí 12 ((u
2m
i,k + ‚Äñxk ‚àí vi‚Äñ
4)
= G(U, V )‚àíH(U, V ),
avec
G(U, V ) = 12
n‚àë
k=1
c‚àë
i=1
(umi,k + ‚Äñxk ‚àí vi‚Äñ
2)2 + œáK(U, V );
H(U, V ) = 12
n‚àë
k=1
c‚àë
i=1
((u2mi,k + ‚Äñxk ‚àí vi‚Äñ
4).
En tenant compte du fait que la fonction Œ∏(x) = f(x)p est convexe pour p ‚â• 1 si f est
convexe non n√©gative, on d√©montre facilement queG(U, V ) etH(U, V ) sont convexes,
par suite Jm est une fonction DC.
La r√©solution de FCM par DCA revient aux calculs de sous-diff√©rentiels deH et de
G‚àó.
Calcul de (Y l, Zl) ‚àà ‚àÇH(U l, V l)
En utilisant les r√®gles de base de calcul de sous-gradient, nous avons :
‚àÇH(U, V ) = (‚àÇUH(U, V ), ‚àÇVH(U, V ))
= (m ¬∑ Uexp(2m‚àí 1), 2
n‚àë
k=1
(‚Äñxk ‚àí vi‚Äñ2(vi ‚àí xk))i=1..c)
(14)
(Uexp(2m ‚àí 1) repr√©sente la matrice dont chaque √©l√©ment est (Uexp(2m ‚àí 1))i,k :=
u2m‚àí1i,k , i = 1, . . . , c, k = 1, . . . , n).
Calcul de (U l+1, V l+1) ‚àà ‚àÇG‚àó(Y l, Zl)
Rappelons que (U l+1, V l+1) ‚àà ‚àÇG‚àó(Y l, Zl) si et seulement si (U l+1, V l+1) est une
solution du probl√®me convexe suivant :
min
{
G(U, V )‚àí
‚å©
(U, V ), (Y l, Zl)
‚å™
: (U, V ) ‚àà ‚àÜ√ó T
}
(15)
qui est solvable par n‚Äôimporte quel algorithme pour la programmation convexe. Dans
l‚Äôimpl√©mentation de notre algorithme nous utilisons la m√©thode de Gradient Projet√©
(Polyak, 1987), vu que la projection d‚Äôun point sur un simplexe ou sur un rectangle est
explicitement d√©termin√©e.
La m√©thode de Gradient Projet√© pour la r√©solution de (15) se d√©crit comme suit :
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
Algorithme GRP (pour r√©soudre (15))
Algorithme 2 : GRP
1. Soient r = 1, (U r, V r) := (U l, V l), une suite {Œªr} telle que :
limr‚Üí+‚àû Œªr = 0,
‚àë+‚àû
r=1 Œªr = +‚àû.
2. Calculer (U r+1, V r+1) de la mani√®re suivante (Proj denote la projection)) :
la kie`me colonne de la matrice U r+1 est
U r+1
(k)
:= Proj‚àÜk
(
U r
(k)
‚àí Œªr
Œ∏r
‚ÄñŒ∏r‚Äñ
)
,
o√π Œ∏r est la kie`me colonne du‚àáUG(U r, V r)‚àí Y l, et la iie`me ligne de
la matrice V r+1 est (T := Œ†pj=1 [Œ±j , Œ≤j])
V r+1i := ProjT
(
V ri ‚àí Œªr
[
‚àáVG(U r, V r)‚àí Z l
]
i
‚Äñ [‚àáVG(U r, V r)‚àí Z l]i ‚Äñ
)
.
3. Si ‚Äñ(U r+1, V r+1)‚àí (U r, V r)‚Äñ ‚â§  alors arr√™ter,
sinon remplacer r par r + 1 et aller √† l‚Äô√©tape 2.
Finalement nous pouvons d√©crire le sch√©ma DCA pour la r√©solution de (4) comme
suivant :
Algorithme 3 : Fuzzy-DCA
Initialisation :
‚Äì Choisir U0 ‚àà IRc.n et V 0 ‚àà IRc.p ;
‚Äì Choisir une tol√©rance  > 0.
R√©peter l = 0, 1, 2, ...
‚Äì Calculer (Y l, Zl) ‚àà ‚àÇH(U l, V l) √† l‚Äôaide de (14).
‚Äì Calculer (U l+1, V l+1) ‚àà ‚àÇG‚àó(Y l, Zl) en r√©solvant le probl√®me (14) par
l‚ÄôAlgorithme GRP.
Jusqu‚Äô√† ‚Äñ(U l+1, V l+1)‚àí (U l, V l)‚Äñ ‚â§ .
3.3 La recherche d‚Äôun bon point initial pour DCA par une proc√©dure
alternative FCM-DCA
La recherche d‚Äôun bon point initial joue un r√¥le crucial dans la r√©solution d‚Äôune
programmation DC par DCA. Elle d√©pend de la structure du probl√®me consid√©r√©
et elle peut √™tre effectu√©e par, par exemple, une m√©thode heuristique bien adapt√©e
au probl√®me. D‚Äôune mani√®re g√©n√©rale, un bon point initial pour le DCA ne doit pas
√™tre un minimum local, parce qu‚Äô√† partir d‚Äôun tel point, le DCA est stationnaire. En
plus, nous observons qu‚Äô√† partir de n‚Äôimporte quel point n‚Äô√©tant pas minimum lo-
cal, la fonction objective diminue rapidement durant quelques premi√®res it√©rations
H. A. Le Thi et al.
de DCA. Nous avons la m√™me remarque pour l‚Äôalgorithme standard FCM. C‚Äôest pour-
quoi, nous proposons une proc√©dure alternative de FCM-DCA pour le probl√®me (4).
Algorithme 4 : FCM-DCA
Initialisation : Initialise la matrice de partition Uo ainsi que les centres V o
al√©atoirement. Soitmaxiter un nombre entier positif.
R√©peter
‚Äì Une it√©ration de FCM : Calculer les centres V l = (vl1, v
l
2, ..., v
l
c) par la formule
vli =
n‚àë
k=1
umikxk/
n‚àë
k=1
umik ‚àÄi = 1, ..., c.
Calculer U l par
ulik =
[
c‚àë
k=1
‚Äñxk ‚àí vi‚Äñ2/(m‚àí1)
‚Äñxk ‚àí vj‚Äñ2/(m‚àí1)
]‚àí1
.
‚Äì Une it√©ration de DCA : ex√©cuter une it√©ration de Fuzzy-DCA avec le point itinial
(U l, Zl) pour obtenir (U l+1, V l+1).
‚Äì l‚Üê l + 1
Jusqu‚Äô√† l = maxiter.
Si nous utilisons l‚Äôalgorithme combin√© de FCM-DCA jusqu‚Äô√† sa convergence, peut-
√™tre l‚Äôefficacit√© de DCA n‚Äôest pas bien exploit√©e. Pour rem√©dier √† cette situation, nous
proposons un algorithme en deux phases. Dans la premi√®re phase, nous ex√©cutons
quelques it√©rations de l‚Äôalgorithme combin√© de FCM-DCA pour trouver un bon point
initial. Et dans la deuxi√®me, √† partir du point trouv√©, nous appliquons DCA jusqu‚Äô√†
sa convergence. Comme on verra dans la suite, parmi diff√©rentes versions de DCA,
cet algorithme en deux phases est la meilleure option.
4 Segmentation des images par la classification floue via
DCA
Nous pr√©sentons dans cette section une application de notre algorithme Fuzzy-
DCA √† la segmentation d‚Äôimage.
4.1 Mod√®le de FCM avec l‚Äôinformation spatiale
Dans la segmentation d‚Äôimage par le mod√®le standard de FCM, chaque pixel xk ‚àà
R
p repr√©sente les donn√©es multispectrales. Cependant, comme mentionn√© pr√©c√©dem-
ment, une des caract√©ristiques importantes d‚Äôune image est que les voisinages du
pixel poss√®dent les valeurs semblables, le rapport spatial est donc int√©ressant pour
la segmentation d‚Äôimage. L‚Äôinformation spatiale est la relation entre le pixel et ses voi-
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
sinages. Il y a diff√©rentes mani√®res d‚Äôincorporer l‚Äôinformation spatiale (voir la figure
1).
Dans notre cadre, nous consid√©rons l‚Äôinformation spatiale de xk comme une va-
leur moyenne de ses voisinages 3 √ó 3, et chaque point xk dans (4) a deux groupes de
valeurs : les valeurs du pixel et les valeurs moyennes de ses voisinages 3√ó 3.
SoitNk les voisinages 3√ó 3 du pixel xk . Les donn√©es entr√©es xk dans notre mod√®le
spatial de FCM sont xk = (xk1, xk2) o√π xk1 repr√©sente les valeurs du pixel de kth de
l‚Äôimage et xk2 = (xk1+
‚àë
i‚ààNk
xi1)/9. D‚Äôo√π, le nombre de variables U n‚Äôest pas chang√© et
V devient une matrice de c√ó 2p dont la ieme ligne est, vi ‚àà R2p, le centre de la r√©gion
Ci.
Le mod√®le spatial de FCM dans notre approche n‚Äôest pas tellement diff√©rent par
rapport √† son mod√®le standard FCM (4) sauf le fait que chaque xk ‚àà Rd est remplac√©
par xk = (xk1, xk2) ‚àà R2p. Par cons√©quent, n‚Äôimporte quel algorithme pour le mod√®le
standard de FCM (4) peut √™tre appliqu√© au mod√®le spatial de FCM. Au point de vue
num√©rique, le probl√®me spatial de FCM est plus difficile car le nombre de variables V
a doubl√©.
FIG. 1 ‚Äì Le pixel et ses 4 voisinages
4.2 Exp√©riences num√©riques
Nous testons nos algorithmes sur deux types d‚Äôimages : une image originale o√π
les contours et les r√©gions sont parfaitement localis√©s, et une image bruit√©e avec un
bruit gaussien. Tous les tests ont √©t√© r√©alis√©s sur un ordinateur de Pentium[R] 4 CPU
3.00GHz 1.00Go RAM.
Dans la premi√®re exp√©rimentation nous comparons la performance de l‚Äôalgorithme
de FCM (Bezdek, 1981) (Algorithme 1) et nos deux algorithmes : Algorithme 3 avec
la proc√©dure de recherche d‚Äôun point initial par Algorithme 4 (not√© Algorithme 4-
3) et Algorithme 4 (proc√©dure alternative FCM-DCA) sur les mod√®les sans ou avec
l‚Äôinformation spatiale. Les r√©sultats sont report√©s dans les figures 2.
Dans la deuxi√®me exp√©rimentation nous comparons la performance de FCM (Al-
gorithme 1 et les deux variants de DCA :Algorithme 3 sans la proc√©dure de recherche
d‚Äôun point initial et Algorithme 4-3. Nous utilisons les m√™mes param√®tres initiaux
pour tous les algorithmes. Les r√©sultats sont pr√©sent√©s dans le tableau 1. Nous utili-
sons les notations suivantes :
H. A. Le Thi et al.
‚Äì Taille : la taille de l‚Äôimage.
‚Äì c : le nombre de r√©gions de l‚Äôimage.
‚Äì iter : le nombre d‚Äôit√©rations de l‚Äôalgorithme.
‚Äì Temps : le temps de calcul de l‚Äôalgorithme en secondes.
4.3 Commentaires
√Ä partir des r√©sultats exp√©rimentaux, nous constatons que :
‚Äì Dans plusieurs images, notre algorithme donne une segmentation presque par-
faite. En plus, sans information spatiale, nos Algorithme 4 et Algorithme 3
peuvent surmonter la segmentation d‚Äôimage bruit√©e dans certains cas.
‚Äì Avec l‚Äôinformation spatiale, Algorithme 3 fonctionne bien sur toutes les images
bruit√©es. Il peut supprimer les bruits de mani√®re efficace. Ainsi les deux algo-
rithmes DCA apportent l‚Äôimage de meilleure qualit√© par rapport √† l‚Äôalgorithme
FCM.
‚Äì Dans la plupart des cas, les deux variantes de DCA sont plus rapides que FCM
(Algorithme 1. Par ailleurs Algorithme 4-3 permet d‚Äôavoir le gain de calcul le
plus important.
5 Conclusion
Dans ce travail, nous proposons une nouvelle m√©thode de segmentation d‚Äôimages
via le mod√®le de FCM en utilisant un nouveau et robuste algorithme bas√© sur la pro-
grammation DC et DCA. Le mod√®le de FCM a √©t√© reformul√© comme une programma-
tion DC sur laquelle un sch√©ma simple et rapide de DCA a √©t√© appliqu√©. La proc√©dure
alternative de FCM-DCA est efficace pour trouver un bon point initial de DCA et pour
acc√©l√©rer sa convergence. L‚Äôalgorithme en deux phases de DCA peut alors √™tre appli-
qu√© dans le probl√®me de la classification de grande dimension. D‚Äôautre part, l‚Äôuti-
lisation d‚Äôinformation spatiale pour la segmentation d‚Äôimages bruit√©es semble √™tre
efficace. Les exp√©riences num√©riques pr√©liminaires prouvent que les algorithmes pro-
pos√©s sont prometteurs pour la segmentation d‚Äôimages bruit√©es.
FIG. 2 ‚Äì L‚Äôimage originale et les r√©sultats de segmentation (c=3).
(a) : L‚Äôimage originale (b) : Le r√©sultat deAlgorithme 1 (c) : Le r√©sultat deAlgorithme
4 (d) : Le r√©sultat de Algorithme 4-3.
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
FIG. 3 ‚Äì L‚Äôimage originale avec le bruit et les r√©sultats de segmentation (c=3).
(a) : L‚Äôimage originale (b) : L‚Äôimage originale avec le bruit Gaussien (c) : Le r√©sultat
de Algorithme 1 (d) : Le r√©sultat de Algorithme 1 avec l‚Äôinformation spatiale. (e) :
Le r√©sultat de Algorithme 4 (f) : Le r√©sultat de Algorithme 4-3 (g) : Le r√©sultat de
Algorithme 4-3 avec l‚Äôinformation spatiale.
FIG. 4 ‚Äì L‚Äôimage m√©dicale originale et les r√©sultats de segmentation (c=2).
(a) : L‚Äôimage originale (b) : Le r√©sultat deAlgorithme 1 (c) : Le r√©sultat deAlgorithme
4 (d) : Le r√©sultat de Algorithme 4-3.
H. A. Le Thi et al.
FIG. 5 ‚Äì L‚Äôimage m√©dicale avec le bruit Gaussien et les r√©sultats de segmentation (c=3).
(a) : L‚Äôimage originale (b) : L‚Äôimage originale avec le bruit Gaussien (c) : Le r√©sultat
de Algorithme 1 (d) : Le r√©sultat de Algorithme 1 avec l‚Äôinformation spatiale. (e) :
Le r√©sultat de Algorithme 4 (f) : Le r√©sultat de Algorithme 4-3 (g) : Le r√©sultat de
Algorithme 4-3 avec l‚Äôinformation spatiale.
FIG. 6 ‚Äì L‚Äôimage m√©dical originale et les r√©sultats de segmentation (c=3).
(a) : L‚Äôimage originale (b) : Le r√©sultat deAlgorithme 1 (c) : Le r√©sultat deAlgorithme
4 (d) : Le r√©sultat de Algorithme 4-3.
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
FIG. 7 ‚Äì L‚Äôimage m√©dicale avec le bruit Gaussien et les r√©sultats de segmentation (c=3).
(a) : L‚Äôimage originale (b) : L‚Äôimage originale avec le bruit Gaussien (c) : Le r√©sultat
de Algorithme 1 (d) : Le r√©sultat de Algorithme 1 avec l‚Äôinformation spatiale. (e) :
Le r√©sultat de Algorithme 4 (f) : Le r√©sultat de Algorithme 4-3 (g) : Le r√©sultat de
Algorithme 4-3 avec l‚Äôinformation spatiale.
TAB. 1 ‚Äì R√©sultats comparatifs de Algorithme 1, Algorithme 3, etAlgorithme 4-3.
Donn√©es Algorithme 1 (FCM) Algorithme 3 Algorithme 4-3
N
‚ó¶ Taille c iter Temps iter Temps iter Temps
1 1282 2 24 1.453 16 1.312 10 1.219
2 1282 2 17 1.003 12 0.985 2 0.765
3 2562 3 36 15.340 24 13.297 2 10.176
4 2562 3 75 31.281 57 30.843 12 26.915
5 2562 3 39 15.750 27 14.687 14 13.125
6 2562 5 91 84.969 75 86.969 78 61.500
7 2562 3 73 31.094 62 34.286 21 24.188
8 2562 3 78 34.512 52 32.162 13 29.182
9 5122 3 49 92.076 41 102.589 46 74.586
10 5122 5 246 915.095 196 897.043 86 691.854
R√©f√©rences
Bezdek (1981). Pattern recognition with fuzzy objective function algorithm. New York,
NY. Plenum Press 1.
Bouman et Liu (1998). Segmentation of textured images using a multiple resolution
H. A. Le Thi et al.
approach. Proc. IEEE Int‚Äôl Conf. on Acoust., Speech and Sig. Proc., NewYork, NY, April
11-14, 1124‚Äì1127.
Campadelli, Medici, et Schettini (1997). Color image segmentation using hopfield
networks. Image and Vision Computing, Vol.15, N‚ó¶.3, 161‚Äì166.
Hung, Yang, et Chen (2006). Parameter selection for suppressed fuzzy c-means with
an application to mri segmentation. Pattern Recognition Letters, Vol.27, 424‚Äì438.
LeThi, H. A. (1997). Contribution √† l‚Äôoptimisation non convexe et l‚Äôoptimisation glob-
ale: Th√©orie, algorithmes et applications. Habilitation √† Diriger des Recherches, Uni-
versit√© de Rouen.
LeThi, H. A., T. Belghiti, et T. PhamDinh (2006). A new efficient algorithm based on
DC programming and DCA for clustering. In Press, Available July 2006, Journal of
Global Optimization.
LeThi, H. A., M. LeHoai, et T. PhamDinh (2005). Optimization based DC program-
ming and DCA for hierarchical clustering. In Press, Available online June 2006, Euro-
pean Journal of Operational Research.
LeThi, H. A. et T. PhamDinh (2005). The DC (difference of convex functions) program-
ming and DCA revisited with DC models of real world nonconvex optimization
problems. Annals of Operations Research 133, 23‚Äì46.
Littman et Ritter (1997). Adaptative color segmentation - a comparison of neural and
statistical methods. IEEE Transactions on Neural Networks, Vol.8, Ncirc.1, 175‚Äì185.
Liu, Y., X. SHEN, et Hani (2003). Multicategory œà-learning and support vector ma-
chine: Computational tools. Journal of Computational and Graphical Statistics 14, 219‚Äì
236.
Neumann, J., C. Schn√∂rr, et G. Steidl (2004). Svm-based feature selection by direct
objective minimisation. Pattern Recognition, Proc. of 26th DAGM Symposium 3175,
212 ‚Äì 219.
Pal, J. P. et S. K. Pal (1993). A review on image segmentation techniques. Pattern
Recognition, Vol.26, 1277‚Äì1294.
Pham (2002). Fuzzy clustering with spatial constraints. Proc. IEEE Intern. Conf. on
Image Processing, New Yord, USA.
PhamDinh, T. et H. A. LeThi (1997). Convex analysis approach to DC programming:
Theory, algorithms and applications. Acta Mathematica Vietnamica, dedicated to Pro-
fessor Hoang Tuy on the occasion of his 70th birthday 22, 289‚Äì355.
PhamDinh, T. et H. A. LeThi (1998). DC optimization algorithms for solving the trust
region subproblem. SIAM J.Optimization 8, 476‚Äì505.
Polyak, T. (1987). Introduction to optimization. Inc., Publications Division.
Rajapakse, Giedd, et Rapoport (2004). Statistical approach to segmentation of singke-
chanel cerebral mr images. IEEE Trans. On Medical Imaging 16.
Weber, S., T. Sch√ºle, et C. Schn√∂rr. (2005). Prior learning and convex-concave regular-
ization of binary tomography. Electr. Notes in Discr. Math 20, 313‚Äì327.
Segmentation d‚Äôimages par la Classification floue bas√©e sur DCA
Zhang et Chen (2004). A novel kernelized fuzzy c-means algorithm with application
in medical image segmentation. Artificial Intelligence in Medicine, Vol.32, 37‚Äì50.
Summary
Wepresent a fast and robust algorithm for image segmentation problems via Fuzzy
C-Means (FCM) clustering model. Our approach is based on DC (Difference of Con-
vex functions) programming and DCA (DC Algorithms) that have been successfully
applied in a lot of various fields of Applied Sciences, including Machine Learning. In
an elegant way, the FCM model is reformulated as a DC program for which a very
simple DCA scheme is investigated. For accelerating the DCA, an alternative FCM-
DCA procedure is developed. Moreover, in the case of noisy images, we propose a
new model that incorporates spatial information into the membership function for
clustering. Experimental results on noisy images have illustrated the effectiveness of
the proposed algorithm and its superiority with respect to the standard FCM algo-
rithm in both running-time and quality of solutions.
