© Revue MODULAD, 2004 - 32 - Numéro 31 
Robustesse de l’estimation spatiale par krigeage simple et par 
régression PLS 
Youssfi ELKETTANI et Driss MENTAGUI 
Faculté des Sciences, Département de mathématiques 
Laboratoire d’analyse convexe et variationnelle 
systèmes dynamiques et processus stochastiques 
BP133, Kénitra, Maroc 
Email : elkettani_y@yahoo.fr , d_mentagui@hotmail.com 
 
Résumé : 
L’étude de la robustesse du krigeage, a montré que, dans le cas où la matrice de covariance 
est bien conditionnée, le krigeage est stable, dans le cas contraire il peut être instable par 
rapport aux perturbations de la fonction de covariance. Nous rappelons l’application de la 
régression PLS a un champ spatial stationnaire de moyenne connue, puis nous comparons 
la robustesse des coefficients du krigeage a celle de la prédiction spatiale par régression 
PLS, ainsi que la robustesse de la précision des deux prédicteurs quand des perturbations 
sont produites sur le paramètre de portée de la fonction de covariance. 
Mots clés: prédiction spatiale, krigeage, régression PLS, robustesse. 
 
Summary: 
The study of kriging robustness has shown that, when the condition number of the 
covariance matrix is small, kriging is stable. Whereas when this number is too big kriging 
may be unstable with respect to perturbations of the covariance function parameter. In 
this paper, we recall the use of the PLS regression in the estimation of a stationnary 
spatial field with known mean, which has positive and meaningful weights. Then we 
compare robustness of the PLS regression estimation to the kriging one when the range 
parameter is perturbed. We will see that when the covariance matrix is well 
conditioned, kriging of all the area from the same observations set, is done with the 
same stability level. However, the PLS regression estimate is always stable even when 
the condition number of the covariance matrix is too big. 
Key words: spatial estimation, kriging, PLS regression, robustness. 
 
Introduction 
Les méthodes de prédiction d’un champ spatial stationnaire passent par la phase 
préalable d’ajustement de la fonction de covariance. Différentes études ont porté sur 
l’impact d’une mauvaise modélisation de la fonction de covariance sur la qualité de 
krigeage. D’une part [13] puis [11] , ont étudié la stabilité de la prédiction quand le 
modèle de covariance théorique est approximé par une suite cn de covariances 
ajustées à partir des observations, ou le nombre n d’observations tend vers l’infini. 
D’autre part [7] ont montré que la stabilité du krigeage ordinaire aux petites 
perturbations des paramètres du modèle de covariance est fonction du nombre de 
conditionnement de la matrice de covariance du champ étudié: plus le nombre de 
© Revue MODULAD, 2004 - 33 - Numéro 31 
conditionnement est grand moins est stable le vecteur des coefficients du krigeage. 
Par une approche infinitésimale [16] a krigé sur un covariogramme perturbé et a 
calculé les perturbations de premier et de second ordre dans les coefficients de 
krigeage. Sans surprise, les grandes perturbations dans les prédictions ont lieu là 
où la variance de krigeage est grande. Pour tester son résultat [16] utilise deux 
covariogrammes isotropes dans R2, le modèle exponentiel et le modèle gaussien. 
L’influence du modèle de covariance sur le krigeage est par ailleurs étudiée par [2] . 
Concernant la stabilité de la précision du krigeage, [7] établissent une majoration de 
la variation relative de la variance d’estimation du même type que les majorations 
qu’ils établissent pour les coefficients du krigeage, alors que [3] étudie l’impact du 
changement de l’effet de pépite sur la variance de l’estimateur. Par ailleurs la 
régression PLS (Partial Least Squares), est une méthode d’analyse multivariée 
particulièrement adaptée aux données multi-colinéaires. Introduite par [17], ses 
propriétés mathématiques sont développées dans [14] et son application à 
l’estimation d’un champs spatial dans [8] a présenté des coefficients proportionnels 
au vecteur des covariances et un algorithme facile pour la sélection d’une 
configuration pour l’estimation locale. 
En statistique multivariée classique, la robustesse de la régression PLS est une 
source d’intérêt (voir [15] par exemple). Le principal objet de notre article ci-dessous 
est d’étendre la réflexion sur la stabilité de la prédiction spatiale locale à l’estimation 
par régression PLS. Nous étudions de façon théorique, et nous illustrons par un 
certain nombre d’exemples simples et représentatifs de différentes situations, la 
robustesse des coefficients d’estimation ainsi que celle de la précision des 
estimateurs, par régression PLS et par krigeage simple, quand des perturbations 
sont produites sur le paramètre de portée de la fonction de covariance. 
Nous montrerons qu’il est plus intéressant, vis à vis de la robustesse des 
coefficients, de faire l’estimation par krigeage quand la matrice de covariance est 
bien conditionnée. Nous verrons qu’en effet, avec une même configuration de points 
d’observations le krigeage permet d’estimer tous les points du voisinage avec le 
même niveau de robustesse. Par contre quand la matrice de covariance est mal 
conditionnée la régression PLS reste stable et donc plus indiquée. Quand à la 
précision des estimations, nous verrons que si le nombre de conditionnement est 
très élevé, la variation relative de la variance de l’estimateur PLS peut être beaucoup 
plus faible que celle du krigeage, alors que quand la matrice k est bien conditionnée, 
les variances du krigeage et de la régression PLS ont des variations relatives de 
même intensité. 
Après un rappel des méthodes d’estimation (chapitre 1), nous présentons les 
résultats sur la robustesse de chaque méthode (chapitre 2) suivis des exemples 
d’illustration des différentes situations (chapitre 3).  
Par souci de clarté du texte, les démonstrations sont reportées en annexe de la 
version PDF. 
 
 
 
© Revue MODULAD, 2004 - 34 - Numéro 31 
 1 L’estimation spatiale locale linéaire 
Soit F un processus spatial stationnaire gaussien, d’espérance m et de fonction de 
covariance c(h) = cov(F(M + h), F(M)), et σ² = var (F (M)) . Le processus F est supposé 
être observé en n points 1M , ..., Mn d’un domaine physique D c 
pR , p=1, 2 ou 3. On 
note Fi = F(Mi), i = 1, ..., n les variables observées et Fo = F(Mo) la variable à estimer, 
Mo un point de D. Notons c = ( 1,..., nc c )' le vecteur colonne ou ic = cov(Fo,Fi), k la 
matrice nxn dont l’élément (i,j) est ijk = cov (Fi,Fj) ,  i i 0 0
1 1Z = (F -m),i=1,...,n ;  Z = (F -m)σ σ  et 
Z = (Z 1 , ..., Zn). Dans toute la suite nous utiliserons la norme 
2
||Kx||L :|| c|| =c'c et || k|| =sup
||x||
 
L’estimation spatiale consiste à prédire, la variable aléatoire Zo par une combinaison 
linéaire des observations zo = Z’w où ni i=1,...,nw=(w ) R∈ est le vecteur des coefficients 
d’estimation. Alors 0 0f =m+  zσ est un estimateur sans biais de Fo. 
 
 1.1 Le krigeage simple 
La méthode du krigeage stationnaire à moyenne connue, dit krigeage simple (ks), 
consiste à chercher le vecteur des coefficients i i= 1 , . . . ,nx = (x )  tel que la combinaison 
linéaire x'Z  minimise l’erreur quadratique. E=E( oz -x’Z)². La résolution de cette 
équation donne: 
x= 1k − c.                                                         (1) 
 L’estimateur de Zo est ksz = Z’x = Z’
1k − c ; et 0F  est estimé sans biais par ks ksf m zσ= + . 
La variance de l’estimateur fks est 
 2ks∑ =c’ 1k − c                                                   (2) 
 et l’erreur quadratique moyenne est: Φ² ( ksf )= E( 0f - ksf )²=
2 1'c k cσ −−  
 
1.2 La régression PLS 
La régression PLS (partial least square) est une méthode explicative d’analyse de 
données multivariées proposée par [17]. Ses propriétés mathématiques sont 
présentées dans [14]. La régression PLS de Zo sur Z = ( 1z ,..., nz ) consiste à chercher la 
combinaison linéaire t = λ’Z , avec || λ || = 1 telle que cov (Zo, t) soit maximale. 
L’expression de l’estimateur PLS  plsz  est donnée par:  
plsz =(C’C/C’KC)C’Z= λ’Z 
 en notant  λ  le vecteur des coefficients :  
λ=(C’C/C’KC)C                                              (3) 
© Revue MODULAD, 2004 - 35 - Numéro 31 
 
Zo est estimé par plsz =(C’C/C’KC)C’Z= λ’Z   et Fo est estimé par pls plsf m zσ= +  
La variance de l’estimateur plsf  est  
2
pls∑ =C’CC’C/C’KC                                                          (4) 
et l’erreur quadratique moyenne est: Φ²( plsf )= E( 0f - plsf )²=σ²- C’CC’C/C’KC 
Ici les coefficients, proportionnels au vecteur λ, sont facilement interprétables et 
sont positifs quand la fonction de covariance l’est. Par ailleurs, [8] propose un 
algorithme de sélection d’une configuration d’observations pour l ’estimation 
locale. Il consiste à supprimer les observations qui ont un faible coefficient par 
la régression PLS. 
 
2 Etude de la robustesse 
Nous allons étudier les variations relatives des coefficients d’estimation ainsi que 
les variations relatives des variances des estimateurs après perturbation des 
coefficients du vecteur c et de la matrice K. 
 
 
2.1 Comparaison des normes des vecteurs des coefficients 
On suppose toujours que k est symétrique définie positive, et nous notons 
( ) -1 || k|| || k ||τ κ =  le nombre de conditionnement de k. Il est égal au rapport de la plus 
grande à la plus petite valeur propre de K. 
 
Théorème 1: 
Les vecteurs x et λ donnés par (1) et (3) vérifient: 
( ) ( )
 || x||  || λ||  || x||τ κτ κ ≤ ≤  
Nous remarquons donc que si K est bien conditionnée (τ(k) de l’ordre de 
l’unité), ||x ||  et ||λ ||  sont du même ordre de grandeur. 
 
2.2 Voisinage d’une fonction de covariance 
Soit Λ une classe de fonctions de covariance c(h), h > 0, et δ un réel dans ]0,1[. On 
dit qu’une covariance q est dans le δ-voisinage, Nδ(c) de c si: 
1 - δ < 
q(h)
c(h) < 1 + δ , h > 0              (6) 
© Revue MODULAD, 2004 - 36 - Numéro 31 
Supposons que F(.) soit stationnaire de fonction de covariance c, et qu’on désire 
estimer Fo = F(Mo) sur la base de n observations F(M 1 ), ...,F(Mn). Les coefficients des 
estimateurs par krigeage et par régression PLS sont alors données par les 
équations (1) et (3). Si maintenant l’estimation se fait à partrir d’une covariance q, 
les équations (1) et (3) deviennent respectivement : 
Q qx =b                                                             (7) 
b’Qb qλ =b’b.b                                                     (8) 
où Q et b sont respectivement la matrice et le vecteur de covariances obtenus à 
partir de q ; xq et λq étant les vecteurs des coefficients du krigeage et de la régression 
PLS associés au choix q. 
D’après (6), on a (1 - δ) || c||  < ||b || < (1 + δ) || c|| . De même, puisque les matrices K et 
Q ont toutes les deux, tous les termes de même signe, on a 
(1 - δ) ||k ||< ||Q || < (1 + δ) ||k || 
 
2.3 Variations relatives des vecteurs des coefficients 
Supposons que les équations (7) et (8) soient obtenues à partir de (1) et (3) par des 
petites perturbations de la covariance et posons: 
b = c + ∆c et Q = k + ∆k 
pour les éléments de la covariance. Et pour les vecteurs des coefficients on pose: 
xq = x + ∆x et λq = λ + ∆λ 
Pour le krigeage simple, une majoration de la variation relative du vecteur des 
coefficients est donnée par une expression similaire à celle obtenue par [7] dans le cas 
du krigeage ordinaire: 
( )
( )
2|| ||
||x|| (1 )
x δτ κ
δτ κ
∆ ≤ −  
Ce qui montre que si δτ(k) < 
2
ε
ε+ alors  
|| ||
||x||
x∆ <ε. 
Les théorèmes 2 et 3 qui suivent établissent des majorations similaires de la 
variation relative du vecteur des coefficients de l’estimation par régression PLS. 
Leurs démonstrations figurent en annexe 1 de la version PDF. 
 
Théorème 2: 
Si 0 < δ < ( )
1
4τ κ , alors  
( )( )
( )
4 1|| ||
||λ| (1 4 )
δ τ κλ
δτ κ
+∆ ≤ −                                               (9) 
© Revue MODULAD, 2004 - 37 - Numéro 31 
et on a aussi  
( )( )
( )
6 1|| || || ||| |
||λ| ||x|| (1 4 )
x δ τ κλ
δτ κ
+∆ ∆− ≤ −  
Ce qui montre que si δ < ( )
1
4 1 τ ετ+ +  alors 
|| ||
||λ|
λ∆  < ε. Par ailleurs dans une 
situation de matrice bien conditionnée, les variations relatives des vecteurs des 
coefficients du krigeage et de la régression PLS sont du même ordre de grandeur. 
Le théorème 3 donne un résultat encore plus fort pour la régression PLS car 
il améliore la condition 0 < 4δ < ( )
1
τ κ  
du théorème 2. Posons r = c’ kc, la norme de c relative à la matrice 
symétrique définie positive K, et α = 
|| c||²
r
 le 
rapport des deux normes dans Lz. 
 
Théorème 3: 
Quel que soit la valeur de δτ(k), on a 
|∆α|
|| || α
||λ| 1
δλ
α
α
+∆ ≤ ∆+
. 
Cette inégalité donne une majoration de || ||
||λ|
λ∆ , valable sans condition sur δτ(k), ce 
qui garantit la stabilité de l ’estimation spatiale par régression PLS même quand 
K est mal conditionnée. 
Enfin le théorème suivant va nous montrer que la variation relative des 
coefficients du krigeage reste bornée, avec une borne supérieure ne dépendant 
que de k, et ce quelle que soit la perturbation effectuée sur le vecteur c. Cette 
propriété qui n’est pas vraie pour la régression PLS signifie qu’à partir de la 
même configuration d’observations , les estimations par krigeage, faites en 
différents points de l’espace (k fixé et c variable), auront la même variation 
relative des vecteurs des coefficients, et donc le même niveau de stabilité. 
 
Théorème 4: 
 Si δτ(k) < 1 alors 
( )
( )
1
-1
3|| || || ||| |
||x|| ||k || (1 )
x k δτ κ
δτ κ
−∆ ∆− ≤ −  
© Revue MODULAD, 2004 - 38 - Numéro 31 
2.4 Variations relatives des variances des estimateurs 
Dans cette partie nous allons établir des majorations des variations relatives 
des variances ce qui permettra de délimiter les variations des bornes des 
intervalles de confiance pouvant être obtenus. Nous énonçons les résultats dans 
les théorèmes 5 et 6, leurs démonstration figurent en annexe 2 de la version PDF. 
Le théorème 5 concerne les résultats sur le krigeage. Soit 2sκ∑ =c’x la variance du 
krigeage simple. 
 
Théorème 5: 
Notons b =
( ) ( )( )
( )
2 1
(1 )
δτ κ τ κ
δτ κ
+
−  et soit δ suffisamment petit de sorte que b [ ]0,1∈ , 
alors l’une des deux expressions suivantes est vraie: 
-1 1 1 ks
ks
b ∆Σ≤ − + − ≤ Σ 1 1 1 2b≤ − + + ≤ − +  
ou bien 
1 2 (1 1 ) ks
ks
b ∆Σ− − ≤ − + + ≤ Σ ( 1 1 ) 2b≤ − + − ≤ −  
Et si b > 1, alors | | 1 1ks
ks
b∆Σ ≤ + +Σ  
L’encadrement de la variation relative de l’écart-type 
de l’estimateur par régression PLS est donné par le théorème 6. Notons 2pls∑ = 
c’λ la variance de l’estimateur PLS. 
Théorème 6: 
Notons d= 
( )( )
( )
2 3 2
(1 4 )
δ τ κ
δτ κ
+
− , et soit δ suffisamment petit de sorte que d [ ]0,1∈ , alors 
l’une des deux expressions suivantes est vraie: 
-1 1 1 pls
pls
d
∆Σ≤ − + − ≤ Σ 1 1 1 2d≤ − + + ≤ − +  
ou bien 
1 2 (1 1 ) pls
pls
d
∆Σ− − ≤ − + + ≤ Σ ( 1 1 ) 2d≤ − + − ≤ −  
Et si d>1, alors | | 1 1pls
pls
d
∆Σ ≤ + +Σ  
© Revue MODULAD, 2004 - 39 - Numéro 31 
Ainsi si k est bien conditionnée, une faible perturbation va conduire à des 
variations relatives des écarts-type, de l’estimateur PLS et du krigeage, bornées et 
du même ordre de grandeur. Par contre si le nombre de conditionnement est très 
élevé, alors b de l’ordre de τ² est beaucoup plus grand que d qui est de l’ordre de τ. 
Et ceci peut induire une variation relative de l’écart-type de l’estimateur PLS 
beaucoup plus faible que celle du krigeage. 
 
3 Exemples: 
Afin d’apprécier la robustesse des estimateurs par krigeage et par régression 
PLS, nous allons considérer divers exemples simples et représentatifs. Après 
quelques remarques sur les fonctions de covariance (chapitre 3.1), nous com-
mençons par présenter des situations de matrices mal conditionnées (chapitre 
3.2). Puis nous illustrerons la stabilité des estimateurs quand la matrice de 
covariance est bien conditionnée (chapitre 3.3). A travers l’exemple 7, nous 
verrons que la variation relative || ||
||x||
x∆ dépent peu de ∆c quand k est bien con-
ditionnée. Enfin nous illustrerons ces résultats également à travers un cas réel 
(chapitre 3.4). 
Dans les exemples de 1 à 6, Z est une fonction aléatoire gaussienne sur R²  
stationnaire de moyenne m = 0 et de covariance c, avec c(0) = σ2 = 1. La fonction 
de covariance q, est obtenue en perturbant le paramètre de la portée p  de c. Les 
points d’observation sont dans le voisinage de l’origine oM (0, 0). La v.a à estimer est 
oz  = Z(X0). 
 
3.1 Fonction de covariance 
Nous étudierons trois familles de modèles de covariance usuels sur R2: la co-
variance exponentielle définie par 
c (h) = c(0) exp –(h/a), a > 0                                  (10) 
la covariance sphérique définie par 
c (h) = c(0)(1-1.5 h
a
⎛ ⎞⎜ ⎟⎝ ⎠ +0.5*
3h
a
⎛ ⎞⎜ ⎟⎝ ⎠ )χ(h<a) , a > 0             (11) 
où χ est la fonction indicatrice; et la covariance gaussienne définie par : 
c (h) = c(0) exp –(h²/a), a > 0                             (12) 
La covariance sphérique présente une portée finie a, distance au delà de laquelle le 
processus n’est plus autocorrélé. Pour les modèles exponentiels et gaussiens, a reste 
un paramètre de portée de la covariance. 
Dans trois exemples nous avons utilisé la même configuration de huit points. Dans 
l’exemple 4, avec la covariance gaussienne, on a obtenu pour nombre de 
conditionnement τ = 202. Puis dans l’exemple 6, avec la covariance exponentielle on 
a eu τ = 3.02. Enfin dans l’exemple 7, avec la covariance sphérique, τ = 5.86. Cette 
© Revue MODULAD, 2004 - 40 - Numéro 31 
différence de la valeur de τ, d’une covariance à l’autre, et qui se traduit par une 
différence dans la stabilité des résultats d’estimation, s’explique théoriquement par 
le théorème de Jaffard qui établit la stabilité pour les matrices dont les termes 
décroissent de manière hyperbolique en s’éloignant de la diagonale [9]. Le théorème 
de Jaffard a été généralisé par Coeurjolly au cas des matrices à décroissance 
exponentielle [5]. La matrice K d’une covariance sphérique, respectivement 
exponentielle, vérifie la propriété de decroissance hyperbolique, respectivement 
exponentielle, et présente donc pour la même configuration, un nombre de 
conditionnement relativement faible par rapport au nombre de conditionnement 
obtenu avec la covariance gaussienne.  
 
3.2 Exemples 1 à 4: Situations avec K mal conditionnée 
 
Le 1er exemple est repris de [7] qui avaient constaté l’instabilité des coefficients du 
krigeage. Les coefficients de l’estimateur par régression PLS est lui plus stable. 
L’exemple 2 qui présente également une matrice mal conditionnée, conduit aux 
mêmes conclusions de krigeage instable alors que l’estimateur par régression PLS est 
stable. L’exemple 3 montre que ce n’est pas toujours le krigeage qui est instable 
puisque nous avons dans cet exemple une très faible variation relative des 
coefficients du krigeage alors que celle de la PLS est bien plus importante sans 
toutefois dépasser la variation relative du vecteur des covariances c. Dans 
l’exemple 4, les deux estimateurs sont stables alors que la matrice des covari-
ances k est mal conditionnée, ce qui montre que dans le cas de matrice de 
covariance mal conditionnée, toutes les situations de stabilité ou non peuvent se 
produire. Toutefois la variation relative du vecteur des coefficients de la PLS reste 
toujours du même ordre de grandeur que celle de c. 
Enfin dans ces quatre exemples, la précision des estimations a été très stable car la 
variation relative des variances des estimateurs a été très faible. 
 
Exemple 1  
Z est obsérvée sur trois points, deux sont très proches. Dans cette situation, [12] 
montre que le krigeage attribue un faible coefficient à l’un des points redondants. 
C’est précisemment la variation de cette faible valeur dans les coefficients du 
krigeage qui s’avère relativement importante. Les modèles de covariance sont 
gaussiens donnés par (12) de paramètre a = 3  pour la fonction de covariance c, 
et a = 0.9 3  pour la covariance perturbée q. Cette variation de a induit des 
variations relatives du vecteur c et de la matrice k de l’ordre de δ = 5%. Les 
coordonnées des observaions, les coefficients x et λ du krigeage et de la régression 
PLS, leurs variations relatives, le vecteur β des valeurs propres de la matrice de 
covariance, et la variation relative du vecteur c conséquente à la perturbation de 
la fonction de covariance, sont présentés au tableau 1. 
© Revue MODULAD, 2004 - 41 - Numéro 31 
Abs ord. KS (x) PLS (λ) 
x
x∆  (en%)  
λ
λ∆ (en%) β  
c
c∆  (en%) 
  -0.4 0 0.5567 0.3603 1.15 0.55 0.6822 3.82 
   0.4 0 0.4552 0.3603 2.41 0.55 0.0124 3.82 
 0.39 0.1 0.1044 0.3596 14.54 0.50 2.3054 3.88 
Tableau 1 
On obtient τ(K) = 185.92. Le 3ème coefficient du krigeage a 
une variation relative de 14.54% dépassant largement δ alors que les 
coefficients de la régression PLS sont stables. Enfin la variation relative des 
écarts-type des estimateurs est, respectivement pour le krigeage et la 
régression PLS: 1.29% et 1.63%. 
 
Exemple 2 :  
Z est obsérvée en quatre points. Les fonctions c, et q sont les mêmes que 
dans l’exemple 1. L’instabilité du krigeage dûe à la variation de très faibles 
pondérations est encore plus marquée, comme le montre le tableau 2. Le 
nombre de conditionnement est τ = 4285.44. Et la variation des écarts-type des 
estimateurs est de  0.1% pour le ks contre 0.01 pour la PLS. Cette dernière est 
plus faible que celle du krigeage car τ est trop élevé. 
 
Abs. ord. ks(x) pls(λ) 
x
x∆  (en%)
λ
λ∆  (en%) β 
c
c∆ (en%) 
-0.1 -0.1 4.77x 10-15 0.2524 90.55 0.21 0.0009 0.47 
0.1 0.1 3.69x 10-15 0.2524 92.7 0.21 0.0183 0.47 
0 0.1 0.5049 0.2550 0.22 0.45 0.1239 0.23 
0 -0.1 0.5049 0.2550 0.22 0.45 3.8569 0.23 
Tableau 2 
 
Exemple 3 : 
 Z est obsérvée sur cinq points, dont un est relativement éloigné de l’origine, 
alors que les quatres autres sont autour et proches de l’origine. La 
covariance est sphérique donnée par (11) de paramètre a = 3  pour c et a = 
0.9 3  pour la covariance perturbée q. 
Les deux portées recouvrent les cinq points d’observation. Le tableau 3 
montre que le krigeage est stable malgré un nombre de conditionnement assez 
élevé τ = 45, alors que la variation relative des coefficients de la régression 
© Revue MODULAD, 2004 - 42 - Numéro 31 
PLS est importante, et dûe principalement à celle du vecteur des covariances c. 
Les variances des estimateurs sont très stables puisque on a  
ksΣ
ks∆Σ = 0.5% et  
plsΣ
pls∆Σ =0.4%. 
 
absc. ord. KS (x) PLS (λ)  
x
x∆ (en%)  
λ
λ∆ (en%) β  
c
c∆ (en%) 
0.62 0.60 -0.0066 0.0861 1.45 20.24 0.0788 23.57 
-0.1 -0.1 0.1276 0.2402 0.41 1.17 0.1056 1.56 
0.1 0.1 0.1323 0.2402 0.18 1.17 0.3187 1.56 
0 0.1 0.3789 0.2500 0.08 1.66 0.9327 1.06 
0 -0.1 0.3784 0.2500 0.09 1.66 3.5642 1.06 
Tableau 3 
 
Exemple 4 : 
Z est obsérvée sur huit points qui forment les sommets et les milieux des côtés 
d’un carré centré en l’origine. Les fonctions de covariance sont gaussiennes de 
paramètre a= 3 , pour c et a = 0.9 3  pour q. Ces valeurs rendent les huit 
observations corrélées entre elles et avec la variable à estimer. Le tableau 4 montre 
que dans cet exemple aussi bien le krigeage que les coefficients de la régression PLS 
sont stables malgré un nombre de conditionnement élevé τ = 202. Et les écarts-type 
des estimateurs sont 
ksΣ
ks∆Σ = 0.15%  et 
plsΣ
pls∆Σ = 1.24%. 
absc. ord. KS (x) PLS (λ) 
x
x∆  (en%) 
λ
λ∆  (en%) β 
c
c∆  (en%) 
-0.4 0 0.5579 0.1615 1.17 4.29 0.1065 3.82 
0.4 0 0.5579 0.1615 1.17 4.29 0.0241 3.82 
0 0.4 0.5579 0.1615 1.17 4.29 0.0241 3.82 
0 -0.4 0.5579 0.1615 1.17 4.29 1.3161 3.82 
-0.4 -0.4 -0.3113 0.1376 2.33 0.63 1.3161 7.80 
0.4 0.4 -0.3113 0.1376 2.33 0.63 0.0462 7.80 
-0.4 0.4 -0.3113 0.1376 2.33 0.63 0.2983 7.80 
0.4 -0.4 -0.3113 0.1376 2.33 0.63 4.8683 7.80 
© Revue MODULAD, 2004 - 43 - Numéro 31 
Tableau 4 
A travers ces quatre exemples, il ressort que dans le cas d’une matrice de 
covariance mal conditionnée, toutes les situations de stabilité des coefficients ou 
non, peuvent avoir lieu pour chacun des deux estimateurs étudiés. Toutefois la 
variation relative des coefficients de la PLS reste du même ordre de grandeur que ceux 
de c, et donc de δ; alors que la variation relative des coefficients du krigeage peut être 
beaucoup plus importante. Par contre dans les quatre exemples traités, les deux 
méthodes ont eu des variances d’estimation robustes.  
 
3.3 Exemples 5 à 7: Situations avec K bien conditionnée: 
Dans les exemples 5 et 6 nous verrons des situations de stabilité des coefficients en 
commençant par le cas trival d’observations indépendantes, nous verrons 
également la proximité de || ||
||x||
x∆  et de 
-1
-1
|| k ||
||k ||
∆ , ainsi que les encadrements des 
variations relatives des variances. L’exemple 7 illustre le faible impact de ∆c sur 
|| ||
||x||
x∆
 quand la matrice K est bien conditionnée et l’intensité de la perturbation δ est 
faible, compte tenu de la proximité de || ||
||x||
x∆  et de 
-1
-1
|| k ||
||k ||
∆ . 
Par contre || ||
||λ|
λ∆   est toujours fortement lié à || ||
||c||
c∆ . 
 
Exemple 5:  
Cet exemple est celui de quatre observations indépendantes. La fonction de 
covariance est sphérique, de portée p = a de sorte que la distance de l’origine aux 
points d’observation soit plus petite que a et que les distances mutuelles entre les 
points d’observation soient toutes supérieures à a. Il est trivial que le nombre de 
conditionnement de la matrice de covariance vaut 1 et que les variations relatives de 
x, de λ  et de c sont égales. En effet, K étant égal à l’identité, on déduit de (1) et 
(3) que x = λ  = c. 
 
Exemple 5bis : 
 Dans cet exemple la matrice de covariance est très proche de l’identité. Elle est 
obtenue en modifiant légèrement la configuration précédente des points 
d’observation de sorte à créer une petite covariance entre deux points 
d’observation. La fonction de covariance est encore sphérique, de portée p = a = 
0.495 pour q et p=a = 0.5 pour c. Le nombre de conditionnement de la matrice de 
covariance est égal à 1.09. Le tableau 5 présente les résultats de cette situation. 
 
 
© Revue MODULAD, 2004 - 44 - Numéro 31 
absc. ord. ks(x) pls(λ) || ||
||x||
x∆ (en%) || ||
||λ|
λ∆  (en%) β  || ||
||c||
c∆ (en%) 
0. 0.4 0.056 0.055 8.29 8.06 1 8.29 
0.1 -0.4 0.042 0.043 9.54 9.67 1.0434 9.9 
-0.3 -0.3 0.031 0.032 11.56 11.78 0.9566 12.02 
0.45 0. 0.014 0.014 20.62 20.37 1 20.62 
Tableau 5 
On constate que les variations relatives des vecteurs des coefficients d’estimations 
sont très proches: || ||
||x||
x∆ = 9.755%, et || ||
||λ|
λ∆  = 9.753%, et tous les deux sont del’ordre de 
δ  puisque || ||
||c||
c∆ = 9.979% et || ||
||k||
k∆ = 0.376%. Les variations relatives observées sont de 
9.63% pour le ks et de 9.62% pour la pls, c. a .d. de l’ordre de δ. 
 
Exemple 6 : 
 Nous reprenons la configuration de 8 points de l’exemple 4, mais avec une fonction 
de covariance exponentielle donnée par (10) de paramètre a= 3 pour c et a = 2.7 pour 
q. Le nomobre de conditionnement vaut τ  = 3, 02. On constate que les variations 
relatives de x et de λ  sont faibles, de l’ordre respectivement de 7% et 9%, et les 
variations relatives des variances sont de 0.9% pour le ks et  de 1.28% pour la pls, 
et ce malgré une perturbation qui a induit une variation relative de c de l’ordre de 
20%. Le tableau 6 présente les résultats obtenus pour cet exemple. 
absc. ord. KS (x) PLS (λ) || ||
||x||
x∆ (en
%) 
|| ||
||λ|
λ∆ (en%) β  || ||
||c||
c∆  (en%) 
-0.4 0 0.1883 0.1471 7.16 4.00 0.7660 14.26 
0.4 0 0.1883 0.1471 7.16 4.00 0.6518 14.26 
0 0.4 0.1883 0.1471 7.16 4.00 0.6518 14.26 
0 -0.4 0.1883 0.1471 7.16 4.00 1.2557 14.26 
-0.4 -0.4 0.0363 0.0895 2.09 9.90 1.2557 20.75 
0.4 0.4 0.0363 0.0895 2.09 9.90 0.6300 20.75 
-0.4 0.4 0.0363 0.0895 2.09 9.90 0.8841 20.75 
0.4 -0.4 0.0363 0.0895 2.09 9.90 1.9049 20.75 
Tableau 6 
© Revue MODULAD, 2004 - 45 - Numéro 31 
 
Exemple 7 :  
Cet exemple illustre la faible influence de ∆c sur les variations relatives du vecteur 
des coefficients || ||
||x||
x∆ . Nous reprenons la configuration de 8 points de l’exemple 6 et 
une fonction de covariance sphérique de paramètre a = 1, pour c et a = 0.95 pour q. 
Le nombre de conditionnement obtenu est τ  = 5.86. Nous allons procéder à 
l’estimation de trois variables: 10Z  = Z(0, 0), 
2
0Z =Z(0.2, 0.3) et 
3
0Z = Z(-0.3, 0.2). D’une 
estimation à l’autre c et ∆c varient, k et ∆k restent inchangés. Enfin les variations 
relatives des variances d’estimation, présentées au tableau 7, ont été très faibles 
pour les 3 points estimés et pour les deux méthodes d’estimation étudiées. Les 
variations relatives 
c
c∆ , 
x
x∆  et 
λ
λ∆  pour 10Z , 20Z  et 30Z  sont présentées au tableau 8. 
1
0Z  = Z(0, 0) 
2
0Z  = Z(0.2, 
0.3) 
3
0Z = Z(-0.3, 
0.2) 
ksΣ
ks∆Σ  
plsΣ
pls∆Σ  
ksΣ
ks∆Σ  
plsΣ
pls∆Σ  
ksΣ
ks∆Σ  
plsΣ
pls∆Σ  
3.55 4.36 1.47 1.04 1.47 1.04 
  Tableau 7 
 
1
0Z  = Z(0, 0) 
2
0Z  = Z(0.2, 0.3) 
3
0Z = Z(-0.3, 0.2) 
c
c∆  
x
x∆  
λ
λ∆  
c
c∆  
x
x∆  
λ
λ∆  
c
c∆  
x
x∆  
λ
λ∆  
6.47 0.94 0.8 23.83 21.61 16.56 2.56 0.77 3.46 
6.47 0.94 0.8 5.36 1.36 0.82 33.61 19.57 25.76 
6.47 0.94 0.8 2.56 0.77 3.46 5.36 1.36 0.82 
6.47 0.94 0.8 33.61 19.57 25.76 23.83 21.61 16.56 
13.94 38.81 6.15 587.48 23.8 547.11 17.15 11.99 10.27 
13.94 38.81 6.15 2.56 0.52 3.46 33.61 12.64 25.76 
13.94 38.81 6.15 17.15 11.99 10.27 2.56 0.52 3.46 
13.94 
Tableau 8
38.81 6.15 33.61 12.64 25.76 587.48 23.8 547.11 
Tableau 8 
 
© Revue MODULAD, 2004 - 46 - Numéro 31 
Nous constatons que les variations relative du vecteur c, qui sont  considérables 
pour certaines composantes n’influent pas, dans cette situation de matrice bien 
conditionnée, sur la variation relative du vecteur des coefficients du krigeage || ||
||x||
x∆ . 
Cette dernière quantité qui vaut 3.58% en M0 et 2.13% en M1 et en M2 reste 
constamment proche de 
1
-1
|| ||
||k ||
k −∆ = 7.77%. Par contre la variation relative du vecteur 
des coefficients de la régression PLS est fortement liée à celle de c.  
 
3.4 Cas du taux de cadmium dans le sol :  
La variable étudiée représente le taux de cadmium dans le sol en ppm; les 
mesures ont été effectuées en 60 sites dans le plan horizontal.  
 
absc. ord. KS (x) PLS (λ) 
x
x∆  (en %)  
∆λ/λ (en %) 
 
∆c/c (en %) 
432 252 0,0009 0,02 32960,94 3,14 5,36 
360 315 -0,0007 0,01 140,03 5,55 14,80 
334 163 0,0001 0,02 115,63 0,17 8,58 
434 312 -0,0001 0,02 80,13 0,48 9,29 
334 271 0,0004 0,01 50,45 4,38 13,53 
281 249 -0,0011 0,01 49,29 6,44 15,77 
370 165 -0,0043 0,02 38,27 2,56 5,99 
350 203 -0,0023 0,02 31,10 1,11 7,56 
254 172 0,0071 0,01 30,89 4,39 13,54 
413 150 -0,0074 0,03 27,60 4,29 4,10 
254 128 -0,0047 0,01 24,98 7,35 16,76 
254 216 -0,0035 0,01 23,82 5,99 15,29 
346 216 -0,0044 0,02 23,12 0,02 8,79 
254 299 0,0008 0,01 22,28 11,74 21,54 
274 231 -0,0038 0,01 21,02 5,71 14,98 
254 257 0,0005 0,01 20,61 8,80 18,33 
360 195 0,0143 0,02 19,69 2,21 6,37 
355 291 0,0017 0,01 18,93 4,27 13,41 
334 194 0,0126 0,02 17,44 0,61 8,10 
© Revue MODULAD, 2004 - 47 - Numéro 31 
278 119 -0,008 0,01 17,29 6,33 15,65 
334 216 -0,0065 0,02 16,64 0,79 9,62 
437 240 0,0129 0,03 14,88 4,17 4,24 
492 216 0,0108 0,02 14,65 3,14 5,36 
334 301 0,0007 0,01 14,61 6,39 15,71 
307 216 -0,0053 0,01 14,40 2,49 11,48 
492 315 -0,0045 0,01 13,92 3,16 12,20 
293 137 -0,0101 0,01 12,91 4,16 13,30 
353 226 -0,0083 0,02 12,26 0,20 8,98 
451 295 0,0165 0,02 10,67 0,75 7,95 
413 216 -0,0158 0,03 10,46 4,17 4,24 
281 216 -0,007 0,01 10,04 4,23 13,37 
Tableau 9 : Points aux coefficients du krigeage instables 
 
 
absc ord. KS (x) PLS (λ) 
x
x∆  (en %) ∆λ/λ 
(en %) 
∆c/c  
(en %) 
367 272 -0,0106 0,01 9,47 2,23 11,20 
367 250 -0,0146 0,02 7,70 0,81 9,65 
286 182 0,052 0,02 7,52 1,69 10,60 
449 268 0,0278 0,02 6,74 2,59 5,95 
286 288 -0,0013 0,01 6,50 8,78 18,32 
442 228 0,0411 0,03 5,88 5,19 3,13 
492 282 -0,0063 0,02 5,58 1,02 9,87 
413 285 -0,016 0,02 5,09 0,13 8,91 
355 118 -0,0209 0,02 5,05 1,33 10,21 
432 140 0,035 0,03 5,01 4,85 3,50 
288 164 -0,009 0,01 4,48 2,70 11,71 
444 190 0,3092 0,04 4,39 7,59 0,51 
444 119 0,0685 0,03 4,07 4,30 4,09 
466 216 0,0407 0,03 3,37 4,75 3,60 
© Revue MODULAD, 2004 - 48 - Numéro 31 
288 311 -0,0005 0,01 3,19 10,22 19,88 
439 216 0,0544 0,03 3,18 5,76 2,50 
360 216 -0,0134 0,02 3,02 0,89 7,80 
492 150 0,0384 0,03 2,89 3,26 5,22 
358 139 -0,0181 0,02 2,75 0,17 8,58 
442 160 0,1596 0,04 2,67 6,62 1,56 
413 172 0,1126 0,03 2,03 5,62 2,66 
386 216 -0,0171 0,02 1,41 2,54 6,00 
274 269 0,0009 0,01 1,20 8,30 17,79 
276 206 -0,0086 0,01 1,10 3,88 12,99 
442 204 0,123 0,04 0,57 6,62 1,56 
370 180 0,0979 0,03 0,42 3,48 4,98 
492 249 -0,0059 0,02 0,38 1,08 7,59 
346 210 -0,0039 0,02 0,27 0,36 8,37 
Tableau 10 Points aux coefficients du krigeage stables 
 
Les données figurent parmi ceux qui illustrent le logiciel Variowin de Pannatier, 
1996 [10]. Le taux de cadmium varie entre 0 et 17.5 ppm. La fonction c(h) 
ajustée aux données est une covariance exponentielle isotrope définie par: c (h) = 
13.25 exp(-h/b), h étant la distance euclidienne dans le plan horizontal, et b = 
142.89 est un paramètre de la portée. La variance est σz = 16.3 et la valeur 
moyenne du processus est m = 6.65. La covariance perturbée q est obtenue à 
partir de c en faisant augmenter la portée de 10%. Nous nous proposons d’estimer 
la valeur de la variable aléatoire au point observé Mo(xo = 492, yo = 150) à partir 
des 59 autres observations. Le nombre de conditionnement obtenu est 85. Les 
tableaux 9 et 10 présentent les coordonnées des points estimés, les coefficients du 
krigeage et de la régression PLS, ainsi que les variations relatives des coeffients 
exprimées en pourcentages, puis en dernière colonne la variation relative du 
vecteur c. Concernant la précision des estimateurs, on obtient  0.63 pour le ks et 
1.2 pour la pls. On constate que malgré le nombre de conditionnement assez 
élevé, nous avons une stabilité dans la précision pour les deux estimateurs 
étudiées ; par contre la variation relative des coefficients du krigeage est 
supérieure à 10% pour 28 points estimés (tableau 9) et inférieure à 10% pour les 
31 autres (tableau 10) alors que les coefficients de l’estimation par régression PLS 
sont tous stables. 
 
 
© Revue MODULAD, 2004 - 49 - Numéro 31 
4 Conclusions: 
Dans cette étude de la robustesse nous avons montré que quand la matrice des 
covariances est bien conditionnée, avec un nombre de conditionnement de l’ordre 
de l’unité, le krigeage et la régression PLS ont des coefficients d’estimation stables 
vis à vis de petites perturbations effectuées sur le paramètre de la portée de la 
fonction de covariance. Par ailleurs, la variation relative des coefficients du krigeage 
dépend peu du vecteur c. Le krigeage permet ainsi, quand la matrice des 
covariances est bien conditionnée, comme le montre l’exemple 7, d’estimer avec la 
même stabilité, différents points à partir de la même configuration des 
observations. Par contre, l’estimateur par régression PLS présente une variation 
relative des coefficients d’estimation bornée et du même ordre de grandeur que celle 
du vecteur des covariances, et ce indépendamment du conditionnement de la 
matrice de covariances. Dans le cas où cette matrice est mal conditionnée, 
l’estimateur par régression PLS est plus intéressant que le krigeage qui peut être 
alors très instable comme l’ont montré différents exemples . Quand à la précision 
de l’estimation, elle a été robuste dans tous les exemples traités, et ce aussi bien 
pour le krigeage que pour la régression PLS. Théoriquement cette dernière 
méthode peut avoir une variation relative de la variance plus faible que celle du 
krigeage quand le nombre de conditionnement τ(K) est très élevé. 
Enfin, cette étude consolide l’apport de l’estimation spatiale par régression PLS 
et confirme son intérêt comme méthode complémentaire à l’estimateur optimal au 
sens des moindres carrés qu’est le krigeage. 
 
Références : 
[1] Ajerame M.1997 : Géostatistique appliquée à la quantification du risque; thèse 
de doctorat à la faculté des sciences agronomiques de LOUVAIN 
[2] Armstrong M. et Wackernagel H.1988 : The influence of the covariance function on 
the kriged estimator; Sciences de la terre, série informatique géologique, Nancy, 
27, 245-262. 
[3] Bardossy A. 1988 : Notes on the robustness of the kriging system. Mathematical 
Geology, 20, 189-203. 
[4] Chilès J.P. and Delfiner P. 1999 : Geostatistics: Modeling Spatial Uncertainty, 
Wiley, New York, USA. 
[5] Coeurjolly J. F. 2000: Inférence statistique pour les mouvementsBrowniens 
fractionnaires et multifrac tionnaires Thèse de Doctorat de l’Université Joseph 
Fourier, Laboratoire de modélisation et Calcul. 
[6] Cressie 1991 : Statistics for spatial data; John Wiley & sons, Inc. 
[7] Diamong P. et Armstrong M. 1984 : Robustness of variogramsand condi-
tioninig of kriging matrices, Mathematical Geology, 16, 809-822. 
 [8] Elkettani M.Y. 2001 : Analyse des redondances et régression PLS appliquées 
aux données spatiales. Comparaison avec l’estimation par krigeage et par inverse 
de la distance. Revue de Statistique Appliquée; 49, 2, 67-82. 
© Revue MODULAD, 2004 - 50 - Numéro 31 
[9]  Jaffard S. 1990 Propriétés des matrices bien localisées près de leur diagonale et 
quelques applications. Ann. Inst. Henri Poincaré, vol 7, no 5, p 461 - 476. 
[10] Pannatier 1996) Data Analysis in 2D, Springer-Verlag. 
[11] Putter H.et Young G.A. 2001 : On the effect of covariance function esti-
mation on the accuracy of kriging predictors Bernoulli, 7, 421-438. 
[12] Rivoirard 1984 : Le comportement des poids de krigeage; thèse de Docteur-
Ingénieur, Ecole des mines de Paris. 
[13] Stein, M.L. 1988 Asymptotically efficient prediction of a random field with a 
mispecified variance function, Ann. Stat.; 16, 55-63. 
[14] Tenenhaus, M.1998 : La régression PLS, théorie et pratique; éditions TECHNIP. 
[15] K. Vanden Branden et M. Hubert 2003 : Robustness properties of a robust 
PLS regression, site internet du CEMAGREF, Publications-du projet 
VISHNU-CAPORAL. 
[16] Warnes, J.J. 1986 : A sensitivity analysis for universal kriging, Mathematical 
Geology, 18, 653-676. 
[17] Wold, Albano et al 1983 : Pattern recognition: Finding and using regu-
larities in multivariate data; Proc IUFOST conf. ”Food research and data 
analysis” , Martens J. ed, Applied sciences publications. London.2 
