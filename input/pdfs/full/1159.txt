EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
122© Revue MODULAD, 2007
Numéro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
123© Revue MODULAD, 2007
Numéro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
124© Revue MODULAD, 2007
Numéro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
125© Revue MODULAD, 2007
Numéro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
126© Revue MODULAD, 2007
Numéro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
127© Revue MODULAD, 2007
Numéro 36
EUROPEAN WORKSHOP ON DATA STREAM ANALYSIS • March, 14-16, 2007 • Caserta, Italy
128© Revue MODULAD, 2007
Numéro 36
1Real-time Ranking of 
Electrical Feeders using 
Expert Advice
Hila Becker 1,2, Marta Arias 1
1Center for Computational Learning Systems
2Computer Science Department
Columbia University
2
Overview
 The problem
› The electrical system
› Available data 
Approach
Challenges
Our solution using Online learning
 Experimental results
23
The Electrical System
1. Generation 2. Transmission
3. Primary
Distribution
4. Secondary Distribution
4
The Problem
Distribution feeder failures result in 
automatic feeder shutdown
› called “Open Autos” or O/As 
O/As stress networks, control centers, and 
field crews 
O/As are expensive ($ millions annually)
 Proactive replacement is much cheaper 
and safer than reactive repair
How do we know which feeders to fix?
35
Some facts about feeders and 
failures
mostly 0-5 failures 
per day
more in the 
summer
 strong seasonality 
effects
6
Feeder data
 Static data
› Compositional/structural
› Electrical
Dynamic data
› Outage history (updated daily)
› Load measurements (updated every 15 
minutes)
 Roughly 200 attributes for each feeder
› New ones are still being added.
47
Overview
The problem
› The electrical system
› Available data 
Approach
Challenges
Our solution using Online learning
Experimental results
8
Machine Learning Approach
Leverage Con Edison’s domain 
knowledge and resources
Learn to rank feeders based on failure 
susceptibility
How?
› Assemble data
› Train ranking model based on past data
› Re-rank frequently using model on current 
data
59
Feeder Ranking Application
Goal: rank feeders according to failure 
susceptibility
› High risk placed near the top
Integrate different types of data
Interface that reflects the latest state of 
the system
› Update feeder ranking every 15 min.
10
Application Structure
Static data
SQL
Server
DB
ML
Engine
ML
ModelsRankings
Decision
Support GUI
Action
Driver
Action
Tracker
Decision Support App
Outage data
Xfmr Stress data
Feeder Load data
611
Decision Support GUI
12
Overview
The problem
› The electrical system
› Available data 
Approach
Challenges
Our solution using Online learning
Experimental results
713
Simple Solution
Supervised batch-learning algorithms
› Use past data to train a model
› Re-rank frequently using this model on current 
data
Use the best performing learning 
algorithm
› How do we measure performance?
› MartiRank - boosting algorithm by [Long & 
Servedio, 2005]
x Use MartiRank for dynamic feeder ranking
14
Performance Metric
Normalized average rank of failed 
feeders
feedersfailures
failurerank
i
i
##
)(
1 ∗−
∑
815
Performance Metric Example
feedersfailures
failurerank
i
i
##
)(
1 ∗−
∑
5833.0
8*3
5321 =++−
08
07
06
15
04
13
12
01
ranking outages
16
Real-time ranking with MartiRank
MartiRank is a “batch” learning 
algorithm
Deal with changing system by:
› generating new datasets with latest data
› Re-training new model, replacing old 
model
› Using newest model to generate ranking
Must implement “training strategies”
917
Real-time ranking with MartiRank
time
time
time
18
How to measure performance 
over time
 Every ~15 minutes, generate new ranking 
based on current model and latest data
Whenever there is a failure, look up its 
rank in the latest ranking before the 
failure
After a whole day, compute normalized 
average rank
10
19
Using MartiRank for real-time 
ranking of feeders
MartiRank seems to work well, but..
› User decides when to re-train
› User decides how much data to use for re-
training
› Performance degrades over time
Want to make system automatic
› Do not discard all old models
› Let the system decide which models to use
20
Overview
The problem
› The electrical system
› Available data 
Approach
Challenges
Our solution using Online learning
Experimental results
11
21
Learning from expert advice
Consider each model as an expert
Each expert has associated weight
› Reward/penalize experts with good/bad 
predictions
› Weight is a measure of confidence in expert’s 
prediction
Predict using weighted average of top-
scoring experts
22
Learning from expert advice
Advantages
› Fully automatic
› Adaptive
› Can use many types of underlying learning 
algorithms
› Good performance guarantees from learning 
theory: performance never too far off from best 
expert in hindsight
Disadvantages
› Computational cost: need to track many models 
“in parallel”
12
23
Weighted Majority Algorithm 
[Littlestone & Warmuth ‘88]
 Introduced for binary classification
› Experts make predictions in [0,1]
› Obtain losses in [0,1]
Pseudocode:
› Learning rate as main parameter, ß in (0,1]
› There are N “experts”, initially weight is 1 for all
› For t=1,2,3, …
x Predict using weighted average of experts’
prediction
x Obtain “true” label; each expert incurs loss li
x Update experts’ weights using wi,t+1 = wi,t • pow(ß,li)
24
Weighted Majority Algorithm 
e1 . . .e2 e3 eNN Experts
w1 w2 w3 wN
1 0 0 1
?
w1*1 +w2*0 +w3*0 +       . . .        +wN*1
>0.5 <0.5
1 0
1
- Calculate li = loss(i) for i=1,…,N
- Update: wi,t+1 = wi,t • pow(ß,li) for learning rate ß, time t and i=1,..,N
13
25
In our case, can’t use WM 
directly
Use ranking as opposed to binary 
classification
More importantly, do not have a fixed set 
of experts
26
Dealing with ranking vs. binary 
classification
 Ranking loss as normalized average rank 
of failures as seen before, loss in [0,1]
 To combine rankings, use a weighted 
average of feeders’ ranks
14
27
Dealing with a moving set of 
experts
Introduce new parameters
› B: “budget” (max number of models) set to 
100
› p: new models weight percentile in [0,100]
› α: age penalty in (0,1]
If too many models (more than B), drop 
models with poor q-score, where
› qi = wi • pow(α, agei)
› I.e., α is rate of exponential decay
28
Online Ranking Algorithm 
e1 . . .e2 e3 eB
w1 w2 w3 wB
?
F1
F4
F3
F2
F5
F4
F2
F1
F3
F5
F1
F3
F5
F4
F2
F1
F3
F4
F2
F5
F1
F3
F4
F2
F5
F3
F1
F4
F2
F5
eB+1 eB+2
wB+1 wB+2
15
29
Other parameters
How often do we train and add new models?
› Hand-tuned over the course of the summer
› Alternatively, one could train when observed 
performance drops .. not used yet
How much data do we use to train models?
› Based on observed performance and early 
experiments
x 1 week worth of data, and
x 2 weeks worth of data
30
Overview
The problem
› The electrical system
› Available data 
Approach
Challenges
Our solution using Online learning
Experimental results
16
31
Experimental Comparison
Compare our approach to
› Using “batch”-trained models
› Other online learning methods
Ranking Perceptron
› Online version
Hand Picked Model
› Tuned by humans with domain knowledge
32
Performance – Summer 2005
17
33
Performance – Winter 2006
34
Parameter Variation - Budget
18
35
Future Work
Concept drift detection
› Add new models only when change is 
detected
Ensemble diversity control
Exploit re-occurring contexts
