S√©lection supervis√©e d‚Äôinstances : une approche descriptive
Sylvain Ferrandiz;, Marc Boull√©
France T√©l√©com R&D,
2, avenue Pierre Marzin, 22300 Lannion
sylvain.ferrandiz@francetelecom.com,
marc.boulle@francetelecom.com,
GREYC, Universit√© de Caen,
boulevard du Mar√©chal Juin, BP 5186, 14032 Caen Cedex,
R√©sum√©. La classification suivant le plus proche voisin est une r√®gle simple et
performante. Sa mise en oeuvre pratique n√©cessite, tant pour des raisons de co√ªt
de calcul que de robustesse, de s√©lectionner les instances √† conserver. La parti-
tion de Voronoi induite par les prototypes constitue la structure sous-jacente √†
cette r√®gle. Dans cet article, on introduit un crit√®re descriptif d‚Äô√©valuation d‚Äôune
telle partition, quantifiant le compromis entre nombre de cellules et discrimi-
nation de la variable cible entre les cellules. Une heuristique d‚Äôoptimisation est
propos√©e, tirant partie des propri√©t√©s des partitions de Voronoi et du crit√®re. La
m√©thode obtenue est compar√©e avec les standards sur une vingtaine de jeux de
donn√©es de l‚ÄôUCI. Notre technique ne souffre d‚Äôaucun d√©faut de performance
pr√©dictive, tout en s√©lectionnant un minimum d‚Äôinstances. De plus, elle ne sur-
apprend pas.
1 Introduction
La classification supervis√©e constitue un probl√®me d‚Äôapprentissage classique. On dispose
dans ce cas, en plus des variables descriptives (ou endog√®nes), d‚Äôune variable cible (ou exo-
g√®ne). En phase d‚Äôexploration des donn√©es, c‚Äôest la d√©pendance de la variable cible vis-√†-vis
des variables descriptives qu‚Äôon vise √† expliciter. En phase de mod√©lisation, le but est de four-
nir la meilleure pr√©diction possible pour toute nouvelle instance √† classifier. Quelle que soit la
situation, la connaissance est √† extraire d‚Äôun √©chantillon de N instances √©tiquet√©es.
Une m√©thode de classification usuelle est la r√®gle de classification suivant le plus proche
voisin introduite par Fix et Hodges (1951). Elle consiste √† attribuer √† une instance l‚Äô√©tiquette
de l‚Äôinstance la plus proche parmi celles constituant l‚Äô√©chantillon. La mise en ≈ìuvre de cette
mod√©lisation soul√®ve deux questions fondamentales :
‚Äì Quelle mesure de similitude employer ?
‚Äì Quelles instances de l‚Äô√©chantillon conserver ?
La premi√®re question couvre plusieurs champs d‚Äôinvestigation : gestion de la pr√©sence
jointe de variables continues et symboliques, normalisation des variables continues, pr√©trai-
tement des variables symboliques, pond√©ration de la contribution des variables, etc. Dans le
cas continu, l‚Äôusage a consacr√© l‚Äôemploi de la distance euclidienne et des distances Lp (p  1)
- 421 - RNTI-E-6
S√©lection supervis√©e d‚Äôinstances
de Minkowski. La distance de Mahalanobis (Duda et al., 2001), en effectuant une transforma-
tion globale des instances et au prix d‚Äôun co√ªt de calcul plus √©lev√©, permet d‚Äôint√©grer dans le
calcul de la similitude les corr√©lations entre couples de variables descriptives. Pour des mesures
successives d‚Äôune m√™me quantit√©, le Dynamic Time Warping est un proc√©d√© traitant la corr√©-
lation temporelle (Berndt et Clifford, 1996). Dans le cas symbolique, la distance de Hamming
est d‚Äôautant plus simplificatrice que le nombre de modalit√©s des variables cro√Æt. C‚Äôest pourquoi
des mesures de similitude bas√©es sur les probabilit√©s d‚Äôoccurence sont souvent utilis√©es, par
Stanfill et Waltz (1986) entre autres. Un proc√©d√© de gestion de la mixit√© (variables continues et
symboliques) est propos√© par Wilson et Martinez (1997a). On ne s‚Äôint√©resse pas dans cet article
√† la question du choix d‚Äôune mesure de similitude et on se focalise sur la s√©lection d‚Äôinstance.
La classification par le plus proche voisin impose, pour chaque nouvelle instance √† clas-
sifier, de parcourir l‚Äôensemble de l‚Äô√©chantillon. Ceci entra√Æne un co√ªt de d√©ploiement r√©dhibi-
toire. Une √©tape de s√©lection des instances permet de diminuer le co√ªt de recherche. Classique-
ment, les m√©thodes pr√©dictives s‚Äôattellent √† qualifier le degr√© d‚Äôutilit√© pr√©dictive d‚Äôune instance
et conservent les instances jug√©es utiles. Ces m√©thodes sont d√©crites √† la section 2.
Les instances s√©lectionn√©es (ou : prototypes) induisent une partition de Voronoi de l‚Äôes-
pace, √† chaque groupe √©tant associ√© un prototype. L‚Äôensemble des instances se r√©partit dans
les groupes de cette partition. On consid√®re alors la distribution des √©tiquettes dans chacun des
groupes. L√† o√π les m√©thodes pr√©dictives ne prennent en compte que l‚Äô√©tiquette du prototype ,
on associe √† chaque prototype cette distribution de probabilit√©. Cette dichotomie refl√®te celle
observ√©e dans tout processus de fouille de donn√©es entre la phase de pr√©paration des donn√©es
et celle de mod√©lisation (Chapman et al., 2000). On pr√©sente √† la section 3 une approche des-
criptive de l‚Äô√©valuation de la qualit√© de telles fonctions, ainsi que le crit√®re qui en d√©coule. La
question du sur-apprentissage √©tant prise en charge par le crit√®re, on propose √† la section 4 une
heuristique d‚Äôoptimisation pouss√©e. Enfin, une comparaison exp√©rimentale sur donn√©es r√©elles
est men√©e √† la section 5.
2 La s√©lection d‚Äôinstances
La m√©thode CNN (pour Condensed Nearest Neighbor) d√©crite par Hart (1968) est la plus
ancienne m√©thode de s√©lection d‚Äôinstances. Toute instance mal classifi√©e par son plus proche
voisin parmi les prototypes d√©j√† s√©lectionn√©s est aussit√¥t conserv√©e. Ce proc√©d√© incr√©mental est
it√©r√© tant qu‚Äôil existe des instances mal classifi√©es par l‚Äôensemble de prototypes. La m√©thode
est consistante, dans le sens o√π tout √©l√©ment de l‚Äô√©chantillon est bien classifi√© par son plus
proche prototype. La complexit√© au pire de cet algorithme est un O(N 3).
Une am√©lioration est propos√©e par Gates (1972) : RNN (pour Reduced Nearest Neighbor).
Une fois la r√®gle CNN appliqu√©e, toute suppression d‚Äôun prototype ne provoquant aucune
mauvaise classification d‚Äôune instance est valid√©e. L‚Äôint√©r√™t de cette m√©thode r√©side dans sa
capacit√© √† produire un sous-ensemble de prototypes de taille minimale relativement √† la condi-
tion de consistance, sous r√©serve qu‚Äôun tel sous-ensemble soit inclus dans la solution propos√©e
par CNN.
Les d√©cisions prises par ce type de techniques sont peu robustes (conservation du bruit, par
exemple). Afin d‚Äôy rem√©dier, une id√©e consiste √† prendre en compte lesK plus proches voisins
(typiquement, K = 3). Le proc√©d√© est d√©cr√©mental et une instance est √©limin√©e si elle est mal
classifi√©e par un vote √† la majorit√© sur ses K plus proches voisins. C‚Äôest la r√®gle ENN, pour
- 422 -RNTI-E-6
S. Ferrandiz et M. Boull√©
Edited Nearest Neighbor, pr√©sent√©e par Wilson (1972). Sa complexit√© algorithmique est un
O(KN2). L‚Äô√©limination est ainsi fiabilis√©e mais limit√©e. Notons que cette m√©thode peut √™tre
appliqu√©e it√©rativement.
Les pr√©c√©dentes m√©thodes √©tant de complexit√© √©lev√©e ou ne r√©alisant pas l‚Äôobjectif d‚Äôune
s√©lection drastique, d‚Äôapr√®s Wilson et Martinez (2001), Aha et al. (1991) ont propos√© une s√©rie
d‚Äôalgorithmes dont IB3 est la version la plus aboutie. Une notion d‚Äôacceptabilit√© est introduite
dans CNN : une instance est conserv√©e si elle est mal classifi√©e par le plus proche prototype
acceptable. La complexit√© algorithmique d‚ÄôIB3 est un O(N 2). Un prototype est acceptable si
son taux de bonne classification est significativement sup√©rieur √† la fr√©quence de sa classe. De
plus, sont √©limin√©s les prototypes peu acceptables. Ceci permet de fiabiliser les d√©cisions tout
en √©vitant d‚Äô√™tre trop conservateur.
Dans ce but √©galement, la notion d‚Äôassociation est utilis√©e par Wilson et Martinez (1997b).
Pour K  N est fix√©, si x est l‚Äôun des K plus proches voisins de y, on dit que y est associ√© √†
x. Autrement dit, x est associ√© √† y s‚Äôil participe √† la classification de y. D√®s lors, x est √©limin√©
si le nombre de ses associ√©s bien classifi√©s ne diminue pas apr√®s sa suppression. La r√®gle ENN
est pr√©liminairement appliqu√©e. De plus, les instances sont consid√©r√©es par ordre d√©croissant
de distance √† la plus proche instance de classe diff√©rente. La m√©thode obtenue, DROP3, est de
complexit√© un O(KN2).
A la crois√©e des chemins entre IB3 et DROP3, on trouve un test statistique √©valuant l‚Äôhy-
poth√®se de non contribution d‚Äôune instance √† la classification de ses associ√©s (Sebban et al.,
2002). Ce crit√®re est param√©trique et son calcul n√©cessite l‚Äôapproximation de la densit√© de la
statistique associ√©e. Une adaptation de l‚Äôalgorithme AdaBoost permettant de traiter des clas-
sifieurs locaux (les prototypes) aboutit √† une heuristique de recherche incr√©mentale. Dans sa
version la plus rapide, l‚Äôalgorithme est de complexit√© un O(KN 2).
Cameron-Jones (1995) a propos√© un crit√®re d‚Äô√©valuation de la qualit√© pr√©dictive d‚Äôun en-
semble de prototypes. L‚Äôapproche adopt√©e est de type MML (pour MinimumMessage Length)
et le crit√®re obtenu s‚Äô√©crit :
c(K;N;E) = F (K;N) +K log2(J) + F (E;N  K) + E log2(J   1);
avecK le nombre de prototypes, N le nombre d‚Äôinstances, E le nombre d‚Äôinstances mal clas-
sif√©es par leur plus proche prototype et J le nombre de classe cibles. La quantit√© F (U; V )
mesure la longueur du mot de code n√©cessaire √† la sp√©cification de U instances parmi V et est
√©valu√©e par la formule :
F (U; V ) = log
 
UX
u=0

V
u
!
;
o√π log(x) d√©signe la somme des termes positifs log2(x), log2(log2(x)), etc. Les termes
K log2(J) etE log2(J 1) correspondent aux longueurs de code n√©cessaires √† la sp√©cification
des √©tiquettes desK prototypes et des E exceptions respectivement.
Une heuristique est √©galement propos√©e, qu‚Äôon nomme ici Explore. Une premi√®re phase
it√©rative consiste √† ajouter une instance si la valeur du crit√®re diminue. Une fois toutes les
instances consid√©r√©es, tout prototype dont la suppression conduit √† la diminution de la valeur du
crit√®re est effectivement √©limin√©. Enfin, 1000 mutations sont √©valu√©es et accept√©es si la valeur
du crit√®re d√©cro√Æt. Une mutation est soit un ajout d‚Äôune instance √† l‚Äôensemble des prototypes,
soit une suppression d‚Äôun prototype, soit un √©change entre une instance et un prototype. Au
final, la m√©thode est de complexit√© un O(N 3) au pire et proche d‚Äôun O(N 2) en moyenne.
- 423 - RNTI-E-6
S√©lection supervis√©e d‚Äôinstances
3 Une approche descriptive
L‚Äôapproche pr√©dictive classique consiste √† √©valuer la qualit√© pr√©dictive d‚Äôun classifieur, en
mesurant le risque structurel empirique par exemple. C‚Äôest le parti pris par l‚Äôensemble des m√©-
thodes de s√©lection. On s‚Äôint√©resse pour notre part √† la qualit√© de la distribution des √©tiquettes
conditionnellement aux instances. D√®s lors, une mesure de cette qualit√© doit √™tre propos√©e et
on adopte ici une approche descriptive.
3.1 Notations
Fixons les notations. On dispose d‚Äôun √©chantillon fini D = fXn; Yng de N instances
√©tiquet√©es. On note D(x) = fXng l‚Äôensemble des instances et D(y) = fYng leurs √©tiquettes.
Les √©tiquettes appartiennent √† un alphabet L = fljg de taille J et les instances √† un ensemble
X. Cet ensemble est muni d‚Äôune mesure de similitude  : X X! R+.
Si P  X, la partition de Voronoi V (P ) = (V (p))p2P associ√©e √† P est d√©finie par :
8p 2 P; V (p) =

x 2 X; p = argmin
p02P
(x; p0)

:
Pour p 2 P , la cellule de Voronoi V (p) contient les points x pour lesquels p est l‚Äô√©l√©ment de
P le plus similaire, relativement √† . L‚Äô√©l√©ment p est appel√© prototype de la cellule V (p). La
figure 1 donnent des exemples de telles partitions.
FIG. 1 ‚Äì Exemple de partitions de Voronoi.
On d√©finit ici un mod√®le (descriptif) comme un couple (v;  ) o√π v est une partition de
Voronoi form√©e deK cellules et  une matrice de taille (K;J) dont le coefficient (k; j) donne
la probabilit√© de l‚Äô√©tiquette j dans la cellule k. Autrement, √† chaque cellule (i.e. √† chaque
prototype) est associ√©e une distribution de probabilit√© sur L.
Si v est une partition de Voronoi compos√©e de K cellules v1; : : : ; vK , le cardinal de
D(x)
T
vk est not√© Nk (1  k  K) et le cardinal de fXn 2 D(x)
T
vk; Yn = ljg est
not√© Nkj . Ainsi, N = N1 +   +NK et Nk = Nk1 +   +Nkj .
- 424 -RNTI-E-6
S. Ferrandiz et M. Boull√©
3.2 Formalisation
On consid√®re le mod√®le comme √©tant al√©atoire et on cherche √† sp√©cifier la probabilit√© jointe
P (V;	; D(y)=D(x)) du mod√®le (V;	) et des √©tiquettes D(y) connaissant les instances D(x).
Cette probabilit√© est d√©compos√©e √† l‚Äôaide de la formule des probabilit√©s it√©r√©es. Du fait qu‚Äôon
s‚Äôint√©resse √† cette probabilit√©, et non √† la probabilit√© de mauvaise classification du classifieur
associ√©, comme d√©crit par Vapnik (1996), on qualifie l‚Äôapproche de descriptive.
Plus pr√©cis√©ment, siK d√©signe le nombre de cellules de V , on commence par √©crire :
P (V;	; D(y)=D(x)) = P (K;V;	; D(y)=D(x));
ce qui permet de comparer des ensembles de prototypes de diff√©rentes tailles. On it√®re ensuite
la d√©pendance en utilisant la formule de Bayes :
P (V;	; D(y)=D(x)) = P (K=D(x))P (V=K;D(x))P (	; D(y)=K; V;D(x)):
On suppose les comportements des distributions dans chaque cellule conditionnellement
ind√©pendants, ce qui donne :
P (	; D(y)=K; V;D(x)) =
KY
k=1
P (	k; D
(y)
k =Vk; D
(x)
k );
avec 	k la ligne k de 	, Vk la keme cellule de V , D(x)k les instances tombant dans Vk et D
(y)
k
leurs √©tiquettes. On applique une nouvelle fois la r√®gle de Bayes et on obtient :
P (V;	; D(y)=D(x)) = P (K=D(x))P (V=K;D(x))
KY
k=1
P (	k=Vk; D
(x)
k )P (D
(y)
k =Vk;	k; D
(x)
k ):
3.3 Sp√©cification
La probabilit√© P (V;	; D(y)=D(x)) a √©t√© d√©compos√©e √† l‚Äôaide de la formule de Bayes.
On sp√©cifie maintenant chacune des probabilit√©s, en pr√©cisant √† chaque √©tape le support de la
probabilit√© concern√©e et en appliquant un a priori uniforme.
En ce qui concerne le nombre de cellules, i.e. la probabilit√© P (K=D(x)), les valeurs pos-
sibles deK sont comprises entre 1 et N . L‚Äôapplication de l‚Äôa priori uniforme donne :
P (K=D(x)) =
1
N
:
La partition de Voronoi est caract√©ris√©e uniquement par ses prototypes. Les ensembles de
prototypes consid√©r√©s sont les parties de D(x). Adopter un a priori uniforme nous conduirait
√† introduire le coefficient binomial
 
N
K

, puisque l‚Äôon doit choisir K prototypes parmi les N
instances. Mais ce coefficient est sym√©trique relativement √† K. Comme on pr√©f√®re les valeurs
faibles de K, on utilise le coefficient
 
N+K 1
K 1

, croissant avec K, proche de
 
N
K

pour les
faibles valeurs deK, nul pourK = 1, caract√©risant ainsi plus finement notre pr√©f√©rence :
P (V=K;D(x)) =
1
 
N+K 1
K 1
 :
- 425 - RNTI-E-6
S√©lection supervis√©e d‚Äôinstances
Dans la keme cellule, on exploite la d√©pendance aux donn√©es et on restreint le support des
distributions possibles aux probabilit√©s rationelles avecNk pour d√©nominateur. Formellement,
le support est 8
<
:

nk1
Nk
; : : : ;
nkJ
Nk

;
JX
j=1
nkj = Nk
9
=
;
;
dont le cardinal est
 
Nk+J 1
J 1
 (1  k  K). L‚Äôadoption d‚Äôun a priori uniforme donne :
P (	k=Vk; D
(x)
k ) =
1
 
Nk+J 1
J 1
 :
La partition et les fr√©quences des classes cibles dans chaque cellule sont √† ce stade connues.
Il reste √† sp√©cifier les √©tiquettes de chaque instance dans chaque cellule. Dans chaque cellule,
le support est restreint relativement √† la d√©pendance : pour la keme cellule (1  k  K), le
probl√®me revient √† placer les √©l√©ments de la cellule dans J urnes, sous la contrainte d‚Äôeffectif
Nkj dans la jeme urne (1  j  J). Le coefficient multinomial donne le nombre exact de ces
possibilit√©s et on obtient :
P (D
(y)
k =Vk; D
(x)
k ) =
1
Nk!
Nk1!:::NkJ !
:
Au final, en prenant l‚Äôoppos√© du logarithme de P (M;D(y)=D(x)), un mod√®le descriptifM
est √©valu√© par la formule suivante :
c(M) = logN + log

N +K   1
K   1

+
KX
k=1
log

Nk + J   1
J   1

+
KX
k=1
log
Nk!
Nk1! : : : NkJ !
:
Le premier terme correspond √† la description du nombre de groupes, le second √† la description
des prototypes, le troisi√®me √† la description des fr√©quences des √©tiquettes dans les cellules et
le dernier √† la description de l‚Äôattribution des √©tiquettes aux instances dans les cellules.
Notons que, d‚Äôapr√®s l‚Äôapproximation de Stirling (log x!  x log x x+O(log x)), le dernier
terme de la formule se comporte asymptotiquement comme N fois l‚Äôentropie conditionnelle
de la distribution des Yn en connaissance de la fonction d‚Äôassignement associ√©e √† la partition :
1
N
KX
k=1
log
Nk!
Nk1! : : : NkJ !
  
KX
k=1
JX
j=1
Nkj
N
log
Nkj
Nk
:
4 Heuristique d‚Äôoptimisation
On dispose d‚Äôun crit√®re √©valuant tout sous-ensemble de l‚Äôensemble des instances. L‚Äôespace
de recherche a pour cardinal 2N , rendant la recherche exhaustive peu r√©aliste. On propose une
nouvelle heuristique, encapsulant une optimisation gloutonne descendante d‚Äôun ensemble de
prototypes dans une m√©ta-heuristique de recherche √† voisinage variable.
- 426 -RNTI-E-6
S. Ferrandiz et M. Boull√©
4.1 Optimisation gloutonne d‚Äôun ensemble de prototypes
L‚Äôheuristique gloutonne GLOUTON(P ) s‚Äôapplique √† tout ensemble P de p prototypes.
Chaque ensemble obtenu par suppression d‚Äôun √©l√©ment de P est √©valu√©. Parmi ces ensembles,
celui minimisant le crit√®re est d√©clar√© vainqueur de l‚Äô√©tape. Ce proc√©d√© est it√©r√© par application
aux vainqueurs successifs jusqu‚Äô√† l‚Äô√©valuation finale d‚Äôun singleton. Le meilleur ensemble
rencontr√© lors du parcours est renvoy√©. Autrement dit, l‚Äôalgorithme GLOUTON(P ) s‚Äô√©crit :
‚Äì S  P
‚Äì PourK = p  1 √† 1 Faire
‚Äì S  le meilleur sous-ensemble de S obtenu par suppression d‚Äôun √©l√©ment de S
‚Äì Retourner le meilleur ensemble de prototypes rencontr√©
Cette m√©thode √©value un O(p2) ensembles et chaque √©valuation n√©cessite pour chaque ins-
tance de rechercher son plus proche prototype. GLOUTON(P ) poss√®de donc basiquement une
complexit√© temporelle un O(Np3). Des astuces d‚Äôimplantation r√©duisent la complexit√© algo-
rithmique √† un O(Np log p).
A chaque √©tape, toute suppression d‚Äôun prototype conduit √† r√©attribuer uniquement les
instances appartenant √† la cellule de ce prototype. A l‚Äô√©tapeK, les N instances ne sont donc √†
traiter qu‚Äôune fois, pour un co√ªt qu‚Äôon peut rendre constant.
En effet, si l‚Äôon dispose pour chaque instance de la liste tri√©e des √©l√©ments de P par distance
croissante, l‚Äôacquisition du plus proche prototype suivant se fait √† co√ªt constant. L‚Äôalgorithme
GLOUTON(P ) se voit donc adjoindre une phase d‚Äôinitialisation qui devient pr√©pond√©rante
avec une complexit√© temporelle un O(Np log p) (construction de N listes tri√©es de taille p).
Le stockage de ces N listes induit une complexit√© spatiale en O(Np).
La valeur du crit√®re est √©galement mise √† jour √† co√ªt constant. Seuls les deux derniers
termes d√©pendent de la r√©partition des instances dans les cellules. La suppression d‚Äôun pro-
totype conduit tout d‚Äôabord √† soustraire sa participation √† la valeur du crit√®re. Ensuite, la
r√©attribution d‚Äôune instance √† son plus proche prototype k suivant induit une simple incr√©-
mentation unitaire des compteurs Nk et Nkj0 , o√π j0 est l‚Äôindice de la classe √† laquelle appar-
tient l‚Äôinstance. Le terme du crit√®re port√© par le prototype k est donc mis √† jour en ajoutant
log(Nk + J)  log(Nkj0 + 1).
Notons que l‚Äôon peut introduire dans GLOUTON une contrainte de pr√©servation de certains
prototypes : si P 0  P , GLOUTON(P ,P 0) n‚Äô√©value la suppression d‚Äôun prototype que si
celui-ci n‚Äôappartient pas √† P 0. Dans l‚Äôoptique d‚Äôune inclusion dans une m√©ta-heuristique, cette
modification permet de limiter la redondance de la recherche.
4.2 Recherche √† voisinage variable
L‚Äôheuristique gloutonne est par nature susceptible de s‚Äôemp√™trer dans un optimum local.
Il est donc naturel d‚Äôenvisager la remise en question de la solution propos√©e par GLOUTON.
Pour cela, on applique la m√©ta-heuristique de recherche √† voisinage variable d√©crite par Han-
sen et Mladenovic (2001). Celle-ci consiste √† modifier localement une solution et √† r√©appliquer
l‚Äôheuristique de base, ici l‚Äôheuristique gloutonne. Si on n‚Äôobtient pas ainsi de meilleure solu-
tion, on r√©it√®re en explorant un voisinage plus √©loign√©. Sinon, on r√©it√®re en consid√©rant un
- 427 - RNTI-E-6
S√©lection supervis√©e d‚Äôinstances
voisinage de taille minimale de la nouvelle meilleure solution. Le nombre d‚Äô√©tape est usuelle-
ment contr√¥l√© par une valeur maximale sp√©cifi√©e par l‚Äôutilisateur.
Une notion de voisinage d‚Äôune solution doit √™tre d√©finie. Pour un ensemble P0 de p proto-
types, un voisin est tout ensemble de prototypes P = P1
F
P2 tel que P1 est inclus dans P0 et
P2 est un ensemble d‚Äôinstances appartenant aux cellules de V (P0) associ√©es aux √©l√©ments de
P0 n P1. Si t 2 [0; 1], le voisinage Vt(P0) contient tous les voisins P = P1
F
P2 de P0 tels
qu‚Äôune proportion t de prototypes dans P0 est remplac√©e par une proportion t d‚Äôinstances dans
l‚Äôunion des cellules correspondantes (cf Figure 2).
FIG. 2 ‚Äì Exemple de partitions voisines pour t = 0:35. (a) R√©partition des instances dans
les cellules de la partition. (b) 2 prototypes (soit 35% des prototypes) sont remis en cause et
3 instances (soit 35% des instances appartenant aux cellules associ√©es) les remplacent. (c)
Partition voisine obtenue.
Un unique param√®tre Niveau quantifie le degr√© d‚Äôoptimisation souhait√© par l‚Äôutilisateur.
Une incr√©mentation unitaire de ce param√®tre revient √† doubler le temps consacr√© √† l‚Äôoptimisa-
tion. L‚Äôalgorithme RVVGLOUTON(Niveau), de complexit√© au pire unO(2NiveauN2 logN),
s‚Äô√©crit alors :
‚Äì DegreMax 2Niveau
‚Äì S0  GLOUTON(D)
‚Äì S  S0
‚Äì Degre 1
‚Äì TantQue Degre < DegreMax Faire
‚Äì t Degre=DegreMax
‚Äì S0  S√©lection d‚Äôune solution dans Vt(S)
‚Äì S  GLOUTON(S0; S
T
S0)
‚Äì Si S0 est meilleur que S0
‚Äì S0  S
0
‚Äì Degre 1
‚Äì Sinon
‚Äì Degre Degre+ 1
‚Äì Fin TantQue
‚Äì Retourner S0
- 428 -RNTI-E-6
S. Ferrandiz et M. Boull√©
5 Exp√©rimentation
On √©value les m√©thodes de s√©lection d‚Äôinstances selon trois axes : la performance pr√©dictive
(i.e. le taux de bonne classification en test), le taux de compression (i.e. le rapport du nombre
de prototypes au nombre d‚Äôinstances) et la robustesse (i.e. le rapport du taux de pr√©diction en
test au taux de pr√©diction en apprentissage). Ces indicateurs sont estim√©s par validation crois√©e
stratifi√©e √† 10 niveaux.
RVVG IB3 DROP3 Explore NN
Iris 95 92 89 95 95
Wine 83 74 71 84 83
Sonar 67 78 82 72 85
Heart 71 56 62 72 63
Bupa 68 54 57 63 61
Ionosphere 88 88 88 87 91
Australian 73 60 68 71 68
Crx 72 62 67 71 67
Breast 97 93 95 96 96
Pima 72 61 68 74 69
Vehicle 63 64 65 63 68
German 71 61 66 70 69
Led 64 67 59 58 72
Yeast 51 46 51 56 54
Segmentation 88 95 92 92 97
Abalone 25 20 23 26 21
Spam 85 77 81 82 85
Waveform 79 70 76 83 77
WaveformNoise 79 68 74 81 76
PenDigits 96 98 94 98 100
Moyenne 74:4 69:2 71:5 74:7 74:8
V/E/D 13=4=3 15=3=2 2=13=5 7=8=5
TAB. 1 ‚Äì Taux de bonne pr√©diction de notre m√©thode RVVGLOUTON(5) (RVVG), d‚ÄôIB3,
DROP3, Explore et de la classification par le plus proche voisin (NN), estim√©s par valida-
tion crois√©e stratifi√©e √† 10 niveaux. Le nombre de Victoire/Egalit√©/D√©faite significatives (au
sens de la statistique de Student au niveau 0.05) de RVVG est report√©.
Notre m√©thode est compar√©e √† IB3, DROP3, Explore et la r√®gle de classification par le plus
proche voisin NN. Le niveau de RVVGLOUTON est fix√© √† 5, ce qui donne un temps de calcul
du m√™me ordre que celui d‚ÄôExplore. Les jeux de donn√©es sont issus de l‚ÄôUCI (Blake et Merz,
1996). Les jeux de donn√©es utilis√©s (tableau 3) sont ceux pour lesquels la performance pr√©-
dictive de la r√®gle de classification par le plus proche voisin est significativement sup√©rieure
√† celle du pr√©dicteur majoritaire (qui attribue √† toute nouvelle instance la classe majoritaire
sur l‚Äô√©chantillon). Afin d‚Äô√©viter toute interf√©rence sur les r√©sultats relative au choix de la dis-
tance, du type de pr√©traitement, etc, on ne consid√®re que des jeux de donn√©es sans valeurs
- 429 - RNTI-E-6
S√©lection supervis√©e d‚Äôinstances
manquantes, que les variables continues. La distance de Minkowski L1 fait office de mesure
de similitude.
Le taux de bonne pr√©diction est report√© dans le tableau 1 et le taux de compression moyen
dans le tableau 2. Les m√©thodes classiques, repr√©sent√©es par IB3 et DROP3, r√©alisent une com-
pression de l‚Äôordre de 20 √† 30 pour cent, avec une perte en terme de performance pr√©dictive
(69:2% et 71:5% respectivement contre 74:8% pour la classification par le plus proche voi-
sin). Notre m√©thode s√©lectionne un minimum d‚Äôinstances (1:7% contre 2:5% pour Explore en
moyenne) sans que la performance pr√©dictive n‚Äôen soit affect√©e.
Les m√©thodes se pla√ßant dans le cadre de l‚Äôapprentissage de mod√®les sont donc plus per-
formantes que les m√©thodes classiques bas√©es sur des d√©finitions (n√©cessairement heuristiques)
d‚Äôutilit√© individuelle. L‚Äôapproche descriptive adopt√©e ici permet de gagner encore en compres-
sion par rapport √† Explore. Ceci conduit √† am√©liorer la robustesse (tableau 2).
La fiabilit√© du r√©sultat est une propri√©t√© importante, que l‚Äôapproche descriptive permet en-
core d‚Äôam√©liorer. Ceci est d‚Äôautant plus int√©ressant que les mod√®les consid√©r√©s (des fonctions
de probabilit√©s conditionnelles) sont plus riches que les mod√®les usuels (des classifieurs) : √†
chaque prototype est associ√© une distribution de probabilit√© sur les √©tiquettes. Notre m√©thode
extrait donc plus de connaissance, et de mani√®re plus fiable. Ceci rend profitable son utilisation
en phase de pr√©paration des donn√©es, l√† o√π les m√©thodes pr√©dictives sont inadapt√©es.
RVVG IB3 DROP3 Explore NN
Compression Moyenne 1:7 32:0 22:2 2:5 100
V/E/D 20=0=0 20=0=0 13=7=0 20=0=0
Robustesse Moyenne 0:97 0:81 0:86 0:94 0:76
V/E/D 13=7=0 12=8=0 5=15=0 16=4=0
TAB. 2 ‚Äì Compression et robustesse moyenne des m√©thodes test√©es estim√©es par validation
crois√©e stratif√©e et nombre de Victoire/Egalit√©/D√©faite significatives (au sens de la statistique
de Student) de notre m√©thode.
6 Conclusion
La classification suivant le plus proche voisin repose sur la construction d‚Äôune partition de
Voronoi. Dans cet article, nous avons propos√© un crit√®re d‚Äô√©valuation des partitions induites
par les ensembles de prototypes inclus dans l‚Äôensemble des instances formant l‚Äô√©chantillon.
L‚Äôapproche descriptive adopt√©e ayant permis de faire porter la gestion du sur-apprentissage par
le crit√®re, nous avons √©galement propos√© une heuristique d‚Äôoptimisation pouss√©e de ce crit√®re.
Les exp√©riences sur jeux de donn√©es de l‚ÄôUCI ont montr√© que notre m√©thode est com-
p√©titive en terme de performance pr√©dictive, tout en s√©lectionnant un minimum d‚Äôinstances.
L‚Äô√©tude de la robustesse a √©galement illustr√© le fait que la m√©thode ne sur-apprend pas : la
d√©cision prise est fiable et pertinente. Cela concourt √† l‚Äôemploi et au d√©ploiement de mod√®les
de classification par le plus proche voisin. De par la richesse de la connaissance extraite, l‚Äôuti-
lisation de la m√©thode n‚Äôest pas limit√©e √† la pr√©diction.
- 430 -RNTI-E-6
S. Ferrandiz et M. Boull√©
Annexe
Jeux Taille Variables Classes Pr√©diction majoritaire
Iris 150 4 3 0.33
Wine 178 13 3 0.40
Sonar 208 60 2 0.53
Heart 270 10 2 0.56
Bupa 345 6 2 0.58
Ionosphere 351 33 2 0.64
Australian 690 6 2 0.56
Crx 690 6 2 0.56
Breast 699 9 2 0.66
Pima 768 8 2 0.65
Vehicle 846 18 4 0.26
German 1000 24 2 0.70
Led 1000 7 10 0.11
Yeast 1484 8 10 0.31
Segmentation 2310 19 7 0.14
Abalone 4177 7 28 0.16
Spam 4307 57 2 0.65
Waveform 5000 21 3 0.34
WaveformNoise 5000 40 3 0.34
PenDigits 7494 16 10 0.10
TAB. 3 ‚Äì Caract√©ristiques des jeux de donn√©es utilis√©s.
R√©f√©rences
Aha, D., D. Kibler, et M. Albert (1991). Instance-based learning algorithms. Machine lear-
ning 6, 37‚Äì66.
Berndt, D. et J. Clifford (1996). Finding patterns in time series : a dynamic programming
approach. Technical report, Advances Knowledge Discovery Data Mining.
Blake, C. et C. Merz (1996). Uci repository of machine learning databases.
http ://www.ics.uci.edu/mlearn/MLRepository.html.
Cameron-Jones, R. (1995). Instance selection by encoding length heuristic with random mu-
tation hill climbing. In Proceedings of the eighth australian joint conference on artificial
intelligence, pp. 99‚Äì106.
Chapman, P., J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, et R. Wirth (2000).
CRISP-DM 1.0 : step-by-step data mining guide.
Duda, R., P. Hart, et D. Stork (2001). Pattern classification. New-York : Wiley & sons.
- 431 - RNTI-E-6
S√©lection supervis√©e d‚Äôinstances
Fix, E. et J. Hodges (1951). Discriminatory analysis. nonparametric discrimination : Consis-
tency properties. Technical Report 4, Project Number 21-49-004, USAF School of Aviation
Medicine, Randolph Field, TX.
Gates, G. (1972). The reduced nearest neighbor rule. IEEE transactions on information
theory 18(3), 431‚Äì433.
Hansen, P. et N. Mladenovic (2001). Variable neighborhood search : principles and applica-
tions. European journal of operational research 130, 449‚Äì467.
Hart, P. (1968). The condensed nearest neighbor rule. IEEE transactions on information
theory 14, 515‚Äì516.
Sebban, M., R. Nock, et S. Lallich (2002). Stopping criterion for boosting-based data reduction
techniques : from binary to multiclass problem. Journal of machine learning research 3,
863‚Äì885.
Stanfill, C. et D. Waltz (1986). Toward memory-based reasoning. Communication of the
ACM 29, 1213‚Äì1228.
Vapnik, V. (1996). The nature of statistical learning theory. New-York : Srpinger-Verlag.
Wilson, D. (1972). Asymptotic properties of nearest neighbor rules using edited data. IEEE
Transactions on systems, man and cybernetics 2, 408‚Äì421.
Wilson, D. et T. Martinez (1997a). Improved heterogeneous distance functions. Journal of
artificial intelligence research 6(1), 1‚Äì34.
Wilson, D. et T. Martinez (1997b). Instance pruning techniques. In D. Fisher (Ed.), Procee-
dings of the 14th international conference on machine learning, San Francisco, pp. 403‚Äì411.
Morgan Kaufmann.
Wilson, D. et T. Martinez (2001). Reduction techniques for instance-based learning algorithms.
Machine learning 38(3), 257‚Äì286.
Summary
The Nearest Neighbor rule is simple and efficient. In practice, instances have to be care-
fully selected, in order to save computing time and to avoid overfitting. The voronoi tesselation
induced by the selected instances (the prototypes) is the underlying structure of such a rule.
From the descriptive approach adopted in this paper results a global evaluation criterion which
makes the compromise between the number of prototypes and the discrimination of the target
feature explicit. An optimisation heuristic is proposed as well. The properties of the Voronoi
partitions and the criterion allows to reduce its algorithmic complexity. The method is com-
pared with several ones from the state of the art on twenty data sets from the UCI repository.
The presented technique does not suffer from any loss of the accuracy and selects less in-
stances. As a consequence, the robustness is improved : data are not overfitted.
- 432 -RNTI-E-6
