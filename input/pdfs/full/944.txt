RÃ©duction des dimensions des donnÃ©es
en apprentissage artificiel
Y. Bennani, S. GuÃ©rif, E. Viennet
UniversitÃ© Paris 13, LIPN - CNRS UMR 7030
99, avenue J.B. ClÃ©ment, F-93430 Villetaneuse
RÃ©sumÃ©. Depuis plusieurs dÃ©cÃ©nies, le volume des donnÃ©es disponibles ne cesse
de croÃ®tre ; alors quâ€™au dÃ©but des annÃ©es 80 le volume des bases de donnÃ©es se
mesurait en mega-octets, il sâ€™exprime aujourdâ€™hui en tera-octets et parfois mÃªme
en peta-octets. Le nombre de variables et le nombre dâ€™exemples peuvent prendre
des valeurs trÃ¨s Ã©levÃ©s, et cela peut poser un problÃ¨me lors de lâ€™exploration et
lâ€™analyse des donnÃ©es. Ainsi, le dÃ©veloppement dâ€™outils de traitement adaptÃ©s
aux donnÃ©es volumineuses est un enjeu majeur de la fouille de donnÃ©es. La rÃ©-
duction des dimensions permet notament de faciliter la visualisation et la com-
prÃ©hension des donnÃ©es, de rÃ©duire lâ€™espace de stockage nÃ©cessaire et le temps
dâ€™exploitation, et enfin dâ€™identifier les facteurs pertinents. Dans cet article, nous
prÃ©sentons un panarama des techniques de rÃ©duction des dimensions essentiel-
lement basÃ©es sur la sÃ©lection de variables supervisÃ©e et non supervisÃ©e, et sur
les mÃ©thodes gÃ©omÃ©triques de rÃ©duction de dimensions.
1 Introduction
La taille des donnÃ©es peut Ãªtre mesurÃ©e selon deux dimensions, le nombre de variables
et le nombre dâ€™exemples. Ces deux dimensions peuvent prendre des valeurs trÃ¨s Ã©levÃ©es, ce
qui peut poser un problÃ¨me lors de lâ€™exploration et lâ€™analyse de ces donnÃ©es. Pour cela, il est
fondamental de mettre en place des outils de traitement de donnÃ©es permettant une meilleure
comprÃ©hension de la valeur des connaissances disponibles dans ces donnÃ©es. La rÃ©duction des
dimensions est lâ€™une des plus vieilles approches permettant dâ€™apporter des Ã©lÃ©ments de rÃ©ponse
Ã  ce problÃ¨me. Son objectif est de sÃ©lectionner ou dâ€™extraire un sous-ensemble optimal de
caractÃ©ristiques pertinentes pour un critÃ¨re fixÃ© auparavant. La sÃ©lection de ce sous-ensemble
de caractÃ©ristiques permet dâ€™Ã©liminer les informations non-pertinentes et redondantes selon
le critÃ¨re utilisÃ©. Cette sÃ©lection/extraction permet donc de rÃ©duire la dimension de lâ€™espace
des exemples et rendre lâ€™ensemble des donnÃ©es plus reprÃ©sentatif du problÃ¨me. En effet, les
principaux objectifs de la rÃ©duction de la dimension sont :
â€¢ faciliter la visualisation et la comprÃ©hension des donnÃ©es,
â€¢ rÃ©duire lâ€™espace de stockage nÃ©cessaire,
â€¢ rÃ©duire le temps dâ€™apprentissage et dâ€™utilisation,
â€¢ identifier les facteurs pertinents.
Les algorithmes dâ€™apprentissage artificiel requiÃ¨rent typiquement peu de traits (features) ou
de variables (attributs) trÃ¨s significatives caractÃ©risant le processus Ã©tudiÃ©. Dans le domaine
RÃ©duction des dimensions des donnÃ©es
de la reconnaissance des formes et de la fouille de donnÃ©es, il pourrait encore Ãªtre bÃ©nÃ©fique
dâ€™incorporer un module de rÃ©duction de la dimension dans le systÃ¨me global avec comme ob-
jectif dâ€™enlever toute information inconsÃ©quente et redondante. Cela a un effet important sur la
performance du systÃ¨me. En effet le nombre de caractÃ©ristiques utilisÃ©es est directement liÃ© Ã 
lâ€™erreur finale. Lâ€™importance de chaque caractÃ©ristique dÃ©pend de la taille de la base dâ€™appren-
tissage (pour un Ã©chantillon de petite taille, lâ€™Ã©limination dâ€™une caractÃ©ristique importante peut
diminuer lâ€™erreur). Il faut aussi noter que des caractÃ©ristiques individuellement peu pertinentes
peuvent Ãªtre trÃ¨s informatives si on les utilise conjointement.
La rÃ©duction de la dimension est un problÃ¨me complexe qui permet de rÃ©duire le volume
dâ€™informations Ã  traiter et faciliter le processus de lâ€™apprentissage. Nous pouvons classer toutes
les techniques mathÃ©matiques de rÃ©duction des dimensions en deux grandes catÃ©gories :
â€¢ la sÃ©lection de variables : qui consiste Ã  choisir des caractÃ©ristiques dans lâ€™espace de
mesure (figure 1),
FIG. 1 â€“ Principe de la sÃ©lection de variables.
â€¢ et lâ€™extraction de traits : qui vise Ã  sÃ©lectionner des caractÃ©ristiques dans un espace trans-
formÃ© (dans un espace de projection) (figure 2)
FIG. 2 â€“ Principe de lâ€™extraction de caractÃ©ristiques.
DÃ©finition 1.1 (Bennani (2001)) Nous appelons "variables" les descripteurs dâ€™entrÃ©e et "traits"
des caractÃ©ristiques construites Ã  partir des variables dâ€™entrÃ©e.
La distinction est nÃ©cessaire dans le cas des mÃ©thodes Ã  noyaux pour lesquelles les traits ne
sont pas explicitement calculÃ©s.
La premiÃ¨re catÃ©gorie est appropriÃ©e quand lâ€™acquisition de mesures des formes est coÃ»-
teuse. Ainsi lâ€™objectif principal de la sÃ©lection de caractÃ©ristiques dans ce cas est de rÃ©duire le
nombre de mesures requises. Par contre, les techniques dâ€™extraction de traits (deuxiÃ¨me catÃ©-
gorie) utilisent toute lâ€™information contenue dans les formes pour la compresser et produire un
Y. Bennani et al.
vecteur de plus petite dimension. Ces techniques projettent un vecteur forme de lâ€™espace de re-
prÃ©sentation dans un espace de dimension plus petite. Les systÃ¨mes dâ€™apprentissage connexion-
niste sont un bon exemple de cette catÃ©gorie. En effet, les modÃ¨les connexionnistes conÃ§us pour
une tache de discrimination fournissent un systÃ¨me avec des aptitudes intÃ©ressantes pour lâ€™ana-
lyse du processus. Les cellules cachÃ©es dâ€™un Perceptron multi-couches apprennent comment
extraire les caractÃ©ristiques significatives du signal dâ€™entrÃ©e.
2 RÃ©duction des dimensions par sÃ©lection de variables
La sÃ©lection de variable est un problÃ¨me difficile qui a Ã©tÃ© Ã©tudiÃ© depuis les annÃ©es 70.
Il revient dans lâ€™actualitÃ© scientifique avec lâ€™apparition des grandes bases de donnÃ©es et les
systÃ¨mes de fouille de donnÃ©es Â«Data MiningÂ» (Liu et Motoda, 1998; Cakmakov et Bennani,
2002; Guyon et al., 2006).
La sÃ©lection de variables a fait lâ€™objet de plusieurs recherches en statistique, et plus parti-
culiÃ¨rement dans des domaines comme la reconnaissance des formes, la modÃ©lisation de sÃ©ries
chronologiques et lâ€™identification de processus. Dans le domaine de lâ€™apprentissage, lâ€™Ã©tude de
la problÃ©matique de la sÃ©lection de variables est assez rÃ©cente. En apprentissage symbolique, de
nombreuses mÃ©thodes ont Ã©tÃ© proposÃ©es pour des tÃ¢ches de classement (discrimination). Dans
le domaine de lâ€™apprentissage connexionniste (Bennani, 2001, 2006), la sÃ©lection de variables
a Ã©tÃ© abordÃ©e Ã  partir dâ€™un problÃ¨me dâ€™optimisation et de choix dâ€™architectures des modÃ¨les,
ainsi des approches trÃ¨s intÃ©ressantes ont Ã©mergÃ©.
La sÃ©lection de variables est une problÃ©matique complexe et dâ€™une importance cruciale
pour les systÃ¨mes dâ€™apprentissage. Afin de mettre en Ã©vidence les deux aspects du processus
de la sÃ©lection de variables : difficultÃ© et importance, nous allons prÃ©senter les Ã©lÃ©ments essen-
tiels que nÃ©cessite gÃ©nÃ©ralement ce processus. Une dÃ©finition de la sÃ©lection de variables peut
sâ€™Ã©noncer de la faÃ§on suivante :
DÃ©finition 2.1 (Bennani (2001)) La sÃ©lection de variables est un procÃ©dÃ© permettant de choi-
sir un sous-ensemble optimal de variables pertinentes, Ã  partir dâ€™un ensemble de variables
original, selon un certain critÃ¨re de performance.
A partir de cette dÃ©finition, on peut se poser trois questions essentielles :
â€¢ Comment mesurer la pertinence des variables ?
â€¢ Comment former le sous-ensemble optimal ?
â€¢ Quel critÃ¨re dâ€™optimalitÃ© utiliser ?
Ces trois questions dÃ©finissent les Ã©lÃ©ments essentiels dâ€™une procÃ©dure de sÃ©lection de va-
riables. En effet, le problÃ¨me de la sÃ©lection de variables consiste Ã  identifier les variables
permettant une meilleure sÃ©paration entre les diffÃ©rentes classes dans le cas dâ€™un classement
et une meilleure qualitÃ© de prÃ©diction dans le cas dâ€™une rÃ©gression. On parle alors de "pouvoir
discriminant" dans le premier cas et de "pouvoir prÃ©dictif" dans le deuxiÃ¨me cas, pour dÃ©si-
gner la pertinence dâ€™une variable. La rÃ©ponse Ã  la premiÃ¨re question consiste Ã  trouver une
mesure de pertinence ou un critÃ¨re dâ€™Ã©valuation J(X) permettant de quantifier lâ€™importance
dâ€™une variable ou dâ€™un ensemble de variablesX . La deuxiÃ¨me question Ã©voque le problÃ¨me du
choix de la procÃ©dure de recherche ou de constitution du sous-ensemble optimal des variables
pertinentes. La derniÃ¨re question demande la dÃ©finition dâ€™un critÃ¨re dâ€™arrÃªt de la recherche.
RÃ©duction des dimensions des donnÃ©es
Le critÃ¨re dâ€™arrÃªt est gÃ©nÃ©ralement dÃ©terminÃ© Ã  travers une combinaison particuliÃ¨re entre la
mesure de pertinence et la procÃ©dure de recherche.
2.1 CritÃ¨res dâ€™Ã©valuation
Lâ€™amÃ©lioration des performances dâ€™un systÃ¨me dâ€™apprentissage par une procÃ©dure de sÃ©-
lection de variables nÃ©cessite dans un premier temps la dÃ©finition dâ€™une mesure de pertinence.
Dans le cas dâ€™un problÃ¨me de classement, on teste, par exemple, la qualitÃ© de discrimination du
systÃ¨me en prÃ©sence ou en absence dâ€™une variable. Par contre, pour un problÃ¨me de rÃ©gression,
on teste plutÃ´t la qualitÃ© de prÃ©diction par rapport aux autres variables. CommenÃ§ons dâ€™abord
par dÃ©finir ce qui est la pertinence dâ€™une variable (ou dâ€™un ensemble de variables).
DÃ©finition 2.2 (Bennani (2001)) Une variable pertinente est une variable telle que sa sup-
pression entraÃ®ne une dÃ©tÃ©rioration des performances (pouvoir de discrimination en classe-
ment ou la qualitÃ© de prÃ©diction en rÃ©gression) du systÃ¨me dâ€™apprentissage.
Plusieurs critÃ¨res dâ€™Ã©valuation ont Ã©tÃ© proposÃ©s, basÃ©s sur des hypothÃ¨ses statistiques ou sur
des heuristiques. Pour un problÃ¨me de classement (discrimination), les critÃ¨res dâ€™Ã©valuation
sont souvent basÃ©s sur les matrices de dispersion intra et inter classes. En effet, ces matrices
sont directement liÃ©es Ã  la gÃ©omÃ©trie des classes et donnent une information significative sur
la rÃ©partition des classes dans lâ€™espace des formes.
On trouve aussi des critÃ¨res dâ€™Ã©valuation qui utilisent des distances probabilistes ou des
mesures dâ€™entropie. Le critÃ¨re dans ce cas est basÃ© sur lâ€™information mutuelle entre le classe-
ment et lâ€™ensemble de variables. Dans le cas des systÃ¨mes dâ€™apprentissage connexionnistes,
lâ€™Ã©valuation des variables se fait en fonction de lâ€™importance des poids qui est dÃ©finie comme
le changement de lâ€™erreur (de classement ou de rÃ©gression) dÃ» Ã  la suppression de ces poids.
2.2 ProcÃ©dures de recherche
En gÃ©nÃ©ral, on ne connaÃ®t pas le nombre optimalm de variables Ã  sÃ©lectionner. Ce nombre
dÃ©pendra de la taille et de la qualitÃ© de la base dâ€™apprentissage (la quantitÃ© et la qualitÃ© dâ€™in-
formation disponible) et de la rÃ¨gle de dÃ©cision utilisÃ©e (le modÃ¨le). Pour un ensemble de n
variables il existe 2n âˆ’ 1 combinaisons de variables possibles oÃ¹ 2 reprÃ©sente deux choix :
sÃ©lectionner ou ne pas sÃ©lectionner une variable. La recherche dâ€™un sous-ensemble de m va-
riables parmi n engendre un nombre de combinaison Ã©gal Ã  :(
n
m
)
=
n!
(nâˆ’m)!m! (1)
En grande dimension (n trÃ¨s grand), le nombre de combinaison Ã  examiner devient trÃ¨s
Ã©levÃ© et une recherche exhaustive nâ€™est pas envisageable. La recherche dâ€™un sous-ensemble
optimal de variables est un problÃ¨me NP-difficile. Une alternative consiste Ã  utiliser une mÃ©-
thode de recherche de type Branch & Bound, (Liu et Motoda, 1998). Cette mÃ©thode de re-
cherche permet de restreindre la recherche et donne le sous-ensemble optimal de variables,
sous lâ€™hypothÃ¨se de monotocitÃ© du critÃ¨re de sÃ©lection J(X).
Le critÃ¨re J(X) est dit monotone si :
X1 âŠ‚ X2 âŠ‚ . . . âŠ‚ Xm =â‡’ J(X1) âŠ‚ J(X2) âŠ‚ . . . âŠ‚ J(Xm) (2)
Y. Bennani et al.
oÃ¹ Xk est lâ€™ensemble contenant k variables sÃ©lectionnÃ©es.
Cependant, la plupart des critÃ¨res dâ€™Ã©valuation utilisÃ©s pour la sÃ©lection ne sont pas mono-
tones et dans ce cas on a recours Ã  la seule alternative basÃ©e sur des mÃ©thodes sous-optimales
comme les procÃ©dures sÃ©quentielles :
â€¢ StratÃ©gie ascendante : Forward Selection (FS),
â€¢ StratÃ©gie descendante : Backward Selection (BS),
â€¢ StratÃ©gie bidirectionnelle : Bidirectional Selection (BiS).
La mÃ©thode FS procÃ¨de par agrÃ©gations successives (par ajouts successifs de variables).
Au dÃ©part lâ€™ensemble des variables sÃ©lectionnÃ©es est initialisÃ© Ã  lâ€™ensemble vide. Ã€ chaque
Ã©tape k, on sÃ©lectionne la variable qui optimise le critÃ¨re dâ€™Ã©valuation J(Xk) et on la rajoute Ã 
lâ€™ensemble des variables sÃ©lectionnÃ©es Xk. Soit X lâ€™ensemble des variables, on sÃ©lectionne la
variable xi telle que :
J(Xk) = max
xiâˆˆX\Xkâˆ’1
J(Xkâˆ’1 âˆª {xi}) (3)
Lâ€™ordre dâ€™adjonction des variables Ã  lâ€™ensemble des variables sÃ©lectionnÃ©es produit une liste
ordonnÃ©e des variables selon leur importance. Les variables les plus importantes sont les pre-
miÃ¨res variables ajoutÃ©es Ã  la liste. NÃ©anmoins, il faut aussi se rappeler que des variables
individuellement peu pertinentes peuvent Ãªtre trÃ¨s informatives si on les utilise conjointement.
La mÃ©thode BS est une procÃ©dure inverse de la prÃ©cÃ©dente (par retraits successifs de va-
riables). On part de lâ€™ensemble complet X des variables et on procÃ¨de par Ã©limination. Ã€
chaque Ã©tape la variable la moins importante selon le critÃ¨re dâ€™Ã©valuation est Ã©liminÃ©e. Le
procÃ©dÃ© continu jusquâ€™Ã  ce quâ€™il reste quâ€™une seule variable dans lâ€™ensemble des variables de
dÃ©part. Ã€ lâ€™Ã©tape k, on supprime la variable xi telle que :
J(Xk) = max
xiâˆˆXk+1
J(Xk+1 \ {xi}) (4)
Une liste ordonnÃ©e selon lâ€™ordre dâ€™Ã©limination des variables est ainsi obtenue. Les variables les
plus pertinentes sont alors les variables qui se trouvent dans les derniÃ¨res positions de la liste.
La procÃ©dure BiS effectue sa recherche dans les deux directions (Forward et Backward)
dâ€™une maniÃ¨re concurrentielle. La procÃ©dure sâ€™arrÃªte dans deux cas : (1) quand une des deux
directions a trouvÃ© le meilleur sous-ensemble de variables avant dâ€™atteindre le milieu de lâ€™es-
pace de recherche ; ou (2) quand les deux directions arrivent au milieu. Il est clair que les
ensembles de variables sÃ©lectionnÃ©es trouvÃ©s respectivement par SFS et par SBS ne sont pas
Ã©gaux Ã  cause de leurs diffÃ©rents principes de sÃ©lection. NÃ©anmoins, cette mÃ©thode rÃ©duit le
temps de recherche puisque la recherche sâ€™effectue dans les deux directions et sâ€™arrÃªte dÃ¨s quâ€™il
y a une solution quelle que soit la direction.
2.3 CritÃ¨res dâ€™arrÃªt
Le nombre optimal de variables nâ€™est pas connu a priori, lâ€™utilisation dâ€™une rÃ¨gle pour
contrÃ´ler la sÃ©lection-Ã©limination de variables permet dâ€™arrÃªter la recherche lorsque aucune
variable nâ€™est plus suffisamment informative. Le critÃ¨re dâ€™arrÃªt est souvent dÃ©fini comme une
combinaison de la procÃ©dure de recherche et du critÃ¨re dâ€™Ã©valuation. Une heuristique, souvent
utilisÃ©e, consiste Ã  calculer pour les diffÃ©rents sous-ensembles de variables sÃ©lectionnÃ©es une
estimation de lâ€™erreur de gÃ©nÃ©ralisation par validation croisÃ©e. Le sous-ensemble de variables
sÃ©lectionnÃ©es est celui qui minimise cette erreur de gÃ©nÃ©ralisation.
RÃ©duction des dimensions des donnÃ©es
2.4 Les diffÃ©rentes approches de sÃ©lection
Il existe trois grandes familles dâ€™approches :
Approches Â« Filtres Â» (Filters) : ces mÃ©thodes sÃ©lectionnent les variables indÃ©pendamment
de la mÃ©thode qui va les utiliser, elles se basent sur les caractÃ©ristiques de lâ€™ensemble des don-
nÃ©es afin de sÃ©lectionner certaines variables et dâ€™Ã©liminer dâ€™autres sous forme de prÃ©-traitement
des donnÃ©es.
Approches Â« Symbioses Â» (Wrappers) : contrairement aux approches filtre qui ignorent to-
talement lâ€™influence des variables sÃ©lectionnÃ©es sur la performance de lâ€™algorithme dâ€™appren-
tissage, les approches â€enveloppantesâ€ utilisent lâ€™algorithme dâ€™apprentissage comme une fonc-
tion dâ€™Ã©valuation.
Approches Â« IntÃ©grÃ©es Â» (Embedded) : ces mÃ©thodes exÃ©cutent la sÃ©lection variable pendant
le processus de lâ€™apprentissage. Le processus de la sÃ©lection de variables est effectuÃ© parallÃ¨-
lement au processus de classement (ou de la rÃ©gression). Le sous-ensemble de variables ainsi
sÃ©lectionnÃ©es sera choisi de faÃ§on Ã  optimiser le critÃ¨re dâ€™apprentissage utilisÃ©.
2.5 SÃ©lection de variables et apprentissage symbolique
La sÃ©lection de variables dans le domaine de lâ€™apprentissage symbolique (Machine Lear-
ning) est souvent limitÃ©e aux tÃ¢ches de discrimination de donnÃ©es discrÃ¨tes. De nombreuses
techniques ont Ã©tÃ© proposÃ©es dans ce domaine. La mÃ©thode FOCUS (Almuallim, 1994) est
basÃ©e sur une exploration exhaustive de tous les sous-ensembles de variables et choisir le plus
petit sous-ensemble qui couvre le mieux la sortie cible. Lâ€™approche ABB (Liu et Motoda, 1998)
estime aussi la pertinence dâ€™une variable par une mesure de recouvrement. Lâ€™avantage de cette
derniÃ¨re mÃ©thode est quâ€™elle est monotone. La mÃ©thode RELIEF (Kira et Rendell, 1992) estime
lâ€™importance dâ€™une variable par comparaison de cette variable et la classe correspondante sur
plusieurs sous-ensembles de donnÃ©es. Plusieurs autres mÃ©thodes ont Ã©tÃ© basÃ©es sur lâ€™utilisation
de lâ€™entropie croisÃ©e ou la courbe ROC. Ces mÃ©thodes sont trÃ¨s intÃ©ressantes mais elles sont
difficilement utilisables pour les problÃ¨mes qui nous concernent dans le cadre de ce projet, i.e.
en grande dimension avec des donnÃ©es gÃ©nÃ©ralement continues.
2.6 SÃ©lection de variables et apprentissage connexionniste
La sÃ©lection de variables dans le domaine connexionniste est trÃ¨s attrayante et soulÃ¨ve de
nombreux enjeux Ã  la fois thÃ©oriques et applicatifs fondamentaux (Bennani, 2001, 2006). En
effet, dans le cas des rÃ©seaux connexionnistes, le processus de la sÃ©lection de variables peut
Ãªtre effectuÃ© parallÃ¨lement au processus de classement - ou de la rÃ©gression. Le sous-ensemble
de variables ainsi sÃ©lectionnÃ©es sera choisi de faÃ§on Ã  optimiser le critÃ¨re dâ€™apprentissage. En
plus, le nombre de variables est directement liÃ© Ã  lâ€™architecture et Ã  la complexitÃ© de la fonction
rÃ©alisable par le systÃ¨me connexionniste.
Dans le cas des systÃ¨mes dâ€™apprentissage connexionniste, le nombre de variables est direc-
tement liÃ© Ã  lâ€™architecture et Ã  la complexitÃ© de la fonction rÃ©alisable par le modÃ¨le connexion-
niste. Plusieurs approches ont Ã©tÃ© proposÃ©es dans la littÃ©rature. La plupart de ces techniques
Y. Bennani et al.
emploient la premiÃ¨re ou la deuxiÃ¨me dÃ©rivÃ©e de la fonction de coÃ»t par rapport aux poids pour
estimer lâ€™importance des connexions.
Les mÃ©thodes les plus largement employÃ©es sont : Optimal Brain Damage (OBD) propo-
sÃ©e par Le Cun et al. (1990), et Optimal Brain Surgeon (OBS) par Hassibi et Stork (1993) qui
est une amÃ©lioration de la prÃ©cÃ©dente. Pedersen et al. (1996) ont proposÃ© Î³OBD et Î³OBS ,
oÃ¹ lâ€™estimation de lâ€™importance dâ€™un poids est basÃ©e sur le changement associÃ© dans lâ€™erreur
de gÃ©nÃ©ralisation si le poids est Ã©laguÃ©. Dâ€™autres variantes dâ€™OBD et dâ€™OBS ont Ã©tÃ© propo-
sÃ©es : Early Brain Damage (EBD) et Early Brain Surgeon (EBS) (Tresp et al., 1996). On peut
citer aussi Optimal Cell Damage (OCD) dÃ©veloppÃ©e par Cibas et al. (1994) qui est une exten-
sion de OBD pour lâ€™Ã©lagage des variables dâ€™entrÃ©e. Ces mÃ©thodes se basent sur lâ€™estimation
systÃ©matique de lâ€™importance dâ€™une connexion qui est dÃ©finie comme le changement de lâ€™er-
reur causÃ© par la suppression de ce poids. Lâ€™emploi des dÃ©rivÃ©es premiÃ¨res pour la sÃ©lection
de variables peut Ãªtre trouvÃ© par exemple dans Dorizzi et al. (1996); Moody (1994); Ruck
et al. (1990). Dâ€™autres mÃ©thodes de sÃ©lection de variables utilisent les paramÃ¨tres du systÃ¨me
dâ€™apprentissage. Certaines de ces mÃ©thodes emploient : des tests statistiques pour Ã©valuer un
intervalle de confiance pour chaque poids (M. et al., 1995), lâ€™information mutuelle pour Ã©va-
luer un ensemble de caractÃ©ristiques et sÃ©lectionner un sous-ensemble pertinent (Battiti, 1994),
des mesures heuristiques basÃ©es sur lâ€™estimation de la contribution des variables dans la prise
de dÃ©cision du systÃ¨me (Bennani et Bossaert, 1995; Yacoub et Bennani, 1997). Dans le cadre
de lâ€™apprentissage bayÃ©sien MacKay (1994); Neal (1994) proposent une mÃ©thode de sÃ©lection
de variables Automatic Relevance Determination (ARD). Cette mÃ©thode utilise des hypothÃ¨ses
de normalitÃ© sur la rÃ©partition des poids du rÃ©seau.
Dans les paragraphes qui suivent, nous allons dÃ©tailler quelques mÃ©thodes en les regroupant
par type.
Les mÃ©thodes connexionnistes de sÃ©lection de variables sont en gÃ©nÃ©ral de type â€œbackwar-
dâ€. Lâ€™idÃ©e gÃ©nÃ©rale est de faire converger un rÃ©seau jusquâ€™Ã  un minimum local en utilisant
toutes les variables et de faire ensuite la sÃ©lection. Lâ€™Ã©tape de sÃ©lection consiste Ã  trier les va-
riables par ordre croissant de pertinence, supprimer la ou les variables les moins pertinentes
et rÃ©-entraÃ®nÃ© le rÃ©seau avec les variables restantes. Ce processus continue tant quâ€™un certain
critÃ¨re dâ€™arrÃªt nâ€™est pas satisfait. Les mÃ©thodes qui suivent cette procÃ©dure comportent donc
deux phases : une phase dâ€™apprentissage et une phase dâ€™Ã©lagage qui peuvent Ãªtre alternÃ©es. On
peut dire quâ€™une â€œvraieâ€ procÃ©dure connexionniste de sÃ©lection de variables suit lâ€™algorithme
gÃ©nÃ©ral suivant :
1. Atteindre un minimum local
2. Calculer la pertinence de chaque entrÃ©e
3. Trier les entrÃ©es par ordre croissant de pertinence
4. Supprimer les entrÃ©es dont la pertinence cumulÃ©e est infÃ©rieur Ã  un seuil fixÃ©
5. Recommencer en 1. Tant que les performances estimÃ©es sur une base de validation ne
chutent pas
Les mÃ©thodes de sÃ©lection de variables en apprentissage connexionniste peuvent se regrouper
en trois grandes familles :
â€¢ Les mÃ©thodes dâ€™ordre zÃ©ro
â€¢ Les mÃ©thodes du premier ordre
â€¢ Les mÃ©thodes du second ordre
RÃ©duction des dimensions des donnÃ©es
2.6.1 MÃ©thodes dâ€™ordre zÃ©ro
Pour estimer la pertinence dâ€™une variable, les mesures dâ€™ordre zÃ©ro utilisent les valeurs
des paramÃ¨tres du systÃ¨me dâ€™apprentissage (les valeurs des connexions, la structure, . . . ). Par
exemple la mesure de pertinence HVS (Yacoub et Bennani, 1997) repose sur les paramÃ¨tres et
la structure du rÃ©seau connexionniste. Dans le cas dâ€™un Perceptron multicouches Ã  une seule
couche cachÃ©e, cette mesure est dÃ©finie par :
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i =
âˆ‘
jâˆˆHidden
[
|Ï‰ji|P
iâ€²âˆˆInput
|Ï‰jiâ€² | Ã—
âˆ‘
kâˆˆOutput
|Ï‰kj |P
jâ€²âˆˆHidden
|Ï‰kjâ€² |
]
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward + rÃ©apprentissage
critÃ¨re dâ€™arrÃªt test statistique
(5)
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i =
âˆ‘
jâˆˆHidden
[
|Ï‰ji|P
iâ€²âˆˆInput
|Ï‰jiâ€² | Ã—
âˆ‘
kâˆˆOutput
|Ï‰kj |P
jâ€²âˆˆHidden
|Ï‰kjâ€² |
]
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward + rÃ©apprentissage
critÃ¨re dâ€™arrÃªt test statistique
(6)
Une autre mÃ©thode dâ€™ordre zÃ©ro trÃ¨s efficace a Ã©tÃ© proposÃ©e par MacKay (1994) : Automatic
Relevance Determination (ARD). Dans cette mÃ©thode la pertinence dâ€™une variable est estimÃ©e
par la variance de ses poids : la variable est Ã©liminÃ©e si la variance correspondante est faible.
2.6.2 MÃ©thodes du premier ordre
La dÃ©rivÃ©e de la fonction Ïˆ que reprÃ©sente un systÃ¨me dâ€™apprentissage connexionniste -
un rÃ©seau - par rapport Ã  chacune de ses variables est trÃ¨s utilisÃ©e comme mesure de perti-
nence des variables. Si une dÃ©rivÃ©e est proche de zÃ©ro pour tous les exemples, alors la variable
correspondante nâ€™est pas utilisÃ©e par le rÃ©seau, et peut donc Ãªtre supprimÃ©.
Dans le cas des PMC - Perceptrons multicouches -, cette dÃ©rivÃ©e peut se calculer comme
une extension de lâ€™algorithme dâ€™apprentissage. Comme ces dÃ©rivÃ©es peuvent prendre aussi bien
des valeurs positives que nÃ©gatives, produisant une moyenne proche de zÃ©ro, câ€™est la moyenne
des valeurs absolues qui est gÃ©nÃ©ralement utilisÃ©e - ce sont les grandeurs des dÃ©rivÃ©es qui nous
intÃ©ressent. On trouve beaucoup de mesures de pertinences basÃ©es sur cette approche.
Y. Bennani et al.
La sensibilitÃ© de lâ€™erreur Ã  la suppression de chaque variable est utilisÃ©e par Moody dans
Moody (1994). Une mesure de sensibilitÃ© est calculÃ©e pour chaque variable xi pour Ã©valuer
la variation de lâ€™erreur en apprentissage si cette variable est supprimÃ©e du rÃ©seau. Le rem-
placement dâ€™une variable par sa moyenne supprime son influence sur la sortie du rÃ©seau. La
dÃ©finition de la pertinence est :
Î¶i = R(Ï‰)âˆ’ RËœ(xi, Ï‰) (7)
avec RËœ(xi, Ï‰) =
1
N
Nâˆ‘
k=1
âˆ¥âˆ¥yk âˆ’ Ïˆ(xk1 , . . . , xki , . . . , xkn)âˆ¥âˆ¥2 (8)
N est la taille de la base dâ€™apprentissage. Quand cette taille est trÃ¨s grande, Moody propose
dâ€™utiliser une approximation qui donne la mÃ©thode de sÃ©lection suivante :
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i
Nâ†’âˆâˆ¼= 1
N
Nâˆ‘
k=1
(
xki âˆ’ xi
) (
yk âˆ’ Ïˆ(xk, Ï‰)) âˆ‚Ïˆ(xk, Ï‰)
âˆ‚xi
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt variation des performances en test
(9)
Ruck et al. (1990) proposent la mÃ©thode suivante :ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i =
Nâˆ‘
k=1
âˆ‘
jâˆˆOutput
âˆ£âˆ£âˆ£âˆ£âˆ‚Ïˆj(xk, Ï‰)âˆ‚xi
âˆ£âˆ£âˆ£âˆ£
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt seuil : moyenne des pertinences
(10)
Refenes et Zapranis (1999) utilisent lâ€™Ã©lasticitÃ© moyenne de la sortie par rapport Ã  chaque
variable : ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i =
1
N
Nâˆ‘
k=1
âˆ£âˆ£âˆ£âˆ£âˆ‚Ïˆ(xk, Ï‰)âˆ‚xi Ã— xiÏˆ(xk, Ï‰)
âˆ£âˆ£âˆ£âˆ£
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt seuil : moyenne des pertinences
(11)
RÃ©duction des dimensions des donnÃ©es
Dans le cas des rÃ©seaux Ã  fonctions radiales RBF - Radial Basis Functions -, Dorizzi et al.
(1996) utilisent le quantile Ã  95% de la distribution des valeurs absolues des dÃ©rivÃ©es de chaque
variable. ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i = q.95
[âˆ£âˆ£âˆ£âˆ£âˆ‚Ïˆ(x, Ï‰)âˆ‚xi
âˆ£âˆ£âˆ£âˆ£]
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt seuil : moyenne des pertinences
(12)
Pour un problÃ¨me de discrimination, Rossi (1996) propose de ne considÃ©rer que les exemples
qui sont prÃ¨s des frontiÃ¨res interclasses :
xk âˆˆ frontier â‰¡ âˆ¥âˆ¥âˆ‡xkÏˆ(xk, Ï‰)âˆ¥âˆ¥ >  (13)
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i = 1|Output|
âˆ‘
xkâˆˆfrontier
âˆ‘
jâˆˆOutput
âˆ£âˆ£âˆ£âˆ‚Ïˆj(xk,Ï‰)âˆ‚xi âˆ£âˆ£âˆ£âˆ¥âˆ¥âˆ¥âˆ‚Ïˆj(xk,Ï‰)âˆ‚x âˆ¥âˆ¥âˆ¥
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt seuil : moyenne des pertinences
(14)
2.6.3 MÃ©thodes du second ordre
Pour estimer la pertinence dâ€™une variable, les mÃ©thodes du second ordre calculent la dÃ©ri-
vÃ©e seconde de la fonction de coÃ»t par rapport aux poids. Ces mesures sont des extensions des
techniques dâ€™Ã©lagage des poids. La technique dâ€™Ã©lagage la plus populaire est Optimal Brain
Damage (OBD) proposÃ©e par Le Cun et al. (1990). OBD est basÃ©e sur lâ€™estimation de la varia-
tion de la fonction de coÃ»t R(w) lorsquâ€™un poids est supprimÃ© du rÃ©seau. Cette variation peut
Ãªtre approximÃ©e Ã  lâ€™aide dâ€™un dÃ©veloppement en sÃ©rie de Taylor :
Î´RËœ(Ï‰i) =
âˆ‘
i
âˆ‚RËœ(Ï‰)
âˆ‚Ï‰i
Î´Ï‰i +
1
2
âˆ‘
i
âˆ‘
j
âˆ‚2RËœ(Ï‰)
âˆ‚Ï‰iâˆ‚Ï‰j
Î´Ï‰iÎ´Ï‰j +O
(
Î´Ï‰3
)
(15)
Sous lâ€™hypothÃ¨se que le rÃ©seau connexionniste a atteint un minimum local, le premier terme
de droite de cette formule est nul. Pour simplifier les calculs, Le Cun et al. (1990) supposent
en outre que la matrice Hessienne est nulle et le coÃ»t est localement quadratique. On obtient
Y. Bennani et al.
alors la formule simplifiÃ©e suivante :
Î´RËœ(Ï‰i) â‰ˆ 12
âˆ‘
i
âˆ‚2RËœ(Ï‰)
âˆ‚Ï‰2i
Î´Ï‰2i +O
(
Î´Ï‰3
)
(16)
â‰ˆ 1
2
HiiÎ´Ï‰
2
i (17)
La pertinence dâ€™une connexion est alors estimÃ©e par :
pertinence(Ï‰i) â‰ˆ 12HiiÏ‰
2
i (18)
La mÃ©thode de sÃ©lection de variables Optimal Cell Damage (OCD) dÃ©veloppÃ©e par Cibas
et al. (1994) est basÃ©e sur la mesure de pertinence ci-dessus. Dans OCD, lâ€™importance de
chaque variable sâ€™obtient en sommant les importances des connexions qui partent de celle-ci :
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i = 12
âˆ‘
jâˆˆfanâˆ’Out(i)
âˆ‚2RËœ(w)
âˆ‚Ï‰2ji
Ï‰2ji
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt test statistique
(19)
oÃ¹ fanâˆ’Out(i) est lâ€™ensemble des neurones qui utilisent comme entrÃ©e la sortie du neurone
i.
Dans OBD et OBS, la sensibilitÃ© dâ€™un poids ne peut Ãªtre Ã©valuÃ©e correctement quâ€™autour
dâ€™un minimum local de la fonction de coÃ»t. Tresp et al. (1996) proposent deux extensions
dâ€™OBD et dâ€™OBS : Early Brain Damage (EBD) et Early Brain Surgeon (EBS). EBD et EBS
peuvent Ãªtre utilisÃ©es avec le â€œearly stoppingâ€ comme critÃ¨re dâ€™arrÃªt de lâ€™apprentissage. Dans
EBD, par exemple, la sensibilitÃ© dâ€™un poids est donnÃ©e par la formule suivante :
pertinence(Ï‰i) =
1
2
âˆ‚2RËœ(w)
âˆ‚Ï‰2ji
Ï‰2ji âˆ’
âˆ‚RËœ(w)
âˆ‚Ï‰ji
Ï‰ji +
(
âˆ‚RËœ(w)
âˆ‚Ï‰ji
)2
âˆ‚2RËœ(w)
âˆ‚Ï‰2ji
(20)
A partir de cette dÃ©finition de pertinence et de la mÃªme faÃ§on que OCD, Leray et Gallinari
RÃ©duction des dimensions des donnÃ©es
(2001) propose la mÃ©thode ECD (Early Cell Damage) :ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
pertinence dâ€™une variable Î¶i =
1
2
âˆ‘
jâˆˆfanâˆ’Out(i)
âˆ‚2RËœ(w)
âˆ‚Ï‰2ji
Ï‰2ji âˆ’
âˆ‚RËœ(w)
âˆ‚Ï‰ji
Ï‰ji +
(
âˆ‚RËœ(w)
âˆ‚Ï‰ji
)2
âˆ‚2RËœ(w)
âˆ‚Ï‰2ji
critÃ¨re dâ€™Ã©valuation J(Xk) =
âˆ‘
xiâˆˆXk
Î¶i
procÃ©dure de recherche Backward
critÃ¨re dâ€™arrÃªt test statistique
(21)
Pour cette mÃ©thode on supprime les variables une par une et on peut utiliser la technique de
early stopping pour arrÃªter lâ€™apprentissage.
2.7 SÃ©lection de variables et apprentissage non supervisÃ©
Contrairement Ã  la sÃ©lection de variables pour les systÃ¨mes dâ€™apprentissage supervisÃ©, rela-
tivement peu dâ€™approches ont Ã©tÃ© proposÃ©es pour lâ€™apprentissage non-supervisÃ© (classification
automatique ou clustering). En effet, le problÃ¨me de la sÃ©lection de variables en classification
automatique est un problÃ¨me beaucoup plus difficile que dans le cas supervisÃ© (discrimination)
oÃ¹ les donnÃ©es sont Ã©tiquetÃ©es (Guyon et al., 2006). Un autre problÃ¨me important associÃ© Ã 
la classification concerne la dÃ©termination automatique du nombre de groupes (clusters) qui
est clairement influencÃ© par lâ€™issue de la sÃ©lection des variables. Enfin, la question ouverte est
comment Ã©valuer/comparer les rÃ©sultats de plusieurs classifications ?
Le thÃ©orÃ¨me dâ€™impossibilitÃ© proposÃ© par (Kleinberg, 2002) indique quâ€™il nâ€™existe pas de
mÃ©thode de classification qui vÃ©rifie simultanÃ©ment les trois propriÃ©tÃ©s suivantes :
â€¢ invariance Ã  lâ€™unitÃ© de mesure des distances : la multiplication par un scalaire de distance
utilisÃ©e par un algorithme ne modifie pas la partition quâ€™il dÃ©couvre,
â€¢ exhaustivitÃ© : pour toute partition de lâ€™ensemble des individus, il existe une distance qui
permette Ã  lâ€™algorithme de classification de la dÃ©couvrir,
â€¢ consistance : si une partition de lâ€™ensemble des individus.
2.7.1 Approches filtres non supervisÃ©es
La majeure partie des approches filtres que lâ€™on rencontre en apprentissage non supervisÃ©
peuvent Ãªtre regroupÃ©es en deux catÃ©gories :
â€¢ celles qui sâ€™appuient sur le calcul de corrÃ©lations ou sur lâ€™estimation de lâ€™information
mutuelle entre variables et qui permettent dâ€™Ã©liminer soit les variables redondantes (Mi-
tra et al., 2002; Vesanto et Alhoniemi, 2000), soit les variables non pertinentes (Sorg-
Madsen et al., 2003) ;
â€¢ celles qui se fondent sur la notion de densitÃ© de lâ€™espace des donnÃ©es (Dash et al., 2002;
He et al., 2006; Pal et al., 2000).
Y. Bennani et al.
CorrÃ©lation et information mutuelle
On considÃ¨re souvent quâ€™un sous-ensemble optimal de variables pertinentes doit Ãªtre mi-
nimal, cela a conduit certains auteurs Ã  proposer des mÃ©thodes de sÃ©lection de variables qui
se focalise sur lâ€™Ã©limination des dimensions redondantes. Ainsi, Mitra et al. (2002) dÃ©finissent
lâ€™indice de compression maximale de lâ€™information (maximal information compression index)
comme la plus petite valeur propre de la matrice de corrÃ©lation des variables prises deux Ã 
deux et proposent une procÃ©dure itÃ©rative dâ€™Ã©limination des attributs redondants en sâ€™appuyant
sur cette mesure de dissimilaritÃ©. Vesanto et Ahola (1999) proposent une dÃ©tection visuelle des
corrÃ©lations en se basant sur la construction dâ€™une carte auto-organisÃ©es.
Ces deux approches sâ€™appuient sur une mesure de corrÃ©lation linÃ©aire entre variables et
elles ne permettent dâ€™Ã©liminer quâ€™une partie de la redondance. Ainsi, si on considÃ¨re un couple
de variables alÃ©atoires (X,Y ) tel que pour toute rÃ©alisation x de la variableX , la rÃ©alisation de
Y est y = x2, le coefficient de corrÃ©lation linÃ©aire entre ces deux variables sera proche de zÃ©ro
bien que lâ€™ensemble {X,Y } soit redondant car la rÃ©alisation de la variable Y peut se dÃ©duire
sans peine de celle de X . De maniÃ¨re naturelle, on peut penser lever cette limitation majeure
en remplaÃ§ant la mesure de corrÃ©lation linÃ©aire par une mesure dâ€™information mutuelle dont
nous rappelons la dÃ©finition ci-dessous :
I(X,Y ) = âˆ’
âˆ«
x,y
p(X = x, Y = y) log
p(X = x, Y = y)
p(X = x) p(Y = y)
dx (22)
= H(X) +H(Y )âˆ’H(X,Y ) (23)
avec H(X) = âˆ’
âˆ«
x
p(X = x) log p(X = x) dx (24)
oÃ¹ H(X) est lâ€™entropie au sens de Shanon associÃ©e Ã  la variable alÃ©atoire X . NÃ©anmoins,
lâ€™Ã©valuation de cette mesure nÃ©cessite de connaÃ®tre dâ€™une part les densitÃ©s de probabilitÃ© des
variables alÃ©atoiresX et Y , et dâ€™autre part leur densitÃ© de probabilitÃ© conjointe. Bien entendu,
ces informations ne sont en pratique pas disponibles et lâ€™estimation, Ã  la fois rigoureuse et
efficace, de lâ€™information mutuelle demeure un problÃ¨me difficile (Kraskov et al., 2004).
Lorsquâ€™on cherche Ã  segmenter un ensemble dâ€™individus en apprentissage non supervisÃ©,
on considÃ¨re quâ€™une dimension pertinente est gÃ©nÃ©ralement liÃ©e Ã  une variable latente qui in-
dique le groupe des observations. Ainsi, en supposant quâ€™il existe au moins deux variables per-
tinentes, (Sorg-Madsen et al., 2003) proposent dâ€™utiliser une stratÃ©gie ascendante guidÃ©e par
une mesure de dÃ©pendance entre chaque couple de variables ; les dimensions indÃ©pendantes de
toutes les autres sont alors associÃ©es Ã  du bruit et sont Ã©liminÃ©es. Ils utilisent deux mesures de
liaisons diffÃ©rentes : lâ€™information mutuelle et le pouvoir prÃ©dictif mutuel.
Utilisation de la densitÃ© des donnÃ©es
Lâ€™objectif de lâ€™apprentissage non supervisÃ© est de dÃ©couvrir et de comprendre la structure
dâ€™un ensemble dâ€™individus ; ainsi, une variable distribuÃ©e uniformÃ©ment peut Ãªtre considÃ©rÃ©e
comme non pertinente car elle ne met aucune structure en exergue. En se basant sur cette
observation, Dash et al. (2002) proposent une nouvelle mesure dâ€™entropie pour guider une ap-
proche ascendante. Une formulation lÃ©gÃ¨rement diffÃ©rente de cette observation a Ã©tÃ© proposÃ©e
par He et al. (2006) : Â« dans de nombreux problÃ¨mes dâ€™apprentissage comme la classification,
RÃ©duction des dimensions des donnÃ©es
la structure locale de lâ€™espace des donnÃ©es est plus importante que la structure globale. Â» Ils
proposent alors une mesure dâ€™Ã©valuation, le score laplacien (Laplacian Score), qui est basÃ©e
sur le respect de la structure dâ€™un graphe de voisinage entre les individus. AntÃ©rieurement, Pal
et al. (2000) avaient utilisÃ© un degrÃ© dâ€™appartenance de deux individus Ã  un mÃªme ensemble
flou quâ€™ils faisaient apprendre Ã  un rÃ©seau neuromimÃ©tique de type perceptron multicouche.
Les poids du rÃ©seaux indiquent alors la contribution de chaque dimension au degrÃ©s dâ€™appar-
tenance et peuvent utilisÃ©s commme mesure dâ€™Ã©valuation.
Approches symbioses et intÃ©grÃ©es
Sorg-Madsen et al. (2003) complÃ¨tent leur approche Â« filtre Â» prÃ©sentÃ©e plus haut en lâ€™hy-
bridant avec une approche Â« symbiose Â» : aprÃ¨s avoir Ã©liminer les variables non pertinentes,
ils estiment les paramÃ¨tres dâ€™un modÃ¨le de mÃ©lange. Ils se ramÃ¨nent ainsi au cas de lâ€™appren-
tissage supervisÃ© et construisent un classificateur naÃ¯f de Bayes dont la prÃ©cision est utilisÃ©e
pour guider une procÃ©dure de recherche ascendante. Dans Dy et Brodley (2000, 2004), les
auteurs restent dans le cadre non supervisÃ© en proposant diffÃ©rentes approches ascendantes ba-
sÃ©es sur les modÃ¨les de mÃ©langes qui sont guidÃ©es soit le maximum de vraisemblance, soit la
sÃ©parabilitÃ© des classes.
GuÃ©rif et Bennani (2006) utilisent une classification Ã  deux niveaux combinÃ©e Ã  la valeur
test (Morineau, 1984) pour identifier les variables les plus significatives ; le premier niveau de
la classification est formÃ©e par une carte auto-organisÃ©e (Kohonen, 2001) qui est segmentÃ©e en
utilisant lâ€™algorithme des k-moyennes associÃ© Ã  lâ€™indice de Davies-Bouldin (Davies et Bouldin,
1979) pour fixer le nombre de groupes (Vesanto et Alhoniemi, 2000). La statistique Î› de Wilks
est utilisÃ©e pour stopper leur procÃ©dure en se basant sur la sÃ©parabilitÃ© des classes.
La sÃ©lection de variables peut Ãªtre vue comme un problÃ¨me de sÃ©lection de modÃ¨les. Ainsi,
Raftery et Dean (2006) adoptent une recherche sÃ©quentielle bidirectionnelle et ils retiennent
le modÃ¨le optimal au sens du critÃ¨re BIC (Bayesian Information Criterion) (Schwarz, 1978).
Leur mÃ©thode permet de considÃ©rer Ã  chaque Ã©tape diffÃ©rents modÃ¨les plus ou moins contraints
comportant un nombre variable de groupes. Dans Law et al. (2004), les auteurs dÃ©finissent une
mesure de saillance (saliency) et ajoutent de nouveaux paramÃ¨tres aux modÃ¨les de mÃ©lange
pour intÃ©grer la sÃ©lection de variables directement Ã  la fonction de coÃ»t optimisÃ©e par lâ€™al-
gorithhme EM. Ils dÃ©terminent le nombre de composantes du mÃ©lange selon le critÃ¨re MML
(Minimum Message Length).
2.7.2 Evaluation et critÃ¨res de validitÃ©
Dans le contexte de la classification automatique, il est naturel de sâ€™interroger sur la vali-
ditÃ© de la partition obtenue. Les groupes dÃ©couverts correspondent-ils Ã  nos connaissances Ã 
priori ? Correspondent-ils vraiment Ã  lâ€™ensemble dâ€™objets dont on dispose ? De deux classifica-
tions, laquelle est la plus pertinente ? Ces diffÃ©rentes questions permettent de distinguer trois
catÃ©gories de critÃ¨res (Jain et Dubes, 1988) :
â€¢ les critÃ¨res externes qui permettent de rÃ©pondre Ã  la premiÃ¨re question et de mesurer
lâ€™adÃ©quation entre une partition et les connaissances Ã  priori dont on dispose ;
â€¢ les critÃ¨res internes qui quantifient lâ€™adÃ©quation entre une partition et lâ€™idÃ©e subjective
que lâ€™on se fait dâ€™une Â« bonne Â» classification ; ainsi, les propriÃ©tÃ©s les plus communÃ©-
ment recherchÃ©es sont la compacitÃ© et la sÃ©parabilitÃ© des groupes dÃ©couverts ;
Y. Bennani et al.
â€¢ les critÃ¨res relatifs qui sâ€™intÃ©ressent Ã  la troisiÃ¨me et derniÃ¨re question et Ã  dÃ©faut de
donner une apprÃ©ciation absolue de la validitÃ© dâ€™une partition, ils permettent dâ€™ordonner
plusieurs classifications et dâ€™en choisir Â« une meilleure Â».
CritÃ¨res externes
Les critÃ¨res externes se ramÃ¨nent au problÃ¨me ancien de la comparaison de partitions et une
littÃ©rature abondante est disponible sur le sujet (Fowlkes et Mallows, 1983; Hubert et Arabie,
1985; MeilaË˜, 2003, 2005, 2007; Rand, 1971; Wallace, 1983). Une maniÃ¨re simple de comparer
deux partitions C et Câ€² consiste Ã  contruire une table de contingence (figure 3) qui donne une
apprÃ©ciation intuitive de leur adÃ©quation. Le calcul de la plupart des critÃ¨res sâ€™appuie dâ€™ailleurs
soit directement sur cette table soit un comptage des accords et des dÃ©saccords que lâ€™on peut
dÃ©duire Ã  lâ€™aide des formules de linÃ©arisation suivantes (Hubert et Arabie, 1985; Jain et Dubes,
1988) :
N00 =
1
2
ï£«ï£­n2 + Kâˆ‘
i=1
Kâ€²âˆ‘
j=1
n2ij âˆ’
ï£«ï£­ Kâˆ‘
i=1
n2i. +
Kâ€²âˆ‘
j=1
n2.j
ï£¶ï£¸ï£¶ï£¸ (25)
N11 =
1
2
Kâˆ‘
i=1
Kâ€²âˆ‘
j=1
nij(nij âˆ’ 1) (26)
N01 =
1
2
ï£«ï£­ Kâ€²âˆ‘
j=1
n2.j âˆ’
Kâˆ‘
i=1
Kâ€²âˆ‘
j=1
n2ij
ï£¶ï£¸ (27)
N10 =
1
2
ï£«ï£­ Kâ€²âˆ‘
i=1
n2i. âˆ’
Kâˆ‘
i=1
Kâ€²âˆ‘
j=1
n2ij
ï£¶ï£¸ (28)
Les nombres de paires dâ€™objets qui sont sÃ©parÃ©s ou regroupÃ©s dans les deux partitions sont
notÃ©s respectivement N00 et N11. N01 indique le nombre de paires dâ€™objets sÃ©parÃ©s dans la
premiÃ¨re partition et regroupÃ©s dans la seconde. De maniÃ¨re analogue, N10 dÃ©signe le nombre
de paires dâ€™objets regroupÃ©s dans la premiÃ¨re partition et sÃ©parÃ©s dans la seconde.
Câ€²1 . . . Câ€²j . . . Câ€²Kâ€²
C1 n11 . . . n1j . . . n1Kâ€² n1.
...
...
. . .
...
. . .
...
...
Ci ni1 . . . nij . . . niKâ€² ni.
...
...
. . .
...
. . .
...
...
CK nK1 . . . nKj . . . nKKâ€² nK.
n.1 . . . n.j . . . n.Kâ€² N
FIG. 3 â€“ Exemple de table de contingence entre de deux partitions C = {Ci : i = 1, . . . ,K}
et Câ€² = {Câ€²j : j = 1, . . . ,K â€²} dâ€™un mÃªme ensemble de N objets ; les marges ni. et n.j
indiquent respectivement les effectifs des classes Ci et Câ€²j .
RÃ©duction des dimensions des donnÃ©es
Il convient de remarquer que les critÃ¨res de comparaison que lâ€™on peut construire Ã  partir
de N00, N11, N01 et N10 correspondent Ã  des mesures de dissimilaritÃ© binaires dont un grand
nombre peuvent sâ€™exprimer sous la forme suivante (Li, 2006) :
dÎ±,Î´ =
N10 +N01
Î±N11 +N10 +N01 + Î´N00
(29)
oÃ¹ Î± et Î´ sont deux paramÃ¨tres qui permettent de pondÃ©rer la prise en compte respective des
regroupement ou sÃ©paration simultanÃ©es dâ€™une paire dâ€™objets dans deux partitions. La table 1
rappelle la dÃ©finition de quelques mesures et le lecteur intÃ©ressÃ© en trouvera une prÃ©sentation
plus complÃ¨te de ces mesures ou de leur propriÃ©tÃ© dans Albatineh et al. (2006), Jouve et al.
(2001), LourenÃ§o et al. (2004), Roux (1985) et Li (2006).
Mesure SimilaritÃ© DissimilaritÃ©
Sokal & Sneath (I)
1
2N11
1
2N11+N10+N01
N10+N01
1
2N11+N10+N01
Rogers & Tanimoto
1
2 (N11+N00)
1
2 (N11+N00)+N10+N01
N10+N01
1
2 (N11+N00)+N10+N01
Jaccard N11N11+N10+N01
N10+N01
N11+N01+N10
Rand N11+N00N11+N10+N01+N00
N10+N01
N11+N10+N01+N00
Czekanowski-Dice 2N112N11+N10+N01
N10+N01
2N11+N10+N01
Sokal & Sneath (II) 2(N11+N00)2(N11+N00)+N10+N01
N10+N01
2(N11+N00)+N10+N01
Kulczynski (II) 12
(
N11
N11+N10
+ N11N11+N01
)
1âˆ’ 12
(
N11
N11+N10
+ N11N11+N01
)
Ochiai N11âˆš
(N11+N10)(N11+N01)
1âˆ’ N11âˆš
(N11+N10)(N11+N01)
Russel & Rao N11N11+N10+N01+N00 1âˆ’ N11N11+N10+N01+N00
TAB. 1 â€“ Quelques mesures de similaritÃ© et de dissimilaritÃ© binaire.
Dâ€™autres critÃ¨res se calculent directement Ã  partir de la table de contingence. Le critÃ¨re de
Larsen, le critÃ¨re de MeilaË˜ & Heckerman (MeilaË˜, 2005, 2006), la distance de transfert maxi-
mum (Charon et al., 2006) ou la variation dâ€™information (MeilaË˜, 2003, 2005, 2007) en sont des
exemples. Enfin, il convient de rappeler quâ€™une part non nÃ©gligeable de la similaritÃ© entre deux
partitions doit Ãªtre attribuÃ©e au hazard et que cela a conduit de nombreux auteurs Ã  proposer
des mÃ©thodes de correction dâ€™indices ; le lecteur qui souhaite approfondir cette question est
invitÃ© Ã  consulter Albatineh et al. (2006), Fowlkes et Mallows (1983), Hubert et Arabie (1985)
ou encore MeilaË˜ (2007).
Y. Bennani et al.
CritÃ¨res internes
Les critÃ¨res internes visent Ã  quantifier lâ€™adÃ©quation entre une partition et lâ€™idÃ©e subjective
que lâ€™on se fait dâ€™une Â« bonne Â» classification en se basant uniquement sur les propriÃ©tÃ©s des
donnÃ©es. Les valeurs de ce type de critÃ¨re sont gÃ©nÃ©ralement trÃ¨s dÃ©pendantes du jeu de don-
nÃ©es utilisÃ© et dÃ©terminer une valeur de rÃ©fÃ©rence peut sâ€™avÃ©rer coÃ»teux. Nous ne dÃ©taillerons
pas ce point dans le cadre de cet article mais le lecteur est invitÃ© Ã  consulter Jain et Dubes
(1988) pour obtenir davantage dâ€™information.
CritÃ¨res relatifs
Les critÃ¨res relatifs sont les plus largement utilisÃ©s et permettent dâ€™ordonner diffÃ©rentes
partitions en fonction de lâ€™idÃ©e subjective que lâ€™on se fait dâ€™une Â« bonne Â» classification ; ainsi,
on recherche gÃ©nÃ©ralement des groupes compacts et bien sÃ©parÃ©s. Etablir une liste exhaustive
de ce type de critÃ¨res dÃ©passe largement le cadre de cet article et seules les dÃ©finitions de deux
indices de ce type sont rappelÃ©es ci-dessous :
â€“ Indice de Dunn : dans le cas dâ€™une classification dure, lâ€™indice de Dunn (Halkidi et al.,
2001, 2002a,b) tient compte Ã  la fois de la compacitÃ© et de la sÃ©parabilitÃ© des groupes :
la valeur de cet indice est dâ€™autant plus faible que les groupes sont compacts et bien
sÃ©parÃ©s. Notons que la complexitÃ© de lâ€™indice de Dunn devient prohibitive dÃ¨s quâ€™on
manipule de grands ensembles dâ€™objets ; il est par consÃ©quent rarement utilisÃ©.
IDunn =
min{Dmin(Ci, Cj) : i 6= j}
max{Smax(Ci)} (30)
oÃ¹Dmin(Ci, Cj) est la distance minimale qui sÃ©pare un objet du groupe Ci dâ€™un objet du
groupe Cj et oÃ¹ Smax(Ci) est la distance maximale qui sÃ©pare deux objets du groupe Ci :
Dmin(Ci, Cj) = min {â€–xâˆ’ yâ€– : x âˆˆ Ci et y âˆˆ Cj} (31)
Smax(Ci) = max {â€–xâˆ’ yâ€– : (x, y) âˆˆ Ci Ã— Ci} (32)
â€“ Indice de Davies-Bouldin : dans le cas dâ€™une classification dure, lâ€™indice de Davies-
Bouldin (Davies et Bouldin, 1979) tient compte Ã  la fois de la compacitÃ© et de la sÃ©-
parabilitÃ© des groupes : la valeur de cet indice est dâ€™autant plus faible que les groupes
sont compacts et bien sÃ©parÃ©s. Cet indice dont la complexitÃ© en Î¸ (K Ã— (N +K)) est
raisonnable favorise les groupes hypersphÃ©riques et il est donc particuliÃ¨rement bien
adaptÃ© pour une utilisation avec la mÃ©thode des K-moyennes.
IDB =
1
K
Kâˆ‘
k=1
max
l 6=k
{
Sc(Ck) + Sc(Cl)
Dce(Ck, Cl
}
(33)
oÃ¹ Sc(Ci) est la distance moyenne entre un objet du groupe Ci et son centre, et oÃ¹
Dce(Ci, Cj) est la distance qui sÃ©pare les centres des groupes Ci et Cj :
Sc(Ci) = 1
Ni
Niâˆ‘
i=1
â€–xâˆ’ Ï‰iâ€– (34)
Dce(Ci, Cj) = â€–Ï‰i âˆ’ Ï‰jâ€– (35)
RÃ©duction des dimensions des donnÃ©es
2.7.3 Approches symbioses et intÃ©grÃ©es non supervisÃ©es
De nombreuses approches de sÃ©lection de variables pour la classification automatique uti-
lisent les modÃ¨les de mÃ©langes comme cadre thÃ©orique (Dy et Brodley, 2000, 2004; Law et al.,
2004; Raftery et Dean, 2006; Sorg-Madsen et al., 2003) et sâ€™appuient sur lâ€™algorithme EM
(Expectation Maximization) (Dempster et al., 1977).
Principes des modÃ¨les de mÃ©lange
On suppose que lâ€™ensemble dâ€™individus dont on dispose a Ã©tÃ© obtenu en fusionnant plu-
sieurs sous-populations qui suivent chacune une loi de probabilitÃ© propre. La probabilitÃ© quâ€™un
individu x soit issu de ce mÃ©lange de paramÃ¨tres Î¸ = (Î±1, Î¸1, . . . , Î±i, Î¸i, . . .) est alors donnÃ©e
par :
p(x|Î¸) =
âˆ‘
i
Î±i Ã— pi(x|Î¸i) (36)
oÃ¹ les coefficients de mÃ©lange Î±i satisfont
âˆ‘
i Î±i = 1, et oÃ¹ les densitÃ©s de probabilitÃ© de
chaque sous-population Ci sont donnÃ©es par les lois pi(x|Î¸i) de paramÃ¨tres Î¸i. Rappelons que
toute distribution continue peut Ãªtre approximÃ©e Ã  lâ€™aide dâ€™un modÃ¨le de mÃ©lange dÃ¨s lors que
ses composantes sont assez nombreuses et que leurs paramÃ¨tres sont bien choisis.
Lâ€™estimation du nombre et des paramÃ¨tres de composantes est un problÃ¨me difficile et dans
la plupart des applications seuls les mÃ©langes de lois normales sont considÃ©rÃ©s. Lorsquâ€™on
impose de plus que toutes les lois normales du mÃ©lange aient la matrice identitÃ© commematrice
de covariance, on retrouve le cas des k-moyennes.
Algorithme EM
Lâ€™algorithme le plus rÃ©pandu pour estimer les paramÃ¨tres dâ€™un mÃ©lange est lâ€™algorithme
EM (Expectation Maximization) introduit par Dempster et al. (1977). Il consiste Ã  itÃ©rer les
deux phases suivantes jusquâ€™Ã  ce que lâ€™amÃ©lioration de la log vraisemblance du modÃ¨le soit
infÃ©rieure Ã  un seuil  > 0 fixÃ© :
1. Estimation : on suppose fixÃ©s les paramÃ¨tres Î¸Ë† =
(
Î±Ë†1, Î¸Ë†1, Î±Ë†2, Î¸Ë†2, . . .
)
du modÃ¨le et
on calcule la probabilitÃ© p(x|Î¸Ë†i) quâ€™un objet x âˆˆ â„¦ ait Ã©tÃ© gÃ©nÃ©rÃ© par la composante
correspondant Ã  la sous-population Ci :
p(x|Î¸Ë†i) = Î±i Ã— p(x|Î¸Ë†i)âˆ‘
k Î±k Ã— p(x|Î¸Ë†k)
(37)
2. Maximisation : on suppose cette fois fixÃ©e la partition floue de lâ€™ensemble des objets
x âˆˆ â„¦ dont les degrÃ©s dâ€™appartenance sont donnÃ©s par les probabilitÃ©s p(x|Î¸Ë†i). On
cherche alors les paramÃ¨tres Î¸Ëœ du modÃ¨le qui maximisent sa log vraisemblance
logL(Î¸|â„¦) =
âˆ‘
xâˆˆâ„¦
p(x|Î¸) (38)
Î¸Ëœ = argmax
Î¸
{logL(Î¸|â„¦)} (39)
Y. Bennani et al.
Les coefficients optimaux du mÃ©lange sont dÃ©finis par :
Î±Ëœi =
1
N
âˆ‘
xâˆˆâ„¦
xÃ— p(x|Î¸Ë†i) (40)
oÃ¹ N est le nombre dâ€™individus prÃ©sents dans â„¦.
2.7.4 StabilitÃ©
Une Ã©tude de la stabilitÃ© dâ€™une classification est un moyen dâ€™Ã©valuer la validitÃ© des groupes
formÃ©s ; en dâ€™autres termes, cela permet de vÃ©rifier que les groupes formÃ©s ne sont pas le fruit
du hasard mais correspondent effectivement Ã  une structure cachÃ©e mais prÃ©sente dans les don-
nÃ©es. Ainsi, Ben-Hur et al. (2002) proposent de combiner les techniques de rÃ©-Ã©chantillonnage
Ã  lâ€™indice de Jaccard pour dÃ©terminer le nombre de groupes naturels dâ€™un ensemble dâ€™indivi-
dus. Fred et Jain (2002, 2005) dÃ©finissent une nouvelle mesure de similaritÃ© entre individus en
sâ€™appuyant sur la stabilitÃ© des groupes formÃ©s par diffÃ©rents algorithmes ou en utilisant diffÃ©-
rentes valeurs de paramÃ¨tres ; deux individus sont dâ€™autant plus similaires quâ€™ils sont souvent
regroupÃ©s.
2.8 RÃ©duction des dimensions par extraction de traits
Les mÃ©thodes utilisÃ©es pour lâ€™extraction de traits sont trÃ¨s variÃ©es, et nous ne prÃ©tendons pas
de ce court document en faire le tour. Nous rappellerons briÃ¨vement les principes des mÃ©thodes
linÃ©aires (ACP, MDS), puis dÃ©crirons quelques mÃ©thodes non linÃ©aires qui ont fait lâ€™objet de
nombreuses Ã©tudes depuis cinq ans. Nous nous intÃ©ressons en particulier aux mÃ©thodes utilisant
des graphes, comme Isomap, LLE et leurs variantes. Dans le cadre du projet InfoMagic, nous
nous pencherons plus particuliÃ¨rement sur les applications de ces mÃ©thodes au traitement des
donnÃ©es textuelles.
On considÃ¨re un espace dâ€™observations Ï‡, qui nâ€™est pas nÃ©cessairement Rn, ce qui permet
de gÃ©nÃ©raliser les mÃ©thodes proposÃ©es aux cas oÃ¹ lâ€™on ne dispose pas dâ€™une reprÃ©sentation vec-
torielle des donnÃ©es Ã  traiter, par exemple les donnÃ©es structurÃ©es (arbres ou graphes). Lâ€™espace
de caractÃ©ristiques H est reliÃ© Ã  lâ€™espace dâ€™observation par une application :
Î¦ : Ï‡â†’ H
x 7â†’ Ï†(x)
Les donnÃ©es dâ€™apprentissage sont un ensemble fini de points {xi}, ou bien, dans le cas de
lâ€™apprentissage supervisÃ©, un ensemble fini de couples (point, Ã©tiquette) {(xi, yi)}.
2.8.1 MÃ©thodes linÃ©aires
Nous rappelons briÃ¨vement les principes de deux mÃ©thodes classiques dâ€™analyse de don-
nÃ©es, qui sont le fondement de plusieurs mÃ©thodes non linÃ©aires plus rÃ©centes.
RÃ©duction des dimensions des donnÃ©es
Analyse en Composantes Principales (ACP)
Lâ€™analyse en composantes principales (ACP) est une ancienne approche (aussi connue sous
le nom de transformation de Karhunen Loeve dans la communautÃ© du traitement de signal), qui
effectue une rÃ©duction de dimension par projection des points orignaux dans un sous-espace
vectoriel de dimension plus rÃ©duite. Lâ€™ACP dÃ©termine des axes de projections orthogonaux,
qui maximisent la variance expliquÃ©e. Dans la base formÃ©e par ces axes, les coordonnÃ©es ne
sont pas corrÃ©lÃ©es.
Lâ€™ACP maximise la variance de la projection dans lâ€™espace de caractÃ©ristiques, ce qui est
Ã©quivalent Ã  minimiser lâ€™erreur quadratique moyenne de reconstruction.
Lâ€™ACP se calcule en diagonalisant la matrice de corrÃ©lations, le plus souvent en utilisant
une dÃ©composition en valeurs singuliÃ¨res (SVD). Lâ€™analyse en composantes principales est trÃ¨s
utilisÃ©e car elle est simple Ã  mettre en oeuvre. Elle est limitÃ©e par son caractÃ¨re linÃ©aire : il est
facile dâ€™imaginer des situations dans lesquelles lâ€™ACP nâ€™apporte aucune information utilisable
(par exemple, des donnÃ©es rÃ©parties sur un tore en dimension n).
Multi-Dimensional Scaling (MDS)
Dans de nombreux cas, on connaÃ®t les distances entre les points dâ€™un ensemble dâ€™apprentis-
sage (on peut utiliser une mesure de similaritÃ© plus sophistiquÃ©e que la distance euclidienne,
comme indiquÃ©e dans la section suivante), et on cherche Ã  obtenir une reprÃ©sentation en faible
dimension de ces points. La mÃ©thode de positionnement multidimensionnel (MDS) permet de
construire cette reprÃ©sentation. Lâ€™exemple classique est dâ€™obtenir la carte dâ€™un pays en partant
de la connaissance des distances entre chaque paire de villes. Lâ€™algorithme MDS est basÃ© sur
une recherche de valeurs propres.
MDS permet de construire une configuration de m points dans Rd Ã  partir des distances
entrem objets. On observe doncm(mâˆ’ 1)/2 distances. Il est toujours possible de gÃ©nÃ©rer un
positionnement de m points en m dimensions qui respecte exactement les distances fournies.
MDS calcule une approximation en dimension d < m.
Lâ€™algorithme est le suivant :
â€“ Moyennes des distances carrÃ©es par rangÃ©es : Âµi =
1
n
âˆ‘
j
Dij
â€“ Double centrage (distance carrÃ©e vers produit scalaire) :
Pij = âˆ’12
(
Dij âˆ’ Âµi âˆ’ Âµj +
âˆ‘
i
Âµi
)
â€“ Calcul des vecteurs propres vj et valeurs propres Î»j principales de la matrice P (avec
les Î»2j les plus grands).
â€“ La i-Ã¨me coordonnÃ©e rÃ©duite de lâ€™exemple j est
âˆš
Î»j vij
Notons que la matrice de distanceD doit Ãªtre semi dÃ©finie positive. Les mÃ©thodes linÃ©aires
comme lâ€™ACP et le MDS ne donnent des rÃ©sultats intÃ©ressants que si les donnÃ©es sont situÃ©es
sur un sous-espace linÃ©aire. Elles ne peuvent traiter le cas oÃ¹ les donnÃ©es sont sur une variÃ©tÃ©
trÃ¨s non linÃ©aire.
Y. Bennani et al.
2.8.2 MÃ©thodes non-linÃ©aires
Les mÃ©thodes linÃ©aires reposent (au moins implicitement) sur lâ€™utilisation dâ€™une distance
euclidienne (liÃ©e au produit scalaire ordinaire). Dans de nombreuses applications, la distance
euclidienne nâ€™a pas grand sens ; elle suppose en particulier que toutes les variables sont com-
parables entre elles (elles doivent donc avoir Ã©tÃ© convenablement normalisÃ©es). La thÃ©orie des
espaces de Hilbert permet de dÃ©finir dâ€™autres produits scalaires, basÃ©s sur des fonctions noyaux
k(x, y). k est alors une mesure de similaritÃ© entre les points de lâ€™ensemble Ã  traiter. Le noyau k
dÃ©fini implicitement une application de lâ€™espace dâ€™origine vers un "espace de caractÃ©ristiques"
H . La dimension de lâ€™espace H est Ã©ventuellement infinie. De nombreuses mÃ©thodes statis-
tiques peuvent sâ€™exprimer en ne recourant quâ€™Ã  des produits scalaires entre les points Ã  traiter
et les exemples dâ€™apprentissage. Si lâ€™on remplace le produit scalaire habituel par un noyau k,
on rend la mÃ©thode non-linÃ©aire ; câ€™est le "truc du noyau" (kernel trick), qui a fait lâ€™objet de
nombreuses recherches depuis son introduction par Vapnik Boser et al. (1992) dans le cadre
des machines Ã  vecteurs de support (SVM).
La notion de noyau peut Ãªtre utilisÃ©e pour la rÃ©duction de dimension, comme nous allons
le voir dans la section suivante.
Kernel PCA
La premiÃ¨re approche permettant dâ€™appliquer lâ€™ACP au cas de donnÃ©es situÃ©es sur une va-
riÃ©tÃ© non linÃ©aire est dâ€™effectuer des approximations locales : on calcule une ACP pour un
groupe de points proches les uns des autres. Cette approche pose le problÃ¨me de la dÃ©finition
des voisinages et du traitement des points nouveaux rencontrÃ©s loin des exemples connus.
Une autre approche, formalisÃ©e par B. SchÃ¶lkopf en 1998 SchÃ¶lkopf et al. (1999), utilise le
kernel trick pour rendre non linÃ©aire lâ€™ACP traditionnelle. En effet, le calcul de lâ€™ACP ne fait
intervenir que des produits scalaires entre les points (pour le calcul de la matrice de covariance)
et ne considÃ¨re jamais les coordonnÃ©es dâ€™un point isolÃ©. Si lâ€™on remplace le produit scalaire
par un noyau, on calcule donc les composantes principales dans lâ€™espace de caractÃ©ristiques
H , et on peut ainsi accÃ©der Ã  des corrÃ©lations dâ€™ordre supÃ©rieur entre les variables observÃ©es.
Remarquons que lâ€™on peut calculer la projection dâ€™un point ne faisant pas partie de lâ€™ensemble
dâ€™apprentissage, ce qui nâ€™est pas le cas de toutes les mÃ©thodes de rÃ©duction de dimension non
linÃ©aires.
Isometric feature mapping (Isomap)
Isomap (Tenenbaum et al., 2000) est une mÃ©thode de rÃ©duction de dimension qui, comme
MDS, part de la connaissance de la matrice des distances entre les paires de points. Le but est
cette fois de trouver une variÃ©tÃ© (non linÃ©aire) contenant les donnÃ©es. On exploite le fait que
pour des points proches, la distance euclidienne est une bonne approximation de la distance
gÃ©odÃ©sique sur la variÃ©tÃ©. On construit un graphe reliant chaque point Ã  ses k plus proches
voisins. Les longueurs des gÃ©odÃ©siques sont alors estimÃ©es en cherchant la longueur du plus
court chemin entre deux points dans le graphe. On peut alors appliquer MDS aux distances
obtenues afin dâ€™obtenir un positionnement des points dans un espace de dimension rÃ©duite.
Locally Linear Embedding (LLE)
LLE (locally linear embedding, ou plongement localement linÃ©aire) (Roweis et Saul, 2000)
RÃ©duction des dimensions des donnÃ©es
a Ã©tÃ© prÃ©sentÃ© en mÃªme temps quâ€™Isomap et aborde le mÃªme problÃ¨me par une voie diffÃ©rente.
Chaque point est ici caractÃ©risÃ© par sa reconstruction Ã  partir de ses plus proches voisins. LLE
construit une projection vers un espace linÃ©aire de faible dimension prÃ©servant le voisinage.
Segmentation spectrale (spectral clustering)
La segmentation spectrale (spectral clustering) (Weiss, 1999; Ng et al., 2002) est une tech-
nique de rÃ©duction de dimension couplÃ©e Ã  une segmentation. Le but est de regrouper les
donnÃ©es de chaque segment (cluster) sur une sous-variÃ©tÃ© linÃ©aire sÃ©parÃ©e de faible dimension.
MÃ©thodes supervisÃ©es (S-Isomap)
Lorsque des informations sur les classes prÃ©sentes dans les donnÃ©es sont disponibles (clas-
sification supervisÃ©e), il est possible dâ€™en tenir compte lors de la construction de la matrice
de distances utilisÃ©e par les mÃ©thodes de rÃ©duction de dimension (Vlachos et al., 2002; Geng
et al., 2005). Cette technique permet Ã  peu de frais dâ€™amÃ©liorer la prÃ©cision du rÃ©sultat. Les au-
teurs proposent de lâ€™utiliser pour construire un classificateur, qui semble obtenir des rÃ©sultats
corrects sur les donnÃ©es testÃ©es (benchmarks acadÃ©miques).
3 Liste des problÃ¨mes rÃ©siduels importants
3.1 SÃ©lection de variables
VariabilitÃ© du sous-ensemble de variable sÃ©lectionnÃ©es
Beaucoup de mÃ©thodes de sÃ©lection de variables sont sensibles Ã  des petites perturbations
des conditions expÃ©rimentales. Si les donnÃ©es ont des variables redondantes, diffÃ©rents sous-
ensembles de variables avec le mÃªme pouvoir prÃ©dictif peuvent Ãªtre obtenus en fonction des
conditions initiales de lâ€™algorithme dâ€™apprentissage : la suppression ou lâ€™ajout de quelques va-
riables ou dâ€™exemples dâ€™apprentissage, ou lâ€™addition de bruit. Cette variabilitÃ© est indÃ©sirable
parce que (i) la variance est souvent le symptÃ´me "dâ€™un mauvais" modÃ¨le qui ne gÃ©nÃ©ralise pas
bien ; (ii) les rÃ©sultats ne sont pas reproductibles ; et (iii) un sous-ensemble de variables ne sera
pas reprÃ©sentatif du problÃ¨me. Une mÃ©thode possible pour stabiliser la sÃ©lection de variables
consiste Ã  utiliser des techniques de â€œbootstrapsâ€. Le processus de sÃ©lection de variables est
rÃ©pÃ©tÃ© avec les sous-Ã©chantillons des donnÃ©es dâ€™apprentissage. Lâ€™union des sous-ensembles de
variables choisies dans les divers â€œbootstrapsâ€ est prise comme le sous-ensemble "stable" final.
Ce sous-ensemble commun peut Ãªtre au moins aussi pertinent que le meilleur sous-ensemble
des â€œbootstrapsâ€. Lâ€™analyse du comportement des variables Ã  travers les divers â€œbootstrapsâ€
pourra aussi fournir une nouvelle comprÃ©hension du problÃ¨me.
ProcÃ©dure de recherche Forward vs. Backward
Le choix de la procÃ©dure de recherche est un sujet encore ouvert. Il est souvent dit que la pro-
cÃ©dure de recherche Forward est, de point de vue calculatoire, plus efficace que la procÃ©dure
Backward pour produire les sous-ensembles de variables pertinentes. Cependant, la recherche
par la procÃ©dure Forward ne tient pas compte du contexte dâ€™autres variables non incluses en-
core. Le problÃ¨me de lâ€™importance mutuelle est absent dans cette procÃ©dure de recherche.
Y. Bennani et al.
SÃ©lection des exemples
Les problÃ¨mes duels de sÃ©lection/construction de variables sont ceux de sÃ©lection/construction
de forme (exemple/observation). La symÃ©trie des deux problÃ¨mes montre que certains algo-
rithmes de sÃ©lection de variables peuvent sâ€™appliquer aussi au choix dâ€™exemples pour les mÃ©-
thodes Ã  noyaux par exemple. La similitude et la complÃ©mentaritÃ© des deux problÃ¨mes sont
Ã©videntes. ParticuliÃ¨rement, des exemples mal Ã©tiquetÃ©s peuvent inciter le choix de fausses
variables pertinentes. Au contraire, si lâ€™Ã©tiquetage est fortement fiable, le choix de fausses va-
riables pertinentes peut Ãªtre Ã©vitÃ© en se concentrant sur les exemples du voisinage de la frontiÃ¨re
de dÃ©cision.
ProblÃ¨me inverse (causalitÃ©)
Dans certains domaines dâ€™application, particuliÃ¨rement dans la bio-informatique, la sÃ©lec-
tion dâ€™un sous-ensemble de variables pertinentes En diagnostic, par exemple, il est important
dâ€™identifier les facteurs qui ont dÃ©clenchÃ© une maladie particuliÃ¨re ou dÃ©mÃªler la chaÃ®ne dâ€™Ã©vÃ©-
nements des causes aux symptÃ´mes. En effet, le problÃ¨me de la causalitÃ© reprÃ©sente une tÃ¢che
plus stimulante que la juste sÃ©lection de variables pertinentes. Au coeur de ce problÃ¨me est la
distinction entre la corrÃ©lation et la causalitÃ©. Les donnÃ©es disponibles pour les chercheurs en
apprentissage artificiel et en statistique nous permettent seulement dâ€™observer des corrÃ©lations.
Par contre le problÃ¨me de causalitÃ© nâ€™est que rarement explorÃ©.
3.2 Extraction de traits
Les mÃ©thodes de rÃ©duction de dimension LLE et Isomap mentionnÃ©es ci-dessus ont de
fort liens avec lâ€™analyse en composantes principales non-linÃ©aire (kernel PCA) (Bengio et al.,
2004a; Burges, 2005). Isomap et LLE ne permettent pas le calcul efficace de la projection dâ€™un
nouveau point, ne faisant pas partie de lâ€™ensemble dâ€™apprentissage. On peut contourner le pro-
blÃ¨me en estimant une application de lâ€™espace dâ€™origine vers lâ€™espace rÃ©duit, par exemple Ã 
lâ€™aide dâ€™un rÃ©seau connexionniste. Cette approche est toutefois coÃ»teuse et peu prÃ©cise. Lâ€™in-
terprÃ©tation de la rÃ©duction de dimension comme un apprentissage des fonctions propres dâ€™un
opÃ©rateur donnÃ© par un noyau dÃ©pendant des donnÃ©es permet une extension naturelle aux points
hors Ã©chantillon (Bengio et al., 2004b; Paiement, 2003). Cette interprÃ©tation des algorithmes
en termes de noyaux est aussi approfondie dans Ham et al. (2004).
Notons aussi que les mÃ©thodes prÃ©sentÃ©es sont susceptibles de mal se comporter en prÃ©-
sence de bruit sur les donnÃ©es. Elles sont aussi sensibles Ã  lâ€™accroissement du nombre de di-
mension de lâ€™espace initial (elles nâ€™Ã©vitent donc pas le curse of dimensionality) car elles re-
posent en dernier ressort sur une recherche des plus proches voisins dans cet espace (Bengio
et al., 2005).
Dans le cadre de la premiÃ¨re phase du projet, nous proposons dâ€™Ã©tudier les applications
des algorithmes de rÃ©duction de dimension prÃ©sentÃ©s au traitement de donnÃ©es textuelles, et de
comparer leurs propriÃ©tÃ©s Ã  celle des mÃ©thodes connexionnistes.
RÃ©fÃ©rences
Albatineh, A. N., M. Niewiadomska-Bugaj, et D. Mihalko (2006). On similarity indices and
correction for chance agreement. Journal of Classification 23(2), 301â€“313.
RÃ©duction des dimensions des donnÃ©es
Almuallim, H. (1994). Learning boolean concepts in the presence of many irrelevant features.
Artificial Intelligence 69, 279â€“306.
Battiti, R. (1994). Using mutual information for selecting features in supervised neural net
learning. IEEE Transactions on Neural Networks 5(4), 537â€“550.
Ben-Hur, A., A. Elisseff, et I. Guyon (2002). A stability based method for discovering structure
in clustered data. In Pacific Symposium on Biocomputing 7, pp. 6â€“17.
Bengio, Y., O. Delalleau, et N. Le Roux (2005). The curse of dimensionality for local kernel
machines. Technical Report 1258.
Bengio, Y., O. Delalleau, N. Le Roux, J.-F. Paiement, P. Vincent, et M. Ouimet (2004a).
Learning eigenfunctions links spectral embedding and kernel PCA. Neural Comp. 16(10),
2197â€“2219.
Bengio, Y., J. Paiement, P. Vincent, O. Delalleau, N. Le Roux, et M. Ouimet (2004b). Out-of-
sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering. In S. Thrun,
L. Saul, et B. Scholkopf (Eds.), Advances in Neural Information Processing Systems 16.
MIT Press, Cambridge, MA.
Bennani, Y. (2001). SystÃ¨mes dâ€™apprentissage connexionnistes : sÃ©lection de variables, Volume
15(3-4) of Revue dâ€™Intelligence Artiticielle. Paris, France : Hermes Science Publications.
Bennani, Y. (2006). Apprentissage Connexionniste. Editions HermÃ¨s Science.
Bennani, Y. et F. Bossaert (1995). A neural network based variable selector. In C. H. Dagli,
M. Akay, C. L. Chen, B. R. Fernandez, et J. Ghosh (Eds.), ANNIEâ€™95, Volume 5, St. Louis,
Missouri, USA, pp. 425â€“430. ASME Press.
Boser, B. E., I. M. Guyon, et V. N. Vapnik (1992). A training algorithm for optimal margin
classifiers. In COLT â€™92 : Proceedings of the fifth annual workshop on Computational
learning theory, New York, NY, USA, pp. 144â€“152. ACM Press.
Burges, C. J. C. (2005). Geometric methods for feature extraction and dimensional reduction
- a guided tour, pp. 59â€“92. Springer.
Cakmakov, D. et Y. Bennani (2002). Feature Selection for Pattern Recognition. Informa Press,
Ed.
Charon, I., L. Denoeud, A. Guenoche, et O. Hudry (2006). Maximum transfer distance between
partitions. Journal of Classification 23(1), 103â€“121.
Cibas, T., F. Fogelman, P. Gallinari, et S. Raudys (1994). Variable selection with optimal cell
damage. In ICANNâ€™94, Volume 1, pp. 727â€“730.
Dash, M., K. Choi, P. Scheuermann, et H. Liu (2002). Feature selection for clustering - a filter
solution. In ICDM, pp. 115â€“122. IEEE Computer Society.
Davies, D. L. et D. W. Bouldin (1979). A cluster separation measure. IEEE Transactions on
Pattern Analysis and Machine Intelligence, PAMI 1(2), 224â€“227.
Dempster, A., N. Laird, et D. Rubin (1977). Maximum likelihood from incomplete data via
the em algorithm. Journal of the Royal Statistical Society B(39), 1â€“38.
Dorizzi, B., G. Pellieux, F. Jacquet, T. Czernikov, et A. Munoz (1996). Variable selection using
generalized rbf networks : Application to forecast french t-bonds. In IEEE-IMACSâ€™96, Lille.
Dy, J. G. et C. E. Brodley (2000). Feature Subset Selection and Order Identification for Unsu-
Y. Bennani et al.
pervised Learning. In Proceedings of the 17th International Conference on Machine Lear-
ning (ICMLâ€™2000), Stanford University, CA.
Dy, J. G. et C. E. Brodley (2004). Feature Selection for Unsupervised Learning. Journal of
Machine Learning Research 5, 845â€“889.
Fowlkes, E. B. et C. L. Mallows (1983). A Method for Comparing Two Hierarchical Cluste-
rings. Journal of the American Statistical Association 78(383), 553â€“569.
Fred, A. et A. Jain (2002). Evidence accumulation clustering based on the k-means algo-
rithm. In Proceedings of the International Workshops on Structural and Syntactic Pattern
Recognition (SSPR).
Fred, A. et A. Jain (2005). Combining Multiple Clustering Using Evidence Accumulation.
IEEE Transactions on Pattern Analysis and Machine Intelligence 27(6), 835â€“850.
Geng, X., D.-C. Zhan, et Z.-H. Zhou (2005). Supervised nonlinear dimensionality reduction
for visualization and classification. IEEE Transactions on Systems, Man, and Cybernetics,
Part B 35(6), 1098â€“1107.
GuÃ©rif, S. et Y. Bennani (2006). Selection of clusters number and features subset during a
two-levels clustering task. In Proceedings of the 10th IASTED International Conference
Artificial intelligence and Soft Computing 2006, pp. 28â€“33.
Guyon, I., S. Gunn, M. Nikravesh, et L. Zadeh (2006). Feature Extraction, Foundations and
Applications, Editors. Series Studies in Fuzziness and Soft Computing, Physica-Verlag.
Springer.
Halkidi, M., Y. Batistakis, et M. Vazirgiannis (2001). On clustering validation techniques.
Intelligent Information Systems Journal 17(2-3), 107â€“145.
Halkidi, M., Y. Batistakis, et M. Vazirgiannis (2002a). Cluster validity methods : Part i. SIG-
MOD Record.
Halkidi, M., Y. Batistakis, et M. Vazirgiannis (2002b). Cluster validity methods : Part ii.
SIGMOD Record.
Ham, J., D. D. Lee, S. Mika, et B. Scholkopf (2004). A kernel view of the dimensionality
reduction of manifolds. In C. E. Brodley (Ed.), ICML. ACM.
Hassibi, B. et D. Stork (1993). Second order derivatives for networks pruning : Optimal brain
surgeon. In Advances in Neural Information Processing Systems 5, pp. 164â€“171. Morgan
Kaufmann Publishers.
He, X., D. Cai, et P. Niyogi (2006). Laplacian score for feature selection. In Y. Weiss, B. SchÃ¶l-
kopf, et J. Platt (Eds.), Advances in Neural Information Processing Systems 18, pp. 507â€“514.
Cambridge, MA : MIT Press.
Hubert, L. et P. Arabie (1985). Comparing partitions. Journal of Classification 2(1), 193â€“218.
Jain, A. K. et R. C. Dubes (1988). Algorithms for clustering data. Upper Saddle River, NJ,
USA : Prentice-Hall, Inc.
Jouve, B., P. Kuntz, et F. Velin (2001). Extraction de structures macroscopiques dans des grands
graphes par une approche spectrale. Extraction des Connaissances et Apprentissage 1(4).
Kira, K. et L. Rendell (1992). A practical approach to feature selection in machine learning.
In Proceedings of International Conference on Machine Learning, pp. 249â€“256.
RÃ©duction des dimensions des donnÃ©es
Kleinberg, J. (2002). An impossibility theorem for clustering. In Proceedings of the 16th
conference on Neural Information Processing Systems.
Kohonen, T. (1995,1997,2001). Self-Organizing Maps (Third Extended Edition ed.), Vo-
lume 30 of Springer Series in Information Sciences. Berlin, Heidelberg, New York : Sprin-
ger.
Kraskov, A., H. StÃ¶gbauer, et P. Grassberger (2004). Estimating mutual information. Phys Rev
E Stat Nonlin Soft Matter Phys 69(6 Pt 2).
Law, M. H. C., M. A. T. Figueiredo, et A. K. Jain (2004). Simultaneous feature selection
and clustering using mixture models. IEEE Transactions on Pattern Analysis and Machine
Intelligence 26(9), 1154â€“1166.
Le Cun, Y., J. Denker, et S. Solla (1990). Optimal brain damage. In Advances in Neural
Information Processing Systems 2, pp. 598â€“605. Morgan Kaufmann Publishers.
Leray, P. et P. Gallinari (2001). De lâ€™utilisation dâ€™OBD pour la sÃ©lection de variables dans les
perceptrons multicouches, pp. 373â€“391.
Li, T. (2006). A Unified View on Clustering Binary Data. Machine Learning 62(3), 199â€“215.
Liu, H. et H. Motoda (1998). Feature Selection for Knowledge Discovery and Data Mining.
Kluwer Academic Publishers.
LourenÃ§o, F., V. Lobo, et F. BaÃ§Ã£o (2004). Binary-based similarity measures for categorical
data and their application in self-organizing maps.
M., C., G. B., G. Y., M. M., et M. C. (1995). Neural modeling for time sseries : A statistical
stepwise method for weight elimination. IEEE Trans. on Neural Networks 6(6).
MacKay, D. (1994). Bayesian methods for backpropagation networks, Chapter 6. New York,
USA : Springer-Verlag.
MeilaË˜, M. (2003). Comparing clusterings by the variation of information. In B. SchÃ¶lkopf
et M. K. Warmuth (Eds.), COLT, Volume 2777 of Lecture Notes in Computer Science, pp.
173â€“187. Springer.
MeilaË˜, M. (2005). Comparing clusterings : an axiomatic view. In L. D. Raedt et S. Wrobel
(Eds.), ICML, pp. 577â€“584. ACM.
MeilaË˜, M. (2006). Comparing clusterings - an information based distance. in print.
MeilaË˜, M. (2007). Comparing clusteringsâ€”an information based distance. Journal of Multi-
variate Analysis 98(5), 873â€“895.
Mitra, P., C. Murthy, et S. Pal (2002). Unsupervised Feature Selection Using Feature Similarity.
IEEE Transactions on Pattern Analysis and Machine Intelligence 24(4).
Moody, J. (1994). Prediction risk and architecture selection for neural networks. In V. Cher-
kassky, J. Friedmann, et H. Wechsler (Eds.), From Statistics to Neural Networks - Theory
and Pattern Recognition Application.
Morineau, A. (1984). Note sur la caractÃ©risation statistique dâ€™une classe et les valeurs-tests.
Bulletin technique 2, Centre international de statistique et dâ€™informatique appliquÃ©es, Saint-
MandÃ©, France.
Neal, R. (1994). Bayesian learning for neural networks. Ph. D. thesis, University of Toronto,
Canada.
Y. Bennani et al.
Ng, A., M. Jordan, et Y. Weiss (2002). On spectral clustering : Analysis and an algorithm. In
NIPS 14 - Advances in Neural Information Processing Systems.
Paiement, J.-F. (2003). GÃ©nÃ©ralisation dâ€™algorithmes de rÃ©duction de dimension. Masterâ€™s
thesis, UniversitÃ© de MontrÃ©al.
Pal, S. K., R. K. De, et J. Basak (2000). Unsupervised Feature Evaluation : A Neuro-Fuzzy
Approach. IEEE Transactions on Neural Networks 11(2), 366â€“376.
Pedersen, M., L. Hansen, et J. Larsen (1996). Pruning with generalization based weight sa-
liencies : Î³obd, Î³obs. In Advances in Neural Information Processing Systems 8. Morgan
Kaufmann Publishers.
Raftery, A. et N. Dean (2006). Variable selection for model-based clustering. Journal of the
American Statistical Assocation 101(473), 168â€“178.
Rand, W. M. (1971). Objective criteria for the evaluation of clustering methods. Journal of the
American Statistical Association 66(336), 846â€“850.
Refenes, A.-P. N. et A. Zapranis (1999). Neural model identification, variable selection and
model adequacy. Journal of Forecasting 18(5), 299â€“332.
Rossi, F. (1996). Attribute suppression with multi-layer perceptron. In Proceedings of IEEEI-
MACSâ€™96, Lille, France.
Roux, M. (1985). Algorithmes de classification. Paris : Masson.
Roweis, S. T. et L. K. Saul (2000). Nonlinear Dimensionality Reduction by Local Linear
Embedding. Science 290, 2323â€“2326.
Ruck, D. W., S. K. Rogers, et M. Kabrisky (1990). Feature selection using a multilayer per-
ceptron. International Journal on Neural Network Computing 2(2), 40â€“48.
SchÃ¶lkopf, B., A. J. Smola, et K.-R. MÃ¼ller (1999). Kernel principal component analysis.
Advances in kernel methods : support vector learning, 327â€“352.
Schwarz, G. (1978). Estimating the Dimension of a Model. The Annals of Statistics 6(2),
461â€“464.
Sorg-Madsen, N., C. Thomsen, et J. PeÃ±a (2003). Unsupervised feature subset selection.
In Proceedings of the Workshop on Probabilistic Graphical Models for Classification,
ECML/PKDD, pp. 71â€“82.
Tenenbaum, J., V. de Silva, et J. Langford (2000). A Global Geometric Framework for Nonli-
near Dimensionality Reduction. Science 290, 2319â€“2323.
Tresp, V., R. Neuneier, et H. G. Zimmermann (1996). Early brain damage. In M. Mozer,
M. Jordan, et T. Petsche (Eds.), Advances in Neural Information Processing Systems (NIPS
1996), pp. 669â€“675. MIT Press.
Vesanto, J. et J. Ahola (1999). Hunting for Correlations in Data Using the Self-Organizing
Map. In H. Bothe, E. Oja, E. Massad, et C. Haefke (Eds.), Proceeding of the International
ICSC Congress on Computational Intelligence Methods and Applications (CIMA â€™99), pp.
279â€“285. ICSC Academic Press.
Vesanto, J. et E. Alhoniemi (2000). Clustering of the self-organizing map. IEEE Transactions
on Neural Networks 11(3), 586â€“600.
Vlachos, M., C. Domeniconi, D. Gunopulos, G. Kollios, et N. Koudas (2002). Non-linear
RÃ©duction des dimensions des donnÃ©es
dimensionality reduction techniques for classification and visualization. In Proceeding of the
International Conference on Knowledge Discovery and Data mining (KDD), pp. 645â€“651.
ACM.
Wallace, D. L. (1983). A Method for Comparing Two Hierarchical Clusterings : Comment.
Journal of the American Statistical Association 78(383), 569â€“576.
Weiss, Y. (1999). Segmentation using eigenvectors : A unifying view. In ICCVâ€™99 : Pro-
ceedings of the International Conference on Computer Vision, Volume 2, Washington, DC,
USA, pp. 975. IEEE Computer Society.
Yacoub, M. et Y. Bennani (1997). HVS : A heuristics for variables selection in multilayer
neural network classifiers. In C. H. Dagli, M. Akay, C. L. Chen, B. R. Fernandez, et J. Ghosh
(Eds.), ANNIEâ€™97, Volume 7, St. Louis, Missouri, USA, pp. 527â€“532. ASME Press.
Summary
Since several years, the volume of available data does not stop growing; whereas at the
beginning of the eighties the amount of databases was measured in mega-bytes, it is expressed
today in tera-bytes and sometimes even in peta-bytes. The number of variables and the number
of examples can take very high values, and that can causes some problems for data exploration
and analysis process. Thus, the development of processing tools adapted to these massive
databases is a major stake for data mining. The dimensionality reduction makes it possible
and facilitates visualization and understanding of the data, reduce the storage space and the
running time, and finally identify the relevant features. In this article, we present a review about
dimensionality reduction techniques primarily based on variables selection in supervised and
unsupervised learning, and some geometrical methods for non linear dimensionality reduction.
