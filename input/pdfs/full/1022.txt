√âtude comparative de deux approches de classication
recouvrante : MOC vs. OKM
Guillaume Cleuziou et Jacques-Henri Sublemontier
Laboratoire d'Informatique Fondamentale d'Orl√©ans (LIFO)
Universit√© d'Orl√©ans
Rue L√©onard de Vinci - 45067 ORLEANS Cedex 2
prenom.nom@univ-orleans.fr
R√©sum√©. La classication recouvrante d√©signe les techniques de regroupements
de donn√©es en classes pouvant s'intersecter. Particuli√®rement adapt√©s √† des do-
maines d'application actuels (e.g. Recherche d'Information, Bioinformatique)
quelques mod√®les th√©oriques de classication recouvrante ont √©t√© propos√©s tr√®s
r√©cemment parmi lesquels le mod√®le MOC (Banerjee et al. (2005a)) utilisant les
mod√®les de m√©langes et l'approche OKM (Cleuziou (2007)) consistant √† g√©n√©ra-
liser l'algorithme des k-moyennes. La pr√©sente √©tude vise d'une part √† √©tudier les
limites th√©oriques et pratiques de ces deux mod√®les, et d'autre part √† proposer
une formulation de l'approche OKM en terme de mod√®les de m√©langes gaus-
siens, laissant ainsi entrevoir des perspectives int√©ressantes quant √† la variabilit√©
des sch√©mas de recouvrements envisageables.
1 Introduction
La classication recouvrante (en anglais overlapping clustering) constitue un domaine de
recherche √©tudi√© depuis les ann√©es 60 et relanc√© par des besoins applicatifs dans des domaines
importants tels que la Recherche d'Information ou encore la Bioinformatique.
Le but recherch√© est alors d'extraire une collection de classes recouvrantes √† partir d'une
population d'individus de telle mani√®re que : chaque individu appartienne √† une ou plusieurs
classes, les individus d'une m√™me classe soient similaires, et deux individus n'appartenant pas
au moins √† une classe commune soient dissimilaires. Diff√©rentes directions ont √©t√© prospect√©es
an d'obtenir ce type de sch√©ma de classication.
Des mod√®les hi√©rarchiques ont √©t√© propos√©s ; Jardine et Sibson (1971) ont permis, en intro-
duisant les k-ultram√©triques, d'envisager des structures hi√©rarchiques (ou pseudo-hi√©rarchiques)
moins contraignantes que les arbres, par exemple des pyramides (Diday (1984)) ou encore
des hi√©rarchies dites ¬ìfaibles¬î √©tudi√©es par Bertrand et Janowitz (2003) notamment. L'un des
avantages de ces mod√®les est de proposer une interpr√©tation visuelle des classes et de leur or-
ganisation. En revanche, ces mod√®les ne permettent pas de prendre en compte la globalit√© des
sch√©mas de recouvrements possibles ; par exemple Bertrand et Janowitz (2003) montrent que
dans une k-hi√©rarchie faible (le mod√®le hi√©rarchique le moins contraignant), ¬ìl'intersection de
(k + 1) classes arbitraires peut √™tre r√©duite √† l'intersection de k de ces classes¬î.
Les approches par partitionnement propos√©es ont consist√© dans un premier temps √† d√©ter-
miner des centres, des axes ou des repr√©sentants de classes auxquels les individus sont affect√©s
√âtude comparative de deux approches de classication recouvrante
relativement √† un seuil d'appartenance. Il s'agit des travaux de Rocchio (1966) repris par Dat-
tola (1968) et plus r√©cemment de la m√©thode des k-moyennes axiales propos√©e par Lelu (1994).
Ces approches, tout comme l'algorithme CBC (Pantel (2003)), sont motiv√©s par le besoin de
mod√®les sp√©ciques pour traiter les donn√©es textuelles mais souffrent d'un probl√®me commun
r√©current que constitue la d√©termination du seuil (similarit√© ou probabilit√© d'appartenance) qui
d√©cidera de l'affectation des individus aux classes extraites. Cleuziou et al. (2004) proposent
alors l'algorithme POBOC pour se d√©gager de la contrainte du seuil ; l'affectation des individus
est effectu√©e ind√©pendamment d'un seuil x√© a priori, uniquement par l'√©tude de la distribution
de leurs proximit√©s avec l'ensemble des classes.
Les m√©thodologies de partitionnement mentionn√©es jusqu'ici s'appuient implicitement sur
une hypoth√®se forte qui consid√®re qu'un ¬ìbon¬î sch√©ma de classication recouvrante (ou re-
couvrement) peut √™tre obtenu par l'extension1 d'un ¬ìbon¬î sch√©ma de classication stricte (ou
partition). D'autres pourront penser que de fa√ßon analogue, un ¬ìbon¬î recouvrement peut √™tre
obtenu par restriction2 d'un ¬ìbon¬î sch√©ma ou. La notion de ¬ìbon¬î sch√©ma restant subjective
√† ce stade, Cleuziou (2007) propose un crit√®re objectif g√©n√©rique de qualit√© d'un sch√©ma de
classication (recouvrant ou non) et montre que pour ce crit√®re, il n'existe pas toujours une
partition optimale qui, par extension, permettrait d'aboutir √† un recouvrement optimal.
Les remarques pr√©c√©dentes nous am√®nent √† consid√©rer une nouvelle voie pour les m√©-
thodes de classication recouvrante : celle qui consiste √† rechercher un ¬ìbon¬î sch√©ma directe-
ment dans l'ensemble des recouvrements possibles. Cette d√©marche a √©t√© adopt√©e par Banerjee
et al. (2005a) et par Cleuziou (2007) dans les algorithmes MOC et OKM respectivement. MOC
(Model-Based Overlapping Clustering) peut √™tre consid√©r√© de fa√ßon simpli√©e comme une g√©-
n√©ralisation de la m√©thode EM (Dempster et al. (1977)) pour la classication recouvrante ; cette
approche que nous d√©taillerons s'appuie en effet sur les mod√®les de m√©langes qui s'av√®rent
√™tre tr√®s performants pour les probl√©matiques de classication stricte. OKM (Overlapping-k-
Means) est une g√©n√©ralisation de l'algorithme bien connu des k-moyennes (MacQueen (1967))
qui allie simplicit√© et rapidit√© pour traiter des probl√®mes concrets de mani√®re efcace.
En notant que la variante classicatoire (CEM) de EM se ram√®ne, sous certaines conditions
restrictives (lois normales, variances sph√©riques et √©gales, proportions √©gales) √† l'algorithme
des k-moyennes (Celleux et Govaert (1992)), il nous a sembl√© indispensable d'√©tudier les ana-
logies entre les deux approches recouvrantes MOC et OKM. L'objectif de cet article est alors
de proposer une formulation de OKM en terme de mod√®les de m√©langes puis de la comparer
th√©oriquement et exp√©rimentalement √† MOC.
L'article est organis√© comme suit : les deux prochaines sections sont d√©di√©es √† la pr√©sen-
tation des deux approches MOC et OKM respectivement ainsi qu'√† la reformulation de OKM
permettant une comparaison analytique des deux mod√©lisations. La section 4 pr√©sente une
discussion sur les mod√®les permettant d'identier leurs principales diff√©rences et leurs cons√©-
quences pr√©visibles sur les sch√©mas de classication. Cette section sera illustr√©e par quelques
exp√©rimentations sur des donn√©es textuelles et biologiques et sera suivie d'une synth√®se de
l'√©tude permettant de d√©gager les principales pistes de r√©exions √† mener.
1On entend par extension, le fait d'effectuer des affectations suppl√©mentaires √† un sch√©ma initialement strict.
2On entend par restriction le fait de d√©cider de l'affectation des individus, ce qui nous ram√®ne au probl√®me du seuil.
G. Cleuziou et J.H. Sublemontier
2 Le mod≈Åle MOC
Banerjee et al. (2005a) ont propos√© r√©cemment un mod√®le g√©n√©ral pour le probl√®me de
classication recouvrante en utilisant les mod√®les de m√©langes. Leur proposition s'appuie sur
les mod√®les probabilistes relationnels ou PRM (Friedman et al. (1999)) d'une part, et sur des
hypoth√®ses de g√©n√©ration probabiliste des observations d'autre part. Nous d√©crivons l'essentiel
de la m√©thode dans cette section en laissant le soin au lecteur de se reporter aux r√©f√©rences
cit√©es pour en obtenir une pr√©sentation approfondie.
2.1 Un mod√®le inspir√© de la BioInfo
Le mod√®le MOC (Model-based Overlapping Clustering) peut √™tre vu comme une instan-
ciation du mod√®le PRM permettant de mod√©liser les relations entre des g√®nes, des processus
et des valeurs d'expressions de ces g√®nes mesur√©es sur des puces ADN. Cette instanciation re-
pose sur l'hypoth√®se que le niveau d'expression d'un g√®ne (observ√© dans une certaine condition
exp√©rimentale) d√©pend des processus auxquels le g√®ne participe et de leur niveau d'inuence
(dans cette m√™me condition exp√©rimentale).
FIG. 1 ¬ñ Instanciation du mod≈Åle PRM.
La gure 1 illustre l'instanciation du mod√®le PRM permettant de mod√©liser le fait que
l'expressionXij d'un g√®ne Xi dans une condition j d√©pend :
¬ñ des niveaux d'inuence fAhjgh des processus fAhg (dans la condition j),
¬ñ de la participationMih (ou non) du g√®ne i √† un processus Ah.
Dans cet exemple on dispose de 2 g√®nes (donn√©es), 2 conditions (dimensions), 3 processus
(classes).
Sous certaines hypoth√®ses de distribution des observations fXijg, la d√©termination des pa-
ram√®tres fAhjg et fMihg s'apparente √† un probl√®me de classication o√π les processus s'iden-
tient aux classes ; plus pr√©cis√©ment √† un probl√®me de classication recouvrante puisque un
g√®ne peut participer naturellement √† plusieurs processus diff√©rents (donc appartenir √† plusieurs
classes).
√âtude comparative de deux approches de classication recouvrante
2.2 Hypoth√®ses de distribution et mod√®les associ√©s
Un premier mod√®le classique consiste √† faire l'hypoth√®se que les observationsXij suivent
des lois normales, de variances constantes . Selon le mod√®le g√©n√©ral pr√©sent√© pr√©c√©demment,
pour un nombre x√© de processus, les moyennes des gaussiennes associ√©es sont d√©termin√©es
par la somme des activit√©s Ahj des processus auxquelsXi participe :
p(Xij jMi; A) = 1p
2
exp

  (Xij   (MA)ij)
2
22

(1)
Le probl√®me de classication sera alors r√©solu par la recherche des param√®tres M et A
maximisant la vraisemblance du mod√®le. Segal et al. (2003) montrent que sous certaines condi-
tions d'ind√©pendance (entre M et A) et d'ind√©pendance conditionnelle (de Xij √©tant donn√©s
Mi etA:j), le probl√®me revient alors √† minimiser l'expression (2) ci-dessous par un algorithme
du type EM (Dempster et al. (1977)).
1
22
jjX  MAjj2   log p(M) (2)
Banerjee et al. (2005a) g√©n√©ralisent ce mod√®le aux familles de distributions exponentielles
en s'appuyant sur le fait qu'il existe une bijection entre les distributions exponentielles et les
divergences de Bregman (Banerjee et al. (2005b)). Ainsi, quelque soit la loi exponentielle
consid√©r√©e, la distribution des observations peut s'exprimer par
p(Xij jMi; A) / expf d(Xij ; (MA)ij)g (3)
avec d la divergence de Bregman associ√©e √† la densit√© exponentielle choisie, en particulier :
¬ñ la distance euclidienne (√©lev√©e au carr√©) pour des densit√©s Gaussiennes,
¬ñ la I-divergence3 pour des densit√©s de Poisson.
Maximiser la (log-)vraisemblance d'un tel mod√®le revient alors √† minimiser l'expression
g√©n√©rale (4). X
ij
d(Xij ; (MA)ij) 
X
i;h
log p(Mih) (4)
2.3 Algorithme de r√©solution
Banerjee et al. (2005a) compl√®tent le mod√®le de classication MOC par une heuristique
g√©n√©rale de minimisation du crit√®re (4) qui consiste √† it√©rer trois √©tapes de mises √† jour : mise
√† jour des coefcients de m√©lange p(Mih) (not√©s ih dans la suite), mise √† jour de la matrice
des appartenances M et mise √† jour de la matrice A dite ¬ìmatrice d'activit√©¬î en r√©f√©rence √†
l'inspiration bioinformatique de la m√©thode.
Si la mise √† jour des ih peut √™tre directement d√©duite des valeurs d'appartenance M et
n'inuence pas les autres param√®tres, la mise √† jour deM et de A n√©cessite un traitement plus
complexe.
En particulier les auteurs proposent l'algorithme dynamicM pour r√©soudre partiellement le
probl√®me de la recherche des composantes binaires optimales d'un vecteurMi repr√©sentant les
appartenances d'un individuXi aux processus (ou classes)A1,...,Ah. Il s'agit d'une heuristique
3√âgalement appel√©e divergence de Kullback-Leibler.
G. Cleuziou et J.H. Sublemontier
permettant d'explorer pour chaqueMi un sous-ensemble des 2k 1 vecteursMi possibles pour
retenir la solution minimisant le crit√®re ou √† d√©faut conserver les anciennes appartenances.
La mise √† jour de A peut √™tre r√©alis√©e dans un cas g√©n√©ral (pour toute divergence de Breg-
man) au moyen d'un algorithme de descente de gradient de la forme :
Anew  A  MT [(X  MA)  00(MA)] (5)
avec  la fonction identiant la divergence de Bregman utilis√©e et  un coefcient d'ap-
prentissage x√©. Dans les cas particulier de divergences simples qui nous int√©resseront plus
particuli√®rement dans les exp√©rimentations √† venir, le probl√®me de minimisation peut √™tre r√©-
solu plus directement.
¬ñ pour la distance euclidienne il s'agit d'une minimisation de type moindres carr√©s r√©solue
par (6) o√πM y d√©signe la pseudo-inverse deM
A =MyX (6)
¬ñ pour la I-divergence, les auteurs s'appuient sur des techniques de factorisation de ma-
trices non-n√©gatives pour aboutir √† la r√®gle de mise √† jour suivante
Anewhj = Ahj
P
iMihXij=(MA)ijP
iMih
(7)
Dans cette derni√®re variante, on peut observer que la r√®gle de mise √† jour propos√©e cor-
respond √† une simplication de la r√®gle plus g√©n√©rale (8) r√©√©tudi√©e r√©cemment par Finesso et
Spreij (2006) :
Anewhj = Ahj
P
iMihXij=(MA)ijP
ijMihAhjXij=(MA)ij
(8)
Dans le cas particulier o√π chaque individu Xi n'appartient qu'√† une seule classe Ah alors
(MA)ij = MihAhj ; en ajoutant √† cela le fait que la I-divergence mesure l'√©cart entre deux
distributions p et q telles que
P
j pj =
P
j qj = 1 la simplication (7) devient en effet possible
(
P
j Xij = 1). Cependant la m√©thode MOC s'int√©ressant justement aux cas o√π chaque individu
peut appartenir √† plusieurs classes, cette simplication devient fausse ; c'est la raison pour
laquelle nous proposerons de conserver la r√®gle de mise √† jour originelle (8) an d'assurer la
d√©croissance du crit√®re (4).
Les trois √©tapes de mises √† jour que nous venons de pr√©senter permettent d'assurer la d√©-
croissance du crit√®re (4) et par cons√©quent d'am√©liorer √† chaque it√©ration (et apr√®s chaque √©tape
de mise √† jour) la vraisemblance du mod√®le probabiliste. L'initialisation du mod√®le (matrices
M et A) est effectu√©e au moyen d'une √©tape de partitionnement (k-moyennes). Enn, de fa√ßon
assez classique l'algorithme MOC it√®re le processus de mises √† jour un nombre maximum de
fois ou jusqu'√† observer une variation epsilonesque du crit√®re objectif.
3 Le mod≈Åle OKM
L'algorithme des k-moyennes pr√©sente un mod√®le th√©orique simple et intuitif, facilement
appr√©hend√© par les praticiens de domaines d'application vari√©s qui, de surcro√Æt, appr√©cient g√©-
n√©ralement l'efcacit√© de cette m√©thode en terme de rapidit√© et de qualit√© des classes obtenues.
√âtude comparative de deux approches de classication recouvrante
L'approche OKM, propos√©e par Cleuziou (2007), est le r√©sultat d'une volont√© de r√©pondre de
mani√®re pragmatique aux besoins applicatifs actuels, en proposant d'√©tendre l'algorithme des
k-moyennes √† la recherche de recouvrements plut√¥t que de partitions.
3.1 Crit√®re objectif et heuristique d'optimisation
Le crit√®re des moindres carr√©s sur lequel repose l'algorithme des k-moyennes est une for-
malisation d√®le de l'objectif vis√© par les m√©thodes de partitionnement √† savoir faire en sorte
que deux individus d'une m√™me classe soient similaires et deux individus de classes diff√©rentes
soient dissimilaires. Comme nous l'avons mentionn√© en introduction, on peut assez naturelle-
ment consid√©rer qu'un bon recouvrement sera caract√©ris√© par :
¬ñ des individus similaires lorsqu'ils appartiennent plut√¥t aux m√™mes classes,
¬ñ des individus dissimilaires lorsqu'ils appartiennent plut√¥t √† des classes diff√©rentes.
Le crit√®re utilis√© dans OKM pour formaliser les caract√©ristiques pr√©c√©dentes introduit la
notion d'image (que l'on notera  (Xi)) d'un individu Xi dans une classication I1; : : : ; Ik.
L'image de Xi correspond √† un point, dans l'espace de repr√©sentation initial, et repr√©sentatif
des classes auxquelles Xi appartient. Par exemple, en reprenant les notations utilis√©es pr√©c√©-
demment, on pourra d√©nir l'image de Xi dans un espace euclidien (Rm; d) par
 j(Xi) =
P
hMihAhjP
hMih
(9)
Dans (9),  j(Xi) d√©signe la ji√®me composante de l'image,Mih 2 f0; 1g l'appartenance
de Xi √† la classe Ih et Ah correspond ici √† la position du centre de la classe Ih. Le crit√®re
objectif que l'on cherchera √† minimiser, √©value la qualit√© d'un recouvrement par la variance
entre les individus et leur image dans la classication :X
i
d2(Xi;  (Xi)) (10)
Pour  (:) bien choisie, on peut noter que ce crit√®re se ram√®ne exactement au crit√®re des
moindres carr√©s lorsque l'on oblige chaque individu √† n'appartenir qu'√† une seule classe
(
P
hMi;h = 1).
L'heuristique de minimisation du crit√®re objectif (10) dans OKM s'apparente √† l'algorithme
des k-moyennes. Apr√®s une √©tape d'initialisation al√©atoire des centres de classesA, deux √©tapes
de mises √† jour (deM puis de A) sont it√©r√©es jusqu'√† la v√©rication d'un crit√®re d'arr√™t portant
sur le nombre d'it√©rations ou la variation du crit√®re objectif.
Mise √† jour deM . Pour chaque individu Xi, la mise √† jour deMi est r√©alis√©e en consid√©rant
qu'un individu ne doit appartenir qu'aux classes dont il est le plus proche au sens de la m√©trique
choisie. Ce principe guide la construction du nouveau vecteur d'appartenance : initialisation
avec affectation au plus proche centre de classe puis ajout de nouvelles affectations dans l'ordre
de proximit√© des centres fAhgh avec Xi tant que le crit√®re objectif s'en trouve am√©lior√© ; le
nouveau vecteur ainsi obtenu ne rempla√ßant le vecteurMi initial que s'il permet d'am√©liorer
le crit√®re objectif.
Mise √† jour de A. Cleuziou (2007) montre que pour la distance euclidienne on peut d√©nir
localement le nouveau centre Ah de la classe Ih de fa√ßon optimale au sens du crit√®re objectif.
G. Cleuziou et J.H. Sublemontier
Une heuristique d'optimisation globale de A consistera donc √† it√©rer les optimisations locales,
en parcourant l'ensemble des classes plusieurs fois4, jusqu'√† aboutir √† un point xe.
Pour compl√©ter cette description, nous pr√©sentons dans le tableau 1 les diff√©rentes variantes
de OKM en fonction des m√©triques consid√©r√©es.
Espace de
M√©trique repr√©sentation Image  Mise √† jour de Ah
distance euclidienne Rm Moyenne des centre de gravit√©centres (cf. (9)) pond√©r√© (Cleuziou (2007))
I-divergence [0; 1]
m Moyenne R√®gle multiplicative de
avec
P
j Xij = 1 des centres (cf. (9)) Finesso&Spreij (cf. (8))
cosinus [0; 1]
m Moyenne quadratique
avec
P
j X
2
ij = 1 des centres Non d√©termin√©
TAB. 1 ¬ñ Variantes de OKM par m√òtrique.
3.2 Reformulation de OKM
En reprenant l'inspiration bioinformatique du mod√®le MOC, on consid√®re qu'une obser-
vation Xij est le r√©sultat d'une certaine combinaison des processus auxquels l'individu Xi
participe. Plut√¥t que de choisir comme combinaison l'addition des processus, nous en choisis-
sons la moyenne. Ainsi sous l'hypoth√®se d'une distribution gaussienne (de variance constante
) des valeursXij nous obtenons
p(Xij jMi; A) = 1p
2
exp

  (Xij   i:(MA)ij)
2
22

(11)
avec  un vecteur totalement d√©ni parM tel que i = 1=
P
hMih.
Dans un processus de classication, l'objectif consiste par exemple √† rechercher les para-
m√®tresM (matrice binaire) et A (matrice r√©elle) maximisant la (log-)vraisemblance des para-
m√®tres √©tant donn√©es les observations : logL(M;AjX) = log p(XjM;A). Le mod√®le MOC
fait l'hypoth√®se qu'il y a ind√©pendance des observations Xij conditionnellement aux Mi et
A:j . Sous ces m√™mes hypoth√®ses, la vraisemblance du mod√®le peut se d√©composer ainsi
L(M;AjX) = p(XjM;A) =
Y
i;j
p(Xi;j jMi; A:j)
En consid√©rant √† pr√©sent la log-vraisemblance et en introduisant l'hypoth√®se de distribution
gaussienne (11), on note que
maxM;A logL(M;AjX)  maxM;A
h
log
Q
i;j p(Xi;j jMi; A:j)
i
 max
M;A
24  1
22
X
i;j
(Xij   i(MA)ij)2
35  min
M;A

1
22
jjX   T I:MAjj2

(12)
4En pratique un seul parcours de l'ensemble des classes suft pour approcher le point xe.
√âtude comparative de deux approches de classication recouvrante
En observant que i(MA)ij =  (Xi) avec  (Xi) l'image de Xi telle que d√©nie en (9),
on montre que minimiser le terme jjX   T I:MAjj2 =PiPj(Xij   i(MA)ij)2 √©quivaut
√† minimiser
P
i d
2(Xi;  (Xi)) (avec d la distance euclidienne) utilis√© dans OKM (cf. (10)).
Nous avons donc d√©montr√© que l'approche OKM, dans sa version initiale utilisant la dis-
tance euclidienne, peut √™tre r√©√©crite comme un mod√®le de m√©langes recouvrant faisant l'hy-
poth√®se que chacune des observations suit une lois normale dont la moyenne correspond √† la
moyenne des processus (ou classes) auxquelles l'individu participe.
4 Discussion et analyse comparative des mod≈Åles
Nous m√®nerons la discussion en deux temps : nous rel√®verons dans un premier temps les
diff√©rences majeures et les limites th√©oriques des deux mod√®les MOC et OKM ; dans un second
temps nous proposerons une √©tude comparative exp√©rimentale des deux approches.
4.1 Discussion sur les mod√®les
Nous avons choisi de comparer analytiquement les deux mod√®les en les exprimant tous
les deux en terme de mod√®les de m√©langes recouvrants, plut√¥t que simplement comme des
techniques de r√©allocation dynamique minimisant un crit√®re d'inertie. On peut ainsi envisager
plus facilement d'extraire ult√©rieurement des classes de formes, volumes et orientations va-
ri√©es. Cependant en l'√©tat actuel des mod√®les, ces variations ne sont pas permises (hypoth√®se
des variances toutes identiques) et les deux formalismes sont strictement √©quivalents.
Ce qui diff√©rencie les mod√®les MOC et OKM concerne la m√©thode de combinaison des
¬ìprocessus¬î d√©terminant les param√®tres de la distribution d'une observation : pour une distri-
bution exponentielle, c'est la moyenne de la distribution qui r√©sulte de cette combinaison.
¬ñ Le mod√®le MOC propose une combinaison par addition, justi√©e par le mod√®le bio-
informatique sous-jacent qui suppose que l'expression observ√©e d'un g√®ne est le r√©sultat
de l¬î'addition¬î des processus dans lesquels ce g√®ne intervient.
¬ñ Le mod√®le OKM utilise la notion d'image qui correspond effectivement √† une combi-
naison des repr√©sentants des classes auxquelles l'individu appartient. La d√©nition de
l'image d√©pend de la m√©trique consid√©r√©e et s'exprime comme une moyenne plut√¥t
qu'une somme : moyenne arithm√©tique ou quadratique par exemple (cf. tableau 1)).
Ce choix de combinaison n'est pas anodin et peut avoir des cons√©quences notables sur la
validit√ò th√©orique du mod√®le d'une part, sur sa sensibilit√ò aux donn√©es d'autre part.
La validit√ò th√©orique du mod√®le peut √™tre remise en cause lorsque la combinaison utilis√©e
n'est pas un endomorphisme car le mod√®le implique de consid√©rer la distance entre chaque
individu et la combinaison associ√©e. Par exemple, la I-divergence permet de comparer deux
distributions p et q dans [0; 1]m telles que
P
j pj =
P
j qj = 1 ; si on peut montrer que l'image
 (Xi) utilis√©e dans OKM reste effectivement dans l'espace des distributions ( (Xi) 2 [0; 1]m
et
P
j  j(Xi) = 1), la combinaison (MA)i utilis√©e dans MOC ne v√©rie pas les caract√©ris-
tiques d'une distribution et l'expression d(Xi; (MA)i) utilis√©e dans (3) n'a pas de sens.
La combinaison peut √©galement jouer un r√¥le important dans la sensibilit√ò du mod√®le, no-
tamment dans la r√©partition des donn√©es pour certaines hypoth√®ses de lois de m√©lange. Prenons
un exemple jouet compos√© de quatre individus d√©crits dans R fX1 = (1:0); X2 = (4:0); X3 =
(5:0); X6 = (6:0)g avec les hypoth√®ses suivantes : distributions gaussiennes des observations
r√©sultant de deux processus/classes (k = 2). La gure 2 illustre la conguration des individus
G. Cleuziou et J.H. Sublemontier
√† classer et, pour chacune des deux approches MOC et OKM, la projection des param√®tres A
dans le m√™me espace de description apr√®s optimisation.
FIG. 2 ¬ñ Observation du param≈Åtre A selon l‚Äôapproche.
La classication nale retourn√©e par OKM s'obtient simplement en affectant chaque in-
dividu Xi √† I1, I2 ou aux deux classes selon que l'individu est plus proche de A1, A2 ou
(A1 +A2)=2 respectivement ; sur l'exemple OKM retournera donc les classes I1 = fX1;X2g
et I2 = fX2; X3; X4g. Dans l'approche MOC on obtient cette fois la classication nale
en comparant les distances de Xi avec A1, A2 et A1 +A2, aboutissant ainsi aux classes
I1 = fX1;X4g et I2 = fX2; X3;X4g dont l'intersection est totalement injusti√©e. Il suf-
rait sur cet exemple de recentrer les individus en z√©ro tout en conservant les distances entre
individus inchang√©es pour obtenir avec MOC les m√™mes classes que pour OKM.
Cet exemple nous a donc permis de mettre en √©vidence que MOC peut √™tre sensible aux
translations de donn√©es, en particulier sous des hypoth√®ses de distributions gaussiennes5. Cette
limitation peut √™tre observ√©e sur un exemple r√©el de donn√©es d'expressions de g√®nes en bio-
informatique6 (gures 3 et 4). M√™me si, sur cet exemple, aucune organisation en classes n'est
FIG. 3 ¬ñ MOC sur les donn√òes initiales. FIG. 4 ¬ñ MOC sur donn√òes translat√òes.
observable, on remarque que l'intersection des deux classes (points violets) est coh√©rente sur
les donn√©es initiales (gure 3) et de nouveau injusti√©es sur les donn√©es translat√©es (gure 4).
Il est donc important de pr√©ciser qu'en pratique, MOC produit des r√©sultats int√©ressants sur des
5On peut montrer que cette sensibilit√© est annihil√©e pour d'autres types de distributions telles que la I-divergence
gr√¢ce aux contraintes v√©ri√©es par les individus (e.g.
P
j Xij = 1).
6Cette exp√©rience a √©t√© r√©alis√©e dans le cadre du projet ANR/ARA Masse de donn√©es "Genomic data to Graph
Structure" : http ://gd2gs.ibisc.univ-evry.fr/
√âtude comparative de deux approches de classication recouvrante
donn√©es d'expressions de g√®nes, pr√©cis√©ment parceque ces donn√©es sont par nature centr√©es
globalement autour de z√©ro7.
4.2 Comparaisons exp√©rimentales
Pour achever la comparaison des deux approches de classication recouvrante √©tudi√©es,
nous pr√©sentons les r√©sultats obtenus par MOC et OKM sur une exp√©rience de classication de
documents textuels qui correspond √† un domaine d'applications cible. L'√©tude est men√©e sur
un sous-ensemble des documents du corpus Reuters, utilis√© et pr√©sent√© de fa√ßon d√©taill√©e par
Cleuziou (2007). Ce corpus pr√©sente l'int√©r√™t que chaque document poss√®de une ou plusieurs
√©tiquettes de classes. Nous n'utilisons pas cette information dans le processus de classication
mais seulement pour √©valuer la qualit√© des sch√©mas de classication recouvrants g√©n√©r√©s par
les m√©thodes. L'√©valuation op√©r√©e consiste √† mesurer l'√©cart entre les associations de docu-
ments connues (pr√©-√©tiquetage) et les associations effectivement retrouv√©es dans la classica-
tion, en utilisant les indices classiques de pr√©cision, rappel et F-mesure8.
Nb.
classes
F-mesure Pr√©cision Rappel
k-moy. OKM MOC k-moy. OKM MOC k-moy. OKM MOC
k=2 0.39 0.42 0.41 0.27 0.27 0.26 0.77 0.95 0.93
k=5 0.34 0.41 0.40 0.27 0.27 0.27 0.46 0.89 0.77
k =10 0.32 0.41 0.39 0.30 0.28 0.28 0.35 0.83 0.65
k =15 0.30 0.40 0.38 0.33 0.28 0.29 0.27 0.76 0.54
k =20 0.27 0.40 0.37 0.34 0.29 0.31 0.23 0.66 0.46
TAB. 2 ¬ñ Classication des donn√òes Reuters en utilisant la I-divergence.
Nb.
classes
F-mesure Pr√©cision Rappel
k-moy. OKM MOC k-moy. OKM MOC k-moy. OKM MOC
k=2 0.38 0.39 0.38 0.24 0.25 0.24 0.84 0.97 0.91
k=5 0.35 0.40 0.37 0.23 0.26 0.24 0.67 0.90 0.76
k =10 0.29 0.40 0.34 0.23 0.26 0.25 0.42 0.84 0.54
k =15 0.29 0.39 0.35 0.25 0.26 0.26 0.37 0.80 0.55
k =20 0.26 0.38 0.36 0.27 0.26 0.29 0.26 0.69 0.50
TAB. 3 ¬ñ Classication des donn√òes Reuters sous l‚Äôhypoth≈Åse de distributions gaussiennes.
Nous pr√©sentons les r√©sultats de l'√©valuation des classications en utilisant la I-divergence
d'une part (tableau 2) et des distributions gaussiennes ou m√©trique euclidienne d'autre part
(tableau 3). Les valeurs report√©es dans les tableaux correspondent √† des moyennes de 10 ex√©-
cutions de chaque m√©thode dans des conditions initiales identiques.
Tout d'abord, nous retrouvons un ph√©nom√®ne connu en Recherche d'Information qui est
que la I-divergence donne de meilleurs r√©sultats que la m√©trique euclidienne pour la classica-
tion de documents. Ceci est notamment d√ª au fait que la I-divergence compare des distributions
7Certains g√®nes s'exprimant positivement, d'autres n√©gativement.
8Cette technique d'√©valuation des m√©thodes est utilis√©e par Cleuziou (2007) et par Banerjee et al. (2005a).
G. Cleuziou et J.H. Sublemontier
de mots plut√¥t que des vecteurs de fr√©quences, r√©duisant ainsi les effets li√©s aux variations de
tailles entre documents. Le second r√©sultat attendu est de constater que la pr√©cision augmente et
que le rappel diminue quand on augmente le nombre de classes ; ceci s'explique par le fait que
lorsque le nombre de classes augmente, le nombre de paires de documents associ√©s diminue
automatiquement.
Enn, en comparant les r√©sultats obtenus par les approches MOC et OKM on observe que
pour les deux m√©triques utilis√©es, OKM g√©n√®re d'avantage de recouvrements que MOC9, ce
qui se traduit par un taux de rappel plus √©lev√© sans pour autant entra√Æner un √©chissement de la
pr√©cision (qui reste d'ailleurs plut√¥t √† l'avantage de OKM). Il semblerait donc, au regard de la
globalit√© des r√©sultats de cette exp√©rience que : les deux approches de classication recouvrante
√©tudi√©es permettent de g√©n√©rer des recouvrements pertinents (comparaison avec l'algorithme
des k-moyennes) et que le gain obtenu soit plus net dans le cas de l'approche OKM notamment
en utilisant la I-divergence. Cette derni√®re remarque corrobore les limites th√©oriques √©nonc√©es
pr√©c√©demment concernant le mod√®le MOC, et en particulier sous les hypoth√®ses de distribu-
tions gaussiennes.
5 Conclusion et prespectives
Dans cet article nous avons √©tudi√© deux approches de classication recouvrante : l'ap-
proche MOC (Banerjee et al. (2005a)) et l'approche OKM (Cleuziou (2007)). En proposant une
formalisation de OKM en terme de m√©lange de lois, nous avons pu constater les fortes analo-
gies qui existent entre les deux approches. Nous avons d√©taill√© leurs diff√©rences fondamentales
et relev√© quelques limites th√©oriques concernant le mod√®le MOC. Ces limites sont susceptibles
de produire des r√©sultats incoh√©rents que nous avons observ√©s exp√©rimentalement.
Nous proposerons par la suite de conrmer les premi√®res observations exp√©rimentales sur
d'autres corpus et d'autres domaines d'application. Nous poursuivrons l'extension des mod√®les
traditionnels de classication dans le but d'extraire des recouvrements avec des classes de
formes, volumes et orientations vari√©es. Enn nous travaillerons sur une variante sph√©rique
du mod√®le OKM utilisant la mesure du cosinus, particuli√®rement adapt√©e au traitement des
documents textuels.
R√òf√òrences
Banerjee, A., C. Krumpelman, J. Ghosh, S. Basu, et R. J. Mooney (2005a). Model-based over-
lapping clustering. In KDD ‚Äô05 : Proceeding of the eleventh ACM SIGKDD international
conference on Knowledge discovery in data mining, New York, NY, USA, pp. 532¬ñ537.
ACM Press.
Banerjee, A., S. Merugu, I. Dhillon, et J. Ghosh (2005b). Clustering with bregman divergences.
J. Mach. Learn. Res. 6, 1705¬ñ1749.
Bertrand, P. et M. F. Janowitz (2003). The k-weak hierarchical representations : An extension
of the indexed closed weak hierarchies. Discrete Applied Mathematics 127(2), 199¬ñ220.
Celleux, G. et G. Govaert (1992). A classication EM algorithm for clustering and two sto-
chastic versions. Computational Statistics and Data Analysis 14(3), 315¬ñ332.
9Sans d√©tailler d'avantage ce point, il est vraisemblable que ce ph√©nom√®ne soit li√© √† l'initialisation via une ex√©cu-
tion de l'algorithme k-moyenne dans l'approche MOC
√âtude comparative de deux approches de classication recouvrante
Cleuziou, G. (2007). Okm : une extension des k-moyennes pour la recherche de classes
recouvrantes. In Journ√òes Francophones d‚Äô Extraction et de Gestion des Connaissances
EGC‚Äô2007, Volume 2, Namur, Belgique. Revue des Nouvelles Technologies de l'Informa-
tion, C√©padu√®s-Edition.
Cleuziou, G., L. Martin, et C. Vrain (2004). PoBOC : an Overlapping Clustering Algorithm.
Application to Rule-Based Classication and Textual Data. In R. L√≥pez de M√°ntaras and L.
Saitta, IOS Press (Ed.), Proceedings of the 16th European Conf. on Articial Intelligence,
Valencia, Spain, pp. 440¬ñ444.
Dattola, R. (1968). A fast algorithm for automatic classication. Technical report, Report
ISR-14 to the National Science Foundation, Section V, Cornell University, Department of
Computer Science.
Dempster, A., N. Laird, et D. Rubin (1977). Maximum Likelihood from Incomplete Data via
the EM Algorithm. Journal of Royal Statistical Society B 39, 1¬ñ38.
Diday, E. (1984). Une repr√©sentation visuelle des classes empi√©tantes : Les pyramides. Tech-
nical report, INRIA num.291, Rocquencourt 78150, France.
Finesso, L. et P. Spreij (2006). Nonnegative Matrix Factorization and I-Divergence Alternating
Minimization. Linear Algebra and its Applications 416, 270¬ñ287.
Friedman, N., L. Getoor, D. Koller, et A. Pfeffer (1999). Learning probabilistic relational
models. In IJCAI, pp. 1300¬ñ1309.
Jardine, N. et R. Sibson (1971). Mathematical Taxonomy. London : John Wiley and Sons Ltd.
Lelu, A. (1994). Clusters and factors : neural algorithms for a novel representation of huge and
highly multidimensional data sets. In E. D. Y. L. . al. (Ed.),New Approaches in Classication
and Data Analysis, Berlin, pp. 241¬ñ248. Springer-Verlag.
MacQueen, J. (1967). Some methods for classication and analysis of multivariate obser-
vations. In Proceedings of the Fifth Berkeley Symposium on Mathematical statistics and
probability, Volume 1, Berkeley, pp. 281¬ñ297. University of California Press.
Pantel, P. (2003). Clustering by Committee. Ph.d. dissertation, Department of Computing
Science, University of Alberta.
Rocchio, J. (1966). Document retrieval systems - optimization and evaluation. Ph.d. thesis,
harvard university, Report ISR-10 to National Science Foundation, Harvard Computation
Laboratory.
Segal, E., A. Battle, et D. Koller (2003). Decomposing gene expression into cellular processes.
Pac Symp Biocomput, 89¬ñ100.
Summary
This paper deals with overlapping clustering methodologies which consist in organizing
data into classes with intersections. This kind of clustering scheem is suitable for impor-
tant elds of application such as Information Retrieval or Bioinformatics. Two approaches
have been recently proposed by Banerjee et al. (2005a) (MOC) and Cleuziou (2007) (OKM).
These two approaches are compared on theoretical and experimental points of view in order to
prospect for new general overlapping clustering models.
