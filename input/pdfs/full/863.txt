OKMED etWOKM : deux variantes de OKM pour la
classification recouvrante
Guillaume Cleuziou
Laboratoire dâ€™Informatique Fondamentale dâ€™OrlÃ©ans (LIFO)
UniversitÃ© dâ€™OrlÃ©ans
Rue LÃ©onard de Vinci - 45067 OrlÃ©ans Cedex 2
Guillaume.Cleuziou@univ-orleans.fr
RÃ©sumÃ©. Cet article traite de la problÃ©matique de la classification recouvrante
(overlapping clustering) et propose deux variantes de lâ€™approche OKM : OKMED
et WOKM. OKMED gÃ©nÃ©ralise k-mÃ©doÃ¯des au cas recouvrant, il permet dâ€™orga-
niser un ensemble dâ€™individus en classes non-disjointes, Ã  partir dâ€™une matrice
de distances. La mÃ©thode WOKM (Weighted-OKM) Ã©tend OKM par une pondÃ©-
ration locale des classes ; cette variante autorise chaque individu Ã  appartenir Ã 
plusieurs classes sur la base de critÃ¨res diffÃ©rents. Des expÃ©rimentations sont rÃ©a-
lisÃ©es sur une application cible : la classification de textes. Nous montrons alors
que OKMED prÃ©sente un comportement similaire Ã  OKM pour la mÃ©trique eucli-
dienne, et offre la possibilitÃ© dâ€™utiliser des mÃ©triques plus adaptÃ©es et dâ€™obtenir
de meilleures performances. Enfin, les rÃ©sultats obtenus avec WOKM montrent
un apport significatif de la pondÃ©ration locale des classes.
1 Introduction
La classification recouvrante (ou overlapping clustering) constitue une problÃ©matique par-
ticuliÃ¨re dans le domaine de la classification non-supervisÃ©e (ou clustering). Il sâ€™agit dâ€™orga-
niser un ensemble dâ€™individus en classes dâ€™individus similaires en autorisant chaque donnÃ©e
Ã  appartenir Ã  plusieurs classes. Ce type de schÃ©ma correspond Ã  une organisation naturelle
des donnÃ©es pour de nombreuses applications. Par exemple, en Recherche dâ€™Information un
mÃªme document peut porter sur une ou plusieurs thÃ©matiques, en Bioinformatique un mÃªme
gÃ¨ne peut intervenir dans un ou plusieurs processus mÃ©taboliques, en Traitement du Langage
un mÃªme verbe peut satisfaire une ou plusieurs grammaires de sous-catÃ©gorisation, etc.
On parle de â€œproblÃ©matiqueâ€ au mÃªme titre que la problÃ©matique gÃ©nÃ©rale de la classifica-
tion, puisquâ€™il nâ€™existe pas dâ€™avantage de solution triviale pour extraire des classes dâ€™individus
similaires qui soient indiscutables et universelles. De surcroÃ®t, la classification recouvrante
offre un espace de solutions plus vaste que dans le cas traditionnel, quâ€™il est donc encore plus
difficile dâ€™explorer.
Durant les quatre derniÃ¨res dÃ©cennies, quelques solutions ont Ã©tÃ© proposÃ©es spÃ©cifiquement
pour la classification recouvrante. Dattola (1968) envisagait une approche de type centres mo-
biles avec affectation multiple des individus dÃ©terminÃ©e par un seuil. Jardine et Sibson (1971),
en introduisant les k-ultramÃ©triques, ont ouvert la voie des recherches fondamentales sur les
OKMED et WOKM : deux variantes de OKM
hiÃ©rarchies recouvrantes : pyramides (Diday (1987)) ou hiÃ©rarchies faibles (Bertrand et Jano-
witz (2003)). Plus rÃ©cemment, sous la pression des applications en Recherche dâ€™Information ou
en Bioinformatique, de nouvelles investigations ont Ã©tÃ© menÃ©es afin dâ€™Ã©tendre les modÃ¨les de
partitionnement (k-moyennes ou CEM) aux cas recouvrants. Ainsi Banerjee et al. (2005) ont
proposÃ© le modÃ¨le MOC qui gÃ©nÃ©ralise CEM et Cleuziou (2008) le modÃ¨le OKM qui gÃ©nÃ©ralise
k-moyennes. Ces deux derniers modÃ¨les sont naturellement trÃ¨s proches et diffÃ¨rent principale-
ment dans la dÃ©finition des intersections entre classes et dans la mise en Å“uvre algorithmique
proposÃ©e (initialisation et mÃ©thode dâ€™affectation en particulier). Une Ã©tude comparative plus
complÃ¨te a Ã©tÃ© proposÃ©e par Cleuziou et Sublemontier (2008).
Le modÃ¨le commun Ã  OKM et MOC permet dâ€™envisager un large Ã©ventail de pistes Ã  explo-
rer tellement les dÃ©clinaisons de k-moyennes sont nombreuses. Par exemple, des variantes de
k-moyennes ont Ã©tÃ© proposÃ©es pour rechercher le nombre k de classes appropriÃ© (D.Pelleg et
Moore (2000)), pour limiter le risque dâ€™une solution localement optimale (Likas et al. (2003))
ou encore pour initialiser lâ€™algorithme de faÃ§on intelligente (PeÃ±a et al. (1999)). Dans la prÃ©-
sente Ã©tude nous avons choisi dâ€™Ã©tudier deux extensions particuliÃ¨res du modÃ¨le OKM pour
rÃ©pondre aux prioritÃ©s dans ce domaine, Ã  savoir la nÃ©cessitÃ© de diversifier les mÃ©triques ac-
ceptables par le modÃ¨le dâ€™une part et la possibilitÃ© dâ€™affecter un mÃªme individus Ã  plusieurs
classes sur la base de caractÃ©ristiques diffÃ©rentes dâ€™autre part.
Nous proposons alors une premiÃ¨re variante (OKMED) qui se fonde sur les mÃ©thodes de
partitionnement autour de mÃ©doÃ¯des et qui permet dâ€™organiser un ensemble dâ€™individus dÃ©crits
par une matrice de distance quelconque en classes recouvrantes dâ€™individus similaires. OK-
MED nÃ©cessite de dÃ©finir judicieusement la notion de reprÃ©sentant dâ€™une intersection et pose
quelques problÃ¨mes de complexitÃ© thÃ©oriques qui peuvent aisÃ©ment Ãªtre contournÃ©s en pra-
tique. La deuxiÃ¨me contribution de lâ€™Ã©tude est la variante pondÃ©rÃ©e (WOKM) qui vient gÃ©nÃ©ra-
liser le modÃ¨le OKM en introduisant une pondÃ©ration des attributs, locale Ã  chaque classe. Cette
approche sâ€™inspire de la version pondÃ©rÃ©e de k-moyennes proposÃ©e par Chan et al. (2004) et
plus fondamentalement des distances adaptatives de Diday et Govaert (1977). WOKM semble
particuliÃ¨rement adaptÃ© Ã  la classification recouvrante : chaque classe Ã©tant â€œcaractÃ©risÃ©eâ€ par
une pondÃ©ration diffÃ©rente des attributs, la description dâ€™un individu est considÃ©rÃ©e diffÃ©rem-
ment dâ€™une classe Ã  une autre et un mÃªme individu peut donc naturellement appartenir Ã  plu-
sieurs classes sur la base dâ€™attributs Ã©ventuellement diffÃ©rents. Nous montrerons que la trans-
position du modÃ¨le initial pondÃ©rÃ© au cas recouvrant nâ€™est pas trivial, nous proposerons des
solutions algorithmiques permettant dâ€™assurer la convergences des algorithmes et montrerons
lâ€™efficacitÃ© de ces choix sur des donnÃ©es rÃ©elles.
Lâ€™article sâ€™organise en quatre principales sections : la section 2 rappelle le cadre formel des
modÃ¨les OKM et MOC afin de mieux apprÃ©hender les deux sections suivantes qui prÃ©sentent
respectivement les variantes OKMED et WOKM. Avant de conclure, la section 5 prÃ©sente les
expÃ©rimentations rÃ©alisÃ©es sur des donnÃ©es rÃ©elles de classification de textes.
2 Cadre formel des modÃ¨les MOC et OKM
Le modÃ¨le MOC proposÃ© par Banerjee et al. (2005) et le modÃ¨le OKM proposÃ© par Cleu-
ziou (2008) reposent tous les deux sur une extension des mÃ©thodes dâ€™agrÃ©gation autour de
centres mobiles au domaine de la classification recouvrante. MOC est initialement formalisÃ©
en terme de modÃ¨les de mÃ©langes (recouvrants). Cependant, lâ€™optimisation du critÃ¨re objectif
(log-vraisemblance) nÃ©cessite dans le cas recouvrant de restreindre le modÃ¨le gÃ©nÃ©ratif (va-
G. Cleuziou
riances constantes et toutes Ã©gales) ainsi que lâ€™algorithme de rÃ©solution (CEM plutÃ´t que EM).
De facto, MOC peut Ãªtre vu comme une mÃ©thode plus classique dâ€™optimisation dâ€™un critÃ¨re
dâ€™inertie de type moindres carrÃ©s.
Soit X = {xi}
n
i=1 un ensemble dâ€™individus dans R
p, la fonction objective des modÃ¨les
MOC et OKM peut sâ€™exprimer de maniÃ¨re unifiÃ©e par :
J ({pic}
k
c=1) =
âˆ‘
xiâˆˆX
â€–xi âˆ’ Ï†(xi)â€–
2 (1)
Dans ce critÃ¨re, les {pic}
k
c=1 dÃ©signent les k classes recouvrantes et Ï†(xi) le reprÃ©sentant
de xi dans le schÃ©ma de classification, appelÃ© â€œimageâ€ de xi par Cleuziou (2007) . Cette image
est dÃ©terminÃ©e par une combinaison des centres {mc}
k
c=1 des classes auxquelles xi appartient :
une somme dans le modÃ¨le MOC et une moyenne dans le modÃ¨le OKM. Soit Ai = {mc|xi âˆˆ
pic} lâ€™ensemble des centres de classes dâ€™appartenance de lâ€™individu xi
Ï†MOC(xi) =
âˆ‘
mcâˆˆAi
mc ; Ï†OKM (xi) =
âˆ‘
mcâˆˆAi
mc
|Ai|
(2)
Ainsi dÃ©fini, le critÃ¨re objectif (1) suggÃ¨re deux remarques :
â€“ Tout dâ€™abord on notera que ce critÃ¨re doit Ãªtre vu comme un critÃ¨re dâ€™inertie au mÃªme
titre que le critÃ¨re des moindres carrÃ©s utilisÃ© dans k-moyennes ; en effet la fonction
objective J exprime lâ€™inertie des individus {xi}
n
i=1 par rapport Ã  leur image {Ï†(xi)}
n
i=1
dans la classification.
â€“ Ensuite on observe que dans le cas de partitions, chaque individu ne possÃ¨de quâ€™une
seule classe dâ€™appartenance (âˆ€i, |Ai| = 1) ; pour les deux modÃ¨les lâ€™image Ï†(xi) de xi
correspond au centremc de la classe dâ€™appartenance de lâ€™individu et la fonction objective
prend la forme du critÃ¨re classique des moindres carrÃ©s (somme des distances au centre) ;
Ã  ce titre MOC et OKM gÃ©nÃ©ralisent k-moyennes.
Lâ€™optimisation1 du critÃ¨re dâ€™inertie (1) est rÃ©alisÃ©e en itÃ©rant les deux Ã©tapes usuelles : cal-
cul des paramÃ¨tres des classes (ici les centres {mc}
k
c=1) puis affectation de chaque individu Ã 
une ou plusieurs classes. MOC et OKM proposent des heuristiques diffÃ©rentes pour lâ€™initialisa-
tion des paramÃ¨tres et pour lâ€™Ã©tape dâ€™affectation multiple qui pose un problÃ¨me combinatoire.
3 OKMED comme gÃ©nÃ©ralisation des k-mÃ©doÃ¯des
3.1 Motivation de lâ€™approche et mÃ©thodes Ã  base de mÃ©doÃ¯des
Les mÃ©thodes de classification de type k-mÃ©doÃ¯des consistent Ã  agrÃ©ger les individus au-
tour de reprÃ©sentants de classes choisis parmi les individus eux-mÃªmes ; on les appelle des
mÃ©doÃ¯des par opposition aux traditionnels centroÃ¯des de classe qui sont dÃ©finis dans lâ€™espace
de description des individus mais nâ€™appartiennent pas nÃ©cessairement Ã  X .
La mÃ©thode PAM (Partitioning Around Medoids) proposÃ©e par Kaufman et Rousseeuw
(1987) est communÃ©ment admise comme lâ€™algorithme de rÃ©fÃ©rence dans ce domaine. PAM
construit un partitionnement des individus par itÃ©ration de deux Ã©tapes : affectation de chaque
individu au mÃ©doÃ¯de le plus proche puis mise Ã  jour des mÃ©doÃ¯des pour chaque classe.
1Lâ€™initialisation des paramÃ¨tres induit une recherche locale et le risque habituel dâ€™aboutir Ã  un optimum local.
OKMED et WOKM : deux variantes de OKM
Dans la seconde Ã©tape, la mise Ã  jour du mÃ©doÃ¯de dâ€™une classe consiste Ã  rechercher parmi
tous les individus de la classe celui qui minimise la somme des distances avec tous les autres
individus de la classe.
Les deux principaux avantages de ces mÃ©thodes sont dâ€™une part leur robustesse face aux
individus atypiques (outliers) et dâ€™autre part la possibilitÃ© quâ€™elles offrent dâ€™utiliser diverses
mÃ©triques puisquâ€™elles nÃ©cessitent uniquement la matrice des distances entre individus en en-
trÃ©e ; câ€™est prÃ©cisÃ©ment ce dernier point qui motive la prÃ©sente Ã©tude. En effet, les modÃ¨les
recouvrants de base MOC et OKM se limitent pour le moment Ã  une famille rÃ©duite de mÃ©-
triques (les divergences de Bregman) et lâ€™extension Ã  dâ€™autres mesures nâ€™est pas triviale.
3.2 Le modÃ¨le OKMED
Nous proposons de conserver pour le modÃ¨le OKMED le critÃ¨re objectif (1) du modÃ¨le
originel OKM, gÃ©nÃ©ralisÃ© cette fois Ã  une distance quelconque entre individus. Soient X =
{xi}
n
i=1 un ensemble dâ€™individus et d une mesure de distance de X Ã— X â†’ R
+, le critÃ¨re
objectif du modÃ¨le OKMED est donnÃ© par :
J ({pic}
k
c=1) =
âˆ‘
xiâˆˆX
d2(xi âˆ’ Ï†(xi)) (3)
Il sâ€™agira donc Ã  nouveau de minimiser lâ€™inertie des individus par rapport Ã  leur image. La
notion dâ€™image quant Ã  elle doit Ãªtre redÃ©finie en prenant appui sur des mÃ©doÃ¯des de classe
plutÃ´t que des centroÃ¯des. Nous dÃ©finissons alors lâ€™image Ï†OKMED(xi) dâ€™un individu xi dans
la classification {pic}
k
c=1 par lâ€™individu de X qui minimise la somme des distances avec tous
les mÃ©doÃ¯des des classes dâ€™appartenance de xi :
Ï†OKMED(xi) = argmin
xjâˆˆX
âˆ‘
mcâˆˆAi
d(xj ,mc) (4)
Notons quâ€™avec cette nouvelle dÃ©finition, la recherche dâ€™une image nÃ©cessite de parcourir
lâ€™ensemble des individus de X . En pratique on pourra se contenter dâ€™effectuer une seule fois
cette recherche pour chaque combinaison2 dâ€™affectations Ai observÃ©e.
Il est enfin utile de prÃ©ciser que dans le cas dâ€™un partitionnement strict, chaque individu
xi Ã©tant affectÃ© Ã  une seule classe pic, lâ€™image Ï†(xi) correspond exactement au mÃ©doÃ¯de mc.
Ainsi le modÃ¨le OKMED, via le critÃ¨re dâ€™inertie (3), doit effectivement Ãªtre vu comme une
gÃ©nÃ©ralisation des modÃ¨les de type k-mÃ©doÃ¯des.
3.3 Lâ€™algorithme OKMED
Dans la lignÃ©e des mÃ©thodes dâ€™agrÃ©gation autour de centres mobiles, nous proposons un
algorithme visant Ã  optimiser le critÃ¨re (3) en deux Ã©tapes : affectation des individus et mise Ã 
jour des paramÃ¨tres. Nous donnons Figure 1 la description de lâ€™algorithme.
Lâ€™affectation dâ€™un individu Ã  une ou plusieurs classes est rÃ©alisÃ©e au moyen de la fonction
ASSIGN() qui se fonde sur lâ€™heuristique proposÃ©e par Cleuziou (2008) . Lâ€™adaptation de cette
heuristique pour OKMED consistera, pour un individu xi, Ã  parcourir lâ€™ensemble des mÃ©doÃ¯des
2Le nombre de combinaisons possibles peut Ãªtre trÃ¨s grand en thÃ©orie, cependant seulement un sous-ensemble de
combinaisons est observÃ© en pratique.
G. Cleuziou
OKMED(D,k,tmax,)
EntrÃ©e : D une matrice de distance (n Ã— n) sur un ensemble dâ€™individus X , k un nombre
de classes, tmax : un nombre maximum dâ€™itÃ©rations (optionnel),  : un paramÃ¨tre dâ€™Ã©volution
minimale du critÃ¨re objectif (optionnel).
Sortie : {pic}
k
c=1 : une classification recouvrante sur X .
1. Tirer alÃ©atoirement k mÃ©doÃ¯des {m
(0)
c }kc=1 dans X ,
t=0.
2. Pour chaque individu xi âˆˆ X calculer les affectations
A
(t+1)
i = ASSIGN(xi, {m
(t)
c }
k
c=1)
en dÃ©duire une classification {pi
(t+1)
c }kc=1 telle que pi
(t+1)
c = {xi|m
(t)
c âˆˆ A
(t+1)
i }
3. Pour chaque classe pi
(t+1)
c successivement, calculer le nouveau mÃ©doÃ¯de
m(t+1)c = MEDOID(pi
(t+1)
c )
4. Si {pi
(t+1)
c } diffÃ©rent de {pi
(t)
c } ou t < tmax ouJ ({pi
(t)
c })âˆ’J ({pi
(t+1)
c }) > , alors t = t+1
et aller Ã  lâ€™Ã©tape 2 ; Sinon retourner la classification {pi
(t+1)
c }kc=1.
FIG. 1 â€“ Lâ€™algorithme OKMED.
{m
(t+1)
c }kc=1 dans un ordre prÃ©cis (du plus proche au plus Ã©loignÃ© au sens de D) et Ã  affecter
xi Ã  la classe correspondante tant que lâ€™inertie d(xi, Ï†(xi)) diminue. La nouvelle affectation
A
(t+1)
i ne sera conservÃ©e que si elle amÃ©liore lâ€™ancienne affectation A
(t)
i en terme dâ€™inertie
toujours. Cette maniÃ¨re de faire assure la dÃ©croissance du critÃ¨re dâ€™inertie pour cette Ã©tape.
La mise Ã  jour des paramÃ¨tres se rÃ©sume ici Ã  rechercher pour chaque classe un nouveau
reprÃ©sentant ou mÃ©doÃ¯de parmi les individus de la classe, qui soit meilleur au sens du critÃ¨re
dâ€™inertie. Lâ€™heuristique de recherche que nous proposons est formalisÃ©e par la fonction ME-
DOID() (voir Figure 2) et privilÃ©gie la recherche dâ€™un mÃ©doÃ¯de pertinent pour la classification
plutÃ´t que du mÃ©doÃ¯de â€œoptimalâ€ pour le critÃ¨re objectif. Ceci pour deux raisons :
â€“ dâ€™une part car il est souhaitable de limiter les Ã©valuations de mÃ©doÃ¯des potentiels qui sont
trÃ¨s coÃ»teuses dans notre modÃ¨le recouvrant car elles nÃ©cessitent, pour chaque individu
de la classe, une recherche dâ€™image tenant compte du nouveau mÃ©doÃ¯de potentiel.
â€“ dâ€™autre part pour Ã©viter autant que possible de choisir comme mÃ©doÃ¯de dâ€™une classe un
individu qui appartiendrait Ã  de nombreuses autres classes ; si un individu propre (i.e.
affectÃ© uniquement) Ã  la classe permet dâ€™amÃ©liorer le critÃ¨re dâ€™inertie, celui-ci sera Ã©lu
parcequâ€™il est un bon reprÃ©sentant de classe et parcequâ€™il suffit Ã  faire dÃ©croÃ®tre le critÃ¨re.
Chacune des deux Ã©tapes - affectations et mise Ã  jour des mÃ©doÃ¯des - permet de faire di-
minuer le critÃ¨re objectif (3). En notant de plus que lâ€™ensemble des classification recouvrantes
de n individus en k classes est fini pour n et k fixÃ©s, il en dÃ©coule la convergence de lâ€™algo-
rithme OKMED. La classification obtenue correspond Ã  un optimum local du critÃ¨re objectif,
la mÃ©thode Ã©tant en effet sensible Ã  lâ€™initialisation.
OKMED et WOKM : deux variantes de OKM
MEDOID(pic)
EntrÃ©e : pic une classe dâ€™individus de X .
Sortie :mc le mÃ©doÃ¯de de la classe pic.
1. Calculer lâ€™inertie des individus de pic :
J (pic) =
âˆ‘
xiâˆˆpic
d2(xi âˆ’ Ï†(xi))
2. Pour p allant de 1 Ã  k faire :
Pour chaque xj âˆˆ pic tel que |Aj | = p faire :
calculer les images Ï†(xi)
â€² avecmc = xj pour tout xi âˆˆ pic
calculer la nouvelle inertie pour pic
J â€²(pic) =
âˆ‘
xiâˆˆpic
d2(xi âˆ’ Ï†
â€²(xi))
si J â€²(pic) < J (pic) retourner xj (nouveau mÃ©doÃ¯de de pic)
FIG. 2 â€“ Mise Ã  jour des mÃ©doÃ¯des de classe.
4 WOKM pour une pondÃ©ration locale des classes
4.1 Motivation et modÃ¨le de rÃ©fÃ©rence
Prenons lâ€™exemple de la classification de documents textuels dÃ©crits par des vecteurs de
frÃ©quences de mots. Si lâ€™objectif est dâ€™organiser les textes de faÃ§on thÃ©matique, il est naturel
dâ€™imaginer que certains documents soient mono-thÃ©matiques (vocabulaire dâ€™un seul thÃ¨me)
et dâ€™autres pluri-thÃ©matiques (vocabulaires de plusieurs thÃ¨mes). PlutÃ´t que dâ€™avoir Ã  choisir
une seule classe, et risquer de perdre une partie importante de lâ€™information du document, la
classification recouvrante offre la possibilitÃ© dâ€™affecter le document Ã  plusieurs classes. Pour
autant, dans les modÃ¨les vus prÃ©cÃ©demment (MOC et OKM), un document qui contiendrait le
vocabulaire dâ€™un thÃ¨me verrait ses chances dâ€™Ãªtre affectÃ© Ã  la classe thÃ©matique correspondante
diminuÃ©es sâ€™il contenait Ã©galement les vocabulaires associÃ©s Ã  des thÃ¨mes diffÃ©rents.
Lâ€™idÃ©e des modÃ¨les avec pondÃ©ration locale des classes, vise justement Ã  Ã©viter le phÃ©no-
mÃ¨ne prÃ©cÃ©dent, en permettant Ã  lâ€™individu dâ€™Ãªtre affectÃ© Ã  une classe sur la base des descrip-
teurs importants pour la dite classe. Ainsi, la prÃ©sence du vocabulaire dâ€™un thÃ¨me suffirait Ã 
dÃ©cider de lâ€™appartenance du document Ã  la classe associÃ©e Ã  ce thÃ¨me. Ce type de modÃ¨le est
donc particuliÃ¨rement appropriÃ© lors de la recherche de classes recouvrantes.
Nous proposons ici dâ€™Ã©tendre le modÃ¨le des k-moyennes pondÃ©rÃ© proposÃ© par Chan et al.
(2004) au cas recouvrant. Ce modÃ¨le gÃ©nÃ©ralise le critÃ¨re des moindres carrÃ©s utilisÃ© dans k-
moyenne par une pondÃ©ration des variables, diffÃ©rente pour chaque classe. Soit X = {xi}
n
i=1
un ensemble dâ€™individus dans Rp, le critÃ¨re sâ€™exprime ainsi :
J ({pic}
k
c=1) =
kâˆ‘
c=1
âˆ‘
xiâˆˆpic
pâˆ‘
v=1
Î»Î²c,v|xi,v âˆ’mc,v|
2 avec âˆ€c,
pâˆ‘
v=1
Î»c,v = 1 (5)
G. Cleuziou
Dans (5), les {Î»c,v} reprÃ©sentent les poids associÃ©s Ã  chaque variable pour chaque classe,
et Î² est un paramÃ¨tre (> 1) permettant de rÃ©gler lâ€™influence de la pondÃ©ration dans le modÃ¨le.
Câ€™est sur cette base de travail que nous proposons le modÃ¨le WOKM qui gÃ©nÃ©ralise Ã  la fois
les modÃ¨les OKM et k-moyennes pondÃ©rÃ©.
4.2 Le modÃ¨leWOKM
IntÃ©grer la pondÃ©ration locale des classes dans le critÃ¨re dâ€™inertie (1) des modÃ¨les recou-
vrants nâ€™est pas trivial. En effet, lâ€™inertie mesure la dispersion des individus vis Ã  vis de leur
image plutÃ´t que de leur reprÃ©sentant de classe. Il sâ€™agit donc de se poser en premier la ques-
tion de dÃ©finir lâ€™image dâ€™un individu dans un environnement de classes avec pondÃ©ration. Nous
proposons de dÃ©finir lâ€™image de xi par la moyenne pondÃ©rÃ©e des centres des classes de xi :
Ï†WOKM (xi) = (Ï†1(xi), . . . , Ï†p(xi)) avec Ï†v(xi) =
âˆ‘
mcâˆˆAi
Î»Î²c,vmc,vâˆ‘
mcâˆˆAi
Î»Î²c,v
(6)
Cette dÃ©finition assure dâ€™une part la gÃ©nÃ©ricitÃ© du modÃ¨le et dâ€™autre part une bonne intui-
tion pour la notion dâ€™image. Par ailleurs, lâ€™image dâ€™un individu xi modÃ©lise en quelque sorte
un point de Rp reprÃ©sentatif de lâ€™intersection des classes de Ai. Puisque chaque classe pic est
caractÃ©risÃ©e par un vecteur de poids Î»c, il convient de proposer une pondÃ©ration des intersec-
tions et donc, par analogie, de proposer un vecteur de poids Î³i pour les images Ï†(xi). On pose
alors :
Î³i,v =
âˆ‘
mcâˆˆAi
Î»c,v
|Ai|
(7)
Il en dÃ©coule le critÃ¨re objectif suivant pour le modÃ¨le WOKM :
J ({pic}
k
c=1) =
âˆ‘
xiâˆˆX
pâˆ‘
v=1
Î³Î²i,v|xi,v âˆ’ Ï†v(xi)|
2 (8)
Ce critÃ¨re est soumis Ã  la contrainte âˆ€c,
âˆ‘p
v=1 Î»c,v = 1 sur les pondÃ©rations locales des
classes, encapsulÃ©es dans la dÃ©finition des poids {Î³i,v}. On note la gÃ©nÃ©ricitÃ© du modÃ¨le en
observant que :
â€“ dans le cas dâ€™affectations strictes, si xi âˆˆ pic alors Ï†v(xi) = mc,v et Î³i,v = Î»c,v ; le
critÃ¨re se ramÃ¨ne alors au critÃ¨re (5) utilisÃ© dans les k-moyennes pondÃ©rÃ©.
â€“ dans le cas de pondÃ©rations uniformes (âˆ€c, âˆ€v, Î»c,v = 1/p), Î³i,v = 1/p et Ï†WOKM (xi) =
Ï†OKM (xi) ; le critÃ¨re se ramÃ¨ne au critÃ¨re (1) utilisÃ© dans OKM, Ã  une constante prÃ¨s.
4.3 Lâ€™algorithmeWOKM
Lâ€™optimisation du critÃ¨re (8) se traduit algorithmiquement par lâ€™itÃ©ration de trois Ã©tapes :
affectation, mise Ã  jour des centres et mise Ã  jour des poids (cf. Figure 3).
Lâ€™Ã©tape dâ€™affectation (ASSIGN) procÃ¨de de faÃ§on similaire aux algorithmes OKM et OK-
MED, par affectation dâ€™un individu Ã  ses classes proches tant que
âˆ‘p
v=1 Î³
Î²
i,v|xi,v âˆ’ Ï†v(xi)|
2
diminue. La mise Ã  jour des centres de classes (CENTROID) peut Ãªtre rÃ©alisÃ©e sur chaque classe
successivement en considÃ©rant les autres centres fixÃ©s ; dans ce contexte on peut montrer3 que
le centre optimal mâˆ—c pour la classe pic est donnÃ© par le centre de gravitÃ© du nuage de points
3La preuve nâ€™est pas prÃ©sentÃ©e ici faute de place ; ce rÃ©sultat sâ€™obtient par minimisation dâ€™une fonction convexe.
OKMED et WOKM : deux variantes de OKM
WOKM(X ,k,tmax,)
EntrÃ©e : X un ensemble dâ€™individus dans Rp, k un nombre de classes, tmax un nombre maxi-
mum dâ€™itÃ©rations (optionnel),  un paramÃ¨tre dâ€™Ã©volution minimale du critÃ¨re objectif (option-
nel).
Sortie : {pic}
k
c=1 : une classification recouvrante sur X .
1. Tirer alÃ©atoirement k centres {m(0)c }kc=1 dans R
p ou dans X ,
Initialiser les poids {Î»(0)c,v} uniformÃ©ment (Î»
(0)
c,v = 1/p),t = 0.
2. Pour chaque individu xi âˆˆ X calculer les affectations
A
(t+1)
i = ASSIGN(xi, {m
(t)
c }
k
c=1)
en dÃ©duire une classification {pi(t+1)c }kc=1 telle que pi
(t+1)
c = {xi|m
(t)
c âˆˆ A
(t+1)
i }
3. Pour chaque classe pi(t+1)c successivement, calculer le nouveau centre
m(t+1)c = CENTROID(pi
(t+1)
c )
4. Pour chaque classe pi(t+1)c successivement, calculer la nouvelle pondÃ©ration
Î»c,. = WEIGHTING(pi
t+1
c )
5. Si {pi(t+1)c } diffÃ©rent de {pi
(t)
c } ou t < tmax ouJ ({pi
(t)
c })âˆ’J ({pi
(t+1)
c }) > , alors t = t+1
et aller Ã  lâ€™Ã©tape 2 ; Sinon retourner la classification {pi(t+1)c }kc=1.
FIG. 3 â€“ Lâ€™algorithmeWOKM.
{(xË†ci , wi)|xi âˆˆ pic}. La notation xË†
c
i dÃ©signe le centre de la classe pic qui permettrait Ã  lâ€™image
de lâ€™individu xi dâ€™Ãªtre confondue avec xi lui-mÃªme (âˆ€v, |xi,v âˆ’ Ï†v(xi)| = 0) et wi dÃ©signe le
vecteur de pondÃ©ration associÃ© et dÃ©fini par : wi,v =
Î³Î²i,vâ€œP
mlâˆˆAi
Î»Î²
l,v
â€
2 .
Enfin la troisiÃ¨me Ã©tape (WEIGHTING), la mise Ã  jour des vecteurs de poids {Î»c}kc=1, re-
vient Ã  rÃ©soudre un problÃ¨me dâ€™optimisation sous contrainte (
âˆ‘p
v=1 Î»c,v = 1) ; contrairement
au modÃ¨le non recouvrant de Chan et al. (2004), le caractÃ¨re recouvrant de notre modÃ¨le ne
permet pas la mise Ã  jour de chaque vecteur Î»c de faÃ§on indÃ©pendante ; le thÃ©orÃ¨me proposÃ©
par Bezdek (1981) pour la classification floue nâ€™assure pas lâ€™optimalitÃ© de la solution dans
ce contexte. NÃ©anmoins, nous utiliserons une heuristique qui sâ€™inspire de ce thÃ©orÃ¨me et qui
consiste, pour chaque classe indÃ©pendamment, Ã  :
1. calculer une nouvelle pondÃ©ration Î»c,v de la classe pic en Ã©valuant la variance des indi-
vidus propres Ã  la classe sur chaque variable :
Î»c,v =
(âˆ‘
{xiâˆˆpic| |Ai|=1}
(xi,v âˆ’mc,v)
2
)1/(1âˆ’Î²)
âˆ‘p
u=1
(âˆ‘
xiâˆˆpic| |Ai|=1
(xi,u âˆ’mc,u)2
)1/(1âˆ’Î²)
2. conserver cette pondÃ©ration seulement si elle amÃ©liore le critÃ¨re dâ€™inertie globale du
modÃ¨le.
G. Cleuziou
Les choix dâ€™affectation et de mise Ã  jour des paramÃ¨tres ont chaque fois Ã©tÃ© effectuÃ©s de
maniÃ¨re Ã  assurer la dÃ©croissance du critÃ¨re objectif et donc la convergence de WOKM.
5 ExpÃ©rimentations
Nous prÃ©sentons une sÃ©rie dâ€™expÃ©rimentations prÃ©liminaires visant Ã  observer le comporte-
ment des deux variantes OKMED et WOKM. Le premier jeu de donnÃ©es utilisÃ© (Iris) est familier
des chercheurs du domaine et permet de se faire une premiÃ¨re idÃ©e sur les performances dâ€™une
mÃ©thode de classification ; le second (Reuters) correspond dâ€™avantage aux applications ciblÃ©es
dans cette Ã©tude puisquâ€™il sâ€™agit de documents textuels multi-labels.
Pour chaque expÃ©rience, lâ€™Ã©valuation proposÃ©e consiste Ã  comparer la classification obte-
nue avec la classification de rÃ©fÃ©rence (labels) en terme de mesures de prÃ©cision, rappel et
F-Score. Ces indices4 sont ceux utilisÃ©s et dÃ©taillÃ©s par Cleuziou (2007) pour OKM et Banerjee
et al. (2005) pour MOC.
5.1 ExpÃ©rimentations sur la base Iris
La base Iris (base de lâ€™UCI repository) comporte 150 individus dans R4 organisÃ©s en trois
classes de mÃªmes tailles, dont lâ€™une (setosa) est connue pour Ãªtre sÃ©parÃ©e des deux autres.
Les valeurs prÃ©sentÃ©es dans le tableau Tab.1 rÃ©sultent dâ€™une moyenne sur 500 exÃ©cutions
des six mÃ©thodes avec les mÃªmes conditions initiales et avec k = 3.
PrÃ©cision Rappel F-Score Affectations
k-moyennes 0.75 0.82 0.78 1.00
k-mÃ©doÃ¯des 0.75 0.84 0.79 1.00
k-moyennes pondÃ©rÃ© 0.85 0.89 0.86 1.00
OKM 0.57 0.98 0.72 1.40
OKMED 0.61 0.88 0.71 1.16
WOKM 0.62 0.98 0.76 1.32
TAB. 1 â€“ Comparaisons des performances des modÃ¨les sur Iris.
Les rÃ©sultats sur les mÃ©thodes non recouvrantes sont donnÃ©es Ã  titre indicatif ; il est en
effet normal que les mÃ©thodes recouvrantes obtiennent des rÃ©sultats infÃ©rieurs puisque le jeu
de donnÃ©es nâ€™est pas multi-labels.
On note en premier lieu que OKMED obtient un F-Score sensiblement Ã©gal Ã  celui de OKM ;
dans la mesure oÃ¹ on observe le mÃªme phÃ©nomÃ¨ne sur leur analogue non-recouvrant, ce rÃ©sultat
vient conforter expÃ©rimentalement le fait que OKMED gÃ©nÃ©ralise k-mÃ©doÃ¯des. On remarquera
Ã©galement, via lâ€™indice dâ€™affectations, que OKMED gÃ©nÃ¨re moins de recouvrements que OKM ;
ce phÃ©nomÃ¨ne sâ€™explique naturellement par le fait que la recherche dâ€™images est limitÃ©e aux
individus de X dans OKMED et Ã©largie Ã  lâ€™ensemble des points de Rp dans OKM.
Enfin, la supÃ©rioritÃ© de la version pondÃ©rÃ©e de k-moyennes est Ã©galement observÃ©e dans
les versions recouvrantes. Ceci vient confirmer empiriquement le modÃ¨le WOKM en tant que
gÃ©nÃ©ralisation du modÃ¨le de Chan et al. (2004) et surtout valide notre intÃ©rÃªt pour les approches
avec pondÃ©ration locale des classes.
4Aussi lâ€™indice â€œaffectationsâ€ correspondant au nombre moyen de classes auxquelles chaque individu appartient.
OKMED et WOKM : deux variantes de OKM
5.2 ExpÃ©rimentations sur les donnÃ©es Reuters
La seconde sÃ©rie dâ€™expÃ©rimentations est rÃ©alisÃ©e sur le corpus Reuters traditionnellement
utilisÃ© comme benchmark en recherche dâ€™information. Nous nous limitons Ã  un sous-ensemble
de 300 documents multi-labels et dÃ©crits par des vecteurs de frÃ©quences sur un vocabulaire
constituÃ© des 500 mots les plus pertinents au sens de lâ€™indice tfxidf.
Afin dâ€™illustrer la possibilitÃ© offerte par OKMED dâ€™avoir recours Ã  des mÃ©triques autres
que la distance euclidienne, nous observons (Figure 4) les performances de OKM, de OKMED
avec la distance euclidienne puis avec la divergence de Kullback-Leibler (ou I-Divergence), en
faisant varier le nombre k de classes.
 0.24
 0.245
 0.25
 0.255
 0.26
 0.265
 0.27
 0.275
 0.28
 2  4  6  8  10  12  14
Pr
ec
is
io
n
Nbre de classes
OKM
OKmed-euclidien
OKmed-Idiv
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 2  4  6  8  10  12  14
R
ap
pe
l
Nbre de classes
OKM
OKmed-euclidien
OKmed-Idiv
 0.25
 0.3
 0.35
 0.4
 0.45
 2  4  6  8  10  12  14
F-
Sc
or
e
Nbre de classes
OKM
OKmed-euclidien
OKmed-Idiv
FIG. 4 â€“ OKMED avec diffÃ©rentes distances.
Nous pouvons ainsi remarquer que OKMED prÃ©sente un comportement stable pour des
distances diffÃ©rentes et surtout que lâ€™utilisation de la I-Divergence, reconnue performante pour
comparer des textes, permet dâ€™obtenir des prÃ©cisions meilleures et inaccessibles par la distance
euclidienne. Ce dernier rÃ©sultat valide lâ€™intÃ©rÃªt du modÃ¨le OKMED.
Enfin, les courbes prÃ©sentÃ©es en Figure 5 prÃ©cisent lâ€™influence des modÃ¨les de pondÃ©ration
locale en particulier pour la classification recouvrante.
Si la pondÃ©ration locale nâ€™apporte pas de changement significatif dans les modÃ¨les non-
recouvrants (k-moyennes â‰ˆ k-moyennes pondÃ©rÃ©), ceci nâ€™est pas vrai dans les modÃ¨les recou-
vrants. En effet, lâ€™apport espÃ©rÃ© par le modÃ¨le WOKM vis Ã  vis de son analogue non pondÃ©rÃ©
OKM, se traduit empiriquement par :
1. une limitation des recouvrements ; le nombre moyen dâ€™affectations plus faible ;
2. une rÃ©duction du rappel ; ce qui est une consÃ©quence directe de lâ€™observation prÃ©cÃ©dente ;
3. une amÃ©lioration significative de la prÃ©cision.
Dâ€™une maniÃ¨re gÃ©nÃ©rale, la pondÃ©ration locale introduite dans WOKM semble venir corri-
ger le modÃ¨le OKM en limitant les affectations multiples â€œparasitesâ€.
6 Conclusion et perspectives
Nous avons exposÃ© dans cette Ã©tude deux contributions dans le domaine de la classification
recouvrante. La premiÃ¨re sâ€™inspire des techniques de partitionnement par agrÃ©gation autour de
mÃ©doÃ¯des en proposant le modÃ¨le OKMED ; ce dernier permet dâ€™organiser un ensemble dâ€™in-
dividus en classes recouvrantes dâ€™individus similaires, sur la base uniquement dâ€™une matrice
de distance. La seconde contribution propose, Ã  travers lâ€™algorithme WOKM, dâ€™introduire une
pondÃ©ration locale des classes dans les modÃ¨les de classification recouvrante.
G. Cleuziou
 0.23
 0.24
 0.25
 0.26
 0.27
 0.28
 0.29
 0.3
 2  4  6  8  10  12  14
Pr
ec
is
io
n
Nbre de classes
k-moyennes
OKM
weight-k-moyennes
WOKM
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 2  4  6  8  10  12  14
R
ap
pe
l
Nbre de classes
k-moyennes
OKM
weight-k-moyennes
WOKM
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 2  4  6  8  10  12  14
F-
Sc
or
e
Nbre de classes
k-moyennes
OKM
weight-k-moyennes
WOKM
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2.8
 2  4  6  8  10  12  14
N
b 
m
oy
en
 a
ff
ec
ta
tio
ns
/in
di
v
Nbre de classes
k-moyennes
OKM
weight-k-moyennes
WOKM
FIG. 5 â€“ Influence de la pondÃ©ration locale des classes.
Ces deux apports se prÃ©sentent comme des gÃ©nÃ©ralisations Ã  la fois des mÃ©thodes de par-
titionnement strict (k-moyennes, k-mÃ©doÃ¯des et k-moyennes pondÃ©rÃ©) et des mÃ©thodes de re-
couvrement (OKM et MOC). Nous avons justifiÃ© les critÃ¨res objectifs associÃ©s, proposÃ© des
algorithmes et heuristiques dâ€™optimisation de ces critÃ¨res puis validÃ© ces mÃ©thodes par des
expÃ©rimentations prÃ©liminaires sur des jeux de donnÃ©es adaptÃ©s.
Nous chercherons Ã  confirmer les bonnes propriÃ©tÃ©s observÃ©es sur ces mÃ©thodes par des
expÃ©rimentations supplÃ©mentaires mettant en jeu dâ€™autres corpus textuels, et dâ€™autres domaines
(e.g. Bioinformatique). Nous envisagerons de poursuivre lâ€™enrichissement de cette famille de
mÃ©thodes de classification recouvrante en explorant dâ€™autres variantes pertinentes telles que
les cartes auto-organisatrices recouvrantes, la classification recouvrante Ã  base de noyaux, etc.
Cependant, de faÃ§on plus directement liÃ©e Ã  cette Ã©tude, les deux variantes proposÃ©es posent
les bases dâ€™une approche combinant les bienfaits des deux modÃ¨les au sein dâ€™un algorithme
dâ€™agrÃ©gation autour de petits ensembles de mÃ©doÃ¯des capturant la forme des clusters.
RÃ©fÃ©rences
Banerjee, A., C. Krumpelman, J. Ghosh, S. Basu, et R. J. Mooney (2005). Model-based over-
lapping clustering. In KDD â€™05 : Proceeding of the eleventh ACM SIGKDD international
conference on Knowledge discovery in data mining, New York, NY, USA, pp. 532â€“537.
ACM Press.
Bertrand, P. et M. F. Janowitz (2003). The k-weak hierarchical representations : An extension
of the indexed closed weak hierarchies. Discrete Applied Mathematics 127(2), 199â€“220.
OKMED et WOKM : deux variantes de OKM
Bezdek, J. C. (1981). Pattern Recognition with Fuzzy Objective Function Algoritms. Plenum
Press, New York.
Chan, E. Y., W.-K. Ching, M. K. Ng, et J. Z. Huang (2004). An optimization algorithm for
clustering using weighted dissimilarity measures. Pattern Recognition 37(5), 943â€“952.
Cleuziou, G. (2007). Okm : une extension des k-moyennes pour la recherche de classes recou-
vrantes. In EGCâ€™2007, Volume 2, Namur, Belgique. Revue des Nouvelles Technologies de
lâ€™Information, CÃ©paduÃ¨s-Edition.
Cleuziou, G. (2008). An Extended Version of the k-Means Method for Overlapping Clustering.
In 19th Conference on Pattern Recognition ICPRâ€™08 (to appear).
Cleuziou, G. et J.-H. Sublemontier (2008). Ã©tude comparative de deux approches de classi-
fication recouvrante : Moc vs. okm. In 8Ã¨mes JournÃ©es Francophones dâ€™Extraction et de
Gestion des Connaissances, Volume 2. Revue des Nouvelles Technologies de lâ€™Information,
CÃ©paduÃ¨s-Edition.
Dattola, R. (1968). A fast algorithm for automatic classification. Technical report, Report
ISR-14 to the National Science Foundation, Section V, Cornell University, Department of
Computer Science.
Diday, E. (1987). Orders and overlapping clusters by pyramids. Technical report, INRIA
num.730, Rocquencourt 78150, France.
Diday, E. et G. Govaert (1977). Classification avec distances adaptatives. RAIRO 11(4), 329â€“
349.
D.Pelleg et A. Moore (2000). X-means : Extending k-means with efficient estimation of the
number of clusters. In Proceedings of the Seventeenth International Conference on Machine
Learning, San Francisco, pp. 727â€“734. Morgan Kaufmann.
Jardine, N. et R. Sibson (1971). Mathematical Taxonomy. London : John Wiley and Sons Ltd.
Kaufman, L. et P. J. Rousseeuw (1987). Clustering by means of medoids. In Dodge, Y. (Ed.)
Statistical Data Analysis based on the L1 Norm, 405â€“416.
Likas, A., N. Vlassis, et J. Verbeek (2003). The global k-means clustering algorithm. Pattern
Recognition 36, 451â€“461.
PeÃ±a, J., J. Lozano, et P. LarraÃ±aga (1999). An empirical comparison of four initialization
methods for the k-means algorithm. Pattern Recognition Letters 20(50), 1027â€“1040.
Summary
This paper deals with overlapping clustering and presents two extensions of the approach
OKM denoted as OKMED and WOKM. OKMED generalizes the well known k-medoid method
to overlapping clustering and help in organizing data with distance matrices as input. WOKM
(Weighted-OKM) proposes a model with local weighting of the classes; this variant is suitable
for overlapping clustering since a single data can matches with multiple classes according to
different characteristics. On text clustering, we show that OKMED has a behavior similar to
OKM but offers to use metrics other than euclidean distance. Then we observe significant
improvement using the weighted extension of OKM.
