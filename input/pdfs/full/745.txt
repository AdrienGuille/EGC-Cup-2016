Le Graphe GeÂ´neÂ´ratif Gaussien
Pierre Gaillard1, MichaeÂ¨l Aupetit2, GeÂ´rard Govaert3
1 CEA, DAM, DIF, F-91297 Arpajon, France
2 CEA, LIST, F-91191 Gif-sur-Yvette cedex, France
3 UTC, U.M.R. C.N.R.S. 6599 Heudiasyc, F-60205 Compie`gne Cedex, France
ReÂ´sumeÂ´ Un nuage de points est plus quâ€™un ensemble de points isoleÂ´s. La distri-
bution des points peut eË†tre gouverneÂ´e par une structure topologique cacheÂ´e, et du
point de vue de la fouille de donneÂ´es, modeÂ´liser et extraire cette structure est au
moins aussi important que dâ€™estimer la seule densiteÂ´ de probabiliteÂ´ du nuage. Dans
cet article, nous proposons un mode`le geÂ´neÂ´ratif baseÂ´ sur le graphe de Delaunay
dâ€™un ensemble de prototypes repreÂ´sentant le nuage de points, et supposant un bruit
gaussien. Nous deÂ´rivons un algorithme pour la maximisation de la vraisemblance
des parame`tres, et nous utilisons le crite`re BIC pour seÂ´lectionner la complexiteÂ´ du
mode`le. Ce travail a pour objectif de poser les premie`res pierres dâ€™un cadre theÂ´orique
baseÂ´ sur les mode`les geÂ´neÂ´ratifs statistiques, permettant la construction automatique
de mode`les topologiques dâ€™un nuage de points.
Keywords : connexiteÂ´, graphe de Delaunay, mode`le geÂ´neÂ´ratif, algorithme EM,
crite`re BIC
1 Introduction
Dans les proble`mes dâ€™apprentissage statistique, on suppose que les donneÂ´es sont geÂ´neÂ´reÂ´es
par une densiteÂ´ de probabiliteÂ´ P : RD â†’ R+. Cependant, le processus sous-jacent de
geÂ´neÂ´ration des donneÂ´es deÂ´fini par la fonction P , posse`de dans de nombreux cas dâ€™inteÂ´reË†t
moins de degreÂ´s de liberteÂ´ que lâ€™espace dâ€™observation de dimension D. La formalisation
de cette intuition est de supposer que les donneÂ´es sont suË†res ou proches dâ€™un ensemble
de varieÂ´teÂ´s, appeleÂ´es varieÂ´teÂ´s principales [1], chacune ayant une dimension intrinse`que
infeÂ´rieure a` la dimension de lâ€™espace dâ€™observation.
Etant donneÂ´ un ensemble x de M points observeÂ´s, dans un espace euclidien a` D di-
mensions, les meÂ´thodes statistiques permettent de reÂ´soudre des proble`mes tre`s geÂ´neÂ´raux
de discrimination, classification ou reÂ´gression, en estimant la densiteÂ´ de probabiliteÂ´ de cet
ensemble (mode`les de meÂ´lange [2], meÂ´thodes a` noyau [3]). Bien que la fonction densiteÂ´ de
probabiliteÂ´ contienne la totaliteÂ´ de lâ€™information extractible de la population dont le nuage
de points est un eÂ´chantillon, celle-ci ne rend pas explicite lâ€™information geÂ´omeÂ´trique et
topologique relatives aux varieÂ´teÂ´s principales. Pourtant, si lâ€™on suppose quâ€™une structure
existe dans les donneÂ´es, lâ€™extraire et la caracteÂ´riser a` partir de la densiteÂ´ sont aussi impor-
tants que dâ€™estimer la densiteÂ´ de probabiliteÂ´ elle-meË†me. Par exemple, dans le contexte dâ€™un
proble`me de classification, la connexiteÂ´ de cette structure semble eË†tre le moyen naturel
pour deÂ´finir des groupes homoge`nes. Lâ€™inteÂ´reË†t dâ€™utiliser cette structure sous-jacente qui
gouverne la distribution des donneÂ´es est majeur, puisque celle-ci peut eË†tre aussi utiliseÂ´e
pour analyser [4], visualiser [5], discriminer les donneÂ´es [6].
De manie`re geÂ´neÂ´rale, on pourrait extraire des caracteÂ´ristiques geÂ´omeÂ´triques de cette
structure telles que la position relative de ses diffeÂ´rentes parties, mais aussi des ca-
racteÂ´ristiques dites topologiques telles que la dimension intrinse`que ou la connexiteÂ´.
cÂ© Revue MODULAD, 2009 -103- NumeÂ´ro 40
Lâ€™Apprentissage de la Topologie est un domaine reÂ´cent en Apprentissage Automatique
[7], dont lâ€™objectif est de deÂ´velopper des meÂ´thodes baseÂ´es sur les statistiques pour retrouver
les invariants topologiques de ces varieÂ´teÂ´s a` partir du nuage de points. La connexiteÂ´ ou la
dimension intrinse`que sont de tels invariants topologiques et dans ces travaux nous nous
focalisons sur lâ€™extraction de la connexiteÂ´ des varieÂ´teÂ´s principales dâ€™un nuage de points.
Dans la Section 2, nous proposons un bref eÂ´tat de lâ€™art du domaine de lâ€™Apprentissage
de la Topologie. Dans la Section 3 et 4, nous preÂ´sentons respectivement le mode`le du
Graphe GeÂ´neÂ´ratif Gaussien (GGG) et un algorithme pour extraire la connexiteÂ´ de varieÂ´teÂ´s
principales. Dans la Section 5 et 6, nous utilisons ce mode`le pour analyser un ensemble
de donneÂ´es ainsi que pour le deÂ´bruitage de donneÂ´es.
2 Etat de lâ€™art
Les approches dâ€™Apprentissage de la Topologie sont geÂ´neÂ´ralement baseÂ´es sur la construc-
tion dâ€™un espace dont la topologie nâ€™est pas contrainte a priori mais au contraire apprise
des donneÂ´es, cela au prix de la visualisabiliteÂ´ (possibiliteÂ´ de structures non connexes et de
dimensions intrinse`ques non homoge`nes non preÂ´servables par projection). Par exemple,
Martinetz [8] ou Aupetit [9] se sont baseÂ´s sur la construction dâ€™un graphe ayant pour
sommets des prototypes, et dont la connexiteÂ´ tendait a` reproduire celle de la structure
sous-jacente aux donneÂ´es.
Martinetz et Schulten [8] ont proposeÂ´ un algorithme de construction dâ€™un graphe appeleÂ´
Triangulation de Delaunay Induite (TDI). Cet algorithme appeleÂ´ Competitive Hebbian
Learning (CHL), consiste a` localiser N0 sommets w = {wn âˆˆ RD}N0n=1 sur la distribution
des donneÂ´es puis a` connecter deux sommets wi et wj sâ€™il existe une donneÂ´e x âˆˆ x dont ils
sont les premier et deuxie`me plus proches voisins. Une telle donneÂ´e est appeleÂ´ teÂ´moin de
lâ€™arc {i, j}, et cet arc fait partie du graphe de Delaunay DG(w) des sommets w.
Dâ€™un point de vue de lâ€™apprentissage statistique, nous observons que le CHL [8] et la
TDI reÂ´sultante ont certaines limites.
1. SensibiliteÂ´ au bruit. Une donneÂ´e est suffisante pour que le CHL creÂ´e un lien de la
TDI, le rendant ainsi peu robuste au bruit. Un processus de veillissement des aË†ges
a eÂ´teÂ´ proposeÂ´ pour filtrer le bruit [10, 11]. Ce processus est eÂ´quivalent a` supprimer
les liens creÂ´eÂ´s par un nombre de donneÂ´es teÂ´moins infeÂ´rieur a` un seuil fixeÂ´. Ceci peut
eË†tre vu comme un filtre baseÂ´ sur la densiteÂ´ de probabiliteÂ´ des donneÂ´es dans la reÂ´gion
dâ€™influence des liens. Cependant aucun crite`re nâ€™a eÂ´teÂ´ proposeÂ´ pour reÂ´gler le seuil.
2. Non-consistance du mode`le. MeË†me une infiniteÂ´ de donneÂ´es eÂ´chantillonneÂ´es aussi
finement que voulues sur la reÂ´alisation geÂ´omeÂ´trique de la TDI ne garantissent pas
dâ€™eË†tre teÂ´moins de tous les liens de la TDI. Ainsi la reÂ´alisation geÂ´omeÂ´trique du mode`le
nâ€™est pas forceÂ´ment repreÂ´sentable par le mode`le lui-meË†me.
3. Aucune mesure de qualiteÂ´. Il nâ€™existe pas de mesures de qualiteÂ´ du graphe
obtenu et donc pas de crite`res permettant de comparer et seÂ´lectionner un graphe
parmi une collection de graphes. Ceci est probleÂ´matique lorsque la dimension de
lâ€™espace dâ€™observation est supeÂ´rieure a` trois puisque la visualisation est impossible.
cÂ© Revue MODULAD, 2009 -104- NumeÂ´ro 40
3 Le Graphe GeÂ´neÂ´ratif Gaussien
Afin de deÂ´passer les limites du Competitive Hebbian Learning (CHL), nous avons
changeÂ´ de point de vue. Si nous consideÂ´rons la densiteÂ´ de probabiliteÂ´ de la population
dont le nuage de points est un eÂ´chantillon, nous souhaitons deÂ´tecter les reÂ´gions de faible
densiteÂ´ qui seÂ´parent les reÂ´gions de forte densiteÂ´, et surtout rendre explicite le reÂ´sultat de
cette seÂ´paration en termes de connexiteÂ´. Il nous faut donc un mode`le de densiteÂ´ parti-
culier en ce quâ€™il rend extractible (calculable) lâ€™information sur la connexiteÂ´. A cette fin,
nous proposons un un mode`le de graphe geÂ´neÂ´ratif qui combine des approches statistiques
et geÂ´omeÂ´triques en deÂ´finissant un mode`le de meÂ´lange gaussien construit a` partir de la
reÂ´alisation geÂ´omeÂ´trique dâ€™un graphe.
Le graphe est un moyen efficace de deÂ´finir un mode`le flexible permettant de caracteÂ´riser
la connexiteÂ´ de varieÂ´teÂ´s meË†me tre`s compliqueÂ´es. De plus, lâ€™on sait facilement extraire la
connexiteÂ´ de la structure discre`te dâ€™un graphe. Le mode`le de meÂ´lange permet quant a`
lui dâ€™inscrire rigoureusement le mode`le dans le cadre de lâ€™apprentissage statistique. En
particulier, lâ€™introduction dâ€™une densiteÂ´ de probabiliteÂ´ deÂ´finit un mode`le de bruit pour les
donneÂ´es et permet lâ€™utilisation de crite`res statistiques pour mesurer la qualiteÂ´ du mode`le.
3.1 Hypotheses du mode`le
Le mode`le que nous proposons repose sur les hypothe`ses suivantes. Elles sont repreÂ´senteÂ´es
par la figure 1.
â€“ Les varieÂ´teÂ´s principales sont inconnues : nous supposons quâ€™il existe un graphe
G engendreÂ´ par des des points w de RD qui a la meË†me connexiteÂ´ que les varieÂ´teÂ´s
principales.
â€“ La densiteÂ´ de probabiliteÂ´ le long des varieÂ´teÂ´s est inconnue : on suppose que
la densiteÂ´ est uniforme sur chaque lien de la reÂ´alisation geÂ´omeÂ´trique du graphe G.
â€“ La nature du bruit est inconnue : nous supposons que le bruit Îº est deÂ´fini par
une densiteÂ´ gaussienne de moyenne 0 et de variance Ïƒ2. Ceci a pour conseÂ´quence
dâ€™inscrire rigoureusement le mode`le dans le cadre de lâ€™apprentissage statistique.
3.2 Description des composants du mode`le
Etant donneÂ´ un ensemble de N0 sommets w = {wn âˆˆ RD}N0n=1 et un graphe G(w,E) les
connectant, le mode`le utiliseÂ´ pour extraire la connexiteÂ´ des varieÂ´teÂ´s principales est baseÂ´
sur deux types de composants que lâ€™on appelle le point-gaussien et le segment-gaussien.
La valeur de la densiteÂ´ pour une donneÂ´e xi âˆˆ x geÂ´neÂ´reÂ´e par un point-gaussien centreÂ´
sur un sommet wn âˆˆ w et de variance Ïƒ2 est :
g0(xi|wn;Ïƒ2) = 1
(2piÏƒ2)
D
2
exp
(
âˆ’(xi âˆ’ wn)
2
2Ïƒ2
)
(1)
Un segment-gaussien est deÂ´fini comme une somme infinie de points-gaussiens uni-
formeÂ´ment distribueÂ´s le long dâ€™un segment : câ€™est lâ€™inteÂ´grale dâ€™un point-gaussien le long
dâ€™un segment. La valeur de la densiteÂ´ dâ€™une donneÂ´e xi âˆˆ x geÂ´neÂ´reÂ´e par un segment-gaussien
cÂ© Revue MODULAD, 2009 -105- NumeÂ´ro 40
Fig. 1 â€“ Illustration des hypothe`ses menant au Graphe GeÂ´neÂ´ratif Gaussien.
Hypothe`se 1 : la connexiteÂ´ des varieÂ´teÂ´s principales Mp est inconnue (1a) ; on suppose
quâ€™il existe un graphe de meË†me connexiteÂ´ (1b). Hypothe`se 2 : la densiteÂ´ pf sur la varieÂ´teÂ´
principale est inconnue ; on suppose que la densiteÂ´ pf est uniforme sur la reÂ´alisation
geÂ´omeÂ´trique des liens du graphe et quâ€™elle est deÂ´finie par un Dirac sur ses sommets.
Hypothe`se 3 : le bruit  est inconnu (3a) ; il est modeÂ´liseÂ´ par une densiteÂ´ gaussienne
isovarieÂ´e (3b).
[wanwbn ] de longueur non-nulle Ln et de variance Ïƒ
2 est :
g1(xi|{wan , wbn};Ïƒ2) =
1
Ln
âˆ« wbn
wan
g0(xi|t;Ïƒ2)dt
=
1
(2piÏƒ2)
D
2 Ln
âˆ« wbn
wan
exp
(
âˆ’(xiâˆ’t)
2
2Ïƒ2
)
dt
(2)
ou` Ln=â€–wbnâˆ’wanâ€–.
Les fonctions g0 et g1 sont positives et on peut prouver que leur inteÂ´grale sur RD est
eÂ´gale a` un, de telle sorte quâ€™elles deÂ´finissent toutes deux des densiteÂ´s de probabiliteÂ´. Des
exemples de densiteÂ´s aossiceÂ´es a` un point-gaussien et un segment-gaussien sont illustreÂ´es
par la figure 2.
Le calcul de la densiteÂ´ geÂ´neÂ´reÂ´e par un segment-gaussien peut eË†tre deÂ´composeÂ´e en deux
parties : une partie qui correspond au bruit gaussien orthogonal au segment passant par
le lien et lâ€™autre correspond au bruit gaussien inteÂ´greÂ´ le long du lien. Pour cela, on deÂ´finit
qin âˆˆ RD, la projection orthogonale de la donneÂ´e xi sur la droite passant par les sommets
wan et wbn :
qin=wan+(wbnâˆ’wan)
Qin
Ln
(3)
avec Qin =
(xiâˆ’wan )T (wbnâˆ’wan )
Ln
. Le scalaire Qin est la coordonneÂ´e du point qin sur un axe
dont lâ€™origine est wan et de vecteur unitaire :
wbnâˆ’wan
||wbnâˆ’wan || .
cÂ© Revue MODULAD, 2009 -106- NumeÂ´ro 40
(a) (b)
Fig. 2 â€“Du point gaussien au segment gaussien : (a) DensiteÂ´ geÂ´neÂ´reÂ´e par un segment
gaussien unidimensionnel de variance Ïƒ2 = 0.01 deÂ´fini sur [0; `], avec ` = 0 (bleu), ` = 0.5
(rouge), ` = 1 (vert), ` = 2 (violet). Lorsque la longueur du segment est nulle (` = 0) la
densiteÂ´ dâ€™un segment-gaussien eÂ´quivaut a` celle dâ€™un point-gaussien. (b) RepreÂ´sentation de
la densiteÂ´ geÂ´neÂ´reÂ´e par deux segments-gaussiens et un point-gaussien de variance Ïƒ2 = 0.01
dans R2.
En utilisant cette projection et le theÂ´ore`me de Pythagore on obtient :
g1(xi|{wan , wbn};Ïƒ2) = 1
(2piÏƒ2)
D
2 Ln
âˆ« wbn
wan
exp
(
âˆ’(xiâˆ’qin)
2 + (qinâˆ’t)2
2Ïƒ2
)
dt
= g
0(xi|qin;Ïƒ2)
Ln
âˆ« Ln
0
exp
(
âˆ’(Qinâˆ’t)
2
2Ïƒ2
)
dt
= g0(xi|qin;Ïƒ2)
âˆš
pi
2
Ïƒ
Ln
Â·
[
erf
(
Qin
Ïƒ
âˆš
2
)
âˆ’ erf
(
Qinâˆ’Ln
Ïƒ
âˆš
2
)] (4)
Ainsi, la densiteÂ´ g1(x) sâ€™exprime analytiquement a` lâ€™aide de la fonction erf , puisque
cette dernie`re admet une expansion en seÂ´rie de Taylor. La deÂ´composition (4) met en
eÂ´vidence une facÂ¸on de geÂ´neÂ´rer des donneÂ´es suivant la densiteÂ´ dâ€™un segment-gaussien
[wan , wbn ] (2).
â€“ On tire un point q âˆˆ RD sur le segment [wan , wbn ] suivant une loi uniforme : p(q) =
1
Ln
;
â€“ On tire une donneÂ´e x suivant une distribution gaussienne centreÂ´e sur q et de variance
Ïƒ2.
3.3 Description du mode`le de meÂ´lange
Etant donneÂ´ un graphe G(w,E), chaque sommet du graphe et chaque lien du graphe
sont alors la base dâ€™un mode`le geÂ´neÂ´ratif. Un point-gaussien est associeÂ´ a` chaque sommet
et un segment gaussien est associeÂ´ a` chaque lien du graphe G(w,E). Le mode`le est donc
baseÂ´ sur les composants eÂ´leÂ´mentaires du graphes, ses sommets et ses liens, qui ont chacun
une dimension intrinse`que d diffeÂ´rente, valant 0 pour les sommets et 1 pour les liens. Par
la suite, on note N0 le nombre de sommets et N1 le nombre de liens du graphe.
Soit z = {zdn âˆˆ {0, 1}|n = 1, ..., Nd; d = 0, 1} la donneÂ´e manquante qui indique quel
composant du mode`le a geÂ´neÂ´reÂ´ la donneÂ´e observeÂ´e x : zdn vaut 1 si le n
e composant de
dimension d a geÂ´neÂ´reÂ´ la donneÂ´e x et vaut 0 sinon. On deÂ´finit la densiteÂ´ de probabiliteÂ´
cÂ© Revue MODULAD, 2009 -107- NumeÂ´ro 40
associeÂ´e a` ces donneÂ´es manquantes comme eÂ´tant :
p(z) =
1âˆ
d=0
Ndâˆ
n=1
(pidn)
zdn (5)
pi0n (resp. pi
1
n) est la probabiliteÂ´ quâ€™une donneÂ´e observeÂ´e x soit issue du point-gaussien
associeÂ´ au sommet wn (resp. un segment-gaussien associeÂ´ au n
e lien du graphe).
Enfin si la donneÂ´e manquante z est connue, on tire la donneÂ´e observeÂ´e x suivant la loi
de ce composant :
p(x|z) =
{
g0n(x;Ïƒ
2) si z0n = 1
g1n(x;Ïƒ
2) si z1n = 1
(6)
Si lâ€™on cherche a` ajuster ce mode`le, comme on ne dispose que des donneÂ´es observeÂ´es, les
valeurs de la variable z eÂ´tant manquantes, lâ€™estimation des parame`tres du mode`le devra
se faire a` partir de la densiteÂ´ p(x). En utilisant les eÂ´quations (5) et (6), on peut deÂ´finir la
densiteÂ´ jointe du mode`le p(x, z) = p(z)p(x|z) puis marginaliser par rapport a` toutes les
valeurs des donneÂ´es manquantes afin dâ€™exprimer la densiteÂ´ p(x).
p(x; Î¸|G(w,E)) =
1âˆ‘
d=0
Ndâˆ‘
n=1
p(x, zdn; Î¸)
=
1âˆ‘
d=0
Ndâˆ‘
n=1
p(zdn)p(x|zdn; Î¸)
=
1âˆ‘
d=0
Ndâˆ‘
n=1
pidng
d
n(x;Ïƒ
2)
(7)
ou` Î¸ deÂ´note lâ€™ensemble des parame`tres du mode`le.
La densiteÂ´ des donneÂ´es p(x) sâ€™exprime donc comme un mode`le de meÂ´lange qui est
deÂ´finie comme la somme pondeÂ´reÂ´e de N0 points-gaussiens et N1 segments-gaussiens. Ainsi,
les proportions pi veÂ´rifient naturellement les deux contraintes suivantes :
1âˆ‘
d=0
Ndâˆ‘
n=1
pidn = 1 et 0 â‰¤ pidn â‰¤ 1 âˆ€ n, d (8)
On appelle ce mode`le le Graphe GeÂ´neÂ´ratif Gaussien (GGG).
Cette interpreÂ´tation du mode`le de meÂ´lange consiste a` consideÂ´rer que connaissant la
position des sommets w, le graphe G(w,E), les proportions pi et la variance du bruit Ïƒ2,
les donneÂ´es observeÂ´es sont geÂ´neÂ´reÂ´es suivant un meÂ´canisme a` deux eÂ´tapes.
1. On tire un composant du meÂ´lange (un sommet ou un lien) suivant la distribution
donneÂ´e par lâ€™eÂ´quation (5).
2. On tire une donneÂ´e observeÂ´e x suivant la loi du composant (6). Comme on lâ€™a vu,
des donneÂ´es issues dâ€™un segment gaussien peuvent eË†tre geÂ´neÂ´reÂ´e a` lâ€™aide dâ€™une donneÂ´e
manquante q uniformeÂ´ment distribueÂ´e le long du segment.
4 Caracteriser la connexiteÂ´
Ayant introduit un mode`le geÂ´neÂ´ratif, la question centrale demeure : comment deÂ´terminer
le graphe G final modeÂ´lisant la connexiteÂ´ des varieÂ´teÂ´s principales ? Pour cela, nous pro-
posons lâ€™algorithme 1 dont lâ€™ideÂ´e principale est double :
cÂ© Revue MODULAD, 2009 -108- NumeÂ´ro 40
â€“ deÂ´finir un mode`le GGG sur G(w,E+) tel que G(w,E) âŠ† G(w,E+) ;
â€“ eÂ´laguer les liens de G(w,E+) qui nâ€™explique pas la connexiteÂ´ des varieÂ´teÂ´s principales
pour en deÂ´duire G(w,E).
Algorithme 1 (Principe)
EntreÂ´es : x, N0
Initialiser la position des sommets : w
Intialiser le graphe geÂ´neÂ´ratif : construire un sur-graphe G(w,E+) et fixer les pa-
rame`tres pi et Ïƒ2.
Apprendre les parame`tres du graphe geÂ´neÂ´ratif : pi, Ïƒ2 et w
Elaguer le graphe geÂ´neÂ´ratif : supprimer les composants associeÂ´s a` une pondeÂ´ration
neÂ´gligeable, pidn â‰¤ Î³, ou` Î³ âˆˆ R+ est le seuil dâ€™eÂ´lagage.
Sortie : G(w,E).
Dans cet algorithme, on peut diffeÂ´rencier 3 proble`mes qui seront traiteÂ´es dans les
paragraphes suivants : (1) lâ€™initialisation (Comment positionner les sommets ? Quel sur-
graphe choisir ?), (2) lâ€™apprentissage des parame`tres du mode`le GGG et (3) la seÂ´lection de
mode`le (Combien de sommet ? Comment choisir le seuil dâ€™eÂ´lagage ?).
4.1 Initialisation
Nous proposons la meÂ´thode suivante pour deÂ´buter lâ€™algorithme avec un bon graphe.
â€“ Nous utilisons un mode`le de meÂ´lange gaussien spheÂ´rique dont la variance est com-
mune a` chaque composant pour positionner les sommets w.
â€“ Nous initialisons la variance Ïƒ2 du bruit gaussien a` la valeur obtenue par le mode`le
de meÂ´lange.
â€“ Nous initialisons les proportions de facÂ¸on eÂ´quiprobable : pidn =
1
N0+N1
âˆ€ n , d.
Ayant localiser les sommets w, il nous faut enfin choisir le graphe G(w,E+). On
peut eÂ´videmment consideÂ´rer le cas du graphe complet des sommets w, car il est simple
a` construire et il est le plus a` meË†me de contenir la connexiteÂ´ des varieÂ´teÂ´s principales.
Cependant, lâ€™eÂ´tat de lâ€™art nous incite a` envisager une autre alternative. Le graphe de
Delaunay, bien que plus long a` construire O(N30 ) [12], semble eË†tre un choix pertinent
puisquâ€™il est composeÂ´ de moins de liens que le graphe complet sans pour autant supprimer
des liens carcteÂ´risant la connexiteÂ´ [8]. Ainsi, on peut consideÂ´rer que le graphe G rechercheÂ´
pour extraire la connexiteÂ´ veÂ´rifie : G(w,E) âŠ† G(w,E+) â‰¡ GD(w).
4.2 Maximisation de la vraisemblance
Etant donneÂ´ un Graphe GeÂ´neÂ´ratif Gaussien (GGG) construit sur G(w,E), la fonction
p(xi; pi,w, Ïƒ) est la densiteÂ´ de probabiliteÂ´ au point xi sachant les parame`tres du mode`le.
Afin de maximiser la vraisemblance par rapport aux parame`tres Î¸ = (pi, Ïƒ2, w) , nous
utilisons le cadre de lâ€™algorithme EM [13]. Si lâ€™on peut deÂ´montrer que lâ€™eÂ´tape de maxi-
misation effectueÂ´e lors de lâ€™algorithme EM est analytique pour les proportions pi et la
variance du bruit Ïƒ2, celle impliquant les sommets w nâ€™est pas directe. Nous proposons
donc une eÂ´tape M approcheÂ´e pour modifier leur position, et on observe empiriquement
que la re`gle de mise a` jour approcheÂ´e augmente la plupart du temps la vraisemblance. Si
cÂ© Revue MODULAD, 2009 -109- NumeÂ´ro 40
ce nâ€™est pas le cas, la mise a` jour nâ€™est pas effectueÂ´e et la position du sommet nâ€™est pas
modifieÂ´e. Les eÂ´quations de mise a` jour des parame`tres sont les suivantes :
pi
d[new]
j =
1
M
âˆ‘M
i=1 zËœ
d
ij [Etape M exacte]
Ïƒ2[new] = 1
DM
âˆ‘M
i=1 [
âˆ‘N0
j=1 zËœ
0
ij(xj âˆ’ wi)2
+
âˆ‘N1
j=1 zËœ
1
ij
g0(xi|qij ;Ïƒ)(I1[(xiâˆ’qij)2+Ïƒ2]+I2)
Lj Â·g1j (xi,Ïƒ)
] [Etape M exacte]
w
[new]
n =
PM
i=1 [zËœ0inxi+
P
jâˆˆEn zËœ
1
ij
g0(xi|qij ;Ïƒ)
Lj Â·g1j (xi;Ïƒ)
(âˆ’E2wbj+E3xi)]
PM
i=1 [zËœ0in+
P
jâˆˆEn zËœ
1
ijE1]
[Etape M approcheÂ´e]
(9)
ou` zËœdij = p(d, j|xi) =
pidj g
d
j (xi;Ïƒ
2)
P1
d=0
PNd
j=1 pi
d
j g
d
j (xi;Ïƒ
2)
, ou` En repreÂ´sente lâ€™ensemble des arcs [waj , wbj ]
ayant wn = waj comme extreÂ´miteÂ´, et ou`
I1 = Ïƒ
âˆš
pi
2
(erf(
Qij
Ïƒ
âˆš
2
)âˆ’ erf(Qijâˆ’Lj
Ïƒ
âˆš
2
))
I2 = Ïƒ
2
(
(Qijâˆ’Lj) exp(âˆ’
(Qijâˆ’Lj)2
2Ïƒ2
)âˆ’Qij exp(âˆ’
(Qij)
2
2Ïƒ2
)
)
E1 =
Ïƒ2
L2j
[e
âˆ’(Qj)2
2Ïƒ2 (Qj âˆ’ 2Lj)âˆ’ e
âˆ’(Qjâˆ’Lj)2
2Ïƒ2 (Qj âˆ’ Lj)] + 1L2j ((Lj âˆ’Qj)
2 + Ïƒ)I1
E2 =
Ïƒ2
L2j
[eâˆ’
(Qjâˆ’Lj)2
2Ïƒ2 Qj âˆ’ eâˆ’
(Qj)
2
2Ïƒ2 (Qj âˆ’ Lj)]âˆ’ 1L2j (Q
2
j âˆ’ LjQj + Ïƒ2)I1
E3 =
1
Lj
[e
âˆ’(Qjâˆ’Lj)2
Ïƒ2 âˆ’ e
âˆ’Q2j
Ïƒ2 + (Qj âˆ’ Lj)I1]
(10)
4.3 SeÂ´lection de mode`le
En apprentissage statistique, seÂ´lectionner un mode`le parcimonieux parmi une collec-
tion de mode`le est un the`me reÂ´current. En particulier, il est connu quâ€™un mode`le geÂ´neÂ´ratif
ne doit pas eË†tre uniquement eÂ´valueÂ´ en fonction de sa vraisemblance mais aussi en terme
de complexiteÂ´. Dans notre cas, il est clair que les parame`tres N0 and Î³ sont lieÂ´s a` la
complexiteÂ´ du graphe geÂ´neÂ´ratif, de telle sorte que nous avons a` faire face a` un proble`me
de seÂ´lection de mode`le. Dans ce contexte, de nombreux crite`res et approches ont eÂ´teÂ´ pro-
poseÂ´s, comme par exemple le crite`re BIC [14]. Ce crite`re reÂ´alise un compromis entre la
vraisemblance et la complexiteÂ´ dâ€™un mode`le et retient le mode`le M qui maximise :
BIC(M) = L(x|Î¸Ë†)âˆ’ Î½
2
log(M) (11)
ou` M est le nombre total de donneÂ´es observeÂ´es, Î½ est le nombre de parame`tres libres du
mode`les, L la log-vraisemblance des parame`tres Î¸ qui sont a` leur maximum de vraisem-
blance Î¸Ë†.
Nous proposons de diviser le proble`me de seÂ´lection de mode`le en deux sous-proble`mes.
Le premier, est la deÂ´termination du seuil deÂ´lagage Î³ lorsque N0 est connu et le second est
la seÂ´lection du nombre de sommets N0.
Soit un graphe geÂ´neÂ´ratif gaussien a` N0 sommets construit sur G
+(w,E+) et soit
GÎ³(w,E
+|pidj â‰¥ Î³) le graphe geÂ´neÂ´ratif gaussien qui ne contient que les composants geÂ´neÂ´ratifs
dont la pondeÂ´ration est supeÂ´rieure a` Î³ : pidj â‰¥ Î³. En faisant varier le parame`tre Î³ de 1
cÂ© Revue MODULAD, 2009 -110- NumeÂ´ro 40
a` 0, on obtient une seÂ´quence emboË†Ä±teÂ´e de graphes4 allant de lâ€™ensemble vide au graphe
G(w,E+) :
G1 = âˆ… âŠ† . . . âŠ† GÎ³ âŠ† . . . âŠ† G0 = G(w,E+) (12)
Pour comparer les mode`les de la seÂ´quence a` lâ€™aide du crite`re BIC, les parame`tres
Î¸Î³ doivent eË†tre a` leur maximum de vraisemblance. Ceci nâ€™est eÂ´videmment pas le cas,
puisque ayant eÂ´lagueÂ´ le graphe initial G(w,E+), tous les mode`les ne veÂ´rifient pas :âˆ‘1
d=0
âˆ‘N
n=1 pi
d
n = 1. Il faut donc pour chaque GÎ³ reÂ´-estimer les parame`tres Î¸Î³. Pour des
raisons de complexiteÂ´, nous proposons que seules les proportions pi soient optimiseÂ´es via
lâ€™eÂ´quation . Dans ce cas, la fonction de vraisemblance est convexe et lâ€™algorithme converge
rapidement. Ceci est aussi motiveÂ´ par le fait que la variance du mode`le seÂ´lectionneÂ´ ne de-
vrait pas eË†tre tre`s diffeÂ´rente de lâ€™estimation obtenue par le mode`le GGG construit sur
G(w,E+).
Parmi la seÂ´quence emboË†Ä±teÂ´e, nous supposons donc que la connexiteÂ´ des varieÂ´teÂ´s prin-
cipales est repreÂ´senteÂ´e par le meilleur mode`le au sens du crite`re BIC.
BIC(GÎ³) = log L(Î¸Ë†Î³;GÎ³, x)âˆ’ Î½Î³
2
log(M) (13)
ou` Î½Î³ est le nombre de parame`tres libres du mode`le geÂ´neÂ´ratif associeÂ´ a` GÎ³. Le nombre de
parame`tres libres pour un graphe geÂ´neÂ´ratif gaussien ayant N0 sommets et N1 liens est :
Î½ = [N0 +N1 âˆ’ 1] + [N0 Ã—D] + 1
= (N0 +N1) +N0 Ã—D (14)
Le premier terme correspond aux proportions5 pi, le second correspond aux coordonneÂ´es
des N0 sommets wn âˆˆ RD et la dernie`re constante correspond a` la variance Ïƒ2 âˆˆ R+.
Pour seÂ´lectionner le nombre de sommets N0, lâ€™on peut reÂ´peÂ´ter cette proceÂ´dure pour
diffeÂ´rentes valeurs de N0 âˆˆ {N [1]0 , ..., N [k]0 , ..., N [K]0 } :
G
(Î³âˆ—,N [k]0 )
â‰¡ max
Î³
BIC(G
(Î³,N
[k]
0 )
) âˆ€ N [k]0 (15)
On obtient K mode`les, chacun associeÂ´ a` une valeur de crite`re BIC(G
(Î³âˆ—,N [k]0 )
). Finale-
ment, on choisit celui dont le nombre de sommets N
[âˆ—]
0 me`ne a` la plus grande valeur :
G
(Î³âˆ—,N [âˆ—]0 )
â‰¡ max
k
BIC(G
(Î³âˆ—,N [k]0 )
) = max
k
max
Î³
BIC(G
(Î³,N
[k]
0 )
) (16)
4.4 Illustration
Les figures 3 et 4 illustrent lâ€™algorithme proposeÂ´ avec un ensemble de donneÂ´es simuleÂ´es :
50 points sont gnÂ´eÂ´reÂ´es par un point de coordoneÂ´es [0.5, 1.5] et 200 points sont geÂ´neÂ´reÂ´es
par une spirale dâ€™eÂ´quation [t cos(2pit), t sin(2pit)] (t âˆˆ [0; 1.1]), tous les deux corrompus
par un bruit gaussien de variance Ïƒ2 = 0.01.
Pour un nombre fixe de sommets N0 = 7 (figure 3) : (a) Les sommets sont localiseÂ´s
a` lâ€™aide dâ€™un mode`le de meÂ´lange. (b) Le graphe de Delaunay est construit et un graphe
4Il peut sâ€™agir dâ€™un objet geÂ´omeÂ´trique pouvant contenir un lien sans ses sommets. Par abus, nous
dirons que GÎ³ est un graphe.
5La somme des pondeÂ´rations eÂ´tant contrainte a` valoir 1, seules N0 + N1 âˆ’ 1 pondeÂ´rations sont
indeÂ´pendantes.
cÂ© Revue MODULAD, 2009 -111- NumeÂ´ro 40
(a) (b) (c)
(d)BIC = âˆ’520.9 (e) BIC = âˆ’412.2 (f) BIC = âˆ’415.4
Fig. 3 â€“ Illustration de lâ€™algorithme pour un nombre de sommets fixe N0 = 7.
(a) BIC = âˆ’436.3 (b) BIC = âˆ’412.2 (c) BIC = âˆ’414.4
Fig. 4 â€“ Illustration de lâ€™algorithme pour N0 = 6, 7, 8.
geÂ´neÂ´ratif est associeÂ´ a` sa reÂ´alisation geÂ´omeÂ´trique. (c) La vraisemblance des parame`tres est
maximiseÂ´e via un algorithme EM approcheÂ´. Lâ€™objectif de lâ€™eÂ´tape dâ€™eÂ´lagage est de supprimer
automatiquement les composants du mode`le geÂ´neÂ´ratif inutiles et en particulier, les arcs du
graphe traversant un trou de densiteÂ´. A cette fin, on construit une seÂ´quence de mode`les
geÂ´neÂ´ratifs emboiteÂ´s en fonction dâ€™un seuil Î³. Les figures (d-f) montrent trois mode`les
conseÂ´cutifs avec en rouge le composant ajouteÂ´ par rapport au mode`le preÂ´ceÂ´dant. Notons
que le dernier mode`le de cette seÂ´quence (G+) est celui de la figure (c). La vraisemblance
de chaque mode`le de la seÂ´quance est a` nouveau optimiseÂ´e par rapport aux proportions
et nous indiquons en dessous de chaque figure la valeur du crite`re BIC correspondant : le
meilleur mode`le au sens du crite`re BIC est celui encadreÂ´.
Pour diffeÂ´rents nombres de sommets N0 = 6, 7, 8, nous reÂ´peÂ´tons lâ€™algorithme ci-dessus
et les mode`les correspondants sont repreÂ´senteÂ´s dans la figure 4(a-c). Le meilleur mode`le
au sens du crite`re BIC est celui encadreÂ´. Notons tout de meË†me que la conneÂ´xiteÂ´ des deux
variteÂ´teÂ´s principales est aussi correctement modeÂ´liseÂ´e pour N0 = 6, 8.
5 ConnexiteÂ´ des donneÂ´es Teapot
5.1 DonneÂ´es
Lâ€™ensemble de donneÂ´es Teapot est constitueÂ´ de 400 images, chacune de taille 101Ã— 76.
Les images, qui correspondent aux donneÂ´es observeÂ´es, repreÂ´sentent une theÂ´ie`re photogra-
phieÂ´e sous diffeÂ´rents angles dâ€™un plan [15]. Dix images de cet ensemble sont repreÂ´senteÂ´es
cÂ© Revue MODULAD, 2009 -112- NumeÂ´ro 40
Fig. 5 â€“ Teapot. Dix images de lâ€™ensemble Teapot original [15]. Dans lâ€™ensemble utiliseÂ´
[16], quelques images ont eÂ´teÂ´ supprimeÂ´es. Elles correspondent a` celles ou` lâ€™anse est face
a` la cameÂ´ra (images barreÂ´es). La couleur des autres cadres code lâ€™angle de rotation de la
theÂ´ie`re.
par la figure 5.
MalgreÂ´ la grande dimensionnaliteÂ´ des images (D = 7676), le protocole mis en oeuvre
pour les geÂ´neÂ´rer, laisse penser que dans lâ€™espace dâ€™observation, les donneÂ´es sont proches
dâ€™une varieÂ´teÂ´ principale ayant la topologie dâ€™un cercle. En effet, les images ne sont pa-
rameÂ´triseÂ´es que par un seul degreÂ´ de liberteÂ´, lâ€™angle de rotation de la theÂ´ie`re.
Dans cette expeÂ´rience, nous utilisons lâ€™ensemble de donneÂ´es deÂ´crit dans [16]. Les images
ont eÂ´teÂ´ converties en niveau de gris et leur taille a eÂ´teÂ´ reÂ´duite. De plus, ce nouvel ensemble
eÂ´tant utiliseÂ´ pour un proble`me de discrimination ou` lâ€™objectif est dâ€™identifier si lâ€™anse de
la theÂ´ie`re est a` gauche ou a` droite de lâ€™image, les auteurs ont retireÂ´ les quelques images
ou` celle-ci est a` peu pre`s au centre. A la fin, ils disposent de 365 images (M = 365) de
taille 16Ã—12 (D = 192). De la sorte, les auteurs creÂ´ent artificiellement deux varieÂ´teÂ´s prin-
cipales deÂ´connecteÂ´es, chacune ayant la topologie dâ€™un demi-cercle : lâ€™une correspondant
aux positions ou` la theÂ´ie`re a lâ€™anse a` droite de lâ€™image et lâ€™autre aux positions ou` lâ€™anse
est a` gauche.
5.2 Visualisation
Afin dâ€™analyser la structure sous-jacente aux donneÂ´es, les techniques de reÂ´duction de
dimension sont largement utiliseÂ´es. Cependant, du fait de la perte dâ€™information quâ€™elles
engendrent, la plupart des distances visualiseÂ´es sont soit comprimeÂ´es soit eÂ´tireÂ´es, et il
est donc difficile de savoir si les formes observeÂ´es existent ou non dans lâ€™espace am-
bient [17]. Dans les expeÂ´riences suivantes, nous montrons que le GGG est une meÂ´thode
compleÂ´mentaire aux techniques de projection â€classiquesâ€ pour analyser un ensemble de
donneÂ´es.
Nous utilisons lâ€™Analyse en Composantes Principales (ACP), le Generative Topogra-
cÂ© Revue MODULAD, 2009 -113- NumeÂ´ro 40
(a) ACP (b) GTM (c) ISOMAP [K = 15]
Fig. 6 â€“ Projections de lâ€™ensemble de donneÂ´es Teapot. La couleur des donneÂ´es
projeteÂ´es correspond aux couleurs de la figure 5 : elle code lâ€™angle de rotation de la
theÂ´ie`re. (a) Projection des donneÂ´es en utilisant les deux premiers axes principaux. (b)
Projection en utilisant le GTM (c) Projection en utilisant lâ€™algorithme ISOMAP a` lâ€™aide
dâ€™un graphe des 15. Aucune de ces projections ne permet de montrer lâ€™existence de deux
varieÂ´teÂ´s principales deÂ´connecteÂ´es.
phic Mapping [18] (GTM) et ISOMAP [19] pour visualiser les donneÂ´es Teapot (figure 6) :
aucune de ces projections nâ€™est en mesure de montrer la structure deÂ´connecteÂ´e originelle.
5.3 Extraction de la connexiteÂ´
Nous optimisons le GGG avec lâ€™algorithme deÂ´crit en section 3 avec un N0 candidat
compris entre 40 et 80, afin de retrouver la connexiteÂ´ des varieÂ´teÂ´s principales de cet
ensemble. Le graphe geÂ´neÂ´ratif optimal est finalement deÂ´fini par 67 prototypes.
Le graphe reÂ´sultant nous informe sur lâ€™existence de 2 composantes connexes en deÂ´pit
de ce que montrent les techniques de projection classiques (figure 6). Lâ€™analyse des degreÂ´s
des sommets6 du graphe (les degreÂ´s valent 2, sauf pour les 4 sommets extreÂ´miteÂ´s des
deux composantes connexes, dont le degreÂ´ vaut 1) montre que chaque composante est
une chaË†Ä±ne de sommets, donc une varieÂ´teÂ´ homeÂ´omorphe a` un segment, montrant que la
dimension intrinse`que de ces deux varieÂ´teÂ´s vaut 1. De plus, le mode`le eÂ´tant geÂ´neÂ´ratif nous
savons aussi que les deux varieÂ´teÂ´s ont a` peu pre`s la meË†me probabiliteÂ´ a priori : 0.507 et
0.493, et que le long des varieÂ´teÂ´s, les donneÂ´es sont a` peu pre`s uniformeÂ´ment distribueÂ´es,
puisque la moyenne et la variance de la quantiteÂ´
pi1j
Lj
sont respectivement : 4.5966e âˆ’ 005
et 1.5720eâˆ’ 010.
Nous construisons le CHL et le CHL filtreÂ´ a` partir des meË†mes sommets. La figure 8
(a) montre que le CHL ne permet pas de retrouver la connexiteÂ´ des varieÂ´teÂ´s principales.
En effet, le graphe construit nâ€™a quâ€™une seule composante connexe. De plus, notons quâ€™il
existe des cycles : le graphe nâ€™est donc pas homeÂ´omorphe aux varieÂ´teÂ´s principales ayant
geÂ´neÂ´reÂ´ les donneÂ´es observeÂ´es. Pour le CHL filtreÂ´, on constate avec la figure 8 (b) quâ€™aucun
seuil T nâ€™est convenable pour obtenir un graphe ayant la meË†me connexiteÂ´ que les varieÂ´teÂ´s
principales.
Nous reÂ´peÂ´tons cette expeÂ´rience 10 fois afin dâ€™eÂ´valuer la robustesse de lâ€™algorithme.
Le tableau 1 preÂ´sente les reÂ´sultats pour les diffeÂ´rents algorithmes. On remarque que le
mode`le GGG permet, a` lâ€™exception dâ€™une fois, de retrouver la topologie attendue. Ceci
peut sâ€™expliquer par trois eÂ´leÂ´ments : (1) en utilisant les meË†mes donneÂ´es, le reÂ´sultat de
6Dans un graphe, le degreÂ´ dâ€™un sommet est le nombre dâ€™arcs qui ont ce sommet comme extreÂ´miteÂ´
cÂ© Revue MODULAD, 2009 -114- NumeÂ´ro 40
Fig. 7 â€“CaracteÂ´risation de la connexiteÂ´ de lâ€™ensemble de donneÂ´es Teapot avec le
GGG. Projection par ACP du graphe GGG optimal. La couleur des sommets correspond
a` la couleur de la donneÂ´e la plus proche suivant le code deÂ´fini par la figure 5. Les extreÂ´miteÂ´s
des deux composantes connexes deÂ´finies par le mode`le GGG optimal sont encercleÂ´es.
(a) CHL [T = 0] (b) CHL [T = 3]
Fig. 8 â€“ CaracteÂ´risation de la connexiteÂ´ de lâ€™ensemble de donneÂ´es Teapot avec
le CHL. Projection par ACP du graphe obtenu par lâ€™algorithme CHL (a) et par sa version
filtreÂ´e (b). La couleur des sommets correspond a` la couleur de la donneÂ´e la plus proche
suivant le code deÂ´fini par la figure 5. (b) Le seuil T = 3 est la premie`re valeur deÂ´connectant
les deux varieÂ´teÂ´s principales, celle eÂ´laguant le lien violet repreÂ´senteÂ´ dans lâ€™agrandissement
de gauche de la figure (b). En utilisant un tel seuil, lâ€™une des deux varieÂ´teÂ´s se trouve eË†tre
morceleÂ´e : la connexiteÂ´ est donc perdue.
cÂ© Revue MODULAD, 2009 -115- NumeÂ´ro 40
GGG CHL [T = 0] CHL [T âˆ—]
ConnexiteÂ´ 100 70 70
Topologie 90 20 20
Tab. 1 â€“ ModeÂ´lisation de la connexiteÂ´ des varieÂ´teÂ´s des donneÂ´es Teapot. Le
tableau donne en pourcentage, le nombre de fois ou` les mode`les GGG, CHL et CHL filtreÂ´
permettent de retrouver la connexiteÂ´ (2 composantes connexes) et la topologie (deux
chaË†Ä±nes de sommets ayant au plus deux voisins) des donneÂ´es Teapot.
(a) (b) (c)
Fig. 9 â€“ DonneÂ´es Teapot et distances. Trois images de lâ€™ensemble Teapot original.
Si la distance entre deux images est mesureÂ´e par la somme des diffeÂ´rences au carreÂ´ de
lâ€™intensiteÂ´ lumineuse entre les pixels de deux images, alors lâ€™image (a) est plus proche de
lâ€™image (c) que de lâ€™image (b). Pourtant, lâ€™angle de rotation seÂ´parant les positions deÂ´crites
par les images (a) et (b) est plus faible quâ€™entre le couple dâ€™images (a) et (c).
lâ€™algorithme deÂ´pend uniquement des conditions initiales. Or, lâ€™influence des conditions
initiales est minimiseÂ´e puisque nous utilisons la strateÂ´gie short EM proposeÂ´e dans [20]
pour deÂ´terminer la position initiale des sommets ; (2) les donneÂ´es respectent assez bien
les hypothe`ses geÂ´neÂ´ratives utiliseÂ´es par le mode`le GGG ; (3) lâ€™eÂ´chantillonnage est dense ce
qui favorise une estimation fiable du crite`re BIC.
Le CHL permet majoritairement de retrouver la connexiteÂ´, cependant le graphe obtenu
preÂ´sente geÂ´neÂ´ralement des cycles qui faussent la topologie. La version filtreÂ´e ne permet
pas dâ€™eÂ´viter ces cycles.
5.4 Naviguer sur la varieÂ´teÂ´
Le graphe geÂ´neÂ´ratif permet aussi de naviguer aiseÂ´ment au travers des donneÂ´es. Suppo-
sons que lâ€™on dispose de trois images repreÂ´senteÂ´es par la figure 9. On peut par exemple se
demander laquelle des images (b) ou (c) vient naturellement apre`s lâ€™image (a). On peut
aussi souhaiter savoir sâ€™il existe une seÂ´rie continue dâ€™images passant par ces trois images.
Pour reÂ´pondre a` la premie`re question, il faut deÂ´finir une distance adeÂ´quate entre les
cÂ© Revue MODULAD, 2009 -116- NumeÂ´ro 40
Fig. 10 â€“DeÂ´bruitrage des chiffres â€1â€â€™ et â€2â€. Premie`re et troisie`me ligne : les images
MNIST orginales. Deuxie`me et quatrie`me ligne, les images deÂ´bruiteÂ´es correspondantes.
images. Si la distance entre deux images est mesureÂ´e par la somme des diffeÂ´rences au carreÂ´
de lâ€™intensiteÂ´ lumineuse entre les pixels des deux images, alors lâ€™image (a) est plus proche
de lâ€™image (c) que de lâ€™image (b). Mais ceci est contradictoire avec notre perception.
En effet, lâ€™angle de rotation seÂ´parant les positions deÂ´crites par les images (a) et (b) est
plus faible quâ€™entre le couple dâ€™images (a) et (c). Il sâ€™agit donc de mesurer les distances
geÂ´odeÂ´siques le long des varieÂ´teÂ´s, ce qui peut eË†tre fait a` lâ€™aide du graphe apreÂ´s avoir projeteÂ´
les donneÂ´es sur le graphe. Pour reÂ´pondre a` la deuxie`me question, il suffit de savoir si les
donneÂ´es se projettent sur une meË†me composante du graphe. Dans lâ€™exemple illustreÂ´ par
la figure 9, la reÂ´ponse est neÂ´gative.
6 DeÂ´bruitage
Dans cette section, nous montrons que lâ€™extraction des variteÂ´teÂ´s peut aussi permettre
le deÂ´bruitage de donneÂ´es. Pour lâ€™illustrer, nous consideÂ´rons un ensemble dâ€™imagettes
repreÂ´setant les chiffres â€1â€ et â€2â€. On dispose dâ€™un ensemble dâ€™apprentissage de 1100
exemples (550 de chaque classe) et nous souhaitons classifier un ensemble de 1100 chiffres
qui forment lâ€™ensemble de test. Nous utilisons lâ€™algorithme de la section 2 pour apprendre
les varieÂ´teÂ´s principales de lâ€™ensemble dâ€™apprentissage de manie`re non-superviseÂ´e (sans te-
nir compte des classes). Le graphe est deÂ´fini par 50 sommets. Les donneÂ´es de lâ€™ensemble
dâ€™apprentissage et de test sont ensuite projeteÂ´es sur le graphe, et ces projections deÂ´finissent
les donneÂ´es â€deÂ´bruiteÂ´esâ€. La figure 10 montre quelques images originales et leur version
deÂ´bruiteÂ´e. Par exemple le â€1â€ original encadreÂ´ en vert a une â€queueâ€ qui disparaË†Ä±t avec
le deÂ´bruitage. De manie`re similaire, le deÂ´bruitage a tendance a` reformer la boucle du â€2â€
encadreÂ´ en rouge.
Ensuite, on classifie les deux ensembles de test (donneÂ´es originales et deÂ´bruiteÂ´es) a`
lâ€™aide du classifieur des K plus proches voisins (classement par vote majoritaire) par
cÂ© Revue MODULAD, 2009 -117- NumeÂ´ro 40
DonneÂ´es MNIST DonneÂ´es deÂ´bruiteÂ´es Prototypes
K = 3 98.8 99.0 98.6
K = 5 98.6 99.0 98.4
K = 10 98.0 99.1 93.4
K = 15 97.9 99.1 91.9
K = 30 96.8 99.0 83.7
Tab. 2 â€“ Classification de donneÂ´es MNIST â€1â€ et â€2â€. Le tableau donne le pour-
centage de taux de bonne classification en utilisant le classifieur des k plus proches voisins
pour les diffeÂ´rents ensembles.
rapport aux ensembles dâ€™apprentissage respectifs. Le tableau 2 donne le reÂ´sultat pour
diffeÂ´rentes valeurs de K. On compare aussi le reÂ´sultat par rapport a` lâ€™algorithme super-
viseÂ´ des k-means construit sur les donneÂ´es orginales : on repreÂ´sente chaque classe (de
lâ€™ensemble dâ€™apprentissage des donneÂ´es originales) par 25 prototypes et on classifie les
donneÂ´es orginales de lâ€™ensemble de test en utilisant ces prototypes (qui sont affecteÂ´s a` une
seule classe). Le tableau 2 montre quâ€™en utilisant les donneÂ´es deÂ´bruiteÂ´es, le classifieur est
moins sensible au parame`tre de voisinage.
7 Conclusion
Nous avons proposeÂ´ un cadre dans lequel le proble`me de lâ€™apprentissage de la topologie
dâ€™un nuage de points peut eË†tre poseÂ´ comme un proble`me dâ€™apprentissage statistique. Nous
avons deÂ´fini un mode`le geÂ´neÂ´ratif baseÂ´ sur le graphe de Delaunay, permettant dâ€™apprendre
la connexiteÂ´ des varieÂ´teÂ´s principales dâ€™un nuage de points. Le Graphe GeÂ´neÂ´ratif Gaussien
(GGG) permet de contourner les limites de lâ€™algorithme Competitive Hebbian Learning
(CHL) pour modeÂ´liser la connexiteÂ´. En particulier, il permet de prendre en compte le bruit,
et de mesurer la qualiteÂ´ du mode`le, meË†me lorsquâ€™aucune visualisation nâ€™est possible.
Nous avons montreÂ´ que le mode`le eÂ´tait utile pour lâ€™analyse exploratoire de donneÂ´es
en fournissant une vue des donneÂ´es compleÂ´mentaire des meÂ´thodes de visualisation par
projection. Nous avons aussi montreÂ´ que le graphe geÂ´neÂ´ratif, en modeÂ´lisant les varieÂ´teÂ´s
principales permet le deÂ´bruitage des donneÂ´es.
Nous eÂ´tudions deÂ´sormais lâ€™utilisation de ce mode`le comme support dâ€™un apprentissage
semi-superviseÂ´ ou` la structure des donneÂ´es non eÂ´tiqueteÂ´es joue un roË†le dans la construction
dâ€™un classifieur [6]. Nous montrons7 que la propagation des eÂ´tiquettes le long des arcs dâ€™un
GGG en tenant compte de la densiteÂ´ de ces arcs (propagation dâ€™autant plus forte que la
densiteÂ´ est forte) est aussi efficace que les autres approches de lâ€™eÂ´tat de lâ€™art geÂ´neÂ´ralement
baseÂ´es sur le graphe des K plus proches voisins, mais ne neÂ´cessite aucun reÂ´glage arbitraire
de meÂ´ta-parame`tres (K par exemple).
Dâ€™un point de vue plus geÂ´neÂ´ral, ce travail se veut une contribution au rapproche-
ment des domaines de lâ€™Apprentissage Statistique et de la Topologie Algorithmique, a` la
frontie`re desquels nous pensons quâ€™il ouvre de nombreuses perspectives.
7Soumission en cours
cÂ© Revue MODULAD, 2009 -118- NumeÂ´ro 40
ReÂ´feÂ´rences
[1] R. Tibshirani. Principal curves revisited. Statistics and Computing, 2 :183â€“190, 1992.
[2] G. McLachlan and D. Peel. Finite Mixture Models. John Wiley & Sons, New York,
2000.
[3] E. Parzen. On estimation of a probability density function and mode. The Annals
of Mathematical Statistics, 33(3) :1065â€“1076, 1962.
[4] P. Gaillard, M. Aupetit, and G. Govaert. Learning topology of a labeled data set
with the supervised generative gaussian graph. Neurocomputing (in press), 2008.
[5] J. Lee, A. Lendasse, N. Donckers, and M. Verleysen. A robust nonlinear projection
method. Eighth European Symposium on ArtiHcial Neural Networks, 2000.
[6] O. Chapelle, B. SchoÂ¨lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT
Press, Cambridge, MA, 2006.
[7] M. Aupetit, F. Chazal, G. Gasso, D. Cohen-Steiner, and P. Gaillard. Topology lear-
ning : New challenges at the crossing of machine learning, computational geometry
and topology, 2007.
[8] T. Martinetz and K. Schulten. Topology representing networks. Neural Networks,
Elsevier London, 7 :507â€“522, 1994.
[9] M. Aupetit. Robust topology representing networks. In Proceedings of the European
Symposium on Artificial Neural Networks, pages 45â€“50, Bruges (Belgium), 2003. d-
side.
[10] T. Martinetz, S. Berkovitch, and K. Schulten. Neural-gas network for vector quan-
tization and its application to time-series prediction. IEEE Transactions on Neural
Networks, 4(4) :558â€“569, 1993.
[11] B. Fritzke. A growing neural gas network learns topologies. In G. Tesauro, D. Tou-
retzky, and T. Leen, editors, Advances in Neural Information Processing Systems 7,
Cambridge, MA, 1995. MIT Press.
[12] E. Agrell. A method for examining vector quantizer structures. Proceedings of IEEE
International Symposium on Information Theory, pages 394â€“394, 1993.
[13] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1) :1â€“38,
1977.
[14] G. Schwartz. Estimating the dimension of a model. The Annals of Statistics, 6 :461â€“
464, 1978.
[15] K. Weinberger and L. Saul. Unsupervised learning of image manifolds by semidefinite
programming. International Journal of Computer Vision, 70(1) :77â€“90, 2006.
[16] X. Zhu and J. Lafferty. Harmonic mixtures : combining mixture models and graph-
based methods for inductive and scalable semi-supervised learning. In Proceedings
of the 22nd International Conference on Machine Learning, pages 1052â€“1059, New
York, USA, 2005. ACM.
[17] M. Aupetit. Visualizing distortions and recovering topology in continuous projection
techniques. Neurocomputing, Elsevier, 70 :1304â€“1330, 2007.
cÂ© Revue MODULAD, 2009 -119- NumeÂ´ro 40
[18] C. Bishop, M. SvenseÂ´n, and C. Williams. GTM : the generative topographic mapping.
Neural Computation, MIT Press, 10(1) :215â€“234, 1998.
[19] V. de Silva and J. Tenenbaum. Global versus local methods in nonlinear dimensio-
nality reduction. In S. Thrun S. Becker and K. Obermayer, editors, Advances in
Neural Information Processing Systems 15, pages 705â€“712. MIT Press, Cambridge,
MA, 2003.
[20] C. Biernacki, G. Celeux, and G. Govaert. Choosing starting values for the em al-
gorithm for getting the highest likelihood in multivariate gaussian mixture models.
Computational Statistics and Data Analysis, 41 :561â€“575, 2003.
cÂ© Revue MODULAD, 2009 -120- NumeÂ´ro 40
