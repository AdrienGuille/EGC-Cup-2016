Modèle à processus latent et algorithme EM pour la
régression non linéaire
Faicel Chamroukhi ∗,∗∗, Allou Samé∗, Gérard Govaert∗∗, Patrice Aknin∗
∗Institut National de Recherche sur les Transports et leur Sécurité
Laboratoire des Technologies Nouvelles
2 rue de la Butte Verte, 93166 Noisy-le-Grand Cedex
{chamroukhi, same, aknin}@inrets.fr
∗∗Université de Technologie de Compiègne
Laboratoire HEUDIASYC, UMR CNRS 6599
BP 20529, 60205 Compiègne Cedex
govaert@utc.fr
Résumé. Cet article propose une méthode de régression non linéaire qui s’ap-
puie sur un modèle intégrant un processus latent qui permet d’activer préféren-
tiellement, et de manière souple, des sous-modèles de régression polynomiaux.
Les paramètres du modèle sont estimés par la méthode du maximum de vrai-
semblance mise en œuvre par un algorithme EM dédié. Une étude expérimen-
tale menée sur des données simulées et des données réelles met en évidence de
bonnes performances de l’approche proposée.
1 Introduction
La régression non linéaire est un problème central dans de nombreux domaines qui concer-
nent la prédiction, le débruitage de signaux et leur paramétrisation. Son but est de caractériser
au mieux la relation (non linéaire) existant entre une variable dépendante (grandeur physique),
que nous supposerons scalaire dans cet article, et une variable indépendante qui, comme c’est
souvent le cas, est liée au temps. Cette relation non linéaire peut être due au fait que les don-
nées sont issues d’un modèle physique intrinsèquement non linéaire par rapport au temps, ou
que le processus de génération des données comporte différents régimes linéaires (voire poly-
nomiaux) qui se succèdent au cours du temps.
Plusieurs modèles ont déjà été proposés dans le cadre de l’apprentissage statistique pour
résoudre ce type de problème. Parmi ces approches, on peut citer les modèles polynomiaux par
morceaux (McGee et Carleton, 1970), (Bellman, 1961; Stone, 1961), (Samé et al., 2007), les
méthodes à base de B-splines (Deboor, 1978; Bishop, 2006), le perceptron multi-couche dans
sa version régressive (Bishop, 2006) et les méthodes à fonctions de base radiale (Bishop, 2006).
La plupart de ces méthodes ramènent généralement le problème de régression non linéaire à
des problèmes de régression linéaire simples à résoudre.
Dans ce travail, nous proposons une méthode alternative qui consiste à remplacer le mo-
dèle de régression non linéaire habituel par un modèle de régression intégrant un processus
Modèle à processus latent et algorithme EM pour la régression non linéaire
caché, qui permet d’activer préférentiellement et de manière souple un modèle de régression
polynomial parmiK modèles. L’utilisation d’une fonction logistique comme loi conditionnelle
des variables latentes assure une souplesse de transition (lente ou rapide) entre les différents
polynômes, ce qui permet d’obtenir une modélisation correcte des non linéarités. La loi condi-
tionnelle de la variable dépendante, sous ce modèle, constitue un mélange d’experts (Jacobs
et al., 1991), (Jordan et Jacobs, 1994) contraint, avec une variance commune pour toutes les
composantes du mélange. Une estimation des paramètres du modèle par l’algorithme EM est
ainsi proposée.
Cette méthode est exploitée dans le cadre d’une application de suivi d’état de fonction-
nement du mécanisme d’aiguillage des rails, où nous avons été amenés à paramétriser des
signaux non linéaires acquis durant des manœuvres d’aiguillage.
Dans la section 2 nous décrivons comment le modèle de régression à processus latent peut
être utilisé dans le cadre de la régression non linéaire. Ensuite nous présentons la méthode
d’estimation des paramètres via l’algorithme EM. La section 4 montre les performances de la
méthode proposée sur des données simulées et la section 5 est consacrée à son application sur
des données réelles issues du domaine ferroviaire.
2 Régression non linéaire et modèle à processus latent
2.1 Cadre général
On suppose disposer d’un échantillon ((x1, t1), . . . , (xn, tn)), où xi désigne la variable
aléatoire scalaire dépendante et ti la variable temporelle indépendante. Le problème de régres-
sion non linéaire, sous sa formulation générique (Antoniadis et al., 1992), consiste à estimer
une fonction f de paramètre θ, en considérant le modèle
M1 : xi = f(ti;θ) + εi, (1)
où les εi sont des bruits gaussiens centrés i.i.d de variance σ2. L’estimation s’effectue généra-
lement par la maximisation de la vraisemblance ou de manière équivalente par la minimisation
du critère des moindres-carrés donné par
C1(θ) =
n∑
i=1
(xi − f(ti;θ))
2. (2)
Dans le modèle M1, la fonction f(t;θ) représente l’espérance de x conditionnellement à t.
La minimisation du critère C1, sous certaines conditions de régularité sur f (Antoniadis et al.
(1992)), fournit un estimateur asymptotiquement gaussien, efficace et sans biais du paramètre
θ. En pratique, on a souvent recours à des algorithmes itératifs d’optimisation qui convergent
localement.
Pour couvrir un panel assez large de fonctions non linéaires de régression qui soient facile-
ment paramétrables, nous optons pour des fonctions qui peuvent s’écrire sous la forme d’une
somme finie de polynômes pondérés par des fonctions logistiques :
f(t;θ) =
K∑
k=1
pik(t;w)β
T
k t, (3)
Chamroukhi et al.
avec
pik(t;w) =
exp(wk0 + wk1t)∑K
`=1 exp(w`0 + w`1t)
, (4)
où le vecteur βk = (βk0, . . . , βkp)T de Rp+1 désigne l’ensemble des coefficients d’un po-
lynôme de degré p et t = (1, t, t2, . . . , tp)T son vecteur de monômes associés. Le vecteur
w = (w10, w11, . . . , wK0, wK1) de R2K désigne l’ensemble des paramètres de la fonction
logistique et θ = (w,β1, . . . ,βK) représente l’ensemble des paramètres de la fonction f .
Ce modèle particulier de régression peut s’interpréter comme étant un modèle formé de
différents sous-modèles de régression activés de manière souple ou brusque par des fonctions
logistiques. Cette méthode permet aussi de détecter les points de rupture d’un signal par le suivi
au cours du temps du processus latent, et de mesurer la vitesse d’évolution des proportions
du mélange (souple ou brusque) grâce à la flexibilité de la transformation logistique utilisée.
L’illustration de cette flexibilité se trouve dans (Chamroukhi et al., 2009).
Pour le modèleM1, défini par les équations (1), (3) et (4), le choix particulier de la fonction
f ne permet pas d’obtenir une solution analytique du problème de minimisation du critère C1.
Il faut s’appuyer sur une procédure numérique d’optimisation du type Gauss-Newton, Newton-
Raphson ou quasi-Newton qui converge localement.
2.2 Modèle de régression à processus latent proposé
Pour atteindre l’objectif d’estimation de la fonction de régression f du modèleM1, nous
proposons d’utiliser une méthode alternative s’appuyant sur un modèle génératif intégrant un
processus latent discret (z1, . . . , zn) avec zi ∈ {1, . . . ,K}. Ce modèle est défini par :
M2 : xi =
K∑
k=1
zikβ
T
k ti + εi ; εi ∼ N (0, σ
2), (5)
où zik vaut 1 si zi = k et vaut 0 sinon, et où les εi sont supposés être indépendants. Les va-
riables zi du processus latent, conditionnellement aux instants ti, sont supposées être générées
indépendamment suivant la loi multinomiale M(1, pi1(ti;w), . . . , piK(ti;w)), où pik(ti;w)
est défini par l’equation (4). À partir du modèleM2, on peut vérifier que conditionnellement
à un sous-modèle de régression k et au temps ti, xi est distribué suivant une loi normale de
moyenne βTk ti et de variance σ2
p(xi|zi = k, ti;Φ) = N (xi;β
T
k ti, σ
2), (6)
où Φ = (θ, σ2) et N (·;µ, σ2) désigne la fonction de densité d’une loi normale d’espérance µ
et de variance σ2. On peut montrer alors que conditionnellement à ti, la variable aléatoire xi
est distribuée suivant le mélange de densités normales
p(xi|ti;Φ) =
K∑
k=1
pik(ti;w)N
(
xi;β
T
k ti, σ
2
)
. (7)
Il faut noter que cette dernière loi conditionnelle est un mélange d’experts (Jacobs et al., 1991),
(Jordan et Jacobs, 1994) contraint, avec une variance commune pour toutes les composantes du
Modèle à processus latent et algorithme EM pour la régression non linéaire
mélange. Le modèleM2 constitue un modèle parcimonieux du modèle plus général supposant
des variances différentes. En pratique, pour les données traitées, ces deux modèles ont conduit
à des ajustements quasi identiques.
On peut alors vérifier que le modèle de régression proposé conduit à la même espérance
conditionnelle que celle du modèleM1 :
E[x|t;Φ] =
∫
R
xp(x|t;Φ)dx
=
K∑
k=1
pik(t;w)
∫
R
xN
(
x;βTk t, σ
2
)
dx
=
K∑
k=1
pik(t;w)β
T
k t
= f(t;θ). (8)
Ainsi, grâce aux propriétés asymptotiques classiques de normalité, d’absence de biais et d’effi-
cacité de l’estimateur du maximum de vraisemblance de θ, la fonction de régression f estimée
à partir du modèleM2 est asymptotiquement identique à celle obtenue à partir du modèleM1.
Ce qui nous conforte sur le fait que le modèle proposé peut être une bonne alternative pour
résoudre le problème de régression non linéaire, si on dispose d’un algorithme adapté pour
l’estimation de ses paramètres.
La section suivante montre comment les paramètres du modèle peuvent être estimés par la
méthode du maximum de vraisemblance.
3 Estimation des paramètres via l’algorithme EM
Dans cette section, compte tenu du fait que la densité de la loi conditionnelle de x s’écrit
sous la forme d’un mélange de densités, nous exploitons le cadre élégant de l’algorithme EM
(Dempster et al., 1977) pour estimer ses paramètres.
Les hypothèses d’indépendance des εi et d’indépendance des zi conditionnellement aux ti
entraînent l’indépendance des xi conditionnellement aux ti. La log-vraisemblance à maximiser
s’écrit donc
L(Φ) =
n∑
i=1
log p(xi|ti;Φ)
=
n∑
i=1
log
K∑
k=1
pik(ti;w)N
(
xi;β
T
k ti, σ
2
)
· (9)
Cette maximisation ne pouvant pas être effectuée analytiquement, nous nous appuyons sur l’al-
gorithme EM (Dempster et al., 1977) pour l’effectuer. L’algorithme EM, dans cette situation,
itère à partir d’un paramètre initial Φ(0) les deux étapes suivantes jusqu’à la convergence.
Chamroukhi et al.
3.1 Étape E (Espérance)
Cette étape consiste à calculer l’espérance de la log-vraisemblance complétée
log p(x, z;Φ) =
n∑
i=1
K∑
k=1
zik log[pik(ti;w)N (xi;β
T
k ti, σ
2)], (10)
conditionnellement aux données observées et au paramètre courant Φ(q) (q étant l’itération
courante). Dans notre situation, cette espérance conditionnelle s’écrit :
Q(Φ,Φ(q)) = E
[
log p(x, z;Φ)|x;Φ(q)
]
=
n∑
i=1
K∑
k=1
E(zik|xi;Φ
(q)) log[pik(ti;w)N (xi;β
T
k ti, σ
2)]
=
n∑
i=1
K∑
k=1
p(zik = 1|xi;Φ
(q)) log[pik(ti;w)N (xi;β
T
k ti, σ
2)]
=
n∑
i=1
K∑
k=1
τ
(q)
ik log[pik(ti;w)N (xi;β
T
k ti, σ
2)], (11)
où
τ
(q)
ik = p(zik = 1|xi;Φ
(q))
=
pik(ti;w
(q))N (xi;β
T (q)
k ti, σ
2(q))∑K
`=1 pi`(ti;w
(q))N (xi;β
T (q)
` ti, σ
2(q))
, (12)
est la probabilité a posteriori que xi soit issu de la kième composante du mélange. Cette étape
nécessite simplement le calcul des τ (q)ik .
3.2 Étape M (Maximisation)
Cette étape (de mise à jour) consiste à calculer le paramètreΦ(q+1) qui maximiseQ(Φ,Φ(q))
par rapport àΦ. La quantitéQ(Φ,Φ(q)) s’écrit sous la forme d’une somme des deux quantités
suivantes :
Q1(w) =
n∑
i=1
K∑
k=1
τ
(q)
ik log pik(ti;w) (13)
et
Q2(β1, . . . ,βK , σ
2) =
n∑
i=1
K∑
k=1
τ
(q)
ik logN (xi;β
T
k ti, σ
2). (14)
Pour maximiser Q(Φ,Φ(q)), il suffit donc de maximiser séparément les quantités Q1(w) et
Q2(β1, . . . ,βK , σ
2).
Modèle à processus latent et algorithme EM pour la régression non linéaire
La maximisation de Q2(β1, . . . ,βK , σ2) est celle classique qu’on rencontre dans un mé-
lange de lois gaussiennes d’espérance décrite par un modèle linéaire. On obtient les β(q+1)k en
résolvant analytiquementK problèmes de moindres-carrés ordinaires pondérés par les τ (q)ik :
β
T (q+1)
k = argmin
β
n∑
i=1
τ
(q)
ik (xi − β
T
ti)
2
= (TTW
(q)
k T)
−1
T
T
W
(q)
k x, (15)
où T est la matrice de régression de dimension [n× (p+ 1)] définie par
T =


1 t1 t
2
1 . . . t
p
1
1 t2 t
2
2 . . . t
p
2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 tn t
2
n . . . t
p
n

 ,
W
(q)
k est une matrice diagonale de dimension [n × n] ayant pour éléments diagonaux
(τ
(q)
1k , . . . , τ
(q)
nk ) et x = (x1, . . . , xn)
T est le vecteur de dimension [(n + 1) × 1] des obser-
vations. La variance σ2(q+1), qui est identique pour toutes les composantes du mélange, est
donnée par :
σ2(q+1) = argmin
σ2
[
n log σ2 +
1
σ2
n∑
i=1
K∑
k=1
τ
(q)
ik (xi − β
T
k ti)
2
]
=
1
n
n∑
i=1
K∑
k=1
τ
(q)
ik (xi − β
T (q+1)
k ti)
2. (16)
La maximisation de Q1 par rapport à w est un problème convexe de régression logistique
multinomial pondéré par les τ (q)ik . Contrairement au cas précédent, cette maximisation ne peut
pas s’effectuer de manière analytique. On a recours à un algorithme, adapté à ce type de pro-
blème, et qui est lui-même itératif : l’algorithme IRLS (Iterative Reweighted Least Squares)
(Green, 1984). Le paragraphe suivant décrit cet algorithme.
Algorithme IRLS (Iteratively Reweighted Least Squares) : l’algorithme IRLS est utilisé
pour maximiser Q1(w) par rapport à w à l’étape M de chaque itération q de l’algorithme
EM. Puisque
∑K
k=1 pik(ti;w) = 1, les valeurs de wK0 et wK1 sont fixées à zéro pour éviter
les problèmes d’identification. L’algorithme IRLS est équivalent à l’algorithme de Newton-
Raphson qui consiste à partir d’un vecteur paramètre initial w(0) et à s’appuyer, à chaque
nouvelle itération c+ 1, sur la formule suivante :
w(c+1) = w(c) −
[
H(w(c))
]
−1
g(w(c)), (17)
oùH(w(c)) et g(w(c)) sont respectivement la hessienne et le gradient deQ1(w) calculés avec
le paramètre w(c). La matrice hessienne H(w(c)) est formée de (K − 1)× (K − 1) matrices
Chamroukhi et al.
blocs Hk`(w(c)) (k, ` = 1, . . . ,K − 1) (Chen et al., 1999), où
Hk`(w
(c)) = −
n∑
i=1
pik(ti;w
(c))[δk` − pi`(ti;w
(c))]vivi
T , (18)
δk` étant le symbole de Kronecker (δk` = 1 si k = `, 0 sinon) et vi = (1, ti)T . Le gradient de
Q1(w) s’écrit :
g(w(c)) = [g1(w
(c)), . . . , gK−1(w
(c))]T , (19)
avec
gk(w
(c)) =
n∑
i=1
[τ
(q)
ik − pik(ti;w
(c))]vTi ∀ k = 1, . . . ,K − 1. (20)
L’équation (17), initialisée avec le paramètre w(q), fournit à la convergence le paramètre
w(q+1).
Chaque itération de l’algorithme EM proposé fait croître le critère de log-vraisemblance
(McLachlan et Krishnan, 1997). Cela résulte de la maximisation de Q(Φ,Φ(q)) et de l’in-
égalité de Jensen (Jensen, 1906). Il a été prouvé par (Jordan et Xu, 1995) et (Xu et Jordan,
1996), notamment pour le cas du mélange gaussien et celui du mélange d’experts, que l’algo-
rithme EM permet de maximiser localement la log-vraisemblance. On peut limiter le nombre
d’itérations de la procédure IRLS interne à l’algorithme EM. Cette version consiste à faire
croître Q(Φ,Φ(q)) à chaque itération au lieu de la maximiser. On peut par exemple limi-
ter le nombre d’itérations de l’IRLS jusqu’à une seule itération (Foulley, 2002). On obtient
ainsi un algorithme EM généralisé (GEM) (Dempster et al., 1977; McLachlan et Krishnan,
1997), qui possède les mêmes propriétés de convergence que l’algorithme EM. En pratique,
nous avons pu observer que cette limitation entrainait une augmentation du nombre d’itéra-
tions de l’algorithme EM. Par conséquent, nous avons opté pour une stratégie qui consiste
à initialiser aléatoirement la procédure IRLS seulement pour la première itération de l’algo-
rithme EM. Pour cette première initialisation, la convergence de l’algorithme IRLS requiert
une quinzaine d’itérations. À partir de la deuxième itération de l’algorithme EM, l’algorithme
IRLS défini par l’équation (17) est initialisé avec le paramètre w(q) estimé à l’itération pré-
cédente de l’algorithme EM. Au delà de la quatrième itération de l’algorithme EM, on ob-
serve que l’algorithme IRLS converge en moins de 5 itérations. Les critères d’arrêt utilisés
pour l’algorithme EM et l’algorithme IRLS consistent en des valeurs seuils sur les varia-
tions relatives des log-vraisemblances à maximiser (|L(Φ(q+1))−L(Φ(q))
L(Φ(q))
| < 10−6 pour EM
et |Q1(w
(c+1))−Q1(w
(c))
Q1(w(c))
| < 10−6 pour IRLS) ainsi qu’à des nombres d’itérations maximum
(1000 pour EM et 50 pour IRLS).
3.3 Choix du nombre de composantes et de l’ordre des polynômes de
régression
Les valeurs optimales du nombre de composantes K du modèle et de l’ordre p des po-
lynômes de régression peuvent être obtenues en maximisant le critère d’information bayésien
Modèle à processus latent et algorithme EM pour la régression non linéaire
(BIC) (Schwarz, 1978) défini comme étant le critère de vraisemblance obtenu à la convergence
de l’algorithme, pénalisé par un terme qui dépend du nombre de paramètres libres du modèle.
La pénalisation utilisée ici est pen = − ν(K,p) log(n)2 où ν(K, p) = K(p+3)− 1 est le nombre
total de paramètres libres à estimer.
Il est possible d’utiliser aussi le critère d’entropie normalisée NEC (Entropy Normalized
Criterion) (Celeux et Soromenho, 1996; Biernacki et al., 1999) adapté au contexte des modèles
de mélange.
3.4 Région de confiance de la courbe de régression f(t;θ)
À partir de la modélisation proposée, on peut montrer, comme dans le cadre des mélanges
hiérarchiques d’experts pour les modèles linéaires généralisés, que sous certaines conditions
de régularité, f(t; θˆ) suit asymptotiquement une loi normale de moyenne f(t;θ) et de variance
s2(t;Φ) (Jiang et Tanner, 1999). Cependant, comme les “vrais” paramètresΦ et θ sont incon-
nus, on propose de les remplacer par leurs estimations Φˆ et θˆ fournies par l’algorithme EM.
Dans le cas de la régression linéaire, la région de confiance approchée au niveau 1 − α est
définie par :
f(t; θˆ)±
√
νθp(νθ, n− νθ;α)s(t; Φˆ) , (21)
où νθ = dim(θ) et p(νθ, n−νθ;α) = p(F (νθ, n−νθ) ≥ α), F étant le 100 (1−α) percentile
d’une variable de Fisher à νθ et n− νθ degrés de liberté (Tomassone et al., 1992).
La fonction
f(t; θˆ) =
K∑
k=1
pik(t; wˆ)βˆ
T
k t (22)
représente la courbe ajustée et
s2(t; Φˆ) =
1
n
D(Φˆ)
T
I(Φˆ)
−1
D(Φˆ) (23)
sa variance empirique,
I(Φˆ) = −E
[∂2 log p(y|t;Φ)
∂Φ∂ΦT
]
Φ=Φˆ
(24)
étant la matrice d’information de Fisher et
D(Φˆ) =
∂f(t;θ)
∂Φ
∣∣∣
Φ=Φˆ
(25)
le gradient de f(t;θ) (Jiang et Tanner, 1999), calculés avec le paramètre estimé Φˆ.
Dans le cas non-linéaire, quand n− νθ est grand, (21) peut être approximée asymptotique-
ment (Tomassone et al., 1992; Gauchi et al., 2010) et on obtient ainsi la région de confiance
approchée au niveau 1− α suivante :
f(t; θˆ)±
√
χ2νθ,1−αs(t; Φˆ). (26)
Chamroukhi et al.
4 Expérimentation sur des données simulées
L’objet de cette partie est d’évaluer l’approche de régression proposée en utilisant des
données simulées. Pour ce faire, nous la comparons à deux méthodes :
– une méthode de régression polynomiale par morceaux (McGee et Carleton, 1970), (Samé
et al., 2007), (Chamroukhi et al., 2009) où l’estimation des paramètres est effectuée par
une procédure de programmation dynamique (Bellman, 1961; Stone, 1961),
– une méthode de régression à processus markovien caché, c’est-à-dire où le processus
caché (z1, . . . , zn) est supposé être une chaîne de Markov (Fridman, 1993), (Rabiner,
1989). Les paramètres de ce modèle sont estimés par l’algorithme de Baum-Welch
(Baum et al., 1970).
La qualité des estimations fournies par chacune des deux méthodes est l’écart quadratique
moyen (EQM) 1
n
∑n
i=1(fsim(ti)−fest(ti))
2 entre la courbe de régression estimée et la courbe
de régression simulée où :
– fest(ti) =
∑K
k=1 pik(ti; wˆ)βˆ
T
k ti pour le modèle proposé,
– fest(ti) =
∑K
k=1 zˆikβˆ
T
k ti pour le modèle de régression par morceaux,
– fest(ti) =
∑K
k=1 ωk(ti; Φˆ)βˆ
T
k ti pour le modèle markovien où les ωk(ti; Φˆ) sont les
probabilités dites de filtrage ωk(ti; Φˆ) = p(zi = k|x1, . . . , xi; Φˆ) qui se calculent via
une procédure du type “forward-backward” (Rabiner, 1989).
Chaque jeu de données est généré en ajoutant un bruit gaussien centré à des points d’une courbe
non linéaire, régulièrement échantillonnés sur l’intervalle temporel [0; 5].
4.1 Paramètres de simulation et réglage des algorithmes
Trois fonctions non linéaires de régression ont été considérées. Ces fonctions et leurs pa-
ramètres associés sont fournis dans le tableau 1. Des exemples de données simulées à partir de
ces courbes sont représentés sur la figure 1.
Fonctions Paramètres
f1(t,θ) =
∑4
k=1 pik(t;w)β
T
k
t β1 = [34,−60, 30] w1 = [547,−154]
β2 = [−17, 29,−7] w2 = [526,−135]
β3 = [185,−104, 15] w3 = [464,−115]
β4 = [−804, 343,−35] w4 = [0, 0]
f2(t,θ) = β
T
1 t1[0;2.5](t) + β
T
2 t1]2.5;5](t) β1 = [33,−20, 4]
β2 = [−78, 47,−5]
f3(t) = 20 sin(1.6pit) exp(−0.7t)
TAB. 1 – Expressions analytiques des courbes de régression utilisées avec leurs paramètres
Les trois algorithmes testés ont été lancés avec (K = 4, p = 2) pour la situation 1,
(K = 2, p = 2) pour la situation 2 et avec (K = 5, p = 3) pour la situation 3. Pour chaque
jeu de données, l’algorithme EM a été lancé à partir de 10 initialisations aléatoires différentes
et seule la solution ayant la plus grande vraisemblance a été retenue.
Modèle à processus latent et algorithme EM pour la régression non linéaire
situation 1
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
0
5
10
15
20
25
30
35
40
t
xt
situation 2
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
5
10
15
20
25
30
35
40
t
xt
situation 3
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
−15
−10
−5
0
5
10
15
20
t
xt
FIG. 1 – Exemple de données simulées à partir des trois courbes
Il faut souligner ici que la première situation a tendance à favoriser l’approche proposée
mais permet de la valider. Les deux autres fonctions considérées sont plus pertinentes pour
évaluer la méthode proposée. Notamment, la troisième situation constitue un exemple typique
de fonction non linéaire.
4.2 Résultats
La figure 2 montre pour chacune des situations, comment varie l’écart entre les courbes
de régression simulées et les courbes estimées, en fonction de la taille d’échantillon et de la
Chamroukhi et al.
variance du bruit. Pour chaque taille d’échantillon et chaque valeur de la variance du bruit,
l’erreur quadratique présentée correspond à une moyenne sur 20 jeux de données différents.
100200 500 1000 2000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Taille d’échantillon (n)
Er
re
ur
 
 
algorithme proposé
régression polynomiale par morceaux
régression polynomiale par HMM
1 2 3 4 5 6 7 10
0
1
2
3
4
5
6
7
Ecart−type (σ)
Er
re
ur
 
 
algorithme proposé
régression polynomiale par morceaux
régression polynomiale par HMM
100200 500 1000 2000
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Taille d’échantillon (n)
Er
re
ur
 
 
algorithme proposé
régression polynomiale par morceaux
régression polynomiale par HMM
1 2 3 4 5 7 10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Ecart−type (σ)
Er
re
ur
 
 
algorithmeproposé
régression polynomiale par morceaux
régression polynomiale par HMM
100200 500 1000 2000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Taille d’échantillon (n)
Er
re
ur
 
 
algorithme proposé
régression polynomiale par morceaux
régression polynomiale par HMM
1 2 3 4 5 6 7 10
0
1
2
3
4
5
6
7
8
9
10
Ecart−type (σ)
Er
re
ur
 
 
algorithme proposé
régression polynomiale par morceaux
régression polynomiale par HMM
FIG. 2 – Erreur entre la courbe estimée et la courbe réelle simulée en fonction de la taille
d’échantillon (gauche) (σ = 1.5) et en fonction de la variance du bruit (droite) (n = 500),
pour la situation 1 (haut), la situation 2 (milieu) et la situation 3 (bas)
Ces graphiques montrent que la méthode proposée donne de meilleurs résultats que la
méthode de régression polynomiale par morceaux et la méthode basée sur un processus mar-
Modèle à processus latent et algorithme EM pour la régression non linéaire
kovien caché. Cette différence entre les résultats fournis par les trois méthodes s’explique par
leur gestion différente des passages d’un sous-modèle à l’autre. En effet, contrairement aux
deux alternatives qui sont adaptées à des signaux avec changement brusques, l’approche pro-
posée permet de s’adapter à la fois aux transitions souples et brusques grâce à la flexibilité de
la fonction logistique qui modélise le processus latent.
En outre, on peut observer que l’écart entre les courbes estimées et les courbes simulées
décroît quand la taille d’échantillon augmente. L’augmentation de la variance du bruit entraîne
quant à elle une augmentation de l’erreur qui est plus prononcée pour le modèle de régres-
sion polynomial par morceaux. La figure 3 montre, pour chacune des situations, un exemple
de courbe de régression estimée avec l’algorithme proposé et la région de confiance à 95 %
correspondante.
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
0
5
10
15
20
25
30
35
40
t
xt
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
0
0.2
0.4
0.6
0.8
1
t
pik
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
5
10
15
20
25
30
35
40
t
xt
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
0
0.2
0.4
0.6
0.8
1
t
pik
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
−15
−10
−5
0
5
10
15
20
t
xt
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
0
0.2
0.4
0.6
0.8
1
t
pik
FIG. 3 – Fonctions de régression f(t;θ) estimées par l’algorithme proposé et régions de
confiance à 95 % associées (à gauche) et proportions pik correspondantes (à droite)
Le tableau 2 montre les pourcentages moyens de choix du nombre de sous-modèles de
régression K et du degré p des polynômes, sélectionnés par le critère BIC, sur les signaux
Chamroukhi et al.
simulés de la situation 1. On observe que les vraies valeurs (K = 4 et p = 2) ont été sélection-
nées avec un pourcentage maximum de 63 %.
p 1 2 3 4 5 6
K
2 0 0 0 13 3 0
3 0 0 7 13 0 0
4 0 63 1 0 0 0
5 0 0 0 0 0 0
6 0 0 0 0 0 0
7 0 0 0 0 0 0
TAB. 2 – Pourcentage de choix de (K, p) par le critère BIC obtenu avec les signaux de la situation 1
Le tableau 3 montre les temps de calculs moyens obtenus pour les trois méthodes. On re-
marque que le modèle proposé et le modèle markovien, qui sont basés sur une méthode itérative
de type EM pour l’estimation des paramètres, ont des temps de calculs quasi identiques qui sont
peu sensibles à la taille d’échantillon. Cependant, la méthode de régression par morceaux est
très coûteuse en temps de calcul à cause de la procédure de programmation dynamique dont le
temps de calcul augmente considérablement avec la taille d’échantillon.
Situation 1 Situation 2 Situation 3
Algorithme : algo 1 algo 2 algo 3 algo 1 algo 2 algo 3 algo 1 algo 2 algo 3
n
100 0.27 0.15 0.12 0.08 0.14 0.07 0.35 0.15 0.17
200 0.45 0.67 0.19 0.09 0.66 0.10 0.48 0.70 0.31
500 1.08 5.09 0.46 0.21 5.05 0.21 1.77 5.33 0.74
1000 1.93 25.86 0.64 0.27 25.65 0.34 3.06 26.59 1.01
2000 2.80 147.7 1.39 0.47 147.2 0.65 5.03 148.2 2.17
TAB. 3 – Temps de calculs moyens en secondes, en fonction de la taille d’échantillon n, obtenus avec
les trois méthodes : approche proposée (algorithme 1), approche de régression par morceaux (algorithme
2) et approche de régression à processus markovien (algorithme 3), pour les trois situations des données
simulées
5 Expérimentation sur des données réelles
Dans le cadre d’une application de suivi d’état de fonctionnement du mécanisme d’ai-
guillage des rails, nous avons été amenés à paramétriser des signaux non linéaires représentant
la puissance consommée par le moteur d’aiguillage durant des manœuvres d’aiguillage. Cette
paramétrisation vise à représenter dans un espace de dimension peu élevée ces signaux non
linéaires qui sont formés de 562 points. Nous avons réalisé cette tache par la méthode de ré-
gression à processus latent proposée, les paramètres de régression étant directement utilisés
comme paramètres des signaux.
Une manœuvre d’aiguillage est constituée de mouvements mécaniques des différents or-
ganes liés à l’aiguille, qui sont mobilisés successivement. Ces mouvements se traduisent, sur
le signal de puissance consommée en fonction du temps, par différentes phases de fonction-
nement (5 phases : Appel moteur, décalage-déverrouillage, translation, verrouillage-calage et
Modèle à processus latent et algorithme EM pour la régression non linéaire
friction). Le nombre des composantes régressivesK du modèle, pour chaque signal considéré,
a été donc fixé à K = 5 qui correspond au nombre de phases d’une manœuvre d’aiguillage.
L’ordre du polynôme a été fixé à p = 3 qui est adapté à la forme des signaux traités.
La figure 4 nous montre trois exemples de signaux réels sur lesquels nous avons appliqué
la méthode. Ces signaux correspondent à trois états de fonctionnement : état sans défaut (a),
état avec défaut tolérable (b) et état avec défaut critique (c).
(a)
0 1 2 3 4 5 6
250
300
350
400
450
500
550
600
temps (secondes)
pu
iss
an
ce
 (w
atts
)
 
 
(b)
0 1 2 3 4 5 6
250
300
350
400
450
500
550
600
temps (secondes)
pu
iss
an
ce
 (w
atts
)
 
 
(c)
0 1 2 3 4 5 6
250
300
350
400
450
500
550
600
temps (secondes)
pu
iss
an
ce
 (w
atts
)
 
 
FIG. 4 – Exemples de signaux de manœuvres d’aiguillage
Chamroukhi et al.
La qualité des estimations fournies par les différentes méthodes est mesurée par l’écart
quadratique moyen (EQM) entre le vrai signal et son estimation. Ce critère est donné par
EQM = 1
n
∑n
i=1(xi − xˆi)
2
.
Les résultats correspondants aux trois situations de signaux de manœuvres d’aiguillages
obtenus par les trois méthodes sont donnés dans le tableau 4. Ils mettent en évidence les bonnes
performances de la méthode proposée en termes d’écart quadratique moyen. On remarque que,
pour la situation (a), les écarts quadratiques moyens obtenus par la régression par morceaux et
la régression par HMM sont légèrement inférieurs à l’écart quadratique moyen obtenu avec la
méthode proposé. Cela est du au fait que le signal de la situation (a) présente clairement des
changements brusques entre les différentes phases.
Situation Approche proposée Régression par morceaux Régression par HMM
(a) 784.93 781.64 783.20
(b) 1800.81 1928.92 1816.31
(c) 309.80 310.25 314.83
TAB. 4 – Erreurs quadratiques moyennes obtenues par les trois méthodes pour les trois signaux de
manœuvres d’aiguillage
La figure 5 montre les résultats graphiques correspondant aux trois signaux présentés. Les
proportions du mélange estimées sont cohérentes avec la réalité des phases des manœuvres
d’aiguillage considérées.
La figure 6 montre la convergence du critère de log-vraisemblance sur le signal réel pré-
senté dans la figure 4 (c). On peut remarquer pour cet exemple que, la log-vraisemblance croît
très rapidement jusqu’à 35 itérations puis augmente très peu jusqu’à la convergence. Pour tous
les signaux traités, nous avons pu constater que l’algorithme EM convergeait en un nombre
d’itérations autour de 90.
Le tableau 5 montre les pourcentages de choix du degré p des polynômes, sélectionnés
par le critère BIC avec K = 5 sous-modèles de régression qui correspondent aux phases
impliquées dans une manœuvre d’aiguillage. Les valeurs p = 3 et p = 4 sont majoritairement
sélectionnées.
p 1 2 3 4 5 6
% 0 9.5238 31.4286 38.0952 14.2857 6.6667
TAB. 5 – Pourcentage de choix de p avecK = 5 obtenu sur 120 signaux réels
6 Conclusion
Une méthode de régression non linéaire a été proposée dans cet article. Cette méthode peut
être vue comme une solution alternative au problème des moindres-carrés pour la régression
non linéaire. Elle s’appuie sur un modèle de régression simple intégrant un processus latent
qui permet d’activer successivement, et manière souple, des sous-modèles de régression poly-
nomiaux. Pour estimer les paramètres du modèle proposé, un algorithme de type EM adapté au
contexte de processus latent a été proposé. La méthode proposée se distingue par le fait d’être
adaptée aux transitions à la fois souples et brusques grâce à la modélisation particulière utilisée
pour le processus latent. Dans l’étude expérimentale menée, les modèles alternatifs considérés
Modèle à processus latent et algorithme EM pour la régression non linéaire
0 1 2 3 4 5 6
250
300
350
400
450
500
550
600
temps (secondes)
pu
iss
an
ce
 (w
att
s)
0 1 2 3 4 5 6
0
0.2
0.4
0.6
0.8
1
temps (secondes)
pik
0 1 2 3 4 5 6
250
300
350
400
450
500
550
600
temps (secondes)
pu
iss
an
ce
 (w
att
s)
0 1 2 3 4 5 6
0
0.2
0.4
0.6
0.8
1
temps (secondes)
pik
0 1 2 3 4 5 6
250
300
350
400
450
500
550
600
temps (secondes)
pu
iss
an
ce
 (w
att
s)
0 1 2 3 4 5 6
0
0.2
0.4
0.6
0.8
1
temps (secondes)
pik
FIG. 5 – Signal et modèle estimé par l’algorithme proposé et régions de confiance au niveau
de confiance de 95 % associées (gauche) et proportions correspondantes (droite), pour la
situation sans défaut (haut), avec défaut mineur (milieu) et avec défaut critique (bas)
sont un modèle de régression par morceaux et un modèle markovien de régression. Les résul-
tats obtenus sur des données simulées et sur des données réelles issues d’une application du
domaine ferroviaire confortent l’intérêt de notre démarche.
Remerciements
Les auteurs remercient vivement les relecteurs pour leurs remarques pertinentes, en parti-
culier sur les intervalles de confiance en régression non linéaire.
Chamroukhi et al.
0 10 20 30 40 50 60
−6000
−5500
−5000
−4500
−4000
−3500
−3000
−2500
−2000
itérations
lo
g−
vr
ai
se
m
bl
an
ce
FIG. 6 – Convergence du critère de log-vraisemblance sur le signal réel présenté dans la
figure 4 (c)
Références
Antoniadis, A., J. Berruyer, et R. Carmona (1992). Régression non linéaire et applications.
Economica.
Baum, L., T. Petrie, G. Soules, et N. Weiss (1970). A maximization technique occurring in
the statistical analysis of probabilistic functions of markov chains. Annals of Mathematical
Statistics 41, 164–171.
Bellman, R. (1961). On the approximation of curves by line segments using dynamic pro-
gramming. 4(6), 284.
Biernacki, C., G. Celeux, et G. Govaert (1999). An improvement of the nec criterion for
assessing the number of clusters in a mixture model. Pattern Recognition Letters 20, 267–
272.
Bishop, C. M. (2006). Pattern recognition and machine learning. U K: Springer Verlag.
Celeux, G. et G. Soromenho (1996). An entropy criterion for assessing the number of clusters
in a mixture model. Journal of Classification 13(2), 195–212.
Chamroukhi, F., A. Samé, G. Govaert, et P. Aknin (2009). A regression model with a hidden
logistic process for signal parameterization. Proceedings of XVIIth European Symposium
on Artificial Neural Networks ESANN, 503–508.
Chen, K., L. Xu, et H. Chi (1999). Improved learning algorithms for mixture of experts in
multiclass classification. Neural Networks 12(9), 1229–1252.
Deboor, C. (1978). A Practical Guide to Splines. Springer-Verlag.
Dempster, A. P., N. M. Laird, et D. B. Rubin (1977). Maximum likelihood from incomplete
data via the em algorithm. Journal of The Royal Statistical Society, B 39(1), 1–38.
Foulley, J. (2002). Algorithme em : Théorie et application au modèle mixte. Journal de la
Société Française de Statistique 143((3-4)), 57–109.
Modèle à processus latent et algorithme EM pour la régression non linéaire
Fridman, M. (1993). Hidden markov model regression. Technical report, Institute of mathe-
matics, University of Minnesota.
Gauchi, J.-P., J.-P. Vila, et L. Coroller (2010). New prediction interval and band in the non-
linear regression model : applications to predictive modeling in foods. Communications in
Statistics - Simulation and Computation 39, 322–334.
Green, P. (1984). Iteratively reweighted least squares for maximum likelihood estimation, and
some robust and resistant alternatives. Journal of The Royal Statistical Society, B 46(2),
149–192.
Jacobs, R. A., M. I. Jordan, S. J. Nowlan, et G. E. Hinton (1991). Adaptive mixtures of local
experts. Neural Computation 1(3), 79–87.
Jensen, J. L. W. V. (1906). Sur les fonctions convexes et les inégalités entre les valeurs
moyennes. Acta Mathematica 30(1), 175–193.
Jiang, W. et M. A. Tanner (1999). On the asymptotic normality of hierarchical mixtures-
of-experts for generalized linear models. IEEE Transactions on Information Theory 46,
1005–1013.
Jordan, M. I. et R. A. Jacobs (1994). Hierarchical mixtures of experts and the em algorithm.
Neural Computation 6, 181–214.
Jordan, M. I. et L. Xu (1995). Convergence results for the em approach to mixtures of experts
architectures. Neural Networks 8(9), 1409–1431.
McGee, V. E. et W. T. Carleton (1970). Piecewise regression. Journal of the American
Statistical Association 65, 1109–1124.
McLachlan, G. J. et T. Krishnan (1997). The EM algorithm and extensions. New York: Wiley.
Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech
recognition. Proceedings of the IEEE 77(2), 257–286.
Samé, A., P. Aknin, et G. Govaert (2007). Classification automatique pour la segmentation des
signaux unidimensionnels. Rencontres de la SFC, ENST, Paris.
Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics 6, 461–464.
Stone, H. (1961). Approximation of curves by line segments. Mathematics of Computa-
tion 15(73), 40–47.
Tomassone, R., E. Lesquoy, et C. Millier (1992). La Régression nouveaux regards sur une
ancienne méthode statistique. Paris : Masson.
Xu, L. et M. I. Jordan (1996). On convergence properties of the em algorithm for gaussian
mixtures. Neural Computation 8(1), 129–151.
Summary
A non linear regression approach which consists of a specific regression model incorpo-
rating a latent process, allowing various polynomial regression models to be activated pref-
erentially and smoothly, is introduced in this paper. The model parameters are estimated by
maximum likelihood performed via a dedicated EM algorithm. An experimental study using
simulated and real data sets reveals good performances of the proposed approach.
