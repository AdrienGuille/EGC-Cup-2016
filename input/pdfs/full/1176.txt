√âvaluation de la r√©gression born√©e
Thierry Foucart
UMR 6086, Universit√© de Poitiers, S P 2 M I, bd 3 t√©l√©port 2
BP 179, 86960 Futuroscope, Cedex FRANCE
R√©sum√©. Le mod√®le lin√©aire est tr√®s fr√©quemment utilis√© en statistique et par-
ticuli√®rement dans les secteurs de l‚Äôassurance, de la banque et du marketing. Il
permet de d√©terminer les variables explicatives qui interviennent dans le risque
mesur√© chez les assur√©s et dans les choix effectu√©s par la client√®le. Le probl√®me
consid√©r√© dans cet article appara√Æt lorsque ces variables sont li√©es statistique-
ment, par exemple le revenu et la cat√©gorie socioprofessionnelle. Les estimations
donn√©es par le crit√®re des moindres carr√©s ordinaires deviennent alors instables
et peuvent prendre des valeurs en contradiction avec les valeurs r√©elles. Il existe
de nombreuses m√©thodes adapt√©es √† ce type de donn√©es. Nous proposons ici
d‚Äô√©valuer l‚Äôefficacit√© de la r√©gression born√©e en proc√©dant par simulation. Les
r√©sultats sont clairs : le gain en pr√©cision et en stabilit√© des coefficients de r√©-
gression est impressionnant.
1 Introduction
Le mod√®le lin√©aire est une des m√©thodes statistiques les plus employ√©es dans les sciences
de l‚Äôhomme et de la soci√©t√©. Il donne en effet une r√©ponse √† la question r√©currente de l‚Äôeffet
propre d‚Äôune variable sur une autre. En assurance automobile par exemple, la question pour-
rait √™tre : l‚Äô√¢ge du conducteur joue-t-il un r√¥le dans le risque d‚Äôaccident ind√©pendamment des
autres facteurs ? Ce risque d√©pend-il de son sexe toutes choses √©gales par ailleurs ? Pour r√©-
pondre √† ces questions, on effectue la r√©gression du risque par les facteurs d‚Äôaccident, et on
√©tudie chacun des coefficients de r√©gression de l‚Äô√¢ge et du sexe : la r√©ponse est consid√©r√©e
comme positive si ce coefficient est significativement non nul.
L‚Äôhypoth√®se ¬´ toutes choses √©gales par ailleurs ¬ª, formalis√©e par le choix des variables explica-
tives du mod√®le lin√©aire, est toutefois tr√®s discut√©e depuis fort longtemps parce qu‚Äôelle ouvre la
porte √† des abus flagrants (Simiand, 1932). Sa formalisation demande beaucoup de pr√©cautions
pour √©viter des contradictions internes. Ces derni√®res se manifestent au plan math√©matique par
une relation lin√©aire exacte entre les variables explicatives. Dans ce dernier cas, l‚Äôanalyse sta-
tistique est impossible, la matrice de corr√©lation n‚Äô√©tant pas inversible.
Ces contradictions ne sont pas toujours totales. Il existe des situations dans lesquelles les va-
riables explicatives ne sont pas li√©es au sens lin√©aire du terme (il n‚Äôexiste pas de combinaison
lin√©aire strictement √©gale √† 0), mais le sont au sens statistique (il existe une combinaison li-
n√©aire ¬´presque ¬ª√©gale √† 0). L‚Äôestimateur des moindres carr√©s ordinaires devient alors peu
pr√©cis, et on est amen√© √† utiliser d‚Äôautres estimateurs dont les plus classiques sont ceux de la
- 131 - RNTI-A-1
√âvaluation de la r√©gression born√©e
r√©gression orthogonale (les variables explicatives sont choisies parmi les composantes princi-
pales de variance suffisante) et de la r√©gression born√©e que nous √©tudions ci-dessous.
Apr√®s avoir d√©fini l‚Äôestimateur de la r√©gression born√©e (ou ridge regression) introduite par
Hoerl et Kennard en 1970, nous donnons quelques exemples des cons√©quences de la colin√©a-
rit√© statistique entre les variables explicatives d‚Äôun mod√®le lin√©aire. Pour pouvoir comparer
les estimations aux valeurs r√©elles des coefficients de r√©gression, nous avons proc√©d√© par si-
mulation. La g√©n√©ralisation de cette proc√©dure donne un √©chantillon de l‚Äôestimateur born√© et
par suite une estimation de l‚Äôerreur quadratique moyenne. La comparaison de cette estimation
avec l‚Äôerreur quadratique de l‚Äôestimateur des moindres carr√©s ordinaires montre clairement que
la r√©gression born√©e am√©liore consid√©rablement les estimations lorsqu‚Äôil existe une colin√©arit√©
statistique entre les variables explicatives.
Les donn√©es analys√©es dans cet article et les logiciels utilis√©s sont disponibles √† l‚Äôadresse sui-
vante : http ://foucart.thierry.free.fr (rubrique colin√©arit√©).
2 Colin√©arit√© et mod√®le lin√©aire.
2.1 Estimateurs dans le mod√®le lin√©aire
Le mod√®le lin√©aire consiste √† repr√©senter une relation entre une variable expliqu√©e not√©e Y
et p variables explicatives X1, . . . , Xp par l‚Äô√©quation ci-dessous
Y = Œ≤0 + Œ≤1X1 + . . .+ Œ≤pXp + Œµ,
dans laquelle
1. les coefficients Œ≤j , j = 1, . . . , p sont des param√®tres th√©oriques appel√©s coefficients de
r√©gression ;
2. la variable Œµ est une variable al√©atoire appel√©e variable r√©siduelle, centr√©e et de variance
œÉ2 appel√©e variance r√©siduelle, ind√©pendante des variables explicatives ;
3. on suppose fr√©quemment que la variable Œµ suit la loi normale N(0, œÉ).
Dans toute la suite du texte, les variables explicatives sont centr√©es et r√©duites et la variable
r√©siduelle suit la loi normale.
On consid√®re un √©chantillon de taille n du vecteur (Y,X1, . . . , Xp). On note X la matrice de n
lignes et p colonnes contenant les observations xi,j(i = 1, . . . , n et j = 1, . . . , p) des variables
Xj et Y la matrice colonne contenant les observations yi(i = 1, . . . , n) de la variable Y . La
matrice R d√©finie ci-dessous est la matrice des corr√©lations observ√©es :
R =
1
n
XtX.
L‚Äôestimateur B des moindres carr√©s ordinaires du vecteur Œ≤ = (Œ≤1, . . . , bp)t est √©gal √† :
B =
1
n
R‚àí1XtX
C‚Äôest un estimateur efficace (de variance minimale dans la classe des estimateurs sans biais).
On note b = (b1, . . . , bp)t l‚Äôobservation du vecteur B et b0 l‚Äôestimation de Œ≤0 d√©duite des
- 132 -RNTI-A-1
T. Foucart
moyennes observ√©es des variables Y et Xj , j = 1, . . . , p. La matrice variance VB de l‚Äôesti-
mateur B est √©gale √† :
VB =
œÉ2
n
R‚àí1.
Le coefficient de d√©termination not√© R2 est le carr√© du coefficient de corr√©lation entre les
valeurs observ√©es yi, i = 1, . . . , n et les valeurs y‚Ä≤i estim√©es par le mod√®le :
y‚Ä≤i = b0 + b1xi,1 + . . .+ bjxi,j + . . .+ bpxi,p.
Les r√©sidus ei (i = 1, . . . , n) sont les diff√©rences entre les valeurs observ√©es de Y et les valeurs
estim√©es :
‚àÄi = 1, . . . , n i = yi ‚àí y‚Ä≤i
On sait que les r√©sidus sont centr√©s et non corr√©l√©s aux variables explicatives. L‚Äôestimation sans
biais de la variance r√©siduelle œÉ2 est donn√©e par :
s,2 =
1
n‚àí p‚àí 1
n‚àë
i=1
2i .
2.2 Colin√©arit√© et estimateur born√©
Les effets de la colin√©arit√© entre les variables explicatives r√©sultent de l‚Äôinversion de la ma-
trice R dans le calcul de l‚Äôestimateur B et de sa matrice varianceVB. La colin√©arit√© cr√©e tout
d‚Äôabord une grande instabilit√© des estimations des coefficients de r√©gression : les variances des
estimateurs, proportionnelles aux termes diagonaux de R‚àí1, sont particuli√®rement √©lev√©es.
Les signes des coefficients estim√©s peuvent m√™me √™tre contraires √† ceux des vraies valeurs. Le
coefficient de d√©termination R2 peut aussi devenir tr√®s instable (Foucart, 2000). L‚Äôinterpr√©ta-
tion des r√©sultats est finalement sujette √† caution. Il n‚Äôest pas toujours facile de d√©tecter cette
colin√©arit√© par une simple lecture de la matrice R. En effet, cette colin√©arit√© appara√Æt lorsqu‚Äôun
coefficient de corr√©lation est proche d‚Äôune des bornes de l‚Äôintervalle dans lequel il peut va-
rier conditionnellement aux autres (Foucart, 1997). On la recherche en examinant les valeurs
propres de la matrice R et des indices comme les facteurs d‚Äôinflation (termes diagonaux de
la matrice R‚àí1) , l‚Äôindice de conditionnement (inverse de la plus petite valeur propre de la
matrice R) et l‚Äôindice de multicolin√©arit√© (moyenne des facteurs d‚Äôinflation) : une petite va-
leur propre et des indices √©lev√©s indiquent une colin√©arit√© statistique. On pourra sur ces points
consulter l‚Äôouvrage de Tomassone ¬´La r√©gression ¬ª(Masson, 1992). Les proc√©dures de simu-
lation utilis√©e ci-dessous visualisent dans un premier temps ces propri√©t√©s connues au plan
math√©matique. Elles montrent aussi que la r√©gression born√©e (ou ridge regression) propos√©e
par Hoerl et Kennard (1970) donne de bien meilleurs r√©sultats que la r√©gression des moindres
carr√©s ordinaires dans le cas de donn√©es statistiquement colin√©aires.
L‚Äôid√©e g√©n√©rale est la suivante : un estimateur efficaceX d‚Äôun param√®tre r√©el m est un estima-
teur de variance minimale dans la classe des estimateurs sans biais (E(X) = ¬µ), mais il ne
minimise pas l‚Äôerreur quadratique d√©finie par E(‚ÄñX ‚Ä≤ ‚àí ¬µ‚Äñ2), dans laquelle X ‚Ä≤ est un estima-
teur quelconque de ¬µ. On peut donc chercher un estimateur biais√©X ‚Ä≤ dont l‚Äôerreur quadratique
- 133 - RNTI-A-1
√âvaluation de la r√©gression born√©e
est plus petite.
Cette situation se pr√©sente dans le cas du mod√®le lin√©aire lorsque les variables explicatives
sont statistiquement colin√©aires. Nous allons v√©rifier par simulation que l‚Äôestimateur Br =
(Br1, . . . , Brp)t d√©fini dans la r√©gression born√©e donne alors de meilleures estimations que
l‚Äôestimateur efficace B = (B1, . . . , Bp)t des moindres carr√©s ordinaires. L‚Äôestimateur Br est
obtenu suivant le crit√®re des moindres carr√©s sous contrainte de norme :
‚ÄñBr‚Äñ2 = b2r1 + b2r2 + . . .+ b2rp ‚â§M
En fait, ce n‚Äôest pas le majorantM que l‚Äôon fixe : on montre en effet qu‚Äôil suffit de remplacer
dans la formule de l‚Äôestimateur des moindres carr√©s ordinaires la matrice R par la matrice
R+kI, o√π I est la matrice identique et k une constante r√©elle positive, pour limiter la norme de
l‚ÄôestimateurBr. En pratique, on recherche la meilleure constante k √† l‚Äôaide de la repr√©sentation
graphique des coefficients de r√©gression en fonction de k, appel√©e ridge trace. Les formules
concernant l‚Äôestimateur born√© sont les suivantes
Br =
1
n
[R+ kI]‚àí1XtY VBr =
œÉ2
n
[R+ kI]‚àí1R[R+ kI]‚àí1
3 Effets de la colin√©arit√©. Exemples.
3.1 Colin√©arit√© et matrice de corr√©lation
On consid√®re quatre variables X1, X2, X4 et X4 dont les coefficients de corr√©lation sont
donn√©s ci-dessous La colin√©arit√© entre les variables ne peut gu√®re √™tre d√©cel√©e par un simple
X1 X2 X3 X4
X1 1.000
X2 0.500 1.000
X3 0.500 0.500 1.000
X4 -0.500 0.400 0.300 1.000
TAB. 1 ‚Äì Matrice de corr√©lation des variables explicatives.
examen de la matrice. Elle peut √™tre mise en √©vidence de plusieurs fa√ßons :
1. Les facteurs d‚Äôinflation fj associ√©s √† chaque coefficient de r√©gression sont √©lev√©s (f1 =
62, f2 = 26, f3 = 14, f4 = 50) ;
2. l‚Äôindice de multicolin√©arit√© I , √©gal √† 1 en l‚Äôabsence de toute colin√©arit√©, est √©lev√© : I =
38 ;
3. la matrice R poss√®de une valeur propre tr√®s faible (Œª4 = 0.007). La combinaison li-
n√©aire des variables X1, X2, X3 et X4 d√©finie par la quatri√®me composante principale
est donc presque constante et √©gale √† 0 : on ne peut pas choisir une valeur deX4 en toute
libert√© lorsque les trois autres sont fix√©es ;
4. On utilise souvent l‚Äôindice de conditionnement, dont on trouvera une analyse dans Bels-
ley (1980) : Œ∫ = 1/Œª4 = 148.83.
- 134 -RNTI-A-1
T. Foucart
3.2 Effet de la colin√©arit√© sur les coefficients de r√©gression estim√©s
Les donn√©es ridge1 contiennent les observations de cinq variables sur cent individus sta-
tistiques obtenues par simulation. On veut expliquer la cinqui√®me variable, Y , par les quatre
premi√®res X1, X2, X3 et X4, dont la matrice de corr√©lation est √©gale √† la pr√©c√©dente. Les va-
riables explicatives sont centr√©es et r√©duites. Les coefficients de corr√©lation observ√©s entre les
variables explicatives et la variable expliqu√©e sont donn√©s dans le tableau ci-dessous
X1 X2 X3 X4
y 0.540 0.216 -0.107 -0.491
TAB. 2 ‚Äì Coefficients de corr√©lation observ√©s entre Y et X1, X2, X3, X4, (donn√©es ridge1).
La r√©gression des moindres carr√©s ordinaires donne les r√©sultats suivants :
degr√© de libert√© Somme des carr√©s Variance estim√©e Pourcentage de variance totale
Tot 99 229.7305 2.320510 1
Exp 4 112.7185 1.088805 0.490655
Res 95 117.0120 1.231705 0.509345
TAB. 3 ‚Äì Analyse de variance (donn√©es ridge1, n = 100).
Estimation √©cart-type t de Student facteur d‚Äôinflation
b1 1.6339 0.8739 1.870 62.00
b2 -0.1482 0.5659 -0.262 26.00
b3 -1.0375 0.4153 -2.498 14.00
b4 0.4439 0.7848 0.566 50.00
b0 -0.1650 0.1110 -1.486
TAB. 4 ‚Äì Estimation des coefficients de r√©gression (donn√©es ridge1).
Les variables explicatives √©tant r√©duites et l‚Äô√©cart-type de la variable expliqu√©e √©gal √† 1.516,
on peut appr√©cier intuitivement la taille des coefficients de r√©gression. Les coefficients de r√©-
gression b1, b3 prennent des valeurs √©lev√©es en valeur absolue. Le coefficient de r√©gression
b2 est n√©gatif, malgr√© un coefficient de corr√©lation positif entre X2 et Y (0.216), et b4 est po-
sitif malgr√© un coefficient de corr√©lation entre X4 et Y fortement n√©gatif (‚àí0.491). Seul b3
est significativement non nul pour un risque de premi√®re esp√®ce Œ± = 5% (t = ‚àí2.498). Le
coefficient de d√©termination (R2 = 0.49) est hautement significatif. Ces r√©sultats peuvent
s‚Äôexpliquer par la forte colin√©arit√© statistique entre X1, X2, X3, et X4 d√©tect√©e en paragraphe
3.1.
3.3 Effet de la colin√©arit√© sur les variances
On √©tudie maintenant les donn√©es ridge2, obtenues par simulation suivant le m√™me mod√®le
que pr√©c√©demment. La matrice de corr√©lation entre les variables explicatives reste √©gale √† R,
les coefficients de r√©gression th√©oriques sont les m√™mes, mais les corr√©lations observ√©es entre
la variable expliqu√©e et les variables explicatives sont les suivantes :
- 135 - RNTI-A-1
√âvaluation de la r√©gression born√©e
X1 X2 X3 X4
y 0.486 0.084 -0.199 -0.584
TAB. 5 ‚Äì Coefficients de corr√©lation observ√©s entre Y et X1, X2, X3, X4, (donn√©es ridge 2).
La r√©gression des moindres carr√©s ordinaires donne les r√©sultats suivants
degr√© de libert√© Somme des carr√©s Variance estim√©e Pourcentage de variance totale
Tot 99 188.1299 1.900302 1
Exp 4 94.14017 0.9109364 0.500400
Res 95 93.98971 0.9893653 0.499600
TAB. 6 ‚Äì Analyse de variance (donn√©es ridge 2, n = 100).
Estimation √©cart-type t de Student facteur d‚Äôinflation
b1 0.4638 0.7832 0.592 62.00
b2 0.3674 0.5072 0.724 26.00
b3 -0.5204 0.3722 -1.398 14.00
b4 -0.5594 0.7033 -0.795 50.00
Cst -0.0985 0.0995 -0.990
TAB. 7 ‚Äì Estimation des coefficients de r√©gression (donn√©es ridge 2).
La situation est paradoxale : le coefficient de d√©termination R2 est hautement significatif
(R2 = 0.50, n = 100), mais aucun des coefficients de r√©gression n‚Äôest significativement
non nul. On peut apporter comme explication une surestimation des √©carts-types des estima-
teurs Bj . Les estimations b1, b2, b3 et b4 ne paraissent pas en effet sp√©cialement grandes (les
variables explicatives sont r√©duites, et l‚Äô√©cart-type de la variable expliqu√©e est √©gal √† 1.372), et
l‚Äôaugmentation des variances des estimateurs due √† la colin√©arit√© a pour effet de diminuer les t
de Student, les rendant ainsi non significatifs.
4 Simulation d‚Äôun √©chantillon de l‚Äôestimateur born√©
4.1 D√©marche
On peut, par simulation, visualiser de fa√ßon plus compl√®te l‚Äôeffet de la colin√©arit√© sur les
coefficients de r√©gression. La d√©marche est la suivante
1. on choisit le nombre de variables explicatives p, le vecteur de r√©gression th√©orique Œ≤ =
(Œ≤1, . . . , Œ≤p)t, le coefficient constant Œ≤0, le coefficient de d√©termination R2 et le nombre
d‚Äôobservations n. On fixe la matrice de corr√©lationR entre les variables explicatives. On
en d√©duit la variance r√©siduelle th√©orique œÉ2 ;
2. on simule un √©chantillon des variables explicatives xi,j de matrice de corr√©lation √©gale
√† R. Il suffit pour cela de simuler un √©chantillon de taille n d‚Äôun vecteur al√©atoire quel-
conque de dimension p, d‚Äôen effectuer l‚Äôanalyse en composantes principales pour obtenir
un vecteur Z dont la matrice de covariance est strictement √©gale √† la matrice identique
- 136 -RNTI-A-1
T. Foucart
I, et d‚Äôeffectuer une transformation lin√©aire de ce dernier de fa√ßon √† obtenir un tableau
de donn√©es X dont la matrice de corr√©lation est strictement √©gale √† R. Cette transfor-
mation est d√©finie par la matrice triangulaire inf√©rieure T obtenue par l‚Äôalgorithme de
Cholesky(Ciarlet, 1989)
R = TTt X = ZTt;
3. on simule un √©chantillon ind√©pendant (Œµi) de taille n de la v.a. Œµ suivant la loi normale
N(0, œÉ), et on en d√©duit les valeurs simul√©es yi, i = 1, . . . , n de la variable Y pour les
valeurs xi,j du tableauX
‚àÄi = 1, . . . , n yi = b0 + b1xi,1 + ...+ bpxi,p + Œµi;
4. on calcule le vecteur de r√©gression estim√© b = (b1, . . . ,bp)t ;
5. on recommence la simulation effectu√©e en 3) pour obtenir une autre simulation du vec-
teur de r√©gression avec le m√™me tableauX, etc.
Chaque √©chantillon de la variable expliqu√©e donne une estimation b du vecteur de r√©gression Œ≤
pour les m√™mes valeurs des variables explicatives. On en d√©duit l‚Äôerreur quadratique ‚Äñb‚àíŒ≤‚Äñ2.
En r√©p√©tant m fois cette op√©ration, on dispose donc d‚Äôun √©chantillon de m vecteurs bl, l =
1, . . . ,m conditionnellement √† X. On peut calculer les erreurs quadratiques dl = ‚Äñbl ‚àí Œ≤‚Äñ
pour l = 1, . . . ,m en choisissant comme estimateur l‚Äôestimateur de la r√©gression born√©e Br
pour diff√©rentes valeurs de k (pour k = 0, l‚Äôestimateur de la r√©gression born√©e est confondu
avec l‚Äôestimateur des moindres carr√©s ordinaires). Certains auteurs donnent des indications sur
le choix de cette constante k (Nordberg, 1982).
4.2 Exemple
Dans l‚Äôexemple ci-dessous, la matrice de corr√©lation R entre les variables explicatives est
donn√©e dans le tableau 1. Les coefficients de r√©gression et la constante choisis sont les suivants :
Œ≤0 = 0 Œ≤1 = 0.5 Œ≤2 = 0.5 Œ≤3 = ‚àí0.5 Œ≤4 = ‚àí0.5
Le mod√®le th√©orique est donc √©gal √†
Y = 0.5X1 + 0.5X2 ‚àí 0.5X3 ‚àí 0.5X4 + Œµ .
Le coefficient R2 √©tant fix√© √† 0.5, la variance r√©siduelle th√©orique est √©gale √† 0.95. On effectue
ensuite la r√©gression lin√©aire born√©e des donn√©es simul√©es en effectuant la d√©marche pr√©c√©-
dente. Le coefficient constant estim√© b0 n‚Äôest pas n√©cessairement nul, mais n‚Äôest pas pris en
compte dans le calcul des distances entre les vecteurs de r√©gression.
La taille de l‚Äô√©chantillon √©tant fix√©e √† n = 100, nous donnons ci-dessous les r√©sultats de la
r√©gression sur un √©chantillon obtenus par l‚Äôestimateur des moindres carr√©s ordinaires (k = 0)
et par l‚Äôestimateur born√© pour diff√©rentes valeurs de la constante k.
- 137 - RNTI-A-1
√âvaluation de la r√©gression born√©e
Coefficients th√©oriques 0.5 0.5 -0.5 -0.5 Carr√© de la distance
Coefficients estim√©s br1 br2 br3 br4 d2
k= 0 (MCO) 0.123 0.850 -0.445 -0.826 0.374
k = 0.01 0.356 0.688 -0.539 -0.611 0.070
k = 0.05 0.452 0.575 -0.543 -0.498 0.010
TAB. 8 ‚Äì coefficients de r√©gression estim√©s pour diff√©rentes valeurs de k. En derni√®re colonne :
carr√© de la distance au vecteur de r√©gression th√©orique.
4.3 G√©n√©ralisation
Pour g√©n√©raliser ces r√©sultats, nous avons g√©n√©r√©, pour les m√™mes valeurs xi,j des variables
explicatives, cinquante √©chantillons de la v.a. Y , puis, pour chaque valeur de k, effectu√© les
cinquante r√©gressions et calcul√© les carr√©s d2l des distances, leur moyenne et leur variance :
k moyenne variance
0 1.383 4.005
0.01 0.242 0.107
0.05 0.045 0.002
TAB. 9 ‚Äì moyennes et variances des carr√©s des distances d2l (l = 1, . . . , 50) pour k =
0, 0.01 et 0.05.
D‚Äôapr√®s le tableau pr√©c√©dent, la r√©gression classique donne des estimations beaucoup plus
√©loign√©es en moyenne des coefficients de r√©gression th√©oriques que la r√©gression born√©e. La
variance des carr√©s des distances est tr√®s √©lev√©e par rapport aux autres variances, et le meilleur
estimateur des trois pr√©c√©dents est celui de la r√©gression born√©e pour k = 0.05.
La fonction de r√©partition observ√©e des carr√©s des distances du vecteur de r√©gression estim√©
par le crit√®re des moindres carr√©s ordinaires au vecteur de r√©gression th√©orique est donn√©e en
figure 1 ci-dessous. Elle met en √©vidence la fr√©quence de vecteurs de r√©gression estim√©s par
les moindres carr√©s ordinaires tr√®s diff√©rents du vecteur th√©orique. La valeur maximale des
carr√©s des distances obtenues par l‚Äôestimateur born√© et calcul√©es sur les cinquante √©chantillons
en posant k = 0.05 est √©gale √† 0.208 : dans plus de 60% des cas, la r√©gression des moindres
carr√©s ordinaires donne un estimation moins bonne du vecteur th√©orique que la pire donn√©e par
la r√©gression born√©e.
Le tableau 10 contient les dix vecteurs de r√©gression obtenus par les moindres carr√©s ordi-
naires les plus √©loign√©s du vecteur th√©orique. Le coefficient de d√©termination R2 et le coeffi-
cient constant b0 sont √† peu pr√®s correctement estim√©s. Ce n‚Äôest pas le cas des coefficients de
r√©gression, tr√®s mal reconstruits. Dans tous les vecteurs de r√©gression estim√©s, un au moins des
coefficients est de signe contraire au coefficient th√©orique et certains autres sont tr√®s √©lev√©s en
valeur absolue. Dans la pratique, le risque d‚Äôobtenir ce genre de r√©sultats est loin d‚Äô√™tre n√©gli-
geable, puisque ces √©chantillons repr√©sentent 20% du nombre total d‚Äô√©chantillons. L‚Äôid√©e qui
vient naturellement est de d√©terminer sur ces cinquante √©chantillons la valeur de k qui donne
les meilleurs r√©sultats. En faisant varier k dans l‚Äôintervalle [0, 1] avec un incr√©ment de 0.001,
on obtient k = 0.078 (tableau 11) :
La moyenne des carr√©s des distances est tr√®s faible par rapport √† celle que l‚Äôon obtient par les
estimateurs des moindres carr√©s ordinaires (0.039 au lieu de 1.383), et ces distances varient
- 138 -RNTI-A-1
T. Foucart
FIG. 1 ‚Äì fonction de r√©partition des carr√©s des distances (r√©gression des moindres carr√©s
ordinaires, √©chantillon simul√© de 50 termes).
R2 b0 b1 b2 b3 b4 d
2
valeurs th√©oriques 0.500 0.000 0.500 0.500 -0.500 -0.500
n‚ó¶ 50 0.597 -0.105 -0.444 1.135 -0.041 -1.495 2.494
n‚ó¶ 5 0.416 -0.231 -0.643 1.089 0.190 -1.413 2.963
n‚ó¶ 44 0.534 -0.020 -0.611 1.166 0.010 -1.690 3.353
n‚ó¶ 32 0.402 0.095 -0.841 1.248 -0.042 -1.413 3.400
n‚ó¶ 46 0.594 -0.081 -0.667 1.348 -0.045 -1.728 3.795
n‚ó¶ 47 0.424 0.117 1.693 -0.341 -1.118 0.714 3.986
n‚ó¶ 25 0.560 0.189 1.898 -0.336 -1.233 0.828 4.956
n‚ó¶ 42 0.589 0.071 -1.028 1.488 0.008 -1.910 5.556
n‚ó¶ 40 0.626 0.110 2.474 -0.642 -1.421 1.097 8.600
n‚ó¶ 23 0.407 0.119 -1.523 1.488 0.448 -2.124 8.605
TAB. 10 ‚Äì les dix vecteurs de r√©gression les plus √©loign√©s du vecteur th√©orique (r√©gression
des moindres carr√©s ordinaires)
beaucoup moins : l‚Äôint√©r√™t de la r√©gression born√©e est √©vident et consid√©rable. La simulation
montre aussi la robustesse de la m√©thode : la moyenne des carr√©s des distances diminue tr√®s
rapidement lorsque k varie de 0 √† 0.05, reste √† peu pr√®s constante lorsque k varie de 0.05 √† 0.1
environ, et augmente ensuite lentement √† partir de 0.1 (cf. figure 2 ci-dessous). La variance des
- 139 - RNTI-A-1
√âvaluation de la r√©gression born√©e
k moyenne variance
0.078 0.039 0.001
TAB. 11 ‚Äì moyenne et variance des carr√©s des distances d2k pour la valeur optimale k =
0.078 (m = 50).
distances, minimale aussi pour k = 0.078, suit la m√™me √©volution. La recherche pr√©cise de la
meilleure valeur de la constante k ne pr√©sente visiblement gu√®re d‚Äôint√©r√™t. Nous avons proc√©d√©
√† plusieurs simulations identiques : les r√©sultats ont toujours √©t√© analogues aux pr√©c√©dents.
 
FIG. 2 ‚Äì moyenne des carr√©s des distances entre le vecteur th√©orique et le vecteur estim√© en
fonction de k.
5 Applications
Dans le cas de donn√©es r√©elles quelconques, on ne conna√Æt ni le vecteur de r√©gression
th√©orique, ni la valeur optimale de la constante k. Pour choisir k, on utilise les ridge traces :
l‚Äôabsence de colin√©arit√© se traduisant par une ridge trace tr√®s r√©guli√®re, on va choisir comme
valeur celle pour laquelle les coefficients de r√©gression sont stabilis√©s.
- 140 -RNTI-A-1
T. Foucart
5.1 R√©gression born√©e en l‚Äôabsence de colin√©arit√© (donn√©es ridge0)
Effectuons d‚Äôabord la r√©gression born√©e dans le cas o√π les variables explicatives ne sont pas
colin√©aires. Les donn√©es analys√©es ci-dessous (fichier ridge 0) ont √©t√© obtenues par simulation
en supposant que les variables explicatives sont non corr√©l√©es. Les coefficients de r√©gression
et le coefficient de d√©termination th√©oriques sont les m√™mes que ceux choisis pr√©c√©demment
pour cr√©er les donn√©es ridge1 et ridge 2.
 
FIG. 3 ‚Äì ridge trace (donn√©es ridge 0).
La figure 3 ci-dessus est la ridge trace obtenue en effectuant les r√©gressions born√©es pour des
valeurs de k variant de 0 √† 1. On observe une tr√®s grande stabilit√© des coefficients de r√©gression
par rapport √† la constante k. Le choix de cette derni√®re n‚Äôintervient gu√®re dans les estimations.
5.2 R√©gression born√©e des donn√©es ridge 1
Revenons au donn√©es trait√©es dans le paragraphe 2.1. La figure 4 ci-dessous donne la re-
pr√©sentation graphique des estimations b1, b2, b3 et b4 des coefficients de r√©gression suivant les
valeurs de k. Pour k = 0, ces valeurs sont celles que l‚Äôon obtient par la r√©gression des moindres
carr√©s ordinaires.
On observe l‚Äôinstabilit√© de ces coefficients pour les faibles valeurs de k. Les coefficients de
r√©gression b1 et b3 diminuent tr√®s rapidement en valeur absolue, au contraire de b2 et b4. On
- 141 - RNTI-A-1
√âvaluation de la r√©gression born√©e
 
FIG. 4 ‚Äì ridge trace (donn√©es ridge 1).
recherche sur ce graphique une valeur de k pour laquelle les coefficients de r√©gression sont
stabilis√©s : on peut prendre ici k = 0.1. La r√©gression born√©e donne alors les r√©sultats suivants :
br1 br2 br3 br4 br0
Estimation 0.585141 0.405312 -0.480752 -0.426282 -0.164952
√©cart-type 0.05550 0.06988 0.07351 0.05690
t de Student 6.955 3.826 -4.315 -4.943
TAB. 12 ‚Äì R√©sultats de la r√©gression born√©e (donn√©es ridge1, k = 0.1).
Le biais de l‚ÄôestimateurBr appara√Æt dans le coefficient de corr√©lation non nul entre les r√©sidus
et la variable expliqu√©e estim√©e par le mod√®le : r = 0.088. Les √©carts-types indiquent une tr√®s
grande stabilit√© de Br autour de son esp√©rance E(Br), et les t de Student montrent que tous
les coefficients sont significatifs. Les simulations pr√©c√©dentes montrent que les coefficients
obtenus sont largement plus proches des vraies valeurs que les estimations donn√©es par le
crit√®re des moindres carr√©s ordinaires.
- 142 -RNTI-A-1
T. Foucart
5.3 R√©gression born√©e des donn√©es ridge2
La ridge trace (figure 5) montre une bonne stabilit√© des coefficients de r√©gression, et les
effets de la colin√©arit√© concernent donc surtout les variances des estimateurs.
 
FIG. 5 ‚Äì ridge trace (donn√©es ridge 2).
On peut le v√©rifier en choisissant une petite valeur de k, par exemple, k = 0.01 ou k = 0.02.
Les r√©sultats pour chacune de ces deux valeurs sont donn√©es dans les tableaux ci-dessous
br1 br2 br3 br4 br0
Estimation 0.468614 0.353795 -0.514348 -0.547913 -0.098518
√©cart-type 0.23287 0.16324 0.13169 0.21075
t de Student 1.467 1.580 -2.848 -1.895
TAB. 13 ‚Äì R√©sultats de la r√©gression born√©e pour k = 0.01 (donn√©es ridge 2).
Il y a tr√©s peu de diff√©rences entre les estimations suivant les valeurs de k, mais les valeurs
sont bien plus stables pour k = 0.02. La colin√©arit√© entre les variables explicatives exerce ici
un effet sur les seules variances, et les estimations obtenues suivant le crit√®re des moindres
- 143 - RNTI-A-1
√âvaluation de la r√©gression born√©e
br1 br2 br3 br4 br0
Estimation 0.466987 0.344683 -0.505688 -0.542312 -0.098518
√©cart-type 0.14950 0.11658 0.10273 0.13704
t de Student 2.277 2.156 -3.589 -2.885
TAB. 14 ‚Äì R√©sultats de la r√©gression born√©e pour k = 0.02 (donn√©es ridge 2)
carr√©s ordinaires sont beaucoup plus proches des valeurs th√©oriques que les √©carts-types des
estimateurs ne l‚Äôindiquent.
6 Conclusion
Les coefficients de r√©gression th√©oriques de ces applications sont en r√©alit√© connus : le
mod√®le utilis√© pour cr√©er les donn√©es ridge1 et ridge 2 est celui qui a √©t√© pr√©cis√© dans le
paragraphe 3.2. Le coefficient de d√©termination est fix√© √† 0.5, et les coefficients de r√©gression
th√©oriques sont :
Œ≤0 = 0 Œ≤1 = 0.5 Œ≤2 = 0.5 Œ≤3 = -0.5 Œ≤4 = -0.5
Les estimations obtenues dans le premier cas par la r√©gression born√©e (donn√©es ridge 1) sont
beaucoup plus proches des valeurs th√©oriques que celles qui sont d√©duites du crit√®re des
moindres carr√©s ordinaires. Dans le second (donn√©es ridge 2), elles sont beaucoup plus stables.
Compte tenu des r√©sultats des simulations donn√©s dans le paragraphe 3, on pouvait s‚Äôy attendre.
L‚Äôint√©r√™t de la r√©gression born√©e appara√Æt finalement sur trois points :
1. Lorsque les coefficients estim√©s par le crit√®re des moindres carr√©s ordinaires sont tr√®s
diff√©rents des vraies valeurs, elle donne des estimations bien meilleures ;
2. elle permet de contr√¥ler la stabilit√© des estimations ;
3. lorsque les variables explicatives sont non corr√©l√©es, elle ne modifie quasiment pas les
estimations ;
4. La stabilit√© des r√©sultats par rapport √† la constante k limite l‚Äôimportance d‚Äôen rechercher
la meilleure valeur : une approximation m√™me grossi√®re, d√©duite simplement de la ridge
trace, donnera des r√©sultats en moyenne bien meilleurs que la r√©gression des moindres
carr√©s ordinaires. Par suite, en effectuant syst√©matiquement une r√©gression born√©e pour
une faible valeur de la constante k (par exemple k = 0.01) , les estimations des coef-
ficients de r√©gression ne peuvent √™tre que meilleures, m√™me lorsque la colin√©arit√© entre
les variables explicatives n‚Äôest pas tr√®s forte.
Toutefois, lorsque les valeurs th√©oriques des coefficients de r√©gression sont elles-m√™mes √©le-
v√©es en valeur absolue, la r√©gression born√©e est √† √©viter. C‚Äôest le cas par exemple lorsque les
coefficients de r√©gression th√©oriques sont √©gaux aux valeurs estim√©es sur les donn√©es ridge 1 :
la ridge trace est la m√™me, et par suite la r√©gression born√©e donne de tr√®s mauvais r√©sultats. Il
est donc indispensable d‚Äôanalyser a priori la taille des coefficients de r√©gression en suivant une
d√©marche critique.
Ces simulations montrent le danger d‚Äôinterpr√©ter le signe des coefficients de r√©gression sans
- 144 -RNTI-A-1
T. Foucart
pr√©caution. M√™me lorsque toutes les hypoth√®ses math√©matiques sont satisfaites (distribution
gaussienne de la variable r√©siduelle, lin√©arit√© des liaisons) ce qui est le cas dans les exemples
donn√©s puisqu‚Äôils sont construits √† partir de ces hypoth√®ses, il est tr√®s possible d‚Äôobtenir des
estimations tr√®s diff√©rentes des valeurs th√©oriques. Lorsque ces hypoth√®ses ne sont qu‚Äôapproxi-
mativement v√©rifi√©es, ce qui est le cas g√©n√©ral des donn√©es r√©elles, la statistique produit des
r√©sultats qu‚Äôil est indispensable d‚Äôexaminer avec prudence et de ne pas prendre pour certains
m√™me s‚Äôils sont largement significatifs.
R√©f√©rences
Belsley D.A., Kuh E., Welsh R.E. (1980). Regression diagnostics : identifying influential data
and sources of collinearity.Wiley, New York.
Ciarlet P.G. 1989). Introduction to Numerical Linear Algebra and Optimisation, London, Cam-
bridge University Press.
Foucart T. (1997). Numerical Analysis of a Correlation Matrix. Statistics, 29/4, p. 347-361.
Foucart T. (2000). Colin√©arit√© et Instabilit√© Num√©rique dans le Mod√®le Lin√©aire, RAIRO Ope-
rations research, Vol.34, 2, p. 199-212.
Hoerl A.E., R.W. Kennard (19701). Ridge regression : biased estimation for nonorthogonal
problems. Technometrics, 12, 55-67.
Hoerl A.E., R.W. Kennard (19702). Ridge regression : Applications to nonorthogonal pro-
blems. Technometrics, 12, 69-82.
Nordberg L., 1982. A procedure of determination of a good ridge parameter in linear regres-
sion. Commun, Statist. Simula. Computa. 11(3), p. 285-289.
Simiand F.,1932. Le salaire, l‚Äô√©volution sociale et la monnaie. Lien internet
http ://www.uqac.uquebec.ca/zone30/Classiques_des_sciences_sociales/index.html.
Tomassone R., Lesquoy E. et Millier C. (1992) : La r√©gression. Nouveaux regards sur une an-
cienne m√©thode statistique,Masson, Paris, 2e ed. .
Summary
The linear model is very frequently used in statistics and particularly in insurance, bank
and marketing. It makes it possible to determine the explanatory variables which play part
in the risk measured in the policy-holders and in the choices carried out by the customers.
The problem considered in this article appears when these variables are dependent statistically,
for example the income and the socio-professional group. Then, the estimates given by the
- 145 - RNTI-A-1
√âvaluation de la r√©gression born√©e
criterion of ordinary least squares become not very reliable and can take values in contradiction
with the real values. There are many methods adapted to this type of data. We propose here to
evaluate the effectiveness of the ridge regression while proceeding by simulations. The results
are clear: the gain in precision and in stability of the regression coefficients is impressive.
- 146 -RNTI-A-1
