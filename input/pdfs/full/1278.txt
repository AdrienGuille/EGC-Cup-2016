Une nouvelle approche de la programmation DC et DCA
pour la classification floue
Le Thi Hoai An‚àó, Le Hoai Minh‚àó‚àó, Pham Dinh Tao‚àó‚àó‚àó
‚àóLITA, UFR MIM, Universit√© Paul Verlaine - Metz, Ile du Saulcy, 57045 Metz Cedex, France
lethi@univ-metz.fr,
http://www.lita;sciences.univ-metz.fr/ lethi/
‚àó‚àóLITA, UFR MIM, Universit√© Paul Verlaine - Metz, Ile du Saulcy, 57045 Metz Cedex, France
lehoai@univ-metz.fr
‚àó‚àó‚àóLMI, INSA de Rouen,BP 08, Place Emile Blondel, 76131 Mont Saint Aignan Cedex, France
pham@insa-rouen.fr
R√©sum√©. Dans cet article, nous nous int√©ressons √† Fuzzy C-Means (FCM),
une technique tr√®s connue pour la classification floue. Nous proposons un
algorithme efficace bas√© sur la programmation DC (Difference of Convexe
functions) et DCA (DC Algorithm) pour r√©soudre ce probl√®me. Les exp√©ri-
ences num√©riques comparatives avec l‚Äôalgorithme standard FCM sur les don-
n√©es r√©elles montrent la robustesse, la performance de cet nouvel algorithme
DCA et sa sup√©riorit√© par rapport √† FCM.
1 Introduction
Le probl√®me de classification automatique (clustering) est consid√©r√© comme une des
probl√©matiques majeures en extraction des connaissances √† partir de donn√©es. Parmi les
techniques de classification, la classification floue (fuzzy) via Fuzzy C-Means (FCM) est
tr√®s connue. FCM a √©t√© introduite par Jim Bezdek en 1981 (Bezdek (1981)) comme une
am√©lioration des m√©thodes clustering pr√©c√©dentes, et a √©t√© beaucoup d√©velopp√©e dans les
ann√©es 90. Cette approche a √©t√© appliqu√©e avec succ√®s dans plusieurs probl√®mes (diagnostic
m√©dical (Whitwell (2005)) , classification de textes (Rodrigues and Sacks (2004))), et est de
plus en plus utilis√© dans le domaine du data mining.
Dans un travail r√©cent (Le Thi et al. 3 (2006)) nous avons formul√© le mod√®le de FCM
pour la classification floue sous la forme d‚Äôun programme DC (Difference of Convexe func-
tions) et d√©velopp√© un sch√©ma de DCA (DC Algorithm) pour sa r√©solution num√©rique. La
programmation DC et DCA ont √©t√© introduits par Pham Dinh Tao en 1985 et intensivement
d√©velopp√©s par Le Thi Hoai An et Pham Dinh Tao depuis 1994 (voir (Le Thi Hoai An (1997))
- (Le Thi et al. 2 (2006)), (Pham Dinh Tao and Le Thi Hoai An (1997)), (Pham Dinh Tao
and Le Thi Hoai An (1998)) et leurs r√©f√©rences) pour devenir maintenant classiques et de
plus en plus populaire. Ils ont √©t√© appliqu√©s avec succ√®s √† nombreux probl√®mes d‚Äôoptimisa-
tion non convexe diff√©rentiable ou non de grande dimension dans diff√©rents domaines des
sciences appliqu√©es, en particulier aux probl√®mes du data mining (voir par exemple (Le Thi
et al. 1 (2006)), (Le Thi et al. 2 (2006)), (Liu et al (2003)), (Neumann et al. (2004)), (Weber
et al. (2005))). Les r√©sultats num√©riques pr√©sent√©s dans (Le Thi et al. 3 (2006)) montrent
que, comme pour les autres probl√®mes d√©j√† trait√©s en data mining, DCA est efficace pour
FCM. Ils prouvent √©galement la superiorit√© de DCA par rapport √† K-means. Cet algorithme
est it√©ratif et consiste en la r√©solution d‚Äôun programme convexe √† chaque it√©ration. Le temps
de calculs de DCA est donc proportionnel √† celui de la m√©thode utilis√©e pour r√©soudre les
programmes convexes g√©n√©r√©s. Dans (Le Thi et al. 3 (2006)) l‚Äôalgorithme du gradient pro-
La programmation DC et DCA pour la classification floue
jet√© a √©t√© utilis√© du fait que la projection en question est explicite, m√™me s‚Äôil est connu pour
√™tre lent. Sans doute qu‚Äôavec d‚Äôautres d√©compositions DC on peut am√©liorer DCA pour la
r√©solution de FCM.
L‚Äôobjectif de ce travail est de d√©velopper un nouveau sch√©ma de DCA dans lequel la
r√©solution des probl√®mes convexes g√©n√©r√©s est moins co√ªteux. Nous proposons une autre
d√©compositionDC qui donne naissance √† un DCA tr√®s simple dont les calculs sont explicites
√† chaque it√©ration : le sous probl√®me convexe est en fait la projection d‚Äôun point sur une
boule. Les exp√©riences num√©riques comparatives entre FCM et l‚Äôalgorithme DCA √©tudi√©
dans (Le Thi et al. 3 (2006)) sur les donn√©es r√©elles montrent la robustesse, la performance
de cette nouvelle version de DCA et sa sup√©riorit√© par rapport √† FCM.
Le papier est organis√© de la fa√ßon suivante. Dans la deuxi√®me section, nous pr√©sentons
la formulation du probl√®me FCM. La r√©solution de ce prob√®me par la programmation DC
et DCA est √©tudi√©e dans la troisi√®me section. Finalement, les r√©sultats num√©riques de nos
algorithmes DCA et FCM sont rapport√©s dans la derni√®re section.
2 Une nouvelle formulation du mod√®le de FCM
Soit X := {x1, x2, ..., xn} l‚Äôensemble de n points √† classer. Chaque point xi est un
vecteur dans l‚Äôespace IRp. Nous avons √† classer ces n points dans c (2 ‚â§ c ‚â§ n) classes
diff√©rentes.
Consid√©rons une matrice de pourcentage U de taille (c√ón) dont chaque √©l√©ment ui,k d√©finit
le pourcentage d‚Äôappartenance d‚Äôun point xk √† la classe Ci. Il est clair que
ui,k ‚àà [0, 1] pour i = 1...c, k = 1...n ;
c‚àë
i=1
ui,k = 1, pour k = 1...n. (1)
Si la matrice de pourcentage U est d√©termin√©e, on en d√©duit la classification selon la r√®gle
suivante : le point xk (pour k = 1, . . . , n) est class√© dans la classe Ci (pour i = 1, . . . , c) si
et seulement si
ui,k = max{uj,k : j ‚àà {1, . . . , c}}.
Consid√©rons la fonction Jm d√©finie par :
Jm(U, V ) =
n‚àë
k=1
c‚àë
i=1
umi,k||xk ‚àí vi||2, (2)
o√π ‚Äñ.‚Äñ d√©signe, dans tout le papier, la norme Euclidienne de l‚Äôespace correspondant, V est
une (c√ó p) - matrice dont chaque ligne vi correspond au centre de la classe Ci, etm ‚â• 1 un
param√®tre entier qui d√©finit le degr√© de flou du mod√®le.
Chercher une classification revient ainsi chercher la matrice de pourcentage U et les centres
vi. Le mod√®le math√©matique de FCM s‚Äô√©crit ainsi :Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£≥
minJm(U, V ) :=
n‚àë
k=1
c‚àë
i=1
umi,k||xk ‚àí vi||2
sous contraintes ui,k ‚àà [0, 1] pour i = 1, ..., c, k = 1, ..., n
c‚àë
i=1
ui,k = 1, k = 1, ..., n
(3)
o√π seule la variable U est a priori born√©e. En fait on peut aussi restreindre la variable V √†
un domaine born√©. En effet, la condition n√©cessaire d‚Äôoptimalit√© du premier ordre en (U, V )
implique
‚àáV Jm(U, V ) = 0,
Le Thi Hoai An et al.
i.e.,
‚àÇviJm(U, V ) =
n‚àë
k=1
umi,k2(vi ‚àí xk) pour i = 1, ..., c, k = 1, ..., n
ou
vi
n‚àë
k=1
umi,k =
n‚àë
k=1
umi,kxk.
D‚Äôautre part, la non-vacuit√© des classes assure que
n‚àë
k=1
umi,k > 0, pour tout i = 1, ..., c.
Par suite
‚Äñvi‚Äñ2 ‚â§
(
n‚àë
k=1
umi,kmaxk=1,...,n ‚Äñxk‚Äñ
)2
(
n‚àë
k=1
umi,k)2
= max2k=1,...,n‚Äñxk‚Äñ := r2.
Consid√©rons les nouvelles variables ti,k telles que ui,k = t2i,k. La contrainte
c‚àë
i=1
ui,k = 1
devient
c‚àë
i=1
t2i,k = 1 ou ‚Äñtk‚Äñ2 = 1 avec tk ‚àà IRc. Soient Sk la sph√®re de rayon 1 dans IRc et
Ri la boule Euclidienne de rayon r dans IRp, on peut reformuler le probl√®me FCM comme :Ô£±Ô£≤Ô£≥minJ2m(T, V ) :=
n‚àë
k=1
c‚àë
i=1
t2mi,k ||xk ‚àí vi||2
sous contraintes T ‚àà S := Œ†nk=1 Sk, V ‚àà C := Œ†ci=1Ri
. (4)
Ce dernier est un probl√®me d‚Äôoptimisation non convexe dont la r√©solution sera d√©crite dans
la suite.
Remarque. Le changement de variables en ti,k nous am√®ne √† travailler sur S, le domaine
r√©alisable des variables ti,k qui est le produit des sph√®res et non le produit des simplexes
comme dans le cas des variables initiales ui,k. On pourrait penser alors que la non convexit√©
de S rend le probl√®me plus difficile qu‚Äôavec sa formulation initiale. Mais ce n‚Äôest pas le cas,
comme on verra dans la suite, car le probl√®me (4) sera reformul√© sous une forme √©quivalente
o√π S est remplac√© par le produit des boules de rayon 1. Ce qui est int√©ressant, tant sur le
plan algorithmique que num√©rique : la nouvelle formulation DC de (4) donne naissance √†
un sch√©ma DCA extr√™mement simple qui ne n√©cessite que des calculs explicites et donc non
co√ªteux. Puisqu‚Äôil s‚Äôagit des calculs des projections d‚Äôun point sur une boule Euclidienne √†
chaque it√©ration.
3 La programmation DC et DCA pour la r√©solution de FCM
Pour faciliter la compr√©hension de notre approche, nous pr√©sentons, en premier lieu de
cette section, une br√®ve description de la programmation DC et DCA.
3.1 Introduction √† la programmation DC et DCA
La programmation DC joue un r√¥le central en programmation non convexe (diff√©ren-
tiable ou non) car la quasi totalit√© des probl√®mes d‚Äôoptimisation de la vie courante est de
nature DC. Elle conna√Æt des d√©veloppements spectaculaires au cours de cette derni√®re d√©cen-
nie. DCA est une m√©thode de descente (de type primal-dual sans recherche lin√©aire) pour la
r√©solution d‚Äôun programme DC de la forme
Œ± := inf{f(x) := g(x)‚àí h(x) : x ‚àà IRp}, (5)
La programmation DC et DCA pour la classification floue
o√π g, h sont les fonctions convexes semi-continues inf√©rieurement et propres sur IRp. Une
telle fonction f est appel√©e fonction DC, et les fonctions convexes g et h les composantes
DC de f . Il est √† noter que la minimisation d‚Äôune fonction DC sur un ensemble convexe
ferm√© C de IRp se ram√®ne √† un probl√®me de type (5) car la contrainte x ‚àà C peut √™tre
incorpor√©e dans la fonction objectif √† l‚Äôaide de la fonction indicatrice œáC d√©finie par œáC = 0
si x ‚àà C, +‚àû sinon. Lorsqu‚Äôune de ses composantes DC est poly√©drale la fonction f est
dite DC poly√©drale et le programme DC correspondant DC poly√©dral. La programmation
DC poly√©drale joue un r√¥le crucial en programmation non convexe.
La congugaison d‚Äôune fonction convexe g, not√©e g‚àó est d√©finie par
g‚àó(y) := sup{„Äàx, y„Äâ ‚àí g(x) : x ‚àà IRp}.
La dualit√© DC est d√©finie via la conjugaison des composantes DC et le programme dual de
(5) est donn√© par (ici l‚Äôespace dual de IRp est identifi√© √† lui-m√™me) :
Œ±D := inf{h‚àó(y) ‚àí g‚àó(y) : y ‚àà IRp}. (6)
Puisque chaque fonction h ‚àà Œì0(IRp) est caract√©ris√©e comme le supremum d‚Äôune famille
finie des fonctions affines, c.√†.d.
h(x) := sup{„Äàx, y„Äâ ‚àí h‚àó(y) : y ‚àà IRp},
on a
Œ± = inf{g(x)‚àí sup{„Äàx, y„Äâ ‚àí h‚àó(y) : y ‚àà IRp} : x ‚àà IRp} = inf{Œ±(y) : y ‚àà IRp},
o√π
Œ±(y) := inf{g(x)‚àí [„Äàx, y„Äâ ‚àí h‚àó(y)] : x ‚àà IRp} (Py).
Il est clair que (Py) est un programme convexe et
Œ±(y) = h‚àó(y) ‚àí g‚àó(y) si y ‚àà dom h‚àó, et +‚àû sinon. (7)
Par suite
Œ± = inf{h‚àó(y) ‚àí g‚àó(y) : y ‚àà dom h‚àó}.
Finallement on obtient, avec la convention naturelle +‚àû‚àí (+‚àû) = +‚àû :
Œ± = Œ±D := inf{h‚àó(y) ‚àí g‚àó(y) : y ‚àà IRp}.
On observe ainsi la sym√©trie parfaite entre les programmes DC primal et dual : le dual de (6)
est exactement (5).
Le transport des solutions optimales globales entre l‚Äôensemble des solutions optimales P
de (5) et celui de (6) not√© D s‚Äôexprime de la mani√®re suivante ((Le Thi Hoai An and Pham
Dinh Tao (2005)), (Pham Dinh Tao and Le Thi Hoai An (1997))) :
‚à™{‚àÇh(x‚àó) : x‚àó ‚àà P} ‚äÇ D et ‚à™ {‚àÇg‚àó(y‚àó) : y‚àó ‚àà D} ‚äÇ P. (8)
La relation (8) indique que la r√©solution d‚Äôun programme DC implique celle de son dual.
D‚Äôautre part, ce transport reste valable entre les ensembles des solutions locales de (5) et (6)
sous certaines hypoth√®ses techniques.
En analyse convexe, (Rockafellar (1976)),(Urruty and Lemarechal (1993))
‚àÇh(x0) := {y ‚àà IRp : h(x) ‚â• h(x0) + „Äàx‚àí x0, y„Äâ, ‚àÄx ‚àà IRp}
Le Thi Hoai An et al.
est appel√© le sous-diff√©rentiel de h au point x0. Tout √©l√©ment de ‚àÇh(x0) est appel√© gradient
de h en x0. Le sous-diff√©rentiel ‚àÇh(x0) est une partie convexe ferm√©e qui coincide avec
le gradient ‚àáh(x0) si et seulement h est diff√©rentiable en x0. Pour un  > 0, le ‚àí sous-
diff√©rentiel de h est d√©fini par
‚àÇh(x0) := {y ‚àà IRp : h(x) ‚â• h(x0) + „Äàx‚àí x0, y„Äâ ‚àí , ‚àÄx ‚àà IRp}.
L‚Äô√©galit√© des valeurs optimales des programmes primal et dual (5) et (6) peut √™tre traduite
de mani√®re √©quivalente par
P = {x‚àó : ‚àÇh(x‚àó) ‚äÇ ‚àÇg(x‚àó), ‚àÄ > 0} .
Mais sauf des cas tr√®s rares, cette condition d‚Äôoptimalit√© globale est impraticable. Nous nous
int√©ressons d√®s lors aux conditions d‚Äôoptimalit√© locale pour les programmes DC (voir (Le
Thi Hoai An (1997)), (Le Thi Hoai An and Pham Dinh Tao (2005)), (Pham Dinh Tao and
Le Thi Hoai An (1997)), (Pham Dinh Tao and Le Thi Hoai An (1998)),(Le Thi Hoai An and
Pham Dinh Tao (2003)) et r√©f√©rences inclues) :
‚àÇh(x‚àó) ‚äÇ ‚àÇg(x‚àó), (9)
et
‚àÇh(x‚àó) ‚à© ‚àÇg(x‚àó) 6= ‚àÖ. (10)
(Un tel point x‚àó v√©rifiant (10) est appel√© point critique de g ‚àí h).
La condition n√©cessaire d‚Äôoptimalit√© locale (9) est √©galement suffisante dans plusieurs
cas rencontr√©s en pratique - par exemple, quand la fonction objectif f := g‚àíh est DC poly√©-
drale avec h poly√©drale, ou quand f est localement convexe en x‚àó.
Bas√© sur les conditions d‚Äôoptimalit√© locale et la dualit√© DC, DCA consiste en la con-
struction de deux suites {xk} et {yk}, candidats respectifs aux solutions des probl√®mes
primal et dual que l‚Äôon am√©liore √† chaque it√©ration (les deux suites {g(xk) ‚àí h(xk)} et
{h‚àó(yk)‚àí g‚àó(yk)} sont d√©croissantes) et qui convergent vers des solutions primale et duale
x‚àó et y‚àó v√©rifiant des conditions d‚Äôoptimalit√© locale. Le sch√©ma g√©n√©ral de DCA prend la
forme :
yk ‚àà ‚àÇh(xk); xk+1 ‚àà ‚àÇg‚àó(yk). (11)
La premi√®re interpr√©tation de DCA est simple : √† chaque it√©ration on remplace dans le
programme DC primal la deuxi√®me composante DC h par sa minorante affine hk(x) :=
h(xk) + „Äàx‚àí xk, yk„Äâ au voisinage de xk pour obtenir le programme convexe suivant
inf{g(x)‚àí hk(x) : x ‚àà IRp} (12)
dont l‚Äôensemble des solutions optimales n‚Äôest autre que ‚àÇg‚àó(yk).
De mani√®re analogue, la deuxi√®me composante DC g‚àó du programme DC dual (6) est
remplac√©e par sa minorante affine (g‚àó)k(y) := g‚àó(yk) + „Äày ‚àí yk, xk+1„Äâ au voisinage de yk
pour donner naissance au programme convexe
inf{h‚àó(y) ‚àí (g‚àó)k(y) : y ‚àà IRp} (13)
dont ‚àÇh(xk+1) est l‚Äôensemble des solutions optimales. DCA op√®re ainsi une double lin√©ari-
sation √† l‚Äôaide des sous-gradients de h et g‚àó. Il est √† noter que DCA travaille avec les com-
posantes DC g et h et non pas avec la fonction f elle-m√™me. Chaque d√©composition DC de
f donne naissance √† un DCA. Pour un programme DC donn√©, la question de d√©composition
DC optimale reste ouverte, en pratique on cherche des d√©compositions DC bien adapt√©es
√† la structure sp√©cifiques du programme DC √©tudi√© pour lesquelles les suites {xk} et {yk}
La programmation DC et DCA pour la classification floue
sont faciles √† calculer, (si possible) explicites pour que les DCA correspondants soient moins
co√ªteux en temps et par cons√©quent capables de supporter de tr√®s grandes dimensions.
La convergence de DCA : ((Le Thi Hoai An and Pham Dinh Tao (1997)), (Le Thi Hoai
An and Pham Dinh Tao (2005)), (Pham Dinh Tao and Le Thi Hoai An (1997)), (Pham
Dinh Tao and Le Thi Hoai An (1998)), (Le Thi Hoai An and Pham Dinh Tao (2003)))
Soient C (resp. D) l‚Äôensemble convexe qui contient la suite {xk} (resp. {yk}) et œÅ(g, C)
(ou œÅ(g) si C = IRp) d√©fini par
œÅ(g, C) = sup
{
œÅ ‚â• 0 : g ‚àí œÅ
2
‚Äñ ¬∑ ‚Äñ2 soit convexe sur C
}
.
DCA est une m√©thodes de descente sans recherche lin√©aire, qui poss√®de les propri√©t√©s
suivantes :
i) Les suites {g(xk) ‚àí h(xk)} et {h‚àó(yk) ‚àí g‚àó(yk)} sont d√©croisantes et
‚Ä¢ g(xk+1)‚àí h(xk+1) = g(xk) ‚àí h(xk) ssi
yk ‚àà ‚àÇg(xk) ‚à© ‚àÇh(xk), yk ‚àà ‚àÇg(xk+1) ‚à© ‚àÇh(xk+1) et [œÅ(g, C) + œÅ(h,C)]‚Äñxk+1 ‚àí
xk‚Äñ = 0. De plus, si g ou h est strictement convexe sur C alors xk = xk+1.
Dans ce cas DCA se termine √† l‚Äôit√©ration k (convergence finie de DCA).
‚Ä¢ h‚àó(yk+1) ‚àí g‚àó(yk+1) = h‚àó(yk) ‚àí g‚àó(yk) ssi xk+1 ‚àà ‚àÇg‚àó(yk) ‚à© ‚àÇh‚àó(yk), xk+1 ‚àà
‚àÇg‚àó(yk+1)‚à© ‚àÇh‚àó(yk+1) et [œÅ(g‚àó, D) + œÅ(h‚àó, D)]‚Äñyk+1 ‚àí yk‚Äñ = 0. De plus, si g‚àó ou
h‚àó est strictement convexe sur D alors yk+1 = yk.
Dans ce cas DCA se termine √† l‚Äôit√©ration k (convergence finie de DCA).
ii) Si œÅ(g, C) + œÅ(h,C) > 0 (resp œÅ(g‚àó, D) + œÅ(h‚àó, D) > 0) alors la s√©rie {‚Äñxk+1 ‚àí xk‚Äñ2
(resp. {‚Äñyk+1 ‚àí yk‚Äñ2} converge.
iii) Si la valeur optimale Œ± du probl√®me (5) est finie et deux suites {xk} et {yk} sont born√©es
alors tout valeur d‚Äôadh√©rence xÀú (resp. yÀú) de la suite {xk} (resp. {yk}) est le point critique de
g ‚àí h (resp. h‚àó ‚àí g‚àó).
iv) DCA a la convergence lin√©aire pour les programmes DC g√©n√©raux.
v) DCA a la convergence finie pour les programmes DC poly√©draux.
Pour une √©tude compl√®te de la programmation DC et DCA, se r√©f√©rer aux (Le Thi Hoai
An (1997)) - (Le Thi Hoai An and Pham Dinh Tao (2005)), (Pham Dinh Tao and Le Thi
Hoai An (1997)), (Pham Dinh Tao and Le Thi Hoai An (1998)) et r√©f√©rences incluses. Il est
√† noter que la recherche d‚Äôune d√©composition DC ad√©quate et celle d‚Äôun bon point initial
sont deux t√¢ches importantes dans la r√©solution d‚Äôun programme non convexe par DCA car
elles conditionnent la r√©ussite du r√©sultant DCA.
3.2 Nouvelle formulation DC de FCM
Dans toute la suite nous utilisons la pr√©sentation matricielle qui nous semble plus co-
mode, sachant que l‚Äôon peut identifier une matrice et un vecteur (par ligne ou par colonne).
La fonction objectif de (4) peut s‚Äô√©crire de la mani√®re suivante :
J2m(T, V ) =
œÅ
2
‚ÄñT‚Äñ2 + œÅ
2
‚ÄñV ‚Äñ2 ‚àí
[œÅ
2
‚Äñ(T, V )‚Äñ2 ‚àí J2m(T, V )
]
.
Pour tout (T, V ) ‚àà S √ó C on a
J2m(T, V ) =
œÅ
2
n+
œÅ
2
‚ÄñV ‚Äñ2 ‚àíH(T, V ).
avec
H(T, V ) = œÅ2‚Äñ(T, V )‚Äñ2 ‚àí J2m(T, V ) (14)
Dans le lemme suivant nous donnerons les conditions pour que la fonction H soit convexe.
Le Thi Hoai An et al.
Lemme : soit B := Œ†nk=1 Bk, o√π Bk est la boule de centre 0 et de rayon 1 dans IRc. La
fonctionH(U, V ) est convexe sur B √ó C pour toute valeur de œÅ telle que
œÅ ‚â• m
n
(2m ‚àí 1)Œ±2 + 1 +
‚àö[m
n
(2m ‚àí 1)Œ±2 + 1
]2
+
16
n
m2Œ±2,
o√π
Œ± = r + max
1‚â§k‚â§n
‚Äñxk‚Äñ .
Preuve : on remarque tout d‚Äôabord que œÅ > 0 carm ‚â• 1.
Puisque
H(T, V ) =
n‚àë
k=1
c‚àë
i=1
[œÅ
2
t2i,k +
œÅ
2
‚Äñvi‚Äñ2 ‚àí t2mi,k ‚Äñxk ‚àí vi‚Äñ2
]
,
H est convexe si toutes les fonctions
hi,k(ti,k, vi) :=
œÅ
2
t2i,k +
œÅ
2
‚Äñvi‚Äñ2 ‚àí t2mi,k ‚Äñxk ‚àí vi‚Äñ2, i = 1, ...c, k = 1, ...n
sont convexes.
Consid√©rons la fonction suivante :
f : IR√ó IR‚Üí IR
f(x, y) = œÅ2x
2 + œÅ2y
2 ‚àí x2my2 . (15)
Le Hessien de la fonction f est donn√© par :
J(x, y) =
(
œÅ ‚àí 2m(2m ‚àí 1)y2x2m‚àí2‚àí4mx2m‚àí1y
‚àí4mx2m‚àí1y œÅ
n
‚àí 2x2m
)
. (16)
Pour tout (x, y) : 0 ‚â§ x ‚â§ 1; ‚Äñy‚Äñ ‚â§ Œ±, on a :
| J(x, y) |= (œÅ ‚àí 2m(2m‚àí 1)y2x2m‚àí2) ( œÅn ‚àí 2x2m) ‚àí 16m2x4m‚àí2y2
‚â• œÅn2 ‚àí
[
2mn (2m‚àí 1)y2x2m‚àí2 + 2x2m
]
œÅ ‚àí 16m2x4m‚àí2y2
‚â• 1nœÅ2 ‚àí 2
(
m
n (2m‚àí 1)Œ±2 + 1
)
œÅ ‚àí 16m2Œ±2
.
Par suite, si
œÅ ‚â• m
n
(2m‚àí 1)Œ±2 + 1 +
‚àö[m
n
(2m‚àí 1)Œ±2 + 1
]2
+
16
n
m2Œ±2 (17)
alors | J(x, y) |‚â• 0, pour tout (x, y) ‚àà IR2 tels que 0 ‚â§ x ‚â§ 1, | y |‚â§ Œ±.
Ainsi avec œÅ d√©fini par (17), la fonction f est convexe sur [0, 1]√ó [‚àíŒ±, Œ±]. Par cons√©quent
les fonctions
Œ∏i,k(ti,k, vi) :=
œÅ
2
t2i,k +
œÅ
2
‚Äñxk ‚àí vi‚Äñ2 ‚àí t2mi,k ‚Äñxk ‚àí vi‚Äñ2
sont convexes sur {0 ‚â§ ti,k ‚â§ 1, ‚Äñvi‚Äñ ‚â§ r} avec œÅ donn√© dans (17) et
Œ± = r +max1‚â§k‚â§n ‚Äñxk‚Äñ .
Il en est de m√™me pour les fonction hi,k, car
hi,k(ti,k, vi) = Œ∏i,k(ti,k, vi) + œÅ„Äàxk, vi„Äâ ‚àí œÅ2 ‚Äñxk‚Äñ
2
.
Ainsi, avec les valeurs donn√©es ci-dessus de œÅ et Œ±, la fonction H(T, V ) est convexe sur
B √ó C. Dans toute la suite nous travaillons avec ces valeurs de œÅ et Œ±.
La programmation DC et DCA pour la classification floue
Il est clair que, pour tout T ‚àà B et un V ‚àà C fix√©, la fonction J2m(T, V ) est concave en
variable T (carH(T, V ) est convexe), par suite son minimum sur B est atteint sur la fronti√®re
S de B,i.e.,
min
{
œÅ
2 ‚ÄñV ‚Äñ2 ‚àíH(T, V ) : (T, V ) ‚àà B √ó C
}
= min
{
œÅ
2 ‚ÄñV ‚Äñ2 ‚àíH(T, V ) : (T, V ) ‚àà S √ó C
}
.
Le probl√®me (4) peut √™tre alors reformul√© comme
min
{œÅ
2
‚ÄñV ‚Äñ2 ‚àíH(T, V ) : (T, V ) ‚àà B √ó C
}
,
ou encore
min
{
œáB√óC(T, V ) +
œÅ
2
‚ÄñV ‚Äñ2 ‚àíH(T, V ) : (T, V ) ‚àà IRc√ón √ó IRc√óp
}
(18)
qui est un programme DC avec la d√©composition DC suivante :
œáB√óC(T, V ) +
œÅ
2
‚ÄñV ‚Äñ2 ‚àíH(T, V ) := G(T, V )‚àíH(T, V ),
o√π
G(T, V ) := œáB√óC(T, V ) + œÅ2 ‚ÄñV ‚Äñ2 (19)
est bien √©videmment une fonction convexe gr√¢ce √† la convexit√© de B et C.
3.3 R√©solution de (18) par DCA
Selon la description de DCA dans la section 2.1, la r√©solution de FCM via la formula-
tion (18) par DCA consiste en la d√©termination de deux suites (Y l, Zl) ‚àà ‚àÇH (T l, V l) et
(T l+1, V l+1) ‚àà ‚àÇG‚àó(Y l, Zl).
La fonction H est diff√©rentiable et son gradient au point (T l, V l) est calcul√© de la
mani√®re suivante :
‚àáH(T l, V l) = œÅ(T l, V l)‚àí (2mt2m‚àí1i,k ‚Äñxk ‚àí vi‚Äñ2, 2
n‚àë
k=1
(vi ‚àí xk)t2mi,k ). (20)
Le calcul de (T l+1, V l+1) ‚àà ‚àÇG‚àó(Y l, Zl) se ram√®ne √† la r√©solution du probl√®me suivant
(voir Section 2.1)
min
{œÅ
2
‚ÄñV ‚Äñ2 ‚àí „Äà(T, V ), (Y l, Zl)„Äâ : (T, V ) ‚àà B √ó C
}
.
Il s‚Äôen suit que (Proj √©tant l‚Äôapplication de projection)
T l+1 = Pr ojB(Y l), V l+1 = Pr ojC(
1
œÅ
Zl).
Plus pr√©cis√©ment :
V l+1i,. =
Ô£±Ô£≤Ô£≥
(Zl)i,.
œÅ
si ‚Äñ(Zl)i,.‚Äñ ‚â§ œÅr
(Zl)i,.r
‚Äñ(Zl)i,.‚Äñ sinon
, i = 1, ...c, (21)
et
T l+1.,k =
{
Y l.,k si ‚ÄñY l.,k‚Äñ ‚â§ 1
(Y l).,k
‚Äñ(Y l).,k‚Äñ sinon
, k = 1, ...n. (22)
Le Thi Hoai An et al.
3.3.1 Sch√©ma DCA
Initialisation :
‚Äì Choisir T 0 ‚àà IRc√ón et V 0 ‚àà IRc√óp. Soit l = 0.
‚Äì Choisir une tol√©rance  > 0.
R√©peter
‚Äì Calculer (Y l, Zl) ‚àà ‚àáH(T l, V l) √† l‚Äôaide de (20) ;
‚Äì Calculer (T l+1, V l+1) ‚àà ‚àÇG‚àó(Y l, Zl) √† l‚Äôaide de (21) et (22) ;
‚Äì l + 1‚Üê‚àí l
Jusqu‚Äô√† ‚Äñ(T l+1, V l+1) ‚àí (U l, V l)‚Äñ ‚â§ (‚Äñ(T l+1, V l+1)‚Äñ).
Construction des classes Soient (T ‚àó, V ‚àó) la solution calcul√©e par DCA et ui,k = t‚àó2i,k. Le
point xk appartient √† la classe Ci si uik = max
j=1..c
uj,k.
4 Exp√©riences num√©riques
Pour comparer la performance de notre algorithme, nous avons r√©alis√© les tests
num√©riques sur deux ensembles des donn√©es : le premier ensemble de donn√©es contient 4
exemples tr√®s connus et beaucoup utilis√©s dans le domaine de classification pour l‚Äô√©valua-
tion des algorithmes :
‚Äì PAPILLON : un jeu de donn√©es connu sous le nom "jeux de papillon".
‚Äì IRIS : IRIS est peut-√™tre le plus connu jeu de test dans le domaine de classification. Il
contient 3 classes, chacune a 50 objets.
‚Äì VOTE : Congressional Votes dataset (Congressional Quarterly Almanac, 98th
Congress, 2nd session 1984, Volume XL : Congressional Quarterly Inc. Washington,
D.C., 1985).
‚Äì GENE : L‚Äôensemble de 384 g√®nes disponible sur
http ://faculty.washington.edu/kayee/cluster/
‚Äì ADN : L‚Äôensemble de 3186 g√®nes disponible sur ftp ://genbank.bio.net, chaque g√®ne
est pr√©sent√© par une s√©quence de 60 √©l√©ments. Ces g√®nes sont class√©s dans 3 clusters
diff√©rents : donors (767 objets), acceptors (765 objets) et le rest.
Le deuxi√®me ensemble de donn√©es est compos√© de deux jeux de donn√©es de biopuces :
‚ÄùYeast‚Äù et ‚ÄùSerum‚Äù t√©l√©chargeables sur
http ://genomics.stanford.edu/.
‚ÄùYeast‚Äù : 2945 points (g√®nes) dans l‚Äôespace de dimension 15 ;
‚ÄùSerum‚Äù : 517 points dans l‚Äôespace de dimension 12 (voir (Dembele et al. (2003)) pour
la description de ces donn√©es).
Les tests ont √©t√© r√©alis√©s sur un ordinateur de 2.8MHz, 512Mb Ram. La valeur de  est
fix√©e √† 10‚àí7. La valeur de m est √©gale √† 2 pour le premier ensemble de donn√©es. Pour les
donn√©es de biopuces nous consid√©rons diff√©rentes valeurs de m dans l‚Äôintervalle (1, 2) (il a
√©t√© prouv√© dans (Dembele et al. (2003)) que le choix de m = 2 n‚Äôest pas convenable √† ces
donn√©es).
Dans le Tableau 1, nous comparons notre nouvelle m√©thode DCA (DCA2) avec l‚Äôalgo-
rithme DCA d√©velopp√© dans (Le Thi et al. 3 (2006)) (DCA1) et une impl√©mentation de la
m√©thode FCM (t√©l√©chargeable sur http ://www-igbmc.u-strasbg.fr/projets/fcm/. Les crit√®res
de comparaison sont : le temp de calcul en seconds (Time), le nombre d‚Äôiterations (Noit) et
POBC (Pourcentage de Objets Bien-Class√©s). Nous constatons que dans tous les 4 jeux de
donn√©es, DCA2 donne toujours le meilleur r√©sultat.
La programmation DC et DCA pour la classification floue
Dans les Tableaux 2 et 3, nous pr√©sentons les r√©sultats comparatifs de ces trois m√©thodes
sur les donn√©es biopuces. Les crit√®res de comparaison sont : la valeur de Jm(U, V ) (Jm), le
co√ªt du cluster (CC), soit‚àëni=1mink=1...c ‚Äñxi ‚àí vk‚Äñ2, le temps de calcul en seconds (Time)
et le nombre d‚Äôit√©ration (Noit).
Data DCA1 DCA2 FCM
Name n p c Noit Time POBC Noit Time POMC Noit Time POBC
PAPILLON 23 4 4 10 0.002 95.7 2 0.001 95.7 18 0.002 95.7
IRIS 150 4 3 23 0.03 92.77 4 0.01 92.77 15 0.03 92.25
VOTE 435 2 2 16 0.05 89.9 4 0.01 92.6 19 0.06 83.7
GENE 384 17 5 16 0.67 88.3 7 0.20 88.9 35 0.73 85.8
ADN 3186 60 3 8 0.78 92 6 0.55 94 25 1.95 89.8
Tableau 1 : R√©sultats comparatifs du premier ensemble de donn√©es
DCA1 DCA2 FCM
m Jm CC Noit Time Jm CC Noit Time Jm CC Noit Time
1.1 23115 64831 33 331 21615 64061 189 33 21712 65868 179 564
1.3 17554 64144 357 64 16789 62681 225 99 16819 62681 543 1886
1.5 10531 43398 54 301 10432 43389 543 167 10652 44367 143 269
1.7 64129 44981 47 197 6126 43792 102 43 6294 43939 44 110
1.9 3676 45012 65 84 3497 43956 101 35 3607 44643 32 77
Tableau 2 : R√©sultats comparatifs de donn√©es de biopuces ‚ÄùYeast‚Äù.
DCA1 DCA2 FCM
m Jm CC Noit Time Jm CC Noit Time Jm CC Noit Time
1.1 1511.11 12237 56 2.3 1511.11 12234 72 1.02 1511.56 13265 198 1.98
1.3 1523.6 9572 78 5.4 1478.6 9572 102 3.4 1554.50 10231 176 5.3
1.5 1645 8642 52 6.7 1586 8034 543 12 1755 9432 69 3.1
1.7 1404.5 6034 41 4.1 1404.5 6021 102 3.2 1415.3 6068 29 1.4
1.9 935 6079 85 2.2 926 6022 101 1.2 935 6079 14 0.7
Tableau 3 : R√©sultats comparatifs de donn√©es de biopuces ‚ÄùSerum‚Äù.
Conclusion. Nous avons introduit une nouvelle formulation DC du mod√®le de FCM pour
la classification floue et d√©velopp√© un sch√©ma de DCA pour sa r√©solution num√©rique. Avec
cette d√©composition DC notre algorithme it√©ratif est extr√™mement simple, il consiste en la
d√©termination de la projection d‚Äôun point sur une boule Euclidienne, qui est explicite et
non co√ªteux. Les r√©sultats num√©riques montrent que, comme pour les autres probl√®mes d√©j√†
trait√©s en data mining, DCA est efficace pour FCM. Ils prouvent la superiorit√© de DCA non
seulement par rapport √† l‚Äôalgorithme FCM standard mais aussi par rapport au sch√©ma de
DCA propos√© dans (Le Thi et al. 3 (2006)). Dans l‚Äô√©tape suivante nous devront exploiter
cet algorithme pour la classification des donn√©es biologiques, en particulier des donn√©√©es de
biopuces.
References
Bezdek (1981). Pattern Recognition with Fuzzy Objective Function Algorithm. New
York, NY. Plenum Press. 1981
Demb√©l√©, D., Kastner, P.. Fuzzy C-means Clustering method for clustering microarray
data. Bioinformatics. Vol. 19, No 8, pp 573-580, 2003 .
F. H√∂ppner, F. Klawonn. Obtaining Interpretable Fuzzy Models from Fuzzy Clustering
and Fuzzy Regression. Proc. of the 4th Int. Conf. on Knowledge-Based Intelligent Engi-
neering Systems and Allied Technologies (KES), Brighton, UK, pp. 162-165, 2000.
Le Thi Hoai An et al.
F. H√∂ppner, F. Klawonn. Fuzzy Clusteringof Sampled Functions. Proc. of the 19th Int.
Conf. of the North American Fuzzy Information Processing Society (NAFIPS), Atlanta,
USA, pp. 251-255, 2000.
F. Klawonn, F. H√∂ppner. What is Fuzzy About Fuzzy Clustering? ‚Äì Understanding and
Improving the Concept of the Fuzzifier. Advances in Intelligent Data Analysis , Berlin
(2003), 254-264, Springer.
J. Neumann, C. Schn√∂rr, G. Steidl. SVM-based Feature Selection by Direct Objective
Minimisation. Pattern Recognition, Proc. of 26th DAGM Symposium, pp. 212 - 219,
LNCS Volume 3175, 2004.
Le Thi Hoai An. Contribution √† l‚Äôoptimisation non convexe et l‚Äôoptimisation globale :
Th√©orie, Algorithmes et Applications. Habilitation √† Diriger des Recherches, Universit√©
de Rouen, (1997).
Le Thi Hoai An and Pham Dinh Tao. Solving a class of linearly constrained indefinite
quadratic problems by DC algorithms. Journal of Global Optimization, Vol 11, No 3, pp
253-285, 1997.
Le Thi Hoai An and Pham Dinh Tao. Large Scale Molecular Optimization from distances
matrices by a DCOptimization approach. SIAM J. Optimization,Vol. 14. No1, pp. 77-117,
2003.
Le Thi Hoai An and Pham Dinh Tao. The DC (difference of convex functions) Program-
ming and DCA revisited withDCmodels of real world nonconvex optimization problems.
Annals of Operations Research 2005, Vol 133, pp. 23-46.
Le Thi Hoai An, T. Belghiti and Pham Dinh Tao. A new efficient algorithm based on DC
programming and DCA for Clustering. In Press, Available July 2006, Journal of Global
Optimization .
Le Thi Hoai An, Le Hoai Minh and Pham Dinh Tao. Optimization based DC program-
ming and DCA for Hierarchical Clustering. n Press, Available online June 2006, Euro-
pean Journal of Operational Research.
Le Thi Hoai An, Le Hoai Minh, Pham Dinh Tao. Une approche de la programmation DC
pour la Classification floue. Actes de XIII√®me Rencontres de la Soci√©t√© Francophone de
Classification SFC‚Äô06, Metz 6-9 Septembre, 2006 .
Pham Dinh Tao and Le Thi Hoai An. Convex analysis approach to DC programming :
Theory, Algorithms and Applications. Acta Mathematica Vietnamica, dedicated to Pro-
fessor Hoang Tuy on the occasion of his 70th birthday, Vol.22, Number 1 (1997), pp.
289-355.
Pham Dinh Tao and Le Thi Hoai An. DC optimization algorithms for solving the trust
region subproblem. SIAM J.Optimization, Vol. 8, pp. 476-505 (1998) .
B. T. Polyak. Introduction to optimization. Inc., Publications Division, 1987 .
Susana Nascimento, Boris Mirkin, and Fernando Moura-Pires. Modeling Proportional
Membership in Fuzzy Clustering. IEEE Transactions on Fuzzy Systems, Vol. 11, NrÀá 2,
April 2003.
Rochafellar, R.T. Convex Analysis (Princeton Landmarks in Mathematics and Physics).
Reprint Princeton University Press.
M.E.S. Mendes Rodrigues and L. Sacks. A scalable hierarchical fuzzy clustering algo-
rithm for text mining. In : Proc. of the 4th International Conference on Recent Advances
in Soft Computing, RASC‚Äô2004, pp.269-274, Nottingham, UK, Dec. 2004 .
Yufeng LIU, Xiaotong SHEN, and Hani DOSS, Multicategory. Multicategory œà-Learning
and Support Vector Machine :Computational Tools. Journal of Computational and
Graphical Statistics,14(1) : 219-236 .
J. B. Hiriart Urruty, and C. Lemar√©chal. Convex Analysis and Minimization Algorithms.
Springer Verlag berlin Heidelberg (1993).
La programmation DC et DCA pour la classification floue
S. Weber, T. Sch√ºle, C. Schn√∂rr. Prior Learning and Convex-Concave Regularization of
Binary Tomography. Electr. Notes in Discr. Math., 20 :313-327, 2005 .
GlennWhitwell, Xiao YingWang, JonathanM. Garibaldi. The Application of a Simulated
Annealing Fuzzy Clustering Algorithm for Cancer Diagnosis. SIP 2005, Japan.
Summary
In this paper, a model of Fuzzy C-means Clustering (FCM), one of the most popular and
best studied fuzzy clustering measures, is discussed. A fast and robust algorithm based on
DC (Difference of Convex functions) programming and DCA (DC Algorithms) is investi-
gated. Preliminary numerical solutions on real-world databases show the efficiency and the
superiority of the appropriate DCA in both the running-time and quality of solutions with
respect to the standard FCM algorithm.
