Prise en compte de connaissances pour la visualisation par
lâ€™intÃ©gration interactive de contraintes
Lionel Martinâˆ—, Matthieu Exbrayatâˆ—, Guillaume Cleuziouâˆ—, FrÃ©dÃ©ric Moalâˆ—
âˆ— Laboratoire dâ€™Informatique Fondamentale dâ€™OrlÃ©ans
UniversitÃ© dâ€™OrlÃ©ans, BP 6759 F-45067 OrlÃ©ans Cedex 2
Prenom.Nom@univ-orleans.fr
http://www.univ-orleans.fr/lifo
RÃ©sumÃ©. La projection et la visualisation dâ€™objets dans un espace Ã  deux ou
trois dimensions constituent une tÃ¢che courante de lâ€™analyse de donnÃ©es. Cette
opÃ©ration induit des pertes dans le sens oÃ¹ certains objets peuvent se retrouver
trÃ¨s proches alors quâ€™ils sont Ã  lâ€™origine assez Ã©loignÃ©s. A partir de cette visuali-
sation, il semble intÃ©ressant dâ€™offrir Ã  lâ€™utilisateur la possibilitÃ© dâ€™apporter de la
connaissance sous forme de contraintes spÃ©cifiant les similaritÃ©s attendues en-
tre divers objets, lorsque ceux ci sont, dans lâ€™espace dâ€™observation, visuellement
trop proches, ou au contraire trop Ã©loignÃ©s.
Nous proposons ici trois types de contraintes et prÃ©sentons une mÃ©thode de rÃ©-
solution de celles-ci dÃ©rivÃ©e de lâ€™analyse en composantes principales (ACP).
Deux types dâ€™expÃ©rimentation sont prÃ©sentÃ©es, reposant respectivement sur un
jeu de donnÃ©es synthÃ©tique et sur des jeux standards. Ces tests montrent quâ€™une
reprÃ©sentation de bonne qualitÃ© peut Ãªtre obtenue avec un nombre limitÃ© de con-
traintes ajoutÃ©es.
1 Introduction
Les mÃ©canismes dâ€™apprentissage automatiques consistent en gÃ©nÃ©ral Ã  Ã©tablir ce qui est
important, vis-Ã -vis dâ€™un objectif donnÃ©, au sein dâ€™une masse dâ€™information disponible. Câ€™est
notamment le cas dans le cadre de la classification automatique, quâ€™elle soit supervisÃ©e ou non.
Les objets que lâ€™on souhaite classer sont frÃ©quemment dÃ©finis par un ensemble de descripteurs,
potentiellement nombreux, qui peuvent sâ€™avÃ©rer redondants, bruitÃ©s, ou tout simplement sans
objet vis-Ã -vis de la classification recherchÃ©e. Par ailleurs, lâ€™abondance de descripteurs peut
rendre peu lisible le mÃ©canisme de classification.
Deux approches sont envisageables afin de rÃ©duire le nombre de descripteurs. La premiÃ¨re
consiste Ã  ne retenir que les descripteurs les plus significatifs (sÃ©lection dâ€™attributs, ou fea-
ture selection, on pourra notamment consulter la synthÃ¨se de Guyon et Elisseeff (2003)) et
la seconde Ã  produire un ensemble restreint de descripteurs synthÃ©tiques, reflÃ©tant au mieux
la rÃ©partition des objets dans lâ€™espace dâ€™origine (rÃ©duction de dimension, ou dimensionality
reduction). Notons que la rÃ©duction de dimension est parfois considÃ©rÃ©e (e.g. dans Blum et
Langley (1997)) comme appartenant Ã  une catÃ©gorie de mÃ©thodes de sÃ©lection dâ€™attributs ap-
pelÃ©e approche par filtre.
IntÃ©gration interactive de contraintes et visualisation
Nous nous intÃ©resserons ici aux mÃ©thodes de rÃ©duction de dimension, lesquelles perme-
ttent frÃ©quemment dâ€™obtenir, au prix dâ€™une perte dâ€™information raisonnable, une information
synthÃ©tique limitÃ©e Ã  quelques descripteurs, ce qui permet de rÃ©duire les coÃ»ts de calcul en
classification automatique (recherche de voisins, estimation de mÃ©lange de lois,...), et autorise
Ã©galement une projection des objets dans un espace visualisable Ã  deux ou trois dimensions.
La dualitÃ© entre visualisation et classification sâ€™avÃ¨re remarquable dans les mÃ©thodes de rÃ©duc-
tion de dimension. La visualisation fournit un outil intuitif, accessible au non spÃ©cialiste. La
classification automatique permet de quantifier objectivement lâ€™efficacitÃ© de la mÃ©thode.
De nombreuses approches ont Ã©tÃ© dÃ©veloppÃ©es au cours des derniÃ¨res dÃ©cennies, lâ€™une des
plus connues Ã©tant lâ€™analyse en composante principale (ACP). Rappelons quâ€™il sâ€™agit dâ€™une
approche non supervisÃ©e sÃ©lectionnant les dimensions orthogonales, issues de combinaisons
linÃ©aires des attributs dâ€™origine, qui prÃ©servent au mieux la dispersion des objets. Elle est rÃ©-
solue par la recherche des plus grandes valeurs propres de la matrice de variance-covariance
des objets dans lâ€™espace dâ€™origine. Dans un cadre supervisÃ©, la notion de classe pourra con-
duire Ã  rechercher un espace de projection qui renforce la compacitÃ© des classes, tout en les
distinguant nettement les unes des autres. Câ€™est notamment le cas de lâ€™Analyse Discriminante
LinÃ©aire (ou ALD, Fisher (1936)), qui maximise le critÃ¨re de Fisher (rapport en variance inter-
classes et variance intra-classes).
Lorsque la rÃ©partition des donnÃ©es prÃ©sente certaines caractÃ©ristiques topologiques (objets
rÃ©partis, Ã  grande Ã©chelle, Ã  proximitÃ© dâ€™une surface donnÃ©e), il est possible dâ€™appliquer des
mÃ©thodes reposant sur le concept de variÃ©tÃ©. Une partie de ces mÃ©thodes procÃ¨de en deux
temps : tout dâ€™abord, on recense les paires dâ€™objets proches dans lâ€™espace dâ€™origine. Ensuite,
on cherche Ã  optimiser un critÃ¨re de projection (par exemple la variance globale) tout en lim-
itant les dÃ©formations des distances entre paires dâ€™objets proches, câ€™est Ã  dire en limitant les
dÃ©formations locales de lâ€™espace. Les projections sont calculÃ©es soit par recherche de valeurs
propres (Tenenbaum et al. (2000)), soit par rÃ©solution dâ€™un systÃ¨me de contraintes (Roweis et
Saul (2000); Weinberger et Saul (2006)). Il existe Ã©galement des approches Ã  une phase, telle
lâ€™analyse en composantes curvilignes (Demartines et HÃ©rault (1997)) qui cherche Ã  minimiser
la somme des diffÃ©rences entre distances dâ€™origine et distance projetÃ©es, une pondÃ©ration au-
torisant alors de plus grandes distorsions pour des paires dâ€™objets distants.
Si lâ€™on se concentre sur la seule projection des objets, les travaux les plus rÃ©cents portent
sur des techniques dâ€™apprentissage (semi-) supervisÃ©, mettant en jeu des critÃ¨res locaux ou
globaux sur la proximitÃ© dâ€™objets appartenant ou non Ã  la mÃªme classe. Lâ€™analyse en com-
posantes pertinentes (Relevant Component Analysis) proposÃ©e par Bar-Hillel et al. (2005) est
une approche semi-supervisÃ©e. Le calcul de la matrice de projection repose ici sur un Ã©chan-
tillon dâ€™objets de diffÃ©rentes classes : lâ€™utilisateur dÃ©clare un ensemble de paires dâ€™objets de
mÃªme classe. Par clÃ´ture transitive, lâ€™algorithme reconstitue des groupes dâ€™objets de mÃªme
classe, appelÃ©s chunklets (lesquels ne constituent quâ€™un Ã©chantillon de lâ€™ensemble X des ob-
jets disponibles). Une matrice de covariance intra-chunklet CË† est produite, laquelle sert ensuite
de base Ã  la projection des objets (Xnew = CË†âˆ’1/2X) et/ou au calcul dâ€™une distance de Ma-
hanalobis entre eux : d(x1, x2) = (x1âˆ’x2)tCË†âˆ’1(x1âˆ’x2). Il est Ã©galement possible dâ€™insÃ©rer
une Ã©tape intermÃ©diaire de rÃ©duction de dimension Ã  partir de CË†.
La classification par maximisation de la marge entre plus proches voisins (Large Margin
Nearest Neighbors, ou LMNN) proposÃ©e par Weinberger et al. (2005) et Weinberger et Saul
(2008) se concentre pour sa part sur la notion de voisinage. Pour chaque objet, on rÃ©pertorie les
L. Martin et al.
objets voisins (dans un rayon donnÃ©), puis on pose un ensemble de contraintes exprimant le fait
que les objets proches de classes diffÃ©rentes doivent Ãªtre plus Ã©loignÃ©s que les objets proches
de mÃªme classe. Cet Ã©loignement se traduit par une marge minimum entre les deux catÃ©gories
dâ€™objets. Les auteurs proposent une formulation du problÃ¨me sous forme dâ€™un programme semi
dÃ©fini, dont la rÃ©solution produit une matrice M servant de support au calcul dâ€™une distance
de Mahanalobis entre les objets. Cette rÃ©solution repose sur un solveur spÃ©cifique. De cette
matrice peut se dÃ©duire une matrice de projection en faible dimension L, telle queM = LtL.
Ces deux approches prÃ©sentent des limitations. Lâ€™approche LMNN introduit des contraintes
pour lâ€™ensemble des objets. La rÃ©solution peut donc sâ€™avÃ©rer lourde, notamment en cas de
zones trÃ¨s hÃ©tÃ©rogÃ¨nes. Lâ€™approche RCA utilise la seule notion de paires dâ€™objets de mÃªme
classe, alors que dâ€™autres types de contraintes, reposant par exemple sur lâ€™Ã©loignement ou le
rapprochement dâ€™objets, Ã©largiraient et assoupliraient les jeux de contraintes possibles. Ces
approches, ainsi que la plupart des approches existantes, supposent en outre que toutes les
contraintes soient fixÃ©es avant la phase de rÃ©duction de dimension. Il serait donc apprÃ©cia-
ble de cumuler la simplicitÃ© de lâ€™approche RCA avec lâ€™apport des contraintes de voisinage de
lâ€™approche MLNN, tout en proposant un mÃ©canisme intuitif et itÃ©ratif dâ€™intÃ©gration de con-
naissances du domaine.
Dans cet article nous nous plaÃ§ons dans un cadre semi-supervisÃ©, dans lequel les objets ne
sont pas associÃ©s Ã  des Ã©tiquettes de classe. Nous proposons dâ€™utiliser trois types de contraintes
de projection, Ã  la fois naturelles et simples dâ€™utilisation :
â€“ positionnement de deux objets : consiste Ã  borner (supÃ©rieurement ou infÃ©rieurement) la
distance entre deux objets,
â€“ positionnement de deux objets (b, c) par rapport Ã  un troisiÃ¨me(a) : consiste Ã  borner le
rapport distance(a, c)/distance(a, b).
â€“ voisinage dâ€™un objet : il sâ€™agit dâ€™une gÃ©nÃ©ralisation du cas prÃ©cÃ©dent. Un objet donnÃ©
doit se retrouver dans le voisinage dâ€™un groupe dâ€™objets donnÃ©s.
Nous souhaitons pouvoir intÃ©grer ces contraintes de maniÃ¨re interactive : lâ€™utilisateur observe
une reprÃ©sentation en deux ou trois dimensions des donnÃ©es, puis peut spÃ©cifier des corrections
attendues en ajoutant des contraintes, par exemple dans le cas oÃ¹ deux objets â€œsemblables
pour lâ€™utilisateurâ€ se retrouvent Ã©loignÃ©s dans la reprÃ©sentation. Il est ainsi possible dâ€™obtenir
une nouvelle reprÃ©sentation Ã  laquelle dâ€™autres contraintes pourront Ãªtre intÃ©grÃ©es de maniÃ¨re
itÃ©rative.
Il est important de noter que cette approche a pour objectif dâ€™intÃ©grer des contraintes de
distances dans un processus de rÃ©duction de dimension pouvant conduire Ã  de la visualisation.
Il diffÃ¨re donc des deux approches prÃ©sentÃ©es prÃ©cÃ©demment (RCA et LMNN), non seulement
dans les types de contraintes dÃ©finies, mais Ã©galement dans le sens quâ€™il offre clairement la pos-
sibilitÃ© dâ€™interactions graphiques. A lâ€™inverse, il diffÃ¨re des autres outils dâ€™exploration visuelle
de donnÃ©es par projection, tels Da Costa et Venturini (2007), dans le sens oÃ¹ il ne sâ€™agit pas
de dÃ©placer le point de vue de lâ€™utilisateur (afin dâ€™identifier des groupes dâ€™objets). Il sâ€™agit ici
dâ€™agir au sein dâ€™une visualisation globale, â€œuniformeâ€ dans le sens ou elle reproduit au mieux
les distances entre tous les objets dans un espace euclidien.
Lâ€™article est organisÃ© comme suit : dans la section 2, nous dÃ©finissons plus formellement les
trois types de contraintes proposÃ©s. En section 3, nous proposons un mÃ©canisme de rÃ©solution
de contraintes dures reposant sur lâ€™algorithme dâ€™Uzawa. En section 4, nous prÃ©sentons lâ€™inter-
IntÃ©gration interactive de contraintes et visualisation
face graphique dâ€™ajout interactif de contraintes. En section 5 nous prÃ©sentons un ensemble de
tests. Nous concluons et proposons diffÃ©rentes pistes en section 6.
2 Formalisation des diffÃ©rents types de contraintes
Soit un ensemble dâ€™observations (objets) x1, ..., xn dÃ©crits dans Rd, oÃ¹ d est le nombre
de dimensions. Dans la suite, nous noterons X = (x1, ..., xn)T la matrice des observations, la
ligne i correspondant Ã  la description de xi. Notre objectif consiste Ã  fournir une reprÃ©sentation
de ces objets en k dimensions (et notamment en trois dimensions) telle que :
â€“ la perte dâ€™information est rÃ©duite, ce qui se traduira par la recherche dâ€™une reprÃ©sentation
de variance (ou inertie) maximale (principe de lâ€™ACP),
â€“ les contraintes spÃ©cifiÃ©es par lâ€™utilisateur sont si possible satisfaites (ces contraintes
seront nommÃ©es contraintes utilisateur).
La reprÃ©sentation sera obtenue par projection du nuage de points dans un sous-espace Ã 
k dimensions (k  d). Il faut donc dÃ©terminer k vecteurs u1, u2 ... uk associÃ©s Ã  la matrice
de projection L = (u1, u2, . . . uk) telle que les lignes de la matrice X.L correspondent aux
projections des observations dans notre sous-espace de dimension k. Soit h(xi) = LT .xi la
projection de xi. Dans ce contexte, la distance euclidienne entre xi et xj aprÃ¨s projection,
d(h(xi)h(xj)), est dÃ©finie par :
d2(h(xi), h(xj)) = < h(xi)âˆ’ h(xj), h(xi)âˆ’ h(xj) > (1)
= (h(xi)âˆ’ h(xj))T .(h(xi)âˆ’ h(xj))
= (LT .xi âˆ’ LT .xj)T .(LT .xi âˆ’ LT .xj)
= (LT .(xi âˆ’ xj))T .(LT .(xi âˆ’ xj))
= (xi âˆ’ xj)T .L.LT .(xi âˆ’ xj) (2)
Dans la suite, nous noteronsM = L.LT la matrice caractÃ©risant la distance. Nous chercherons
ici Ã  rÃ©soudre notre problÃ¨me de rÃ©duction de dimension, prÃ©servant au mieux lâ€™inertie, sous
les contraintes introduites dans les sections suivantes.
2.1 Correction de position de 2 objets (C2)
Pour ce premier type de correction, lâ€™utilisateur souhaite modifier la distance entre deux
objets xa et xb. Soit dËœ la distance souhaitÃ©e entre les deux objets projetÃ©s aprÃ¨s correction.
Deux cas de figure se prÃ©sentent :
â€“ on souhaite rapprocher xa et xb, ce qui peut sâ€™exprimer par : d2(h(xa), h(xb)) â‰¤ dËœ
â€“ on souhaite Ã©loigner xa et xb, ce qui peut sâ€™exprimer par : d2(h(xa), h(xb)) â‰¥ dËœ
On peut donc dÃ©finir de telles contraintes par un quadruplet (a, b, dËœ, Î±), avec Î± = 1 pour les
contraintes correspondant au premier cas et Î± = âˆ’1 pour le deuxiÃ¨me cas. Ainsi, la contrainte
associÃ©e Ã  (a, b, dËœ, Î±) sâ€™Ã©crit :
Î± âˆ— [d2(h(xa), h(xb))âˆ’ dËœ] â‰¤ 0 (3)
Dans la suite, nous noterons C2 lâ€™ensemble des contraintes de ce type.
L. Martin et al.
2.2 Correction de position de 2 objets par rapport Ã  un 3Ã¨me (C3)
Pour ce deuxiÃ¨me type de correction, on souhaite modifier le positionnement relatif dâ€™un
objet xc par rapport Ã  deux objets xa et xb. Deux cas sont envisageables :
â€“ on souhaite rapprocher xc de xa, de faÃ§on Ã  avoir d2(h(xa), h(xc)) â‰¤ d2(h(xa), h(xb)).
On prÃ©cise lâ€™intensitÃ© de rapprochement grÃ¢ce Ã  une variable Î´. Notre contrainte devient :
d2(h(xa), h(xc)) â‰¤ Î´ âˆ— d2(h(xa), h(xb))
â€“ on souhaite Ã©loigner xc de xa, de faÃ§on Ã  avoir d2(h(xa), h(xc)) â‰¥ d2(h(xa), h(xb)).
On prÃ©cise lâ€™intensitÃ© de cet Ã©loignement grÃ¢ce Ã  une variable Î´. Notre contrainte de-
vient : d2(h(xa), h(xc)) â‰¥ Î´ âˆ— d2(h(xa), h(xb))
Dans la suite, nous notons C3 lâ€™ensemble des contraintes de ce type. Une telle contrainte sera
un quintuplet (a, b, c, Î´abc, Î±), avec Î± = 1 pour les contraintes correspondant au premier cas,
Î± = âˆ’1 pour le deuxiÃ¨me cas. Ainsi, si (a, b, c, Î´, Î±) âˆˆ C3, la contrainte associÃ©e sâ€™Ã©crit :
Î± âˆ— [d2(h(xa), h(xc))âˆ’ Î´ âˆ— d2(h(xa), h(xb))] â‰¤ 0 (4)
2.3 Modification de voisinage (Cv)
Ce troisiÃ¨me type de contrainte a pour objectif de repositionner un objet xa dans le voisi-
nage dâ€™un groupe dâ€™objets. Soit V sa le voisinage source de xa et V ca son voisinage cible. On
souhaite que xa soit Ã  la fois rapprochÃ© des objets de V ca et Ã©loignÃ© des objets de V sa. Cela
peut se traduire par un jeu de contraintes portant respectivement sur les objets de ces deux
voisinages.
Concernant le voisinage cible, la liste des ka objets composant V ca est dÃ©terminÃ©e par lâ€™util-
isateur. Concernant le voisinage source, on retient les k plus proches voisins de xa. La valeur
de k peut Ãªtre fixÃ©e arbitrairement (par exemple k = 3) ou Ãªtre Ã©gale Ã  ka. Les contraintes de
modification de voisinage peuvent sâ€™exprimer comme suit :
â€“ Pour tout xi âˆˆ V sa, notons dai la distance observÃ©e, dans lâ€™espace de projection courant,
entre xa et xi. Lâ€™utilisateur souhaite que la distance observÃ©e aprÃ¨s correction soit plus
grande, ce que lâ€™on peut traduire par la contrainte suivante :
âˆ€xi âˆˆ V sa d2(h(xa), h(xi)) â‰¥ Î² âˆ— dai
Î² Ã©tant un paramÃ¨tre Ã  fixer (par exemple, Î² = 5).
Notons cependant que d(xa, xi), la distance entre xa et xi dans lâ€™espace dâ€™origine (avant
projection), constitue par nature une borne supÃ©rieure de d2(h(xa), h(xi)). Nous pou-
vons donc modifier notre contrainte pour exprimer que d2(h(xa), h(xi)) doit Ãªtre assez
proche de d2(xa, xi) :
âˆ€xi âˆˆ V sa d2(h(xa), h(xi)) â‰¥ Î³ âˆ— d2(xa, xi)
oÃ¹ Î³ âˆˆ [0, 1] est un paramÃ¨tre Ã  fixer (e.g. 0,75). Nous retenons cette formulation.
â€“ Concernant les objets de V ca, nous pourrions essayer de minimiser d2(h(xa), h(xi)).
Nous pouvons Ã©galement ajouter une contrainte spÃ©cifiant que d2(h(xa), h(xi)) est bor-
nÃ©e supÃ©rieurement par une valeur Ã  fixer. Nous pouvons considÃ©rer comme borne une
valeur dÃ©pendant de la distance moyenne dV ca entre les objets de V ca :
âˆ€xi âˆˆ Va d2(h(xa), h(xi)) â‰¤  âˆ— dV a
oÃ¹  est un paramÃ¨tre Ã  fixer (e.g. 1,5). Nous retenons cette formulation proche de celle
obtenue pour les objets de V sa.
IntÃ©gration interactive de contraintes et visualisation
Dans la suite nous noterons Cv lâ€™ensemble des contraintes de ce type. Une contrainte appar-
tenant Ã  Cv est ainsi un quintuplet (a, i, d, Î¸, Î±) tel que :
Î± âˆ— [(d(h(xa), h(xi))âˆ’ Î¸ âˆ— d)] â‰¤ 0 (5)
avec :
â€“ Î¸ = Î³, Î± = âˆ’1 et d = d(xa, xi) pour les contraintes issues des voisinages sources, et
â€“ Î¸ = , Î± = 1 et d = dV ca pour les contraintes issues des voisinages cibles.
3 RÃ©solution
Nous pouvons rÃ©sumer notre problÃ¨me dâ€™optimisation sous contraintes par :
ï£±ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£³
MaxL
âˆ‘
i,j(xi âˆ’ xj)t.L.Lt.(xi âˆ’ xj)
âˆ€i, j < ui, uj >= Î´(i, j)
âˆ€(a, b, dËœ, Î±) âˆˆ C2 Î± âˆ— [d2(h(xa), h(xb))âˆ’ dËœ] â‰¤ 0
âˆ€(a, b, c, Î´, Î±) âˆˆ C3 Î± âˆ— [d2(h(xa), h(xc))âˆ’ Î´ âˆ— d2(h(xa), h(xb))] â‰¤ 0
âˆ€(a, i, d, Î¸, Î±) âˆˆ Cv Î± âˆ— [d2(h(xa), h(xi))âˆ’ Î¸ âˆ— d] â‰¤ 0
Dans toutes les contraintes prÃ©cÃ©dentes interviennent des termes d2(h(xa), h(xi)) dÃ©pendant
de la solution de notre problÃ¨me (i.e. de la matrice L). Comme prÃ©cÃ©demment, on peut Ã©crire
d2(h(xa), h(xi)) = (xa âˆ’ xi)t.L.Lt.(xa âˆ’ xi) et noter que :
d2(h(xa), h(xi)) = (xa âˆ’ xi)t.L.Lt.(xa âˆ’ xi) (6)
=
âˆ‘
j=1..k
utj .(xa âˆ’ xi).(xa âˆ’ xi)t.uj (7)
oÃ¹ (xaâˆ’xi).(xaâˆ’xi)t est une matrice nÃ—n. Dans la suite nous noteronsXa,i cette matrice,
et ainsi : d2(h(xa), h(xi)) =
âˆ‘
j=1..k u
t
j .Xa,i.uj .
Contrairement au cas sans contraintes, on ne peut plus chercher successivement les vecteurs
u1, u2 . . . uk, puisquâ€™ici les contraintes utilisateurs sont globales : ces contraintes ne doivent
pas Ãªtre respectÃ©es sur chaque dimension de projection indÃ©pendamment, mais globalement
dans lâ€™espace de projection. Nous devons donc traiter la recherche des k vecteurs uj simul-
tanÃ©ment.
Nous pouvons rÃ©Ã©crire le critÃ¨re Ã  maximiser :
âˆ‘
i,j
(xi âˆ’ xj)t.L.Lt.(xi âˆ’ xj) =
âˆ‘
j=1..k
utj .X
t.X.uj (8)
L. Martin et al.
ConsidÃ©rons dans un premier temps le lagrangien L de ce problÃ¨me, en ne tenant pas
compte des contraintes dâ€™orthogonalitÃ© des vecteurs uj :
L(L, Î», Âµ, Ïˆ, Ï) = âˆ’
âˆ‘
j=1..k
utj .X
t.X.uj +
âˆ‘
j=1..k
Î»j(utj .uj âˆ’ 1)
+
âˆ‘
(ai,bi,dËœi,Î±i)âˆˆC2
Âµi âˆ— ((
âˆ‘
j=1..k
utj .Xai,bi .uj)âˆ’ dËœi) âˆ— Î±i)
+
âˆ‘
(ai,bi,ci,Î´i,Î±i)âˆˆC3
Ïˆi âˆ— ((
âˆ‘
j=1..k
utj .Xai,ci .uj)âˆ’ (Î´i âˆ— (
âˆ‘
j=1..k
utj .Xai,bi .uj)) âˆ— Î±i)
+
âˆ‘
(ai,ii,di,Î¸i,Î±i)âˆˆCv
Ïi âˆ— (((
âˆ‘
j=1..k
utj .Xai,ii .uj)âˆ’ Î¸âˆ—di) âˆ— Î±i)
DÃ©rivons ce Lagrangien par rapport Ã  uj (en multipliant par 2 pour simplifier) :
2
âˆ‚L(L, Î», Âµ, Ïˆ, Ï)
âˆ‚uj
= âˆ’Xt.X.uj + Î»j .uj +
âˆ‘
(ai,bi,dËœi,Î±i)âˆˆC2
ÂµiÎ±iXai,bi .uj
+
âˆ‘
(ai,bi,ci,Î´i,Î±i)âˆˆC3
ÏˆiÎ±i âˆ— (Xai,ci .uj âˆ’ Î´i âˆ— (Xai,bi .uj))
+
âˆ‘
(ai,ii,di,Î¸i,Î±i)âˆˆCv
ÏiÎ±i âˆ— (Xai,ii .uj)
Soit XC la matrice dÃ©finie par :
XC = Xt.X âˆ’
âˆ‘
(ai,bi,dËœi,Î±i)âˆˆC2
ÂµiÎ´iXai,bi
âˆ’
âˆ‘
(ai,bi,ci,Î´i,Î±i)âˆˆC3
ÏˆiÎ´i âˆ— (Xai,ci âˆ’ Î´i âˆ— (Xai,bi))
âˆ’
âˆ‘
(ai,ii,di,Î¸i,Î±i)âˆˆCv
ÏiÎ±i âˆ— (Xai,ii)
la dÃ©rivÃ©e partielle du lagrangien sâ€™Ã©crit alors :
2
âˆ‚L(L, Î», Âµ, Ïˆ, Ï)
âˆ‚uj
= âˆ’XC .uj + Î»j .uj (9)
Notons au passage que le Lagrangien sâ€™Ã©crit :
L(L, Î», Âµ, Ïˆ, Ï) = âˆ’
âˆ‘
j=1..k
utjXC .uk +
âˆ‘
j=1..k
Î»j(utj .uj âˆ’ 1) (10)
Lâ€™annulation des dÃ©rivÃ©es partielles donneXC .uj = Î»j .uj . En dâ€™autres termes, les solutions uâˆ—j
sont les vecteurs propres de la matrice XC , associÃ©e aux valeurs propres Î»j , ce qui a deux con-
sÃ©quences notables : tout dâ€™abord, bien que nâ€™ayant pas tenu compte de la contrainte dâ€™orthog-
onalitÃ© des vecteurs uj , les solutions obtenues sont orthogonales. Nous pouvons par ailleurs en
dÃ©duire que la fonction duale q(Î», Âµ, Ïˆ, Ï) du problÃ¨me prÃ©sente une forme assez simple :
IntÃ©gration interactive de contraintes et visualisation
q(Î», Âµ, Ïˆ, Ï) = MinLL(L, Î», Âµ, Ïˆ, Ï)
En effet, en utilisant les conditions dâ€™optimalitÃ© prÃ©cÃ©dentes :
q(Î», Âµ, Ïˆ, Ï) = MinL âˆ’
âˆ‘
j=1..k
utjXC .uj +
âˆ‘
j=1..k
Î»j(utj .uj âˆ’ 1)
= MinL âˆ’
âˆ‘
j=1..k
uTj .(Î»juj) +
âˆ‘
j=1..k
Î»j(utj .uj âˆ’ 1)
= MinL âˆ’
âˆ‘
j=1..k
Î»j ||uj ||2 +
âˆ‘
j=1..k
Î»j(||uj ||2 âˆ’ 1)
= MinL âˆ’
âˆ‘
j=1..k
Î»j
les contraintes imposant ||uj ||2 = 1 pour la solution optimale.
Finalement, puisque le problÃ¨me dual consiste Ã  maximiser la fonction duale, la solution op-
timale correspond Ã  maximiser la somme de k valeurs propres de la matrice XC , câ€™est-Ã -dire
les Î»j correspondent aux k plus grandes valeurs propres de cette matrice. NÃ©anmoins, pour
calculer les valeurs propres de XC , il faudrait connaÃ®tre les valeurs des variables duales Âµ, Ïˆ et
Ï, valeurs dont nous ne disposons pas. Pour cette raison, nous proposons dâ€™utiliser lâ€™algorithme
itÃ©ratif dâ€™Uzawa, qui permet dâ€™obtenir des approximations de ces variables. Cet algorithme est
dÃ©crit dans la section suivante.
3.1 Mise en oeuvre de lâ€™algorithme dâ€™Uzawa
Lâ€™algorithme dâ€™Uzawa, introduit par Arrow et al. (1958), repose sur lâ€™idÃ©e de trouver un
point selle du lagrangien par approximations successives. Dans la formulation classique de
lâ€™algorithme, on considÃ¨re le problÃ¨me dâ€™optimisation suivant :ï£±ï£²ï£³ MinxJ(x)h(x) = 0
g(x) â‰¤ 0
oÃ¹ les fonctions h et g dÃ©signent en rÃ©alitÃ© des familles de fonctions hi et gj . Le lagrangien
sâ€™Ã©crit dans ce cas :
L(x, Î», Âµ) = J(x) +âˆ‘i Î»ihi(x) +âˆ‘j Âµjgj(x)
Lâ€™algorithme dâ€™Uzawa consiste Ã  fixer des valeurs initiales des coefficients de Lagrange (Î»0, Âµ0),
chercher lâ€™optimum du lagrangien, puis Ã  corriger les coefficients de Lagrange en fonction de
cette solution. Ce processus est itÃ©rÃ© jusquâ€™Ã  convergence (la convergence est garantie).
1 - Fixer Î»0, Âµ0,
2 - ItÃ©rer pour n â‰¥ 0 (Ï est un paramÃ¨tre) :
2.1 Calculer la solution xn de :MinxL(x, Î»n, Âµn)
2.2 Mettre Ã  jour (Î»n, Âµn) par :
Î»n+1i = Î»
n
i + Ï âˆ— hi(xn)
Âµn+1j = max(0, Âµ
n
j + Ï âˆ— gj(xn))
2.3 Tester la convergence : ||xn+1 âˆ’ xn|| < 
L. Martin et al.
3.2 Application de lâ€™algorithme dâ€™Uzawa
Dans notre cas, lâ€™algorithme dâ€™Uzawa se simplifie lÃ©gÃ¨rement. Il nâ€™est pas nÃ©cessaire dâ€™ap-
proximer les valeurs de Î»i : les xn Ã©tant ici les vecteurs propres u1 . . . uk, les valeurs propres
en dÃ©coulent directement. Nous fixerons donc seulement les valeurs des coefficients Âµn, câ€™est-
Ã -dire, dans notre cas, des coefficients Âµ, Ïˆ.
Nous initialisons ces coefficients Ã  0. La premiÃ¨re itÃ©ration sera donc une simple ACP
puisque, dans ce cas, XC = Xt.X ,
Ensuite, les contraintes non satisfaites vont permettre de corriger la matrice XC : dans
lâ€™algorithme dâ€™Uzawa, si une contrainte (gj(xn) â‰¤ O) nâ€™est pas satisfaite, alors gj(xn) > O,
ce qui implique que la mise Ã  jour : Âµn+1j = max(0, Âµ
n
j + Ï âˆ— gj(xn)) va rendre Âµn+1j > 0.
La matrice XC sera donc calculÃ©e en tenant compte de gj(xn). Cette mise Ã  jour de XC est
assez intuitive : prenons par exemple une contrainte de type C2 : d2(h(xa), h(xb)) â‰¥ dËœ ; si
cette contrainte nâ€™est pas satisfaite, dans le calcul de XC , il faut ajouter Ã  XT .X la matrice
cXa,b (oÃ¹ c est une constante). Or Xa,b est dÃ©jÃ  dans XT .X , cela revient donc Ã  augmenter le
â€œpoidsâ€ de la distance d2(h(xa), h(xb)) dans le critÃ¨re Ã  optimiser (sans contraintes). Tant que
la contrainte ne sera pas satisfaite, on continuera dâ€™augmenter ce poids...
En rÃ©sumÃ©, Ã  chaque itÃ©ration, il suffit de mettre Ã  jour la matrice XC en y ajoutant des
matrices de la forme cXa,b, puis de diagonaliser la matrice XC .
Notons que cette mÃ©thode permet de manipuler des â€œcontraintes souplesâ€ dans le sens oÃ¹
la convergence peut Ãªtre atteinte avant que les contraintes soient toutes satisfaites, permettant
ainsi de ne pas Ãªtre pÃ©nalisÃ© par des contraintes difficiles Ã  satisfaire.
4 Mise en Å“uvre graphique
Lâ€™ajout interactif des contraintes a Ã©tÃ© implantÃ© dans le cadre dâ€™un logiciel de visualisation
3D. Lâ€™utilisateur procÃ¨de ainsi : il dÃ©finit le type de contrainte Ã  appliquer puis sÃ©lectionne les
objets concernÃ©s. Il dispose alors dâ€™un â€œascenceurâ€ horizontal afin de spÃ©cifier Ã  la fois le sens
de la contrainte (contraintes de type C2 et C3) et lâ€™intensitÃ© de celle-ci.
4.1 Ajout de contraintes de type C2
La figure 1 reprÃ©sente la fenÃªtre de saisie de contrainte pour un couple dâ€™objets donnÃ©.
Ces deux objets sont sÃ©lectionnÃ©s dans la fenÃªtre 3D, et sont visualisÃ©s respectivement par
des cibles rouge et verte (fig. 2). Nous pouvons observer la prÃ©sence dâ€™un ascenceur, dont le
curseur est placÃ© par dÃ©faut sur la valeur â€65â€, ce qui signifie que dans la projection courante, la
distance entre les deux points (5,62) correspond Ã  65% de leur distance dans lâ€™espace dâ€™origine
(8,58).
Si lâ€™on dÃ©place lâ€™ascenceur vers la droite, la contrainte reviendra Ã  Ã©loigner les deux objets
(avec, comme borne supÃ©rieure, leur distance dans lâ€™espace dâ€™origine). Si on le dÃ©place vers
la gauche, la contrainte indiquera un rapprochement des deux objets (avec, comme borne in-
fÃ©rieure, une distance nulle). Ici, nous choisissons de rapprocher les deux objets : lâ€™ascenceur
est placÃ© sur 20.
La figure 4 prÃ©sente lâ€™espace de projection recalculÃ© avec application de cette seule con-
trainte. Nous pouvons observer que les deux objets se retrouvent visuellement plus proches.
IntÃ©gration interactive de contraintes et visualisation
FIG. 1 â€“ Saisie de con-
traintes de type C2
FIG. 2 â€“ Les objets contraints
Si nous sÃ©lectionnons Ã  nouveau ces deux objets, nous pouvons vÃ©rifier, grÃ¢ce Ã  la fenÃªtre de
saisie de contraintes, que leur nouvelle distance reprÃ©sente 20% de leur distance dans lâ€™espace
dâ€™origine (fig. 3).
FIG. 3 â€“ Distance ob-
servÃ©e aprÃ¨s recalcul
FIG. 4 â€“ Les objets contraints aprÃ¨s recalcul
L. Martin et al.
4.2 Ajout de contraintes de type C3
La figure 5 reprÃ©sente la fenÃªtre de saisie de contrainte pour un triplet dâ€™objets donnÃ©. Les
trois objets sont sÃ©lectionnÃ©s dans la fenÃªtre 3D, et sont visualisÃ©s respectivement par des cibles
rouge, verte et bleue (fig. 6). Il est possible de choisir un attribut utilisÃ© pour lâ€™affichage dâ€™un
label ce qui amÃ©liore lâ€™utilisation de cette interface (ici les donnÃ©es reprÃ©sentÃ©es correspondent
au jeu de donnÃ©es â€œzooâ€ de lâ€™UCI repository).
Sur cette figure, lâ€™ascenceur indique le rapport entre les distances AC et AB (seasnake-
tuatara et seasnake-carp). Il est initialement de 162 (la distance d(seasnake, tuatara) vaut 162%
de la distance d(seasnake, carp)). Nous choisissons de rendre â€œtuataraâ€ plus proche de â€œseasnakeâ€
que ne lâ€™est â€œcarpâ€ (le rapport est diminuÃ©, grÃ¢ce Ã  lâ€™ascenceur, Ã  50).
FIG. 5 â€“ Saisie de con-
traintes de type C3
FIG. 6 â€“ Les objets contraints
La figure 8 prÃ©sente lâ€™espace de projection recalculÃ© avec application de cette seule con-
trainte. Nous pouvons observer que â€œtuataraâ€ est dÃ©sormais plus proche de â€œseasnakeâ€ que ne
lâ€™est â€œcarpâ€. Si nous sÃ©lectionnons Ã  nouveau ces trois objets, nous pouvons vÃ©rifier, grÃ¢ce Ã  la
fenÃªtre de saisie de contraintes, que le nouveau rapport de distance est proche de 50 (fig. 7).
5 ExpÃ©rimentations
Lâ€™un des principaux objectifs de la mÃ©thode proposÃ©e ici consiste Ã  fournir un outil inter-
actif de manipulation graphique, dans lequel des utilisateurs peuvent poser des contraintes et
observer leur impact sur la projection des objets. NÃ©anmoins, lâ€™efficacitÃ© dâ€™une manipulation
graphique est par nature difficile Ã  Ã©valuer objectivement. Nous proposons donc un protocole
de validation permettant de mesurer sâ€™il est possible dâ€™atteindre une reprÃ©sentation satisfaisante
avec un nombre limitÃ© de contraintes.
IntÃ©gration interactive de contraintes et visualisation
FIG. 7 â€“ Distance ob-
servÃ©e aprÃ¨s recalcul
FIG. 8 â€“ Les objets contraints aprÃ¨s recalcul
Pour mesurer ce degrÃ© de satisfaction, nous proposons de comparer la reprÃ©sentation obtenue
(en trois dimensions) Ã  une organisation de rÃ©fÃ©rence. On peut estimer quâ€™un utilisateur jugera
satisfaisante une organisation dans laquelle les objets de mÃªme classe sont proches et les
objets de classes diffÃ©rentes Ã©loignÃ©s. Or il sâ€™agit lÃ  de la dÃ©finition du critÃ¨re de Fisher
variance intraâˆ’classs
variance interâˆ’classes , critÃ¨re que maximise lâ€™analyse discriminante (ALD). En consÃ©quence,
nous utiliserons comme organisation de rÃ©fÃ©rence celle produite par application de lâ€™ALD
Ã  notre jeu de donnÃ©es. Naturellement, dans une utilisation rÃ©elle, non supervisÃ©e, lâ€™utilisa-
teur ne dispose pas, a priori, dâ€™information de classe. Nous allons donc utiliser pour nos tests
des jeux de donnÃ©es Ã©tiquetÃ©s. Lâ€™Ã©tiquette des objets sera utilisÃ©e afin de guider un utilisateur
simulÃ© dans ces choix. En dâ€™autre termes, elle permettra la mise en place dâ€™une â€œconnaissance
dâ€™expertâ€, et servira donc de maniÃ¨re indirecte. Lâ€™Ã©tiquette des objets nâ€™intervient jamais di-
rectement dans le processus de calcul de projection.
Nous noterons dald la distance induite dans lâ€™espace de reprÃ©sentation obtenu par ALD,
restreint aux trois dimensions les plus significatives (lâ€™utilisateur manipulant les objets dans
un espace 3D). Afin de pouvoir utiliser cette approche, nous utiliserons des jeux de donnÃ©es
Ã©tiquetÃ©es. Par ailleurs, afin de simuler le comportement de lâ€™utilisateur, nous utiliserons un
processus automatique de choix de contraintes, choisissant en prioritÃ© la correction des dÃ©-
formations les plus importantes (par rapport Ã  lâ€™organisation proposÃ©e par lâ€™ALD). La section
suivante prÃ©sente les diffÃ©rents types de contraintes gÃ©nÃ©rÃ©es.
5.1 GÃ©nÃ©ration de contraintes
Rappelons que notre processus repose sur une projection initiale de type ACP. Nous noterons
d(a, b) la distance entre les objets a et b aprÃ¨s projection. Implicitement, nous considÃ©rons ici
L. Martin et al.
que la projection se fait en trois dimensions. Nous Ã©tudions cinq types de contraintes :
C2inf : pour gÃ©nÃ©rer des contraintes du type d(a, b) â‰¤ dËœ, nous devons choisir deux objets a
et b ainsi quâ€™une valeur de distance seuil dËœ. Puisque lâ€™on souhaite ici rapprocher les pro-
jections des objets a et b, il faut choisir deux objets Ã©loignÃ©s dans la projection courante,
mais proches relativement Ã  la distance dald. Le couple dâ€™objets le plus mal reprÃ©sentÃ©
sera donc celui pour lequel le rapport d(a,b)dald(a,b) est maximum. La distance seuil choisie
sera alors dald(a, b),
C2sup : dans le mÃªme esprit, nous pourrons introduire la contrainte d(a, b) â‰¥ dald(a, b) en
choisissant le couple (a, b) qui minimise d(a,b)dald(a,b) ,
C3ald : les contraintes de type C3 sont de la forme d(a, c) â‰¤ Î´d(a, b). Il est donc nÃ©cessaire
de choisir trois objets et une valeur seuil. Nous proposons ici de gÃ©nÃ©rer ce type de
contraintes pour des objets a, b, c tels que a et c soient de mÃªme classe, b dâ€™une autre
classe, lâ€™idÃ©e Ã©tant de rapprocher c (projetÃ©) de a (projetÃ©), en le positionnant par rapport
Ã  b. Nous choisissons les objets a, b, c qui maximisent d(a,c)d(a,b) . Le seuil choisi ici sera :
Î´ = dald(a,c)dald(a,b) ,
C31 : PlutÃ´t que de fixer le seuil Î´ en fonction de lâ€™ALD, nous proposons ici de le fixer
simplement Ã  1. Ainsi, la contrainte produite est : d(a, c) â‰¤ d(a, b), i.e. lâ€™objet c doit
Ãªtre plus proche de lâ€™objet a que ne lâ€™est lâ€™objet b. Le choix des objets a, b et c se fait sur
le mÃªme critÃ¨re que prÃ©cÃ©demment.
C31/2 : pour ce dernier type de contraintes, nous fixons le seuil Î´ Ã  1/2, ce qui a pour objectif
de sÃ©parer plus nettement les classes.
Dans nos expÃ©rimentations, chaque test porte exclusivement sur lâ€™un de ces types de con-
traintes. Les contraintes sont introduites une Ã  une, suivant lâ€™ordre de prioritÃ© spÃ©cifiÃ© ci-dessus.
La premiÃ¨re contrainte est dÃ©terminÃ©e en fonction de la distance initiale (ACP sans con-
traintes), ce qui fournit une seconde reprÃ©sentation (ACP avec une contrainte), induisant une
nouvelle distance qui permet de construire une seconde contrainte, et ainsi de suite. Ce mod-
Ã¨le simule ainsi le comportement dâ€™un utilisateur qui ajoute itÃ©rativement des contraintes en
fonction de la reprÃ©sentation 3D courante (laquelle tient compte des contraintes prÃ©cÃ©dentes).
Les contraintes de voisinage Cv Ã©tant des ensembles de contraintes de types C2, nous
nâ€™avons pas inclu ces contraintes dans nos expÃ©rimentations
5.2 CritÃ¨re dâ€™inertie sur un jeu de donnÃ©es artificiel
Nous considÃ©rons ici un jeu de donnÃ©es artificiel, oÃ¹ les 75 objets sont des mots dÃ©crits par
48 attributs :
â€“ 14 attributs syntaxiques incluant : le nombre de caractÃ¨res, de voyelles, de consonnes, le
nombre dâ€™occurrence des 10 lettres les plus frÃ©quentes dans la langue franÃ§aise,
â€“ 4 attributs binaires reprÃ©sentant la catÃ©gorie syntaxique (adjectif, nom, verbe, nom pro-
pre), certains mots peuvent appartenir Ã  plusieurs catÃ©gories (ex. orange),
â€“ 20 attributs sÃ©mantiques reprÃ©sentant des nombres dâ€™occurrences de ces mots dans des
collections de documents,
â€“ 10 attributs de bruit, gÃ©nÃ©rÃ©s alÃ©atoirement.
IntÃ©gration interactive de contraintes et visualisation
Lâ€™intÃ©rÃªt de ce jeu de donnÃ©es est de pouvoir choisir diffÃ©rentes classifications cibles (syntaxe,
sÃ©mantique, catÃ©gorie). La classe cible choisie ici sera la classe thÃ©matique. Les mots sont
ainsi rÃ©partis en 4 classes thÃ©matiques disjointes, ces classes sont essentiellement induites par
les 20 attributs sÃ©mantiques. Par ailleurs, lâ€™un des objectifs est de tester si cette mÃ©thode permet
facilement Ã  lâ€™utilisateur de mettre lâ€™accent sur les caractÃ©ristiques importantes et de rÃ©duire
lâ€™impact des attributs de bruit.
Le critÃ¨re de qualitÃ© que nous avons retenu est :Q = variance interâˆ’classevariance totale : plus sa valeur est
Ã©levÃ©e, plus les classes sont â€œsÃ©parÃ©esâ€. Ce critÃ¨re est calculÃ© sur la reprÃ©sentation associÃ©e aux
trois dimensions les plus significatives (laquelle correspond Ã  une bonne organisation visuelle).
La figure 9 prÃ©sente lâ€™Ã©volution de ce critÃ¨re suivant le nombre et le type de contraintes.
La ligne horizontale supÃ©rieure reprÃ©sente le rapport variance interâˆ’classevariance totale obtenu par ALD, i.e.
la valeur optimale pour ce rapport (94,2%). On peut remarquer que la plupart des types de
contraintes permet dâ€™amÃ©liorer ce rapport, Ã  lâ€™exception des contraintes C2sup. Ces derniÃ¨res
tendent Ã  Ã©loigner des objets, et devraient donc permettre dâ€™amÃ©liorer le critÃ¨re. Cependant,
dans lâ€™espace dâ€™origine, la variance totale est plus grande (les objets sont en moyenne plus
Ã©loignÃ©s). Or les seuils sont choisis par rapport aux distances induites par lâ€™ALD. Par con-
sÃ©quent, les seuils choisis sont vraisemblablement assez mal adaptÃ©s. De plus, ce type de con-
traintes est en fait redondant avec le critÃ¨re optimisÃ© dans lâ€™ACP. Les contraintes de type C2inf
fournissant ici les meilleurs rÃ©sultats, nous les utiliserons pour la suite de nos tests.
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Va
ria
nc
e 
in
te
r-c
la
ss
e/
va
ria
nc
e 
to
ta
le
Nombre de contraintes
C2inf
C3_1/2
C3_lda
C3_1
C2sup
FIG. 9 â€“ Evolution de variance interâˆ’classevariance totale (donnÃ©es synthÃ©tiques)
5.3 Visualisation du jeu de donnÃ©es artificiel
La figure 10 prÃ©sente les organisations visuelles obtenues avec respectivement 0, 5, 30 et
100 contraintes de type C2inf gÃ©nÃ©rÃ©es automatiquement, pour le mÃªme jeu de donnÃ©es que
dans la section prÃ©cÃ©dente. A titre de comparaison, nous prÃ©sentons Ã©galement la projection
obtenue par ALD. Chaque classe est caractÃ©risÃ©e par une forme et une couleur. Nous pouvons
L. Martin et al.
Sans contraintes Avec 5 contraintes Avec 30 contraintes
Avec 100 contraintes Projection par ALD
FIG. 10 â€“ Projections 3D obtenues avec un nombre croissant de contraintes C2inf
constater que la sÃ©paration des classes progresse rapidement lorsque le nombre de contraintes
augmente. Ces tests nous permettent de montrer que, pour ce jeu de donnÃ©es, avec une trentaine
de contraintes, le rapport est proche de lâ€™optimal. Cela correspond Ã  un nombre de contraintes
tout Ã  fait acceptable si elles doivent Ãªtre ajoutÃ©es par un utilisateur. De plus, les temps de
calcul pour la rÃ©solution sont ici dâ€™environ une seconde dans le cas avec 100 contraintes.
5.4 CritÃ¨re dâ€™inertie sur des jeux de donnÃ©es usuels
Nous avons rÃ©alisÃ©s les mÃªmes tests que ceux que la section 5.2 sur 6 jeux de donnÃ©es
usuels en apprentissage : breast cancer, glass, iris, wine, yeast et zoo (de lâ€™UCI repository). La
figure 11 prÃ©sente lâ€™Ã©volution du critÃ¨reQ pour les contraintesC2inf et confirment la tendance
observÃ©e sur le jeu de donnÃ©es artificiel.
Afin de pouvoir prÃ©senter ces courbes sur un mÃªme graphique, les valeurs reprÃ©sentÃ©es sont
relative Ã  la valeur du mÃªme critÃ¨re dans le cadre de lâ€™ALD : QALD. Les courbes reprÃ©sen-
tent donc lâ€™Ã©volution du taux Q/QALD, une valeur proche de 100% correspond donc Ã  une
rÃ©partition variance inter-classe/variance intra-classe proche de celle obtenue grÃ¢ce Ã  lâ€™ALD.
Les variations observÃ©es ici sont proches des celles obtenues sur le jeu de donnÃ©es artificiel.
Nous pouvons remarquer que pour la plupart des jeux de donnÃ©es, le taux optimal est obtenu
IntÃ©gration interactive de contraintes et visualisation
avec un nombre trÃ¨s rÃ©duit de contraintes, Ã  lâ€™exception de â€œbreast cancerâ€ pour lequel une
trentaine de contraintes est nÃ©cessaire.
Une anomalie semble se produire sur le jeu â€œglassâ€ puisque le taux obtenu par notre ap-
proche dÃ©passe celui obtenu par ALD alors que ce dernier est optimal, il nâ€™est donc pas pos-
sible de le dÃ©passer thÃ©oriquement. Une explication possible pour ce phÃ©nomÃ¨ne concerne la
variance totale qui est trÃ¨s faible pour lâ€™ALD (0.18 contre 1.89 pour notre approche). La con-
figuration obtenue par ALD correspond donc Ã  une trÃ¨s faible dispersion globale, il est trÃ¨s
possible que la solution effectivement obtenue par ALD ne soit pas exacte : en effet, lâ€™ALD
nÃ©cessite lâ€™inversion dâ€™une matrice, cette opÃ©ration est connue comme Ã©tant assez sensible aux
variations, par exemple dues aux approximations liÃ©es aux calculs sur les flottants.
 30
 40
 50
 60
 70
 80
 90
 100
 110
 0  20  40  60  80  100
Ta
ux
 d
â€™in
er
tie
 in
te
r-c
la
ss
e 
re
la
tiv
e 
(%
)
Nombre de contraintes (de type C2inf)
breast
glass
iris
wine
yeast
zoo
FIG. 11 â€“ Evolution de variance interâˆ’classevariance totale (donnÃ©es UCI)
5.5 Variation du taux de classification
Nous proposons ici un second critÃ¨re consistant Ã  mesurer le taux de classification correcte
par validation croisÃ©e. Les contraintes et la reprÃ©sentation sont calculÃ©es Ã  partir dâ€™un ensemble
test. Lâ€™ensemble dâ€™apprentissage est ensuite projetÃ© dans lâ€™espace de reprÃ©sentation obtenu oÃ¹
les objets sont classÃ©s en fonction de leur(s) plus proche(s) voisin(s). La figure 12 prÃ©sente les
rÃ©sultats obtenus sur notre jeu de donnÃ©es artificiel (10 exÃ©cutions de 5-fold cross-validation,
1-plus proche voisins). La ligne horizontale supÃ©rieure correspond au taux obtenu par ALD.
Les contraintes nâ€™apportent pas ici dâ€™amÃ©lioration du taux de classification, Ã  lâ€™exception
des contraintesC31 etC31/2. Nous expliquons ce rÃ©sultat par le fait que ces deux types de con-
traintes ne dÃ©pendent pas du rÃ©sultat de lâ€™ALD. Or ce dernier est assez sensible au phÃ©nomÃ¨ne
de sur-apprentissage sur ce jeu de donnÃ©es (100% de classification correcte sur lâ€™ensemble
dâ€™apprentissage, 78.5% sur lâ€™ensemble test). De mÃªme, pour les autres types de contraintes, les
mauvaises performances obtenues sont sans doute liÃ©es Ã  lâ€™utilisation de lâ€™ALD. Notons que
lâ€™on retrouve des rÃ©sultats analogues sur les jeux de donnÃ©es de lâ€™UCI.
L. Martin et al.
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
Ta
ux
 d
e 
cla
ss
ific
at
io
n 
co
rre
ct
e
Nombre de contraintes
C2inf
C3_1/2
C3_lda
C3_1
C2sup
FIG. 12 â€“ Taux de classification correcte (donnÃ©es synthÃ©tiques)
5.6 Souplesse de la mÃ©thode
Dans la simulation proposÃ©e, lâ€™utilisateur choisit systÃ©matiquement dâ€™agir sur la plus forte
distorsion observÃ©e vis-Ã -vis de la projection par analyse discriminante. Afin de relativiser
lâ€™importance du choix de la distorsion, nous proposons quelques mesures complÃ©mentaires.
La figure 13 prÃ©sente lâ€™Ã©volution du rapport variance interâˆ’classevariance totale dans un cadre assoupli : lâ€™u-
tilisateur ne choisit plus Ã  chaque Ã©tape la distorsion la plus importante, mais la centiÃ¨me dis-
torsion par ordre dâ€™importance dÃ©croissante. Nous pouvons observer que les rÃ©sultats sont trÃ¨s
similaires Ã  ceux de la figure 11. En consÃ©quence, un utilisateur rÃ©el disposera dâ€™une certaine
marge de manoeuvre dans le choix des distorsions Ã  corriger.
 30
 40
 50
 60
 70
 80
 90
 100
 110
 0  20  40  60  80  100
Ta
ux
 d
â€™in
er
tie
 in
te
r-c
la
ss
e 
re
la
tiv
e 
(%
)
Nombre de contraintes
breast
glass
iris
wine
yeast
zoo
FIG. 13 â€“ Evolution de variance interâˆ’classevariance totale (choix assoupli)
IntÃ©gration interactive de contraintes et visualisation
5.7 Mesures complÃ©mentaires
Nous proposons ici deux mesures complÃ©mentaires portant sur les jeux de test de lâ€™UCI,
concernant lâ€™Ã©volution des distorsions lors de lâ€™ajout de contraintes. La figure 14 prÃ©sente
lâ€™Ã©volution de la somme des distorsions observÃ©es. Etant donnÃ©e la diversitÃ© des jeux, nous
avons retenu une Ã©chelle logarithmique en ordonnÃ©e. Dans lâ€™ensemble, on observe que cette
somme dÃ©croit rapidement puis se stabilise aprÃ¨s un nombre de contraintes en rapport avec la
somme initiale des distorsions. Cette derniÃ¨re donne donc une indication du nombre de con-
traintes nÃ©cessaires. Il convient toutefois de souligner quâ€™en utilisation rÃ©elle, semi-supervisÃ©e,
lâ€™utilisateur ne dispose dâ€™aucun moyen dâ€™estimer automatiquement les distorsions et donc leur
somme. Le principal enseignement de cette figure est quâ€™un choix judicieux des contraindre
aura tendance Ã  rÃ©duire les distorsions. Notons que les calculs effectuÃ©s en mode assoupli (voir
section prÃ©cÃ©dente) conduisent Ã  des courbes similaires.
 0.1
 1
 10
 100
 1000
 10000
 100000
 0  20  40  60  80  100
So
m
m
e 
to
ta
le
 d
es
 d
ist
or
sio
ns
 (lo
g)
Nombre de contraintes
breast
glass
iris
wine
yeast
zoo
FIG. 14 â€“ Evolution de la somme des distorsions en fonction du nombre de contraintes
La figure 15 montre la dÃ©croissance rapide des distorsions lors de lâ€™ajout de contraintes.
Pour cette figure, nous avons tout dâ€™abord calculÃ© les distorsions avant ajout de contraintes,
et nous avons notÃ© la valeur de la milliÃ¨me distorsion par ordre dÃ©croissant. Nous avons en-
suite notÃ©, aprÃ¨s chaque ajout de contrainte, le nombre de distorsions prÃ©sentant une valeur
supÃ©rieure Ã  ce seuil. Nous pouvons observer que dans lâ€™ensemble ce nombre chute rapide-
ment. Nous pouvons en dÃ©duire que lâ€™ajout de chaque contrainte a une forte influence sur
lâ€™ensemble des distorsions. A nouveau, les calculs effectuÃ©s en mode assoupli conduisent Ã  des
rÃ©sultats similaires.
6 Conclusion et perspectives
Nous avons prÃ©sentÃ© dans cet article une mÃ©thode de rÃ©duction de dimension autorisant
lâ€™ajout itÃ©ratif et intuitif de contraintes de positionnement des objets dans lâ€™espace de projec-
tion. Nous avons observÃ© que pour diffÃ©rents jeux de donnÃ©es, lâ€™ajout dâ€™un nombre restreint de
contraintes permet dâ€™obtenir une solution satisfaisante. En consÃ©quence, une telle approche ne
L. Martin et al.
 0
 200
 400
 600
 800
 1000
 0  5  10  15  20  25N
om
br
e 
de
 d
ist
or
sio
ns
 s
up
er
ie
ur
es
 a
u 
se
ui
l
Nombre de contraintes
breast
glass
iris
wine
yeast
zoo
FIG. 15 â€“ Evolution du nombre des distorsions les plus fortes
peut que contribuer Ã  la diffusion des mÃ©thodes de rÃ©duction de dimension auprÃ¨s dâ€™utilisateurs
ne disposant pas de connaissances spÃ©cifiques sur ces techniques.
De plus, cette approche introduit le concept dâ€™ajout progressif de contraintes, pour lequel
elle propose une solution. Ce concept nous semble particuliÃ¨rement prometteur, puisquâ€™il per-
met de limiter la complexitÃ© du systÃ¨me de contraintes sous-jacent.
Dans le cadre dâ€™un tel outil, il est important que les temps de calcul restent suffisament
faibles pour permettre lâ€™interaction avec un utilisateur. Il convient donc de souligner que lors
des tests menÃ©s, les temps de calcul sont toujours restÃ©s faibles, en gÃ©nÃ©ral en deÃ§a dâ€™une
seconde, alors que nous utilisions une machine standard, de type ordinateur portable.
Dans notre approche, nous utilisons un calcul de projection sous contraintes, Ã  partir des
dimensions dâ€™origine. Une autre mÃ©thode envisageable aurait consistÃ© Ã  effectuer une pro-
jection initiale, puis Ã  ne travailler ensuite que sur la matrice de distances / dissimilaritÃ©s
sous-jacente, par le biais, par exemple, dâ€™une technique de positionnement multidimension-
nel comme le least square scaling proposÃ© par Sammon (1977). Cela pourra faire lâ€™objet de
rÃ©flexions ultÃ©rieures.
Ces travaux sâ€™inscrivent dans le cadre dâ€™un projet ANR, dont lâ€™un des objectifs est de per-
mettre Ã  des utilisateurs dâ€™obtenir une organisation (visuelle) de diffÃ©rents styles dâ€™Ã©critures.
Dans ce cadre, chaque objet reprÃ©sente une image (associÃ©e Ã  un style dâ€™Ã©criture) dÃ©crite par
un ensemble de caractÃ©ristiques extraites automatiquement Ã  partir de lâ€™image seule. Les styles
dâ€™Ã©criture ne sont pas Ã©tiquetÃ©s, mais les palÃ©ographes sont capables de quantifier la ressem-
blance entre deux styles dâ€™Ã©criture (images). Une validation de cette approche par des experts
du domaine est en cours. Elle pourrait conduire Ã  lâ€™introduction de nouveaux types de con-
traintes.
RÃ©fÃ©rences
Arrow, K., L. Hurwicz, et H. Uzawa (1958). Studies in Nonlinear Programming. Stanford,
CA : Stanford University Press.
IntÃ©gration interactive de contraintes et visualisation
Bar-Hillel, A., T. Hertz, N. Shental, et D. Weinshall (2005). Learning a mahalanobis metric
from equivalence constraints. Journal of Machine Learning Research 6, 937â€“965.
Blum, A. L. et P. Langley (1997). Selection of relevant features and examples in machine
learning. Artificial Intelligence 97, 245â€“271.
Da Costa, D. et G. Venturini (2007). A visual and interactive data exploration method for large
data sets and custering. In Springer (Ed.), ADMA2007, Number 4632 in LNAI, Harbin,
China, pp. 553â€“561.
Demartines, P. et J. HÃ©rault (1997). Curvilinear component analysis : A self-organizing neural
network for nonlinear mapping of data sets. IEEE Transaction on Neural Networks 8(1),
148â€“154.
Fisher, R. (1936). The use of multiple measurements in taxonomic problems. Annals of Eu-
genics 7, 179â€“188.
Guyon, I. et A. Elisseeff (2003). An introduction to variable and feature selection. Journal of
Machine Learning Research 3, 1157â€“1182.
Roweis, S. T. et L. K. Saul (2000). Nonlinear Dimensionality Reduction by Locally Linear
Embedding. Science 290(5500), 2323â€“2326.
Sammon, P. H. (1977). A discrete least squares method. Mathematics of Computation 31(137),
60â€“65.
Tenenbaum, J. B., V. de Silva, et J. C. Langford (2000). A global geometric framework for
nonlinear dimensionality reduction. Science 290(5500), 2319â€“2323.
Weinberger, K. Q., J. Blitzer, et L. K. Saul (2005). Distance metric learning for large margin
nearest neighbor classification. In NIPS.
Weinberger, K. Q. et L. K. Saul (2006). Unsupervised learning of image manifolds by semidef-
inite programming. International Journal of Computer Vision 70(1), 77â€“90.
Weinberger, K. Q. et L. K. Saul (2008). Fast solvers and efficient implementations for distance
metric learning. In W. W. Cohen, A. McCallum, et S. T. Roweis (Eds.), ICML, Volume 307
of ACM International Conference Proceeding Series, pp. 1160â€“1167. ACM.
Summary
Projecting and visualizing objects in a two- or tree-dimension space is a current data anal-
ysis task. From this visualization it might be of interest to offer to the user the ability to add
knowledge in the form of (di)similarity constraints among objects, when those appear either to
close or to far in the observation space. In this paper we propose three kinds of constraints and
present a resolution method that derives from PCA. Experiments have been performed with
both synthetic and usual datasets. They show that a representation with a high value of quality
criterion can be obtained with a limited set of constraints.
