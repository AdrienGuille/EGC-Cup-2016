Plans d’expe´riences optimaux :
un expose´ didactique
Jean-Pierre Gauchi1
INRA
Unite´ de Mathe´matiques et Informatique Applique´es (MIA)
Jouy-en-Josas, France
jean-pierre.gauchi@jouy.inra.fr
Re´sume´
Cet article est un expose´ didactique sur les plans d’expe´riences optimaux, essentiel-
lement pour les mode`les line´aires. Son objectif est de donner les de´finitions et les modes
de construction des principaux plans optimaux rencontre´s, avec un e´clairage base´ sur des
exemples tre`s simples. Le public vise´ est d’abord celui des personnels de recherche qui
souhaitent ame´liorer leur de´marche expe´rimentale, face a` des proble`mes expe´rimentaux
caracte´rise´s par des contraintes de divers types.
Abstract
This paper is a didactic article on the optimal experimental designs, primarily for the
linear models. Its objective is to give the definitions and the methods of construction
of the principal optimal plans met, with a lighting based on very simple examples. The
public concerned is initially that of the engineers of research who wish to improve their
experimental approach, relatively to experimental problems characterized by constraints
of various types.
Mots-cle´s : Plans d’expe´riences, optimalite´, re´gression, didactique.
1 Introduction
Cet article est un expose´ didactique, e´le´mentaire meˆme, sur les plans d’expe´riences
optimaux. Il reprend certains e´le´ments, en les simplifiant, de Gauchi (1997), publication
ou` le lecteur pourra comple´ter ses connaissances sur ce sujet, ainsi que dans l’excellent
ouvrage de Atkinson et Donev (1992). L’objectif de cet article est de de´finir pre´cise´ment les
crite`res d’optimalite´ les plus fre´quemment rencontre´s, et de montrer quelques algorithmes
de construction de base, a` l’aide d’exemples tre`s simples. Le public vise´ est d’abord celui
des personnels de recherche qui souhaitent ame´liorer leur de´marche expe´rimentale, face a`
des proble`mes caracte´rise´s par des contraintes de divers types, tout en ge´rant au mieux
la variabilite´ expe´rimentale. Le deuxie`me paragraphe illustre ces types de contraintes
fre´quemment rencontre´es dans quelques situations classiques. Les outils ne´cessaires sont
a` l’intersection de la statistique, de l’analyse convexe et de l’optimisation. Le pre´-requis
pour la lecture de cet article est une formation e´le´mentaire en statistique et en calcul
matriciel d’une part, et une connaissance des plans d’expe´riences factoriels usuels d’autre
part.
c© Revue MODULAD, 2005 -139- Nume´ro 33
Cette conception de l’optimalite´ dans le domaine des plans d’expe´riences, outils eux-
meˆmes fonde´s par le statisticien Fisher et ses ”disciples” dans les 1925 a` 1935, est due
aux travaux de Kiefer a` partir de la fin des anne´es 1950, puis a` ceux de Fedorov a` partir
de la fin des anne´es 1960. Plus tard, plusieurs plans classiques ont pu apparaˆıtre comme
des plans optimaux pour des crite`res d’optimalite´ propose´s par Kiefer. Si les objectifs
des plans classiques et des plans optimaux sont essentiellement les meˆmes, les me´thodes
pour les construire e´taient au de´part fondamentalement diffe´rentes, essentiellement d’es-
sence alge´brique pour les premiers et algorithmique pour les seconds. Toutefois, on a pu
voir que certains types de plans classiques (recherche de plans en blocs e´quilibre´s par
exemple) ne´cessitaient e´galement l’usage d’algorithmes performants pour supple´er aux
me´thodes alge´briques. Et de fac¸on analogue, il faut aussi rappeler que les algorithmes
utilise´s pour construire les plans optimaux sont tre`s souvent ame´liore´s ou qualifie´s graˆce a`
des re´sultats importants de l’analyse convexe et de l’optimisation (Fedorov, 1980, Silvey,
1980). Finalement, aujourd’hui, la dichotomie quant aux me´thodes de construction des
plans n’est plus aussi marque´e, et on soulignera par ailleurs que la construction de plans
par des me´thodes alge´briques reste toujours un the`me de recherche fructueuse conduisant
a` des plans particulierement adapte´s aux facteurs explicatifs qualitatifs.
Bien que cette the´orie de l’optimalite´ puisse prendre en compte plusieurs formes de
mode`le de la re´ponse, par exemple jusqu’a` des syste`mes d’e´quations diffe´rentielles, nous
limiterons notre expose´ a` des mode`les de forme analytique explicite : les mode`les de
re´gression ou d’analyse de la variance, line´aires dans les parame`tres, et les mode`les de
re´gression (intrinse`quement) non line´aire dans les parame`tres. Enfin, nous supposerons que
les facteurs expe´rimentaux e´tudie´s (discrets ou continus) sont, par nature, controˆlables, et
l’erreur faite dans leur re´glage est ne´gligeable au regard de celle de l’expe´rience (au sens
de sa reproductiblite´ ou de la re´pe´tabilte´ des mesures re´alise´es). Dans le langage usuel des
statisticiens on parle de mode`les a` effets ”fixes”.
La structure de cet article est la suivante : apre`s une section consacre´e a` l’e´vocation de
quelques situations expe´rimentales re´elles a` but de sensibilisation, deux sections suivent,
l’une pour les mode`les line´aires et l’autre pour les mode`les de re´gression non line´aire. Une
conclusion cloˆture l’article.
2 Quelques situations expe´rimentales re´elles
Pour entrer rapidement dans le vif du sujet nous pre´sentons maintenant quelques
exemples de situations expe´rimentales particulie`res mais concre`tes et fre´quentes (de´ja`
pre´sente´es dans Gauchi (1997)). Celles-ci sont difficiles ou meˆme impossibles a` appre´hender
avec des plans classiques, notamment les plans factoriels (complets et fractionnaires) ou
les plans de surface de re´ponse, a` cause des contraintes qui les caracte´risent.
2.1 Domaine expe´rimental sous contraintes
On cherche a` mode´liser a` l’aide d’un polynome le rendement d’une re´action chimique en
fonction de deux facteurs continus, la tempe´rature du milieu re´actionnel et la pression
dans le re´acteur, pour disposer in fine a` la fois d’estimations fiables des parame`tres tra-
duisant l’effet de ces facteurs, et d’un mode`le pre´dictif de bonne qualite´ sur le domaine
expe´rimental explore´. On ne dispose pas de mode`le the´orique pour la re´ponse ”rende-
c© Revue MODULAD, 2005 -140- Nume´ro 33
ment” qui lie les re´actifs en pre´sence. On se contente donc, en premie`re approximation,
du polynome du second degre´ :
y = β0 + β1x1 + β2x2 + β11x
2
1 + β22x
2
2 + β12x1x2 + ε (1)
ou` y est la re´ponse, et les coefficients βi, βii et β12 repre´sentent respectivement les effets
principaux, les effets quadratiques et l’interaction double des deux facteurs continus, pa-
rame`tres que l’on cherche a` estimer ; ε est un terme d’erreur, de distribution inconnue
mais suppose´ de variance constante sur le domaine que l’on cherche a` explorer.
Supposons que les e´chelles des deux facteurs x1 et x2 soient code´es de telle fac¸on que
leurs niveaux extreˆmes minimum et maximum soient −1 et +1. Puisque ce sont des fac-
teurs continus ils de´finissent donc un domaine expe´rimental de dimension 2 repre´sente´
par l’inte´rieur (compact) d’un carre´ dont les coordonne´es des sommets sont (−1,+1),
(+1,+1), (−1,−1), (+1,−1). Pour le mode`le (1) et ce domaine on sait qu’un plan
d’expe´riences possible, orthonorme´ si on transforme les termes quadratiques par codage
avec les polynoˆmes orthonorme´s (Kobilinsky, 1988), pourrait eˆtre constitue´ de re´pe´ttions
d’expe´riences en les neuf points de support d’un plan factoriel 32 (voir Gauchi, 1997,
page 327). Pour des raisons de faisabilite´ chimique imaginons maintenant que l’on ne
puisse pas re´aliser d’expe´riences dans certaines zones du domaine expe´rimental telles que
trois expe´riences parmi les neuf deviennent interdites. Le domaine expe´rimental carre´ est
devenu un polygone quelconque (voir Tableau II pour un exemple). La qualite´ du plan
se de´grade, notamment il y a perte d’orthogonalite´. Avec le plan forme´ de re´pe´titions
d’expe´riences sur les six points de support restants, il sera difficile de distinguer les in-
fluences marginales de ces deux facteurs, et la pre´cision des estimateurs que l’on obtiendra
sera beaucoup plus faible. La question qui se pose est donc de trouver d’autres expe´riences
dans ce domaine sous contraintes pour permettre une estimation optimise´e de ce mode`le.
2.2 Mode`le polynomial particulier
Un cas e´galement fre´quent est celui ou` certains termes sont absents du de´veloppement
limite´ habituel. On peut donner, a` titre d’illustration, la forme polynomiale incomple`te
pour quatre xi continus :
y = β0 +
4∑
i=1
βixi + β12x1x2 + β14x1x4 + β11x
2
1 + β33x
2
3 + ε
Avec ce mode`le particulier un plan de surface de re´ponse usuel ne sera pas optimal.
2.3 Mode`le en Structure-Activite´
Les e´tudes de structure-activite´ ou de se´lection de mole´cules constituent un domaine
d’application pour lequel l’approche plan classique est particulie`rement inadapte´e. Le but
est de construire un mode`le quantitatif de la re´ponse ”activite´ biologique” en fonction des
descripteurs mole´culaires physico-chimiques (facteurs explicatifs) a` partir de mesures sur
un sous-ensemble de mole´cules. L’objectif est donc de se´lectionner des mole´cules dont on
va e´tudier l’activite´. On cherche a` attribuer a` chacune des mole´cules une valeur nume´rique
(une mesure en fait) traduisant leur importance dans l’e´tablissement d’un mode`le pre´dictif
de l’activite´ biologique de la meilleure qualite´ possible, c’est-a`-dire dont les pre´dictions
seront de variance minimale, en moyenne.
c© Revue MODULAD, 2005 -141- Nume´ro 33
2.4 Discrimination de mode`les
C’est une situation ou` on cherche un plan optimal pour a` la fois discriminer des mode`les
et faire de l’estimation de parame`tres. C’est notamment la situation ou` le chercheur
he´site entre plusieurs mode`les possibles pour expliquer ou repre´senter son phe´nome`ne.
Les mode`les sont line´aires ou non, peuvent eˆtre emboˆıte´s ou non, mais les solutions en
termes de crite`res d’optimalite´ ne couvrent pas toutes ces situations dont certaines sont
encore du domaine de la recherche en Statistique.
2.5 Mode`le non line´aire en cine´tique chimique
En cine´tique chimique on e´tudie souvent l’e´volution de la concentration d’un compose´
B issu d’un compose´ A et redonnant lui-meˆme un compose´ C. C’est une double re´action
du premier ordre, irre´versible, symbolise´e par : A → B → C. Le mode`le exponentiel a`
deux compartiments
η(t, θ) =
θ1
θ1 − θ2 [exp(−θ2t)− exp(−θ1t)] + ε (2)
solution d’un syste`me de deux e´quations diffe´rentielles line´aires du premier ordre, traduit
bien cette e´volution. On a : η(t, θ) la concentration du compose´ B de´pendant du temps t
et des parame`tres θ1 et θ2 constantes de vitesse des premie`re et seconde de´compositions,
et les conditions initiales (concentrations) [A]0 = 1, [B]0 = 1, [C]0 = 1. La question est
de trouver les temps optimaux auxquels faire les mesures de vitesse pour aboutir a` une
estimation optimise´e des parame`tres.
3 Plans optimaux pour mode`les line´aires
La the´orie de l’optimalite´ en plans d’expe´riences se fonde historiquement dans le cadre
du mode`le line´aire − re´gression et analyse de la variance − et de´bute avec les travaux de
Kiefer (1959, 1961, 1974), Kiefer et Wolfowitz (1959, 1960), Fedorov, (1969, 1972), Wynn
(1970), sans oublier l’article pionnier et particulie`rement pe´dagogique de Box et Lucas
(1959). Comme on peut formuler le mode`le d’analyse de variance comme un mode`le de
re´gression (voir §3.3), on se basera sur celui-ci pour exposer cette the´orie.
3.1 Le mode`le de re´gression line´aire
Pour exprimer la relation qui lie une re´ponse continue y de´pendant de h facteurs
explicatifs continus, controˆlables, xj, et leurs transforme´es continues, on conside`re simul-
tane´ment le mode`le de re´gression line´aire :
yik = f(xi)
Tβ + εik ; i = 1, . . . , NS ; k = 1, . . . , ri (3)
et le plan d’expe´riences discret associe´ :
ξN,NS ,{r} =

x1 . . . xi . . . xNS
r1 . . . ri . . . rNS
ri ≥ 1,∀i ;
∑NS
i=1 ri = N
 (4)
c© Revue MODULAD, 2005 -142- Nume´ro 33
ou` (les matrices et les vecteurs sont note´s en gras, et T de´signe l’ope´rateur de transposition
usuel) :
– yik est la k
ie`me observation de la re´ponse collecte´e au ie`me point de support xi de´fini
ci-dessous ; on notera y le vecteur (N × 1) des yik, N est le nombre total d’obser-
vations yik collecte´es sur les NS points de support, selon le sche´ma de re´pe´tition
{r} = {r1, ..., rNS}.
– xi est un vecteur (h × 1) dont les composantes sont les valeurs des h facteurs ex-
plicatifs xj, j = 1, ..., h, c’est-a`-dire xi = (xi1, ..., xij, . . . , xih)
T pour l’expe´rience i
(appele´e aussi une condition expe´rimentale ou un traitement) ; quand l’indice de
l’expe´rience ne sera pas pre´cise´ on notera simplement x ; on a xi ∈ Ξ ⊂ Rh, Ξ e´tant
le domaine experimental, suppose´ sous-ensemble compact de Rh ; dans les cas les
plus courants Ξ est un hyper-rectangle, ou un polye`dre convexe, ou un ellipso¨ıde
(d’excentricite´ 0 ou plus).
– f(xi) est un vecteur (p× 1), p ≥ h, dont les composantes fj(xi) sont les valeurs des
fonctions de re´gression fj(x), au point xi, avec j = 0, . . . , p− 1 ; par exemple, pour
le mode`le :
y = β0 + β1x1 + β2x2 + β3x3 + β11x
2
1 + β23x2x3 + β4Log(x3) + ε (5)
on a h = 3 et p = 7, et les fonctions de re´gression sont : f0(x) = 1, f1(x) = x1,
f2(x) = x2, f3(x) = x3, f4(x) = x
2
1, f5(x) = x2x3, f6(x) = Log(x3).
La matrice XN , de dimensions (N × p), est appele´e matrice du mode`le ; ses lignes
sont forme´es par les vecteurs lignes f(xi)
T ; les h colonnes de XN relatives aux h
facteurs constituent une matrice (N × h), appele´e matrice du plan (4) ; les p − h
colonnes restantes sont forme´es par les transforme´es des h facteurs explicatifs (f0(x),
f4(x), f5(x), f6(x) dans (5)).
– β est un vecteur (p × 1) des p parame`tres (ou coefficients), inconnus, certains, du
mode`le.
– ξN,NS ,{r} est un plan discret a` N expe´riences appartenant a` Ψ l’ensemble discret
des plans d’expe´riences engendre´ par toutes les valeurs possibles de NS tel que
p ≤ NS ≤ Nmax, et pour tous les sche´mas possibles de re´pe´tition {r}. En ge´ne´ral
Nmax est fixe´ a priori pour un proble`me donne´ (de´pend du budget alloue´).
– εik est l’erreur (scalaire) ale´atoire attache´e a` l’observation yik ; c’est une erreur
expe´rimentale ; le vecteur des N erreurs εik sera note´ ε, d’espe´rance E(ε) = 0,
et de matrice de variance inconnue (en ge´ne´ral) Σ = diag {σ2i , i = 1, . . . , N} ; si on
suppose que la variance est homoge`ne on posera σ2i = σ
2, ∀i ; on noteraW = Σ−1
la matrice de ponde´ration.
Remarque : dans la pratique, en situation de plan d’expe´riences (ou` l’on ne dispose
pas encore des observations yik) il faut disposer d’estimations pre´alables et sans biais
de σ2 ou des σ2i , calcule´es a` partir de donne´es ante´rieures.
Matriciellement on pourra e´crire :
y = XNβ + ε
3.2 Estimation
Si la variance est homoge`ne un estimateur usuellement utilise´ dans ce contexte est
l’estimateur des moindres carre´s βˆMC obtenu en minimisant par rapport a` β le crite`re
c© Revue MODULAD, 2005 -143- Nume´ro 33
”somme des carre´s des erreurs”
SN(β) =
N∑
i=1
ε2i
= (y −XNβ)T (y −XNβ)
On obtient facilement, sous re´serve d’inversibilite´ de XTX :
βˆMC = (X
T
NXN)
−1XTNy
et sa variance (minimale) :
V(βˆMC) = σ
2(XTNXN)
−1 (6)
Si la variance est he´te´roge`ne (les σi sont diffe´rents) on minimise la ”somme des carre´s des
erreurs ponde´re´es” :
SN(β) = (y −XNβ)TW (y −XNβ)
et la solution est l’estimateur des moindres carre´s ponde´re´s βˆMCP :
βˆMCP = (X
T
NWXN)
−1XTNWy
de variance
V(βˆMC) = (X
T
NWXN)
−1
Pour alle´ger l’e´criture par la suite, mais sans perte de ge´ne´ralite´, on supposera la variance
homoge`ne e´gale a` σ2 ; on notera de fac¸on simplifie´e βˆMC par βˆ.
On appelle MN = X
T
NXN la matrice d’information du plan (4), et M
N
N = (1/N)MN
la matrice des moments ou matrice d’information moyenne par unite´ pour un plan discret
a` N expe´riences ; elle est normalise´e par de´finition. Cette normalisation est essentielle
puisqu’elle va autoriser la comparaison des matrices d’information de plans a` nombres
d’expe´riences diffe´rents. Par ailleurs, les observations e´tant suppose´es inde´pendantes, la
proprie´te´ d’additivite´ nous autorise a` e´crire cette matrice d’information comme une somme
ponde´re´e de NS matrices f(xi)f(xi)
T de rang 1 :
MNN =
Ns∑
i=1
ri
N
f(xi)f(xi)
T (7)
3.3 Nature et transformation des facteurs explicatifs
Par de´finition, dans un mode`le de re´gression les facteurs explicatifs doivent eˆtre de
nature au moins quantitative, et sont soit discrets (par exemple un facteur tempe´rature
ne pouvant prendre que les niveaux discrets 0, 10, 20, ...., 100, sur une e´chelle de 0
a` 100◦C), soit continus. Ne´anmoins, on peut vouloir e´tudier l’effet de facteurs de na-
ture intrinse`quement qualitative (donc discre`te), par exemple l’influence de trois types de
mate´riels. Par ailleurs, les effets des facteurs explicatifs (quantitatifs), et de leurs trans-
forme´es (telles les interactions et les effets quadratiques) se traduisent par les valeurs des
composantes βˆj de βˆ.
Deux proble`mes se posent donc : le premier est de rendre tous les facteurs quantitatifs,
et le second est de les exprimer dans des e´chelles telles que les effets βˆj soient tous
c© Revue MODULAD, 2005 -144- Nume´ro 33
comparables entre eux. Pour re´soudre le premier deux cas se pre´sentent selon le nombre
de modalite´s prises par le facteur qualitatif : si celui-ci pre´sente deux modalite´s seulement,
il suffira de les coder syme´triquement autour de 0, par exemple -1 et +1 ; s’il existe q
modalite´s (q > 2) ce cas est plus de´licat mais on peut utiliser les solutions propose´es
par Hardin et Sloane (1993). Pour le second proble`me, en supposant maintenant que
tous nos facteurs sont quantitatifs, on doit rendre leurs e´chelles comparables. Ceci se fait
facilement par une transformation au moyen des polynomes orthonorme´s (Kobilinsky,
1987). Par exemple, pour un facteur a` quatre niveaux -3, -1, +1, +3 on obtiendra les
niveaux respectifs −3/√5, −1/√5, +1/√5, +3/√5 pour coder sa transforme´e line´aire,
puis les niveaux +1, −1, −1, +1 pour coder sa transforme´e quadratique, et enfin les
niveaux −1/√5, +3/√5, −3/√5, +1/√5 pour coder sa transforme´e cubique. Les trois
vecteurs correspondants sont tous de meˆme norme (= 2) et orthogonaux entre eux.
3.4 Crite`re de D-optimalite´
Il existe aujourd’hui de nombreux crite`res d’optimalite´. Il n’est pas question d’en dres-
ser une liste exhaustive ici, mais uniquement de pre´senter les plus fre´quemment utilise´s.
Pour eˆtre complet, il faudrait faire cette pre´sentation dans le cadre de mesures continues
(voir par exemple Silvey, 1980), mais dans le cadre de cet expose´ on se contentera de les
pre´senter dans le cadre de plans discrets comme de´finis en (4).
3.4.1 De´finition
Un plan d’expe´riences D-optimal (D comme de´terminant) a` N expe´riences mini-
mise le de´terminant de la matrice de variance de βˆ, ou de fac¸on e´quivalente (ce qui
est nume´riquement plus le´ger) maximise le de´terminant de la matrice d’informationMN :
ξDN,N∗S ,{r∗} = Arg
{
max
ξN∈Ψ
det(MN(ξN,NS ,{r}))
}
(8)
On remarque dans cette de´finition que pour un nombre total fixe d’expe´riences N ,
le proble`me de maximisation prend en compte aussi la recherche des valeurs optimales
N∗S, {r∗} . C’est un proble`me d’optimisation multiple. On peut aussi se contenter de re-
chercher le maximum pour NS et {r} fixe´s, ou NS fixe´ seulement, selon le type de proble`me
expe´rimental a` re´soudre.
3.4.2 Exemple
Pour le mode`le de droite de re´gression y = β0 + β1x + ε, pour lequel on suppose σ
2
connue, on souhaite estimer au mieux (variances petites et covariance nulle) les parame`tres
β0 et β1, a` partir d’observations yi recueillies en x ∈ [−1,+1]. Dans ce but on envisage
quatre plans discrets que l’on donne dans le tableau I ainsi que leurs de´terminants bruts
et norme´s. On remarque que detMNN est bien un de´terminant norme´ car :
detMNN = det(
1
N
MN) = det(
1
N
X
T
NXN) = N
−p det(X
T
NXN) = N
−p det(MN)
c© Revue MODULAD, 2005 -145- Nume´ro 33
ξ13,3,{r} =
{ −1 0 +1
1 1 1
}
−→ detM(1)N = 6 ; detMN(1)N = 0.67
ξ23,3,{r} =
{ −1 −1 +1
1 1 1
}
−→ detM(2)N = 8 ; detMN(2)N = 0.89
ξ32,2,{r} =
{ −1 +1
1 1
}
−→ detM(3)N = 4 ; detMN(3)N = 1
ξ44,2,{r} =
{ −1 +1
2 2
}
−→ detM(4)N = 16 ; detMN(4)N = 1
Tableau I : Quatre plans discrets pour estimer une droite de re´gression, et les de´terminants
bruts et norme´s correspondants.
A l’examen du tableau I le plan ξ44,2 apparaˆıt tre`s nettement comme le D-meilleur
plan. Si on compare les de´terminants norme´s, il est e´quivalent au plan ξ32,2, c’est-a`-dire
que l’information (au sens de la D-optimalite´) moyenne par expe´rience est e´quivalente,
mais bien suˆr le plan ξ44,2 apportera globalement plus d’information que le plan ξ
3
2,2. En
outre, on remarque que les plans ξ13,3 et ξ
2
3,3 apportent plus d’information (globalement) que
le plan ξ32,2 mais l’efficacite´ de chacune des expe´riences de ces deux plans est moins bonne
compte tenu des valeurs prises par leurs de´terminants norme´s. Typiquement, l’expe´rience
”centrale” du plan ξ13,3 est tre`s peu performante vis-a`-vis de notre objectif. Enfin, on
rappelle qu’un plan orthogonal est tel que X
T
NXN = NIN ou` IN est la matrice indentite´
N ×N . On ve´rifie ainsi facilement que les plans ξ34,2 et ξ42,2 sont orthogonaux. En re´sume´,
le plan ξ44,2 est tre`s supe´rieur aux autres au sens de la D-optimalite´ : la formule (6) nous
indique que les estimateurs de la pente et de l’ordonne´e a` l’origine auront une covariance
nulle ; en outre, pour N = 4, ils seront de variance minimale.
3.4.3 D-efficacite´
On de´finit la D-efficacite´ d’un plan discret comme le pourcentage :
Deff = 100×
[
det(MNN)
det(M(piD))
] 1
p
(9)
ou`M(piD) est la matrice d’information d’un plan D-optimal a` mesure continue qu’il nous
faut de´finir succinctement maintenant, cette efficacite´ pre´sentant un grand inte´reˆt pratique
de`s lors que l’on sait calculer un plan D-optimal a` mesure continue.
3.4.4 Plan D-optimal a` mesure continue
Cette notion est une ide´e majeure et originale due a` Kiefer. Il conside`re le plan
d’expe´riences comme une mesure de probabilite´ continue sur un domaine expe´rimental
compact, on parle de plan a` mesure continue (”design measure”). C’est donc un objet
mathe´matique plutoˆt qu’un outil ope´rationnel pour faire des expe´riences re´elles. Mais
cet outil va s’ave´re´ eˆtre tre`s fructueux quand on va le comple´ter avec la proprie´te´ de
convexite´ de l’ensemble des matrices d’information continues correspondantes. En ef-
fet, un des re´sultats importants sera que le de´terminant ou son logarithme (fonction
concave) d’une matrice d’information continue atteint un maximum global sur l’ensemble
de de´finition, convexe, constitue´ des matrices d’information continues. La mesure corres-
pondante a` ce maximum est appele´e une mesure D-optimale, note´e ici piD, et M(piD) est
c© Revue MODULAD, 2005 -146- Nume´ro 33
la matrice d’information de cette mesure D-optimale. On comprend maintenant la per-
tinence et l’utilite´ de la D-efficacite´ de´finie en (9). Des algorithmes spe´cifiques (Fedorov,
1969, 1972, Wynn, 1970) ont e´te´ propose´s pour calculer ce type de mesure et en de´duire
des approximations discre`tes. En outre, il a e´te´ de´montre´ que si on souhaite discre´tiser la
mesure piD alors le maximum de points de support de la mesure discre`te, donc du support
d’un plan D-optimal discret, est NSmax = p(p+ 1)/2.
3.4.5 Re´pe´tition des expe´riences
Dans le cas de plans discrets, pour N fixe´, la valeur de det(MN) de´pend conjointement
des quatre e´le´ments suivants : la position et le nombre des NS points de support, le nombre
de re´pe´titions et leur re´partition sur ces NS points de support. Illustrons ce phe´nome`ne
avec l’exemple tre`s simple propose´ par Atkinson et Hunter (1968). Soit le mode`le line´aire :
y = β1x1 + β2x2 + ε (10)
avec x1 et x2 continus, de´finis sur [0, 1], variance des erreurs constante. On peut ve´rifier
facilement que tous les plans discrets de la forme :
ξ2,2,{r} =
{
(x1 = 0; x2 = 1) (x1 = 1; x2 = α)
1 1
}
(11)
avec 0 ≤ α ≤ 1 sont globalement D-optimaux avec un de´terminant det(M2) = 1. Pour
N = 6, la triple re´pe´tition de l’un quelconque de ces plans (11) conduit a` det
[
M6(ξ6,2,{3,3})
]
=
9 alors que le plan suivant constitue´ de la re´pe´tition d’un plan a` trois points de support
ξ6,3,{r} =
{
(x1 = 0; x2 = 1) (x1 = 1; x2 = 0) (x1 = 1; x2 = 1)
2 2 2
}
(12)
conduit a` det
[
M6(ξ6,3,{2,2,2})
]
= 12, ce qui est nettement D-meilleur.
Il apparaˆıt donc, comme annonce´ plus haut, que la valeur du de´terminant est lie´e de
fac¸on complexe a` la structure du plan. Ne´anmoins, dans certaines conditions l’optimalite´
consiste a` faire une re´partition e´quitable, c’est-a`-dire une re´partition telle que l’e´cart entre
les nombres de re´pe´tition ne diffe`rent pas plus d’une unite´, sur un support a` nombre de
points e´gal au nombre de parame`tres du mode`le (NS = p). Notamment, il faut tout
d’abord de´terminer le meilleur plan (globalement) D-optimal a` N = NS = p points (le
plan Dp-optimal), et ensuite ve´rifier si la condition ne´cessaire et suffisante de Vila (1991)
est satisfaite. Alors, si on note DN le plan N expe´riences, tout plan discret globalement
DN -optimal, avec N multiple de p (nombre de parame`tres du mode`le), consiste en N/p
re´pe´titions du plan globalement Dp-optimal.
3.4.6 Construction algorithmique des plans D-optimaux
Comme les plans D-optimaux sont largement re´pandus et sont des outils efficaces, on
montre maintenant quelques algorithmes tre`s utilise´s pour les construire. On a vu plus
haut la distinction de nature entre plans discrets et continus, les algorithmes respectifs
sont diffe´rents aussi.
c© Revue MODULAD, 2005 -147- Nume´ro 33
Construction des plans D-optimaux discrets Il existe maintenant de nombreux
algorithmes pour construire des plans optimaux, mais certains des algorithmes les plus
re´cents (par exemple base´s sur le recuit simule´, ou les algorithmes ge´ne´tiques) ne montrent
pas une vitesse significativement plus grande (et souvent meˆme infe´rieure en ce qui
concerne le recuit simule´) que les algorithmes pionniers, dits d’e´change, de Fedorov (1969,
1972) et de Mitchell (1974). Aussi, on exposera ici l’algorithme a` e´change double de Fedo-
rov qui a le me´rite d’eˆtre facile a` programmer et a` imple´menter dans tout langage usuel.
L’objectif est de se´lectionner N points parmi un ensemble de NC points candidats, en-
semble de´fini a priori (typiquement par les noeuds d’une grille a` maillage serre´ sur Ξ), tels
que le N -plan ait le de´terminant norme´ de sa matrice d’information MN le plus grand
possible parmi les de´terminants de tous les N -plans possibles. On atteint cet objectif avec
l’algorithme ite´ratif a` e´change double de Fedorov, qui ne pre´sente pas de preuve formelle
de convergence, mais qui dans la pratique converge tre`s souvent vers le maximum glo-
bal, maximum que l’on sait calculer par ailleurs (voir plus loin la construction des plans
continus).
Algorithme d’e´change double de Fedorov
– e´tape 1 : On choisit ale´atoirement (ou selon un choix circonstancie´) N expe´riences
parmi lesNC pour constituer unN -plan d’expe´riences initial non de´ge´ne´re´ (det
(
MNN
) 6=
0) et tel que N = NS ≥ p,
– e´tape 2 : On souhaite e´changer une expe´rience de ce plan initial par une expe´rience
de l’ensemble candidat dans le but d’une augmentation maximale du de´terminant de
MNN . La particularite´ de cet algorithme est que cet e´change se fait en un seul coup.
Si on de´signe par i une expe´rience du plan initial et j une expe´rience de l’ensemble
candidat on de´termine le couple (i, j) le plus performant dans cette augmentation
en calculant pour les N × (NC−1) couples possibles la valeur de ∆F = det(MNN). Si
plusieurs couples distincts provoquent des augmentations comparables de ∆F (a` un
e´cart ε pre`s, fixe´ au pre´alable) on choisit l’un de ces couples ale´atoirement. Notons
(imax, jmax) le meilleur couple.
– e´tape3 : On re´alise effectivement l’e´change de imax par jmax correspondant a` ∆F max
et on retourne a` l’e´tape 2 si le crite`re d’arreˆt n’est pas satisfait.
Pour calculer le de´terminant de l’ite´ration courante Fedorov s’appuie sur le the´ore`me
suivant.
The´ore`me :
Apre`s l’e´change de l’expe´rience i par l’expe´rience j la nouvelle matrice d’information
(a` l’ite´ration t + 1) s’exprime en fonction de la matrice d’information de l’ite´ration t
comme suit :
(XTNXN)[t+1] = (X
T
NXN)[t] − f(xi)f(xi)T + f(xj)f(xj)T
Et le nouveau de´terminant est lie´ au pre´ce´dent par :
det(XTNXN)[t+1] = det(X
T
NXN)[t] × [1 + ∆(i, j)]
avec :
∆(i, j) = d(xj)−
[
d(xi)d(xj)− d2(xi,xj)
]− d(xi)
c© Revue MODULAD, 2005 -148- Nume´ro 33
ou` :
d(xi) = f(xi)
T (XTNXN)
−1
[t]
f(xi)
d(xj) = f(xj)
T (XTNXN)
−1
[t]
f(xj)
d(xi,xj) = f(xi)
T (XTNXN)
−1
[t]
f(xj) fonction de covariance
= f(xj)
T (XTNXN)
−1
[t]
f(xi)
Plusieurs tests d’arreˆt sont possibles, Mathieu (1981) propose de stopper les ite´rations
quand ∆(i, j) devient infe´rieur a` ε, valeur faible choisie a priori, et surtout montre com-
ment les calculs peuvent eˆtre re´duits de 25% a` 75% suivant les cas. Enfin, on peut imposer
a` l’algorithme la possibilite´ de re´pe´ter ou non des expe´riences de´ja` se´lectionne´es. Comme
on ne connaˆıt pas a priori la meilleure valeur de N , on relance l’algorithme pour plusieurs
valeurs de N comprises entre N = p et Nmax < NC . En trac¸ant ensuite le graphique de
l’e´volution du de´terminant norme´ de MN , en fonction de ce nombre N variable, on peut
repe´rer sur ce graphique des pics correspondants a` des plans D-optimaux performants
(voir l’illustration plus loin).
Construction des plans D-optimaux continus On se contentera d’exposera le prin-
cipe de l’algorithme de Torsney (1988), algorithme rapide, encore acce´le´re´ graˆce a` la
proposition de Pronzato (2004). Comme l’algorithme pre´ce´dent on construit d’abord un
ensemble de points candidats (points de support des expe´riences candidates) forme´ par les
noeuds d’une grille a` maillage tre`s serre´ sur Ξ. Cet ensemble est le support discret d’une
mesure uniforme de masse pi[t] = 1/N [t] en chacun des points xi, avec pi
[t=0] = 1/NC . Soit :
d(xi, pi
[t]) = f (xi)
T 1
N [t]
M−1
N [t]
f (xi)
On calcule d(xi, pi
[t]) pour chaque xi, et on met a` jour la nouvelle mesure par :
pi
[t+1]
i = pi
[t]
i
d(xi, pi
[t]
i )
p
, i = 1, . . . , N [t]
Si une mesure devient ne´gligeable en un point xk a` l’ite´ration t = τ alors on pose pi
[t]
k = 0,
∀t > τ , et N [t+1] = N [t]−1. Si pour tout point xu de mesure non ne´gligeable on a d(xu, pi[t]u )
−p < ε, ε e´tant un petit nombre re´el positif satisfaisant a` la pre´cision souhaite´e, alors
on stoppe l’algorithme. La convergence (monotone) vers une mesure D-optimale piD est
de´montre´e. Si le maillage est tre`s serre´ le de´terminant correspondant de M(piD) est une
bonne approximation du maximum global qui peut eˆtre atteint par ce de´terminant.
Illustration Soit le mode`le (1) et le domaine Ξ sous contraintes, polygonal et convexe
dont les coordonne´es des sommets et des milieux d’areˆtes apparaissent au tableau II. On
suppose que ces NC = 17 points forment un ensemble candidat de bonne qualite´.
c© Revue MODULAD, 2005 -149- Nume´ro 33
n◦ point de support X1 X2
1 0.0 1.0
2 0.5 0.6
3 1.0 0.2
4 1.0 0.0
5 1.0 −0.2
6 0.9 −0.6
7 0.8 −1.0
8 0.2 −1.0
9 0.0 −1.0
10 −0.5 −0.9
11 −1.0 −0.8
12 −1.0 −0.2
13 −1.0 0.4
14 −0.9 0.7
15 −0.6 1.0
16 −0.3 1.0
17 0.0 0.0
Tableau II : Illustration : coordonne´es des sommets et des milieux d’areˆtes de Ξ. Il apparaˆıt
aussi le point (0, 0) ; ces NC = 17 points forment le re´seau candidat.
Un algorithme d’e´changes usuel (tel celui explicite´ plus haut) trouve en quelques secondes
deux plans D-optimaux discrets correspondant a` des pics du graphique obtenu comme
explique´ plus haut :
ξD6,6,{r} =
{
n◦ des points de support −→ 1 3 7 11 14 17
{r} −→ 1 1 1 1 1 1
}
det(M66) = 0.001502
et :
ξD14,8,{r} =
{
n◦ des points de support −→ 1 3 7 9 11 13 15 17
{r} −→ 2 2 2 1 2 2 1 2
}
det(M1414) = 0.001603
On remarque avec le second plan que l’on re´sout simultane´ment le proble`me d’optimisation
multiple e´voque´ au §3.4.1, a` savoir, pour N fixe´, de trouver N∗S et {r∗} en meˆme temps que
le maximum du de´terminant. Puis l’algorithme de Torsney nous donne une approximation
raisonnable (NC ne vaut que 17, mais on pourrait montrer que le re´seau candidat conside´re´
est de bonne qualite´ relativement au mode`le postule´) du maximum global correspondant
a` une mesure piD, soit 0.001637. On peut maintenant qualifier ces deux plans par la D-
efficacite´ calcule´e avec la formule (9). On trouve 98.6% pour le premier et 99.6% pour
le second. Les deux plans obtenus sont de qualite´ suffisante pour eˆtre re´ellement mis en
oeuvre au laboratoire. On pre´fe´rera le second si le budget disponible l’autorise car les
re´pe´titions le rendent plus robuste a` une erreur de manipulation. Toutefois, si le mode`le
postule´ (1) n’est pas un bon mode`le pour repre´senter le phe´nome`ne e´tudie´ ces plans D-
optimaux ne sont pas robustes vis-a`-vis de l’erreur de mode`le. Pour se prote´ger de ce
risque on conseille alors de s’orienter vers des plans construits a` partir du crite`re J qui
fait l’objet du paragraphe 3.10.
c© Revue MODULAD, 2005 -150- Nume´ro 33
3.4.7 Extensions de la D-optimalite´
Parmi les nombreuses extensions de la D-optimalite´ citons les deux extensions sui-
vantes assez largement utilise´es :
Crite`re de DA-optimalite´ : Sibson (1974) propose de ge´ne´raliser le crite`re de D-
optimalite´ si on s’inte´resse a` des combinaisons line´aires particulie`res des estimateurs. Si
A est la matrice (s× p), de rang s < p, des coefficients des combinaisons line´aires on a
ξDAN,N∗S ,{r∗} = Arg
{
min
ξN∈Ψ
det(A′M−1N A)
}
(13)
Crite`re de DS-optimalite´ : Il arrive parfois que l’on attache plus d’importance a`
certains parame`tres parmi l’ensemble des parame`tres du mode`le, s parame`tres d’inte´reˆt
majeur parmi les p. Les r = p − s sont par exemple des parame`tres de nuisance. Un
crite`re d’optimalite´ peut se construire par restriction au sous-ensemble des s parame`tres.
La DS-optimalite´ re´pond a` ce but (Karlin et Studden, 1966, Silvey, 1980). On trouvera
une application de ce crite`re dans Gauchi (2005).
3.5 Crite`re de A-optimalite´
Le crite`re A vise a` mimimiser la trace de M−1N :
ξAN,N∗S ,{r∗} = Arg
{
min
ξN∈Ψ
[Tr
(
M−1N
)
]
}
(14)
et donc minimiser ce crite`re e´quivaut a` minimiser la somme des variances des estimateurs.
Nume´riquement, le calcul d’un plan A-optimal est plus couˆteux que celui d’un plan D-
optimal puisqu’il faut inverserMN . Enfin, notons qu’un plan A-optimal de´pend des unite´s
des variables explicatives, ce qui est un handicap supple´mentaire a` l’utilisation de ce
crite`re.
3.6 Crite`re de E-optimalite´
Soit λ1, . . . , λp les valeurs propres de la matrice MN associe´e au plan ξN . Les valeurs
propres deM−1N sont 1/λ1, . . . , 1/λp. Un plan E-optimal a` N expe´riences vise a` minimiser
la valeur propre maximale de M−1N . C’est un crite`re de type min-max. On e´crit :
ξEN = Arg
{
min
ξN ²Ξ
[max
i
(1/λi)]
}
(15)
D’apre`s certains auteurs la E-optimalite´ semble impliquer un e´talement des expe´riences
sur l’espace expe´rimental, contrairement a` la D-optimalite´. Remarquons enfin que la
de´termination d’un plan E-optimal est une taˆche nume´riquement lourde, puisqu’elle ne´cessite
le calcul re´pe´te´ des valeurs propres de MN ; ce peut eˆtre une raison de sa faible utili-
sation. En outre, il n’existe pas de re´sultats the´oriques sur l’optimalite´ des re´pe´titions
d’expe´riences analogues a` celui e´voque´ pour le crite`re D-optimalite´.
c© Revue MODULAD, 2005 -151- Nume´ro 33
3.7 Interpre´tation ge´ome´trique des crite`res D, A et E
Si ε ∼ N (0,Σ) on sait que le domaine de confiance de niveau α pour β est un
hyperellipso¨ıde centre´ sur βˆ dont la frontie`re est de´finie par l’ine´galite´ :
(β − βˆ)TMN(β − βˆ) ≤ ρ2
ou` ρ2 est e´gal a` :
– ps2Fα(p,N−p) si la variance expe´rimentale σ2 est inconnue et que s2 en est une esti-
mation inde´pendante a` N−p ddl ; Fα(p,N−p) est le fractile a` α% de la distribution
F de Fisher-Snedecor a` p et N − p degre´s de liberte´,
– σ2χ2α(p) sinon, avec χ
2
α(p) le fractile a` α% de la distribution du Khi-2 a` p degre´s de
liberte´.
CommeMN intervient dans cette de´finition il apparaˆıt ainsi que les crite`res d’optimalite´
de´finis plus haut de´pendent de cet ellipso¨ıde relativement a` ses caracte´ristiques de volume,
de forme et d’orientation. Ces crite`res d’optimalite´ prennent alors une signification tre`s
naturelle.
3.8 Crite`re de G-optimalite´
3.8.1 De´finition
Plutoˆt que s’inte´resser directement a` la pre´cision de l’estimateur βˆ on peut souhaiter un
mode`le avec lequel les pre´dictions elles-meˆmes seront les plus pre´cises possible. Le crite`re
de G-optimalite´ vise a` minimiser le maximum de la variance de la re´ponse pre´dite. Si on
appelle d(xi) = f(xi)
TM−1N f(xi) la fonction de variance au point xi, on a var (yˆ (xi)) =
σ2d(xi). Un plan G-optimal discret a` N expe´riences est de´fini par :
ξGN = Arg
{
min
ξN∈Ψ
{
max
xi∈Ξ
d(xi)
}}
(16)
3.8.2 Exemple
Pour le plan ξ13,3 du §3.4.2 on a (a` σ2 pre`s) : var(yˆ (x)) = 13 + 12x2 et pour le plan ξ23,3
on a var(yˆ (x)) = 1
8
(3 + 2x+ 3x2). Aucun de ces deux plans n’est G-meilleur que l’autre
pour toutes les valeurs de x ∈ [−1,+1]. Mais, sur le meˆme intervalle, pour le premier plan
on a max{var(yˆ (x))} = 1
3
+ 1
2
= 5
6
tandis que pour le second on a max{var(yˆ (x))} =
1
8
(3+2+3) = 1. Donc le plan ξ13,3 est G-meilleur que le plan ξ
2
3,3. On voit ainsi qu’un plan
discret D-optimal n’est pas force´ment G-optimal sur le meˆme domaine. En revanche, on
verra ci-dessous qu’un plan continu D-optimal est aussi G-optimal.
3.9 Equivalence entre G- et D-optimalite´s continues
3.9.1 Le TEG, un the´ore`me fondamental
Cette e´quivalence, dans le cas continu uniquement, s’appuie sur le the´ore`me fonda-
mental d’e´quivalence ge´ne´rale (TEG) de Kiefer et Wolfowitz (1960), e´nonce´ pour un
mode`le line´aire. Il a e´te´ e´tendu par la suite (Whittle, 1973) a` tout crite`re diffe´rentiable
et convexe. Il est explicite´ dans Gauchi (1997). Ce the´ore`me montre, notamment, qu’un
c© Revue MODULAD, 2005 -152- Nume´ro 33
plan D-optimal continu est aussi un plan G-optimal continu et inversement, d’une part,
et d’autre part que [d(xi, piD)] = p, i = 1, . . . , NS, c’est-a`-dire que la fonction de variance
vaut toujours p en tous les points de support xi ou` la mesure continue D-optimale piD est
discre´tise´e. Nous donnons maintenant deux exemples tre`s simples d’utilisation du TEG.
Exemple 1 : un plan G-optimal pour la re´gression simple
Soit le mode`le :
E(yx) = β0 + β1x , x ∈ [−1 , + 1]
On souhaite re´aliser 6 expe´riences (c’est-a`-dire recueillir 6 observations). Dans ce but on
propose le plan discret ξ6,2,{3,3} pour lequel les points de support sont −1 et +1. Ce plan
est D- et G-optimal car par la formule (7) :
M66 =
3
6
(
+1 −1
−1 +1
)
+
3
6
(
+1 +1
+1 +1
)
=
(
+1 0
0 +1
)
La fonction de variance au point-support −1 s’e´crit :
d(−1) = ( +1 −1 )( +1 0
0 +1
)(
+1
−1
)
= 2
et au point-support +1 :
d(+1) =
(
+1 +1
)( +1 0
0 +1
)(
+1
+1
)
= 2
Donc :
max
−1≤x≤1
{
d(x , ξ6,2,{3,3})
}
= max
−1≤x≤1
(1 + x2) = 2 = p
En chaque point-support du plan la fonction de variance vaut p.
Exemple 2 : un plan G-optimal pour la re´gression quadratique
Soit le mode`le :
E(yx) = β0 + β1x+ β11x
2 , x ∈ [−1 , + 1]
Le plan discret :
ξ9,3,{r} =
{ −1 0 +1
3 3 3
}
est D- et G-optimal car :
M99 =
3
9
 +1 −1 +1−1 +1 −1
+1 −1 +1
+ 3
9
 +1 0 00 0 0
0 0 0
+ 3
9
 +1 +1 +1+1 +1 +1
+1 +1 +1

=
 +1 0 2/30 2/3 0
2/3 0 2/3

et : (
M99
)−1
=
3
4
 +4 0 −40 +2 0
−4 0 +6

c© Revue MODULAD, 2005 -153- Nume´ro 33
d’ou` :
max
−1≤x≤1
{
d(x , ξ9,3,{3,3,3})
}
= max
−1≤x≤1
{
3
4
(4− 6x2 + 6x4)
}
= 3 = p
et donc :
d (−1) = 3 ; d (0) = 3 ; d (+1) = 3
On peut donc ve´rifier facilement si un plan discret pre´sente simultane´ment les proprie´te´s
de D- et G-optimalite´ en calculant sa fonction de variance d (xi) en chaque point de son
support.
3.10 Crite`re de J -optimalite´
3.10.1 De´finition
Le crite`re J − crite`re appele´ IMSE en anglais pour ”Integrated Mean Square Error”
− est connu en franc¸ais sous le nom de crite`re de “variance+biais”. Il a e´te´ propose´ par
Box et Draper (1959). On le trouvera largement explicite´ dans Khuri et Cornell (1987).
L’originalite´ de ce crite`re par rapport aux crite`res de´ja` vus plus haut est qu’il prend
en compte a` la fois la variance de la pre´diction de la re´ponse et le biais induit sur les
estimateurs du mode`le postule´ quand celui-ci n’est pas force´ment le ”bon” mode`le. En
ge´ne´ral, le ”bon” mode`le est un sur-mode`le, c’est-a`-dire un mode`le englobant le mode`le
postule´ et qui contient des termes supple´mentaires. On cherche a` se ”prote´ger” contre
ce sur-mode`le, c’est-a`-dire a` minimiser l’impact de l’omission des termes supple´mentaires
sur la qualite´ des estimateurs obtenus avec le ”faux” mode`le postule´. Pour pre´senter plus
facilement ce crite`re, mais sans sacrifier a` l’esprit pe´dagogique de l’article, on se placera
dans un cadre continu et on utilisera donc une mesure de re´fe´rence pi ∈ D ensemble des
mesures possibles sur Ξ (de dimension h).
Soit x le vecteur dont les composantes sont les h facteurs explicatifs continus. Posons :
– y (x) = f1 (x)
T β1+ ε , le mode`le postule´ pour la re´ponse y, ou` f1 (x)
T est le vecteur
ligne (1× p1) des p1 fonctions de re´gression de x, et β1le vecteur des p1 parame`tres
a` estimer,
– y (x) = η(x)+ε, le ”vrai” mode`le (ou sur-mode`le) avec η(x) = f1 (x)
T β1+f2 (x)
T β2,
ou` f2 (x)
T est le vecteur ligne (1× p2) des p2 fonctions de re´gression de x ”oublie´es”,
et β2 le vecteur des p2 parame`tres correspondants a` estimer.
Le crite`re ”variance +biais” consiste a` minimiser par rapport a` la mesure pi la quantite´
suivante :
J =
ND
σ2
∫
Ξ
E [yˆ(x)− η(x)]2 dpi(x)
avec :
– N = nombre total d’expe´riences du plan
– D−1 = ∫
Ξ
dpi(x),
– yˆ(x) : la pre´diction du mode`le postule´,
Matriciellement posons :
– y = XN,1β1 + ε
– yˆ = XN,1βˆ1 la pre´diction du mode`le postule´ ou` βˆ1 = (X
T
N,1XN,1)
−1XTN,1y,
– η(x) = XN,1β1 +XN,2β2
c© Revue MODULAD, 2005 -154- Nume´ro 33
L’estimation β˘1 de β1 par le “vrai” mode`le est telle que :
E
(
β˘1
)
= β1 + (X
T
N,1XN,1)
−1XTN,1XN,2β2 = β1 +Aβ2
avec A =(XTN,1XN,1)
−1XTN,1XN,2 appele´e matrice d’alias.
3.10.2 De´composition du crite`re J
On peut le de´composer en deux termes : J = V +B ou` :
V =
ND
σ2
∫
Ξ
var(yˆ(x))dpi(x)
est la variance de la pre´diction inte´gre´e et normalise´e et :
B =
ND
σ2
∫
Ξ
[E(yˆ(x))− η(x)]2dpi(x)
est le carre´ du biais inte´gre´ sur Ξ
′
et normalise´.
Si on de´signe la matrice des moments du plan par MN,X1 = N
−1XTN,1XN,1 on peut
exprimer V sous une forme plus concise en utilisant var(yˆ(x)) = σ2N−1f1(x)TM−1N,X1f1(x).
On aboutit ainsi a` :
V = Tr[µ11M
−1
N,X1
] = NTr[µ11
(
XTN,1XN,1
)−1
] (17)
ou` la matrice (p1 × p1) des moments µ11est de´finie par :
µ11 = D
∫
Ξ
f1 (x) f1 (x)
T dpi(x)
De meˆme on peut exprimer B sous la forme :
B =
N
σ2
βT2 ∆β2 (18)
ou` :
∆ = ATµ11A− ATµ12 − µ21A+ µ22
µ12 = D
∫
Ξ
f1 (x) f2 (x)
T dpi(x)
µ21 = µ
T
12
µ22 = D
∫
Ξ
f2 (x) f2 (x)
T dpi(x)
On remarque que (18) de´pend du vecteur de parame`tres inconnus, et donc il faudra
apporter des informations (par une approche baye´sienne par exemple) sur ce vecteur pour
calculer (18).
Si on construit un plan en ignorant B cela revient a` minimiser V ce qui revient a` de´finir
un crite`re de V -optimalite´ (Fedorov, 1969, 1972, le de´nomme crite`re de Q-optimalite´).
Minimiser B en ignorant V est plus rare. En re´sume´ le crite`re J revient a` minimiser :
γ1V + γ2B ; 0 < γ1, γ2 ≤ 1
meˆme si le plus souvent on choisit γ1 = γ2 = 1. Illustrons ce crite`re avec les mode`les
e´le´mentaires suivants.
c© Revue MODULAD, 2005 -155- Nume´ro 33
3.10.3 Application aux mode`les de droite et de parabole
Pour une illustration claire de ce crite`re on reprend ici l’exemple de Khuri et Cornell
(1987). Soit a` ajuster une droite a` partir de N donne´es (xi, yi) sur Ξ = [−1,+1]. Sans
perte de ge´ne´ralite´ on supposera les xi centre´s de sorte que
∑N
i=1 xi = 0.
On a :
yˆ = βˆ0 + βˆ1x et yvrai = β0 + β1x+ β2x
2 + ε
a/ Calculons le terme V
On a :
var(yˆ) = var(βˆ0) + x
2var(βˆ1) =
σ2
N
+ x2
σ2∑N
i=1 x
2
i
et ainsi il vient en posant le moment [11] =
∑N
i=1 x
2
i
N
:
V =
1
2
∫ +1
−1
(
1 +
x2∑N
i=1 x
2
i
)
dx = 1 +
1
3
∑N
i=1 x
2
i
= 1 +
1
3 [11]
b/ Calculons le terme B
On part de :
B =
N
2σ2
∫ +1
−1
[
E(yˆ)− β0 − β1x− β2x2
]2
dx
En posant [111] = N−1
∑N
i=1 x
3
i on obtient :
B =
Nβ22
σ2
∫ +1
−1
{(
[11]2 + [111] x
)
[11]
− x2
}2
dx
=
Nβ22
σ2
{
[11]2 +
[111]2
3 [11]2
− 2
3
[11] +
1
5
}
=
Nβ22
σ2
{
[11]2 − 2
3
[11] +
1
5
}
Si on pose [111] = 0 correspondant au cas ou` les xi sont syme´triquement re´partis autour
de 0 on a :
J = 1 +
1
3 [11]
+
(
β2
σ/
√
N
)2{(
[11]− 1
3
)2
+
4
45
}
Il nous faut maintenant e´mettre des hypothe`ses sur β2 :
– Un premier cas extreˆme : quand la courbure est tre`s petite par rapport a` l’e´cart-type
de l’e´chantillon de taille N c’est-a`-dire quand β2 << σ/
√
N , alors J est minimise´
en rendant [11] grand, c’est-a`-dire en minimisant V .
– Un second cas extreˆme : quand β2 >> σ/
√
N est grand, alors J est minimise´ en
posant [11] = 1
3
, c’est-a`-dire en minimisant B.
– Pour les cas interme´diaires ou` V et B sont simultane´ment diffe´rents de ze´ro, Box
et Draper (1987) proposent de conside´rer le rapport g = V
B
et e´tablissent le tableau
suivant :
c© Revue MODULAD, 2005 -156- Nume´ro 33
g = V
B
√
Nβ2
σ
[11]optimal Joptimal Jtout biais (V = 0, B > 0)
1/2 6.540 0.363 5.755 5.799
1 4.499 0.388 3.718 3.798
2 2.994 0.433 2.656 2.797
4 1.822 0.519 2.052 2.296
10 0.501 1.000 1.467 2.022
Tableau III : Influence du rapport g sur le crite`re J .
On note dans ce tableau l’importante contribution du biais dans le crite`re. Appliquons les
re´sultats du tableau III dans le cas de nos trois hypothe`ses. On sait de´ja` que les expe´riences
doivent se re´partir syme´triquement autour de ze´ro, centre de l’intervalle [−1,+1].
– a/ Un plan discret “tout-variance” (V > 0, B = 0) conduit a` :
– placer N/2 expe´riences respectivement en −1 et +1 si N est pair,
– si N est impair une expe´rience sera place´e au centre.
– b/ Un plan “tout biais” (V = 0, B > 0) conduit a` :
– placer N/2 expe´riences en −0.58 et +0.58, si deux niveaux seulement sont auto-
rise´s,
– si N est impair une expe´rience est place´e au centre et (N − 1)/2 expe´riences
respectivement en xi = ±
√
N/3(N − 1),
– si trois niveaux de x sont autorise´s et si N est multiple de 3, alors N/3 expe´riences
se placent respectivement en −√0.5, 0,+√0.5.
– c/ Un plan e´quilibre´ V = B conduit a` :
– placer N/2 expe´riences respectivement en ±0.623 si N est pair,
– sinon, placer une expe´rience au centre et (N − 1)/2 respectivement en des sites
de´pendants de N (a` chaque fois [11] vaut 0.388), soit :
– si N = 3 alors xi = ±0.763
– si N = 5 alors xi = ±0.696
– si N = 7 alors xi = ±0.672
– si N est multiple de 3 et si on place N/3 en chaque site, les sites sont toujours
xi = ±0.763.
Ce crite`re J est un crite`re tre`s performant quand le mode`le postule´ est suspecte´ d’oublier
des termes que l’on peut spe´cifier, car de nombreuses simulations ont montre´ que les plans
obtenus sont plus robustes que ceux obtenus avec le crite`re D. La proce´dure GOSSET de
Hardin et Sloane (1993) permet de les construire.
4 Plans optimaux pour mode`les de re´gression non
line´aire
4.1 Le cadre non line´aire
Ce paragraphe me´riterait a` lui seul tout un article didactique. Pour cette raison nous
serons volontairement tre`s succincts. Ne´anmoins, nous indiquerons pre´cise´ment le cadre
du mode`le non line´aire concerne´, et citerons les principales approches rencontre´es dans la
litte´rature. Ces approches sont de´taille´es dans Gauchi (1997).
La non line´arite´ des mode`les de re´gression parame´trique conside´re´s peut s’exercer de
diffe´rentes manie`res, e´ventuellement simultane´ment :
c© Revue MODULAD, 2005 -157- Nume´ro 33
– Dans la forme analytique du mode`le elle-meˆme, relativement a` ses parame`tres, on
utilise alors la de´nomination de mode`le intrinse`quement non line´aire. On peut en
donner une de´finition plus formelle :
De´finition
Un mode`le de re´gression est dit intrinse`quement non line´aire si au moins une de´rive´e
premie`re de la fonction du mode`le par rapport aux parame`tres de´pend d’au moins un
de ces parame`tres. Si certaines de´rive´es ne de´pendent pas des parame`tres, le mode`le
sera qualifie´ de partiellement non line´aire.
Par exemple, le mode`le de Michaelis-Menten : η(θ, x) = θ1x/(θ2+x) est partiellement
intrinse`quement non line´aire en les parame`tres.
– Dans l’expression de la variance des erreurs, repre´sente´e alors par un mode`le pa-
rame´trique : var(εi) = σ
2(x, θ, β) ou` β est le parame`tre spe´cifique de la variance.
Par exemple, on peut imaginer σ2(x, β) = exp(βx) avec β > 0.
Dans ce cadre non line´aire le point crucial par rapport a` la construction d’un plan
d’expe´riences est que la matrice d’information de´pend maintenant des parame`tres incon-
nus θ∗ (et e´ventuellement β aussi), contrairement au cas line´aire du paragraphe pre´ce´dent.
Quels que soient les crite`res d’optimalite´ imagine´s θ∗ sera pre´sent (sous une forme ponc-
tuelle ou de distribution) dans la de´finition de ces crite`res. Indiquons maintenant les
notations relatives au mode`le de re´gression non line´aire.
4.2 Le mode`le de re´gression non line´aire
On conside`re simultane´ment le mode`le de re´gression non line´aire parame´trique sui-
vant
yik = ηξ (xi, θ
∗) + εik ; i = 1, . . . , NS ; k = 1, . . . , ri (19)
et le plan d’expe´riences associe´ (4), ou`, en plus des notations du §3.1, ηξ (.) est une fonction
scalaire continue de´finie sur Ξ×Θ, value´e dans R pour le plan (4), doublement de´rivable
relativement aux parame`tres et aux facteurs explicatifs ; on notera ηξ (.) le vecteur N × 1
correspondant. On supposera les erreurs gaussiennes. Pour l’estimation de θ∗ on utilise
l’estimateur des moindres carre´s nonline´aire de´fini par :
θˆ =Argmin
θ²Θ
‖y − ηξ (x, θ∗)‖2 (20)
qui jouit de bonnes proprie´te´s asymptotiques dans le cas nonline´aire (Jenrich 1969) : il
est asymptotiquement distribue´ comme N (θ∗,MF (ξ, θ∗)−1), ou`MF (ξ, θ∗) est la matrice
d’information de Fisher (voir, par exemple, Atkinson and Doneev (1992) pour des de´tails
sur celle-ci) de´finie ici pour un plan a` N expe´riences par :
MFN (ξ, θ
∗) =
1
σ2
JN (x, θ
∗)T JN (x, θ∗) (21)
ou` JN (x, θ
∗) est la matrice jacobienne N×p, de terme ge´ne´ral ∂η(x, θ∗)/∂θ∗j , j = 1, . . . , p,
et ou` le symbole ξ remplace ξN,NS ,{r} pour simplifier l’e´criture.
4.3 Le plan D-optimal local
Le principe de l’optimalite´ locale est duˆ a` Chernoff (1953) : la matrice jacobienne
JN (x, θ
∗) joue le roˆle de matrice du mode`le, et θ∗ inconnu sera remplace´ par une valeur
c© Revue MODULAD, 2005 -158- Nume´ro 33
a priori θ0. L’emploi de la matrice jacobienne provient de la line´arisation du mode`le η (.)
autour de la solution aux moindres carre´s θˆ. Les de´finitions et the´ore`mes du paragraphe
pre´ce´dent restent alors valables ”localement”. Le crite`re de D-optimalite´ devient le crite`re
de D-optimalite´ locale, qu’on note parfois D(θ0)-optimalite´, et se de´finit comme en (8),
avec MN (ξ) remplace´e par M
F
N (ξ, θ0). Ainsi, meˆme le TEG se ge´ne´ralise au cas non
line´aire (White, 1973). On utilise les meˆmes algorithmes que pour le cas line´aire pour
construire le plan D(θ0)-optimal.
On peut faire le commentaire suivant quant au choix θ0. Celui-ci est base´ sur une connais-
sance pre´alable (the´orique ou expe´rimentale) qui implique par exemple que l’on sache
que les de´rive´es du mode`le varient lentement dans le domaine e´tudie´ et que le domaine
parame´trique Θ soit bien choisi (qu’il contienne effectivement θ∗).
Exemple
Conside´rons le mode`le (2) de la situation expe´rimentale du §2.5. Apre`s avoir calcule´ les
de´rive´es ∂η(t, θ∗)/∂θ∗1 et ∂η(t, θ
∗)/∂θ∗2, on peut exprimer la matrice J2(θ0, ξ(t1, t2)) en
fonction de deux variables t1 et t2, avec θ0 = (0.7 0.2)
T fourni par Box et Lucas (1959).
Le de´terminant de la matrice d’information locale s’e´crit alors :
∆2 = det
(
JT2 (θ0, ξ(t1, t2))J2(θ0, ξ(t1, t2))
)
= [detJ2(θ0, ξ(t1, t2))]
2
On cherche le couple (t∗1, t
∗
2) qui le maximise. Comme il n’y a pas de solution analytique
ici, Box et Lucas (1959) utilisent une me´thode nume´rique classique qui conduit au plan
optimal cherche´ (a` deux points de support) :
t∗1 = 1.229 t
∗
2 = 6.858 avec ∆
max
2 = 0.657
Par ailleurs, on rappelle (Vila, 1985) que pour N > 2 les plans D(θ0)-optimaux se
concentrent tous sur le meˆme support (1.229 ; 6.858). Si l’information sur θ∗, au tra-
vers de θ0, est tre`s douteuse, on aura un plan localement D(θ0)-optimal de mauvaise
qualite´. Une alternative est alors le plan suivant.
4.4 Le plan D-optimal baye´sien
Intuitivement, il est raisonnable de penser que, si on dispose d’une connaissance a
priori assez fiable concernant la loi de probabilite´ du parame`tre θ∗, le plan sera plus efficace
(robuste) en injectant cette information lors de sa construction, plutoˆt que de se contenter
d’une valeur isole´e θ0. Le principe de cette approche est donc base´ sur le the´ore`me de Bayes
qui permet d’exprimer la densite´ de probabilite´ a posteriori du parame`tre inconnu θ∗ ayant
observe´ le vecteur y et en disposant d’une loi a priori pour θ∗ :
p (θ∗ | y(ξ)) = L (y(ξ) | θ
∗)P0(θ0,H)∫
Θ
L (y(ξ) | θ∗)P0 (θ0,H) dθ∗
avec :
– y(ξ) vecteur des observations recueillies au plan ξ,
– L (y(ξ) | θ∗) la vraisemblance de ce vecteur,
– P0(θ0,H) la densite´ de probabilite´ a priori de θ∗, d’espe´rance θ0 et de matrice de
variance H.
– p (θ∗ | y(ξ)) la densite´ de probabilite´ a posteriori de θ∗ ayant observe´ le vecteur y(ξ).
c© Revue MODULAD, 2005 -159- Nume´ro 33
Le parame`tre vectoriel θ∗ est alors conside´re´ comme un parame`tre ale´atoire. Sur ce prin-
cipe Draper et Hunter (1967) proposent une approche qui revient a` de´finir un plan D-
optimal baye´sien. Ensuite, Pronzato (1986) propose plusieurs crite`res baye´siens, plus ro-
bustes. Puis Chaloner et Verdinelli (1995) proposent dans ce contexte baye´sien un crite`re
base´ sur la maximisation du logarithme du de´terminant de la matrice d’information de
Fisher. On trouvera une application de ce crite`re dans Gauchi (2005).
4.5 Le plan X-optimal
Dans cette approche on ne fonde plus le crite`re d’optimalite´ sur la line´arisation implicite
du mode`le non line´aire autour d’une valeur a priori θ0 du parame`tre vectoriel inconnu θ
∗,
mais plutoˆt sur la minimisation de l’espe´rance du volume d’une re´gion de confiance exacte
pour le parame`tre θ∗. On rappelle qu’une telle re´gion est dite exacte si sa probabilite´ C(θ∗)
de recouvrir la vraie valeur θ∗ est 1 − α, pour un niveau α choisi et ∀N . Le crite`re se
nomme le crite`re de X-optimalite´, et il en existe aussi une version baye´sienne appele´e le
crite`re de X ′-optimalite´ (Vila et Gauchi, 2005).
4.6 Les plans TXP -optimal et DXP -optimal
Cette approche, tre`s rigoureuse mais qui conduit a` des crite`res tre`s techniques et tre`s
lourds nume´riquement, est essentiellement due a` Pazman et Pronzato (1992) et Gauchi
et Pazman (2003, 2005). On ne peut pas de´tailler ici cette approche, mais on indique
seulement qu’elle est base´e sur la fonction de densite´ exacte de probabilite´ (a` distance
finie) de l’estimateur des moindres carre´s non line´aire, densite´ e´tablie par Pazman (1984).
5 Conclusion
Bien suˆr, d’autres crite`res d’optimalite´ existent, re´pondant a` des objectifs pre´cis. On
renvoie le lecteur a` Gauchi (1997) pour un panorama assez large sur le sujet. Par ailleurs, il
faut souligner que, dans le cas non line´aire, il est ne´cessaire de faire appel a` des me´thodes
puissantes d’optimisation, notamment stochastiques, pour calculer les plans optimaux,
elles sont e´voque´es par exemple dans Vila et Gauchi (2005), et Gauchi et Pazman (2003).
Enfin, il faut indiquer qu’il existe plusieurs logiciels commerciaux pour calculer des
plans optimaux, essentiellement dans le cas line´aire. Par exemple, sans eˆtre exhaustif et
sans pre´juger de la qualite´ des logiciels non nomme´s, on peut citer la proce´dure OPTEX
du logiciel SAS/QC (SAS Institute), les logiciels STATGRAPHICS (Socie´te´ Sigma Plus),
NEMROD (Socie´te´ LPRAI), NEUROPEX (Socie´te´ NETRAL). Ce dernier calcule aussi
des plans X-optimaux. Toutes les informations sur ces logiciels sont facilement accessibles
par le moteur de recherche Google sur Internet.
Re´fe´rences
Atkinson, A.C., Donev, A.N. (1992). Optimum experimental designs. Oxford : Cla-
rendon Press.
Atkinson, A.C., Hunter, W.G. (1968). The design of experiments for parameter esti-
mation. Technometrics, 10, 2, 271-289.
c© Revue MODULAD, 2005 -160- Nume´ro 33
Box, G.E.P., Draper, N.R. (1959). A basis for selection of a response surface design.
J. of the Amer. Stat. Association, 54, 622-653.
Box, G.E.P., Draper, N.R. (1987). Empirical Model Building and Response Surfaces.
Wiley. New-York.
Box, G.E.P., Lucas, H.L. (1959). Design of experiments in nonlinear situations. Bio-
metrika, 46, 77-90.
Chaloner, K., Verdinelli, I. (1995). Bayesian experimental design : a review. Statistical
Science, 10, 3, 273-304.
Chernoff, H., (1953). Locally optimum designs for estimating parameters. Ann. of
Math. Statist. 24, 586-602.
Draper, N.R., Hunter, W.G. (1967). The use of prior distributions in the design of
experiments for parameter estimation in nonlinear situations. Biometrika, 54, 147-
153.
Fedorov, V.V. (1969, en russe, 1972, en anglais). Theory of Optimal Experiments.
Academic Press, New York.
Fedorov, V.V. (1980). Convex design theory. Math. Operationsforsch. Statist., Ser.
Statistics, 11, 3, 403-411.
Gauchi, J.-P. (1997). Plans d’expe´riences optimaux pour mode`les line´aires, Chap. 7 et
8, In Plans d’Expe´riences-Applications a` l’Entreprise, Editeur TECHNIP, Paris 1997,
(Editeurs scientifiques : JJ. Droesbeke, J. Fine et G. Saporta).
Gauchi, J.-P. (2005). Optimal statistical designs for the accurate estimation of the pa-
rameters of a growth rate model for Listeria monocytogenes. Technical Report n◦2005-
2 of the MIA Unit, INRA, Research Center of Jouy-en-Josas, France. On line at :
http ://www.inra.fr/miaj/public/nosdoc/inrajouymia2005T2.pdf.
Gauchi, J.-P., Pa´zman, A. (2003). Distribution of the least squares estimator, stochas-
tic optimization and optimum designs in nonlinear models. Technical Report of the
Biometrics Unit, n◦2003-7, INRA, Jouy-en-Josas, France.
Gauchi, J.-P., Pa´zman, A. (2005). Designs in nonlinear regression by stochastic mi-
nimization of functionals of the mean square error matrix. In Press in J. of Stat.
Planning and Inference.
Hardin, R.H., Sloane, N.J.A. (1993). A new approach to the construction of optimal
design. J. of Stat. Planning and Inference, 37, 339-369.
Jenrich, R.I. (1969). Asymptotic properties of nonlinear least squares estimators. Ann.
of Stat., 40, 2, 633-643.
Karlin, S., Studden, W.J. (1966). Optimal experimental designs. Ann. of Math. Stat.,
37, 783-815.
Khuri, A.I., Cornell, J.A. (1987). Response surfaces : designs and analyses. Marcel
Dekker.
Kiefer, J. (1959). Optimum experimental designs. J.R. Statist. Soc. B., 21,272-319.
Kiefer, J. (1961). Optimum designs in regression problems II. Ann. of Math. Stat., 32,
298-325.
Kiefer, J. (1974). General equivalence theory for optimum designs (approximate
theory). Ann. of Statist., 2, 849-879.
c© Revue MODULAD, 2005 -161- Nume´ro 33
Kiefer, J., Wolfowitz, J. (1959). Optimum designs in regression problems. Ann. of
Math. Statist., 30, 271-294.
Kiefer, J., Wolfowitz, J. (1960). The equivalence of two extremum problems. Canad.
J. Math., 12, 363-366.
Kobilinsky, A. (1988).Tactiques en analyse de variance et en re´gression. Revue Modu-
lad, 1, 25-58.
Mathieu, D. (1981). Contribution de la me´thodologie de la recherche expe´rimentale a`
l’e´tude des relations structure-activite´. The`se. Universite´ d’Aix-Marseille. France.
Mitchell, T.J. (1974). An algorithm for the construction of D-optimal experimental
designs. Technometrics, 16, 203-210.
Pa´zman, A. (1984). Probability distribution of the multivariate nonlinear least squares
estimator. Kybernetika, 20, 209-230.
Pa´zman, A., Pronzato, L. (1992). Nonlinear experimental design based on the distri-
bution of estimators. J. of Stat. Planning and Inference, 33, 385-402.
Pronzato, L. (1986). Synthe`se d’expe´riences robustes pour mode`les a` parame`tres in-
certains. The`se. Universite´ d’Orsay.
Pronzato, L. (2004). A minimax equivalence theorem for optimum bounded design
measure. Statistics & Probability Letters, 68, 325-331.
Sibson, R. (1974). DA-optimality and duality, in J. Gani, K.Sarkadi, I. Vincze (Eds),
Progress in Statistics. Proceedings 9th European Meeting of Statisticians, Budapest
1972, 2, 667-692. Amsterdam, North Holland.
Silvey, S.D. (1980). Optimal designs : an introduction to the theory for parameter
estimation. London : Chapman and Hall.
Torsney, B. (1988). Computing optimizing distributions with applications in design, es-
timation and image processing, in Optimal design and Analysis of experiments (Dodge,
Fedorov, Wynn Eds), 361-370.
Vila, J.P. (1985). Etude et comparaison de crite`res de plans d’expe´riences optimaux
pour l’estimation des parame`tres d’un mode`le de re´gression non line´aire. The`se Uni-
versite´ d’Orsay.
Vila, J.P. (1991). Local optimality of replications from a minimal D-optimal design
in regression : a sufficient and a quasi-necessary condition. J. of Stat. Planning and
Inference, 29, 261-277.
Vila, J.P., Gauchi, J.-P. (2005). Optimal designs based on exact confidence regions for
parameter estimation of a nonlinear regression model. En cours de soumission a` J. of
Stat. Planning and Inference.
White, L.V. (1973). An extension of the general equivalence theorem to nonlinear
models. Biometrika, 60, 2, 345-348.
Whittle, P. (1973). Some general points in the theory of optimum experimental design.
J. Roy. Statist. Soc, B, 35, 123-130.
Wynn, H.P. (1970). The sequential generation of D-optimum experimental designs.
Ann. of Math. Stat., 41, 1655-1664.
c© Revue MODULAD, 2005 -162- Nume´ro 33
