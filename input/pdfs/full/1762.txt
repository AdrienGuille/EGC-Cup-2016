Extration de (( pepites )) de onnaissane dans les
donnees : une nouvelle appro he et une etude de la
sensibilite au bruit
Jero^me Aze, Yves Ko drato
CNRS, Lab oratoire de Reherhe en Informatique
Ba^t. 490, Universite. Paris-Sud
91405 Orsay Cedex - Frane
faze,ykglri.fr
Resume. La plupart des metho des p ermettant d'extraire des regles d'as-
so iation dans les donnees sont basees sur l'utilisation de mesures et de
seuils predenis par l'exp ert p our optimiser la reherhe des regles. Le
hoix des seuils p ermettant de separer les regles interessantes des regles
triviales est di√Üile, m e^me p our un exp ert du domaine etudie.
Si l'on onsidere que les donnees sont bruitees et que l'extration d'infor-
mations du typ e (( pepites )) de onnaissane ('est-a-dire, regles ayant un
faible supp ort) p eut interesser l'exp ert, alors les metho des lassiques sont
souvent mises en defaut dans de telles situations.
Nous prop osons don une nouvelle mesure d'extration des regles d'as-
so iation, app elee (( moindre-ontradition )). Nous montrons que ette
mesure (i) p ermet d'extraire des (( pepites )) de onnaissane dans les
donnees, sans p our autant e^tre submerges par le nombre de regles ayant
un faible supp ort, (ii) se omp orte legerement mieux que les mesures las-
siques etudiees lorsque les donnees sont bruitees.
Mots Cles : regles d'asso iation, mesures de qualite, bruit.
1 Intro dution
La deouverte non sup ervisee de regles d'asso iation dans les bases de donnees est
partiulierement interessante et de nombreux travaux ont ete eetues p our arateriser
les motifs ahes dans les bases de donnees (Agrawal et al. 1993, Agrawal et Srikant
1995, Brin et al. 1997, Lavra et al. 1999, Sahar 1999, Gras et al. 2001). Les eets du
bruit et le omp ortement de es algorithmes ont ete p eu etudies, bien que les bases de
donnees reelles soient rarement parfaites. Nous p ensons qu'il est interessant d'etudier le
omp ortement des algorithmes d'extrations de regles d'asso iation lorsque les donnees
ontiennent entre 1% et 10% de bruit. Il nous semble qu'au dela de 10% de bruit, les
donnees sont trop imparfaites p our p ouvoir e^tre orretement analysees.
Ayant fait ette onstatation, nous prop osons, dans et artile, une mesure d'inter e^t
qui est moins sensible au bruit que la plupart des mesures lassiques (que nous avons
etudiees).
Comme nous l'avons deja montre dans (Aze et Ko drato 2002a, Aze et Ko drato
2002b), de nombreuses mesures d'inter e^t lassiques, omme le supp ort, la onane
Extration de (( pepites )) de onnaissane
et la dep endane, sont tres sensibles a dierentes formes de bruits, et ette sensibilite
est prinipalement liee au fait que es mesures sont basees sur l'utilisation de seuils
d'elagages p ermettant de determiner un ensemble de regles interessantes. L'ensemble
des regles qui sont pro hes du seuil d'elagage p euvent basuler d'un o^te ou de l'autre
de e seuil, lorsque les donnees sont bruitees, et provo quer ainsi l'apparition ou la
disparition de nouvelles regles.
De plus, omme l'a deja montre A. Freitas (Freitas 1998), les regles ayant un faible
supp ort p euvent e^tre tres interessantes et il est di√Üile, m e^me p our un exp ert du
domaine, de denir un seuil d'elagage p ermettant de separer e√Üaement, en deux
lasses, les regles interessantes de elles qui ne le sont pas.
Une des appro hes p ossibles, est elle prop osee par S. Sahar (Sahar 1999) qui
presente les regles les plus generales a un exp ert. Si l'exp ert deide de lasser une
(ou plusieurs) de es regles omme etant non interessantes, alors ette regle et toutes
es speialisations sont elaguees. Cette appro he p ermet, a partir d'un ensemble de
regles obtenues de maniere (( lassique )) (Agrawal et Srikant 1995), d'elaguer tres ra-
pidement l'ensemble des regles. Notre appro he ne s'opp ose pas a elle-i, en revanhe,
nous essayons de reduire le plus p ossible l'ensemble initial de regles (qui p eut e^tre tres
volumineux).
Nous prop osons don un algorithme p ermettant d'extraire un ensemble de (( pepites
de onnaissane )) a partir d'une base de donnees, sans avoir b esoin de demander a
l'exp ert de denir un ensemble de seuils d'elagage. Ces pepites de onnaissane sont
representees sous la forme de regles d'asso iation, araterisees par un faible supp ort
et une onane stritement superieure a 50%. La plupart des metho des lassiques
pro duisent un volume de regles inexploitables par un exp ert des que le supp ort minimal
devient trop faible. Ces metho des ne p ermettent don pas d'extraire e√Üaement les
pepites de onnaissane que nous reherhons.
Nous prop osons don une appro he, basee sur la reherhe non exhaustive des regles
d'asso iation les moins-ontredites. Nous intro duisons une mesure, app elee (( moindre-
ontradition )) (Aze 2003), que nous noterons ontramin. L'algorithme p ermettant
d'extraire les pepites de onnaissane reherhees est base sur l'utilisation du ontexte
des regles en ours d'extration. Parmi es regles, seules les moins ontredites sont
retenues et prop osees a l'exp ert. L'elagage ainsi eetue est don dep endant des donnees
et n'est pas one diretement a l'exp ert.
Pour illustrer l'inter e^t des pepites de onnaissane, onsiderons le probleme lassique
de la reherhe de onnaissanes dans des bases de donnees transationnelles (ensemble
de tikets de aisse d'un sup ermarhe par exemple). Pour de telles donnees, l'exp ert (le
direteur du sup ermarhe) est interesse par des regles nouvelles qui p euvent l'amener a
faire des prots. Il semble raisonnable de onsiderer que les regles lassiques et veriees
par de nombreux onsommateurs sont deja onnues et exploitees par les exp erts. Par
exemple, les regles d'asso iation ahat de beurre ! ahat de pain ou ahat de lait !
ahat d'eau semblent oherentes et representent des onnaissanes deja onnues du
domaine. D'autres regles p euvent e^tre tres p ertinentes mais n' e^tre veriees que par une
sous-p opulation reduite des onsommateurs, par exemple ahat de lait infantile et de
ouhes pour bebe ! ahat de lingettes.
Toutes es onnaissanes sont deja onnues des exp erts et eux-i sont interesses
RNTI - 1
Aze et Ko drato.
√©l√©ments tels queEnsemble des donn√©es valides
√©l√©ments tels que√©l√©ments tels que
H
B
B = Vrai et H = Faux
supp ort de B ! H
H = Vrai
B = Vrai
Fig. 1 { Presentation de P (B ^ H ) et de P (B ^ :H ).
par des onnaissanes nouvelles m e^me si elles ne sont pas toujours veriees par une
ma jorite de onsommateurs. Nous p ensons don que les pepites de onnaissane p euvent
arateriser es nouvelles onnaissanes tant reherhees. Ces pepites p euvent mettre en
evidene de nouveaux omp ortements parmi les onsommateurs et initer les dirigeants
a reer des promotions sur ertains pro duits p our enourager la generalisation de es
nouveaux omp ortements. Une pepite p otentiellement interessante serait une regle de
la forme A ! B ave A un pro duit p eu ou^teux et B un pro duit onereux. Une des
exploitations p ossible de ette pepite etant : realiser une promotion liant A et B , ei
dans le but d'initer d'avantage de onsommateurs a aheter le pro duit B , sahant que
le pro duit A p eut aussi les interesser.
Nous presentons aussi dierentes formes de bruits p ouvant p erturb er les donnees.
Nous prop osons une amelioration de notre algorithme p ermettant de prendre en ompte
la nature bruitee des donnees et d' e^tre don moins sensible aux eets du bruit sur
l'ensemble des regles d'asso iation obtenu a partir de es donnees.
Dans la setion suivante, apres avoir derit les mesures les plus ouramment uti-
lisees, nous serons en mesure de denir plus preisement e que nous entendons par
(( inter e^t )).
2 Quelques mesures d'(( intere^t ))
L'ensemble des mesures presentees, supp ort, onane et dep endane, sont relatives
a des variables disretes ou b o oleennes. La Figure 1 presente les elements de base
p ermettant de omprendre le prinip e de es dierentes mesures. (Lavra et al. 1999)
presente un plus large ensemble de mesures de qualite. Il n'en reste pas moins que es
trois mesures onstituent les elements de base de la plupart des algorithmes existants.
Plaons nous dans le adre de la deouverte de formes du typ e B ! H (app elees
(( regles d'asso iation entre deux elements ))). Nous utilisons la denition des regles
d'asso iation intro duites dans (Agrawal et Srikant 1994).
Le supp ort de [B ! H ‚ÑÑ est deni par P (B ^ H ). Il exprime la probabilite que
B et H soient vrais ensemble. Notons que lorsque P (B ) et P (H ) sont tous les deux
tres imp ortants ('est-a-dire, ils determinent haun une olonne de la base ne onte-
RNTI - 1
Extration de (( pepites )) de onnaissane
nant pratiquement que des valeurs egales a V r ai lorsqu'on se plae dans un ontexte
b o oleen), l'etude de e typ e de relation est sans reel inter e^t bien que le supp ort de e
typ e de relation soit toujours tres eleve.
La onane de la regle [B ! H ‚ÑÑ est denie par P (H jB ) =
P (B ^H )
P (B )
. Cette me-
sure exprime la probabilite onditionnelle que H soit vrai, sahant que B est vrai. Les
regles ayant une onane inferieure a 0:5 sont plus inrmees par les donnees pluto^t
que onrmees, ainsi es regles ne sont pas (( interessantes )).
La dep endane de H par rapp ort a B est denie par Abs(P (H jB )   P (H )) si
P (B ) 6= 0, o u Abs designe la fontion valeur absolue (Ko drato 2000). Si B n'est
pas absurde, 'est-a-dire, si sa probabilite d'o urrene est non nulle, e qui signie que
P (H jB ) =
P (B ^H )
P (B )
est alulable, alors, en presene de B ! H , la probabilite d'obtenir
H sahant B doit e^tre superieure a P (H ). Ainsi, plus la dierene est imp ortante entre
P (H jB ) et P (H ), plus la dep endane de la relation est elevee. Cep endant, notons que
lorsque P (H ) est eleve alors la dep endane est faible ( si P (H ) est eleve, pro he de 1,
nous avons P (H jB )  P (H ) et don P (H jB )   P (H )  0).
Nous allons illustrer, au moyen des trois as presentes dans la Figure 2, ertaines
proprietes de la dep endane. Les trois as presentes sur la Figure 2 representent un as
partiulier et interessant de dep endane. Dans haun de es as, nous avons P (H jB ) =
1, mais les raisonnements qui suivent s'etendent trivialement au as o u P (H jB ) est
(( grand )).
cas 2 cas 3cas 1
B
f
2
H
f
2
B
f
2
H
0
f
2
H
0
f
2
B
0
f
2
Fig. 2 { Trois as il lustrant dierents omportements de la dependane.
Dans le as 1, P (H
f 2
) est faible, ainsi e as orresp ond au as d'une dep endane
elevee, ombinee a un tres faible supp ort. Ce as met en evidene des relations presentant
un inter e^t tres eleve ar les ensembles onsideres sont tres p etits (ils p euvent don e^tre
inonnus de l'exp ert), et les relations onsiderees ne sont jamais inrmees.
Dans le as 2, B
f 2
est (( noye )) dans H
0
f 2
et, il est relativement intuitif de onsiderer
que H
0
f 2
dep end moins de B
f 2
, alors que H
f 2
dep end fortement de B
f 2
, dans le as 1.
Dans e as, la dep endane 1   P (H
0
f 2
), est faible, omme nous nous y attendions.
RNTI - 1
Aze et Ko drato.
Le omp ortement de la dep endane sur es deux premiers as est onforme a nos
attentes. Pour es deux as, l'utilisation de la dep endane, omme mesure d'inter e^t,
p eut sembler aeptable.
L'etude du as 3, p our lequel P (H
0
f 2
) et P (B
0
f 2
) sont tous les deux tres eleves,
montre que la dep endane est faible. La regle d'asso iation B
0
f 2
! H
0
f 2
orresp ond au
as d'une relation relativement triviale et sans reel inter e^t, en terme de onnaissanes
nouvelles. Cep endant, nous allons voir que la denition de la dep endane est relative-
ment ontre-intuitive dans e as. Nous p ouvons noter une ontradition entre l'intui-
tion lassique de la dep endane et e que nous app ellerons la (( dep endane disrete ))
('est-a-dire o u les attributs sont de nature b o oleenne). L'intuition lassique, utilisee
par de nombreux auteurs, rep ose sur la theorie des probabilites qui preise que la proba-
bilite onjointe de deux evenements indep endants, P (B ^ H ), est egale a P (B )  P (H ).
Ainsi, selon l'intuition lassique, la dep endane entre deux evenements, B et H , est
elevee lorsque la dierene entre P (B ^ H ) et P (B )  P (H ) est imp ortante.
Dans le as disret que nous etudions ii, B
0
f 2
et H
0
f 2
sont manifestement tres
dep endants ar B
0
f 2
= V r ai nous p ermet de predire H
0
f 2
= V r ai ave une onane
donnee par
P (B
0
f 2
^H
0
f 2
)
P (B
0
f 2
)
. Cep endant, nous avons bien P (H
0
f 2
)  P (B
0
f 2
)  1  P (B
0
f 2
^
H
0
f 2
).
L'ob jetif de et artile n'est pas d'etudier les details de e paradoxe, mais nous
voulons le mettre en evidene de maniere a montrer que la mesure que nous prop o-
sons, la moindre-ontradition (ontramin), presente, omme la dep endane, des
proprietes paradoxales, lorsque le as 3 est pris en onsideration.
Cette mesure est onstruite de maniere a favoriser les as o u B est pratiquement
totalement inlus dans H , 'est-a-dire o u [P (B ^ H )   P (B ^ :H )‚ÑÑ est (( etonnamment ))
eleve ('est-a-dire, des as o u l'impliation B ! H est rarement inrmee). Pour prendre
en onsideration le as 2 presente i-dessus, nous prop osons de normaliser notre mesure
par rapp ort a P (H ). La ontramin, d'une regle B ! H , est don denie de la maniere
suivante :
ontr amin(B ! H ) =
P (B ^ H )   P (B ^ :H )
P (H )
Le omp ortement de ette mesure est satisfaisant dans les as 1 et 2. Nous avons
bien ontr amin(B
f 2
! H
f 2
) > ontr amin(B
f 2
! H
0
f 2
). Par ontre, dans les as o u
P (B
f 2
)
P (H
f 2
)

P (B
0
f 2
)
P (H
0
f 2
)
, alors notre mesure ne p ermet pas de dierenier le as 1 du as 3.
Ce probleme doit e^tre resolu en utilisant d'autres mesures (par exemple le supp ort de
la regle).
Le supp ort, la onane et la dep endane presentent dierents asp ets de l'inter e^t
d'une regle. La denition de la ontramin rassemble quelques (( qualites )) de haune de
es mesures. La ontramin nous p ermet d'extraire et d'ordonner des regles interessantes
a partir d'une base de donnees.
Preisons les dierents asp ets de la notion d'inter e^t que nous allons utiliser dans la
suite de et artile : (i) une regle interessante doit e^tre plus onrmee par les donnees
qu'inrmee (sinon, elle ne serait pas (( su^re ))), (ii) elle ne doit pas e^tre vraie p our
toutes les donnees (sinon elle serait triviale), (iii) la dep endane entre la premisse de la
regle, B , et la onlusion de elle-i, H , doit e^tre elevee (sinon, nous aurions une regle
RNTI - 1
Extration de (( pepites )) de onnaissane
(( faible ))), (iv) elle doit p ermettre d'ameliorer la onnaissane d'un exp ert du domaine,
(v) elle doit e^tre p eu sensible au bruit present dans les donnees. Nous n'etudierons pas
la ondition (iv) dans la suite de et artile. La ondition (v) sera ab ordee en detail
dans la setion 5.
Pour satisfaire les onditions (i) et (ii), nous ne retenons que les regles B ! H ayant
une onane superieure a 0; 5.
Comme nous l'avons indique preedemment, la ondition (iii) n'est pas remplie par
la dep endane, ar omme nous p ouvons le voir dans la Figure 2, p our les as 2 et
3, la dep endane entre B
0
f 2
et H
0
f 2
est faible alors que B
0
f 2
p ermet de predire H
0
f 2
de maniere parfaite dans les deux as. C'est p ourquoi, nous allons etudier ontramin
denie preedemment.
3 Algorithme p our la deouverte des regles les moins-
ontraditoires
Notre ob jetif prinipal etant lie a la deouverte de (( pepites )) de onnaissane
dans les donnees, nous ne p ouvons pas xer de seuil minimal p our le supp ort. De plus,
ontramin ne p ossede pas de propriete de monotonie ou d'anti-monotonie (une mesure
S est anti-monotone si (8R ; R
0
tq R 2 R
0
al or sS (R ) > S (R
0
)). Il est don di√Üile (voire
imp ossible) de reherher de maniere exhaustive l'integralite des regles d'asso iation
les moins-ontraditoires.
C'est p ourquoi, nous avons ete onduits a limiter le probleme que nous traitons : nous ne
reherhons que des regles d'asso iation ayant les proprietes suivantes : (i) e^tre les regles
presentant les plus grandes valeurs p our ontramin, (ii) e^tre telles que les premisses
des regles ne ontiennent pas plus de K attributs et telles que la onlusion de elles-i
soit reduite a un seul attribut.
Une extension limitee a des regles ayant plusieurs onlusions est en ours, les limites
de l'extension seront ditees par un exp ert du domaine.
Nous prop osons un algorithme p ermettant d'extraire les regles d'asso iation les
moins-ontredites satisfaisant les p oints (i) et (ii).
Pour satisfaire la premiere ondition, (i), nous prop osons d'utiliser une appro he
lassique en Analyse de Donnees (Daude 1992), a savoir normaliser les valeurs de
haque mesure par rapp ort a l'ensemble des mesures observees. Cette normalisation
est eetuee de la maniere suivante : (1) la valeur de la mesure est entree par rapp ort
a la valeur moyenne des mesures observees, puis (2) ette valeur est reduite par rapp ort
a l'eart-typ e observe p our es m e^mes regles.
La deuxieme ondition, (ii), est justiee par le fait que nous onsiderons que des
regles d'asso iation ayant un grand nombre d'attributs en premisse et plus d'un attribut
en onlusion, sont di√Üilement interpretables. M e^me p our un exp ert du domaine, de
trop longues regles sont di√Üilement omprehensibles, 'est p ourquoi, nous avons hoisi
de limiter le nombre d'attributs en premisse et en onlusion des regles reherhees.
Le Tableau 1 presente les notations utilisees dans l'algorithme 1.
RNTI - 1
Aze et Ko drato.
Algorithme 1 ExtraireP

epites(D
p
n
; K
max
; min
sup
)
Entree:
D
p
n
: la base de donnees etudiee ontenant n individus derit par p attributs
K
max
: nombre maximal d'attributs en premisse, deni par l'utilisateur
min
sup
: supp ort minimal p our les pepites de onnaissane
Sortie:
E : ensemble des regles d'asso iation les moins-ontredites par les donnees ontenant
au plus K
max
attributs en premisse et telles que fonf iane(R ) > 0:5; 8R 2 E g.
Debut
K = 1
T
K
= 
0
= 
0
= 0
E = ;
E
1
= f l'ensemble de tous les attributs de la base etudiee g
Pour tout (X
j
2 E
1
) faire
tant que (E
K
6= ;) and (T
K
< 1) and (K  K
max
) faire
   Generation de C
K
a partir de E
K
si (K = 1) alors
   Cas partiulier o u E
K
ne ontient que des attributs
C
K
= fX
i
! X
j
=X
i
2 E
1
; X
j
2 E
1
; i 6= j ; suppor t(X
i
! X
j
) > min
sup
g
sinon
   Cas general o u E
K
ontient des regles
C
K
= fX
i
; X
l
! X
j
=X
i
! X
j
2 E
K
; X
l
! X
j
2 E
K
; i 6=
l ; suppor t(X
i
; X
l
! X
j
) > min
sup
g
n si
E
+
K
= fR 2 C
K
=ontr amin(R ) > T
K
g
E
 
K
= fR 2 C
K
=ontr amin(R )  T
K
g
si (E
+
K
= ;) alors
T
K +1
= T
K
   Dans e as, 
K
et 
K
ne sont pas alulables
E
K +1
= E
 
K
sinon
Calul de 
K
et 
K
E
0
K
= fR 2 E
+
K
=
ontr amin(R) 
K

K
> 1g
E = E [ E
0
K
   Preparation du niveau suivant
E
K +1
= E
 
K
[ (E
+
K
  E
0
K
)
   fregles non proposees a l'expertg
T
K +1
= 
K
+ 
K
n si
K = K + 1
n tant que
n p our
Retourner E
Fin
RNTI - 1
Extration de (( pepites )) de onnaissane
symb ole signiation
D
p
n
base de donnees ontenant n individus derits par p attributs
K nombre d'attributs dans la premisse d'une regle d'asso iation A ! B
E
K
ensemble de toutes les regles d'asso iation utilise
omme ensemble generateur p our C
K
C
K
ensemble des regles d'asso iation (( andidates )) obtenues
a partir de E
K
ontenant les regles d'asso iation les moins-ontraditoires
T
K
seuil d'elagage p our les regles d'asso iation les moins-ontraditoires
E
+
K
sous-ensemble des regles de C
K
dont la moindre ontradition
se situe au dela du seuil T
K
E
 
K
sous-ensemble des regles de C
K
dont la moindre ontradition
se situe en dessous du seuil T
K

K
moyenne des regles d'asso iation les moins-ontraditoires appartenant a E
+
K

K
eart-typ e des regles d'asso iation les moins-ontraditoires appartenant a E
+
K
E
0
K
ensemble des regles d'asso iation les moins-ontraditoires parmi E
+
K
E ensemble des regles d'asso iation telles que
la moindre-ontradition soit la plus elevee
Tab. 1 { Notations utilisees dans l'algorithme 1.
3.1 Details de l'etap e iterative : K  1 ('est-a-dire, B ontient
K attributs)
Les regles appartenant a E
0
K
sont onservees sans e^tre mo diees ar, a l'etap e K +
1, les nouvelles regles obtenues ne doivent pas e^tre des speialisations de regles deja
obtenues a l'etap e K . Cette heuristique est denie de maniere a ne pas surharger
l'exp ert. Notre mesure ne p ossedant pas de propriete de monotonie, nous ne p ouvons
pas garantir qu'il n'existe pas de regle B ^ X ! H plus speique que B ! H et qui soit
moins ontredite. Cep endant, si nous hoisissons de speialiser l'integralite des regles
les moins ontredites, nous obtenons alors un ensemble de regles trop volumineux p our
p ouvoir e^tre soumis a l'exp ert. Nous p ensons qu'il est plus aise p our l'exp ert d'examiner
un ensemble de regles generales et d'etudier, si b esoin, un sous-ensemble de regles issu
de la speialisation d'une regle qu'il aura expliitement hoisie.
Ainsi, ette heuristique p eut e^tre onsideree omme une implantation des travaux
de S. Sahar (Sahar 1999).
Il est imp ortant de noter que l'utilisation de ette heuristique implique que les regles
soient onsiderees omme des quantiations universelles. Don l'exp ert p eut rejeter
une generalisation et aepter une de es speialisations (par exemple, tous les humains
aiment un autre humain : rejetee et Pierre Curie aimait Marie : aeptee). L'exp ert doit
don avoir onsiene de e omp ortement et agir en onsequene lorsqu'il analyse les
regles.
Comme le nombre de regles situees au-dessus d'un seuil de ontramin predeni
augmente tres rapidement ave K , et omme nous voulons eviter d'engendrer trop de
regles, nous devons denir une nouvelle heuristique p ermettant de faire en sorte que
le seuil d'elagage augmente ave K . Nous nous fo alisons alors sur la speialisation
des regles appartenant a E
K +1
= E
 
K
[ (E
+
K
  E
0
K
) et nous initialisons le nouveau seuil
RNTI - 1
Aze et Ko drato.
d'elagage, p our les regles les moins-ontraditoires, ave 
K
+ 
K
, de maniere a ne
retenir que les (( meilleures )) regles les moins-ontraditoires.
Il est aise de montrer que et algorithme onverge lorsque K augmente. A haque
iteration de l'algorithme, les regles obtenues, appartenant a E
0
K
, sont telles que les
valeurs de ontramin sont stritement superieures a elles obtenues a l'etap e preedente.
Ainsi, si les onditions d'arr e^t E
K
= ; ou K > K
max
ne sont pas atteintes, nous p ouvons
garantir que la ondition d'arr e^t T
K
= 1 sera atteinte. En eet, par onstrution, nous
avons 8K ; 
K
> T
K
, 
K
 0 et T
K +1
= 
K
+ 
K
, don 
K
> 
K  1
et T
K +1
> T
K
.
Nous avons T
0
= 0, T
K
est stritement roissant en fontion de K , don, 9K tq T
K
= 1.
3.2 Estimation de la omplexite de l'algorithme
Dans le pire des as, l'ensemble E
+
K
est vide 8K ; 1  K  K
max
et le supp ort des
regles n'est jamais nul. Dans e as, l'algorithme ne trouve auune pepite et auun
elagage ne p eut e^tre realise a l'etap e K p our reduire le nombre de regles andidates a
l'etap e K + 1. Le ou^t de l'algorithme est don exp onentiel en fontion du nombre p
d'attributs de la base de donnees ar, p our haque valeur de K , le nombre de regles
andidates est egal a C
K
p
.
Dans le as general, p our haque valeur de K , l'ensemble E
+
K
ontient des regles qui
p ourront e^tre utilisees p our elaguer l'espae de reherhe et ainsi minimiser le ou^t de
alul des regles a l'etap e K + 1. L'estimation exate du gain represente par et elagage
est di√Üile a realiser. Si on note k le premier niveau sur lequel l'elagage est eetue et
N
+
K
le nombre de regles elaguees, le gain de notre algorithme, par rapp ort a Apriori ,
est egal C
k +1
p
  G
k
p
, o u G
k
p
represente le nombre de regles non speialisees ar elaguees
au niveau k . Or le alul exat de G
k
p
est di√Üile a ause des reouvrements entre
regles elaguees. Par exemple, onsiderons les attributs fA; B ; C ; D ; E ; F g ; si les regles
A; B ! C et A; D ! C appartiennent a E
+
2
alors les regles A; B ; D ! C , A; B ; E ! C
et A; B ; F ! C sont elaguees par la regle A; B ! C et les regles A; D ; B ! C ,
A; D ; E ! C et A; D ; F ! C sont elaguees par la regle A; D ! C . Determiner le
reouvrement de es deux ensembles de regles, ii A; B ; D ! C , est un probleme non
trivial.
Le ou^t de l'appro he reste exp onentiel mais inferieur a elui d'Apriori .
4 Les bases de donnees etudiees
Avant d'utiliser notre metho de sur des bases de donnees reelles, nous avons hoisi
de la tester sur huit bases de donnees de l'UCI (Blake et Merz 1998). Les bases de
l'UCI sont initialement prevues p our evaluer des systemes d'apprentissage sup ervises.
Notre appro he etant non sup ervisee, nous assimilons les lasses ontenues dans es
bases a de simples attributs. Ainsi, les onlusions des regles d'asso iation obtenues ne
sont pas limitees aux lasses denies dans les donnees.
Dans ette premiere serie d'experienes, nous n'avons retenu que des bases de
donnees disretes, puis nous avons transforme toutes es bases en bases de donnees
b o oleennes.
RNTI - 1
Extration de (( pepites )) de onnaissane
Base # d'attributs # d'attributs # d'enregistrements Valeurs absentes ?
disrets b o oleens
ar 7 25 1728 non
monks-1 7 19 432 non
monks-2 7 19 432 non
monks-3 7 19 432 non
mushro oms 23 125 8124 oui
nursery 9 32 12960 non
ti-ta-to e 10 29 958 non
votes 17 49 435 oui
Tab. 2 { Quelques bases de donnees etudiees.
Dans la base (( mushro oms )), les valeurs manquantes orresp ondent a une mesure
non eetuee. Nous les remplaons par la valeur (( faux )) e qui presente le seul defaut
d'augmenter artiiellement la ontradition des regles. Dans la base (( votes )), les
valeurs manquantes orresp ondent a un vote non exprime. Elles ont ete remplaees par
la valeur (( non exprime )) qui s'a joute aux valeurs disretes p ossibles.
4.1 Resultats obtenus
Lors de la validation de notre appro he, nous avons voulu mettre en evidene les
deux p oints suivants :
1. montrer que le volume de regles engendrees par notre appro he est inferieur au
volume de regles engendrees par Apriori, ave les m e^mes ontraintes initiales :
suppor t > 0 et onf iane > 0; 5 ;
2. montrer que l'ensemble des regles obtenues par notre appro he represente une
soure de onnaissanes interessantes p our l'exp ert du domaine ;
Lors de la premiere phase de la validation, le volume de regles extraites par notre al-
gorithme est ompare ave les regles extraites par l'algorithme Apriori (Agrawal et al.
1993). Les ontraintes suivantes sont utilisees p our eetuer l'extration : suppor t
mini
=
0, onf iane
mini
= 0; 5, K
max
= 3.
Les resultats obtenus sont presentes dans le tableau 3.
Nous p ouvons voir que le volume de regles engendre par notre appro he est nette-
ment inferieur a elui obtenu par une appro he de reherhe exhaustive des regles d'as-
so iation. L'appliation d'un p ost-traitement, repro duisant le omp ortement de notre
algorithme, sur l'ensemble des regles engendrees par Apriori, en eliminant toutes les
regles de la forme X ^ Z ! Y si la regle X ! Y est telle que la moindre ontra-
dition de ette regle est superieure au seuil determine par la moyenne et l'eart-typ e
observe sur les regles du m e^me typ e. L'un des avantages de notre appro he reside dans
le fait que nous ne sommes pas ontraints de pro duire l'integralite des regles p our
ensuite devoir les elaguer a l'aide d'heuristiques. Ainsi, les speialisations des regles
(( moindre-ontraditoires )) ne sont pas engendrees.
Les resultats obtenus sont detailles dans le tableau 4.
RNTI - 1
Aze et Ko drato.
Base Apriori notre appro he
ar 1617 5
monks-1 1935 17
monks-2 1796 20
monks-3 1987 17
mushro oms 680088 386
nursery 3319 7
ti-ta-to e 8094 32
votes 17681 232
Tab. 3 { Tail le des ensembles de regles obtenues.
Bases Apriori notre appro he
K = 1 K = 2 K = 3 K = 1 K = 2 K = 3
ar 30 281 1306 2 3 0
monks-1 46 366 1523 9 8 0
monks-2 50 343 1403 10 10 0
monks-3 46 372 1569 9 8 0
mushro oms 1727 48316 630045 265 121 0
nursery 22 378 2919 4 3 0
ti-ta-to e 28 802 7264 0 32 0
votes 628 14338 161925 114 118 0
Tab. 4 { Detail des ensembles de regles obtenus.
RNTI - 1
Extration de (( pepites )) de onnaissane
La deuxieme phase de la validation neessite l'exp ertise d'un speialiste du domaine.
Il est indisp ensable de faire valider les ensembles de regles obtenus par un exp ert, de
maniere a valider l'appro he globale. Le faible nombre de regles obtenues n'est pas
garant de la qualite de elles-i, seul un exp ert p eu appreier la qualite des regles.
Malheureusement, les exp erts sont rares et nous ne disp osons pas de speialistes
p our les bases de donnees etudiees. Cep endant, en utilisant nos onnaissanes generalistes,
nous avons pu valider les regles obtenues a partir des bases ar, nursery et ti-ta-toe.
Ces regles, p eu nombreuses et failement interpretables p our un neophyte, representent
des onnaissanes valides (p our ar et nursery) mais non nouvelles, ompte tenu de
nos onnaissanes. Notre manque d'exp ertise sur es donnees ne nous p ermet pas de
onlure sur la qualite des regles obtenues par rapp ort aux onnaissanes (( attendues ))
a partir de es bases. Les regles obtenues a partir de ti-ta-toe ne sont pas tres infor-
matives.
Ces resultats ne p ermettent pas d'evaluer la apaite de notre algorithme a extraire
les pepites de onnaissanes. Seuls des resultats valides par des exp erts p euvent e^tre
onsideres omme une validation p ositive p our notre appro he.
Nous avons etudie des donnees issues de pro essus anres dans la vie reelle et
interessant des exp ert. Ces donnees ont ete obtenues a partir de orpus de textes et
sont relatives, d'une part a des intro dutions d'artiles sientiques du domaine de la
fouille de donnees erits en langue anglaise et d'autre part a des textes resumant des
questionnaires de ressoures humaines erits en langue franaise et fourni par la so iete
PerformanSe
1
. Les resultats obtenus sur es deux bases de donnees orresp ondent a
des regles d'asso iation extraites en utilisant la moindre ontradition. Les exp erts ont
analyse et valide les regles obtenues. Ces regles ont ete onsiderees omme interessantes
et une partie d'entre elles representaient des onnaissanes nouvelles p our les exp erts
(Ko drato et al. 2003).
Ces deux experimentations sur des donnees reelles nous ont don p ermis de valider
notre appro he.
La setion suivante presente une etude omparative du omp ortement de la moindre
ontradition ave d'autres mesures de qualite en presene de donnees bruitees. Nous
verrons qu'il est di√Üile de travailler ave des donnees bruitees et que la moindre
ontradition est la mesure qui presente le meilleur omp ortement ompte tenu du
adre experimental etudie.
5

Etude du bruit
Comme nous etudions le probleme de la detetion de regles d'asso iation, tous les
attributs p euvent se trouver soit dans la premisse, soit dans la onlusion d'une regle.
Ainsi, l'ensemble des attributs p eut e^tre aete par le bruit, mais un attribut bruite
ne p eut mo dier que les regles ontenant et attribut donne. C'est p ourquoi nous nous
sommes fo alises sur l'etude de trois dierents typ es de bruit.
a. La premiere metho de onsiste a intro duire du bruit dans un attribut X , par exemple,
1. http://www.p erfomanse.fr
RNTI - 1
Aze et Ko drato.
et a etudier les eets du bruit sur les regles ontenant et attribut donne. L'en-
semble des regles ne ontenant pas l'attribut X n'est pas mo die et l'eet du
bruit est lie au nombre de regles ontenant l'attribut X . De maniere a preiser
notre appro he, onsiderons le as d'une regle liant deux attributs binaires, par
exemple X = V r ai et Y = V r ai (e typ e de regle sera elui que nous etudierons
dans la suite de ette setion). Supp osons que et attribut X soit bruite, ave 5%
de bruit. Nous intro duisons le bruit en renversant 5% des valeurs de l'attribut X ,
'est-a-dire en hangeant les valeurs Vrai par Faux et inversement. La quantite
de regles telles que X = V r ai et Y = v r ai est alors mo diee.
En fait, nous observons deux hangements dus a e typ e de bruit :
{ Quand une valeur V r ai devient egale a F aux, des regles ontenant X = V r ai
et Y = V r ai vont dispara^ tre.
{ Quand une valeur F aux devient egale a V r ai, des regles ontenant X = V r ai
et Y = V r ai vont appara^ tre.
Ave ette premiere metho de, nous p ouvons isoler le bruit et omprendre ses
eets. La prop ortion de regles qui disparaissent est liee a la presene de regles
ontenant X = V r ai et Y = V r ai dans la base de donnees ; inversement, la
prop ortion de regles reees par le bruit est liee a la presene de regles ontenant
X = F aux et Y = V r ai dans la base de donnees.
L'algorithme 2 p ermet d'intro duire e typ e de bruit dans une base de donnees.
Pour failiter la omprehension des algorithmes, nous avons intro duit les nota-
tions suivantes. Soit D
p
n
une base de donnees derites par un ensemble O =
fo
i
j1  i  ng d'ob jets et un ensemble A = fa
j
j1  j  pg d'attributs b o oleens.
Nous notons 
j
i
la valeur de l'attribut a
j
p our l'ob jet o
i
. Pour eviter toute am-
big uite, nous notons D
p
n
(
j
i
) la valeur de l'attribut a
j
p our l'ob jet o
i
dans la base
de donnees D
p
n
. Lorsqu'il n'y a pas d'ambig uite sur les notations, n et p p euvent
e^tre omis.
b. La seonde metho de onsiste a intro duire le bruit de maniere aleatoire dans la base
de donnees. Le but de ette experiene est de montrer omment les dierentes
mesures reagissent lorsque les attributs sont bruites de maniere aleatoire ave un
faible p ourentage de bruit.
L'algorithme 3 p ermet d'intro duire e typ e de bruit dans les donnees.
. La troisieme metho de onsiste a intro duire dierents niveaux de bruit sur quelques
attributs de la base. Le but de ette experiene est d'etudier la sensibilite au bruit
des mesures lorsque les donnees sont globalement p eu bruitees (1%) mais que e
bruit est lie a quelques attributs seulement. Nous avons l'impression que e typ e
de bruit reete mieux les situations reelles auxquelles nous sommes onfrontes.
En eet, il est p eu probable que toutes les donnees soient bruitees, 'est-a-dire
inorretes. Si tel etait le as, a priori auune metho de ne p ourrait extraire des
onnaissanes valides a partir de es donnees. Par ontre, il est relativement
RNTI - 1
Extration de (( pepites )) de onnaissane
Algorithme 2 Intro duireBruit-a
Entree: D
p
n
: base de donnees a bruiter
Entree: j : indie de l'attribut bruite (entre 0 et 1)
Entree: 
noise
: bruit intro duit dans les donnees
Sortie: D
0
: base de donnees bruitee
Debut
nb obj ets a br uiter = int(
noise
 p)
   int(x) : partie entiere de (x)
D
0
 D
p our i = 1; i  n; i + + faire
nonB r uite[i‚ÑÑ = 1
n p our
p our (k = 1; k  nb obj ets a br uiter ; k + +) faire
Repeter
i  entier hoisi aleatoirement entre 1 et n
Jusqu'a e que (nonB r uite[i‚ÑÑ = 1)
nonB r uite[i‚ÑÑ = 0
D
0
(
j
i
) = D (
j
i
)
   introdution du bruit en inversant la valeur de D (
j
i
)
n p our
Retourner D
0
Fin
Algorithme 3 Intro duireBruit-b
Entree: D
p
n
: base de donnees a bruiter
Entree: 
noise
: bruit intro duit dans les donnees (entre 0 et 1)
Sortie: D
0
: base de donnees bruitee
Debut
nb obj ets a br uiter = int(
noise
 p  n)
   int(x) : partie entiere de (x)
D
0
 D
p our (i = 1; i  n; i + +) faire
p our (j = 1; j  p; j + +) faire
nonB r uite[i‚ÑÑ[j ‚ÑÑ = 1
n p our
n p our
p our (k = 1; k  nb obj ets a br uiter ; k + +) faire
Repeter
i  entier hoisi aleatoirement entre 1 et n
j  entier hoisi aleatoirement entre 1 et p
Jusqu'a e que (nonB r uite[i‚ÑÑ[j ‚ÑÑ = 1)
nonB r uite[i‚ÑÑjj ‚ÑÑ = 0
D
0
(
j
i
) = D (
j
i
)
   introdution du bruit en inversant la valeur de D (
j
i
)
n p our
Retourner D
0
Fin
RNTI - 1
Aze et Ko drato.
raisonnable de onsiderer que la ma jorite des attributs de la base sont ables, et
que seulement quelques attributs sont bruites.
Par exemple, onsiderons une base d'individus p our lesquels, les informations
suivantes sont renseignees : a^ge, taille, p oids, sexe, nationalite. Cette base de
donnees est remplie par un employe de mairie disp osant de la arte d'identite
de haque individu. La probabilite p our que les informations : taille, sexe, a^ge et
nationalite soient inorretes est tres faible ar es informations sont presentes
sur une arte d'identite. Par ontre, le p oids n'y gure pas, la probabilite d'erreur
est don b eauoup plus elevee p our et attribut que p our les quatre preedents.
Cette troisieme metho de essaye don de rendre ompte de e phenomene en intro-
duisant dierents niveaux de bruit, sur quelques attributs de la base. Le nombre
d'attributs a bruiter est un parametre ontro^le par l'exp ert du domaine etudie.
En fontion du typ e de donnees manipulees et des onditions dans lesquelles es
donnees ont ete olletees, le nombre d'attributs bruites p eut varier signiative-
ment.
De maniere a mieux omprendre l'impat de e bruit sur les regles obtenues,
nous devons intro duire du bruit sur quelques attributs, par exemple 3, et e p our
dierents niveaux de bruit, par exemple 1%, 5% et 10%. Pour haque triplet de
ouple (Attr ibut; br uit), nous observons un ertain bruit resultant dans les regles
obtenues. La moyenne de es dierentes valeurs de bruit nous p ermet d'appreier
l'impat moyen de e bruit sur notre appro he. La ombinatoire de ette metho de
est elevee et l'evaluation de e bruit sur une base de donnees telle que (( Mush-
ro oms )) est tres ou^teuse en temps de alul.
Dans (Aze et Ko drato 2002a, Aze et Ko drato 2002b), nous avons deja etudie le
omp ortement de la moindre-ontradition ave es trois dierents typ es de bruit p our
la detetion des regles d'asso iation ontenant exatement un attribut en premisse et
en onlusion. Nous avons observe que les deux dernieres formes de bruit sont elles
qui ont les eets les plus imp ortants sur les regles les moins-ontraditoires.
Les experienes preedentes ont montre que la seonde forme de bruit est elle qui
degrade le plus l'ensemble des regles obtenues. Nous avons don hoisi d'ameliorer
notre algorithme en eliminant les regles d'asso iation qui sont trop sensibles a ette
seonde forme de bruit.
En fait, nous prop osons de mo dier notre algorithme de maniere a p ouvoir rejeter les
regles d'asso iation qui presentent une tres grande valeur de la moindre-ontradition,
pro he de un, et qui sont tres sensibles au bruit. Ces regles d'asso iation ne sont
jamais (ou rarement) inrmees par les donnees mais elles p ossedent un tres faible
supp ort. Des que nous intro duisons une faible quantite de bruit, e typ e de regles
d'asso iation dispara^ t de l'ensemble des regles les moins-ontraditoires, ei etant du^
a l'apparition de ontraditions liees au bruit. Ces as orresp ondent aux (( pepites ))
de onnaissanes reherhees, mais, lorsque les donnees sont bruitees, es (( pepites ))
sont partiulierement instables. Cep endant, il existe un supp ort minimal tel que es
relations restent stables m e^me en presene de bruit. Nous nous prop osons don d'essayer
de determiner de maniere automatique e supp ort minimal.
RNTI - 1
Extration de (( pepites )) de onnaissane
Weiss et Hirsh ont montre dans (Weiss et Hirsh 1998) que les (( small disjunts ))
sont tres sensibles au bruit et qu'ils degradent signiativement les resultats du pro-
essus d'apprentissage. Ces observations, eetuees dans le adre de l'apprentissage
sup ervise, p euvent e^tre reutilisees dans le adre de la deouverte non sup ervisee de
regles d'asso iation, o u les regles d'asso iation ayant un tres faible supp ort p euvent
e^tre omparees aux (( small disjunts )) de par le fait que seuls p eu d'individus de la
base de donnees verient es regles.
Pour prendre e probleme en onsideration, nous intro duisons le onept de (( supp ort
minimum p our la resistane a un -bruit )). Ce onept est evalue omme etant le sup-
p ort stable p our lequel les regles d'asso iation, ayant une valeur elevee (pro he de 1)
p our la moindre-ontradition, ne disparaissent plus lorsque nous intro duisons N fois
% de bruit dans les donnees. Ce supp ort minimum est utilise omme seuil d'elagage
p our l'etap e suivante de l'algorithme, et un supp ort est onsidere omme stable lorsqu'il
ne varie plus lors des dierentes iterations.
Il existe en eet des regles d'asso iation ayant une moindre ontradition tres elevee
mais un supp ort tres faible. Ces regles orresp ondent, par denition, aux pepites de
onnaissane que nous reherhons. Le probleme est qu'il est di√Üile de determiner,
lorsque leur supp ort est tres faible s'il s'agit bien de pepites ou alors si es regles sont
liees a un bruit present dans les donnees. Si es regles sont dues au bruit alors l'in-
tro dution volontaire de bruit dans les donnees devrait les alterer et elles ne devraient
plus e^tre detetees par l'algorithme de reherhe des pepites.
La onnaissane du supp ort minimum p our la resistane a un -bruit p ermet de
determiner le supp ort en dessous duquel es regles instables apparaissent.
Nous obtenons ainsi un nouvel algorithme qui est tres semblable a la metho de
lassique p ermettant de rendre les reseaux de neurones resistants au bruit (voir, par
exemple, (Haykin 1998)).
Nommons S

noise
le (( supp ort minimal p our la resistane a un -bruit )).
Pour l'instant, l'implantation atuelle ne alule le supp ort minimum que p our les
regles ayant exatement un seul attribut en premisse.
Cette nouvelle heuristique ameliore e√Üaement l'algorithme, p our une des bases de
donnees etudiees, omme nous allons le voir dans la setion suivante.
5.1 Validation experimentale de l'appro he
Nous avons teste l'algorithme 4 sur des donnees artiielles. Notre ob jetif est de
montrer que l'impat des donnees bruitees sur les regles extraites est non negligeable
et qu'il est di√Üile de prop oser des solutions ables p our eviter es problemes. La
solution prop osee p our reduire l'impat du bruit a ete testee en suivant le proto ole
experimental que nous presentons i-apres.
Dans les experimentations realisees, nous avons engendre des bases de donnees
artiielles en utilisant l'outil IBMDataGen disp onible sur la page p ersonnelle de M.
Zaki
2
. Cet outil p ermet d'engendrer des bases de donnees transationnelles. Le nombre
de transations, ainsi que le nombre d'attributs de la base sont parametrables.
2. http://www.s.rpi.edu/zaki/software/
RNTI - 1
Aze et Ko drato.
Algorithme 4 D eteteS

noise
Entree: D
p
n
: la base de donnees etudiee
Entree: 
noise
: bruit intro duit dans les donnees
Entree: N : nombre d'iterations p our obtenir le supp ort minimal S
 noise
Sortie: S

noise
: supp ort minimal garantissant la stabilite des regles les moins-
ontraditoires en presene d'un bruit 
noise
Debut
S

noise
= 0; E
noise
= ;; S
pr e
noise
= 0; N = 1
tant que (S

noise
6= S
pr e
noise
) and (N  100) faire
S
pr e
noise
= S

noise
si (E
noise
= ;) alors
E
0
1
= E xtr air eP epites(D ; 1; S

noise
)
E
noise
= E
0
1
sinon
E
0
1
= E xtr air eP epites(D
0
; 1; S

noise
)
E
noise
= E
noise
\ E
0
1
   regles stables
E
 
noise
= E
noise
  E
0
1
   regles disparues a ause du bruit
E
+
noise
= E
0
1
  E
noise
   regles apparues a ause du bruit
S
 
max
= suppor t(R ) tq R 2 E
 
noise
^ 8R
0
2 E
 
noise
; suppor t(R )  suppor t(R
0
)
S

noise
= max(S

noise
; S
 
max
)
n si
si (S

noise
6= S
pr e
noise
) alors
N = 1
   Supports dierents : redemarrage d'une nouvel le boule de 100 iterations
sinon
N = N +1
   Supports identiques don poursuite de la boule ourante de bruitage
n si
D
0
) I ntr oduir eB r uit   b(D ; 
noise
)
n tant que
Retourner S

noise
Fin
RNTI - 1
Extration de (( pepites )) de onnaissane
Pour haqune des bases de donnees engendrees, l'algorithme 1 d'extration des
pepites de onnaissane a ete applique ave les parametres K
max
= 1 et min
sup
= 0.
Nous avons teste dierentes mesures de qualite en remplaant simplement, dans le ur
de l'algorithme, la mesure ontramin par l'une des mesures suivantes : la Conane,
l'Intensite d'Impliation Classique (Gras 1979), l'Intensite d'Impliation Entropique
(Gras et al. 2001), la Nouveaute (Lavra et al. 1999) et l'Indie de Lvinger (Lvinger
1947). Nous nous sommes limites a es mesures ar nous p ensons qu'elles illustrent
su√Üsament d'asp ets dierents de la qualite des regles.
Les experimentations ont ete realisees sur des bases de donnees ontenant relative-
ment p eu d'attributs (en l'o urene 40 attributs b o oleens) et 30000 transations.
Dix bases de donnees dierentes ont ete engendrees. Et p our haque base de donnees
obtenues, nous avons realise dix fois les experimentations liees au bruit. Pour haque
experimentation, le bruit intro duit dans les donnees varie de 1% a 10%.
Le Tableau 5 presente les resultats obtenus p our es dierentes experimentations
sans la detetion du supp ort minimal S

noise
et le Tableau 6 presente les resultats
obtenus ave la detetion du supp ort minimal S

noise
.
Les Tableaux 5 et 6 s'interpretent de la faon suivante : p our une mesure de qualite,
l'impat du bruit (ave ou sans detetion du supp ort) intro duit dans les donnees (1%,
2%, 5% ou 10%) se manifeste de deux manieres : les regles qui disparaissent (-) et les
regles qui apparaissent (+), par rapp ort aux regles obtenues a partir des donnees non
bruitees.
bruit observe en fontion du bruit intro duit
Mesure 1% 2% 5% 10%
- (%) +(%) -(%) + (%) -(%) + (%) - (%) +(%)
Contramin 5,32 2,5 8,2 2,76 10,77 5,32 16,68 7,39
Conane 15,98 6,52 22,96 8,55 31,57 14,2 42,55 22,9
Nouveaute 9,48 4,92 13,05 6,99 21,75 11,77 31,88 18,65
Indie de Lvinger 12,96 8,3 19,3 10,47 27,95 16,83 40,33 29,76
I IC 10,22 17,64 13,43 24,19 19,73 35,54 26,1 45,07
I IE 11,51 12,19 13,85 17,23 19,2 25,15 30,35 33,52
Tab. 5 { Resultats obtenus sur une base de donnees ontenant 30000 transations et
40 attributs, sans detetion du support S

noise
.
bruit observe en fontion du bruit intro duit
Mesure 1% 2% 5% 10%
- (%) + (%) - (%) + (%) - (%) + (%) - (%) + (%)
Conane 9,94 13,72 21,25 32,57 35,63 46,47 44,81 68,7
Nouveaute 32,08 21,67 39,33 23 71,48 40,89 72,35 18,56
Indie de Lvinger 11,54 35,38 28,0 50,99
I IC 12,38 26,19 19,3 35,44 24,19 45,82
I IE 15,42 19,58 12,49 27,52 17,02 32,9 28,46 54,33
Tab. 6 { Resultats obtenus sur une base de donnees ontenant 30000 transations et
40 attributs, ave detetion du support S

noise
.
RNTI - 1
Aze et Ko drato.
Pour la moindre ontradition, nous n'avons auun resultat onernant l'impat
de la detetion du supp ort S

noise
ar p our les diverses experimentations eetuees,
e supp ort etait nul et n'engendrait don auun elagage. Nous ne p ouvons don rien
onlure sur l'utilisation de S

noise
p our la moindre ontradition.
Par ontre, p our les autres mesures, nous p ouvons voir que globalement le bruit ob-
serve p our les regles interessantes qui disparaissent a diminuer de maniere signiative.
L'appro he retenue p our reduire l'impat du bruit provo que une augmentation dra-
matique du bruit lie au nombre de regles erronees qui sont detetees dans les donnees
bruitees.
La solution que nous avons prop ose p ermet don bien de reduire le p ourentage de
regles interessantes et qui disparaissent lorsque les donnees sont bruitees, par ontre,
le p ourentage de regles erronees induites par ette appro he rend elle-i inutilisable
ar les regles purement dues au bruit sont di√Üles a deteter et seul l'exp ert p eut les
eliminer. Or le temps de l'exp ert est preieux et il est don preferable de ne pas le
solliiter ave des regles probablement erronees.
Nous prop osons don dans la setion suivante une solution p ermettant d'evaluer la
(( abilite )) d'une mesure lorsque le probleme de l'extration de pepites de onnaissane
est ab ordee en presene de donnees bruitees.
5.2

Evaluation de la (( abilite )) des mesures de qualite
N'oublions pas qu'un de nos ob jetifs ma jeurs est de minimiser le travail de l'utili-
sateur. Il est don imp ortant de lui prop oser un ensemble de regles qui verie au mieux
les riteres de qualite imp oses par l'utilisateur et qui soit le plus resistant p ossible au
bruit.
Nous prop osons don une appro he visant, non pas a reduire l'impat du bruit dans
les regles extraites, mais p ermettant d'asso ier a haque regle prop osee a l'exp ert une
estimation de sa resistane au bruit.
Considerons une base de donnees B a partir de laquelle un ensemble de regles E
est extrait (en utilisant la mesure de qualite m). Nous prop osons d'intro duire du bruit
dans B , puis a partir de la nouvelle base bruitee B
0
, d'extraire le nouvel ensemble de
regles E
0
. Cette premiere etap e, en tout p oint identique a elle deja presentee dans les
setions preedentes, est suivie d'une phase de omparaison des regles de E ave elles
de E
0
. L'ob jetif de ette omparaison est de verier si les regles trouvees a partir de B
sont presentes dans E
0
. Si tel est le as, alors es regles sont onsiderees omme ables.
La repetition de ette operation p ermet d'obtenir p our haque regle de E un p our-
entage de (( abilite )) (par rapp ort au bruit).
L'algorithme 5 orresp ond a ette nouvelle appro he.
Nous avons valide ette nouvelle appro he sur le m e^me typ e de donnees arti-
ielles que elles utilisees preedemment. Nous nous sommes fo alises sur les pepites de
onnaissane ontenant exatement un attribut en premisse et un en onlusion. Les
resultats obtenus p ermettent d'etablir un lassement entre les dierentes mesures de
qualite etudiees. Le lassement obtenu est relatif a la abilite moyenne observee sur
N = 20 iterations, et p our 10 bases de donnees dierentes ontenant 30000 transations
derite par 50 attributs. Pour obtenir e lassement, nous avons bruite les donnees en
intro duisant entre 1 et 10% de bruit de la forme b dans les donnees.
RNTI - 1
Extration de (( pepites )) de onnaissane
Algorithme 5 Calul de la abilite des regles.
Entree:
B
p
n
: la base de donnees etudiee

noise
: bruit intro duit dans les donnees
N : nombre d'iterations p our obtenir le supp ort minimal S
 noise
Sortie:
E : ensemble de regles
Debut
E
noise
= ; N = 1
E = E xtr air eP epites(B ; 1; 0)
   on assoie un ompteur aux regles pour determiner leur abilite
Pour tout (R 2 E ) faire
R :pt  0
n p our
p our (i = 1;i  N ; i + +) faire
B
0
= I ntr oduir eB r uit   b(B ; 
noise
)
E
0
= E xtr air eP epites(B
0
; 1; 0)
E
stable
= E \ E
0
   regles stables
   on inremente le ompteur assoie aux regles
Pour tout (R 2 E
stable
) faire
R :pt  R :pt + 1
n p our
n p our
   on normalise le ompteur assoie aux regles
Pour tout (R 2 E ) faire
R :pt  
R:pt
N
n p our
Retourner E
Fin
RNTI - 1
Aze et Ko drato.
50
55
60
65
70
75
80
85
90
95
1 2 3 4 5 6 7 8 9 10
%
 fi
ab
ilit
e 
ob
se
rv
ee
% bruit introduit
Contramin
IIE
IIC
Loevinger
Nouveaute
Confiance
Fig. 3 {

Evolution de la abilite en fontion du bruit introduit.
La gure 3 presente l'evolution de la abilite, p our les six mesures etudiees, en
fontion du bruit intro duit dans les donnees.
l'Intensite d'Impliation Classique et Entropique sont les deux mesures les moins
ables lorsque les donnees sont bruitees. La Moindre Contradition est la mesure la
plus able. Et les autres mesures (Nouveaute, Lvinger et Conane) ont un om-
p ortement relativement pro he lorsque les donnees sont bruites. Le lassement de es
quatre mesures varie en fontion du bruit intro duit dans les donnees.
Pour ompleter es resultats et valider l'appro he retenue, nous avons aussi mesure
le p ourentage de regles qui, p our haque mesure, sont presentes dans E et ne sont
jamais retrouvees dans E
0
p our l'ensemble des 20 experimentations realisees. Seule la
Moindre Contradition arrive a retrouver sur l'ensemble des 20 experimentations au
moins une fois haque regle de E .
6 Conlusions et p ersp etives
L'extration de (( pepites )) de onnaissane dans des donnees orrelees est di√Üile
voire imp ossible ave les appro hes lassiques basees sur l'utilisation du supp ort et de
la onane. L'appro he prop osee ii p ermet d'extraire de telles pepites, representees
par des regles d'asso iation ayant de faibles supp orts et representant les regles les moins
ontredites par les donnees.
L'extration de es regles est fondee sur l'utilisation du ontexte lo al de l'en-
RNTI - 1
Extration de (( pepites )) de onnaissane
semble des regles en ours d'evaluation. Cette appro he, lassique dans le domaine de
la fouille de donnees, p ermet ii de reduire signiativement le nombre de regles pro-
p osees a l'exp ert. Son travail est don allege et les onnaissanes obtenues sont valides
et interessantes.
Les resultats obtenus ne representent qu'une faible partie des onnaissanes ahees
dans les donnees mais es onnaissanes, souvent ignorees par les metho des lassiques,
p euvent e^tre utiles p our l'exp ert.
Les heuristiques d'elagage utilisees dans notre algorithme p euvent e^tre appliquees
aux regles obtenues a partir d'une appro he lassique. Cep endant, le nombre de regles a
analyser est tres volumineux et, ontrairement a notre appro he, de nombreuses regles
sont inutilement engendrees ar nalement elaguees par les heuristiques utilisees.
La validation de notre appro he doit e^tre p oursuivie sur des orpus de tailles plus
imp ortantes. De tels travaux sont en ours, en ollab oration ave un exp ert, sur un
orpus des liens-DNA des levures.
Conernant l'etude du bruit, les resultats obtenus sont assez enourageants, mais
ils montrent que l'extration de regles insensibles au bruit est tres di√Üile a realiser.
Nous avons montre, a travers les diverses appro hes envisagees et les experimentations
asso iees, que la prise en onsideration du bruit p ouvant exister dans les donnees ou
p ouvant les alterer represente un asp et imp ortant de la qualite des onnaissanes
extraites a partir des donnees.
Bien que nous ayons reduit notre etude a une forme de bruit relativement simpliste,
nous avons onstate qu'il est di√Üile de prop oser des solutions p ermettant de reduire
l'impat du bruit sur les onnaissanes obtenues.
L'appro he que nous avons prop osee, fondee sur la detetion automatique du sup-
p ort minimal p ermettant de ne plus p erdre les meilleures regles lorsque les donnees
sont bruitees, a p ermis de reduire signiativement le p ourentage de regles disparais-
sant lorsque les donnees sont bruitees. Malheureusement, ette premiere appro he a
entra^ ne une augmentation dramatique du p ourentage de regle apparaissant lorsque
les donnees sont bruitees et e faisant augmente signiativement le travail de l'exp ert
p our trier les b onnes regles des regles erronees.
Nous avons don opte p our une nouvelle appro he dont l'ob jetif ma jeur est d'as-
sister l'exp ert dans le hoix et l'analyse des regles que nous lui prop osons pluto^t que de
onevoir un ltre visant a seletionner automatiquement les meilleures regles. Cette
nouvelle appro he p ermet d'asso ier a haque regle detetee par l'algorithme d'extra-
tion des pepites de onnaissane dans les donnees un p ourentage de abilite orresp on-
dant a la resistane de la regle lorsque les donnees ontiennent 
noise
% de bruit reparti
de maniere aleatoire. Les resultats obtenus sur des donnees aleatoires ont montre que,
parmi les diverses mesures etudiees, la Moindre Contradition est elle qui se omp orte
le mieux lorsque les donnees sont bruitees. En eet, la Moindre Contradition est la
mesure ayant le p ourentage moyen de resistane au bruit le plus eleve et e p our toutes
les valeurs de bruit intro duit, 
noise
2 [1::10‚ÑÑ.
La metho de prop osee p our extraire les pepites de onnaissane a ete validee sur
des donnees reelles et l'a jout de l'indiateur de abilite aux pepites de onnaissane
p ermet aux exp erts de mieux evaluer la qualite des resultats obtenus. Cette demarhe
doit enore e^tre validee sur d'autres bases de donnees mais les premiers resultats sont
RNTI - 1
Aze et Ko drato.
prometteurs.
Referenes
Agrawal, R., Imielinski, T., et Swami, A. N. (1993). Mining asso iation rules b etween
sets of items in large databases. Dans Proeedings of the 1993 ACM SIGMOD
International Conferene on Management of Data, pages 207{216.
Agrawal, R. et Srikant, R. (1994). Fast algorithms for mining asso iation rules. Dans
Bo a, J. B., Jarke, M., et Zaniolo, C., editors, Pro. 20th Int. Conf. Very Large
Data Bases, VLDB, pages 487{499. Morgan Kaufmann.
Agrawal, R. et Srikant, R. (1995). Mining sequential patterns. Dans Yu, P. S. et Chen,
A. S. P., editors, Eleventh International Conferene on Data Engineering, pages
3{14, Taip ei, Taiwan. IEEE Computer So iety Press.
Aze, J. (2003). Une nouvelle mesure de qualite p our l'extration de pepites de onnais-
sanes. RSTI serie RIA-ECA, 17(1-2-3):171{182.
Aze, J. et Ko drato, Y. (2002a). evaluation de la resistane au bruit de quelques
mesures d'extration de regles d'asso ation. Extration des onnaissanes et ap-
prentissage, 1(4):143{154.
Aze, J. et Ko drato, Y. (2002b). A study of the eet of noisy data in rule extration
systems. Dans Proeedings of the Sixteenth European Meeting on Cybernetis and
Systems Researh (EMCSR'02), volume 2, pages 781{786.
Aze, J. et Ro he, M. (2003). Une appliation de la fouille de textes : l'extration des
regles d'asso iation a partir d'un orpus speialise. RSTI serie RIA-ECA, 17(1-2-
3):283{294.
Blake, C. et Merz, C. (1998). UCI rep ository of mahine learning databases.
Brin, S., Motwani, R., Ullman, J. D., et Tsur, S. (1997). Dynami itemset ounting
and impliation rules for market basket data. Dans Pekham, J., editor, SIGMOD
1997, Proeedings ACM SIGMOD International Conferene on Management of
Data, May 13-15, 1997, Tuson, Arizona, USA, pages 255{264. ACM Press.
Daude, F. (1992). Analyse et justiation de la notion de ressemblane entre variables
qualitatives dans l'optique de la lassiation hierarhique par AVL. PhD thesis,
Universite de Rennes 1.
Freitas, A. A. (1998). On ob jetive measures of rule surprisingness. Dans Priniples
of Data Mining and Know ledge Disovery, pages 1{9.
Gras, R. (1979). Contribution a l'etude experimentale et a l'analyse de ertaines aqui-
sitions ognitives et de ertains ob jetifs didatiques en mathematiques. Master's
thesis, Universite de Rennes 1.
RNTI - 1
Extration de (( pepites )) de onnaissane
Gras, R., Kuntz, P., et Briand, H. (2001). Les fondements de l'analyse statistique impli-
ative et quelques prolongements p our la fouille des donnees. Revue Mathematique
et Sienes Humaines, 154-155:9{29.
Haykin, S. (1998). Neural Networks - A Comprehensive Foundation. Prentie Hall,
2nd edition.
Ko drato Y., Aze J., Ro he M. et Matte-Tailliez O. (2003). Des textes aux asso iations
qu'ils ontiennent. numero speial RNTI "Entreposage et Fouil le de donnees", JDS
2003, 1:171-182
Ko drato, Y. (2000). Comparing mahine learning and knowledge disovery in data-
bases : An appliation to knowledge disovery in texts.
Lavra, N., Flah, P., et Zupan, B. (1999). Rule evaluation measures: A unifying
view. Dans Dzeroski, S. et Flah, P., editors, Ninth International Workshop on
Indutive Logi Programming (ILP'99), volume 1634 of Leture Notes in Artiial
Intel ligene, pages 174{185. Springer-Verlag.
Lvinger, J. (1947). A systemati approah to the onstrution and evaluation of tests
of ability. Psyhologial Monographs, 61:1{49.
Sahar, S. (1999). Interestingness via what is not interesting. Dans Know ledge Disovery
and Data Mining, pages 332{336.
Weiss, G. M. et Hirsh, H. (1998). The problem with noise and small disjunts. Dans
Pro. 15th International Conf. on Mahine Learning, pages 574{578. Morgan Kauf-
mann, San Franiso, CA.
Summary
Most of the lassial approahes for the extration of asso iation rules are based
on the use of thresholds, set by the exp ert, to prune the searh spae. The hoie of
these thresholds, supp osed to e√Üiently separate the set of interesting rules from the
set of obvious rules, is quite di√Üult even for a domain exp ert. Considering that the
data may b e noisy and that the extration of \nuggets" of knowledge (i.e., asso iation
rules with small supp ort) may b e of partiular interest to the exp ert, then the lassial
metho ds are often unable to deal with this problem.
We prop ose a new asso iation rule extration measure alled \least-ontradition".
We show that this measure (i) enables us to extrat \nuggets" of knowledge from the
data, without drowning in a huge amount of rules having small supp orts, (ii) reats
somewhat less badly to noise than the other lassial measures.
Keywords : Asso iation rules, measures of quality, noise.
RNTI - 1
