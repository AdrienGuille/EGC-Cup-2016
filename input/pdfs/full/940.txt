État de l’art sur les méthodes statistiques d’apprentissage
actif
Alexis Bondu∗, Vincent Lemaire∗
∗France Telecom R&D
2 avenue Pierre Marzin 22300 Lannion, France
(alexis.bondu,vincent.lemaire)@orange-ftgroup.com
Résumé. L’apprentissage statistique désigne un vaste ensemble de méthodes et
d’algorithmes qui permettent à un modèle d’apprendre un comportement grâce à
des exemples. L’apprentissage actif regroupe un ensemble de méthodes de sélec-
tion d’exemples utilisées pour construire l’ensemble d’apprentissage du modèle
de manière itérative, en intéraction avec un expert humain. Toutes les stratégies
ont en commun de chercher à utiliser le moins d’exemples possible et de selec-
tionner les exemples les plus informatifs. Après avoir formalisé le problème de
l’apprentissage actif et après l’avoir situé par rapport aux autres modes d’appren-
tissage existant dans la littérature, cet article synthétise les principales approches
d’apprentissage actif et les illustre grâce à des exemples simples.
1 Introduction
En 1964, Freinet écrit dans ses invariants pédagogiques : "La voie normale de l’acquisition
n’est nullement l’observation, l’explication et la démonstration, processus essentiel de l’École,
mais le tâtonnement expérimental, démarche naturelle et universelle"(Freinet, 1964). Au début
du XXè siècle, le pédagogue suisse Adolphe Ferrière (Ferrière, 1922) a été l’un des premiers
à employer le terme "d’école active". L’expression "apprentissage actif" désigne en premier
lieu une méthode d’enseignement permettant d’améliorer l’apprentissage des élèves en leur
donnant un rôle actif.
L’apprentissage actif est une approche qui implique les élèves en les mettant en situation
de progresser et en favorisant leurs interactions avec le groupe. Cette méthode d’enseignement
amène les élèves à construire leurs propres connaissances en se basant sur les expériences
qu’ils ont vécues. Le rôle du professeur est de choisir judicieusement les mises en situation
pour atteindre l’objectif pédagogique le plus rapidement possible.
Les méthodes d’apprentissage actif en informatique sont nées d’un parallèle entre la pé-
dagogie active et la théorie de l’apprentissage. L’apprenant est désormais un modèle (statis-
tique) et non plus un élève. Les intéractions de l’étudiant avec son professeur correspondent
à la possibilité pour le modèle d’interagir avec un expert humain (aussi appellé "oracle"). Les
exemples d’apprentissage sont autant de situations utilisées par le modèle pour générer de la
connaissance.
Les méthodes d’apprentissage actif permettent au modèle d’intéragir avec son environne-
ment en sélectionnant les situations les plus "informatives". Le but est d’entraîner un modèle
Synthèse sur l’Apprentissage Actif
en utilisant le moins d’exemples possible. La construction de l’ensemble d’apprentissage est
réalisée en interaction avec un expert humain de manière à maximiser les progrès du modèle.
Le modèle doit être capable de détecter les exemples les plus utiles pour son apprentissage et
de demander à l’oracle : "Que faut-il faire dans ces situations ?".
Cet article de synthèse a pour but de présenter les principales approches d’apprentissage
actif recensées dans la littérature et de les illustrer grâce à des exemples simples. Les diffé-
rentes stratégies d’apprentissage actif sont traitées de manière générique, c’est-à-dire indépen-
dament du modèle d’apprentissage envisagé (celui qui apprend à l’aide des exemples délivrés
par l’oracle suite à ses demandes). Il existe d’autres approches qui sont, soit des spécifications
de ces algorithmes génériques, soit des méthodes très dépendantes du modèle. Elle ne sont pas
traitées dans cet article.
La première section de l’article introduit le sujet et formalise l’apprentissage actif de ma-
nière générique. Les notations utilisées dans la suite du document sont également détaillées
dans cette partie. Le but de cette section est de situer l’apprentissage actif par rapport aux
autres méthodes d’apprentissage statistique présentes dans la littérature. La deuxième section
est dédiée aux quatre principales approches d’apprentissage actif. Enfin, la dernière section est
une discussion sur les questions soulevées par cet état de l’art.
2 Apprentissage Actif
2.1 Généralité
L’apprentissage statistique (non supervisé, semi-supervisé, supervisé1... etc) a pour but
d’inculquer un comportement à un modèle en se basant sur des observations et sur un al-
gorithme d’apprentissage. Les "observations" sont des instanciations du problème à résoudre
et constituent les données d’apprentissage. A l’issue de son entraînement, on espère que le
modèle se comportera correctement face à de nouvelles situations, on parle de capacité de
généralisation.
Imaginons un modèle de classification binaire qui cherche à distinguer les personnes "heu-
reuses" et "tristes" à partir de leur photo d’identité (voir figure 1). Si le modèle parvient à faire
de bonnes prédictions pour des individus qu’il n’a pas vu lors de son entraînement, alors le
modèle généralise correctement ce qu’il a appris à de nouveaux cas.
FIG. 1 – Exemple illustratif
1On ne parle pas ici de l’apprentissage par renforcement, le lecteur interessé pourra se repporter à (Harmon, 1996)
A. Bondu et al.
La nature des données utilisées varie selon le mode d’apprentissage (voir figure 2). L’ap-
prentissage non supervisé utilise des données démunies d’étiquette. Dans ces conditions, le
modèle ne reçoit aucune information lui indiquant quelles devraient être ses sorties ou même
si celles-ci sont correctes. L’apprenant doit donc découvrir par lui-même les corrélations exis-
tant entre les exemples d’apprentissage qu’il observe. Dans le cadre de l’exemple illustratif
évoqué précédement, le modèle effectue son apprentissage en se basant sur des photos d’iden-
tité démunies d’étiquette et n’a aucune indication sur ce qu’on cherche à lui faire apprendre.
Parmi les méthodes d’apprentissage non supervisée on peut citer les méthodes de "clustering"
(Jain et al., 1999) et les méthodes d’extraction de règles d’association (Jamy et al., 2005).
FIG. 2 – Quelles données pour quel type d’apprentissage?
Lemode d’apprentissage semi-supervisémanipule conjointement des données étiquetées et
non étiquetées. Parmi les utilisations possibles de ce mode d’apprentissage, on peut distinguer
le "clustering" semi-supervisé et la classification semi-supervisée :
– le "clustering" semi-supervisé cherche à regrouper entre elles les instances les plus si-
millaires en utilisant l’information apportée par les données étiquetées. A l’issue de
l’apprentissage, deux instances qui ont des étiquettes différentes n’appartiennent pas
au même groupe. Il existe des approches de "clustering" semi-supervisé considérant des
informations suplémentaires (Cohn et al., 2003). Ces informations permettent notam-
ment spécifier dans l’ensemble d’apprentissage (indépendament des étiquettes) si deux
instances doivent ou non appartenir au même groupe.
– la classification semi-supervisée (Chapelle et Zien, 2005) se base dans un premier temps
sur les données étiquetées pour séparer les instances en fonction de leurs étiquettes. En-
suite, les données non étiquetées sont utilisées pour affiner le modèle prédictif. On peut
par exemple utiliser les données non étiquetées pour estimer les densités de probabilité
de chacune des classes (Chappelle, 2005).
Dans le cadre de l’exemple illustratif, les exemples d’apprentissage pour le mode semi-supervisé
seraient un mélange de photos démunies d’étiquette et de photos associées à une étiquette.
Lors d’un apprentissage supervisé, le modèle s’entraîne sur des données étiquetées. Ces
exemples d’apprentissage sont autant d’instanciations du problème à résoudre pour lesquelles
le modèle connaît la réponse attendue. Un algorithme d’apprentissage est utilisé pour régler les
paramètres du modèle en se basant sur l’ensemble d’apprentissage. Dans le cadre de l’exemple
illustratif évoqué ci-dessus, les exemples d’apprentissage pour le mode supervisé seraient des
photos d’identité associées à une étiquette ayant pour valeur "heureux" ou "triste".
Synthèse sur l’Apprentissage Actif
Enfin, l’apprentissage actif permet au modèle de construire son ensemble d’apprentissage
au cours de son entraînement, en interaction avec un expert (humain). L’apprentissage débute
avec peu de données étiquetées. Ensuite, le modèle sélectionne les exemples (non étiquetés)
qu’il juge les plus "instructifs" et interroge l’expert à propos de leurs étiquettes. Dans notre
exemple illustratif, le modèle présente des photos à l’oracle pour obtenir les étiquettes asso-
ciées. Les stratégies d’apprentissage actif et les exemples jouets présentés dans cet article sont
utilisés dans le cadre de la classification. Il est évident que ces approches peuvent être transpo-
sée au cas de la régression.
La particularité de l’apprentissage actif réside dans l’interaction du modèle avec son envi-
ronnement. Contrairement à la stratégie "passive" où les exemples sont choisis avant l’appren-
tissage, de manière aléatoire, les stratégies "actives" permettent d’accélérer l’apprentissage
en considérant d’abord les exemples les plus informatifs. Cette approche est particulièrement
avantageuse lorsque les données sont coûteuses à acquérir et à étiqueter.
2.2 Deux sénarii possibles
L’apprentissage actif a pour but de détecter les exemples les plus "instructifs" pour les
étiqueter, puis de les incorporer à l’ensemble d’apprentissage. Rui Castro (Castro et Nowak,
2005) distingue deux scénarii possibles : l’échantillonnage adaptatif et l’échantillonnage sélec-
tif (voir figure 3). Il s’agit de deuxmanières différentes de poser le problème de l’apprentissage
actif.
Apprentissage
Apprentissage 
       Actif
Apprentissage
       Passif
Séléctif Adaptatif
Echantillonnage Echantillonnage
FIG. 3 – Apprentissage : vue générale
Il est important de distinguer les données brutes et les descripteurs qui leur sont associés
(voir figure 4). Dans notre exemple illustratif, les données brutes sont les photos d’identité à
proprement parlé et les descripteurs sont des attributs décrivant les photos (pixel, luminosité,
contraste... etc). Le modèle fait correspondre la prédiction de la classe "heureux" ou "triste"
à chaque vecteur de descripteurs placé en entrée. Il est important de noter que le calcul des
descripteurs à partir des données brutes n’est pas forcement "bijectif". Il est parfois impossible
de reconstituer une donnée brute en se basant sur des descripteurs.
A. Bondu et al.
FIG. 4 – Chaîne de traitement d’un modèle prédictif
Dans le cas de l’échantillonnage adaptatif (Singh et al., 2006), le modèle demande à
l’oracle des étiquettes correspondant à des vecteurs de descripteurs. Le modèle n’est pas res-
treint et peut explorer tout l’espace de variation des descripteurs, à la recherche de zones à
échantillonner plus finement. Dans certains cas l’échantillonnage adaptatif peut poser pro-
blème lors de sa mise en oeuvre. En effet, il est difficile de savoir si les vecteurs de descripteurs
générés par le modèle ont toujours un sens vis à vis du problème initial. Pour illustrer ceci, on
peut se reporter à la figure 4. Supposons que le modèle demande l’étiquette associée au vecteur
[10, 4, 5.., 12], on ne sait pas si cet ensemble de descripteurs correspond à une photo de visage
humain ou bien à une fleur ou encore à un animal.
Dans le cas de l’échantillonnage sélectif (Roy et McCallum, 2001), le modèle n’observe
qu’une partie restreinte de l’univers matérialisée par des exemples d’apprentissage démunis
d’étiquette. Par conséquent, les vecteurs d’entrées sélectionnés par le modèle correspondent
toujours à une donnée brute. On emploie généralement l’image d’un "sac" d’instances pour
lesquelles le modèle peut demander les labels associés. Dans l’exemple illustratif précédent,
le modèle demande l’étiquette associée au vecteur [10, 4, 5.., 12] qui correspond à une photo
d’identité dont on dispose. L’oracle aura beaucoup plus de facilité à étiqueter une photo qu’un
ensemble de descripteurs.
Dans la suite de cet article, on se place du point de vue de l’échantillonnage sélectif. Cet
état de l’art s’intéresse aux problèmes d’apprentissage pour lesquels il est facile d’obtenir
un grand nombre d’instances non étiquetées et pour lesquels l’étiquetage est coûteux. Dans la
pratique, le choix de l’échantillonnage sélectif ou adaptatif dépend essentiellement du domaine
d’application. Selon les cas le modèle est autorisé (ou non) à "générer" de nouvelles instances.
2.3 Notations
Cette section définit l’ensemble des notations utilisées dans cet article, ces notations sont
également illustrées grâce à un problème jouet.
SoitM ∈ M le modèle prédictif dont on cherche à faire l’apprentissage grâce à un algo-
rithme L. La figure 5 représente les différents ensembles mis en jeux. L’ensemble X ⊆ Rn
représente toutes les entrées possibles du modèle et x ∈ X en est une instance particulière.
On définit également Y l’ensemble des réponses potentielles du modèle. Soit y ∈ Y un label2
particulier associé à une entrée x ∈ X.
2On entend par label, soit une valeur discrète dans un problème de classification, soit une valeur continue dans un
problème de régression
Synthèse sur l’Apprentissage Actif
Lors de son appentissage, le modèle n’observe qu’une partie Φ ⊆ X de l’univers. On
dispose d’un ensemble fini de points d’observation et on ne connaît pas nécessairement les
labels associés à ces points. Soient Ux la partie des instances observables pour lesquelles on
ne connaît pas les labels et Lx la partie des instances observables pour lesquels on dispose des
labels associés. On a : Φ ≡ Ux ∪ Lx et Ux ∩ Lx ≡ ∅.
Le concept cible que l’on veut apprendre au modèle peut être vu comme une fonction
f : X→ Y, avec f(x1) la réponse arrendue du modèle pour l’entrée x1. On définit également
f̂ : X → Y la réponse effective du modèle, il s’agit d’une estimation du concept cible. Les
éléments de Lx et les labels qui leur sont associés constituent un ensemble d’apprentissage T .
Les exemples d’appentissage sont des couples (x, f(x)) : ∀x ∈ Lx, ∃!(x, f(x)) ∈ T .
FIG. 5 – Ensembles mise en jeux
Illustration des notations :
Pour bien comprendre ces notations, considérons le problème jouet suivant (Guestrin et al.,
2005) (voir figure 6). On dispose d’une pièce équipée de capteurs de température fixés au
plafond. On cherche à estimer la température en tout point de la pièce, en se basant sur quelques
points de mesure. Pour ce problème le vecteur d’entrée du modèle est de dimension deux
(R2), puisqu’on considère le plan sur lequel sont fixés les capteurs. On se restreint à la pièce,
l’ensemble X ⊆ R2 est donc délimité par les murs. Y ≡ [−50C, 100C] est l’ensemble des
températures envisageables dans une pièce.
Ici, l’ensemble des points observables Φ ⊆ X correspond à l’ensemble des capteurs. Il
s’agit bien d’un problème d’échantillonnage sélectif puisqu’on ne peut obtenir la température
que pour l’ensemble (le "sac") des capteurs. Supposons maintenant qu’une partie de ces cap-
teurs soit en panne. Lx est l’ensemble des capteurs en état de fonctionner et Ux est l’ensemble
des capteurs hors service. Le concept qu’on cherche à apprendre peut être vu comme une fonc-
tion f : X → Y qui à chaque point de la pièce associe sa température. Dans le cadre de cet
exemple jouet, l’apprentissage actif est une approche capable de désigner les capteurs à réparer
prioritairement pour améliorer la prédiction du modèle.
FIG. 6 – Estimation de la température dans une pièce
A. Bondu et al.
3 Méthodes d’apprentissage actif
3.1 Introduction
Le problème de l’échantillonnage sélectif a été posé formellement par Muslea (Muslea,
2002) (voir Algorithme 1). Celui-ci met en jeu une fonction d’utilité, Utile(u,M), qui estime
l’intérêt d’une instance u pour l’apprentissage du modèleM. Grâce à cette fonction, le modèle
présente à l’oracle les instances pour lesquelles il espère la plus grande amélioration de ses
performances.
L’Algorithme 1 est générique dans la mesure où seule la fonction Utile(u,M) doit être
modifiée pour exprimer une stratégie d’apprentissage actif particulière. Comment peut-on "pré-
juger" efficacement de l’intérêt d’un exemple pour l’apprentissage d’un modèle, avant même
que celui-ci n’ait été étiqueté ? C’est ce que nous allons voir dans la suite de cet état de l’art.
Étant donnés :
• M un modèle prédictif muni d’un algorithme d’apprentissage L
• Les ensembles Ux et Lx d’exemples non étiquetés et étiquetés
• n le nombre d’exemples d’apprentissage souhaité.
• L’ensemble d’apprentissage T avec ‖T‖ < n
• La fonction Utile : X ×M → ℜ qui estime l’utilité d’une instance pour l’apprentissage
d’un modèle.
Répéter
(A) Entraîner le modèleM grâce à L et T (et éventuellement Ux).
(B) Rechercher l’instance q = argmaxu∈Ux Utile(u,M)
(C) Retirer q de Ux et demander l’étiquette f(q) à l’oracle.
(D) Ajouter q à Lx et ajouter (q, f(q)) à T
Tant que ‖T‖ < n
Algorithme 1: échantillonnage sélectif, Muslea 2002
3.2 Echantillonnage par incertitude
Cette stratégie d’apprentissage actif (Lewis et Gale, 1994) (Thrun et Möller, 1992) est
basée sur la confiance que le modèle a en ses prédictions. Le modèle d’apprentissage utilisé
doit être capable de fournir une réponse au problème traité et d’estimer la fiabilité de ses
réponses. Pour illustrer ceci, prenons l’exemple des fenêtres de Parsen (Chappelle, 2005) dont
la "sortie" est une estimation de probabilité conditionnelle, définie de la manière suivante :
Pˆ (yj |un) =
∑N
i=1 f(ui)=yj
K(un, ui)∑N
i=1K(un, ui)
Synthèse sur l’Apprentissage Actif
où
K(un, ui) = e
||un−ui||
2
2σ2
Dans le cadre de cet exemple, on choisit d’utiliser un noyau gaussien de norme L2 et on
se place dans le cas d’une classification. Finalement, on dispose des probabilités d’observer
chacune des classes yj pour une instance un. Cela permet au modèle de faire une prédiction
en choisissant la classe la plus probable pour l’instance un. La probabilité d’observer la classe
choisie peut être vue comme la confiance que le modèle a en sa prédiction. Le choix des
nouveaux exemples à étiqueter se déroule en deux étapes :
• on utilise le modèle dont on dispose à l’itération t et on prédit les étiquettes des
exemples non-étiquetés ;
• on sélectionne les exemples pour lesquels la prédiction est la plus incertaine.
L’incertitude d’une prédiction peut également être définie par rapport à un seuil de décision.
Prenons l’exemple d’un réseau de neurones qui ne possède qu’un seul neurone de sortie et qui
est utilisé dans le cadre d’une classification binaire. Supposons que la sortie du réseau de
neurones soit une valeur continue comprise entre 0 et 1. Un seuil de décision est défini pour
faire correspondre une sortie à une des deux classes. Plus la sortie du réseau de neurones est
proche du seuil de décision, plus la décision sera considérée comme incertaine.
La figure 7 représente un problème de classification binaire. La séparatrice correspondant
au seuil de décision du modèle est tracée ainsi que des lignes de niveaux relatifs aux valeurs
de sortie du modèle. Les données non-étiquetées qui se situent à proximité de la séparatrice
(au sens des lignes de niveaux) sont considérées comme étant les plus incertaines, elles seront
donc choisies pour être étiquetées par l’oracle.
FIG. 7 – échantillonnage par incertitude : Classification binaire
Cette première approche a l’avantage d’être intuitive, facile à mettre en oeuvre et rapide.
L’échantilonnage par incertitude montre cependant ses limites lorsque le problème à résoudre
n’est pas séparable par le modèle. En effet, cette stratégie aura tendance à sélectionner les
exemples à étiqueter dans des zones de mélange, où il n’y a peut être plus rien à apprendre.
A. Bondu et al.
3.3 Echantillonnage par comité de modèles
L’échantillonnage par comité de modèles (ou "Query-by-Committee") est une stratégie qui
vise à réduire l’espace des versions3 d’un problème d’apprentissage (Dima et Hebert, 2005).
Rappelons que l’espace des versions S ⊆ H est l’ensemble des hypothèses consistantes avec
les données d’apprentissage étiquetées, cet espace représente les informations contenues dans
l’ensemble d’apprentissage.
Réduction de l’espace des versions :
Seung (Seung et al., 1992) est le précurseur de l’échantillonnage par comité. Cette approche
génère une multitude de modèles d’apprentissage qui sont entraînés en parallèle sur les mêmes
données. En considérant que chacun de ces modèles trouvent une hypothèse consistante h ∈ S,
on dispose alors d’un "échantillon" d’hypothèses qu’on espère représentatif de l’espace des
versions. On cherche à mesurer le désaccord au sein du comité de modèles lors de la prédic-
tion du label des exemples non étiquetés. Les exemples qui suscitent le plus grand désaccord
sont ceux qui ont la plus forte probabilité de réduire S une fois étiquetés.
Dans la pratique, on n’a pas la garantie que le comité de modèles échantillonne correcte-
ment l’espace de versions. En effet, il peut être impossible pour les modèles de trouver des
hypothèses consistantes avec les données d’apprentissage. Cela peut être simplement dû à du
bruit présent dans les données. Ou encore, le problème traité peut être trop complexe par rap-
port au type de modèles utilisés. Pour pallier à cette difficulté, on peut utiliser des modèles
"stochastiques"4, ce qui favorise la diversité des hypothèses.
L’échantillonnage par comité peut être utilisé avec des modèles génératifs5(McCallum et
Nigam, 1998). Cela permet d’échantillonner l’espace des versions en définissant une distribu-
tion de probabilité sur les paramètres des modèles. Il s’agit d’une solution élégante mais qui
n’est pas applicable dans le cas général, où on ne possède pas de distribution de probabilité sur
les paramètres des modèles.
Il existe également un moyen d’utiliser des modèles dont l’apprentissage est "détermi-
niste" dans le cadre d’un echantillonnage par comité de modèles. Abe et Mamitsuka (Abe et
Mamitsuka, 1998) proposent le "Query-by-Bagging" qui permet d’entraîner les modèles sur
des sous-ensembles d’exemples disjoints. Les sous-ensembles d’exemples ont une distribution
de probabilité similaire à celle de la totalité des exemples. Dans ce cas, on peut utiliser des mo-
dèles "déterministes" (tels que des régressions linéaires) et obtenir des hypothèses différentes.
La diversité des hypothèses est due aux sous-ensembles d’exemples utilisés pour l’apprentis-
sage de chaque modèle.
Mesure de désacord :
Yoav Freund (Freund et al., 1997) propose une approche qui se base sur la théorie de l’informa-
tion et estime le désaccord du comité de modèles par une mesure d’entropie sur les prédictions.
Dans le cadre de cette approche le gain d’information est un bon estimateur de la réduction de S
(c’est-à-dire de l’amélioration du comité de modèles après l’ajout d’un nouvel exemple). Plus
précisément, considérons un exemple non étiqueté noté u ∈ Ux qui peut potentiellement être
3Pour plus de détails sur l’espace des versions, se reporter à la section 3.4.
4on entend par modèles "stochastiques" des modèles dont l’apprentissage n’est pas déterministe
5un modèle "génératif" est un modèle capable de générer des donnés aléatoires et de modéliser la distribution de
probabilité qui régit ces données.
Synthèse sur l’Apprentissage Actif
associé à ‖Y‖ étiquettes. On peut estimer l’entropie des prédictions des modèlesM1, ...,Mm
de la manière suivante :
Ĥ(u|M1, ...,Mm) =
‖Y‖∑
j=1
−P̂ (yi|u,M1, ...,Mm)logP̂ (yi|u,M1, ...,Mm)
Notons que la probabilité d’observer la classe yi conditionnellement à l’instance u est
estimée grâce à l’ensemble des prédictions du comité de modèles. Cette expression estime
l’entropie des étiquettes associées à une instance u, sur l’espace de versions.
Dans la littérature, il existe d’autres métriques pouvant quantifier le désaccord au sein d’un
comité de modèles. Dans le cas d’une classification binaire, on peut se baser sur le nombre
de modèles qui prédisent l’une ou l’autre des classes (Abe et Mamitsuka, 1998). Dans ce cas,
le score représentant le désaccord peut être donné par l’effectif des modèles qui prédisent la
classe minoritaire. Cela peut s’écrire de la manière suivante (avec f̂(u) la prédiction du comité
de modèle et f̂Mk(u) la prédiction du modèleMk) :
Desaccord(u) =
m∑
k=1
1{bfMk(u) 6=
bf(u)}
La "Kullback-Leibler-divergence-to-the-mean" (McCallum et Nigam, 1998) est une autre
métrique qui a l’avantage de prendre en compte la confiance des prédictions faites par les
modèles pour mesurer leur désaccord. Cette métrique est définie comme étant la moyenne (sur
les m modèles) de la "KL-divergence" entre la distribution des classes estimée par un modèle
Mk du comité et la distribution moyenne estimée grâce à la totalité du comité. Cela peut
s’écrire de la manière suivante :
Desaccord(u) =
1
m
m∑
k=1
Div(P (Y|u,Mk)‖P (Y|u,M1...Mm))
Desaccord(u) =
1
m
m∑
k=1
‖Y‖∑
j=1
P (yj |u,Mk)Log
P (yj |u,Mk)
P (yj |u,M1...Mm)
3.4 Arbre sur l’espace des versions
Contrairement à l’échantillonnage par comité (voir section 3.3), certains modèles d’ap-
prentissage permettent de manipuler directement l’espace des versions (sans avoir recourt à un
comité de modèles). On citera à titre d’exemple la stratégie qui consiste à choisir les exemples
à étiqueter dans la marge d’un SVM (Tong et Koller, 2000) ou celle des SG-net (Cohn et al.,
1994) qui est basée sur les réseaux de neurones. Il existe de nombreuses autres approches d’ap-
prentissage actif basées sur la réduction de l’espace des versions, notamment celles qui sont
dépendantes du modèle. On ne les sitera pas toutes, le but de cette section étant de présenter
cette approche d’un point de vue générique.
L’apprentissage actif peut être vu comme le parcours d’un arbre de décision construit sur
l’espace des versions du modèle (voir figure 8). Le nœud principal de l’arbre correspond au
A. Bondu et al.
premier exemple qu’on soumet à l’oracle (partie A de la figure 8). Selon l’étiquette attribuée à
cet exemple (partie B de la figure 8), on se place sur une des branches issues de ce nœud. Cela
à pour effet de désigner le prochain nœud à parcourir et donc le prochain exemple à étiqueter.
Dans le cadre de cette approche, toutes les décisions sont prises lors de la construction de
l’arbre de décision.
FIG. 8 – Arbre sur l’espace des versions : cas d’un problème binaire
Soient les ensembles d’apprentissage Ux et Lx précédemment décrits. On définit H l’en-
semble des hypothèses que le modèleM ∈ M peut atteindre. Rappelons qu’une hypothèse f̂
représente le concept à apprendre. L’espace des versions S ⊆ H est l’ensemble des hypothèses
consistantes avec Lx. On définit également s ⊆ Ux l’ensemble des données "non étiquetées"
classées différemment par les hypothèses de S.
On cherche à déterminer l’hypothèse h ∈ H consistante avec toutes les données d’appren-
tissage6, dont la construction requiert un minimum d’étiquetages. L’étiquetage d’une instance
x ∈ s réduit l’espace des versions. Ce nouvel espace S′ dépend de l’étiquette qu’on attribue à
l’instance x.
On définit pi la distribution de probabilité des hypothèses de H. La stratégie gloutonne pro-
posée par Sanjoy Dasgupta (Dasgupta, 2005) consiste à demander le label de l’instance xi ∈ s
qui conduit "potentiellement"7 à des espaces des versions de même poids, au sens de pi.
Exemple illustratif :
Pour illustrer ceci, décrivons cette stratégie dans le cas d’un problème de classification à deux
classes. Pour chaque instance xi ∈ s, on définit S+i ⊆ S (resp S−i ⊆ S) l’ensemble des hy-
pothèses consistantes qui classent xi positivement (resp négativement). On cherche à étiqueter
l’instance qui sépare S en deux sous-ensembles S+i et S
−
i les plus équiprobables possibles au
sens de pi.
Comme on peut le voir à travers l’algorithme 2, cette stratégie peut être représentée par
un arbre de décision, dans lequel un nœud Ni est un "test" sur le label de l’instance xi. Les
6On suppose dans cette approche qu’il existe au moins une hypothèse h ∈ H consistante avec toutes les données
d’apprentissage
7Selon l’étiquette attribuée à xi
Synthèse sur l’Apprentissage Actif
branches connectées à ce nœud correspondent aux différentes valeurs possibles du label de xi.
Les feuilles de l’arbre sont des hypothèses h ∈ H. La hauteur de l’arbre représente le nombre
maximal de questions posées à l’oracle.
Étant donnés :
• N ≡ {N1[x1], N2[x2], ..., Nn[xn]} les nœuds de l’arbre et les instances testées
• S ≡ {S1, S2, ..., Sn} les espaces des versions du modèle sur les nœuds
• s ∈ Ux les instances classées différemment par les hypothèse de S
• Y ≡ {y1, y2, ..., ym} les labels qui correspondent auxm branches connectées à un nœud.
• {S1h,i, .., S
m
h,i} les sous-ensembles de Sh dus au test de xi au nœud h
• Ent : HN → ℜ l’entropie d’un ensemble d’hypothèses
• Gain : HN × X→ ℜ le gain d’information
N ← {N1}
S← {S1}
Répéter
Pour chaque nœud Nh ∈ N faire
Pour chaque instance candidate xi ∈ s faire
Calculer Ent(Sh)
Pour chaque label yj ∈ Y faire
Calculer Ent(Sjh,i)
Fin Pour
Calculer Gain(Sh, xi)
Fin Pour
Sélectionner q = argmaxx∈s Gain(Sh, x) pour le nœud courant
On affecte q au nœud courant Nh ← Nh[q]
On retire q des instances candidates s← s \ {q}
Pour chaque label yj ∈ Y faire
création d’un nouveau nœud N ← N ∪ {N[h×‖Y‖+j]}
création de l’espace des versions associé S← S ∪ {S[h×‖Y‖+j]}
Fin Pour
Fin Pour
Tant qu’il y a plusieurs hypothèses dans les feuilles de l’arbre
Algorithme 2: Construction d’un arbre sur l’espace des versions
Le problème jouet présenté par la figure 9 illustre la construction d’un arbre de décision
sur l’espace des versions d’un modèle d’apprentissage. Le problème traité dans cet exemple
est une classification binaire dans le plan. On suppose que le concept cible est une sépara-
trice linéaire contrainte de passer par un certain point. L’espace des versions du modèle est
l’ensemble des hypothèses consistantes avec les données étiquetées. Sur la partie gauche de
la figure 9, il s’agit de l’ensemble des droites qui passent par le point noir et qui séparent
correctement les données étiquetées. Dans le cadre de cet exemple jouet on suppose que la
distribution de probabilité des hypothèses est uniforme. Dans la partie centrale de la figure 9,
A. Bondu et al.
on trouve l’instance non étiquetée qui sépare l’espace des versions en deux sous-espaces les
plus équiprobables possibles. Le test de l’étiquette de cette instance correspond à un nœud de
l’arbre. La partie droite de la figure 9 montre que l’étiquetage de cette instance réduit environ
de moitié l’espace des versions (au sens de pi).
FIG. 9 – Arbre sur l’espace des versions : problème jouet
Lors de la construction de cet arbre de décision, on cherche les meilleures instances à
tester pour chacun des nœuds. Pour cela, on mesure pour chaque xi ∈ s le gain d’information.
Ce dernier est calculé grâce à l’entropie de l’espace des versions "courant" et des n sous-
ensembles8 induits par le test du label de xi. On sélectionne l’instance dont le "test" induit le
plus grand gain d’information pour la construction du nœud courant. L’entropie "Ent" et le gain
d’information "Gain" sont définis de la manière suivante (les Sji étant les n sous-ensembles
induits par le test de l’instance xi) :
Ent(S) =
∑
S
−logpi(h).pi(h)
Gain(S, xi) = Ent(S)−
n∑
j=1
‖Sji ‖
‖S‖
Ent(Sji )
Dans le cadre de l’apprentissage actif, la qualité Q de la stratégie utilisée peut être définie
comme étant la hauteur moyenne de l’arbre T résultant de cette stratégie, puisqu’on cherche
à poser le moins de questions possible à l’oracle. Cela s’écrit de la manière suivante (avec
Height(h) la hauteur de la feuille correspondant à l’hypothèse h) :
Q(T , pi) =
∑
h∈H pi(h).Height(h)
De manière générale, l’efficacité d’une stratégie d’apprentissage actif dépend de la typolo-
gie des données d’apprentissage. Il y a des problèmes triviaux qui nécessitent tous les labels
manquants pour trouver la bonne hypothèse. Sanjoy Dasgupta (Dasgupta, 2005) établit des
bornes théoriques pour cette méthode sur le nombre de labels demandés à l’oracle.
8Pour la classification à deux classes, les deux sous-ensembles en question sont S+i et S
−
i
Synthèse sur l’Apprentissage Actif
Pour conclure, la construction d’un arbre de décision sur l’espace des versions est difficile à
mettre en œuvre dans la pratique car on dispose rarement de l’espace des versions d’un modèle
et de la distribution de probabilité des hypothèses. Néanmoins, cette approche constitue une
vue théorique intéressante de l’apprentissage actif.
3.5 Réduction de l’erreur de généralisation du modèle
L’approche basée sur la réduction de l’erreur de généralisation du modèle (Cohn et al.,
1995) choisit les exemples à étiqueter de manière à minimiser cette erreur notée E(Mt). Dans
la pratique cette erreur ne peut pas être calculée. Pour cela il faudrait disposer de l’ensemble des
entrées du modèle notéX. On peut cependant exprimer l’erreur de généralisation du modèleM
à l’instant t en utilisant une fonction de "coût" (Loss(Mt, x)) qui évalue l’erreur du modèle
pour une entrée particulière x ∈ X.
E(Mt) =
∫
X
Loss(Mt, x)P (x)dx
On définitMt+1(x⋄,y⋄), le même modèle à l’itération t + 1. Ce modèle prend en compte un
nouvel exemple d’apprentissage noté (x⋄, y⋄). Dans la pratique, la sortie du modèle y⋄ n’est
pas connue puisque x⋄ est une donnée non étiquetée. Pour estimer l’erreur de généralisation
à l’itération t + 1, il faut envisager toutes les possibilités de l’ensemble Y et les pondérer par
leur probabilité. L’erreur de généralisation "attendue" s’écrit alors :
E(Mt+1x⋄ ) =
∫
X
∫
Y
P (y|x⋄)Loss(Mt+1(x⋄,y), x)P (x)dxdy
Cette stratégie sélectionne l’instance q qui minimise E(Mt+1x⋄ ). Une fois étiquetée, cette ins-
tance est incorporée à l’ensemble d’apprentissage. On espère ainsi construire itérativement un
ensemble d’apprentissage optimal.
Sans disposer de tous les éléments de X, Nicholas Roy (Roy et McCallum, 2001) montre
comment cette stratégie peut être mise en œuvre en utilisant uniquement les données d’ap-
prentissage. On estime l’erreur de généralisation en considérant seulement les exemples dont
on dispose à l’instant t et en adoptant un à priori uniforme pour P (x) :
Ê(Mt) = 1‖Lx‖
∑‖Lx‖
i=1 Loss(M
t, xi)
Pour choisir l’instance à étiqueter, le modèle est ré-entraîné plusieurs fois en considérant
un exemple supplémentaire. Chaque instance xi ∈ Ux et chaque label yj ∈ Y peuvent s’asso-
cier pour former cet exemple supplémentaire. Pour chaque exemple candidat xi, on entraîne le
modèle plusieurs fois en fixant la valeur de l’étiquette puis en mesurant l’erreur de générali-
sation Ê(Mt+1(xi,yj)). Lorsque toutes les étiquettes ont été envisagées pour une instance xi, on
estime l’erreur de généralisation attendue après l’étiquetage de cette instance notée Ê(Mt+1xi ).
Pour se faire, on utilise un modèle capable d’estimer P (yj |xi), les probabilités des labels yi
sachant l’instance xi.
Après avoir traité tous les exemples candidats, il ne reste plus qu’à choisir ceux qui mi-
nimisent l’erreur de généralisation attendue, et les faire étiqueter par l’oracle. Ce procédé est
répété de manière itérative pour enrichir l’ensemble d’apprentissage. Une vue synthétique de
A. Bondu et al.
cette approche est présentée par l’algorithme (3). Il existe autant de variantes de cette stratégie
qu’on peut imaginer de fonction de coût. Considérons maintenant un cas d’utilisation de ce
type d’approche à titre illustratif.
Étant donnés :
• M un modèle prédictif muni d’un algorithme d’apprentissage L
• Les ensembles Ux et Lx d’exemples non étiquetés et étiquetés
• n le nombre d’exemples d’apprentissage souhaité.
• L’ensemble d’apprentissage T avec ‖T‖ < n
• Y l’ensemble des labels qui peuvent être attribués aux instances de Ux
• Loss : M→ ℜ l’erreur de généralisation du modèle
• Err : Ux ×M → ℜ l’erreur de généralisation attendue pour le modèleM entraîné avec
une instance supplémentaire, T ∪ (xi, f(xi))
Répéter
(A) Entraîner le modèleM grâce à L et T
Pour chaque instance xi ∈ Ux faire
Pour chaque label yj ∈ Y faire
i) Entraîner le modèleMi,j grâce à L et (T ∪ (xi, yj))
ii) Calculer l’erreur de généralisation bE(Mt+1
(xi,yj)
)
Fin Pour
Calculer l’erreur de généralisation attendue
bE(Mt+1xi ) =
P
yj∈Y
bE(Mt+1(xi,yj∗)).P (yj |xi)
Fin Pour
(B) Rechercher l’instance q = argminu∈Ux bE(Mt+1xi )
(C) Retirer q de Ux et demander l’étiquette f(q) à l’oracle.
(D) Ajouter q à Lx et ajouter (q, f(q)) à T
Tant que ‖T‖ < n
Algorithme 3: Apprentissage actif "optimal", de Nicholas Roy 2000
X. Zhu (Zhu et al., 2003) propose d’approcher l’erreur de généralisation par le risque em-
pirique et d’utiliser une fenêtre de Parzen à noyau gaussien (Parzen, 1962) comme modèle
d’apprentissage. Ici, le risque R(M) est défini comme étant la somme des probabilités que
le modèle prenne une mauvaise décision sur l’ensemble d’apprentissage. On note P (yi|ln) la
probabilité réelle d’observer la classe yi pour l’instance ln ∈ Lx. Le risque empirique s’écrit
alors selon l’équation 1, avec 1 la fonction indicatrice égale à 1 si f(ln) 6= yi et égale à 0 sinon.
Le modèle que l’on utilise doit être un estimateur de densité dont la sortie est la probabi-
lité P (yi|ln) d’observer la classe yi conditionnellement à l’instance ln. Sous cette condition,
on peut approximer le risque empirique en adoptant un a priori uniforme sur les P (ln) (voir
équation 2). Le but de cette stratégie est de sélectionner l’instance non étiquetée ui ∈ Ux qui
minimisera le risque à l’itération t + 1. On estime R(M+un) le risque "attendu" après l’éti-
quetage de l’instance un, on se base sur les données étiquetées dont on dispose. On suppose
(dans le cas d’une classification binaire) que f(un) = y1 [resp f(un) = y0] pour estimer
Synthèse sur l’Apprentissage Actif
Rˆ(M+(un,y1)) [resp Rˆ(M+(un,y0))]. L’équation 3 montre comment agréger les estimations
de risque selon les probabilités d’observer chacune des classes.
Pour exprimer la stratégie de réduction du risque sous forme algorithmique, il suffit de rem-
placer l’étape (B) de l’Algorithme 1 par : "Rechercher l’instance q = argminu∈Ux Rˆ(M+un)".
R(M) =
N∑
n=1
∑
yi=0,1
1{f(ln) 6=yi} P (yi|ln)P (ln) avec ln ∈ Lx (1)
Rˆ(M) =
1
N
N∑
n=1
∑
yi=0,1
1{f(ln) 6=yi} Pˆ (yi|ln) (2)
Rˆ(M+un) = Pˆ (y1|un)Rˆ(M
+(un,y1)) + Pˆ (y0|un)Rˆ(M
+(un,y0)) avec un ∈ Ux (3)
Les approches d’apprentissage actif par réduction d’erreur sont des stratégies efficaces
puisqu’elles ont un caractère exhaustif. Elles examinent tous les exemples candidats et toutes
les valeurs d’étiquette possibles. Ces stratégies se différencient entres elles par la fonction de
coût utilisée pour rendre compte de la qualité du modèle. Il est important de noter que ces stra-
tégies sont fortement combinatoires. En effet, le choix de n exemples engendre n.‖Y‖.‖Ux‖
entraînements du modèle.
4 Discussion et perspectives
A l’issue de cet état de l’art plusieurs questions peuvent être soulevées.
l’évaluation des stratégies d’apprentissage actif
La qualité d’une stratégie active est généralement représentée par une courbe mesurant la per-
formance du modèle en fonction du nombre d’exemples étiquetés (voir figure 10). Le critère de
performance (axes des ordonnées de la figure 10) utilisé peut prendre plusieurs formes selon
le problème traité. Ce type de courbe permet uniquement de comparer les stratégies entre elles
de manière ponctuelle, c’est-à-dire pour un nombre d’exemples fixé. Si on observe des courbes
qui se croisent, il est impossible de déterminer si une stratégie est meilleure qu’une autre (sur
la totalité du jeu de données). Une piste pour résoudre ce problème serait de mesurer l’apport
d’une stratégie active par rapport à la stratégie alléatoire et d’intégrer ce nouveau critère sur la
totalité du jeu de données. Nous nous intéressons actuellement à ce sujet.
L’ensemble de test
les méthodes d’apprentissage actif sont généralement utilisées dans le cas où l’acquisition des
données est coûteuse. Dans la pratique, on ne dispose pas d’ensemble de test et il est difficile
d’évaluer la qualité du modèle au cours de son apprentissage.
les critères d’arrêt d’un apprentissage actif
On peut soit définir un nombremaximal d’exemples à étiqueter, soit chercher un critère plus fin
qui n’ajoute des exemples que si les progrès du modèle sont avérés. Des travaux futurs seront
réalisés dans ce sens.
A. Bondu et al.
FIG. 10 – Performance vs nombre d’exemples utilisés
D’une manière générale, les stratégies d’apprentissage actif permettent d’estimer l’utilité
des exemples d’apprentissage. Ces mêmes critères pourraient être utilisés dans le cadre d’un
apprentissage en ligne. L’ensemble d’apprentissage serait constitué des N exemples les plus
"utiles" vus jusqu’à présent (avec N fixé). Cette approche permettrait de considérer des pro-
blèmes d’apprentissage non stationnaires et donc d’effectuer un apprentissage qui s’adapte aux
variations du système observé.
Références
Abe, N. et H. Mamitsuka (1998). Query learning strategies using boosting and bagging. In
ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning,
San Francisco, CA, USA, pp. 1–9. Morgan Kaufmann Publishers Inc.
Castro, R. andWillett, R. et R. Nowak (2005). Faster rate in regression via active learning. In
NIPS (Neural Information Processing Systems), Vancouver.
Chapelle, O. et A. Zien (2005). Semi-supervised classification by low density separation. In
Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics.
Chappelle, O. (2005). Active learning for parzen windows classifier. In AI & Statistics,
Barbados, pp. 49–56.
Cohn, D., R. Caruana, et A. McCallum (2003). Semi-supervised clustering with user feedback.
Technical Report TR2003-1892, Cornell University.
Cohn, D. A., L. Atlas, et R. E. Ladner (1994). Improving generalization with active learning.
Machine Learning 15(2), 201–221.
Cohn, D. A., Z. Ghahramani, et M. I. Jordan (1995). Active learning with statistical models.
In G. Tesauro, D. Touretzky, et T. Leen (Eds.), Advances in Neural Information Processing
Systems, Volume 7, pp. 705–712. The MIT Press.
Dasgupta, S. (2005). Analysis of greedy active learning strategy. In NIPS (Neural Information
Processing Systems), San Diego.
Synthèse sur l’Apprentissage Actif
Dima, C. et M. Hebert (2005). Active learning for outdoor obstacle detection. In Proceedings
of Robotics: Science and Systems, Cambridge.
Ferrière, A. (1922). L’école active. Editions Forums.
Freinet, C. (1964). Les invariants pédagogiques. Bibliothèque de l’école moderne.
Freund, Y., H. S. Seung, E. Shamir, et N. Tishby (1997). Selective sampling using the query
by committee algorithm. Machine Learning 28(2-3), 133–168.
Guestrin, c., A. Krause, et P. Singh (2005). Near-optimal sensor placements in gaussian
processes. In ICML (International Conference on Machine Learning), Bonn.
Harmon, M. (1996). Reinforcement learning: a tutorial. http://eureka1.aa.wpafb.af.mil/
rltutorial/.
Jain, A. K., M. N. Murty, et P. J. Flynn (1999). Data clustering: a review. ACM Computing
Surveys 31(3), 264–323.
Jamy, I., T.-Y. Jen, D. Laurent, G. Loizou, et O. Sy (2005). Extraction de règles d’association
pour la prédiction de valeurs manquantes. Revue Africaine de la Recherche en Informatique
et Mathématique Appliquée ARIMA Spécial CARI04, 103–124.
Lewis, D. et A. Gale (1994). A sequential algorithm for training text classifiers. In W. B. Croft
et C. J. van Rijsbergen (Eds.), Proceedings of SIGIR-94, 17th ACM International Conference
on Research and Development in Information Retrieval, Dublin, pp. 3–12. Springer Verlag,
Heidelberg.
McCallum, A. K. et K. Nigam (1998). Employing EM in pool-based active learning for text
classification. In J. W. Shavlik (Ed.), Proceedings of ICML-98, 15th International Confer-
ence on Machine Learning, Madison, US, pp. 350–358. Morgan Kaufmann Publishers, San
Francisco, US.
Muslea, I. (2002). Active Learning With Multiple View. Phd thesis, University of southern
california.
Parzen, E. (1962). On estimation of a probability density function and mode. Annals of
Mathematical Statistics 33, 1065–1076.
Roy, N. et A. McCallum (2001). Toward optimal active learning through sampling estimation
of error reduction. In Proc. 18th International Conf. on Machine Learning, pp. 441–448.
Morgan Kaufmann, San Francisco, CA.
Seung, H. S., M. Opper, et H. Sompolinsky (1992). Query by committee. In Computational
Learning Theory, pp. 287–294.
Singh, A., R. Nowak, et P. Ramanathan (2006). Active learning for adaptive mobile sensing
networks. In IPSN ’06: Proceedings of the fifth international conference on Information
processing in sensor networks, New York, NY, USA, pp. 60–68. ACM Press.
Thrun, S. B. et K. Möller (1992). Active exploration in dynamic environments. In J. E.
Moody, S. J. Hanson, et R. P. Lippmann (Eds.), Advances in Neural Information Processing
Systems, Volume 4, pp. 531–538. Morgan Kaufmann Publishers, Inc.
Tong, S. et D. Koller (2000). Support vector machine active learning with applications to text
classification. In P. Langley (Ed.), Proceedings of ICML-00, 17th International Conference
on Machine Learning, Stanford, US, pp. 999–1006. Morgan Kaufmann Publishers, San
A. Bondu et al.
Francisco, US.
Zhu, X., J. Lafferty, et Z. Ghahramani (2003). Combining active learning and semi-supervised
learning using gaussian fields and harmonic functions. In ICML (International Conference
on Machine Learning), Washington.
Summary
Machine learning indicates a vast whole of methods and algorithmswhich allow a model to
learn a behavior thanks to examples. Active learning gathers a whole of methods of examples
selection used to build training set for the predictive model. All the strategies aim to use the
less examples as possible and to select the most informative examples. After having formalized
the active learning problem and after having located it in the literature, this article synthesizes
the main approaches of active learning and illustrates them thanks to toyes examples.
