Optimisation du Primal pour les SVM
Trinh-Minh-Tri Doâˆ—, Thierry ArtiÃ¨resâˆ—
âˆ—LIP6, UniversitÃ© Pierre et Marie Curie
104 avenue du PrÃ©sident Kennedy, Paris, France
{Trinh-Minh-Tri.Do, Thierry.Artieres}@lip6.fr
RÃ©sumÃ©. Lâ€™apprentissage de SVM par optimisation directe du primal est trÃ¨s
Ã©tudiÃ© depuis quelques temps car il ouvre de nouvelles perspectives notamment
pour le traitement de donnÃ©es structurÃ©es. Nous proposons un nouvel algorithme
de ce type qui combine de faÃ§on originale un certain nombre de techniques et
idÃ©es comme la mÃ©thode du sous-gradient, lâ€™optimisation de fonctions continues
non partout diffÃ©rentiables, et une heuristique de shrinking.
1 Introduction
Les Machines Ã  Vecteurs de Support (SVM) sont une mÃ©thode trÃ¨s populaire dâ€™appren-
tissage supervisÃ© pour la classification et la rÃ©gression. Dans sa forme la plus simple pour la
classification bi-classes, cette mÃ©thode est basÃ©e sur un classificateur linÃ©aire sÃ©parant deux
ensembles de points par un hyperplan. Lâ€™idÃ©e originale est de trouver un hyperplan sÃ©parant
"au mieux" les points par la maximisation de la marge entre lâ€™hyperplan sÃ©parateur et les points
dans la base dâ€™apprentissage. Cette formulation conduit Ã  un problÃ¨me dâ€™optimisation dâ€™une
fonction convexe sous des contraintes linÃ©aires. RÃ©cemment des extensions de cette technique
de base et de lâ€™approche de maximisation de la marge ont Ã©tÃ© proposÃ©es pour le traitement de
donnÃ©es structurÃ©es comme les sÃ©quences, les arbres etc (Tsochantaridis et al., 2004).
La mÃ©thode originale de Vladimir Vapnik pour rÃ©soudre le problÃ¨me dâ€™optimisation avec
contraintes des SVMs consiste Ã  introduire des multiplicateurs de Lagrange pour chaque
contrainte, et dâ€™optimiser le problÃ¨me dual Ã©quivalent. Cet algorithme est coÃ»teux en temps
et en mÃ©moire. Par exemple, lâ€™espace mÃ©moire nÃ©cessaire (la matrice noyau est de taille N
au carrÃ©, si N est le nombre dâ€™exemples). Ces caractÃ©ristiques de complexitÃ© rendent difficile
lâ€™emploi de machines Ã  vecteurs support et plus gÃ©nÃ©ralement de mÃ©thodes de maximisation de
la marge dans certaines situations, lorsque lâ€™on traite des donnÃ©es structurÃ©es ou bien lorsque
lâ€™on dispose de trÃ¨s grandes quantitÃ©s de donnÃ©es dâ€™apprentissage. Plusieurs voies ont Ã©tÃ© sui-
vies pour dÃ©passer les problÃ¨mes posÃ©s par lâ€™optimisation dans ce cadre.
Certains travaux ont portÃ© sur lâ€™optimisation efficace du dual, par le contrÃ´le du nombre
de contraintes actives (Joachims, 2006), ou par la dÃ©composition du problÃ¨me dâ€™apprentissage
(Osuna et al., 1997). Dans ce dernier cas, lâ€™algorithme SMO ou SVMLight par exemple, on ne
sâ€™intÃ©resse Ã  une itÃ©ration donnÃ©e quâ€™Ã  un nombre limitÃ© de variables actives.
Des travaux plus rÃ©cents ont portÃ© sur lâ€™optimisation directe de la forme primale par lâ€™usage
de la fonction hinge(z)=max(0,z). Cela permet de se ramener Ã  un problÃ¨me dâ€™optimisation
sans contraintes oÃ¹ la fonction objectif est convexe. La difficultÃ© de ces derniÃ¨res approches
Optimisation du Primal pour les SVM
vient du fait que la fonction hinge nâ€™est pas dÃ©rivable en 0. Divers travaux ont alors proposÃ©
dâ€™utiliser une version dÃ©rivable partout de cette fonction hinge par lissage ou bien en utilisant
un coÃ»t quadratique car dans ce cas des mÃ©thodes dâ€™optimisation standard peuvent Ãªtre ap-
pliquÃ©es. Par exemple, (Chapelle, 2007) a montrÃ© quâ€™il Ã©tait possible dâ€™utiliser la mÃ©thode de
Newton ou bien du gradient conjuguÃ©.
Une autre approche pour optimiser le primal consiste Ã  utiliser la mÃ©thode du sous-gradient
directement sur le problÃ¨me dâ€™optimisation de la fonction objectif non diffÃ©rentiable (Zhang,
2004). Lâ€™avantage de cette approche est sa simplicitÃ©, mais la vitesse de convergence est trÃ¨s
dÃ©pendante du rÃ©glage du pas de gradient. Une exception est lâ€™algorithme Pegasos (Shalev-
Shwartz et al., 2007) qui est le seul algorithme de ce type ne nÃ©cessitant pas le rÃ©glage dâ€™un
hyperparamÃ¨tre pour le pas de gradient.
Dans ce travail, nous nous plaÃ§ons dans le cadre de lâ€™optimisation de fonction continue
non partout dÃ©rivable. Lâ€™idÃ©e principale est que la minimisation du primal peut Ãªtre attaquÃ©e
comme un problÃ¨me minimax sur un ensemble de fonctions quadratiques convexes dÃ©finies
sur des sous-espaces de lâ€™espace des paramÃ¨tres. Nous montrons que la fonction objectif nâ€™est
pas dÃ©rivable sur les frontiÃ¨res entre ces sous-espaces de dÃ©finition et que ces frontiÃ¨res cor-
respondent Ã  des hyperplans associÃ©s Ã  chaque exemple dâ€™apprentissage. En analysant ces hy-
perplans dans lâ€™espace des paramÃ¨tres, nous dÃ©crivons une mÃ©thode efficace pour calculer la
direction de plus grande pente et estimer le pas optimal. En exploitant ces rÃ©sultats, nous pro-
posons un nouvel algorithme dâ€™optimisation qui se compare favorablement aux algorithmes de
type Pegasos en mode batch et en mode on-line.
Nous dÃ©crivons tout dâ€™abord le cadre dâ€™optimisation dans lequel nous nous plaÃ§ons et les
outils que nous allons utiliser. Ensuite nous dÃ©crivons notre algorithme en dÃ©tails puis le vali-
dons expÃ©rimentalement en le comparant Ã  des algorithmes de rÃ©fÃ©rence.
2 PrÃ©liminaires
Nous formalisons tout dâ€™abord le problÃ¨me que nous attaquons ici et qui consiste Ã  optimi-
ser une fonction objectif non partout diffÃ©rentiable. Nous rappelons ensuite quelques rÃ©sultats
sur ce type de fonctions et dÃ©crivons briÃ¨vement une mÃ©thode classique pour leur optimisation.
2.1 Formalisation
ConsidÃ©rons un problÃ¨me de classification de donnÃ©es dâ€™entrÃ©e x en 2 classes y âˆˆ
{âˆ’1,+1}. ConsidÃ©rons une base dâ€™apprentissage BA = {(x1, y1), ..., (xN , yN )} de N
exemples, oÃ¹ xi âˆˆ Rd et yi âˆˆ {âˆ’1,+1}. Nous nous intÃ©ressons Ã  lâ€™apprentissage du clas-
sifieur linÃ©aire : hw(x) = sign(ã€ˆx,wã€‰) oÃ¹ w âˆˆ Rd est lâ€™ensemble des paramÃ¨tres du classi-
fieur Ã  apprendre. Notons que w divise Rd en deux sous-espaces, la frontiÃ¨re est un hyperplan
dâ€™Ã©quation Hw : ã€ˆx,wã€‰ = 0. Lâ€™idÃ©e principale de lâ€™apprentissage vaste marge est de trouver
un hyperplan sÃ©parant "au mieux" les points par maximisation de la marge entre lâ€™hyperplan
sÃ©parateur et les points de la base dâ€™apprentissage. Cette formulation conduit Ã  un problÃ¨me
dâ€™optimisation dâ€™une fonction convexe sous des contraintes linÃ©aires :
T.M.T DO et al.
minw Î»2 â€–wâ€–2 + 1N
âˆ‘
i=1..N Î¾i
sous les contraintes yi ã€ˆxi, wã€‰ â‰¥ 1âˆ’ Î¾i âˆ€i
Î¾i â‰¥ 0 âˆ€i
(1)
oÃ¹ Î» est un hyper-paramÃ¨tre de lâ€™algorithme qui permet de rÃ©gler lâ€™importance des deux
termes de la fonction objectif. Les Î¾i sont des variables non-nÃ©gatives qui reprÃ©sentent des
pÃ©nalitÃ©s pour les cas oÃ¹ la contrainte de marge nâ€™est pas respectÃ©e pour lâ€™exemple xi.
En introduisant la fonction hinge(z) = max(0, z), on obtient un problÃ¨me Ã©quivalent sans
contraintes :
min
w
f(w) = min
w
Î»
2
â€–wâ€–2 + 1
N
âˆ‘
i=1..N
max(0, 1âˆ’ yi ã€ˆxi, wã€‰) (2)
Notons que cette fonction objectif est quadratique par morceau et convexe mais quâ€™elle nâ€™est
pas diffÃ©rentiable (par rapport Ã  w) en certains points, sur les hyperplansHi : 1âˆ’ yi ã€ˆxi, wã€‰ =
0. Il y a un hyperplan par exemple dâ€™apprentissage. En des points qui nâ€™appartiennent Ã  aucun
de ces hyperplans la fonction objectif est localement quadratique.
Chaque hyperplan Hi divise lâ€™espace des w en deux sous-espaces : Hi+ : {w|1 âˆ’
yi ã€ˆxi, wã€‰ â‰¥ 0} et Hiâˆ’ : {w|1âˆ’ yi ã€ˆxi, wã€‰ â‰¤ 0}. Ces N hyperplans divisent ainsi lâ€™espace en
hypercubes Ck dÃ©finis par : Ck =
â‹‚
i=1..N H
i
Ïƒki
oÃ¹ Ïƒki âˆˆ {+,âˆ’}. Dans chaque hypercube
Ck, la fonction objectif a une forme quadratique :
f(w) =
Î»
2
â€–wâ€–2 + 1
N
âˆ‘
iâˆˆIk
(1âˆ’ yi ã€ˆxi, wã€‰) âˆ€w âˆˆ Ck (3)
oÃ¹ Ik = {i|Ïƒki = +} est lâ€™ensemble des indices des exemples qui violent la contrainte de la
marge pour w âˆˆ Ck. Par construction cet ensemble est identique pour tous les w dâ€™un mÃªme
hypercube Ck, câ€™est pourquoi nous ne marquons pas la dÃ©pendance de Ik Ã  w.
Avant de prÃ©senter notre algorithme en dÃ©tail, nous commenÃ§ons par quelques rÃ©sultats
thÃ©oriques sur lâ€™optimisation de fonctions non-diffÃ©rentiables.
2.2 GÃ©nÃ©ralitÃ©s sur lâ€™optimisation de fonctions non diffÃ©rentiables
Dans cette section, nous prÃ©sentons quelque rÃ©sultats concernant lâ€™optimisation et lâ€™ana-
lyse des fonctions convexes non-diffÃ©rentiables. Ces rÃ©sultats, et leurs dÃ©rivations, peuvent
Ãªtre trouvÃ©s dans (Bertsekas et al., 2003; Demyanov et Vasilev, 1985). Nous introduisons tout
dâ€™abord la dÃ©finition du sous-gradient et de la sous-diffÃ©rentielle dâ€™une fonction convexe.
Soit f : Rd â†’ R une fonction convexe, un vecteur d âˆˆ Rd est appelÃ© sous-gradient au
point x0 si :
f(x) â‰¥ f(x0) + ã€ˆxâˆ’ x0, dã€‰ âˆ€x âˆˆ Rd (4)
Lâ€™ensemble de tous les sous-gradients en x0 est appelÃ© la sous-diffÃ©rentielle en x0, et est
notÃ© par âˆ‚f(x0). La sous-diffÃ©rentielle est un ensemble compact, non vide et convexe.
Optimisation du Primal pour les SVM
ThÃ©orÃ¨me 1 (Demyanov and Vasilev) : Une condition nÃ©cessaire pour une fonction conti-
nue non partout dÃ©rivable, Ã©ventuellement non convexe, f(x) : Rd â†’ R dâ€™atteindre un mi-
nimum en xâˆ— est que 0 âˆˆ âˆ‚f(xâˆ—). Pour une fonction convexe, la condition est Ã©galement
suffisante. Si 0 /âˆˆ âˆ‚f(xâˆ—) alors la direction âˆ† = âˆ’argmindâˆˆâˆ‚f(x) â€–dâ€– est la direction du
sous-gradient de plus grande pente.
Proposition 1 : Soit {fj : Rd â†’ R, j = 1, ...,m} un ensemble de fonctions convexes et
f =
âˆ‘
j=1..m fj , alors
âˆ‚f(x) =
ï£±ï£²ï£³u = âˆ‘
j=1..m
dj
âˆ£âˆ£âˆ£(d1, .., dm) âˆˆ âˆ‚f1(x)Ã— âˆ‚f2(x)Ã— ...Ã— âˆ‚fm(x)
ï£¼ï£½ï£¾ (5)
ThÃ©orÃ¨me 2 (Danskinâ€™s) : Soit {fj : Rd â†’ R, j = 1, ...,m} un ensemble de fonctions
convexes diffÃ©rentiables, alors la sous-diffÃ©rentielle de f = maxj=1..m fj est
âˆ‚f(x) = conv {âˆ‡fj(x)|j âˆˆ I(x)} (6)
oÃ¹ I(x) = {i|fi(x) = maxj fj(x)}, âˆ‡fj(x) est le gradient de fj(x) en x, et conv(.) signifie
enveloppe convexe.
2.3 MÃ©thode du sous-gradient
Un faÃ§on dâ€™optimiser le problÃ¨me de lâ€™Ã©quation (2) est dâ€™utiliser la mÃ©thode du sous-
gradient, qui converge vers le minimum global (Bertsekas et al., 2003). Nous comparerons
notre algorithme Ã  une mÃ©thode de rÃ©fÃ©rence de ce type (Shalev-Shwartz et al., 2007). A chaque
itÃ©ration, le nouveau vecteur de paramÃ¨tres wt est calculÃ© par :
wt+1 = wt âˆ’ Î·tâˆ‡t (7)
oÃ¹ Î·t est le pas de sous-gradient etâˆ‡t âˆˆ âˆ‚f(wt) est un sous-gradient quelconque de la fonction
f Ã  wt.
3 Descente de sous-gradient pour lâ€™optimisation du primal
Dans ce travail, nous proposons dâ€™appliquer un algorithme du type descente de sous-
gradient. La diffÃ©rence avec la mÃ©thode du sous-gradient est que nous choisissons la direction
du sous-gradient selon la plus grande pente (plutÃ´t que de prendre un sous-gradient quelconque
dans la sous-diffÃ©rentielle), et que nous proposons une mÃ©thode optimale pour dÃ©terminer le
pas de gradient. Lâ€™algorithme est rÃ©sumÃ© par le pseudo code dÃ©crit ce-dessous.
Cet algorithme a comme nous le verrons une complexitÃ© linÃ©aire dans le nombre
dâ€™exemples dâ€™apprentissage Ã  chaque itÃ©ration. En effet, il passe en revue tous les exemples
dâ€™apprentissage pour dÃ©terminer la direction de descente de plus grande pente et pour ensuite
calculer le pas de gradient optimal dans cette direction. Nous dÃ©crivons maintenant ces Ã©tapes
de calcul de la direction de plus grande pente du gradient et de calcul du pas de gradient opti-
mal.
T.M.T DO et al.
EntrÃ©es : BA = {(x1, y1), ..., (xN , yN )}
Sorties : w
Initialization : w1 = 0; t = 1 ;
tant que vrai faire
Calculer la direction du sous-gradient de plus grande penteâˆ†t;
si â€–âˆ†tâ€– = 0 alors return wt; ;
Calculer le pas de gradient optimal Î·t ;
Mise Ã  jour : wt+1 = wt + Î·tâˆ†t ;
t = t+ 1;
fin
3.1 Direction du sous-gradient de plus grande pente
Rappelons que nous souhaitons minimiser la fonction objectif sous sa forme primale don-
nÃ©e dans lâ€™Eq. (2). Nous cherchons dans un premier temps Ã  dÃ©terminer la sous-diffÃ©rentielle de
f(w) en un "point" w. Pour cela nous commenÃ§ons par nous intÃ©resser Ã  la sous-diffÃ©rentielle
de la fonction Ã©lÃ©mentaire max(0, 1âˆ’yiã€ˆxi,wã€‰N ). Cette fonction est diffÃ©rentiable sauf pour les
points w tel que yi ã€ˆxi, wã€‰ = 1. Pour un point de ce type on obtient en appliquant le thÃ©orÃ¨me
2 :
âˆ‚
(
max(0,
1âˆ’ yi ã€ˆxi, wã€‰
N
)
)
= conv
{
0,âˆ’yixi
N
}
=
{
âˆ’Î²i yixi
N
âˆ£âˆ£âˆ£Î²i âˆˆ [0, 1]} (8)
Par ailleurs en un point w tel que yi ã€ˆxi, wã€‰ 6= 1 la fonction est dÃ©rivable et sa sous-
diffÃ©rentielle est rÃ©duite Ã  sa dÃ©rivÃ©e. La sous-diffÃ©rentielle de la fonction Ã©lÃ©mentaire sâ€™ex-
prime donc suivant les cas par :
âˆ‚
(
max(0,
1âˆ’ yi ã€ˆxi, wã€‰
N
)
)
=
ï£±ï£²ï£³ 0 si yi ã€ˆxi, wã€‰ > 1{âˆ’Î²i yixiN /Î²i âˆˆ [0, 1]} si yi ã€ˆxi, wã€‰ = 1âˆ’yixiN si yi ã€ˆxi, wã€‰ < 1 (9)
Finalement, en utilisant la proposition 1 de la section prÃ©cÃ©dente, on obtient :
âˆ‚f(w) =
ï£±ï£²ï£³Î»w + âˆ‘
i:yiã€ˆxi,wã€‰<1
âˆ’yixi
N
+
âˆ‘
i:yiã€ˆxi,wã€‰=1
(âˆ’Î²i yixi
N
)
âˆ£âˆ£âˆ£ Î²i âˆˆ [0, 1]âˆ€i
ï£¼ï£½ï£¾ (10)
Il est intÃ©ressant de noter que cette formulation est proche de celle utilisÃ©e dans (Eizin-
ger et Plach, 2003). Cela vient de la prÃ©sence de fonctions hinge dans la formalisation de
lâ€™apprentisage du perceptron, qui peut sâ€™Ã©crire comme la minimisation dâ€™une fonction linÃ©aire
par morceaux, en fait la somme des fonctions hinge(âˆ’yi ã€ˆxi, wã€‰). A noter que dans ce cas la
fonction objectif est non strictement convexe et quâ€™il peut exister, dans le cas sÃ©parable, une in-
finitÃ© de solutions. Dans notre formalisation la fonction objectif comporte un terme quadratique
â€–wâ€–2 et des termes hinge(1 âˆ’ yi ã€ˆxi, wã€‰), ce qui rend le problÃ¨me strictement convexe. Par
ailleurs, notre fonction objectif vise Ã  maximiser la marge ce qui doit conduire Ã  de meilleures
propriÃ©tÃ©s de gÃ©nÃ©ralisation.
Optimisation du Primal pour les SVM
La recherche de la direction du sous-gradient de plus grande pente peut Ãªtre vu comme le
problÃ¨me des moindres carrÃ©s suivant :
Î²âˆ— = argminÎ²=(Î²i1 ,Î²i2 ,...,Î²iK )âˆˆ[0,1]K â€–d0 âˆ’
âˆ‘
iâˆˆ{i1,...,iK}
Î²iyixiâ€– (11)
oÃ¹ nous notons {i1, ..., iK} = {i|yi ã€ˆxi, wã€‰ = 1} et en posant d0 = Î»w +âˆ‘
i:yiã€ˆxi,wã€‰<1(âˆ’yixiN ) qui est indÃ©pendant des paramÃ¨tres dâ€™optimisation Î²i. Ce problÃ¨me peut
Ãªtre rÃ©solu par des procÃ©dures standard (voir (Boyd et Vandenberghe, 2004)). Une fois Î²âˆ— cal-
culÃ© on obtient la direction du gradient de plus grande pente par la direction donnÃ©e dans le
ThÃ©orÃ¨me 1 avec les valeurs de Î²âˆ— donnÃ©es dans lâ€™Eq. (11).
Notons pour terminer que si K = 0 alors la fonction est diffÃ©rentiable en w et la direction
du gradient de plus grande pente est simplementâˆ†t = âˆ’d0.
3.2 Pas de gradient optimal
FIG. 1 â€“ Lâ€™espace des w.
Une fois la direction de recherche âˆ†t dÃ©terminÃ©e comme dÃ©crit Ã  la section prÃ©cÃ©dente,
nous proposons ici une mÃ©thode pour dÃ©terminer le pas de gradient de faÃ§on optimale. Cela
revient Ã  rÃ©soudre le problÃ¨me dâ€™optimisation unidimensionnel suivant :
Î·t = argmin
Î·
f(wt + Î·âˆ†t) (12)
Nous nous intÃ©ressons Ã  la droite Dt = {w = wt + Î·âˆ†t|Î· âˆˆ R} et au comportement
de f(w) sur cette droite. Si lâ€™on "avance" sur la droite Dt dans la direction de âˆ†t, câ€™est Ã 
dire quâ€™on examine les w = wt + Î·âˆ†t pour Î· croissant, alors w va successivement croiser
des hyperplans Hi frontiÃ¨res entre hypercubes et traverser dâ€™autres hypercubes. Au passage,
la fonction f(w) change dâ€™une fonction quadratique dâ€™un hypercube Ã  une autre (voir figure
1). La fonction de Î· que nous cherchons Ã  optimiser, g(Î·) = f(wt + Î·âˆ†t) est une fonction
quadratique par morceaux (voir figure 2). Les points non-diffÃ©rentiables correspondent aux
intersections entre wt + Î·âˆ† et les hyperplan Hi sÃ©parateurs.
T.M.T DO et al.
(a) (b)
FIG. 2 â€“ Recherche du pas de gradient optimal.
On peut caractÃ©riser lâ€™intersection, si elle existe, entre la droite Dt et un hyperplan Hi :
1 âˆ’ yi ã€ˆxi, wã€‰ = 0 par une valeur particuliÃ¨re de Î·, que nous notons Î·i. Elle est dÃ©terminÃ©e
par :
1âˆ’ yi
âŒ©
xi, (wt + Î·iâˆ†t)
âŒª
= 0
â‡â‡’ yi ã€ˆxi, wtã€‰+ Î·iyi ã€ˆxi,âˆ†tã€‰ = 1
â‡â‡’ Î·i = 1âˆ’yiã€ˆxi,wtã€‰yiã€ˆxi,âˆ†tã€‰
(13)
Une valeur positive de Î·i signifie que lâ€™hyperplanHi est "devant" wt (dans la directionâˆ†t) et
une valeur nÃ©gative de Î·i signifie le contraire.
Imaginons, sans perte de gÃ©nÃ©ralitÃ©, que la droite dÃ©crite par w = wt + Î·âˆ†t en faisant
croÃ®tre Î· de zÃ©ro vers lâ€™infini traverse successivement les hyperplansH1,H2,H3... (voir Figure
2a). Sur le segment
[
Î·n, Î·n+1
]
, g(Î·) vaut :
gn(Î·) =
Î»
2
â€–wt + Î·âˆ†tâ€–2 + 1
N
âˆ‘
iâˆˆIk(n)
(1âˆ’ yi ã€ˆxi, (wt + Î·âˆ†t)ã€‰) (14)
oÃ¹ k(n) est lâ€™indice de lâ€™hypercube correspondant au nieme segment. Sa dÃ©rivÃ©e par rapport Ã 
la variable Î· est :
âˆ‚gn(Î·)
âˆ‚Î· = Î» ã€ˆ(wt + Î·âˆ†t),âˆ†tã€‰+ 1N
âˆ‘
iâˆˆIk(n)(âˆ’yi ã€ˆxi,âˆ†tã€‰)
= Î» ã€ˆwt,âˆ†tã€‰+ Î»Î· ã€ˆâˆ†,âˆ†tã€‰ âˆ’
âˆ‘
iâˆˆIk(n)
yiã€ˆxi,âˆ†tã€‰
N
(15)
A lâ€™optimum cette dÃ©rivÃ©e, si elle existe, est nulle. Dans ce cas :
Î·nopt =
âˆ‘
iâˆˆIk(n)
yiã€ˆxi,âˆ†tã€‰
N âˆ’ Î» ã€ˆwt,âˆ†tã€‰
Î» ã€ˆâˆ†t,âˆ†tã€‰ (16)
Si Î·nopt appartient effectivement au n
ieme segment, et que g(Î·) est dÃ©rivable en cette valeur
alors le pas optimal vaut Î·nopt. Cependant, g(Î·) peut ne pas Ãªtre dÃ©rivable Ã  lâ€™optimum (cf.
figure 2b), cela signifie que lâ€™optimum correspond Ã  un Î· sur une frontiÃ¨re entre segments (i.e.
le w optimal est sur une frontiÃ¨re, un hyperplan sÃ©parateur, entre hypercubes). Dans ce cas,
pour tout n lâ€™optimum de gn(Î·) est "en dehors" du nieme segment. Une mÃ©thode simple pour
Optimisation du Primal pour les SVM
vÃ©rifier si le nieme point dâ€™intersection Î·n est la solution est de calculer Î·nâˆ’1opt et Î·
n
opt, si Î·
n
opt â‰¤
Î·n â‰¤ Î·nâˆ’1opt alors Î·n est la solution, sinon Î·n nâ€™est pas la solution. Au final, notre algorithme
pour trouver le pas de gradient optimal est dÃ©crit par le pseudo code ci-dessous. Dans ce code,
nous notons L le nombre dâ€™hyperplans Ã  envisager (a priori inconnu mais nÃ©cessairement L â‰¤
N ), câ€™est Ã  dire ceux correspondant Ã  des exemples pour lesquels il existe une intersection.
EntrÃ©es : BA,wt,âˆ†t
Sorties : Î·t
Initialization;
Estimer I0 â† {i|yi ã€ˆxi, wã€‰ â‰¤ 1âˆ€w âˆˆ [wt, wt + Î·1âˆ†]};
Estimer les Î· corespondant aux intersections de Dt avec les hyperplans Hi, sÃ©lectioner
des Î· non-negatives et les trier dans lâ€™ordre croissant : Î·1, Î·2, Î·3..., Î·L;
On note i1, i2, ..., iL les indices des hyperplans (i.e. exemples dâ€™apprentissage)
correspondents;
n = 0; Î·0 = 0;
tant que vrai faire
Î·nopt =
P
iâˆˆIk(n)
yiã€ˆxi,âˆ†tã€‰
N âˆ’Î»ã€ˆwt,âˆ†tã€‰
Î»ã€ˆâˆ†t,âˆ†tã€‰ ;
si (Î·nopt â‰¤ Î·n) alors return Î·n ;
sinon si (n = L) or (Î·n â‰¤ Î·nopt â‰¤ Î·n+1) alors return Î·nopt;
sinon In+1 = In xor {in+1};n = n+ 1
fin
4 Shrinking et ComplexitÃ©
Lâ€™algorithme itÃ©ratif prÃ©sentÃ© dans la section prÃ©cÃ©dente se compare avantageusement Ã 
dâ€™autres algorithmes dâ€™optimisation batch proposÃ©s rÃ©cemment, en termes de performance et
de complexitÃ© algorithmique. Nous proposons ici une variante beaucoup plus rapide de cet
algorithme. Rappelons que lâ€™algorithme passe en revue tous les exemples dâ€™apprentissage (ou
plutÃ´t les hyperplans correspondants) pour dÃ©terminer la direction de descente de plus grande
pente et pour ensuite calculer le pas de gradient optimal dans cette direction. Or en pratique,
il y a le plus souvent trÃ¨s peu dâ€™exemples qui contribuent Ã  lâ€™estimation de la direction de
recherche et Ã  lâ€™estimation du pas de gradient optimal. Ce sont des exemples que nous appelons
des exemples actifs, les autres Ã©tant passifs. On peut donc espÃ©rer casser la complexitÃ© de
lâ€™algorithme en rÃ©duisant le problÃ¨me dâ€™estimation Ã  chaque itÃ©ration en ne considÃ©rant que
les exemples actifs. Nous proposons ici dâ€™utiliser une mÃ©thode de shrinking similaire Ã  ce qui
est utilisÃ© dans les techniques dâ€™optimisation du dual pour sÃ©lectionner Ã  une itÃ©ration donnÃ©e
un nombre restreint de variables actives (Joachims, 1999).
Pour minimiser la fonction objectif de lâ€™Eq. (2), nous nâ€™allons considÃ©rer Ã  chaque itÃ©ration
quâ€™un nombre limitÃ© dâ€™hyperplans actifs, en cherchant Ã  optimiser la fonction :
f(w) =
Î»
2
â€–wâ€–2 + 1
N
âˆ‘
iâˆˆLI
(1âˆ’ yi ã€ˆxi, wã€‰) + 1
N
âˆ‘
iâˆˆLA
max(0, 1âˆ’ yi ã€ˆxi, wã€‰) (17)
T.M.T DO et al.
oÃ¹ LA reprÃ©sente la liste des exemples actifs et LI reprÃ©sente la liste des exemples inactifs
violant la marge. Lâ€™itÃ©ration dâ€™optimisation est rÃ©alisÃ©e comme dÃ©crit Ã  la Section 3, la seule
diffÃ©rence venant du fait que certains hyperplans ne sont pas considÃ©rÃ©s (les termes corres-
pondants sont rajoutÃ©s Ã  d0, cf. Eq. (11)) et que lâ€™on cherche une solution wt+1 dans lâ€™espace
dÃ©limitÃ© par les hyperplans actifs. La procÃ©dure de sÃ©lection des exemples/hyperplans est heu-
ristique. A lâ€™initialisation tous les exemples sont actifs. A une itÃ©ration t on considÃ¨re comme
actifs lesKt hyperplans les plus proches de la solution courante. Afin dâ€™obtenir une optimisa-
tion trÃ¨s rapide, on rÃ©duit de moitiÃ© le nombreKt dâ€™hyperplans actifs Ã  chaque itÃ©ration.
Pour garantir que la solution est correcte vis Ã  vis des hyperplans sÃ©lectionnÃ©s, on restreint
de plus lâ€™espace de recherche Ã  une boule B(wt, Rt) centrÃ©e autour de la solution courante
et de rayon Rt, qui est calculÃ©e comme la distance maximale entre la solution courante et
les hyperplans actifs sÃ©lectionnÃ©s. Il sâ€™agit dâ€™une heuristique qui permet dâ€™Ã©viter la plupart du
temps de traverser un hyperplan inactif lors de la mise Ã  jour de w. Si câ€™est le cas la solution
trouvÃ©e est identique Ã  celle que lâ€™on trouverait avec tous les hyperplans actifs. Si ce nâ€™est pas
le cas on nâ€™a plus cette garantie. Câ€™est la raison pour laquelle nous avons choisi, rÃ©guliÃ¨rement,
de tout rÃ©initialiser en rÃ©activant tous les hyperplans, soit lorsque le nombre Kt est infÃ©rieur Ã 
un seuil (e.g. 10) soit lorsque le rayon Rt tombe Ã  zÃ©ro. Cette procÃ©dure permet de garantir la
convergence comme pour lâ€™algorithme originel de la Section 3.
EntrÃ©es : BA
Sorties : w
Initialisation t = 1;LA = {Hi|i = 1..N};LI = âˆ…;Rt = inf
tant que vrai faire
1. Estimer la direction de recherche optimaleâˆ†t en considÃ©rant la fonction objectif
de lâ€™Eq. (11), pour les hyperplans actifs de LA
2. si |âˆ†t| = 0 alors
ArrÃªt
fin
3. Estimer le pas optimal Î·t pour la fonction objectif de lâ€™Eq. (12), dans la direction
âˆ†t et pour les hyperplans actifs de LA
4. Contraindre la nouvelle solution Ã  appartenir Ã  B(wt, Rt) : Î·t = min(Î·t,
Rtâˆ’1
â€–âˆ†tâ€– )
5. Mise Ã  jour de w : wt+1 = wt + Î·tâˆ†t;
6. t = t+ 1;Mise Ã  jour LA, LI , et Rt
7. si (|LA| < 10 or (Rt = 0)) alors
LA = {Hi|i = 1..N};LI = âˆ…;Rt = inf
fin
fin
ComplexitÃ© La complexitÃ© de chaque itÃ©ration de lâ€™algorithme sans shrinking est en
O(2Nd). En effet, les opÃ©rations coÃ»teuses Ã  chaque itÃ©ration sont lâ€™estimation de (yi ã€ˆxi, wã€‰)
et de yi ã€ˆxi,âˆ†tã€‰. Il y a 2 Ã— N calculs de ce type. En utilisant le shrinking, avec le mÃªme rai-
sonnement la complexitÃ© dâ€™une itÃ©ration est en 2 Ã— O(|LA|d). Lâ€™algorithme est une itÃ©ration
de plusieurs cycles. Dans chaque cycle, on part de N hyperplans actifs, puis on divise par 2 le
Optimisation du Primal pour les SVM
nombre dâ€™hyperplans actifs Ã  chaque itÃ©ration jusquâ€™Ã  arriver Ã  un nombre faible (e.g. <10). La
complexitÃ© dâ€™un cycle est donc en 2Ã—O(Nd+ (N/2)d+ (N/4)d+ ...) = 4Ã—O(Nd).
5 ExpÃ©riences
Nous dÃ©crivons ici des rÃ©sultats expÃ©rimentaux obtenus en classification dâ€™images de
chiffres manuscrits sur la base MNIST1. Elle contient 60000 exemples dâ€™apprentissage et
10000 exemples de test, les images sont en dimension 28x28. Nous avons prÃ©traitÃ© les donnÃ©es
via une analyse en composantes principales (ACP) afin de rÃ©duire la dimension des donnÃ©es
Ã  50 dimensions (on ne garde que les 50 composantes des images sur les 50 axes principaux
dâ€™inertie). Il sâ€™agit dâ€™un prÃ©traitement standard sur ces donnÃ©es, dÃ©crit par exemple dans (Le-
Cun et al., 1998).
Nous avons comparÃ© notre algorithme notÃ© HyperPass (Hyperplane Passenger) avec lâ€™algo-
rithme Pegasos, basÃ© sur la mÃ©thode de sous-gradient. Ce dernier sâ€™est montrÃ© expÃ©rimentale-
ment trÃ¨s efficace par rapport aux mÃ©thodes dâ€™optimisation du dual pour les bases de donnÃ©es
de grand dimension. Tout dâ€™abord nous comparons la vitesse de convergence de Pegasos en
mode batch et de notre algorithme sans la stratÃ©gie de shrinking. La figure 3 montre lâ€™Ã©volu-
tion du primal en fonction du nombre de passages sur la base de donnÃ©es. On voit que notre
algorithme HyperPass converge beaucoup plus rapidement que Pegasos. Notons toutefois que
chaque passage de la base de donnÃ©es correspond Ã  une itÃ©ration de HyperPass avec complexitÃ©
2Ã—O(Nd), tandis que la complexitÃ© dâ€™une itÃ©ration de Pegasos est en O(Nd). On voit Ã©gale-
ment que la courbe de Pegasos est large car la valeur primal oscille beaucoup entre itÃ©rations
successives.
FIG. 3 â€“ MNIST â€™0â€™ vs all - Primal objectif.
Nous avons Ã©galement Ã©tudiÃ© sur le mÃªme problÃ¨me la version HyperPass avec shrinking
et observÃ© que cet algorithme converge aprÃ¨s quelques cycles dâ€™itÃ©rations (lâ€™algorithme se ter-
mine au 7ieme cycle, ce qui correspond Ã  une complexitÃ© de 28 Ã— O(Nd)). Nous comparons
les solutions de HyperPass Shrinking et de la version on-line de Pegasos (beaucoup plus ra-
pide que la version batch) pour une mÃªme complexitÃ© algorithmique (tableau 1). Le paramÃ¨tre
K de Pegasos reprÃ©sente le nombre dâ€™exemples utilisÃ©s pour faire un pas de sous-gradient,
1http ://yann.lecun.com/exdb/mnist/index.html
T.M.T DO et al.
K = N correspond au mode batch. On voit dans ce tableau que que la solution de Hyper-
Pass est meilleure que celles de Pegasos en termes de valeur du primal et de taux dâ€™erreur de
classification en test.
MÃ©thodes nombre dâ€™itÃ©rations complexitÃ© valeur du primal erreur de test (%)
HyperPass Shrinking 7 cycles 28Ã—O(Nd) 0.5446187 18.40
Pegasos K=N 28 28Ã—O(Nd) 5.0695504 33.97
Pegasos K=100 28Ã—N/10 28Ã—O(Nd) 0.5464795 18.87
Pegasos K=10 28Ã—N/10 28Ã—O(Nd) 0.5464416 18.65
TAB. 1 â€“ Comparaison de HyperPass Shrinking et de Pegasos avec une mÃªme complexitÃ© en
temps.
Enfin, nous comparons la solution de HyperPass Shrinking Ã  celles obtenues avec lâ€™algo-
rithme Pegasos en le faisant tourner jusquâ€™Ã  arriver Ã  une valeur du primal seuil Ã©gale Ã  celle
obtenue par HyperPass +, pour diffÃ©rentes valeurs de . Le tableau 2 montre que Pegasos des-
cend trÃ¨s vite au debut lorsque K est petit, mais que la vitesse de convergence est plus faible
ensuite. Par exemple, dans le cas K = 10, il faut 51 passages pour arriver Ã  une solution avec
 = 0.001 tandis quâ€™il faut 246 passages pour arriver Ã  une solution avec  = 0.0001.
MÃ©thodes nombre dâ€™itÃ©rations complexitÃ© valeur du primal erreur de test (%)
HyperPass Shrinking 7 cycles 28Ã—O(Nd) 0.5446187 18.40
Pegasos K=N
 = 0.01 911 911Ã—O(Nd) 0.5545929 18.63
 = 0.001 1273 1273Ã—O(Nd) 0.5455888 18.47
 = 0.0001 1361 1361Ã—O(Nd) 0.5447137 18.45
 = 0.00001 1376 1376Ã—O(Nd) 0.5446285 18.42
Pegasos K=100
 = 0.01 18Ã—N/100 18Ã—O(Nd) 0.5491473 19.02
 = 0.001 40Ã—N/100 40Ã—O(Nd) 0.5455721 18.70
 = 0.0001 320Ã—N/100 320Ã—O(Nd) 0.5447172 18.48
 = 0.00001 767Ã—N/100 767Ã—O(Nd) 0.5446287 18.49
Pegasos K=10
 = 0.01 8Ã—N/10 8Ã—O(Nd) 0.5540219 18.79
 = 0.001 51Ã—N/10 51Ã—O(Nd) 0.5455784 18.51
 = 0.0001 246Ã—N/10 246Ã—O(Nd) 0.5447186 18.46
 = 0.00001 667Ã—N/10 667Ã—O(Nd) 0.5446285 18.48
TAB. 2 â€“ Comparaison de HyperPass Shrinking et de Pegasos avec tolÃ©rance  (voir texte).
6 Conclusions
Nous avons proposÃ© dans ce travail un nouvel algorithme dâ€™apprentissage vaste marge pour
des machines de type SVM. Cet algorithme optimise la fonction objectif sous sa forme primale
en combinant la mÃ©thode du sous-gradient, des rÃ©sultats sur lâ€™optimisation de fonctions non
Optimisation du Primal pour les SVM
partout diffÃ©rentiables, et la nature de la fonction objectif que lâ€™on cherche Ã  minimiser et qui
sâ€™exprime comme un maximum de fonctions quadratiques. Nous avons proposÃ© une version
de base de cet algorithme et une version exploitant une stratÃ©gie de shrinking beaucoup plus
rapide et tout aussi performante. Ces algorithmes hÃ©ritent des propriÃ©tÃ©s de convergence des
algorithmes de sous-gradient. Nous les avons comparÃ© aux algorithmes les plus rÃ©cents et nous
avons dÃ©montrÃ© sur ces expÃ©riences leur supÃ©rioritÃ©.
Ces rÃ©sultats sont dâ€™ores et dÃ©jÃ  trÃ¨s encourageants et nous travaillons en ce moment Ã 
Ã©tendre ce travail dans plusieurs directions, la kernÃ©lisation de la mÃ©thode permettant dâ€™ap-
prendre des classifieurs non linÃ©aires, et le calcul de la vitesse de convergence.
RÃ©fÃ©rences
Bertsekas, D. P., A. Nedic, et A. E. Ozdaglar (2003). Convex analysis and optimization.
Boyd, S. et L. Vandenberghe (2004). Convex Optimization. Cambridge University Press.
Chapelle, O. (2007). Training a support vector machine in the primal. Neural Computa-
tion 19(5), 1155â€“1178.
Demyanov, V. et L. Vasilev (1985). Nondifferentiable optimization.
Eizinger, C. et H. Plach (2003). A new approach to perceptron training. In IEEE Transactions
on Neural Networks, pp. 216â€“221.
Joachims, T. (1999). Making large-scale svm learning practical. In Advances in Kernel Me-
thods - Support Vector Learning, B. SchÃ¶lkopf and C. Burges and A. Smola (ed.). MIT Press.
Joachims, T. (2006). Training linear SVMs in linear time. In ACM SIGKDD International
Conference On Knowledge Discovery and Data Mining (KDD), pp. 217 â€“ 226.
LeCun, Y., L. Bottou, Y. Bengio, et P. Haffner (1998). Gradient-based learning applied to
document recognition. Proceedings of the IEEE 86(11), 2278â€“2324.
Osuna, E., R. Freund, et F. Girosi (1997). Training support vector machines : An application
to face detection. In CVPRâ€™97.
Shalev-Shwartz, S., Y. Singer, et N. Srebro (2007). Pegasos : Primal estimated sub-gradient
solver for svm. In ICML â€™07, New York, NY, USA, pp. 807â€“814. ACM Press.
Tsochantaridis, I., T. Hofmann, T. Joachims, et Y. Altun (2004). Support vector machine
learning for interdependent and structured output spaces. In ICML â€™04, pp. 104â€“112.
Zhang, T. (2004). Solving large scale linear prediction problems using stochastic gradient
descent algorithms. In ICML â€™04, New York, NY, USA, pp. 116. ACM Press.
Summary
Learning SVM through direct optimization of primal is a recent and much studied problem
since it opens new perspectives, for instance for dealing with structured outputs. We propose
a new algorithm of this kind that combines in an original way a number of techniques and
ideas, such as sub-gradient methods, optimization of non differentiable functions, and shrink-
ing heuristics.
