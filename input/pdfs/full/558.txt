RÃ©gression floue et crÃ©dibiliste par SVM pour la
classification des images sonar
Hicham Laanayaâˆ—,âˆ—âˆ—, Arnaud Martinâˆ—âˆ—
Driss Aboutajdineâˆ— Ali Khenchafâˆ—âˆ—
âˆ—GSCM-LRIT, UniversitÃ© Mohammed V-Agdal, FacultÃ© des sciences de Rabat, Maroc
aboutaj@fsr.ac.ma,
http://www.fsr.ac.ma/GSCM/
âˆ—âˆ—ENSIETA-E3I2-EA3876, 2, rue FranÃ§ois Verny 29806 Brest cedex 9,
laanayhi, Arnaud.Martin, Ali.Khenchaf@ensieta.fr
http://www.ensieta.fr/e3i2/
RÃ©sumÃ©. La classification des images sonar est dâ€™une grande importance par
exemple pour la navigation sous-marine ou pour la cartographie des fonds ma-
rins. En effet, le sonar offre des capacitÃ©s dâ€™imagerie plus performantes que les
capteurs optiques en milieu sous-marin. La classification de ce type de donnÃ©es
rencontre plusieurs difficultÃ©s en raison des imprÃ©cisions et incertitudes liÃ©es au
capteur et au milieu. De nombreuses approches ont Ã©tÃ© proposÃ©es sans donner
de bons rÃ©sultats, celles-ci ne tenant pas compte des imperfections des donnÃ©es.
Pour modÃ©liser ce type de donnÃ©es, il est judicieux dâ€™utiliser les thÃ©ories de lâ€™in-
certain comme la thÃ©orie des sous-ensembles flous ou la thÃ©orie des fonctions
de croyance. Les machines Ã  vecteurs de supports sont de plus en plus utilisÃ©es
pour la classification automatique aux vues leur simplicitÃ© et leurs capacitÃ©s de
gÃ©nÃ©ralisation. Il est ainsi possible de proposer une approche qui tient compte
de ces imprÃ©cisions et de ces incertitudes au cÅ“ur mÃªme de lâ€™algorithme de
classification. Lâ€™approche de la rÃ©gression par SVM que nous avons introduite
permet cette modÃ©lisation des imperfections. Nous proposons ici une application
de cette nouvelle approche sur des donnÃ©es rÃ©elles particuliÃ¨rement complexes,
dans le cadre de la classification des images sonar.
1 Introduction
Les images sonar sont utilisÃ©es pour leur rapiditÃ© Ã  imager de grandes zones lÃ  oÃ¹ lâ€™optique
ne peut le faire. On les retrouve ainsi dans de nombreuses applications telles que lâ€™aide Ã  la
navigation sous-marine, ou la cartographie sous-marine.
Les images sonar sont entachÃ©es de plusieurs imprÃ©cisions et incertitudes dues Ã  lâ€™instru-
mentation utilisÃ©e (le capteur sonar) et au milieu marin. Les paramÃ¨tres qui entrent en jeu pour
la reconstruction de ces images (gÃ©omÃ©trie du dispositif, coordonnÃ©es du bateau, mouvements
du sonar,. . .) sont aussi entachÃ©s des bruits de mesure. Il sâ€™ajoute Ã  ceci des interfÃ©rences dues
Ã  des trajets multiples des signaux utilisÃ©s, Ã  des bruits de chatoiement ou encore Ã  la faune et
RÃ©gression floue et crÃ©dibiliste par SVM pour la classification des images sonar
la flore. Ces imperfections rendent la tache difficile pour la caractÃ©risation des fonds marins Ã 
partir de ce type de donnÃ©es. Il est donc nÃ©cessaire de proposer des algorithmes, robustes aux
imperfections, pour la classification automatiques des images sonar.
Plusieurs choix sont envisageable pour remÃ©dier aux problÃ¨mes dâ€™imperfections : soit nous
tentons de supprimer ces imperfections, ce qui nÃ©cessite une comprÃ©hension, souvent diffi-
cile, de la physique qui a conduit Ã  ces imperfections ; soit nous cherchons Ã  dÃ©velopper des
processus de traitement robustes Ã  ces imperfections ; soit nous cherchons Ã  les modÃ©liser.
Le cadre thÃ©orique des thÃ©ories de lâ€™incertain offre la possibilitÃ© de modÃ©liser finement
ces imperfections. Parmi elles, la thÃ©orie des ensembles flous et la thÃ©orie des fonctions de
croyance permettent de tenir compte des incertitudes et imprÃ©cisions.
De nombreuses approches ont Ã©tÃ© proposÃ©es pour la classification des images sonar par
exemple dans Laanaya et al. (2005b) et Leblond et al. (2005). Ces approches ne tiennent pas
compte de lâ€™incertitude de lâ€™expert lors de la segmentation de ces images. Nous adopterons
dans ce papier lâ€™approche que nous avons proposÃ©e dans Laanaya et al. (2006) avec une rÃ©-
solution du problÃ¨me dâ€™optimisation adaptÃ©e Ã  la classification automatique des images sonar.
Cette approche a donnÃ© des rÃ©sultats intÃ©ressants sur des donnÃ©es gÃ©nÃ©rÃ©es, nous montrerons
ici son intÃ©rÃªt sur les donnÃ©es complexes que sont les images sonar.
Ainsi, nous prÃ©senterons une description rapide des fonctions dâ€™appartenance et des fonc-
tions de croyance utilisÃ©es par lâ€™approche de la rÃ©gression par SVM. Nous rappelons ensuite,
lâ€™approche de la rÃ©gression par SVM aprÃ¨s une brÃ¨ve introduction du principe des SVM. Cette
approche est comparÃ©e au SVM classique et discutÃ©e dans une derniÃ¨re partie Ã  partir dâ€™images
sonar.
2 ThÃ©ories de lâ€™incertain
Nous avons vu dans Martin (2005) que les thÃ©ories de lâ€™incertain telles que la thÃ©orie
des sous-ensembles flous introduite par Zadeh (1965), la thÃ©orie des possibilitÃ©s de Dubois
et Prade (1987) ou encore la thÃ©orie des fonctions de croyance de Dempster (1967) et Shafer
(1976) permettent la modÃ©lisation de donnÃ©es incertaines et imprÃ©cises dans le cadre de la
classification dâ€™images sonar.
Ces thÃ©ories sont fondÃ©es sur les fonctions dâ€™appartenance pour les premiÃ¨res et sur les
fonctions de croyance pour la derniÃ¨re. Afin dâ€™intÃ©grer directement les contraintes liÃ©es Ã  ces
fonctions dans un algorithme de classification, nous rappelons ici les caractÃ©ristiques des fonc-
tions dâ€™appartenance de la thÃ©orie des sous-ensembles flous et des fonctions de croyance de
Dempster et Shafer.
2.1 Les fonctions dâ€™appartenance
Les fonctions dâ€™appartenance permettent de dÃ©crire une appartenance floue Ã  une classe.
Ainsi lâ€™appartenance dâ€™une observation x Ã  une classe Ci parmiNc classes, est donnÃ©e par une
fonction Âµi(x) telle que : ï£±ï£´ï£²ï£´ï£³
Âµi(x) âˆˆ [0, 1]
Ncâˆ‘
i=1
Âµi(x) = 1.
(1)
H. Laanaya et al.
Dans ce cas, nous considÃ©rons les classes floues. Dans le cas de classes nettes, il est possible
de considÃ©rer les distributions de possibilitÃ©. Typiquement x peut reprÃ©senter une partie du
fond marin et Ci le type de sÃ©diment prÃ©sent sur lâ€™image x. Nous verrons au paragraphe 4.1.2
comment ces fonctions Âµi peuvent Ãªtre choisie dans notre application.
2.2 Les fonctions de croyance
La thÃ©orie des fonctions de croyance est fondÃ©e sur la manipulation des fonctions de masse.
Les fonctions de masse sont dÃ©finies sur lâ€™ensemble de toutes les disjonctions du cadre de dis-
cernement Î˜ = {C1, . . . , CNc} et Ã  valeurs dans [0, 1], oÃ¹ Ci reprÃ©sente lâ€™hypothÃ¨se â€œlâ€™ob-
servation appartient Ã  la classe iâ€. La contrainte de normalitÃ© couramment employÃ©e est ici
donnÃ©e par : âˆ‘
Aâˆˆ2Î˜
mj(A) = 1, (2)
oÃ¹ m(.) reprÃ©sente la fonction de masse. La premiÃ¨re difficultÃ© est donc de dÃ©finir ces fonc-
tions de masse selon le problÃ¨me. Nous verrons comment il est possible de le faire pour notre
application dans la section 4.1.2. A partir de ces fonctions de masse, dâ€™autres fonctions de
croyance peuvent Ãªtre dÃ©finies, telles que les fonctions de crÃ©dibilitÃ©, reprÃ©sentant lâ€™intensitÃ©
que toutes les sources croient en un Ã©lÃ©ment, et telles que les fonctions de plausibilitÃ© reprÃ©-
sentant lâ€™intensitÃ© avec laquelle on ne doute pas en un Ã©lÃ©ment.
Afin de conserver un maximum dâ€™informations, il est prÃ©fÃ©rable de rester Ã  un niveau crÃ©-
dal (i.e. de manipuler des fonctions de croyance) pendant lâ€™Ã©tape de manipulation des infor-
mations pour prendre la dÃ©cision sur les fonctions de croyance Ã  lâ€™issue de la manipulation de
ces fonctions. Si la dÃ©cision prise par le maximum de crÃ©dibilitÃ© peut Ãªtre trop pessimiste, la
dÃ©cision issue du maximum de plausibilitÃ© est bien souvent trop optimiste. Le maximum de la
probabilitÃ© pignistique, introduite par Smets (1990), reste le compromis le plus employÃ©. La
probabilitÃ© pignistique est donnÃ©e pour tout X âˆˆ 2Î˜, avec X 6= âˆ… par :
betP(X) =
âˆ‘
Y âˆˆ2Î˜,Y 6=âˆ…
|X âˆ© Y |
|Y |
m(Y )
1âˆ’m(âˆ…) . (3)
2.3 Similitudes
Ainsi les fonctions dâ€™appartenance et les fonctions de masse permettent une modÃ©lisation
de lâ€™incertitude et de lâ€™imprÃ©cision Ã  partir de points de vue diffÃ©rents.
Ces fonctions ont toutes deux la particularitÃ© dâ€™Ãªtre Ã  valeurs dans [0,1] et dâ€™avoir une
contrainte de normalitÃ© Ã©quivalente. Nous allons voir dans la section suivante comment intÃ©grer
ces contraintes dans une rÃ©gression linÃ©aire multiple.
3 RÃ©gression floue et crÃ©dibiliste par SVM
Nous avons proposÃ©e dans Laanaya et al. (2006) une nouvelle approche pour la classifi-
cation automatique fondÃ©e sur une rÃ©gression effectuÃ©e Ã  partir des SVM. Cette approche a
RÃ©gression floue et crÃ©dibiliste par SVM pour la classification des images sonar
montrÃ© des performances remarquables sur des donnÃ©es gÃ©nÃ©rÃ©es. Afin dâ€™assoir les notations
utiles pour la suite, nous rappelons le principe des SVM sur laquelle sâ€™appuie la rÃ©gression
floue et crÃ©dibiliste prÃ©sentÃ©e ensuite.
3.1 Principe du classifieur SVM
Les machines Ã  vecteurs de support initiÃ©es par Vapnik (1998), sont avant tout une ap-
proche de classification linÃ©aire Ã  deux classes. Elles tentent de sÃ©parer des individus issus de
deux classes (+1 et -1) en cherchant lâ€™hyperplan optimal qui sÃ©pare les deux ensembles, en
garantissant une grande marge entre les deux classes. Un nombre rÃ©duit dâ€™exemples pour la
recherche de lâ€™hyperplan est suffisant pour la description de cet hyperplan.
Dans le cas oÃ¹ les exemples sont linÃ©airement sÃ©parables, on cherche lâ€™hyperplan
y = w.x+ b qui maximise la marge entre les deux ensembles oÃ¹ w.x est le produit scalaire de
w et x. Ainsi w est la solution du problÃ¨me dâ€™optimisation convexe :
Min â€–wâ€–2/2 (4)
sous les contraintes :
yt(w.xt + b)âˆ’ 1 â‰¥ 0 âˆ€t = 1, . . . , l, (5)
oÃ¹ les xt âˆˆ IRd reprÃ©sentent les l donnÃ©es dâ€™apprentissage , et yt âˆˆ {âˆ’1,+1} la classe. Ce
problÃ¨me dâ€™optimisation se rÃ©sout par la mÃ©thode du lagrangien.
Dans le cas oÃ¹ les donnÃ©es ne sont pas linÃ©airement sÃ©parables, les contraintes (5) sont
relachÃ©es par lâ€™introduction de termes positifs Î¾t. Nous cherchons alors Ã  minimiser :
1
2
â€– w â€–2 +C
lâˆ‘
t=1
Î¾t, (6)
sous les contraintes donnÃ©es pour tout t :{
yt(w.xt + b) â‰¥ 1âˆ’ Î¾t
Î¾t â‰¥ 0 (7)
oÃ¹ C est une constante choisie par lâ€™utilisateur. Le problÃ¨me se rÃ©sout alors de maniÃ¨re simi-
laire au cas linÃ©airement sÃ©parable.
Afin de classer un nouvel Ã©lÃ©ment x il suffit dâ€™Ã©tudier la fonction de dÃ©cision donnÃ©e par :
f(x) = sign(
âˆ‘
tâˆˆSV
ytÎ±
0
txt.xâˆ’ b0), (8)
oÃ¹ SV = {t ;Î±0t > 0} pour le cas sÃ©parable et SV = {t ; 0 < Î±0t < C} pour le cas
non sÃ©parable, est lâ€™ensemble des vecteurs de support, et Î±t â‰¥ 0 sont les multiplicateurs de
Lagrange.
Dans les cas non linÃ©aire, le principe des SVM est de projeter, par une fonction noyau,
les donnÃ©es de dÃ©part dans un espace de grande dimension (Ã©ventuellement infinie). Ainsi la
classification dâ€™un nouvel Ã©lÃ©ment x est donnÃ©e par la fonction de dÃ©cision :
f(x) = sign(
âˆ‘
tâˆˆSV
ytÎ±
0
tK(x, xt)âˆ’ b0) (9)
H. Laanaya et al.
oÃ¹ K est la fonction noyau, dont les plus utilisÃ©es sont le noyau polynomial K(x, xt) =
(x.xt + 1)d, d âˆˆ IN, et le noyau gaussien K(x, xt) = eâˆ’Î³â€–xâˆ’xtâ€–2 , Î³ âˆˆ IR+. Le choix
du noyau et lâ€™optimisation des paramÃ¨tres de celui-ci reste dÃ©licat selon lâ€™application.
3.2 RÃ©gression floue et crÃ©dibiliste par SVM
Nous avons situÃ© cette approche dans la littÃ©rature Laanaya et al. (2006). Ainsi elle est
novatrice par la prise en compte des contraintes similaires de normalisation des fonctions de
croyance et dâ€™appartenance dans le problÃ¨me de rÃ©gression multiple. De plus nous proposons
ici dâ€™employer une rÃ©solution du problÃ¨me dâ€™optimisation pouvant gÃ©rer de grande quantitÃ© de
donnÃ©es.
Soient les vecteurs dâ€™apprentissage xt âˆˆ IRd et les fonctions associÃ©es yt âˆˆ IRN , oÃ¹ N =
Nc le nombre de classes dans le cas des fonctions dâ€™appartenance et N = 2Nc dans le cas des
fonctions de masse. Ainsi par la rÃ©gression multiple linÃ©aire, nous cherchons une fonctionnelle
f = (f1, . . . , fN ) oÃ¹ les fn sont linÃ©aires, de forme fn(x) = wn.x + bn. Nous cherchons
Ã  dÃ©terminer cette fonctionnelle telle que pour les (xt, yt) de la base dâ€™apprentissage |ytn âˆ’
wn.xt + bn| ne dÃ©passe pas un certain  fixÃ© pour tout n. Nous supposons ainsi que tous les
points sont Ã  lâ€™intÃ©rieur du cylindre dÃ©fini par . Afin de gÃ©nÃ©raliser, nous associons un facteur
C pour les points qui sont Ã  lâ€™extÃ©rieur du cylindre dÃ©fini par . Le problÃ¨me dâ€™optimisation
convexe revient donc Ã  celui exposÃ© dans la section 3.1, et le critÃ¨re Ã  minimiser est :
1
2
Nâˆ‘
n=1
â€–wnâ€–2 + C
Nâˆ‘
n=1
lâˆ‘
t=1
(Î¾tn + Î¾âˆ—tn), (10)
sous les contraintes donnÃ©es pour tout t et tout n :ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
ytn âˆ’ wn.xt âˆ’ bn â‰¤ + Î¾tn,
wn.xt + bn âˆ’ ytn â‰¤ + Î¾âˆ—tn,
Nâˆ‘
n=1
(wn.xt + bn) = 1,
wn.xt + bn â‰¥ 0,
wn.xt + bn â‰¤ 1,
Î¾tn, Î¾
âˆ—
tn â‰¥ 0.
(11)
Le lagrangien est donc donnÃ© par :
L =
1
2
Nâˆ‘
n=1
â€–wnâ€–2 + C
Nâˆ‘
n=1
lâˆ‘
t=1
(Î¾tn + Î¾âˆ—tn)âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
(Î·tnÎ¾tn + Î·âˆ—tnÎ¾
âˆ—
tn) (12)
âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î±tn(+ Î¾tn âˆ’ ytn + wn.xt + bn)
âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î±âˆ—tn(+ Î¾
âˆ—
tn + ytn âˆ’ wn.xt âˆ’ bn)
âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î²tn(wn.xt + bn)âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î²âˆ—tn(1âˆ’ wn.xt âˆ’ bn)
RÃ©gression floue et crÃ©dibiliste par SVM pour la classification des images sonar
âˆ’
lâˆ‘
t=1
Î³t
(
1âˆ’
Nâˆ‘
n=1
(wn.xt + bn)
)
oÃ¹ les Î·, Î±, Î² et Î³ sont les multiplicateurs de Lagrange et sont positifs.
Au point selle du lagrangien L, on a pour tout t et tout n, âˆ‚L/âˆ‚bn = 0, âˆ‚L/âˆ‚wn = 0,
âˆ‚L/âˆ‚Î¾tn = 0 et âˆ‚L/âˆ‚Î¾âˆ—tn = 0. Ainsi :ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
lâˆ‘
t=1
Ïƒtn = 0,
wn =
lâˆ‘
t=1
Ïƒtnxt,
Î·tn = C âˆ’ Î±tn,
Î·âˆ—tn = C âˆ’ Î±âˆ—tn,
(13)
avec Ïƒtn = Î±tn âˆ’ Î±âˆ—tn + Î²tn âˆ’ Î²âˆ—tn âˆ’ Î³t.
En intÃ©grant ces Ã©quations (13) dans le lagrangien (Ã©quation (12)), le problÃ¨me revient Ã 
maximiser :
âˆ’1
2
Nâˆ‘
n=1
lâˆ‘
t,tâ€²=1
ÏƒtnÏƒtâ€²nxt.xtâ€² âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î²âˆ—tn +
Î³t
N
âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î±âˆ—tn(+ ytn)âˆ’
Nâˆ‘
n=1
lâˆ‘
t=1
Î±tn(âˆ’ ytn)
sous les contraintes : ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
lâˆ‘
t=1
Ïƒtn = 0,
Î±tn âˆˆ [0, C],
Î±âˆ—tn âˆˆ [0, C],
Î²tn â‰¥ 0,
Î²âˆ—tn â‰¥ 0,
Î³t â‰¥ 0.
Enfin, pour prÃ©dire la nÃ¨me sortie yËœn, dâ€™un nouvel Ã©lÃ©ment x, on calcule :
yËœn =
lâˆ‘
t=1
Ïƒtnxt.x+ bn,
oÃ¹ bn est dÃ©duite des conditions de Kuhn, Karush et Tucker :ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£³
Î±tn(+ Î¾tn âˆ’ ytn + wn.xt + bn) = 0,
Î±âˆ—tn(+ Î¾âˆ—tn + ytn âˆ’ wn.xt âˆ’ bn) = 0,
(C âˆ’ Î±tn)Î¾tn = 0,
(C âˆ’ Î±âˆ—tn)Î¾âˆ—tn = 0,
Î²tn(wn.xt + bn) = 0,
Î²âˆ—tn(1âˆ’ wn.xt âˆ’ bn) = 0.
H. Laanaya et al.
Si pour un t0, Î±t0n âˆˆ]0, C[ alors, Î¾t0n = 0, ainsi bn = yt0n âˆ’ wn.xt0 âˆ’ , un raisonnement
identique sur Î±âˆ— donne bn = yt0n âˆ’ wn.xt0 + .
La rÃ©solution du systÃ¨me dâ€™optimisation de la rÃ©gression par SVM pour des problÃ¨mes de
grande dimension nÃ©cessite des mÃ©moires de stockage de grande taille. Ainsi, lâ€™application des
algorithmes dâ€™optimisation classiques est difficile. Ces limites ont Ã©tÃ© constatÃ©es dans Laanaya
et al. (2006). Une solution est dâ€™utiliser des mÃ©thodes dâ€™optimisation itÃ©ratives, oÃ¹ on essaye de
rÃ©soudre des sous-problÃ¨mes du problÃ¨me principale. Nous avons adaptÃ© la rÃ©solution par SMO
(Sequential Minimal Optimization) dÃ©veloppÃ©e par Platt (1998) pour les machines Ã  vecteurs
de support, pour notre problÃ¨me dâ€™optimisation. Il rÃ©sout des sous-problÃ¨mes de dimension
deux dâ€™une maniÃ¨re analytique. Nous pouvons ainsi rÃ©soudre des problÃ¨mes de grande taille
avec une vitesse remarquable.
Si on suppose que la relation entre les xt et les sorties yËœt est non-linÃ©aire, nous pouvons re-
prÃ©senter les donnÃ©es de dÃ©part en utilisant un noyau. Ainsi le produit scalaire entre les donnÃ©es
de la base dâ€™apprentissage peut Ãªtre donc substituÃ© par un noyau : le produit scalaire x.xâ€² de-
vient K(x, xâ€²). Une rÃ©gression linÃ©aire peut alors sâ€™appliquer dans lâ€™espace de reprÃ©sentation.
Pour une observation x, la sortie yËœ se prÃ©dit en considÃ©rant les N valeurs :
yËœn =
lâˆ‘
t=1
ÏƒtnK(x, xt) + bn.
A partir de cette approche de rÃ©gression sur les fonctions dâ€™appartenance ou les fonctions
de croyance, nous obtenons un classifieur en prenant la dÃ©cision via le maximum des fonctions
dâ€™appartenance ou le maximum de la probabilitÃ© pignistique.
4 ExpÃ©rimentations
Nous prÃ©sentons ici lâ€™application de notre approche pour la classification des images sonar.
En effet, lâ€™environnement sous-marin lui mÃªme est trÃ¨s incertain et les systÃ¨mes de mesure
sont complexes et imprÃ©cis. Il est particuliÃ¨rement important de classifier les fond marins pour
de nombreuses applications telles que la navigation et la cartographie sous-marine. Nous trou-
verons plusieurs Ã©tudes sur la classification de images sonar, citons par exemple Martin et al.
(2004), Laanaya et al. (2005a), Laanaya et al. (2005b) et Leblond et al. (2005).
Les donnÃ©es Ã  classifier sont ainsi entachÃ©es de nombreuses imperfections dues aux bruits
de mesure, aux interfÃ©rences des signaux utilisÃ©s pour lâ€™acquisition, aux bruits de chatoiement
et Ã  la faune et la flore.
4.1 Base de donnÃ©es
La base de donnÃ©es est constituÃ©e de 42 images sonar fournies par GESMA (Groupe
dâ€™Etudes Sous-Marines de lâ€™Atlantique) et ont Ã©tÃ© obtenues Ã  partir dâ€™un sonar Klein 5400
au large des cÃ´tes finistÃ©riennes. Ces images ont Ã©tÃ© labellisÃ©es Ã  partir dâ€™un logiciel dÃ©ve-
loppÃ© spÃ©cialement en spÃ©cifiant le type du sÃ©diment prÃ©sent (sable, ride, vase, roche, caillou-
tis ou ombre) (voir figure 1) et le degrÃ© de certitude de lâ€™expert (sÃ»r, moyennement sÃ»r ou
non sÃ»r). Parmi ces sÃ©diments, nous avons considÃ©rÃ© trois classes distinctes, particuliÃ¨rement
RÃ©gression floue et crÃ©dibiliste par SVM pour la classification des images sonar
Sable Ride Roche
Cailloutis Roche et sable Ride et vase
FIG. 1 â€“ Exemple dâ€™image sonar (fournit par le GESMA) et dâ€™imagettes Ã©tiquetÃ©es.
importantes pour la navigation sous-marine et les sÃ©dimentologues. Ainsi la premiÃ¨re classe
regroupe roche et cailloutis, la deuxiÃ¨me classe les rides et la troisiÃ¨me le sable et les vases.
Lâ€™unitÃ© de classification retenue est lâ€™imagette de taille 32Ã—32 pixels (soit environ 640Ã—640
cm.
4.1.1 Extraction de paramÃ¨tres
Afin de rÃ©duire les problÃ¨mes de reprÃ©sentativitÃ© des imagettes qui comportent plus dâ€™un
sÃ©diment et les problÃ¨mes liÃ©s Ã  lâ€™Ã©valuation (cf. Martin et al. (2006)), nous ne considÃ©rons
ici que les imagettes homogÃ¨nes (imagettes avec un seul type de sÃ©diment). Nous avons ainsi
31957 imagettes.
Nous avons calculÃ© sur ces imagettes six paramÃ¨tres extraits Ã  partir des matrices de co-
occurrence calculÃ©s sur les imagettes Martin et al. (2004). Les matrices de cooccurrence Cd
sont calculÃ©es en comptant les occurrences identiques de niveaux de gris entre deux pixels
contigus dans une direction d donnÃ©e. Quatre directions sont considÃ©rÃ©es : 0, 45, 90 et 135
degrÃ©s. Dans ces quatre directions six paramÃ¨tres dâ€™Haralick sont calculÃ©s : lâ€™homogÃ©nÃ©itÃ©, le
contraste, lâ€™entropie, la corrÃ©lation et lâ€™uniformitÃ©. Lâ€™homogÃ©nÃ©itÃ© qui a une valeur Ã©levÃ©e pour
des images uniformes ou possÃ©dant une texture pÃ©riodique dans la direction d est donnÃ©e par :
NGâˆ‘
i=1
NGâˆ‘
j=1
C2d(i, j) (14)
oÃ¹ NG est le niveau de gris des imagettes.
Lâ€™estimation du contrast est donnÃ©e par :
1
NG âˆ’ 1
NGâˆ’1âˆ‘
k=0
k2
NGâˆ‘
i,j=1,|iâˆ’j|=k
Cd(i, j) (15)
H. Laanaya et al.
Lâ€™entropie qui a de faibles valeurs sâ€™il y a peu de probabilitÃ©s de transition Ã©levÃ©es dans Cd, est
dÃ©finie par :
1âˆ’
NGâˆ‘
i=1
NGâˆ‘
j=1
p(i, j) log(Cd(i, j)) (16)
La corrÃ©lation entre les lignes et les colonnes de la matrice est donnÃ©e par :
NGâˆ‘
i=1
NGâˆ‘
j=1
(iâˆ’ Âµx)(j âˆ’ Âµy)Cd(i, j)
ÏƒxÏƒy
(17)
oÃ¹ Âµx, Ïƒx, Âµy , Ïƒy reprÃ©sentent respectivement les moyennes et Ã©cart-types des distributions
marginales des Ã©lÃ©ments de la matrice de cooccurrence.
La directivitÃ© qui dÃ©finie lâ€™existence dâ€™une direction privilÃ©giÃ©e de la texture est calculÃ©e
par :
NGâˆ‘
i=1
Cd(i, i) (18)
Lâ€™uniformitÃ© qui caractÃ©rise la proportion dâ€™un mÃªme niveau de gris est donnÃ©e par :
NGâˆ‘
i=1
Cd(i, i)2 (19)
Nous avons moyennÃ© ces paramÃ¨tres selon les quatre directions d, ainsi chaque imagette
est reprÃ©sentÃ©e uniquement par six parammÃ¨tres.
4.1.2 ModÃ©lisation des fonctions floues et crÃ©dibilistes
Nous avons utilisÃ© lâ€™approche de Keller et al. (1985) pour calculer la fonction dâ€™apparte-
nance des vecteurs dâ€™apprentissage que nous utiliserons pour lâ€™apprentissage du SVM flou, et
lâ€™approche de DenÅ“ux (1995) pour estimer les fonctions de masses que nous utiliserons pour
lâ€™apprentissage du SVM crÃ©dibiliste.
Lâ€™approche de Keller et al. (1985) est celle dâ€™un k-plus proches voisins flou. Les fonctions
dâ€™appartenance dâ€™un vecteur dâ€™apprentissage xt sont estimÃ©es dans un premier temps par :
Âµi(xt) =
ki(xt)
kf
, (20)
oÃ¹ kf est le nombre de plus proches voisins choisi pour le voisinage flou VKf et
ki(xt) = |Ci âˆ© VKf (xt)|. Dans un second temps, nous calculons la fonction dâ€™appartenance
pour un vecteur x Ã  classifier :
Âµi(x) =
lâˆ‘
t=1
Âµi(xt)
â€–xâˆ’ xtâ€–2
lâˆ‘
t=1
1
â€–xâˆ’ xtâ€–2
. (21)
RÃ©gression floue et crÃ©dibiliste par SVM pour la classification des images sonar
La norme employÃ©e est ici la norme euclidienne.
La classe dâ€™appartenance de x est ensuite dÃ©cidÃ©e de maniÃ¨re classique comme la classe
donnant le maximum des fonctions dâ€™appartenance prÃ©dites par notre rÃ©gression.
Lâ€™approche de DenÅ“ux (1995) calcule une estimation des fonctions de masses Ã  partir dâ€™un
modÃ¨le de distance : {
mk(Ci|x(t,k))(x) = Î±ieÎ³id2(x,x(t,k))
mk(Î˜|x(t,k))(x) = 1âˆ’ Î±ieÎ³id2(x,x(t,k))
(22)
oÃ¹ Ci est la classe associÃ©e Ã  x(t,k), qui sont les k vecteurs dâ€™apprentissage les plus proches
de la valeur x et la distance employÃ©e est la distance euclidienne. Î±i et Î³i sont des coefficients
dâ€™affaiblissement, et de normalisation. Les k fonctions de masse ainsi calculÃ©es pour chaque x
sont combinÃ©es par la rÃ¨gle orthogonale normalisÃ©e de Dempster-Shafer. Cette rÃ¨gle est donnÃ©e
pour deux experts et pour tout A âˆˆ 2Î˜, A 6= âˆ… par :
m(A) =
âˆ‘
Bâˆ©C=A
m1(B)m2(C)
1âˆ’
âˆ‘
Bâˆ©C=âˆ…
m1(B)m2(C)
, (23)
etm(âˆ…) = 0.
La dÃ©cision est ensuite prise par le maximum sur les fonctions de masse prÃ©dites par notre
rÃ©gression. Dans ce cas, il est Ã©quivalent au maximum de probabilitÃ© pignistique car les seuls
Ã©lÃ©ments focaux sont les singletons et lâ€™ignorance.
4.2 RÃ©sultats
Nous avons effectuÃ© un tirage alÃ©atoire de 3000 imagettes homogÃ¨nes sur toute la base de
donnÃ©es (31957 imagettes), ainsi la base dâ€™apprentissage contient des effectifs diffÃ©rents pour
les trois classes : 15.53% des imagettes contiennent du roche et cailloutis, 11.85% sont des
imagettes rides et les 72.62% restants sont du sable et de la vase. La base de test est constituÃ©e
de 1000 imagettes choisies de faÃ§on alÃ©atoire. Nous avons rÃ©pÃ©tÃ© cette opÃ©ration 10 fois afin
dâ€™obtenir des estimations plus fiables des taux de classification.
Nous avons comparÃ© la classification fondÃ©e sur les machines Ã  vecteurs de support donnÃ©e
par le logiciel libSVM de Chang et Lin (2001) et une version modifiÃ©e de ce dernier quâ€™on a
dÃ©veloppÃ©e pour intÃ©grer notre approche.
Les matrices de confusion normalisÃ©es obtenues par le SVM classique (avec les paramÃ¨tres
par dÃ©faut de libSVM : noyau gaussien avec Î³ = 1 et C=1), SVM crÃ©dibiliste et SVM flou avec
un noyau gaussien, avec Î³ = 1, C = 1 et  = 0.1, sont donnÃ©es ci-dessous.
SVM classique SVM crÃ©dibiliste SVM flouï£«ï£­ 61.62 25.26 13.1223.08 62.59 14.32
4.99 10.97 84.04
ï£¶ï£¸ ï£«ï£­ 70.59 15.73 13.6816.81 70.28 12.91
6.42 13.91 79.66
ï£¶ï£¸ ï£«ï£­ 68.99 16.60 14.4116.18 66.09 17.74
5.85 7.59 86.56
ï£¶ï£¸
Nous avons ainsi obtenu un taux de 78.02Â±5.14% pour un SVM classique et des taux de
77.14Â±5.22% pour le SVM crÃ©dibiliste et 81.40Â±4.83% pour le SVM flou. Dans le cas du
H. Laanaya et al.
SVM classique nous avons obtenu un vecteur de bonne classification de [61.62Â±4.85 62.59Â±5.52
84.04Â±1.69], [70.59Â±4.54 70.28Â± 5.22 79.66Â±1.83] par la rÃ©gression crÃ©dibiliste par SVM
et [68.99Â±4.61 66.09Â±5.6 86.56Â±1.55] par la rÃ©gression floue par SVM. Les vecteurs dâ€™er-
reurs sont donnÃ©s par : [26.21 27.76 14.84] pour le SVM classique, [20.51 22.27 16.82] pour
le SVM crÃ©dibiliste et [21.01 23.00 14.76] pour le SVM flou. En comparaison avec lâ€™approche
classique des SVM, nous avons ainsi une amÃ©lioration significative de la classification des
trois classes pour la rÃ©gression floue issue des SVM et une amÃ©lioration significative avec la
rÃ©gression crÃ©dibiliste issue des SVM pour les deux premiÃ¨res classes (la classe des roches et
des cailloutis, et la classe des rides) ces deux derniÃ¨res classes sont particuliÃ¨rement difficiles
Ã  classifier du fait de leur faible reprÃ©sentativitÃ©. Notons que câ€™est lâ€™approche crÃ©dibiliste qui
donne les meilleurs rÃ©sultats pour ces deux classes. Ainsi la nouvelle approche a apportÃ© une
amÃ©lioration pour la classification des diffÃ©rentes classes des images sonar qui sont particuliÃ¨-
rement difficile Ã  caractÃ©risÃ©es.
5 Conclusion
Nous avons proposÃ© dans ce papier une nouvelle rÃ©solution de lâ€™approche de rÃ©gression
floue et crÃ©dibiliste Ã  partir de machines Ã  vecteurs de support pour la classification prÃ©cÃ©-
dement introduite. Les rÃ©sultats obtenus sur les images sonar ont montrÃ© lâ€™intÃ©rÃªt de cette
approche. En particulier lâ€™approche crÃ©dibiliste donne de trÃ¨s bons rÃ©sultats sur des donnÃ©es
faiblement apprises, alors que lâ€™approche floue permet dâ€™avoir une meilleure classification pour
chaque classe considÃ©rÃ©e.
Nous nâ€™avons donnÃ© ici que des rÃ©sultats en utilisant des valeurs empiriques pour les pa-
ramÃ¨tres (C,  et Î³). Le rÃ©glage de ces paramÃ¨tres peut se faire en utilisant les algorithmes
gÃ©nÃ©tiques par exemple, il est possible aussi dâ€™intÃ©grer lâ€™optimisation de ces constantes dans le
problÃ¨me dâ€™optimisation gÃ©nÃ©rale des SVM pour la rÃ©gression.
RÃ©fÃ©rences
Chang, C. C. et C. J. Lin (2001). Libsvm : a library for support vector machines. Software
available at http ://www.csie.ntu.edu.tw/âˆ¼cjlin/libsvm/ .
Dempster, A. P. (1967). Upper and lower probabilities induced by a multivalued mapping.
Annals of Mathematical Statistics 83, 325â€“339.
DenÅ“ux, T. (1995). A k-nearest neighbor classification rule based on dempster-shafer theory,.
IEEE Transactions on Systems, Man, and Cybernetics - Part A : Systems and Humans 25(5),
804â€“813.
Dubois, D. et H. Prade (1987). ThÃ©orie des possibilitÃ©s. Masson.
Keller, J. M., M. Gray, et J. Givens (1985). A fuzzy k-nn neighbor algorithm. IEEE Transac-
tions on Systems, Man, and Cybernetics 15, 580â€“585.
Laanaya, H., A. Martin, D. Aboutajdine, et A. Khenchaf (20-23 June 2005a). A new dimen-
sionality reduction method for seabed characterization : Supervised curvilinear component
analysis. IEEE OCEANSâ€™05 EUROPE, Brest, France.
RÃ©gression floue et crÃ©dibiliste par SVM pour la classification des images sonar
Laanaya, H., A. Martin, A. Khenchaf, et D. Aboutajdine (15-18 March 2005b). Feature selec-
tion using genetic algorithm for sonar images classification with support vector machines.
European Conference on Propagation and Systems, Brest, France.
Laanaya, H., A. Martin, A. Khenchaf, et D. Aboutajdine (19-20 Octobre 2006). Classification
par rÃ¨gression floue et crÃ¨dibiliste Ã  base de machines Ã  vecteurs de support. LFA 2006,
Toulouse, France.
Leblond, I., M. Legris, et B. Solaiman (20-23 June 2005). Use of classification and segmen-
tation of sidescan sonar images for long term registration. IEEE Oceansâ€™05 Europe, Brest,
France.
Martin, A. (Novembre 2005). Fusion de classifieurs pour la classification dâ€™images sonar.
RNTI Extraction des connaissances : Etat et perspectives, 259â€“268.
Martin, A., H. Laanaya, et A. Arnold-Bos (2006). Evaluation for uncertain image classification
and segmentation. Pattern Recognition 39.
Martin, A., G. Sevellec, et I. Leblond (21-22 October 2004). Characteristics vs decision fusion
for sea-bottom characterization. Colloque CaractÃ©risation in-situ des fonds marins, Brest,
France.
Platt, J. (1998). Sequential minimal optimization : A fast algorithm for training support vector
machines. Microsoft Research Technical Report MSR-TR-98-14.
Shafer, G. (1976). A mathematical theory of evidence. Princeton University Press.
Smets, P. (1990). Constructing the pignistic probability function in a context of uncertainty.
Uncertainty in Artificial Intelligence 5, 29â€“39.
Vapnik, V. N. (1998). Statistical Learning Theory. John Wesley and Sons.
Zadeh, L. A. (1965). Fuzzy sets. Information Control 8, 338â€“353.
Summary
The sonar image classification is of great importance, for underwater navigation or for
seabed cartography. Indeed, the sonar is more suitable than optical captors for seabed imagery.
The classification of such kind of data encounters several difficulties due to the imprecisions
and uncertainties present on these data. Many approaches were proposed without giving good
results, they do not take into account the data imperfections. To model this kind of data, it is
judicious to use the uncertain theories like the fuzzy subsets theory or the belief function theory.
The support vector machines are more and more used for automatic classification due to their
simplicity and their generalization capacities. Thus, it is possible to propose an approach that
take into account these imprecisions and uncertainties. The regression by SVM approach that
we have proposed model these imperfections. We propose here an application of this new
approach for particularly complex real data in the framework of sonar image classification.
