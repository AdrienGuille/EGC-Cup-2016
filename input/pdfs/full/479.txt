Sï°ï¡ï­ï³, une nouvelle approche incrÃ©mentale pour
lâ€™extraction de motifs sÃ©quentiels frÃ©quents dans les
Data streams
Lionel Vï©ï®ï£ï¥ï³ï¬ï¡ï³âˆ—, Jean-Ã‰mile Sï¹ï­ï°ï¨ï¯ï²âˆ—, Alban Mï¡ï®ï£ï¨ï¥ï²ï¯ï®âˆ—âˆ— et Pascal Pï¯ï®ï£ï¥ï¬ï¥ï´âˆ—âˆ—âˆ—
âˆ—Gï²ï©ï­ï¡ï¡ï§, UniversitÃ© des Antilles et de la Guyane, Martinique, France.
{lionel.vinceslas,je.symphor}@martinique.univ-ag.fr
âˆ—âˆ—alban@mancheron.infos.st
âˆ—âˆ—âˆ—
ï¥ï­ï¡-ï¬ï§2ï©ï°/site EERIE, Parc Scientifique Georges Besse, 30035 NÃ®mes Cedex, France.
pascal.poncelet@ema.fr
RÃ©sumÃ©. Lâ€™extraction de motifs sÃ©quentiels frÃ©quents dans les data
streams est un enjeu important traitÃ© par la communautÃ© des chercheurs
en fouille de donnÃ©es. Plus encore que pour les bases de donnÃ©es, de
nombreuses contraintes supplÃ©mentaires sont Ã  considÃ©rer de par la na-
ture intrinsÃ¨que des streams. Dans cet article, nous proposons un nouvel
algorithme en une passe : SPAMS, basÃ© sur la construction incrÃ©mentale,
avec une granularitÃ© trÃ¨s fine par transaction, dâ€™un automate appelÃ© SPA,
permettant lâ€™extraction des motifs sÃ©quentiels dans les streams. Lâ€™infor-
mation du stream est apprise Ã  la volÃ©e, au fur et Ã  mesure de lâ€™insertion
de nouvelles transactions, sans prÃ©-traitement a priori. Les rÃ©sultats ex-
pÃ©rimentaux obtenus montrent la pertinence de la structure utilisÃ©e ainsi
que lâ€™efficience de notre algorithme appliquÃ© Ã  diffÃ©rents jeux de donnÃ©es.
1 Introduction
ConcernÃ© par de nombreux domaines dâ€™application (e.g. le traitement des donnÃ©es
mÃ©dicales, le marketing, la sÃ©curitÃ© et lâ€™analyse financiÃ¨re), lâ€™extraction de motifs sÃ©-
quentiels frÃ©quents est un domaine de recherche actif qui intÃ©resse la communautÃ© des
chercheurs en fouille de donnÃ©es. Initialement, les premiers travaux prÃ©sentÃ©s traitent
du cas des bases de donnÃ©es statiques et proposent des mÃ©thodes dites exactes dâ€™ex-
traction de motifs sÃ©quentiels. On peut citer, Ã  titre dâ€™exemple, les algorithmes GSP,
SPADE, PrefixSpan et SPAM, respectivement proposÃ©s par Srikant et Agrawal (1996);
Zaki (2001); Pei et al. (2001); Ayres et al. (2002). Plus rÃ©cemment ces derniÃ¨res annÃ©es,
de nouvelles applications Ã©mergentes, telles que lâ€™analyse de traffic dans les rÃ©seaux,
la fouille de donnÃ©es â€œclickstreamâ€1 ou encore la dÃ©tection de fraudes et dâ€™intrusions,
induisent de nouvelles problÃ©matiques qui impactent les mÃ©thodes de fouilles. En
1clickstream : flot de requÃªtes dâ€™utilisateurs sur des sites web.
Sï°ï¡ï­ï³, Sequential Patterns Automaton forMining Streams
effet, ces applications supposent la prise en compte dâ€™un nouveau type de donnÃ©es
plus connus sous lâ€™intitulÃ© data streams. Le traitement dâ€™un data stream doit satisfaire
de nouvelles contraintes. Les donnÃ©es sont gÃ©nÃ©rÃ©es rapidement de faÃ§on continue
voire illimitÃ©e et donc ne peuvent Ãªtre complÃ¨tement stockÃ©es en mÃ©moire. On ne
peut, de plus, sâ€™autoriser quâ€™un seul et unique passage sur les donnÃ©es qui doivent
Ãªtre traitÃ©es le plus rapidement possible. Ainsi, les premiÃ¨resmÃ©thodes dâ€™extraction de
motifs sÃ©quentiels ne sont plus du tout adaptÃ©es. Il a Ã©tÃ© montrÃ© dans (cf. Garofalakis
et al. (2002)) que des mÃ©thodes approchÃ©es Ã©taient tout Ã  fait adaptÃ©es au contexte
des streams. Toutefois, ces mÃ©thodes doivent trouver un compromis satisfaisant entre
les temps de rÃ©ponses, la consommation mÃ©moire et la qualitÃ© des rÃ©sultats tant en
prÃ©cision quâ€™en rappel. A lâ€™heure actuelle, il existe peu de travaux qui ont abordÃ© la
problÃ©matique de lâ€™extraction de motifs dans les streams et ils se rÃ©partissent en deux
catÃ©gories. La premiÃ¨re concerne les mÃ©thodes oÃ¹ le traitement des donnÃ©es sâ€™effectue
par batchs de transactions. On peut citer, les travaux de Kum et al. (2002); Marascu et
Masseglia (2006) avec respectivement les algoritmes ApproxMap et SMDS. Ils effec-
tuent un clustering du stream en plusieurs batchs, selon la similaritÃ© entre les motifs et
procÃ¨dent par compression de sÃ©quences similaires selon une mÃ©thode dâ€™alignement
multiple, afin de rÃ©duire la consommation mÃ©moire. Ces mÃ©thodes prÃ©sentent lâ€™in-
convÃ©nient dâ€™effectuer un traitement hors ligne du stream, car ne peuvent sâ€™exÃ©cuter
quâ€™aprÃ¨s lâ€™obtention dâ€™un groupe dedonnÃ©es. Elles requiÃ¨rent donc des tailles critiques
suffisantes pour les batchs, ce qui nâ€™est pas complÃ¨tement rÃ©aliste compte tenu des
contraintes inhÃ©rentes aux streams. La seconde catÃ©gorie concerne les mÃ©thodes qui
opÃ¨rent directement par transactions. Chang et Lee (2005) ont proposÃ© lâ€™algorithme
eISeq, basÃ© sur une structure dâ€™arbre, effectuant un traitement en ligne des donnÃ©es
par transaction parcourue en une seule passe. Plus lesmotifs sont longs Ã  traiter, moins
lâ€™algorithme est performant du fait de la phase de gÃ©nÃ©ration de tous les sous motifs.
Par exemple, si < a1, Â· Â· Â· , ai > est un motif, il y a (2i âˆ’ 1) sous motifs Ã  crÃ©er. Pour pal-
lier cette difficultÃ©, Li et Chen (2007) ont prÃ©sentÃ© lâ€™algorithme GraSeq, dÃ©veloppant
une approche Ã  partir dâ€™une structure de type graphe orientÃ© pondÃ©rÃ© permettant de
limiter la phase de gÃ©nÃ©ration de sous motifs. Toutefois, cette approche suppose un
prÃ©-traitement des transactions pour en effectuer un regroupement.
Dans cet article, nousproposonsunnouvel algorithmeune-passe :SPAMS (Sequential
Pattern Automaton for Mining Streams). Il est basÃ© sur la construction et la mise Ã 
jour incrÃ©mentale par transaction dâ€™une structure dâ€™automate : SPA (Sequential Pattern
Automaton) et permet lâ€™extraction des motifs sÃ©quentiels dans les data streams. Il ne
nÃ©cessite pas dâ€™effectuer un prÃ©-traitement visant Ã  opÃ©rer un regroupement de tran-
sactions. SPA est un automate dont, seuls sont parcourus, les Ã©tats et les transitions
qui doivent lâ€™Ãªtre, lors de lâ€™insertion des nouvelles transactions du stream. On Ã©vite,
ainsi les parcours multiples de la structure, prohibitifs et pÃ©nalisants pour les perfor-
mances des algorithmes. De plus, notre approche permet de rÃ©duire sensiblement la
gÃ©nÃ©ration des motifs candidats tout en conservant une qualitÃ© dâ€™approximation trÃ¨s
satisfaisante tant en rappel quâ€™en prÃ©cision. La suite de lâ€™article est organisÃ©e de la
maniÃ¨re suivante. La section 2 prÃ©sente formellement la problÃ©matique. La section 3
rapelle les concepts requis pour prÃ©senter notre approche qui est dÃ©crite Ã  la section 4.
Les expÃ©rimentations sont fournies Ã  la section 5 et la conclusion est proposÃ©e dans la
L. Vï©ï®ï£ï¥ï³ï¬ï¡ï³ & al.
derniÃ¨re section.
2 DÃ©finition du problÃ¨me
Dans cette section, nous donnons une dÃ©finition formelle du problÃ¨me de lâ€™extrac-
tion des motifs sÃ©quentiels dans les data streams. Tout dâ€™abord, nous rÃ©sumons la
description formelle prÃ©sentÃ©e par Srikant et Agrawal (1996), classiquement utilisÃ©e
dans le cas des bases de donnÃ©es statiques. Nous Ã©tendons ensuite la problÃ©matique
au cas des data streams.
Soit I = {i1, i2, . . . , im} un ensemble ordonnÃ© dâ€™items utilisÃ©s dans une base de
donnÃ©esDB de transactions, oÃ¹ chaque transaction tr est identifiÃ©e de maniÃ¨re unique
par un Cid, un temps et est associÃ©e Ã  un ensemble dâ€™items de I. Un ensemble X âŠ† I
est appelÃ© un itemset. Une sÃ©quence s est un ensemble dâ€™itemsets ordonnÃ©s selon leur
temps et est reprÃ©sentÃ©e par < s1s2 Â· Â· Â· sn >, oÃ¹ s j, pour j âŠ† [1..n], est un itemset. Une k-
sÃ©quence est une sÃ©quence de k items oude longueur k. Une sÃ©quence Sâ€² =< sâ€²1s
â€²
2 Â· Â· Â· s
â€²
n >
est une sous sÃ©quence dâ€™une autre sÃ©quence S =< s1 s2 Â· Â· Â· sn >, que lâ€™on note Sâ€² â‰º S
sâ€™il existe des entiers i1 < i2 < Â· Â· Â· i j Â· Â· Â· < in tels que sâ€²1 âŠ† si1 , s
â€²
2 âŠ† si2 , Â· Â· Â· , s
â€²
n âŠ† sin .
Toutes les transactions relatives Ã  unmÃªme Cid sont regroupÃ©es et triÃ©es selon leur
ordre dâ€™apparition, pour obtenir une sÃ©quence de donnÃ©es. Une sÃ©quence de donnÃ©es
contient une sÃ©quence S, si S en est une sous-sÃ©quence. Le support dâ€™une sÃ©quence S,
notÃ© supp(S), correspond au nombre dâ€™occurences de S dans DB. Pour dÃ©cider si une
sÃ©quence est frÃ©quente ou non, une valeur de support minimum, notÃ©e Ïƒ, est spÃ©cifiÃ©e
par lâ€™utilisateur. Une sÃ©quence S est dite Î¸-frÃ©quente si supp(S) â‰¥ Ïƒ, oÃ¹ Ïƒ = pÎ¸ Ã— |DB|q
avec Î¸ âˆˆ]0; 1] et |DB| la taille de la base de donnÃ©es. Etant donnÃ©e une base de donnÃ©es
de transactions de clientsCid, le problÃ¨mede lâ€™extractiondemotifs sÃ©quentiels consiste
Ã  trouver toutes les sÃ©quences, dont le support est supÃ©rieur ou Ã©gal au support
minimum fixÃ© par lâ€™utilisateur dansDB. Cette problÃ©matique, Ã©tendue au cas des data
streams, peut sâ€™exprimer simplement comme suit. Formellement, un data stream DS
peut Ãªtre dÃ©fini par une suite de transactions DS = (T1,T2, Â· Â· Â· ,T j, Â· Â· Â· ). Chacune des
transactions, identifiÃ©e par un Tid, est associÃ©e Ã  un Cid (voir par exemple la table 1).
Lâ€™extraction des motifs sÃ©quentiels frÃ©quents revient Ã  trouver toutes les sÃ©quences,
dont le support est supÃ©rieur ou Ã©gal au support minimum fixÃ© par lâ€™utilisateur pour
la partie connue du stream Ã  un moment donnÃ©.
C1 C2 C3
T1 T2 T3 T4 T5 T6 T7
bd abd acd bcd bd ab c
Tï¡ï¢. 1 â€“ Ensembles des transactions par Cid construites sur I = {a, b, c, d}.
Sï°ï¡ï­ï³, Sequential Patterns Automaton forMining Streams
3 PrÃ©requis sur les couvertures statistiques
Nous reprenons ci-dessous le thÃ©orÃ¨me proposÃ© et prouvÃ© par Laur et al. (2007)
concernant les couvertures statistiques. Les auteurs proposent de biaiser la valeur du
support fixÃ© par lâ€™utilisateur afin dâ€™obtenir des ensembles de motifs qui permettent de
maximiser les rÃ©sultats tant en rappel quâ€™en prÃ©cision.
ThÃ©orÃ¨me 1 âˆ€Î¸, 0 < Î¸ â‰¤ 1,âˆ€Î´, 0 < Î´ â‰¤ 1, soit m et mâˆ— respectivement le nombre de motifs
(Î¸-frÃ©quent et Î¸-infrequent) dans la partie connue du stream et dans tout le stream, si on
choisit  tel que :
 â‰¥
âˆš
1
2m
ln
mâˆ—
Î´
,
cela implique que le Rappel = 1 et respectivement la PrÃ©cision = 1 avec une probabilitÃ©
dâ€™au moins 1 âˆ’ Î´, aprÃ¨s suppression de tous les motifs qui ne sont pas Î¸â€²-frequent Ã  partir de
lâ€™observation, avec Î¸â€² = Î¸âˆ’  et respectivement Î¸â€² = Î¸+ . Le paramÃ¨tre Î´ est le paramÃ¨tre de
risque statistique fixÃ© par lâ€™utilisateur et les valeurs Î¸â€² = Î¸ Â±  sont les supports statistiques.
La sup-(Î¸, )-couverture reprÃ©sente le quasi-optimal plus petit ensemble de motifs
avec une probabilitÃ© dâ€™au moins 1 âˆ’ Î´ contenant tous les motifs qui sont Î¸-frÃ©quent
dans tout le stream (Ã©ventuellement infini). Il nâ€™y a pas de rÃ©sultats faux nÃ©gatifs avec
une forte probabilitÃ©. La inf-(Î¸, )-couverture reprÃ©sente le quasi-optimal plus grand
ensemble demotifs avec une probabilitÃ© dâ€™aumoins 1âˆ’Î´ contenant tous les motifs qui
sont Î¸-frÃ©quent dans tout le stream (Ã©ventuellement infini). Dans cet ensemble, il nâ€™y
a pas de rÃ©sultats faux positifs avec une forte probabilitÃ©, mais avec des faux nÃ©gatifs.
Par quasi-optimal, les auteurs expriment que toute technique obtenant de meilleures
bornes est condamnÃ©e Ã  se tromper (i.e. le critÃ¨re Ã  maximiser nâ€™est plus Ã©gal Ã  1). Ils
prÃ©cisent aussi, quâ€™ils ne font aucune hypothÃ¨se concernant la distribution du stream.
4 Lâ€™approche SPAMS
Notre approche, repose sur la construction incrÃ©mentale dâ€™une structure dâ€™au-
tomate qui permet lâ€™indexation des motifs sÃ©quentiels du stream. Le traitement du
stream sâ€™effectue avec une granularitÃ© trÃ¨s fine, par transaction et pour chaque tran-
saction item par item. Il nâ€™est point nÃ©cessaire de procÃ©der Ã  un prÃ©-traitement pour
regrouper les transactions par Cid. Aussi, nous ne prÃ©-supposons au dÃ©part de lâ€™algo-
rithme ni la connaissance de lâ€™alphabet des items ni la connaissance du nombre des
Cid du stream. Cette information est apprise incrÃ©mentalement, Ã  la volÃ©e, au fur et Ã 
mesure de lâ€™insertion des nouvelles transactions du stream. Par ailleurs, afin dâ€™obtenir
une qualitÃ© dâ€™approximation satisfaisante, tant en rappel quâ€™en prÃ©cision, en plus des
motifs sÃ©quentiels Î¸-frÃ©quents, nous indexons Ã©galement les motifs (Î¸ âˆ’ ) frÃ©quents
de la couverture statistique supÃ©rieure. On conserve donc ici, le nombre minimal de
motifs candidats supplÃ©mentaires limitant ainsi lâ€™explosion combinatoire.
L. Vï©ï®ï£ï¥ï³ï¬ï¡ï³ & al.
4.1 SPA : lâ€™automate des motifs sÃ©quentiels
Nous dÃ©finissons formellement dans cette section lâ€™automate SPA. Pour une infor-
mation dÃ©taillÃ©e sur la thÃ©orie des automates, nous suggÃ©rons la prÃ©sentation faite par
Hopcroft et Ullman (1990). SPA est un automate fini dÃ©terministe, i.e. un quintuple tel
que SPA =
(
Q, q0, F, I, Î´
)
oÃ¹
â€¢ Q est un ensemble fini dâ€™Ã©tats dont chaque Ã©tat est reprÃ©sentÃ© par un identifiant
unique associÃ© Ã  une valeur de support.
â€¢ q0 âˆˆ Q est lâ€™Ã©tat initial, dont le support correspond au nombre de clients lu pour
le stream en cours dâ€™acquisition.
â€¢ F âŠ† Q est lâ€™ensemble des Ã©tats finaux, i.e. les Ã©tats ayant un support supÃ©rieur ou
Ã©gal au support seuil.
â€¢ Î£ âŠ† I est lâ€™alphabet des items reconnus.
â€¢ Î´ : Q Ã— Î£ â†’ Q est la fonction de transition non totale permettant dâ€™indexer les
motifs sÃ©quentiels frÃ©quents du stream.
4.2 Lâ€™algorithme SPAMS
4.2.1 PrÃ©sentation
Le principe de SPAMS repose sur lâ€™idÃ©e centrale suivante concernant la construc-
tion de lâ€™automate. Nous construisons SPA en imposant la condition suivante : un Ã©tat
ne peut Ãªtre accessible que par un seul item, avec une valeur de support qui corres-
pond au nombre dâ€™occurences de chaque motif sÃ©quentiel reconnu Ã  cet Ã©tat. Si aprÃ¨s
lecture dâ€™un item la condition prÃ©cÃ©dente ne peut plus Ãªtre vÃ©rifiÃ©e pour un Ã©tat (i.e. le
nombre dâ€™occurences de certains motifs sÃ©quentiels reconnus Ã  cet Ã©tat doit Ãªtre incrÃ©-
mentÃ©), il convient de dupliquer lâ€™Ã©tat. Le nouvel Ã©tat crÃ©Ã©, recoit les seules transitions
entrantes appartenant aux motifs sÃ©quentiels dont le support doit Ãªtre incrÃ©mentÃ© et
recoit Ã©galement les transitions sortantes de lâ€™Ã©tat dupliquÃ©. AprÃ¨s initialisation de
lâ€™automate SPA, notre algorithme se dÃ©compose en trois modules principaux. Le pre-
mier, Iï®ï³ï¥ï²ï¥ï², permet la lecture et lâ€™insertion dâ€™un nouvel item acquis dans le stream.
Le second, Sïµï°ï°ï²ï©ï­ï¥ï², permet de supprimer, pendant lâ€™exÃ©cution du module Iï®ï³ï¥ï²ï¥ï²,
les Ã©tats et les transitions relatifs aux sÃ©quences devenues non (Î¸ âˆ’ )-frÃ©quentes. Le
dernier, Sïµï©ï¶ï¡ï®ï´, permet de terminer le traitement dâ€™une transaction avant de passer Ã 
la transaction suivante. Nous utiliserons le symbole â€™-â€™ comme sÃ©parateur des itemsets
Ã  lâ€™intÃ©rieur des sÃ©quences.
â€¢ Initialisation de lâ€™automate : crÃ©ation de lâ€™Ã©tat initial q0 et dâ€™un Ã©tat transitoire,
notÃ© qâˆ, utilisÃ© pendant la construction. Ce dernier nâ€™appartient pas Ã  lâ€™automate
final.
â€¢ Lecture et Insertion dâ€™un item Î± pour un client cid donnÃ© : Ã  chaque insertion
dâ€™un item Î±, il va sâ€™agir dans un premier temps de dÃ©terminer tous les Ã©tats
derriÃ¨res lesquels il faudra ajouter une transition libellÃ©e par lâ€™item Î±, et cela afin
dâ€™ indexer incrÃ©mentalement toutes les sous-sÃ©quences incluses dans la sÃ©quence
en cours de lecture. Pour cela on maintient une liste Qcid âŠ† Q correspondant aux
Ã©tats atteints lors du traitement des sÃ©quences du client cid.
Sï°ï¡ï­ï³, Sequential Patterns Automaton forMining Streams
Î£ : Alphabet de lâ€™automate.
Q : Ensemble des Ã©tats de lâ€™automate.
T : Ensemble des transitions de lâ€™automate.
Ts : Ensemble des transitions entrantes sur lâ€™Ã©tat s âˆˆ Q.
Qâˆ’ : Ensemble des Ã©tats de lâ€™automate accessibles par lâ€™item â€™-â€™.
Qcid : Ensemble des Ã©tats atteints par le client cid.
Î£cid : Alphabet du client cid.
Ta : Ensemble des transitions atteintes lors de lâ€™insertion dâ€™un item.
|s| : Support de lâ€™Ã©tat s âˆˆ Q.
s
Î±
7âˆ’â†’ sâ€² : Transition de lâ€™Ã©tat s Ã  lâ€™Ã©tat sâ€², Ã©tiquetÃ© par Î±.
C : Ensemble des identifiants client.
Tï¡ï¢. 2 â€“ Notations utilisÃ©es dans Sï°ï¡ï­ï³
1. Si cid < C, alors C = C âˆª { cid } , Qcid = { q0 } et Î£cid = âˆ….
2. Ensuite, âˆ€s âˆˆ Qcid , si @ sâ€² âˆˆ Q | s
Î±
7âˆ’âˆ’â†’ sâ€² âˆˆ T , alors T = T âˆª {s
Î±
7âˆ’âˆ’â†’ qâˆ}.
3. On rÃ©cupÃ¨re lâ€™ensemble des derniÃ¨res transitions atteintes Ta âŠ† T tel que
Ta =
{
s
Î±
7âˆ’âˆ’â†’ sâ€² âˆˆ T | s âˆˆ Qcid et sâ€² âˆˆ Q
}
.
4. Pour tout Ã©tat sâ€² tel que s
Î±
7âˆ’âˆ’â†’ sâ€² âˆˆ Ta, on effectue les opÃ©rations suivantes :
(i) Si sâ€² , qâˆ et |Tsâ€² | = |Tsâ€² âˆ© Ta| , alors Qcid = Qcid âˆª { sâ€² }.
De plus, si Î± < Î£cid alors Î£cid = Î£cid âˆª { Î± } et |sâ€²| = |sâ€²| + 1.
(ii) Sinon, i.e. sâ€² = qâˆ ou |Tsâ€² | , |Tsâ€² âˆ© Ta|, on crÃ©e un nouvel Ã©tat p et
Qcid = Qcid âˆª { p }. Si Î± < Î£cid alors Î£cid = Î£cid âˆª { Î± } et |p| = |sâ€²| + 1, sinon
|p| = |sâ€²|.
Ensuite, âˆ€sâ€²
Î²
7âˆ’â†’ z âˆˆ T, avec Î² âˆˆ Î£ et z âˆˆ Q , on a T = T âˆª
{
p
Î²
7âˆ’â†’ z
}
.
Ensuite, âˆ€z
Î±
7âˆ’â†’ sâ€² âˆˆ Tsâ€² âˆ© T
a, on a T = T \
{
z
Î±
7âˆ’â†’ sâ€²
}
âˆª
{
z
Î±
7âˆ’â†’ p
}
avec z âˆˆ Q.
â€¢ Suppression dâ€™un Ã©tat s : la suppression dâ€™un Ã©tat sde lâ€™automate, est rÃ©alisÃ©e par
le module Sïµï°ï°ï²ï©ï­ï¥ï². Elle consiste Ã  supprimer lâ€™Ã©tat s et toute sa descendance,
i.e. tous les Ã©tats et transitions accessibles Ã  partir de lâ€™Ã©tat s.
â€¢ Transaction suivante : quand tous les items Î±i dâ€™une transaction donnÃ©e (pour
un client cid) ont Ã©tÃ© lus et insÃ©rÃ©s dans lâ€™automate, le module Sïµï©ï¶ï¡ï®ï´ est appelÃ©.
Ce module fonctionne en trois Ã©tapes :
(i) rÃ©duction de lâ€™ensemble des Ã©tats atteints : Qcid = Qcid \ {Qcid âˆ© Qâˆ’ âˆª {q0}}
(ii) appel du module Iï®ï³ï¥ï²ï¥ï² sur lâ€™item â€™-â€™.
(iii) mise Ã  jour de lâ€™ensemble des Ã©tats atteints : Qcid = Qcid âˆª { q0 }.
4.2.2 Exemple de construction & pseudo-code
Afin dâ€™illustrer lâ€™exÃ©cution de Sï°ï¡ï­ï³, nous reprenons lâ€™exemple de la table 1, traitÃ©
sous forme de stream, reprÃ©sentÃ©e par le schÃ©ma de la figure 1. Nous conservons ici
L. Vï©ï®ï£ï¥ï³ï¬ï¡ï³ & al.
toute la gÃ©nÃ©ralitÃ© du stream et ne prÃ©supposons pas dâ€™ordonancement des transac-
tions par Cid. Les schÃ©mas des figures 2, 3, 5, 6, 7 et 9 illustrent la lecture et lâ€™insertion
dâ€™items (voir les explications de la section 4.2.1 pour les points sur lâ€™initialisation et la
lecture et lâ€™insertion dâ€™items). Les schÃ©mas des figures 4, 8 illustrent la fin du traite-
ment de transactions (voir les explications de la section 4.2.1 pour la partie transaction
suivante). Nous prÃ©sentons Ã  la figure 10 le pseudo-code de notre algorithme.
SPAMS
ï¸·            ï¸¸ï¸¸            ï¸·
(1, 1, b) (1, 1, d)
ï¸·                      ï¸¸ï¸¸                      ï¸·
(2, 4, b) (2, 4, c) (2, 4, d)
ï¸·                      ï¸¸ï¸¸                      ï¸·
(1, 2, a) (1, 2, b) (1, 2, d) Â· Â· Â·
Fï©ï§. 1 â€“ Exemple de data stream non ordonnÃ© (voir table 1)
q0
1
qâˆ
0
b
(i) Lecture de lâ€™item b
q0
1
qâˆ
0
q1
1B
(ii) CrÃ©ation de q1 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
1b
(iii) Automate final
Fï©ï§. 2 â€“ Lecture et insertion de lâ€™item b (transaction 1)
q0
1
qâˆ
0
q1
1b
d
d
(i) Lecture de lâ€™item d
q0
1
qâˆ
0
q1
1
q2
1
b
d
d
(ii) CrÃ©ation de q2 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
1
q2
1
b
d
d
(iii) Automate Final
Fï©ï§. 3 â€“ Lecture et insertion de lâ€™item d (transaction 1)
q0
1
qâˆ
0
q1
1
q2
1
b
d
d
âˆ’
âˆ’
(i) Lecture de lâ€™item âˆ’
q0
1
qâˆ
0
q1
1
q3
1
q2
1
b
d
d
âˆ’
âˆ’
(ii) CrÃ©ation de q3 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
1
q3
1
q2
1
b
d
d
âˆ’
âˆ’
(iii) Automate final
Fï©ï§. 4 â€“ Fin du traitement de la transaction 1
Lâ€™automate final comprend 21 Ã©tats et 27 transitions indexant tous les motifs sÃ©-
quentiels de la table 1. Par manque de place, nous ne reprÃ©sentons pas cet automate.
Sï°ï¡ï­ï³, Sequential Patterns Automaton forMining Streams
q0
1
qâˆ
0
q1
1
q3
1
q2
1
b
d
d
âˆ’
âˆ’
(i) Lecture de lâ€™item b
q0
1
qâˆ
0
q1
2
q3
1
q2
1
b
d
d
âˆ’
âˆ’
(ii) Automate final
Fï©ï§. 5 â€“ Lecture et insertion de lâ€™item b (transaction 2)
q0
1
qâˆ
0
q1
2
q3
1
q2
1
b
d
d
âˆ’
âˆ’
c
c
(i) Lecture de lâ€™item c
q0
1
qâˆ
0
q1
2
q3
1
q2
1
q4
1
b
d
d
âˆ’
âˆ’
c
c
(ii) CrÃ©ation de q4 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
2
q3
1
q2
1
q4
1
b
d
d
âˆ’
âˆ’
c
c
(iii) Automate final
Fï©ï§. 6 â€“ Lecture et insertion de lâ€™item c (transaction 2)
q0
1
qâˆ
0
q1
2
q3
1
q2
1
q4
1
b
d
d
âˆ’
âˆ’
c
c
d
(i) Lecture de lâ€™item d
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
b
d
d
âˆ’
âˆ’
c
c
d
(ii) CrÃ©ation de q5 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
b
d
d
âˆ’
âˆ’
c
c
d
(iii) Automate final
Fï©ï§. 7 â€“ Lecture et insertion de lâ€™item d (transaction 2)
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
b
d
d
âˆ’
âˆ’
c
c
d
âˆ’
âˆ’
âˆ’
âˆ’
(i) Lecture de lâ€™item âˆ’
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
q6
1
b
d
d
âˆ’
âˆ’
c
c
d
âˆ’
âˆ’
âˆ’
âˆ’
(ii) CrÃ©ation de q6 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
q6
1
b
d
d
âˆ’
âˆ’
c
c
d
âˆ’
âˆ’
âˆ’
âˆ’
(iii) Automate final
Fï©ï§. 8 â€“ Fin de traitement de la transaction 2
5 ExpÃ©rimentations
Plusieurs expÃ©rimentations ont Ã©tÃ© rÃ©alisÃ©es afin de tester lâ€™efficacitÃ© de notre ap-
proche. Des expÃ©rimentations empiriques ont Ã©tÃ© faites sur des jeux de donnÃ©es syntÃ©-
L. Vï©ï®ï£ï¥ï³ï¬ï¡ï³ & al.
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
q6
1
b
d
d
âˆ’
âˆ’
c
c
d
âˆ’
âˆ’
âˆ’
âˆ’
a
a
(i) Lecture de lâ€™item a
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
q6
1
q7
1
b
d
d
âˆ’
âˆ’
c
c
d
âˆ’
âˆ’
âˆ’
âˆ’
a
a
(ii) CrÃ©ation de q7 et dÃ©placement des
transitions atteintes
q0
1
qâˆ
0
q1
2
q3
1
q2
2
q4
1
q5
1
q6
1
q7
1
b
d
d
âˆ’
âˆ’
c
c
d
âˆ’
âˆ’
âˆ’
âˆ’
a
a
(iii) Automate final
Fï©ï§. 9 â€“ Lecture et insertion de lâ€™item a (transaction 3)
tiques gÃ©nÃ©rÃ©s Ã  partir du simulateur IBM2. Nous avons fait varier le nombre de clients
D, le nombre de transactions par client C, le nombre moyen dâ€™items par transaction T,
la taille moyenne des sÃ©quences maximales S et la taille moyenne des transactions des
sÃ©quences maximales I. Nous avons utilisÃ© une implÃ©mentation en C++ de SPAMS
compilÃ©e avec lâ€™option -03 du compilateur g++, sur un Intel Pentium D, 2Go ram.
Nous illustrons sur les figures 11-(i) et 11-(ii) les performances en temps et en consom-
mation mÃ©moire de SPAMS, pour diffÃ©rentes valeurs de support, sur les jeux de don-
nÃ©esdepetite,moyenne et grande taille, respectivementD7C7T7S7I7,D8C10T10S10I10
et D18C18T18S18I18. Les figures 11-(iii) Ã  11-(v) reprÃ©sentent lâ€™Ã©volution du temps,
de la mÃ©moire et du nombre de Cid en fonction du nombre de transactions sur
D18C18T18S18I18 avec un support fixÃ© Ã  20%.
La figure 11-(vi) permet dâ€™illustrer sur le jeu de donnÃ©es D18C18T18S18I18 que le
support statistique utilisÃ© tend vers le support seuil au fur et Ã  mesure de lâ€™acquisition
des transactions, diminuant ainsi lâ€™ensemble des motifs (Î¸ âˆ’ )-frÃ©quents de la cou-
verture statistique. Pour le calcul de  (voir section 3), nous avons choisi la valeur de
0.01 pour le risque statistique Î´. Ces expÃ©rimentations montrent que nous trouvons
un compromis trÃ¨s satisfaisant entre les performances temporelles, la consommation
mÃ©moire et la qualitÃ© des rÃ©sultats de lâ€™extraction tant en prÃ©cision quâ€™en rappel. Elles
montrent aussi, lâ€™applicabilitÃ© et le passage Ã  lâ€™Ã©chelle de lâ€™algorithme SPAMS.
6 Conclusion
Dans cet article, nous apportons une contribution originale en proposant un nouvel
algorithme une-passe : SPAMS, basÃ© sur lâ€™Ã©laboration dâ€™un nouvel automate nommÃ©
SPA, qui permet de traiter de faÃ§on efficace la problÃ©matique de lâ€™extraction des motifs
sÃ©quentiels frÃ©quents dans les data streams. SPA prÃ©sente des propriÃ©tÃ©s incrÃ©men-
tales qui permettent son initialisation sans aucune information. Sa mise Ã  jour, dans
le cas des data streams, se fait avec une granularitÃ© trÃ¨s fine par transaction, sans
quâ€™il soit nÃ©cessaire de procÃ©der Ã  un prÃ©-traitement pour regrouper les transactions
par client Cid. Par ailleurs, chaque transaction est acquise item par item et nous ne
prÃ©-supposons au dÃ©part de lâ€™algorithme ni la connaissance de lâ€™alphabet des items ni
2simulateur disponible Ã  lâ€™adresse http://www.almaden.ibm.com/cs/quest
Sï°ï¡ï­ï³, Sequential Patterns Automaton forMining Streams
Algorithme 1 â€“ Sï°ï¡ï­ï³
donnÃ©es : Stream, Î¸
so r t i e : ï³ï°ï¡Î¸
dÃ©but
CrÃ©er les Ã©tats q0 et qâˆ
Tâ† âˆ… ; Qâ†
{
q0, qâˆ
}
|q0| â† 1 ; |qâˆ| â† 0 ; cidâ† null
tidâ† null ; Câ† âˆ… ; min_supâ† 0
pour chaque (cidâ€², tidâ€², Î±) âˆˆ Stream f a i r eï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
si (cid , cidâ€²) ou (tid , tidâ€²) alorsâŒŠ
Sïµï©ï¶ï¡ï®ï´(cid, cidâ€²)
cidâ† cidâ€²; tidâ† tidâ€²
Iï®ï³ï¥ï²ï¥ï²(Î±, cid)
f in
Algorithme 2 â€“ Sïµï©ï¶ï¡ï®ï´
donnÃ©es : cid, cidâ€²
dÃ©but
s i cid , null a lorsâŒŠ
Iï®ï³ï¥ï²ï¥ï²(â€²âˆ’â€², cid)
Qcid â† Qcid âˆ©Q
âˆ’ âˆª { q0 }
s i cidâ€² < C a lorsï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Câ† Câˆª { cidâ€² }
Qcidâ€² â† { q0 }
â†
âˆš
1
2.|C| ln
|C|
Î´
min_supâ† d(Î¸ âˆ’ ) Ã— |C|e
f in
Algorithme 3 â€“ Sïµï°ï°ï²ï©ï­ï¥ï²
donnÃ©es : s
dÃ©but
Â· Supprimer les transitions entrantes de s
pour chaque s
Î²
7âˆ’â†’ sâ€² âˆˆ T f a i r eâŒŠ
Sïµï°ï°ï²ï©ï­ï¥ï²(sâ€²)
Â· Supprimer lâ€™Ã©tat s
f in
Algorithme 4 â€“ Iï®ï³ï¥ï²ï¥ï²
donnÃ©es : Î±, cid
dÃ©but
Ta â† âˆ…
pour chaque s âˆˆ Qcid f a i r eï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
si âˆƒ sâ€² âˆˆ Q | s
Î±
7âˆ’â†’ sâ€² âˆˆ T alorsâŒŠ
Ta â† Ta âˆª
{
s
Î±
7âˆ’â†’ sâ€²
}
sinonâŒŠ
Ta â† Ta âˆª
{
s
Î±
7âˆ’â†’ qâˆ
}
Qâ€² â†
{
sâ€² âˆˆ Q | s
Î±
7âˆ’â†’ sâ€² âˆˆ Ta
}
pour chaque sâ€² âˆˆ Qâ€² f a i r eï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
si |sâ€²| + 1 >= min_sup alorsï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
si sâ€² , qâˆ et |Tsâ€² âˆ© Ta| = |Tsâ€² | alorsï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
si Î± < Î£cid alorsâŒŠ
|sâ€²| â† |sâ€²| + 1
Î£cid â† Î£cid âˆª {Î±}
sinonï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Â· CrÃ©Ã©r un Ã©tat sâ€²â€²
Â· Qâ† Qâˆª {sâ€²â€²}
si Î± < Î£cid or sâ€² = qâˆ alorsâŒŠ
|sâ€²â€²| â† |sâ€²| + 1
Î£cid â† Î£cid âˆª {Î±}
Â· Dupliquer les transitions sortantes
de sâ€² sur sâ€²â€²
Â· DÃ©placer les transitions Tsâ€² âˆ© Ta
de sâ€² vers sâ€²â€²
Ta â† Ta \ {Tsâ€² âˆ© T
a}
si |sâ€²| < min_sup alors Sïµï°ï°ï²ï©ï­ï¥ï²(sâ€²)
f in
Fï©ï§. 10 â€“ Pseudo-code de lâ€™algorithme Sï°ï¡ï­ï³
la connaissance du nombre des clients du stream. Cette information est apprise incrÃ©-
mentalement, Ã  la volÃ©e, au fur et Ã mesure de lâ€™insertion des nouvelles transactions du
stream. Les expÃ©rimentations montrent que nous trouvons un compromis satisfaisant,
en limitant la phase de gÃ©nÃ©ration des candidats avec la couverture statistique, tout en
conservant de bons rÃ©sultats tant en rappel quâ€™en prÃ©cision avec Ã©galement de bonnes
performances en temps et enmÃ©moire. Plusieurs perspectives peuvent Ãªtre envisagÃ©es
Ã  la suite de ce travail. Cela concerne, par exemple, la suppression incrÃ©mentale de
motifs sÃ©quentiels dans lâ€™automate, ou encore la reprÃ©sentation de motifs tels que les
L. Vï©ï®ï£ï¥ï³ï¬ï¡ï³ & al.
motifs fermÃ©s et maximaux.
RÃ©fÃ©rences
Ayres, J., J. Gehrke, T. Yiu, et J. Flannick (2002). Sequential pattern mining using a
bitmap representation. pp. 429â€“435. ACM Press.
Chang, J. H. et W. S. Lee (2005). Efficient mining method for retrieving sequential
patterns over online data streams. J. Inf. Sci. 31(5), 420â€“432.
Garofalakis, M., J. Gehrke, et R. Rastogi (2002). Querying and mining data streams :
you only get one look a tutorial. In SIGMOD â€™02 : Proceedings of the 2002 ACM
SIGMOD international conference on Management of data, New York, NY, USA, pp.
635â€“635. ACM.
Hopcroft, J. E. et J. D. Ullman (1990). Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley Longman Publishing Co., Inc.
Kum, H., J. Pei, W.Wang, et D. Duncan (2002). Approxmap : Approximate mining of
consensus sequential patterns.
Laur, P.-A., R.Nock, J.-Ã‰. Symphor, et P. Poncelet (2007). Mining EvolvingData Streams
for Frequent Patterns. Pattern Recognition 40(2), 492â€“503.
Li, H. et H. Chen (2007). Graseq : A novel approximate mining approach of sequential
patterns over data stream. In ADMA, pp. 401â€“411.
Marascu, A. et F. Masseglia (2006). Extraction de motifs sÃ©quentiels dans les flots de
donnÃ©es dâ€™usage du web. In EGC, pp. 627â€“638.
Pei, J., J. Han, B.Mortazavi-asl, H. Pinto, Q. Chen, U. Dayal, etM. chunHsu (2001). Pre-
fixspan : Mining sequential patterns efficiently by prefix-projected pattern growth.
pp. 215â€“224.
Srikant, R. et R. Agrawal (1996). Mining sequential patterns : Generalizations and
performance improvements. pp. 3â€“17.
Zaki, M. J. (2001). Spade : an eficient algorithm for mining frequent sequences. In
Machine Learning Journal, special issue on Unsupervised Learning, pp. 31â€“60.
Summary
Mining sequential patterns on data streams is a new challenging problem for the
datamining community since data arrives sequentially in the form of continuous rapid
streams. More still than for databases, many additional constraints have to be con-
sidered due to the intrinsic nature of the streams. In this paper, we propose a new
one-pass algorithm named: SPAMS, based on the incremental updating of an automa-
ton structure: SPA, for mining sequential patterns in data streams. The information
of the stream is learned progressively, from the insertion of new transactions, without
preprocessing step a priori. The experimental results obtained show the relevance of
the structure used, as well as the efficiency of our algorithm applied on datasets.
Sï°ï¡ï­ï³, Sequential Patterns Automaton forMining Streams
 1
 10
 100
 1000
 10000
 0.1  0.15  0.2  0.25  0.3
te
m
p
s
 e
n
 s
e
c
o
n
d
e
s
support
Temps de construction de l'automate
D7C7T7S7I7
D8C10T10S10I10
D18C18T18S18I18
(i) temps
 10000
 100000
 1e+06
 0.1  0.15  0.2  0.25  0.3m
e
m
o
ir
e
 v
ir
tu
e
lle
 m
a
x
im
a
le
 (
K
o
)
support
Consommation memoire de SPAMS
D7C7T7S7I7
D8C10T10S10I10
D18C18T18S18I18
(ii) mÃ©moire
 10
 100
 0  5  10  15  20  25  30
te
m
p
s
 e
n
 s
e
c
o
n
d
e
s
transactions (x 10000)
D18C18T18S18I18 (Î¸=0.2)
SPAMS
(iii) Temps sur D18C18T18S18I18
 100000
 1e+06
 0  5  10  15  20  25  30m
e
m
o
ir
e
 v
it
u
e
lle
 m
a
x
im
a
le
 (
K
o
)
transactions (x 10000)
D18C18T18S18I18 (Î¸=0.2)
SPAMS
(iv) MÃ©moire sur D18C18T18S18I18
 100
 1000
 10000
 100000
 0  5  10  15  20  25  30
n
o
m
b
re
 d
e
 c
lie
n
ts
transactions (x 10000)
D18C18T18S18I18 (Î¸=0.2)
SPAMS
(v) Acquisition des clients sur D18C18T18S18I18
 0.01
 0.1
 1
 0  5  10  15  20  25  30
v
a
le
u
r
transactions (x 10000)
D18C18T18S18I18 (Î¸=0.2)
Î¸ - Îµ
Î¸
Îµ
(vi) Ã‰volution du support statistique sur
D18C18T18S18I18
 0.1
 1
 0.1  0.15  0.2  0.25
v
a
le
u
r
support
D18C18T18S18I18
Rappel
Precision
(vii) Rappel et prÃ©cision sur D18C18T18S18I18
 10
 100
 1000
 0  5  10  15  20  25  30n
o
m
b
re
 d
'e
ta
ts
 e
t 
d
e
 t
ra
n
s
it
io
n
s
transactions (x 10000)
D18C18T18S18I18 (Î¸=0.2)
etats
transitions
(viii) Nombre dâ€™Ã©tats et de transitions sur
D18C18T18S18I18
Fï©ï§. 11 â€“ ExpÃ©rimentations de SPAMS
