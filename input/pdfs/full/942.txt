MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
Sujeevan Aseervatham, Emmanuel Viennet
UniversitÃ© de Paris-Nord, LIPN - UMR CNRS 7030
99, avenue Jean-Baptiste ClÃ©ment
93430 Villetaneuse, France
{PrÃ©nom.Nom}@lipn.univ-paris13.fr
RÃ©sumÃ©. Cet article Ã©bauche un Ã©tat de lâ€™art sur lâ€™utilisation des noyaux pour le
traitement des donnÃ©es structurÃ©es. Les applications modernes de la fouille de
donnÃ©es sont de plus en plus confrontÃ©s Ã  des donnÃ©es structurÃ©es, notamment
textuelles. Les algorithmes dâ€™apprentissage doivent donc Ãªtre capables de tirer
parti des informations apportÃ©es par la structure, ce qui pose dâ€™intÃ©ressants pro-
blÃ¨mes de reprÃ©sentation des donnÃ©es. Lâ€™une des approches possibles consiste
Ã  utiliser les noyaux de Mercer. Ces noyaux permettent de calculer la similaritÃ©
entre deux donnÃ©es de type quelconque, et peuvent Ãªtre utilisÃ©s par une large
gamme dâ€™algorithmes dâ€™apprentissage (Machines Ã  Vecteur de Support, ACP,
Analyse Discriminante, Perceptron, etc). Nous prÃ©sentons dans cet article les
principaux noyaux proposÃ©s ces derniÃ¨res annÃ©es pour le traitement des struc-
tures telles que les sÃ©quences, les arbres et les graphes.
1 Introduction
Les techniques dâ€™apprentissage statistique sont gÃ©nÃ©ralement conÃ§ues pour travailler sur
des donnÃ©es vectorielles ; chaque mesure est reprÃ©sentÃ©e par un ensemble de donnÃ©es numÃ©-
riques de taille fixe. Pendant plusieurs dÃ©cennies, les recherches en statistique se sont centrÃ©es
sur des problÃ¨mes comme la normalisation des donnÃ©es, le traitement des valeurs manquantes,
etc. Depuis une dizaine dâ€™annÃ©es, sous la pression des applications, nous sommes confrontÃ©s Ã 
des problÃ¨me dans lesquels la structure des donnÃ©es porte une information essentielle : textes
en langage naturel, documents XML, sÃ©quences biologiques, analyse de scÃ¨nes (images), ana-
lyse des rÃ©seaux sociaux. Pour attaquer ces problÃ¨mes, il est nÃ©cessaire trouver un moyen de
traiter lâ€™information structurelle, par exemple en calculant une mesure de similaritÃ© entre deux
structures.
De nombreux systÃ¨mes dâ€™apprentissage numÃ©rique des donnÃ©es textuelles utilisent une
reprÃ©sentation du texte en "sac de mot". Ce type de codage, qui a lâ€™avantage de la simplicitÃ©,
nâ€™utilise que les frÃ©quences dâ€™apparition des mots dans les documents et perd toute information
liÃ©e Ã  lâ€™ordre des Ã©lÃ©ments (ordre des mots, structure en paragraphes ou sections, etc).
Depuis une petite dizaine dâ€™annÃ©es, une nouvelle famille dâ€™algorithmes dâ€™apprentissage
basÃ©s sur la notion de noyaux, fait lâ€™objet dâ€™intenses recherches. Les noyaux, proposÃ©s par V.
Vapnik pour les machines Ã  vecteur de support (SVM) (Vapnik, 1995), permettent de dÃ©finir des
mesures de similaritÃ© non linÃ©aires. En simplifiant, la fonction noyau calcule un produit scalaire
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
(utilisÃ© commemesure de similaritÃ©) entre deux Ã©lÃ©ments Ã  traiter. Or, de nombreuses mÃ©thodes
dâ€™apprentissage statistique peuvent se formuler en ne recourant quâ€™Ã  des calculs de produits
scalaires entre les Ã©lÃ©ments (exemples dâ€™aprentissage ou nouveaux points). Lâ€™utilisation dâ€™une
fonction noyau permet alors dâ€™utiliser ces mÃ©thodes bien connues et performantes (analyse
discriminante, analyse en composantes principales, perceptron, etc) avec un traitement intÃ©grÃ©
qui peut Ãªtre non linÃ©aire et incorporer des connaissances sur lâ€™application (par exemple sur la
structure des donnÃ©es). RÃ©cemment, lâ€™utilisation de noyaux spÃ©cifiques pour le traitement de
donnÃ©es textuelles structurÃ©es a commencÃ© Ã  faire lâ€™objet de recherches.
Dans le cadre du projet InfoM@gic (pÃ´le IMVN Cap Digital), nous travaillons sur lâ€™ap-
plication des mÃ©thodes Ã  noyaux au traitement de donnÃ©es textuelles structurÃ©es, et cet article
prÃ©sente un bref Ã©tat de lâ€™art dans ce domaine. Nous passons en revue les principaux types de
noyaux proposÃ©s ces derniÃ¨res annÃ©es pour le traitement des sÃ©quences et plus gÃ©nÃ©ralement
des donnÃ©es structurÃ©es (arbres, graphes, etc).
2 Le noyau de convolution
Le noyau de convolution appelÃ© R-noyau (Haussler, 1999) permet de dÃ©finir un cadre gÃ©-
nÃ©ral pour les noyaux appliquÃ©s aux donnÃ©es structurÃ©es telles que les arbres et les graphes.
Les donnÃ©es structurÃ©es sont des objets pouvant Ãªtre dÃ©composÃ©s en sous-objets jusquâ€™Ã  at-
teindre une unitÃ© atomique. Lâ€™idÃ©e du R-noyau est de calculer la similaritÃ© entre deux Ã©lÃ©ments
x et y en effectuant une dÃ©composition de x et de y. Plus formellement, soit x un Ã©lÃ©ment
appartenant Ã  un ensemble X dâ€™Ã©lÃ©ments structurÃ©s de mÃªme type, x peut Ãªtre dÃ©composÃ© en
sous Ã©lÃ©ments (x1, . . . , xD) oÃ¹ xd peut Ãªtre un Ã©lÃ©ment structurÃ© ou non appartenant Ã  Xd. On
dÃ©finit la relation binaire
R : X1 Ã— . . .Ã—XD â†’ X
qui associe les parties dâ€™un Ã©lÃ©ment x Ã  x et la relation inverse
Râˆ’1 : {x = (x1, . . . , xD)|R(x, x)}
qui retourne pour x lâ€™ensemble de toutes les dÃ©compositions possibles. Le noyau de convolu-
tion (R-noyau) pour deux Ã©lÃ©ments x et y de X est alors :
kR(x, y) =
âˆ‘
xâˆˆRâˆ’1(x)
âˆ‘
yâˆˆRâˆ’1(y)
dâˆ
i=1
ki(xi, yi)
avec ki un noyau calculant la similaritÃ© entre les Ã©lÃ©ments xi et yi de mÃªme structure i.e.
xi, yi âˆˆ Xi. Il est facile de montrer que si les ki sont des noyaux valides alors kR est valide.
En effet, si ki est valide alors sa matrice de Gram est semi-dÃ©finie positive et le produit et la
somme de matrices semi-dÃ©finies positives sont des matrices semi-dÃ©finies positives.
De plus, le R-noyau peut Ãªtre dÃ©fini par rÃ©currence lorsque les parties xi dâ€™un Ã©lÃ©ment
x sont de mÃªme type de structure (xi, x âˆˆ X ). Le critÃ¨re dâ€™arrÃªt est dÃ©fini pour lâ€™Ã©lÃ©ment
atomique (par exemple une feuille pour une structure arborescente).
La possibilitÃ© de dÃ©composer le calcul de la similaritÃ© dâ€™Ã©lÃ©ments permet de traiter aisÃ©ment
des structures complexes. Toutefois, elle nÃ©cessite, en contre-partie, un temps de calcul non
S. Aseervatham et E. Viennet
nÃ©gligeable. Ainsi, il est nÃ©cessaire de spÃ©cialiser ce noyau selon le type de structure afin de
rÃ©duire la complexitÃ©.
3 Les noyaux pour les sÃ©quences de caractÃ¨res
Les sÃ©quences de caractÃ¨res sont considÃ©rÃ©es comme faisant partie des donnÃ©es structurÃ©es
car dâ€™une part une sÃ©quence peut Ãªtre dÃ©composÃ©e en sous-partie ainsi les sÃ©quences possÃ¨dent
la propriÃ©tÃ© des donnÃ©es structurÃ©es vue dans le paragraphe prÃ©cÃ©dent et dâ€™autre part les carac-
tÃ¨res de la sÃ©quence sont ordonnÃ©s.
Les sÃ©quences sont gÃ©nÃ©ralement rencontrÃ©es dans les domaines liÃ©s Ã  la bioinformatique mais
aussi dans les documents en langage naturel. En effet, on peut considÃ©rer tout un document
comme Ã©tant une sÃ©quence. Ainsi, deux documents peuvent Ãªtre considÃ©rÃ©s comme proches
sâ€™ils partagent un nombre important de sous-sÃ©quences identiques. Cette approche permet alors
de tenir compte des mots composÃ©s comme par exemple â€œÃ©conomieâ€ et â€œmicroÃ©conomieâ€ qui
ne peuvent Ãªtre traitÃ©s par lâ€™approche en sac de mots. De plus, les mots composÃ©s sont trÃ¨s
prÃ©sents dans le domaine de la chimie et les domaines connexes oÃ¹ il nâ€™est pas rare dâ€™avoir des
noms de molÃ©cules composÃ©es.
Bien que cette approche donne de meilleurs rÃ©sultats que lâ€™approche classique en sac de
mots, elle nâ€™en reste pas moins coÃ»teuse en temps de calcul.
3.1 Le noyau p-Spectrum
Le noyau p-spectrum (ou n-gram) (Leslie et al., 2002; Lodhi et al., 2002) est le noyau le
plus simple pour le traitement de sÃ©quences. Il permet dâ€™Ã©valuer le nombre de sous-sÃ©quences
contiguÃ«s de taille p (ou n) que deux documents ont en communs. Plus le nombre de sous-
sÃ©quences en commun est important et plus la similaritÃ© des deux documents sera importante.
Soit lâ€™alphabet Î£, lâ€™espace associÃ© au noyau p-spectrum sera de dimension card(Î£)p. Le uÃ¨me
composant du vecteur Î¦p(s) associÃ© Ã  la sÃ©quence s avec u âˆˆ Î£P est :
Î¦pu(s) = card({(v1, v2)|s = v1uv2})
Avec v1uv2 dÃ©signant la concatÃ©nation des sÃ©quences v1, u et v2.
Le noyau p-spectrum est alors :
kp(s1, s2) = ã€ˆÎ¦p(s1),Î¦p(s2)ã€‰
La complexitÃ© de ce noyau est O(p|s1||s2|). Toutefois, il est possible de rÃ©duire cette com-
plexitÃ© Ã  O(pÃ—max(|s1|, |s2|)) en utilisant la programmation dynamique et des structures de
donnÃ©es appropriÃ©es comme les arbres de suffixes (Shawe-Taylor et Cristianini, 2004).
Le noyau p-Spectrum a Ã©tÃ© utilisÃ© par Leslie et al. (2002) pour la classification de sÃ©quences
de protÃ©ines avec lâ€™algorithme SVM (SÃ©parateur Ã  Vaste Marge). Les rÃ©sultats obtenus sur la
base de donnÃ©es SCOP (Structural Classification of Proteins) ont montrÃ© que la classification
par SVM avec le noyau p âˆ’ Spectrum donne des rÃ©sultats semblables aux mÃ©thodes gÃ©nÃ©-
ratives basÃ©es sur les modÃ¨les de Markov cachÃ©s. Cependant, la mÃ©thode SVM avec le noyau
Fisher reste la plus performante pour la classification de sÃ©quences de protÃ©ines.
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
3.2 Le noyau All-SubSequences
Le noyau All-SubSequences (Shawe-Taylor et Cristianini, 2004) permet de tenir compte
des sous-sÃ©quences non contiguÃ«s de toutes tailles. Ainsi, la fonction Î¦A(s) permet de plon-
ger la sÃ©quence s dans un espace vectoriel dans lequel le uÃ¨me composant de Î¦A(s) indique
la frÃ©quence dâ€™occurrence de la sÃ©quence u dans s. On dira que u est une sous-sÃ©quence non
contiguÃ« de s, sâ€™il existe un ensemble I = {i1, . . . , i|u|} tel que âˆ€j âˆˆ {2, . . . , |u|}, ijâˆ’1 < ij
et âˆ€j âˆˆ {1, . . . , |u|}, u(j) = s(ij) (u(j) dÃ©signant le jÃ¨me Ã©lÃ©ment de u). On notera s[I] = u
pour dÃ©signer le fait que chaque Ã©lÃ©ment de u(j) est identique Ã  lâ€™Ã©lÃ©ment s(ij) avec ij âˆˆ I .
Î¦Au (s) =
âˆ‘
I:s[I]=u
1
Dâ€™oÃ¹ :
kA(s, t) =
âˆ‘
(I1,I2):s[I1]=t[I2]
1
Il est possible de dÃ©finir ce noyau de maniÃ¨re rÃ©cursive. En effet, il suffit de remarquer
que toute sous-sÃ©quence de s contenant le dernier caractÃ¨re a de s, tel que s = sâ€²a, ne peut
apparaÃ®tre dans t quâ€™entre le premier caractÃ¨re de t et la derniÃ¨re occurrence de a dans t. Ainsi,
on a :
kA(s, ) = 1
kA(sâ€²a, t) = kA(sâ€², t) +
âˆ‘
k:t[k]=a
kA(sâ€², t[1 . . . k âˆ’ 1])
Lâ€™avantage de ce noyau est quâ€™il est capable de capturer tous les motifs communs Ã  deux
sÃ©quences. Le dÃ©savantage est que lâ€™espace de projection est de trÃ¨s haute dimension entraÃ®nant
un temps de calcul important.
3.3 Le noyau p-Fixed length SubSequence
Le noyau p-Fixed length SubSequence (Shawe-Taylor et Cristianini, 2004) est un compro-
mis entre le noyau p-Spectrum et le noyau All-SubSequences. Il permet de limiter la recherche
de sous-sÃ©quences Ã  des sous-sÃ©quences non contiguÃ«s de taille p. Ainsi, la fonction de projec-
tion Î¦F (s) sera composÃ©e des Ã©lÃ©ments Î¦Au (s) tels que |u| = p.
De mÃªme que prÃ©cÃ©demment, on pourra dÃ©finir ce noyau par rÃ©currence en notant que la rÃ©cur-
rence sera dÃ©finie sur la sÃ©quence, en retirant Ã  chaque Ã©tape le dernier Ã©lÃ©ment de la sÃ©quence,
mais aussi sur la taille p du motif. En effet, si le dernier caractÃ¨re du motif a Ã©tÃ© fixÃ©, le prÃ©fixe
du motif ne peut Ãªtre constituÃ© que de pâˆ’ 1 Ã©lÃ©ments.
k0(s, t) = 1
kp(s, ) = 0 pour p > 0
kp(sâ€²a, t) = kp(sâ€², t) +
âˆ‘
k:t[k]=a
kpâˆ’1(sâ€², t[1 . . . k âˆ’ 1])
S. Aseervatham et E. Viennet
3.4 Le noyau String Subsequence (SSK)
Lâ€™un des inconvÃ©nients des noyaux traitant les sous-sÃ©quences non contiguÃ«s vus prÃ©cÃ©-
demment est quâ€™ils ne tiennent pas compte de la distance sÃ©parant les Ã©lÃ©ments non contiguÃ«.
En effet, prenons lâ€™exemple de deux sÃ©quences "aaab" et "aab", la sÃ©quence "ab" est une sous-
sÃ©quence des deux premiÃ¨res mais elle est plus similaire Ã  la deuxiÃ¨me quâ€™Ã  la premiÃ¨re. Or,
les noyaux All-SubSequences et p-Fixed length SubSequence attribueront la mÃªme valeur aux
couples ("aaab", "ab") et ("aab", "ab").
Le noyau String Subsequence (Lodhi et al., 2002) permet de tenir compte de la disconti-
nuitÃ© dans le calcul de la similaritÃ© en pondÃ©rant les sÃ©quences en fonction de leur taille. Pour
une sÃ©quence s, la fonction de projection Î¦SSK(s) sera dÃ©fini pour tout u âˆˆ Î£n par :
Î¦SSKu (s) =
âˆ‘
I:u=s[I]
Î»card(I)
Le noyau SSK, de paramÃ¨tre n, pour deux sÃ©quences s et t est alors :
knSSK(s, t) =
âˆ‘
uâˆˆÎ£n
âˆ‘
I:u=s[I]
âˆ‘
J:u=t[J]
Î»card(I)+card(J)
Comme pour les noyaux prÃ©cÃ©dents, en utilisant la programmation dynamique, on peut rÃ©-
duire la complexitÃ© Ã  O(n.|s|.|t|).
Des expÃ©rimentations ont Ã©tÃ© menÃ©es dans (Lodhi et al., 2002) pour Ã©valuer les noyaux
SSK, p-Spectrum et le noyau standard Bag Of Words (Joachims, 2002). La base de donnÃ©es
utilisÃ©e est la base Reuters-21578 contenant des documents en langage naturel. Lâ€™expÃ©rience
consistait Ã  effectuer un classement binaire des documents aprÃ¨s avoir effectuÃ© un apprentis-
sage sur les donnÃ©es prÃ©vues Ã  cet effet. Les documents ont Ã©tÃ© prÃ©-traitÃ©s en Ã©liminant les
mots dâ€™arrÃªts et les signes de ponctuations. Les rÃ©sultats ont montrÃ©s que les string kernels
sont plus performants que lâ€™approche standard du Bag Of Words. De plus, le noyau SSK est le
plus performant lorsque la valeur de pondÃ©ration est choisie judicieusement. De mÃªme, lorsque
la taille p est choisie convenablement, le noyau p-Spectrum donne les meilleurs rÃ©sultats.
Les deux inconvÃ©nients pour lâ€™utilisation de ces noyaux sont dâ€™une part le temps de calcul et
dâ€™autre part le choix des paramÃ¨tres qui doit Ãªtre fait de maniÃ¨re spÃ©cifique Ã  chaque applica-
tion.
3.5 Le noyau SÃ©quence marginalisÃ©e
Le noyau marginalisÃ© pour les sÃ©quences a Ã©tÃ© introduit par Kashima et Tsuboi (2004) afin
dâ€™Ã©tiqueter des structures complexes tels que les sÃ©quences, les arbres et les graphes.
Le problÃ¨me de lâ€™Ã©tiquetage consiste Ã  affecter Ã  une donnÃ©e x = (x1, . . . , xT ) un groupe
dâ€™Ã©tiquettes y = (y1, . . . , yT ) âˆˆ Î£Ty . Par exemple, en langage naturel x peut reprÃ©senter une
phrase, et le problÃ¨me consiste Ã  attribuer Ã  chaque mot de x une Ã©tiquette dÃ©signant son groupe
grammatical (Part Of Speech tagging).La figure 1(a) montre un exemple de couple (x, y) oÃ¹
les nÅ“uds noirs reprÃ©sentent les Ã©lÃ©ments xi et les nÅ“uds blancs les Ã©tiquettes yi associÃ©es Ã 
xi.
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
Pour rÃ©soudre le problÃ¨me de lâ€™Ã©tiquetage, lâ€™approche standard consiste Ã  attribuer Ã  x la sÃ©-
quence dâ€™Ã©tiquettes y tel que : y=argmaxy
âˆ‘
i logP (yi|xi). Il sâ€™agit ici de maximiser la pro-
babilitÃ© que la sÃ©quence entiÃ¨re soit correcte. Une autre approche consiste Ã  maximiser indivi-
duellement la probabilitÃ© quâ€™une Ã©tiquette yi corresponde Ã  lâ€™Ã©lÃ©ment xi. Soit :
yi = argmaxyP (yi = y|xi) = argmaxy
âˆ‘
y:yi=y
P (y|x)
Partant de cette derniÃ¨re approche, Kashima et Tsuboi (2004) proposent une mÃ©thode
pour Ã©tiqueter une sÃ©quence. Cette mÃ©thode se base sur lâ€™utilisation dâ€™un perceptron Ã  noyau
(Shawe-Taylor et Cristianini, 2004).
Afin dâ€™Ã©tiqueter les Ã©lÃ©ments individuellement, une sÃ©quence (x,y) est dÃ©composÃ©e en triplets
(x, t, yt) oÃ¹ yt est le tÃ¨me Ã©lÃ©ment de x. Pour cela, le perceptron Ã  noyau est entraÃ®nÃ© avec des
sÃ©quences Ã©tiquetÃ©es auxquelles ont Ã©tÃ© ajoutÃ©s des exemples nÃ©gatifs. Ces exemples nÃ©gatifs
sont obtenus pour chaque couple (x,y) en modifiant les valeurs de y. Ainsi pour ce couple
on obtient un ensemble de positifs {(x, i, yi) : 1 â‰¤ i â‰¤ dim(x)} et un ensemble de nÃ©gatifs
{(x, i, z) : 1 â‰¤ i â‰¤ dim(x) âˆ€z âˆˆ Î£y}. AprÃ¨s apprentissage, on affecte Ã  un Ã©lÃ©ment ut dâ€™une
sÃ©quence u lâ€™Ã©tiquette y qui maximise le score calculÃ© par le perceptron avec un noyau margi-
nalisÃ©.
Le noyau marginalisÃ© proposÃ© est le suivant :
k(x,xâ€², Ï„, t, yÏ„ , yâ€²t) =
âˆ‘
z:zÏ„=yÏ„
âˆ‘
zâ€²:zâ€²t=y
â€²
t
P (z|x)P (zâ€²|xâ€²) ã€ˆÎ¦(x, z; Ï„),Î¦(xâ€², zâ€²; t)ã€‰ (1)
yÏ„ et yâ€²t Ã©tant les Ã©tiquettes respectives des Ã©lÃ©ments xÏ„ et x
â€²
t ; Ï†f (x,y; t) indiquant la frÃ©-
quence dâ€™occurrence de f dans (x,y) incluant la tÃ¨me position.
La combinatoire induite par le noyau de lâ€™Ã©quation 1 peut-Ãªtre diminuÃ©e en effectuant une re-
cherche bidirectionnelle au sein dâ€™une sÃ©quence. En effet, la composante Ï†f (x,y; t) du vecteur
Î¦(x,y; t) indique le nombre dâ€™occurrence de f dans (x,y) incluant la tÃ¨me position de (x,y)
((xt, yt)). Il est alors possible de dÃ©composer f en fu et fd tel que ((xt, yt)) soit le dernier
Ã©lÃ©ment de fu et le premier Ã©lÃ©ment de fd. Le calcul se limitera alors Ã  Ã©valuer les deux en-
sembles Fu = {(xi, xi+1, . . . , xt)|1 â‰¤ i â‰¤ t} et Fd = {(xt, xt+1, . . . , xk)|t â‰¤ k â‰¤ dim(x)}.
Les autres composantes de lâ€™espace seront obtenues par combinaison de Fu et de Fd i.e.
F = Fu Ã— Fd.
La figure 1 illustre ce principe : le couple de sÃ©quences (a) peut Ãªtre obtenu en combinant (b)
et (c).
Afin dâ€™effectuer la dÃ©composition dâ€™une sÃ©quence en fonction de la position t, on supposera
que :
P (y|x) =
âˆ
t
P (yt|xt)
On posant xu(t) = (x0, . . . , xt) et xd(t) = (xt, . . . , xT ), de mÃªme pour yu(t) et yd(t), on
obtient :
P (y|x) = P (yu(t)|xu(t)). P (yt|xt)(P (yt|xt))2 .P (yd(t)|xd(t))
S. Aseervatham et E. Viennet
   N      V    Det   N      N      V        V    Det   N
  Jeff  mange  la  pomme   Jeff  mange    mange  la  pomme
           (a)                 (b)              (c)
t t t
FIG. 1 â€“ (a) Exemple de couple de sÃ©quences (x, y). (b) sous-sÃ©quences de (a) oÃ¹ les xi avec
i > t ont Ã©tÃ© Ã©liminÃ©s. (c) sous-sÃ©quences de (a) obtenues en Ã©liminant les xi de (a) pour i < t.
En dÃ©composant, lâ€™Ã©quation 1 on obtient :
k(x,xâ€², Ï„, t, yÏ„ , yâ€²t) = ku(x,x
â€², Ï„, t).kp(x,xâ€², Ï„, t, yÏ„ , yâ€²t).kd(x,x
â€², Ï„, t)
avec :
ku(x,xâ€², Ï„, t) =
âˆ‘
yu(Ï„)
âˆ‘
yâ€²u(t)
P (yu(Ï„)|xu(Ï„))P (yâ€²u(t)|xâ€²u(t))
. ã€ˆÎ¦(xu(Ï„),yu(Ï„); Ï„),Î¦(xâ€²u(t),yâ€²u(t); t)ã€‰
kd(x,xâ€², Ï„, t) =
âˆ‘
yd(Ï„)
âˆ‘
yâ€²d(t)
P (yd(Ï„)|xd(Ï„))P (yâ€²d(t)|xâ€²d(t))
. ã€ˆÎ¦(xd(Ï„),yd(Ï„); Ï„),Î¦(xâ€²d(t),yâ€²d(t); t)ã€‰
kp(x,xâ€², Ï„, t, yÏ„ , yâ€²t) =
P (yÏ„ |xÏ„ )P (yâ€²t|xâ€²t). ã€ˆÎ¦(xÏ„ , yÏ„ ; Ï„),Î¦(xâ€²t, yâ€²t; t)ã€‰
(
âˆ‘
zÏ„
âˆ‘
zâ€²t
P (zÏ„ |xÏ„ )P (zâ€²t|xâ€²t). ã€ˆÎ¦(xÏ„ , zÏ„ ; Ï„),Î¦(xâ€²t, zâ€²t; t)ã€‰)2
(2)
La complexitÃ© pour lâ€™Ã©valuation de ces noyaux peut Ãªtre ramenÃ©e Ã  O(T.T â€²) avec T et T â€²
la taille des sÃ©quences en utilisant la programmation dynamique. On obtient alors les noyaux
suivants :
ku(x,xâ€², Ï„, t) =
{
0 si Ï„ = 0 ou t = 0
c2k(xÏ„ , xâ€²t)(ku(x,x
â€², Ï„ âˆ’ 1, tâˆ’ 1) + 1)
kd(x,xâ€², Ï„, t) =
{
0 si Ï„ > dim(x) ou t > dim(xâ€²)
c2k(xÏ„ , xâ€²t)(kd(x,x
â€², Ï„ + 1, t+ 1) + 1)
k(xÏ„ , xâ€²t) =
{
0 si xÏ„ 6= xâ€²tâˆ‘
y P (y|xÏ„ )P (y|xâ€²t)
kp(x,xâ€², Ï„, t, yÏ„ , yâ€²t) =
{
0 si (xÏ„ , yÏ„ ) 6= (xâ€²t, yâ€²t)
cP (yÏ„ |xÏ„ )P (yâ€²t|xâ€²t)
(c2k(xÏ„ ,xâ€²t)2)
La constante c est utilisÃ©e pour pondÃ©rer les termes Ï†f selon la taille de f . Il est aussi
possible de permettre des discontinuitÃ©s dans les sous-sÃ©quences comme dans le cas du noyau
String Subsequence. Il suffit alors simplement de modifier ku et kd (voir (Kashima et Tsuboi,
2004) pour plus de dÃ©tails).
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
Ce noyau, combinÃ© au noyau polynomial de degrÃ© deux, a Ã©tÃ© utilisÃ© avec un perceptron
pour rÃ©soudre un problÃ¨me de reconnaissance dâ€™entitÃ©s nommÃ©es et un problÃ¨me dâ€™extraction
dâ€™information. La loi uniforme est utilisÃ© pour modÃ©liser P (yt|xt). Les expÃ©riences ont Ã©tÃ©
menÃ©es en utilisant la validation croisÃ©e Ã  3 blocs.
En outre, pour chaque expÃ©rience le noyau marginalisÃ© est comparÃ© Ã  un perceptron utilisant
le modÃ¨le de Markov cachÃ© (Collins, 2002). Lâ€™idÃ©e de base de cet algorithme est dâ€™utiliser
lâ€™algorithme de Viterbi sur un modÃ¨le de Markov cachÃ© afin dâ€™attribuer la meilleure sÃ©quence
dâ€™Ã©tiquettes Ã  une sÃ©quence de termes. Le perceptron est utilisÃ© pour attribuer un score Ã  une
sÃ©quence Ã©tiquetÃ©e. Ce score sera utilisÃ© par lâ€™algorithme de Viterbi pour trouver la sÃ©quence
dâ€™Ã©tiquettes optimale.
Pour la reconnaissance dâ€™entitÃ©s nommÃ©es, les donnÃ©es utilisÃ©es sont un sous-ensemble
dâ€™un corpus espagnol fourni par CoNLL2002. Ce corpus est composÃ© de 300 phrases com-
prenant au total 8541 termes. Lâ€™objectif est dâ€™attribuer Ã  chaque terme lâ€™un des neuf labels
dÃ©signant le type dâ€™entitÃ© nommÃ©e (lâ€™un des neufs labels correspond au type "non-entitÃ© nom-
mÃ©e"). Les rÃ©sultats montrent que le noyau marginalisÃ© a un taux de reconnaissance, tant au
niveau de la prÃ©cision que du rappel, supÃ©rieur Ã  celui du modÃ¨le de Markov cachÃ©.
La deuxiÃ¨me expÃ©rience consistait Ã  extraire des informations concernant lâ€™utilisation de
produits. A partir dâ€™une base de 184 phrases (soit 3570 termes) en japonais, lâ€™objectif est
de reconnaÃ®tre le nom du produit, le vendeur, le nombre de produits achetÃ©s, les raisons de
lâ€™achat etc. Ainsi, il sâ€™agit dâ€™attribuer Ã  chaque terme lâ€™une des 12 Ã©tiquettes correspondants
aux informations citÃ©es.
Les termes ont Ã©tÃ© annotÃ©s en effectuant une analyse lexicale. En outre, un noyau marginalisÃ©
sur les arbres (voir la section 4) a Ã©tÃ© utilisÃ©. Pour ce noyau, les donnÃ©es ont Ã©tÃ© structurÃ©es
en arbre lexical de dÃ©pendance reprÃ©sentant la structure linguistique de la phrase en terme de
dÃ©pendance entre les mots.
Les expÃ©riences montrent que les noyaux marginalisÃ©s sont plus performants que le perceptron
utilisant le modÃ¨le de Markov cachÃ©. De plus, le noyau marginalisÃ© sur les arbres obtient de
meilleurs rÃ©sultats que le noyau sur les sÃ©quences. Ce rÃ©sultat peut Ãªtre expliquÃ© par le fait que
le noyau sur les arbres tire avantage de lâ€™information structurelle contrairement au noyau sur
les sÃ©quences.
4 Les noyaux pour les arbres
Les arbres sont des structures de donnÃ©es permettant de reprÃ©senter efficacement des don-
nÃ©es organisÃ©es de maniÃ¨re hiÃ©rarchique. Ainsi, ils sont communÃ©ment utilisÃ©s dans de nom-
breux domaines.
La majoritÃ© des documents structurÃ©s et semi-structurÃ©s, tels que les documents XML, sont
reprÃ©sentÃ©s de maniÃ¨re arborescente. Ainsi, il peut Ãªtre intÃ©ressant de tenir compte de cette
structure dans lâ€™Ã©valuation des critÃ¨res de similaritÃ©s entre ces diffÃ©rents documents.
S. Aseervatham et E. Viennet
4.1 Le noyau Tree kernel
Le noyau Tree Kernel (appelÃ© aussi parse tree kernel) (Collins et Duffy, 2002) a Ã©tÃ© dÃ©fini
pour le calcul de similaritÃ© entre les arbres grammaticaux (ou arbres syntaxiques). Un arbre
syntaxique est obtenu Ã  partir dâ€™une phrase en la dÃ©composant en groupe grammatical. La fi-
gure 2 montre lâ€™arbre syntaxique associÃ© Ã  la phrase â€œJeff mange la pommeâ€.
Phrase
GN
VN
Jeff
GV
GN
mange Det N
la pomme
FIG. 2 â€“ Arbre syntaxique pour la phrase "Jeff mange la pomme"
Definition 1 (arbre propre) Un arbre propre est un arbre ayant une racine et au moins un
nÅ“ud fils.
Definition 2 (Sous-arbre complet) Un arbre S est dit sous-arbre complet dâ€™un arbre T si et
seulement si il existe un nÅ“ud n tel que lâ€™arbre induit par n (de racine n) est Ã©gal Ã  S. On
notera Ï„T (n) lâ€™arbre complet induit par le nÅ“ud n de T .
Definition 3 (Sous-arbre co-enracinÃ©) Un arbre S est dit sous-arbre co-enracinÃ© dâ€™un arbre
propre T si et seulement si S les propriÃ©tÃ©s suivantes sont vÃ©rifiÃ©es :
1. S est un arbre propre,
2. la racine de S (rac(S)) est identique Ã  la racine de T (rac(T )) (si S et T sont des arbres
dont les nÅ“uds sont Ã©tiquetÃ©s alors les Ã©tiquettes de rac(S) et de rac(T ) doivent Ãªtre
identiques),
3. âˆ€i, filsi(rac(S)) = filsi(rac(T )),
4. S peut Ãªtre obtenu Ã  partir de T en supprimant des sous-arbres de filsi(rac(T )).
La notion de sous-arbre co-enracinÃ© permet de garantir la consistance des rÃ¨gles gramma-
ticales. Par exemple, pour lâ€™arbre T de la figure 2, il existe des arbres co-enracinÃ©s de T qui
produisent : "N V la N", "Jeff mange GN", "GN V GN", ... Toutefois, il nâ€™existe pas dâ€™arbres
co-enracinÃ©s de T produisant "Jeff mange Detâ€, "GN mange N", ...
Soit Î“ lâ€™ensemble de tous les arbres propres possibles, un arbre T peut Ãªtre plongÃ©, par une
fonction Î¦r, dans un espace vectoriel de caractÃ©ristique (feature-space). Le uÃ¨me composant
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
de Î¦r, associÃ© Ã  Su âˆˆ Î“, donne le nombre de nÅ“ud n de T tel que Su est un sous-arbre
co-enracinÃ© de Ï„T (n) (RfreqT (Su)), soit :
Î¦rSu(T ) = RfreqT (Su) =
âˆ‘
nâˆˆT
ISu(Ï„T (n)) (3)
Avec ISu(Ï„T (n)) = 1 si lâ€™arbre Su est un sous-arbre co-enracinÃ© de Ï„T (n) et 0 sinon.
Le noyau permettant de calculer la similaritÃ© entre deux arbres T1 et T2 est :
ktree(T1, T2) = ã€ˆÎ¦r(T1),Î¦r(T2)ã€‰
=
âˆ‘
SuâˆˆÎ“
Î¦rSu(T1).Î¦
r
Su(T2)
=
âˆ‘
SuâˆˆÎ“
(
âˆ‘
n1âˆˆT1
ISu(Ï„T1(n1))).(
âˆ‘
n2âˆˆT2
ISu(Ï„T2(n2)))
=
âˆ‘
n1âˆˆT1
âˆ‘
n2âˆˆT2
âˆ‘
SuâˆˆÎ“
ISu(Ï„T1(n1)).ISu(Ï„T2(n2))
=
âˆ‘
n1âˆˆT1
âˆ‘
n2âˆˆT2
krtree(Ï„T1(n1), Ï„T2(n2)) (4)
krtree(T1, T2) indique le nombre de sous-arbres co-enracinÃ©s quâ€™ont en commun les arbres T1
et T2. Cette fonction retourne 0 si 1) les racines sont diffÃ©rentes, ou 2) si le nombre de fils
de T1 et de T2 ne correspondent pas ou 3) si âˆƒi, filsi(T1) 6= filsi(T2). Dans les autres cas,
on peut dÃ©finir krtree(T1, T2) par rÃ©currence. En effet, cette fonction sera Ã©gale au produit des
nombres de sous-arbres co-enracinÃ©s communs Ã  chacun des fils de T1 et de T2. Il est Ã  noter
que si krtree(T1, T2) 6= 0mais que krtree(Ï„T1(filsi(rac(T1)), filsi(rac(T2))) = 0, il existe un
unique sous-arbre co-enracinÃ© commun Ã  T1 et T2 pour la partie du sous-arbre induit par filsi.
On peut ainsi dÃ©finir krtree(T1, T2) dans le cas non nul :
krtree(T1, T2) =
âˆ
i
(krtree(Ï„T1(filsi(rac(T1))), Ï„T2(filsi(rac(T2)))) + 1)
La complexitÃ© temporelle du noyau ktree(T1, T2) est O(|T1||T2|) (Collins et Duffy, 2002)
avec |T | le nombre de nÅ“uds dans T .
Collins et al. ont utilisÃ© ce noyau pour associer Ã  une phrase lâ€™arbre syntaxique le plus plausible
(parsing) (Collins et Duffy, 2002). La dÃ©composition dâ€™une phrase en arbre syntaxique est
un problÃ¨me difficile. En effet, lâ€™ambiguÃ¯tÃ© sous-jacente au langage naturel entraÃ®ne plusieurs
dÃ©compositions possibles pour une mÃªme phrase. Lâ€™objectif proposÃ© par Collins et al. est de
sÃ©lectionner lâ€™arbre le plus probable par une approche discriminante.
Soit F une fonction permettant de gÃ©nÃ©rer un ensemble dâ€™arbres syntaxiques pour une phrase,
les donnÃ©es sont reprÃ©sentÃ©s par un couple (s, F (s)). Pour lâ€™ensemble dâ€™apprentissage, lâ€™arbre
syntaxique correct pour chaque s est connu dans F (s). Un sÃ©parateur est alors "appris" en
utilisant un perceptron. Pour une phrase s, on lui associe lâ€™arbre Tsi de F (s) tel que :
Tsi = argmaxTâˆˆF (s)(w
âˆ—.Î¦r(T ))
S. Aseervatham et E. Viennet
Avec wâˆ— le vecteur optimal associÃ© Ã  :
w =
âˆ‘
s,j>1
Î±s,j(Î¦r(Tsi)âˆ’ Î¦r(Tsj ))
Avec si une phrase dâ€™apprentissage, s1 âˆˆ F (s) lâ€™arbre syntaxique correct de s et sj âˆˆ F (s).
Les expÃ©riences sur le corpus Penn treebank ATIS, qui est un corpus anglais annotÃ© sous forme
arborescente, ont montrÃ© que lâ€™utilisation de cette mÃ©thode amÃ©liore de prÃ¨s de 4% les rÃ©sultats
obtenus par une mÃ©thode conventionnelle stochastique (Probabilistic Context Free Grammar).
4.2 Le noyau Tree kernel gÃ©nÃ©ralisÃ©
Le noyau Tree Kernel a Ã©tÃ© essentiellement dÃ©veloppÃ© pour traiter des arbres spÃ©cifiques
telles que les arbres syntaxiques ; il se base sur les propriÃ©tÃ©s suivantes :
1. les descendants dâ€™un nÅ“ud nâ€™ont jamais les mÃªmes Ã©tiquettes que les nÅ“uds ancÃªtres
2. le noyau utilise la notion de sous-arbre co-enracinÃ© pour calculer la similitude entre deux
arbres.
Une gÃ©nÃ©ralisation de ce noyau a Ã©tÃ© proposÃ©e dans (Kashima et Koyanagi, 2002) pour
traiter des arbres complexes tels que les arbres XML et HTML. Toutefois, on impose que
lâ€™arbre soit Ã©tiquetÃ© et ordonnÃ© (tel que câ€™Ã©tait le cas pour les arbres syntaxiques).
Le cadre gÃ©nÃ©ral du noyau Tree Kernel reste valide. En effet, pour gÃ©nÃ©raliser le noyau, il suffit
de modifier la fonction IS(T ) et de changer le noyau spÃ©cifique krtree(T1, T2). Dans le cas
dâ€™un arbre quelconque Ã©tiquetÃ© et ordonnÃ© T , IS(T ) retournera la frÃ©quence dâ€™occurrence du
sous-arbre S dans T .
Definition 4 (Sous-arbre) Un arbre S (possÃ©dant au moins un nÅ“ud) est un sous-arbre de T
si et seulement si il existe un nÅ“ud n de T tel que l(n) = l(rac(S)) (l(n) correspondant Ã  lâ€™Ã©ti-
quette du nÅ“ud n) et une liste ordonnÃ©e dâ€™indexes {j1, . . . , jk} tel que âˆ€i, l(filsi(rac(S))) =
l(filsji(Ï„T (n))) avec i < ji et Ï„S(filsi(rac(S))) soit, soit une feuille soit un sous-arbre de
Ï„T (filsji(Ï„T (n))) partageant la mÃªme racine.
Ã‰tant donnÃ©e cette dÃ©finition de sous-arbre, le noyau krtree(T1, T2) peut Ãªtre dÃ©fini comme
Ã©tant la fonction qui retourne le nombre de sous-arbres commun Ã  T1 et T2 avec pour racine
rac(T1), en tenant compte de la frÃ©quence dâ€™occurrence dans chaque arbre. Autrement dit, il
sâ€™agit de la somme, pour chaque sous-arbre possible S de racine rac(T1), des produits des
nombres dâ€™occurrences de S dans T1 et dans T2.
Les arbres Ã©tant ordonnÃ©s, le noyau sur T1 et T2 peut Ãªtre calculÃ© en introduisant une rÃ©cur-
rence sur le nombre de fils de T1 (nf(rac(T1))) et le nombre de fils de T2. Ainsi, la fonction
ST1,T2(i, j) est introduite pour calculer k
r
tree(T1i, T2j) tel que T1i est le sous-arbre de T1 ob-
tenu en supprimant tous les fils dâ€™index supÃ©rieurs Ã  i, ainsi que leurs descendants, de mÃªme
pour T2j .
krtree(T1, T2) =
{
0 si l(rac(T1)) 6= l(rac(T2))
ST1,T2(nf(rac(T1)), nf(rac(T2)))
(5)
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
avec
ST1,T2(0, 0) = ST1,T2(i, 0) = ST1,T2(0, j) = 1
ST1,T2(i, j) = ST1,T2(iâˆ’ 1, j) + ST1,T2(i, j âˆ’ 1) (6)
âˆ’ST1,T2(iâˆ’ 1, j âˆ’ 1)
+ST1,T2(iâˆ’ 1, j âˆ’ 1).krtree(Ï„T1(filsi(rac(T1))), Ï„t2(filsj(rac(T2))))
On gÃ©nÃ©ralise ce noyau en sâ€™appuyant sur deux idÃ©es ; dâ€™une part utiliser la similaritÃ© entre
les Ã©tiquettes, dâ€™autre part reprÃ©senter les sous-arbres non-contigus (Kashima et Koyanagi,
2002).
Soit Î£ lâ€™ensemble de toutes les Ã©tiquettes et f : Î£ Ã— Î£ â†’ [0, 1] indiquant un score de "mu-
tation" entre deux Ã©tiquettes tel que f(e, a) indique la probabilitÃ© dâ€™acceptation de la mutation
de lâ€™Ã©tiquette a vers e, la fonction de similaritÃ© entre deux Ã©tiquettes de deux nÅ“uds n1 et n2
est :
Sim(l(n1), l(n2)) =
âˆ‘
aâˆˆÎ£
f(l(n1), a).f(l(n2), a)
En introduisant la fonction de similaritÃ© dans lâ€™Ã©quation 5, on obtient :
krtree(T1, T2) = Sim(l(n1), l(n2)).ST1,T2(nf(rac(T1)), nf(rac(T2))) (7)
On peut encore gÃ©nÃ©raliser en utilisant, au niveau du noyau, de la notion dâ€™Ã©lasticitÃ© des
sous-arbres. Ainsi, la dÃ©finition de sous-arbre se voit Ã©largie en permettant la non contiguÃ¯tÃ©
au niveau des nÅ“uds dâ€™un chemin. Il nâ€™est ici plus nÃ©cessaire quâ€™un chemin dâ€™un sous-arbre
apparaisse de maniÃ¨re contiguÃ« dans un arbre. Cependant, la contrainte dâ€™arbre ordonnÃ© reste
valable.
Definition 5 (Sous-arbre non contigu) Un arbre S (possÃ©dant au moins un nÅ“ud) est un
sous-arbre de T si et seulement si il existe un nÅ“ud n de T tel que l(n) = l(rac(S)) (l(n) et
une liste ordonnÃ©e dâ€™indexes {j1, . . . , jk} tel que âˆ€i, Ï„S(filsi(rac(S))) soit un sous arbre de
Ï„T (filsji(Ï„T (n))).
Afin de prendre en compte la dÃ©finition de sous-arbre non-contigu, il est nÃ©cessaire de gÃ©-
nÃ©raliser la formule de krtree(T1, T2) (Ã©quations 5 et 7). En effet, cette formule retourne une
valeur nulle (ou faible selon la similaritÃ©) si les Ã©tiquettes des racines sont diffÃ©rentes. En
dâ€™autre terme, tout sous-arbre commun Ã  T1 et T2 doit Ãªtre enracinÃ© aux nÅ“uds racines de T1
et T2 impliquant ainsi que les arbres possÃ¨dent la mÃªme racine. Or, dans le cas des sous-arbres
non-contigus, un sous-arbre commun Ã  T1 et T2 peut Ãªtre construit par combinaison (ordonnÃ©e)
Ã  partir de sous-arbres enracinÃ©s Ã  nâ€™importe quels nÅ“uds descendants de T1 et T2.
Soit krold le noyau dÃ©fini par lâ€™Ã©quation 5, le noyau spÃ©cifique pour les arbres Ã©lastiques
utilisÃ©s par le noyau Tree Kernel (Ã©quation 4) est :
krtree(T1, T2) =
âˆ‘
n1âˆˆT1
âˆ‘
n2âˆˆT2
krold(Ï„T1(n1), Ï„T2(n2))
S. Aseervatham et E. Viennet
Cette formule peut Ãªtre calculÃ©e efficacement de maniÃ¨re rÃ©cursive :
krtree(T1, T2) = k
r
old(T1, T2) +
âˆ‘
ni=filsi(T1)
krtree(Ï„T1(ni), T2)
+
âˆ‘
nj=filsj(T2)
krtree(T1, Ï„T2(nj))
âˆ’
âˆ‘
ni=filsi(T1)
âˆ‘
nj=filsj(T2)
krtree(Ï„T1(ni), Ï„T2(nj))
De mÃªme que dans le cas du noyau parse tree kernel vu dans la section prÃ©cÃ©dente, la com-
plexitÃ© du noyau tree kernel est O(|T1|.|T2|) dans les diffÃ©rents cas de gÃ©nÃ©ralisation.
Kashima et Koyanagi (2002) ont utilisÃ© les noyaux gÃ©nÃ©ralisÃ©s Ã©lastiques et non Ã©lastiques,
sans tenir compte des mutations dâ€™Ã©tiquettes, pour la classification et lâ€™extraction dâ€™information
Ã  partir de documents HTML. Les performances ont Ã©tÃ© Ã©valuÃ©es en utilisant la mÃ©thode de
validation croisÃ©e leave-one-out. Lâ€™apprentissage a Ã©tÃ© effectuÃ© en utilisant lâ€™algorithme du
perceptron "kernelisÃ©".
Pour la classification de documents, une base de donnÃ©es comprenant 30 documents HTML
en japonais et 30 documents HTML en anglais a Ã©tÃ© construite en extrayant les documents
alÃ©atoirement sur les sites web amÃ©ricains et japonais dâ€™IBM. La classification devant Ãªtre
purement structurelle, seules les balises HTML ont Ã©tÃ© prÃ©servÃ©es, Ã©liminant ainsi les attributs
et les donnÃ©es. De plus, les noyaux sur arbres ont Ã©tÃ© combinÃ©s avec le noyau polynomial. Les
rÃ©sultats ont montrÃ©s que le noyau non Ã©lastique Ã©tait de 12% plus performant que le noyau
Ã©lastique en atteignant prÃ©s de 80% de bon classement. Ces rÃ©sultats ont Ã©tÃ© obtenus avec un
noyau polynomial dâ€™ordre 4 (resp. 3).
Lâ€™extraction dâ€™information consiste Ã  apprendre et Ã  reconnaÃ®tre une information prÃ©cise
dans des documents HTML. Il sâ€™agit ici du marquage des nÅ“uds pertinents. Le problÃ¨me
du marquage consiste Ã  apprendre Ã  partir dâ€™arbres correctement marquÃ©s puis Ã  marquer les
nÅ“uds pertinents des arbres non traitÃ©s. Ce problÃ¨me peut Ãªtre ramenÃ© Ã  un problÃ¨me de clas-
sification en effectuant une transformation du marquage. En effet, pour un nÅ“ud marquÃ©, on
insÃ¨re entre le nÅ“ud concernÃ© et le nÅ“ud pÃ¨re, un nÅ“ud portant une Ã©tiquette appropriÃ©e pour
signaler le marquage. Puis, un ensemble dâ€™arbres nÃ©gatifs est gÃ©nÃ©rÃ©s Ã  partir des arbres cor-
rects en retirant le marquage et en les plaÃ§ant sur des nÅ“uds non initialement marquÃ©s. Un
apprentissage peut ensuite Ãªtre effectuÃ© sur ces donnÃ©es. Pour le marquage sur un arbre, on
effectue pour chacun de ses nÅ“uds un marquage puis on le classe.
Pour lâ€™expÃ©rimentation, une base a Ã©tÃ© crÃ©Ã©e Ã  partir de 54 pages HTML extraites dâ€™un cata-
logue de vente dâ€™ordinateurs portables dâ€™IBM Japon. Lâ€™objectif de lâ€™expÃ©rience Ã©tait de retrou-
ver lâ€™image de lâ€™ordinateur Ã  vendre. Pour cela, les nÅ“uds contenants les images pertinentes
ont Ã©tÃ© marquÃ©s et les donnÃ©es textuelles prÃ©sentes dans les pages ont Ã©tÃ© Ã©liminÃ©es. Les rÃ©-
sultats ont montrÃ© que le noyau Ã©lastique a permis dâ€™extraire lâ€™information avec une prÃ©cision
de 99.3% et un rappel de 68.6% contre une prÃ©cision de 11.9% et un rappel de 79.6% pour le
noyau non Ã©lastique. Ces rÃ©sultats ont Ã©tÃ© obtenus sans la combinaison avec le noyau polyno-
mial. En effet, ce dernier nâ€™a pas permis dâ€™amÃ©liorer les rÃ©sultats.
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
4.3 Le noyau Tree kernel marginalisÃ©
Le noyau marginalisÃ© pour les arbres a Ã©tÃ© introduit par Kashima et Tsuboi (2004) pour
rÃ©pondre aux problÃ¨mes dâ€™Ã©tiquetages (voir la section sur le noyau marginalisÃ© sur les sÃ©-
quences).
Lâ€™objectif de ce noyau est de permettre de calculer la similaritÃ© entre deux arbres Ã©tiquetÃ©s. Un
arbre Ã©tiquetÃ© Ã©tant simplement un arbre oÃ¹ chaque nÅ“ud reprÃ©sente un Ã©lÃ©ment observable
(un terme) et Ã  chaque nÅ“ud est associÃ© une Ã©tiquette. Lâ€™avantage de rÃ©soudre un problÃ¨me
dâ€™Ã©tiquetage en utilisant un modÃ¨le de donnÃ©e arborescent, plutÃ´t que sÃ©quentiel, est quâ€™il est
possible dâ€™exploiter lâ€™information structurelle pour amÃ©liorer la discrimination.
Le noyau marginalisÃ© sur les arbres est obtenu en intÃ©grant le noyau Tree kernel gÃ©nÃ©ralisÃ©
dans le cadre thÃ©orique du noyau marginalisÃ© dÃ©fini par la sÃ©rie dâ€™Ã©quation 2.
Ainsi, kd(T1, T2, Ï„, t) est le noyau ne prenant en compte que les sous-arbres ayant la mÃªme
racine rac(T1Ï„ ) (T1Ï„ indique ici le sous-arbre de T1 induit par le nÅ“ud dâ€™index Ï„ ). De mÃªme,
ku(T1, T2, Ï„, t) ne prend en compte que les sous-arbres ayant une feuille correspondant au
nÅ“ud dâ€™index Ï„ de T1 (rac(T1Ï„ )).
Pour le calcul de kd, on se base sur les Ã©quations 5 et 6. Lâ€™Ã©quation 6 calcule la somme
des contributions des sous-arbres communs Ã  T1 et T2 en explorant Ã  chaque niveau les fils de
droite Ã  gauche. En modifiant cette Ã©quation on obtient :
SF (T1, T2, Ï„, t, 0, 0) = SF (T1, T2, Ï„, t, i, 0) = SF (T1, T2, Ï„, t, 0, j) = 1
SF (T1, T2, Ï„, t, i, j) = SF (T1, T2, Ï„, t, iâˆ’ 1, j) + SF (T1, T2, Ï„, t, i, j âˆ’ 1)
âˆ’SF (T1, T2, Ï„, t, iâˆ’ 1, j âˆ’ 1)
+SF (T1, T2, Ï„, t, iâˆ’ 1, j âˆ’ 1).kd(T1, T2, ch(T1, Ï„, i), ch(T2, t, j))
Avec ch(T1, Ï„, i) lâ€™index, dans T1, du iÃ¨me fils de la racine de T1Ï„ .
Le noyau kd devient alors :
kd(T1, T2, Ï„, t) = c2k(T1Ï„ , T2t).(1 + SF (T1, T2, Ï„, t, nf(rac(T1Ï„ )), nf(rac(T2t))))
Pour ku, le calcul sâ€™effectue de la feuille vers la racine. Ainsi, on utiliser la fonction
pa(T1, Ï„) qui calculera lâ€™index du pÃ¨re du Ï„ Ã¨me nÅ“ud dans T1. En outre, il faut aussi tenir
compte des frÃ¨res gauches et des frÃ¨res droits du Ï„ Ã¨me nÅ“ud de T1. Pour la contribution des
frÃ¨res gauches, la fonction SF pourra Ãªtre utilisÃ©e. Quant aux frÃ¨res droits, il faudra les explo-
rer de la gauche vers la droite. On utilisera la fonction n = chID(T1, Ï„) pour indiquer que le
nÅ“ud dâ€™index Ï„ est le nÃ¨me fils de son pÃ¨re. On dÃ©finit donc, une fonction symÃ©trique Ã  SF :
SB(T1, T2, Ï„, t, i, j) = 1 si i â‰¥ nf(rac(T1Ï„ )) ou si j â‰¥ nf(rac(T2t))
SB(T1, T2, Ï„, t, i, j) = SB(T1, T2, Ï„, t, i+ 1, j) + SB(T1, T2, Ï„, t, i, j + 1)
âˆ’ SB(T1, T2, Ï„, t, i+ 1, j + 1)
+ SB(T1, T2, Ï„, t, i+ 1, j + 1).kd(T1, T2, ch(T1, Ï„, i), ch(T2, t, j))
S. Aseervatham et E. Viennet
Lâ€™expression de ku est :
ku(T1, T2, Ï„, t) = c2k(T1Ï„ , T2t).(1 + ku(T1, T2, Ï„, t))
. SF (pa(T1, Ï„), pa(T2, t), Ï„, t, chID(T1, Ï„)âˆ’ 1, chID(T2, t)âˆ’ 1)
. SB(pa(T1, Ï„), pa(T2, t), Ï„, t, chID(T1, Ï„) + 1, chID(T2, t) + 1)
Les expÃ©riences menÃ©es sur ce noyau sont dÃ©crites dans la section sur le noyau marginalisÃ©
pour les sÃ©quences.
5 Les noyaux pour les graphes
Le graphe est une structure de donnÃ©e trÃ¨s utilisÃ©e dans le domaine informatique pour
modÃ©liser des informations structurÃ©es complexes. Les sÃ©quences et les arbres Ã©tudiÃ©s prÃ©cÃ©-
demment peuvent Ãªtre vus comme des graphes acycliques orientÃ©s (dans le cas dâ€™une sÃ©quence,
le graphe est de degrÃ© maximum 1).
Nous dÃ©finirons un graphe Ã©tiquetÃ©G par le triplÃ© (V,E, Ïƒ) oÃ¹ V est lâ€™ensemble des nÅ“uds de
G, Ïƒ lâ€™ensemble des Ã©tiquettes tel que Ïƒi reprÃ©sente lâ€™Ã©tiquette du nÅ“ud i (il est aussi possible
dâ€™Ã©tiqueter les arcs : on notera Ïƒ(i,j) lâ€™Ã©tiquette de lâ€™arc reliant le nÅ“ud i Ã  j) et E, une matrice
dâ€™adjacence tel que Eij = 1 si et seulement si il existe un arc reliant le nÅ“ud i au nÅ“ud j (afin
de simplifier lâ€™Ã©criture on utilisera la mÃªme notation i, j pour dÃ©signer les indexes dans E que
pour designer les nÅ“uds de V ). Lâ€™une des propriÃ©tÃ©s de la matrice dâ€™adjacence est que [En]ij
indique le nombre de chemins de longueur n reliant le nÅ“ud i au nÅ“ud j.
La conception dâ€™un noyau nÃ©cessite une dÃ©finition de la similaritÃ© entre deux graphes.
Pour cela, deux approches ont Ã©tÃ© proposÃ©es (GÃ¤rtner, 2003; GÃ¤rtner et al., 2006). La premiÃ¨re
consiste Ã  dÃ©terminer si les deux graphes sont isomorphes (ils ne se distinguent que par lâ€™ordre
des nÅ“uds) ou Ã  dÃ©terminer le nombre de sous-graphes isomorphes communs. Cependant, il
est connu que ce problÃ¨me est fortement combinatoire.
La deuxiÃ¨me approche consiste Ã  projeter le graphe G dans un espace vectoriel oÃ¹ chaque
dimension est indexÃ©e par un graphe H tel que la valeur de la projection de G sur cet axe
reprÃ©sente la frÃ©quence dâ€™occurrence de H , en tant que sous-graphe, dans G. Il est alors pos-
sible de concevoir un noyau qui identifie certaines propriÃ©tÃ©s dans les sous-graphes H . En
particulier, ce noyau peut effectuer le produit scalaire dans lâ€™espace vectoriel en se limitant
aux sous-graphes qui sont des chemins hamiltoniens (H est un chemin hamiltonien de G si
et seulement si H est un sous-graphe de G et si H est de mÃªme ordre que G i.e. H contient
tous les nÅ“uds de G exactement une fois). Comme pour la premiÃ¨re approche, le problÃ¨me du
chemin hamiltonien est NP-difficile.
Afin de rÃ©duire la complexitÃ© dans lâ€™Ã©valuation de la similaritÃ© dâ€™autres approches ont
Ã©tÃ© explorÃ©es. Lâ€™approche la plus rÃ©pandue consiste Ã  calculer la similaritÃ© en se basant sur
les chemins parcourus (GÃ¤rtner et al., 2003, 2006; Kashima et Inokuchi, 2002; Kashima et al.,
2003). Lâ€™un des problÃ¨mes principaux de cette approche est quâ€™il existe une infinitÃ© de chemins
possibles dÃ¨s lors quâ€™il existe dans le graphe un cycle. On a alors le noyau :
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
k(G,Gâ€²) = lim
nâ†’âˆ
nâˆ‘
i=1
âˆ‘
pâˆˆPi(G)
âˆ‘
pâ€²âˆˆPi(Gâ€²)
Î»i.kp(p, pâ€²) (8)
Avec Pl(G) lâ€™ensemble des chemins de G de longueur l, Î»l un rÃ©el pondÃ©rant les chemins
de longueur l (on fixera Î»l = Î»l) et kp un noyau dÃ©fini sur les chemins. Il existe plusieurs
faÃ§ons de dÃ©finir kp selon quâ€™on veuille tenir compte des Ã©tiquettes sur les nÅ“uds, sur les arcs
ou encore permettre des discontinuitÃ©s (gap).
Dans (GÃ¤rtner et al., 2003, 2006), kp est dÃ©fini sur des chemins contiguÃ«s en tenant compte
des Ã©tiquettes sur les nÅ“uds et sur les arcs. Ainsi, le noyau sur les arcs revient Ã  Ã©numÃ©rer le
nombre de chemins communs aux deux graphes.
De plus, lâ€™Ã©quation 8 est rÃ©Ã©crite plus Ã©lÃ©gamment en utilisant la propriÃ©tÃ© de la matrice dâ€™adja-
cence. Pour cela, un nouveau graphe GÃ— : (VÃ—, EÃ—, ÏƒÃ—) est introduit en effectuant le produit
direct des graphes G : (V,E, Ïƒ) et Gâ€² : (V â€², Eâ€², Ïƒâ€²) :
VÃ— = {(v, vâ€²) âˆˆ V Ã— V â€²|Ïƒv = Ïƒvâ€²}
ÏƒÃ—k=(v,vâ€²) = Ïƒv
Pour (i, j) correspondant Ã  ((u, uâ€²), (v, vâ€²)) âˆˆ V 2Ã—
[EÃ—]i,j =
{
1 si [E]u,v = [Eâ€²]u,vâ€² = 1 et Ïƒ(u,v) = Ïƒâ€²(uâ€²,vâ€²)
0 sinon
ÏƒÃ—(i,j) = Ïƒ(u,v)
Lâ€™Ã©quation 8 devient :
k(G,Gâ€²) = lim
nâ†’âˆ
nâˆ‘
i=1
|VÃ—|âˆ‘
u,v
Î»i.[EiÃ—]u,v
Lâ€™expression EiÃ— peut Ãªtre simplifiÃ©e si la matrice EÃ— est diagonalisable. Dans le cadre
dâ€™un graphe non-orientÃ©, la matrice EÃ— Ã©tant une matrice rÃ©elle et symÃ©trique, elle peut Ãªtre
diagonalisÃ©e.
Ainsi, si EÃ— peut Ãªtre exprimÃ© sous la forme Tâˆ’1.D.T alors EÃ—i = Tâˆ’1.Di.T . On peut alors
rÃ©Ã©crire le noyau sous la forme :
k(G,Gâ€²) =
|VÃ—|âˆ‘
u,v
(Tâˆ’1.( lim
nâ†’âˆ
nâˆ‘
i=1
Î»i.Di).T )u,v
Pour calculer la limite, GÃ¤rtner et al. (2003) propose dâ€™utiliser une dÃ©composition en sÃ©rie
exponentielle ou en sÃ©rie gÃ©omÃ©trique.
La dÃ©composition en sÃ©rie exponentielle se base sur lâ€™Ã©galitÃ©
eÎ².E = lim
nâ†’âˆ
nâˆ‘
i=0
(Î²E)i
i!
S. Aseervatham et E. Viennet
Ainsi, en fixant Î»i = Î²
i
i! , on obtient :
k(G,Gâ€²) =
|VÃ—|âˆ‘
u,v
(Tâˆ’1.eÎ².D.T )u,v
De mÃªme, la dÃ©composition en sÃ©rie gÃ©omÃ©trique se base sur, pour Î³ < 1 :
lim
nâ†’âˆ
nâˆ‘
i=0
Î³i =
1
1âˆ’ Î³
En fixant Î³ = Î».D et en veillant Ã  ce que Î».D < I, on a :
k(G,Gâ€²) =
|VÃ—|âˆ‘
u,v
(Tâˆ’1.(Iâˆ’ Î».D)âˆ’1.T )u,v
Dans Kashima et Inokuchi (2002), les auteurs ont dÃ©composÃ© la similaritÃ© de deux graphes
en une somme de similaritÃ© entre nÅ“uds :
k(G,Gâ€²) =
1
|V |.|V â€²|
âˆ‘
viâˆˆV,vjâˆˆV â€²
kn(vi, vj)
Avec V et V â€² lâ€™ensemble des nÅ“uds de G et respectivement de Gâ€².
La similaritÃ© entre deux nÅ“uds sera dâ€™autant plus importante quâ€™il existera des chemins longs
communs aux deux graphes issus de ces nÅ“uds. Pour assurer la terminaison du calcul, une
probabilitÃ© 1âˆ’ Î» est fixÃ©e pour terminer le chemin et une probabilitÃ© Î» pour continuer vers un
successeur du nÅ“ud courant. Ainsi, plus le chemin sera long et plus la probabilitÃ© de terminer
le chemin deviendra importante. On obtient, ainsi, le noyau suivant :
kn(u, uâ€²) = I(u, uâ€²).((1âˆ’ Î») + Î».
âˆ‘
(v,vâ€²)âˆˆAG(u)Ã—AGâ€² (uâ€²)
IA((u, v), (uâ€², vâ€²))
|AG(u)|.|AGâ€²(uâ€²)| .kn(v, v
â€²))
Avec AG(u) = {v âˆˆ V |Euv = 1}, I(u, uâ€²) = 1 si les Ã©tiquettes des nÅ“uds u de G et uâ€² de Gâ€²
sont identiques ou 0 sinon et de mÃªme pour IA((u, v), (uâ€², vâ€²)) qui retourne 1 si lâ€™Ã©tiquette de
lâ€™arc reliant u et v de G est identique Ã  lâ€™Ã©tiquette de lâ€™arc reliant uâ€² et vâ€² de Gâ€².
Dans Kashima et al. (2003), un noyau marginalisÃ© sur tous les chemins possible est proposÃ©
en ne considÃ©rant que des graphes orientÃ©s. Ainsi, le noyau est dÃ©fini par :
k(G,Gâ€²) = lim
Lâ†’âˆ
Lâˆ‘
l=1
âˆ‘
h
âˆ‘
hâ€²
kz(G,Gâ€²,h,hâ€²).P (h|G).P (hâ€²|Gâ€²)
La probabilitÃ© a posteriori dâ€™avoir un chemin h de G de longueur l (P (h|G)) est dÃ©fini en
fonction de la probabilitÃ© de dÃ©buter un chemin par un nÅ“ud h1(Ps(h1)), la probabilitÃ© de
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
terminer ce chemin par un nÅ“ud hl (Pq(hl)) et les probabilitÃ©s dâ€™effectuer une transition dâ€™un
nÅ“ud hi vers un nÅ“ud hi+1 (Pt(hi+1|hi)). Dâ€™oÃ¹ :
P (h|G) = Ps(h1).
l=|h|âˆ
i=2
Pt(hi|hiâˆ’1).Pq(hl)
Le noyau kz effectue la comparaison des deux chemins h et hâ€² des graphes respectifs G et
Gâ€². Pour cela, le noyau calcule le produit des similaritÃ©s entre les Ã©tiquettes des nÅ“uds et des
arcs du chemins :
kz(G,Gâ€²,h,hâ€²) =
{
0 si |h| 6= |hâ€²|
ke(Ïƒh1 , Ïƒ
â€²
hâ€²1
)
âˆl
i=2 ke(Ïƒ(hiâˆ’1,hi), Ïƒ
â€²
(hâ€²iâˆ’1,h
â€²
i
).ke(Ïƒhl , Ïƒ
â€²
hâ€²l
)
On rappelle que Ïƒhi indique lâ€™Ã©tiquette du nÅ“ud hi de G et que Ïƒ(hi,hi+1) indique lâ€™Ã©ti-
quette de lâ€™arc reliant le nÅ“ud hi Ã  hi+1. Ã‰tant donnÃ©es deux Ã©tiquettes e et eâ€², on peut dÃ©finir
ke comme Ã©tant un noyau retournant 1 si e = eâ€² ou 0 sinon. Toutefois, on peut dÃ©finir un noyau
plus complexe si les Ã©tiquettes sont des rÃ©elles avec une certaine mÃ©trique. On pourrait alors
dÃ©finir un noyau gaussien qui tolÃ©rerait certaines diffÃ©rences entre les Ã©tiquettes.
Outre les graphes que nous venons de voir, Suzuki et al. ont introduit dans (Suzuki et al.,
2003a,b) la notion de graphe acyclique orientÃ© hiÃ©rarchique (HDAG). Les HDAG sont des
graphes dont certains nÅ“uds contiennent des graphes acycliques orientÃ©s. Cette structure a
Ã©tÃ© proposÃ©e pour permettre la reprÃ©sentation de documents textuels ainsi que dâ€™informations
connexes. En effet, un document textuel peut subir de multiple prÃ©-traitement et des infor-
mations grammaticales et sÃ©mantiques peuvent lui Ãªtre ajoutÃ©es. Ces informations combinÃ©es
entre elles forment des structures hiÃ©rarchiques complexes.
En outre, un noyau a Ã©tÃ© proposÃ© pour calculer la similaritÃ© entre les HDAG. Ce noyau a Ã©tÃ©
Ã©valuÃ© sur un problÃ¨me de classification multi-classe avec lâ€™algorithme SVM et la mÃ©thode "un
contre tous" (un classifieur SVM par classe). Une base de donnÃ©es de 3000 questions divisÃ©es
en 148 classes a Ã©tÃ© utilisÃ©e pour lâ€™expÃ©rimentation. Les questions ont Ã©tÃ© prÃ©-traitÃ©es Ã  lâ€™aide
dâ€™un parser. En outre, les entitÃ©s nommÃ©es ont Ã©tÃ© Ã©tiquetÃ©es et les informations sÃ©mantiques
ajoutÃ©s.
Les rÃ©sultats ont montrÃ©s que le noyau HDAG Ã©tait plus performant que le noyau SubString
Kernel et le noyau "sac de mots".
6 Conclusion
Nous avons prÃ©sentÃ© une collection de mÃ©thodes assez variÃ©es, adaptÃ©es aux diffÃ©rents cas
rencontrÃ©s lors du traitement des donnÃ©es structurÃ©es. La diversitÃ© de ces mÃ©thodes rend pour
lâ€™instant dÃ©licate toute Ã©valuation comparative de leurs performances respectives. Le caractÃ¨re
trÃ¨s rÃ©cent de ces travaux fait quâ€™il nâ€™existe pas pour lâ€™instant dâ€™Ã©tude expÃ©rimentale compara-
tive sÃ©rieuse des comportements de ces diffÃ©rents algorithmes sur des donnÃ©es issues du monde
rÃ©el et plus particuliÃ¨rement du langage naturel.
Insistons sur le fait que la souplesse des mÃ©thodes Ã  noyaux facilite la construction de mÃ©thodes
ad-hoc adaptÃ©es Ã  la structure du problÃ¨me Ã  traiter.
S. Aseervatham et E. Viennet
RÃ©fÃ©rences
Collins, M. (2002). Discriminative training methods for hidden markov models : Theory and
experiments with perceptron algorithms. In Proceedings of EMNLP.
Collins, M. et N. Duffy (2002). Convolution kernels for natural language. In NIPS : Advances
in Neural Information Processing Systems 14, pp. 625â€“632. MIT Press.
GÃ¤rtner, T. (2003). A survey of kernels for structured data. SIGKDD Explor. Newsl. 5(1),
49â€“58.
GÃ¤rtner, T., K. Driessens, et J. Ramon (2003). Graph kernels and gaussian processes for rela-
tional reinforcement learning. In ILP, pp. 146â€“163.
GÃ¤rtner, T., Q. V. Le, et A. J. Smola (2006). A short tour of kernel methods for graphs.
Haussler, D. (1999). Convolution kernels on discrete structures. Technical Report UCS-CRL-
99-10, UC Santa Cruz.
Joachims, T. (2002). Learning to Classify Text Using Support Vector Machines : Methods,
Theory and Algorithms. Norwell, MA, USA : Kluwer Academic Publishers.
Kashima, H. et A. Inokuchi (2002). Kernels for graph classification. In ICDM â€™02 : Procee-
dings of the First International Conference On Data Mining, Workshop on Active Mining.
Kashima, H. et T. Koyanagi (2002). Kernels for semi-structured data. In ICML â€™02 : Procee-
dings of the Nineteenth International Conference on Machine Learning, San Francisco, CA,
USA, pp. 291â€“298. Morgan Kaufmann Publishers Inc.
Kashima, H. et Y. Tsuboi (2004). Kernel-based discriminative learning algorithms for labeling
sequences, trees, and graphs. In ICML â€™04 : Proceedings of the twenty-first international
conference on Machine learning, New York, NY, USA, pp. 58. ACM Press.
Kashima, H., K. Tsuda, et A. Inokuchi (2003). Marginalized kernels between labeled graphs.
In T. Faucett et N. Mishra (Eds.), Proceedings of the 20th International Conference on Ma-
chine Learning, pp. 321â€“328. AAAI Press.
Leslie, C. S., E. Eskin, et W. S. Noble (2002). The spectrum kernel : A string kernel for svm
protein classification. In Pacific Symposium on Biocomputing, pp. 566â€“575.
Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, et C. J. C. H. Watkins (2002). Text
classification using string kernels. Journal of Machine Learning Research 2, 419â€“444.
Shawe-Taylor, J. et N. Cristianini (2004). Kernel Methods for Pattern Analysis. Cambridge
University Press.
Suzuki, J., T. Hirao, Y. Sasaki, et E. Maeda (2003a). Hierarchical directed acyclic graph kernel :
methods for structured natural language data. In ACL â€™03 : Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics, Morristown, NJ, USA, pp. 32â€“39.
Association for Computational Linguistics.
Suzuki, J., Y. Sasaki, et E. Maeda (2003b). Kernels for structured natural language data. In
NIPS : Advances in Neural Information Processing Systems 16. MIT Press.
Vapnik, V. N. (1995). The nature of statistical learning theory. New York, NY, USA : Springer-
Verlag New York, Inc.
MÃ©thodes Ã  noyaux appliquÃ©es aux textes structurÃ©s
Summary
This paper review the application of kernel methods to the mining of structured data. Mod-
ern applications of data mining must handle structured data, e.g. for text mining, and learning
algorithms should benefit of the use of this structural information, which is an interesting chal-
lenge. One of the possible approach to this problem is the use of Mercerâ€™s kernels. These
kernels compute a similarity measure between complex data, and can be used in a lot of learn-
ing algorithms (Support Vector Machines, PCA, Discriminant Analysis, Perceptron, etc). We
present the most important kernels proposed during the last years to handle structured data like
sequences, trees and graphs.
