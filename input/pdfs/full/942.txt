Partitionnement des donnÃ©es pour les problÃ¨mes
de classement difficiles:
Combinaison des cartes topologiques mixtes et SVM
Mustapha Lebbahâˆ—, Mohamed Ramzi Temanniâˆ—,âˆ—âˆ—, Christine Poitou-Bernertâˆ—âˆ—,âˆ—âˆ—âˆ—,âˆ—âˆ—âˆ—âˆ—
Karine Clementâˆ—âˆ—,âˆ—âˆ—âˆ—,âˆ—âˆ—âˆ—âˆ—, Jean-Daniel Zuckerâˆ—,âˆ—âˆ—
âˆ— UniversitÃ© Paris 13, UFR de SantÃ©,
MÃ©decine et Biologie Humaine (SMBH) - LÃ©onard de Vinci- LIM&BIO
74, rue Marcel Cachin 93017 Bobigny Cedex France
nom@limbio-paris13.org,
http://www.limbio-paris13.org
âˆ—âˆ— Inserm, U755 Nutriomique, 75004 Paris, France;
âˆ—âˆ—âˆ— University Pierre and Marie Curie-Paris 6, Faculty of Medicine,
Les Cordeliers, 75004 Paris, France;
âˆ—âˆ—âˆ—âˆ— AP-HP, HÃ´tel-Dieu Hospital, Nutrition department,
1 Place du parvis Notre-Dame, 75004 Paris, France
prÃ©nom.nom@htp.aphp.fr
RÃ©sumÃ©. Dans ce papier, nous prÃ©sentons un modÃ¨le pour aborder les pro-
blÃ¨mes de classement difficiles. Ces problÃ¨mes ont souvent la particularitÃ© dâ€™avoir
des taux dâ€™erreurs en gÃ©nÃ©ralisations trÃ¨s Ã©levÃ©s et ce quelles que soient les mÃ©-
thodes utilisÃ©es. Pour ce genre de problÃ¨me, nous proposons dâ€™utiliser un modÃ¨le
de classement combinant le modÃ¨le de partitionnement des cartes topologiques
mixtes et les machines Ã  vecteur de supports (SVM). Le modÃ¨le non supervisÃ©
est dÃ©diÃ© Ã  la visualisation et au partitionnement des donnÃ©es composÃ©es de va-
riables quantitatives et/ou qualitatives. Le deuxiÃ¨me modÃ¨le supervisÃ©, est dÃ©diÃ©
au classement. Dans ce papier, nous prÃ©sentons une combinaison de deux mo-
dÃ¨les qui permettent dâ€™amÃ©liorer la visualisation des donnÃ©es et dâ€™augmenter
les performances en classement. Ce modÃ¨le consiste Ã  entraÃ®ner des cartes auto-
organisatrices pour construire une partition organisÃ©e des donnÃ©es, constituÃ©e
de plusieurs sous-ensembles qui vont servir Ã  reformuler le problÃ¨me de clas-
sement initial en sous-problÃ¨me de classement. Pour chaque sous-ensemble, on
entraÃ®ne un classeur SVM spÃ©cifique. Pour la validation de notre modÃ¨le (CT-
SVM), nous avons utilisÃ© quatre jeux de donnÃ©es. La premiÃ¨re base est un ex-
trait dâ€™une grande base mÃ©dicale sur lâ€™Ã©tude de lâ€™obÃ©sitÃ© Ã  lâ€™HÃ´pital HÃ´tel-Dieu
de Paris, et les trois derniÃ¨res bases sont issues de la littÃ©rature. Les rÃ©sultats
obtenus montrent lâ€™apport de ce modÃ¨le dans la visualisation et le classement de
donnÃ©es complexes.
Partitionnement par les cartes topologiques mixtes et classement par les SVM
1 Introduction
En apprentissage artificiel, on distingue deux grands thÃ¨mes, lâ€™apprentissage supervisÃ© et lâ€™ap-
prentissage non supervisÃ© ; la plupart des problÃ¨mes dâ€™apprentissage sont traitÃ©s par lâ€™une des
deux approches. Souvent pour les problÃ¨mes de classement, on a recours Ã  de nombreuses mÃ©-
thodes qui sont Ã©valuÃ©es sur leur capacitÃ© Ã  prÃ©dire correctement la classe des observations qui
nâ€™ont pas participÃ© Ã  la phase dâ€™apprentissage. Pour lâ€™apprentissage non supervisÃ©, les critÃ¨res
de qualitÃ© sont plus difficiles Ã  dÃ©finir ; ils sâ€™articulent autour de lâ€™interprÃ©tation des regroupe-
ments ou des partitions obtenues.
Parmi les problÃ¨mes dâ€™apprentissage, il existe une catÃ©gorie de problÃ¨mes qui sont appelÃ©s
dans la littÃ©rature : difficiles, complexes, hÃ©tÃ©rogÃ¨nes. Ces problÃ¨mes ont souvent la particula-
ritÃ© dâ€™avoir des rÃ©sultats non satisfaisants quelques soit la mÃ©thode standard utilisÃ©e (arbre
de dÃ©cision, SVM, MLP,... ). En dâ€™autres termes, ces mÃ©thodes obtiennent lÃ©gÃ¨rement un
meilleur rÃ©sultat quâ€™un algorithme qui ferait un tirage alÃ©atoire. Dans dâ€™autres problÃ¨mes, ap-
pelÃ©s aussi problÃ¨mes difficiles, on dispose de donnÃ©es issues de sources diffÃ©rentes ou des
donnÃ©es mixtes. Ces problÃ¨mes existent dans de nombreux domaines ; souvent ils sont mal
posÃ©s, les bases dâ€™apprentissage disposent de peu dâ€™observations et/ou le nombre de variables
est trop grand. Parmi ces domaines, on retrouve le domaine mÃ©dical, spÃ©cialement les bases
cliniques ou les bases biopuces,(Weston et al (2001); Tamayo et al (1999)). De nombreuses
mÃ©thodes sont dÃ©veloppÃ©es pour ce genre de problÃ¨me qui consiste Ã  utiliser les techniques de
sÃ©lection de variables (Golub et al (1999); Xing et al (2001); Peng et al (2005)), la refor-
mulation de problÃ¨me en utilisant le classement heuristique, Clancey (1985), et le boosting
(Philip et al (2003)). Dâ€™autres mÃ©thodes consistent Ã  combiner ou fusionner les classifieurs
Egmont-Petersen et al (1999); Liu et al (2001).
Dans ce papier nous prÃ©sentons un modÃ¨le combinant deux modÃ¨les dâ€™apprentissages pour
aborder les problÃ¨mes de classement difficiles ou "complexes". La question qui se pose pour
ce genre de base complexe, faut-il aborder le problÃ¨me de classement dâ€™une maniÃ¨re globale
ou trouver un moyen de le diviser en sous-problÃ¨mes ?. Notre modÃ¨le consiste Ã  partitionner
les donnÃ©es pour sÃ©lectionner le classifieur adÃ©quat pour chaque observation,(Liu et al (2001);
Rybnik et al (2003)).
Quelques mÃ©thodes spÃ©cifiques combinant lâ€™apprentissage non supervisÃ© et supervisÃ©, aussi
bien dans le domaine de la classification hiÃ©rarchique et les arbres de dÃ©cision que pour la
recherche de partitions ont Ã©tÃ© dÃ©veloppÃ©es. Dans le domaine de la classification hiÃ©rarchique,
Hyun-Chul et al (2003); Benabdeslem (2006) proposent des modÃ¨les combinant la classi-
fication hiÃ©rarchique en entraÃ®nant un SVM binaire dans chaque nÅ“ud du dendogramme de
la classification hiÃ©rarchique. Lebrun et al (2004); Sungmoon et al (2004); Shaoning et al
(2005) proposent dâ€™utiliser une autre mÃ©thode de partitionnement qui permet de construire une
partition de sous ensembles ordonnÃ©s sous forme dâ€™arbre binaire afin dâ€™apprendre un SVM
binaire au niveau de chaque nÅ“ud de lâ€™arbre.
Dans Wu et al (2004), les auteurs proposent dâ€™utiliser les cartes topologiques de Kohonen (Ko-
honen (1995)) pour filtrer les donnÃ©es. Les observations non Ã©tiquetÃ©es hÃ©ritent de lâ€™Ã©tiquette
de la classe du vote majoritaire de son sous-ensemble. A la fin de cette phase, un seul SVM
RNTI - X - 2
Lebbah et al.
est appris sur lâ€™ensemble dâ€™apprentissage initial rÃ©Ã©tiquetÃ©, sans prendre en compte la parti-
tion de donnÃ©es. Nous trouvons aussi lâ€™utilisation dâ€™autre mÃ©thodes de partitionnement comme
le k-means lorsqu ?un sous-ensemble de la partition est constituÃ© Ã  100% dâ€™une seule classe
alors toutes les observations de ce sous-ensemble sont remplacÃ©es par leur rÃ©fÃ©rent (reprÃ©sen-
tant), calculÃ© par le K-means (K-moyennes), dans la base dâ€™apprentissage dÃ©diÃ©e au SVM. Ce
procÃ©dÃ© permet de rÃ©duire significativement la taille de la base et par consÃ©quent le temps de
calcul sur de grandes bases de donnÃ©es. Dâ€™autres mÃ©thodes sont aussi inspirÃ©es des mÃ©thodes
de partitionnement et de classement comme la dÃ©finition de cartes topologiques dans lâ€™espace
de redescription (Sungmoon et al (2004)) ou lâ€™utilisation des vecteurs supports pour dÃ©finir
une partition (Ben-Hur et al (2001)).
Toutes ces mÃ©thodes ont un point commun, celui de montrer que la visualisation et le prÃ©-
traitement des donnÃ©es sont des Ã©tapes importantes dans la phase exploratoire de lâ€™analyse
de donnÃ©es. Cette phase permet dâ€™inclure les connaissances de lâ€™expert du domaine avant la
phase de classement. La difficultÃ© en classement augmente dâ€™autant plus quâ€™il sâ€™agit de don-
nÃ©es complexes, par exemple donnÃ©es mixtes (quantitatives et qualitatives) ou des donnÃ©es
biomÃ©dicales, pour lesquelles il existe moins de mÃ©thodes standards.
Notre approche consiste Ã  diviser le problÃ¨me global de classement en sous-problÃ¨me de clas-
sement guidÃ© par la structure et lâ€™organisation des donnÃ©es de la base dans lâ€™espace des don-
nÃ©es. Ce modÃ¨le est basÃ© sur le partitionnement des donnÃ©es, avec une mÃ©thode non supervisÃ©e,
en une partition constituÃ©e de plusieurs sous-ensembles organisÃ©s, en tenant en compte de la ty-
pologie des donnÃ©es, qui vont servir Ã  dÃ©finir un classifieur pour chacun en utilisant les SVMs,
Vapnik (1995). La tÃ¢che de partitionnement des donnÃ©es de notre modÃ¨le est rÃ©alisÃ©e Ã  lâ€™aide
des cartes auto-organisatrices (Kohonen (1995); Lebbah et al (2005)).
Les cartes topologiques sont utilisÃ©es dans notre modÃ¨le parce quâ€™elles permettent Ã  la fois
dâ€™Ãªtre utilisÃ© comme outils de visualisation et de partitionnement non supervisÃ© de diffÃ©rents
types de donnÃ©es (quantitatives et qualitatives). Elles permettent de projeter les donnÃ©es sur des
espaces discrets qui sont gÃ©nÃ©ralement de dimensions deux. Le modÃ¨le de base, proposÃ© par
Kohonen (Kohonen (1995)), est uniquement dÃ©diÃ© aux donnÃ©es numÃ©riques. Des extensions
et des reformulations du modÃ¨le de Kohonen ont Ã©tÃ© proposÃ©es dans la littÃ©rature, bishop et al
(1998); Lebbah et al (2000, 2005). Une gÃ©nÃ©ralisation des cartes topologiques sera prÃ©sentÃ©e
dans ce papier.
Les machines Ã  vecteurs de support ont Ã©tÃ© dÃ©veloppÃ©es dans les annÃ©es 90 par Vapnik (1995).
Ces mÃ©thodes ont Ã©tÃ© utilisÃ©es dans notre modÃ¨le parce quâ€™elles sâ€™avÃ¨rent particuliÃ¨rement ef-
ficaces, car elles peuvent traiter des problÃ¨mes mettant en jeu un grand nombre de variables
ou un petit nombre dâ€™observations (individus), et quâ€™elles assurent une solution unique (pas
de problÃ¨mes de minimum local comme pour les rÃ©seaux de neurones). Lâ€™algorithme sous sa
forme initiale revient Ã  chercher une frontiÃ¨re de dÃ©cision linÃ©aire entre deux classes, mais
ce modÃ¨le peut considÃ©rablement Ãªtre enrichi en se projetant dans un autre espace permettant
ainsi dâ€™augmenter la sÃ©parabilitÃ© des donnÃ©es. Ce cas dâ€™utilisation des machines Ã  vecteurs de
support est le plus utilisÃ©, car la plupart des problÃ¨mes rÃ©els sont non linÃ©airement sÃ©parables.
Dans ce cas, on se sert plutÃ´t de lâ€™astuce du noyau (kernel trick), par exemple : noyau linÃ©aire,
RNTI - X - 3
Partitionnement par les cartes topologiques mixtes et classement par les SVM
noyau polynomial, noyau Gaussien (Radial). Ces fonctions sont des fonctions non linÃ©aires,
elles jouent un rÃ´le similaire au rÃ´le du produit scalaire dans les problÃ¨mes dâ€™optimisation, et
elles sont vues comme une mesure de similaritÃ©.
Pour la comprÃ©hension de notre modÃ¨le, nous prÃ©sentons dans la section 1.1 les diffÃ©rentes
notations utilisÃ©es. Dans la section 2 nous prÃ©sentons le modÃ¨le des cartes topologiques pour
lâ€™analyse des donnÃ©es mixtes qui est une gÃ©nÃ©ralisation des cartes topologiques classiques de
Kohonen, ainsi que lâ€™amÃ©lioration apportÃ©e Ã  ce modÃ¨le, par rapport Ã  la version prÃ©sentÃ©e
dans Lebbah et al (2005), pour quâ€™il prenne en compte la particularitÃ© des donnÃ©es mixtes.
Pour simplifier la prÃ©sentation du papier, le modÃ¨le SVM ne sera pas prÃ©sentÃ©. Dans la sec-
tion 3, nous prÃ©sentons le modÃ¨le que nous proposions pour le classement. Dans la section 4,
une validation du modÃ¨le sur des donnÃ©es issues de la littÃ©rature ainsi que des donnÃ©es mÃ©-
dicales rÃ©elles. Cette validation permet de dÃ©montrer que notre modÃ¨le peut Ãªtres utilisÃ© pour
augmenter les performances en classement sur certaines bases de donnÃ©es.
1.1 Notations-DÃ©finitions
Ce paragraphe introduit les notations de base utilisÃ©es. Lâ€™ensemble D reprÃ©sente lâ€™espace des
observations ; les observations sont supposÃ©es quantitatives ou qualitatives et en dimension
multiple ; on suppose que chaque observation est de dimension d. On suppose, par la suite
que lâ€™on dispose dâ€™observations correspondant Ã  N individus reprÃ©sentÃ©s par lâ€™ensemble des
couples A = {(zi, yi); i = 1..N} oÃ¹ lâ€™observation est zi et yi lâ€™Ã©tiquette de sa classe. Cette
Ã©tiquette sera utilisÃ©e dans lâ€™apprentissage supervisÃ© (SVM).
La mÃ©thode de partitionnement cherche Ã  dÃ©terminer une partition deD enNcell sous-ensembles
qui sera notÃ©e P = {P1, ..., PNcell}. A chaque sous-ensemble Pc, on associe un vecteur rÃ©fÃ©-
rentwc âˆˆ D qui sera le reprÃ©sentant ou le "rÃ©sumÃ©" de lâ€™ensemble des observations de Pc. Par
la suite nous notonsW = {wc; c = 1..Ncell} lâ€™ensemble des vecteurs rÃ©fÃ©rents. La partition
P de D peut Ãªtre dÃ©fini dâ€™une maniÃ¨re Ã©quivalente avec la fonction dâ€™affectation Ï† qui est une
application de D dans lâ€™ensemble fini des indices I = {1, 2, ..., Ncell}.
Dans le cas oÃ¹ il y a eu regroupement des sous-ensembles, nous avons dÃ©fini une application
surjective Ï‡ de I dans lâ€™ensemble des indices J = {1, 2, ..., S} oÃ¹ 1 â‰¤ S â‰¤ Ncell. Si on
utilise ces dÃ©finitions, le sous-ensemble Pc est alors reprÃ©sentÃ© par Pc = {z âˆˆ D/Ï†(z) =
c, Ï‡(c) âˆˆ J }, (si Ï‡(c) = 1 alors P = Pc = A). On notera pas la suite lâ€™ensemble des indices
Ip des sous-ensemblepurses tel que Ip = {c/âˆ€z âˆˆ Pc, Ï‡(Ï†(z)) = c, vote(Pc) = yc}. yc est
lâ€™Ã©tiquette du vote majoritaire Ã  100% du sous-ensemble Pc en utilisant la fonction vote.
2 Cartes Topologique Mixtes
On suppose que lâ€™on dispose dans le cas des cartes topologiques de la base dâ€™apprentissage
A sans les Ã©tiquettes A = {zi; i = 1..N}. Les observations zi sont composÃ©es de deux
parties : la partie numÃ©rique zri = (z1ri , z2ri , ..., znri ) (zri âˆˆ Rn), et la partie binaire zbi =
(z1bi , z
2b
i , ..., z
mb
i ) (zbi âˆˆ Î²m = {0, 1}m). Avec ces notations une observation zi = (zri , zbi ) est
RNTI - X - 4
Lebbah et al.
Ï†
 
 

 


  
 
 
  


  
 
	 	
	 	 




 


  
 
 
 






















ff
ff
fi
fi
fl
fl
ffi
ffi


 
 
!
!
" "
" "
# #
# #
$ $
$ $
% %
% %
& &
& &
' '
' '
( (
( (
) )
) )
* *
* *
+ +
+ +
, ,
, ,
- -
- -
. .
. .
/ /
/ /
0 0
0 0
1 1
1 1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
:
:
;
;
<
<
=
=
>
>
?
?
@
@
A
A
B
B
C
C
D
D
E
E
F
F
G
G
H
H
I
I
J
J
K
K
L
L
M
M
N
N
O
O
P
P
Q
Q
R
R
S
S
T
T
U
U
V V
V V
W W
W W
X X
X X
Y Y
Y Y
Z Z
Z Z
[ [
[ [
\ \
\ \
] ]
] ]
^ ^
^ ^
_ _
_ _
` `
` `
a a
a a
b b
b b
c c
c c
d d
d d
e e
e e
f
f
g
g
h
h
i
i
j
j
k
k
l
l
m
m
n
n
o
o
p
p
q
q
r
r
s
s
t
t
u
u
v
v
w
w
x x
x x
y y
y y
z z
z z
{ {
{ {
| |
| |
} }
} }
~ ~
~ ~
 
 
Â€ Â€
Â€ Â€
Â Â
Â Â
Â‚ Â‚
Â‚ Â‚
Âƒ Âƒ
Âƒ Âƒ
Â„ Â„
Â„ Â„
Â… Â…
Â… Â…
Â† Â†
Â† Â†
Â‡ Â‡
Â‡ Â‡
Âˆ Âˆ
Âˆ Âˆ
Â‰ Â‰
Â‰ Â‰
Å
Å
Â‹
Â‹
ÂŒ
ÂŒ
Â
Â
Â
Â
Â
Â
Â
Â
Â‘
Â‘
Â’
Â’
Â“
Â“ Â”
Â”
Â•
Â•
Â–
Â–
Â—
Â—
Â˜
Â˜
Â™
Â™ Âš
Âš
Â›
Â›
Âœ
Âœ
Â
Â
Â
Â
ÂŸ
ÂŸ
Â 
Â 
Â¡
Â¡
Â¢
Â¢
Â£
Â£ Â¤
Â¤
Â¥
Â¥
Â¦
Â¦
Â§
Â§ Â¨ Â¨
Â¨ Â¨
Â© Â©
Â© Â©
Âª Âª
Âª Âª
Â« Â«
Â« Â«
Â¬ Â¬
Â¬ Â¬
Â­ Â­
Â­ Â­
Â® Â®
Â® Â®
Â¯ Â¯
Â¯ Â¯
Â° Â°
Â° Â°
Â± Â±
Â± Â±
Â² Â²
Â² Â²
Â³ Â³
Â³ Â³
Â´ Â´
Â´ Â´
Âµ Âµ
Âµ Âµ
Â¶
Â¶
Â·
Â·
Â¸
Â¸
Â¹
Â¹
Âº
Âº
Â»
Â» Â¼ Â¼
Â¼ Â¼
Â½ Â½
Â½ Â½
Â¾ Â¾
Â¾ Â¾
Â¿ Â¿
Â¿ Â¿
Ã€ Ã€
Ã€ Ã€
Ã Ã
Ã Ã
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ã‚
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ãƒ
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã„
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã…
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã†
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ã‡
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ãˆ
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
Ã‰
ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ ÃŠ
Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹ Ã‹
ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ ÃŒ
Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã
Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã
Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã
Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã Ã
Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘ Ã‘
Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’ Ã’
Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“ Ã“
Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã” Ã”
c
r
D
C
z
Ã•
Ã•
Ã–
Ã–
FIG. 1 â€“ Carte topologique de dimension 10Ã—10, (Î´(c, r) = 4). Ï† est la fonction dâ€™affectation
de lâ€™espace des donnÃ©esD dans lâ€™espace de la carte C.
de dimension d = n+m (numÃ©rique et binaire).
Comme tout modÃ¨le de cartes topologiques, nous supposons que lâ€™on dispose dâ€™une carte dis-
crÃ¨te C ayant Ncell cellules structurÃ©es par un graphe non orientÃ©. Cette structure de graphe
permet de dÃ©finir une distance, Î´(r, c) entre deux cellules r et c de C, comme Ã©tant la longueur
de la plus courte chaÃ®ne permettant de relier les cellules r et c, (voir figure 1). Le systÃ¨me de
voisinage est dÃ©fini grÃ¢ce Ã  la fonction noyau K (K â‰¥ 0 et lim
|x|â†’âˆ
K(x) = 0). Lâ€™influence
mutuelle entre deux cellules c et r est dÃ©finie par la fonctionK(Î´(c, r)).
A chaque cellule c de la carte, est associÃ©e un vecteur rÃ©fÃ©rent wc = (wrc,wbc ) de dimension
d oÃ¹ wrc âˆˆ Rn et wbc âˆˆ Î²m. Par la suite nous notons W lâ€™ensemble des vecteurs rÃ©fÃ©rents
constituÃ©s par les parties numÃ©riques et par la partie binaire.
Dans la section suivante nous prÃ©sentons un modÃ¨le original de cartes topologiques dÃ©diÃ©es
aux donnÃ©es mixtes avec la prise en compte des deux espaces rÃ©el et binaire en dÃ©finissant un
hyper-paramÃ¨tre pour contrÃ´ler les variables quantitatives et qualitatives codÃ©es en binaires.
Lâ€™algorithme dâ€™apprentissage associÃ© est dÃ©rivÃ© de lâ€™algorithme Batch de Kohonen dÃ©diÃ© aux
donnÃ©es numÃ©riques (Kohonen (1995)) et de lâ€™algorithme BinBatch dÃ©diÃ© aux donnÃ©es bi-
naires (Lebbah et al (2000)). Dans cet algorithme, lâ€™indice de similaritÃ© et lâ€™estimation des
vecteurs rÃ©fÃ©rents sont spÃ©cifiques pour chaque partie de la base : câ€™est la distance euclidienne
avec le vecteur moyen pour la partie numÃ©rique et la distance de Hamming et le centre mÃ©dian
pour la partie binaire.
2.1 Minimisation de la fonction de coÃ»t
Comme dans le cas des cartes topologiques (Kohonen (1995)) nous proposons de minimiser
la fonction de coÃ»t suivante :
E(Ï†,W) =
âˆ‘
ziâˆˆApp
âˆ‘
râˆˆC
K(Î´(Ï†(zi), r))||zi âˆ’wr||
2 (1)
RNTI - X - 5
Partitionnement par les cartes topologiques mixtes et classement par les SVM
OÃ¹ Ï† affecte chaque observation z Ã  une cellule unique de la carte C.
Dans cette expression ||z âˆ’wr||2 reprÃ©sente le carrÃ© de la distance euclidienne. Etant donnÃ©
que, pour les donnÃ©es binaires la distance euclidienne nâ€™est rien dâ€™autre que la distance de
HammingH, la distance euclidienne peut Ãªtre rÃ©Ã©crite : ||zâˆ’wr||2 = ||zrâˆ’wrr ||2+H(zb,wbr).
Pour contrÃ´ler les deux parties des donnÃ©es (rÃ©elles et binaires), nous avons utilisÃ© un hyper-
paramÃ¨tre F , qui respecte la propriÃ©tÃ© 0 â‰¤ F â‰¤ 1 pour pondÃ©rer les variables rÃ©elles et
qualitatives codÃ©es en binaires. Cette pondÃ©ration permet de pallier le problÃ¨me dâ€™Ã©chelle entre
les variables binaires et rÃ©elles normalisÃ©es entre 0 et 1. Ainsi, la distance est Ã©gale Ã  :
||zâˆ’wr||
2 = (1âˆ’ F )||zr âˆ’wrr ||
2 + FH(zb,wbr).
Utilisant cette expression, la fonction de coÃ»t devient :
E(Ï†,W) = (1âˆ’ F )
âˆ‘
ziâˆˆApp
âˆ‘
râˆˆC
K(Î´(Ï†(zi), r))Deuc(z
r
i ,w
r
r)
+F
âˆ‘
ziâˆˆApp
âˆ‘
râˆˆC
K(Î´(Ï†(zi), r))H(z
b
i ,w
b
r) (2)
La fonction de coÃ»t peut Ãªtre encore rÃ©Ã©crite :
E(Ï†,W) = (1âˆ’ F )Esom(Ï†,W
r) + FEbin(Ï†,W
b) (3)
OÃ¹
Esom(Ï†,W) =
âˆ‘
ziâˆˆApp
âˆ‘
râˆˆC
KT (Î´(Ï†(zi), r))||z
r
i âˆ’w
r
r ||
2 (4)
est la fonction de coÃ»t classique utilisÃ©e par lâ€™algorithme de Kohonen (la version batch), Ko-
honen (1995).
Et
Ebin(Ï†,W) =
âˆ‘
ziâˆˆApp
âˆ‘
râˆˆC
KT (Î´(Ï†(zi), r))H(z
b
i ,w
b
r) (5)
est la fonction de coÃ»t classique utilisÃ©e par lâ€™algorithme BinBatch, Lebbah et al (2000).
Dans le cas particulier oÃ¹ F âˆˆ {0, 1} la fonction de coÃ»t (3) est rÃ©duite Ã  la fonction de coÃ»t
(4) ou (5) utilisÃ©es respectivement dans le cas numÃ©rique et binaire. Pour les autres valeurs de
F les deux parties sont prises en compte avec leurs pondÃ©rations. Le choix du paramÃ¨tre F est
dÃ©terminÃ© par expÃ©rimentation.
Pour un paramÃ¨tre F fixÃ©, La minimisation de la nouvelle fonction de coÃ»t globale (3) est
rÃ©alisÃ©e Ã  lâ€™aide dâ€™une procÃ©dure itÃ©rative en deux phases :
1. Phase dâ€™affectation : mise Ã  jour de la fonction dâ€™affectation Ï† associÃ©e Ã  lâ€™ensembleW
fixÃ©. On affecte chaque observation z au rÃ©fÃ©rent dÃ©fini Ã  partir de lâ€™expression suivante :
âˆ€z, Ï†(z) = argmin
c
((1 âˆ’ F )||zr âˆ’wrc ||
2 + FH(zb,wbc)) (6)
RNTI - X - 6
Lebbah et al.
2. Phase de dâ€™optimisation : La fonction dâ€™affectation Ã©tant fixÃ©e Ã  sa valeur courante,
choisir le systÃ¨me de rÃ©fÃ©rents qui minimise la fonction E(Ï†,W) dans lâ€™espace Rn Ã—
Î²m. Ceci nous amÃ¨ne Ã  minimiser la fonction Esom(Ï†,W) (4) dans Rn et la fonction
Ebin(Ï†,W) (5) dans Î²m. Ces deux minimisations permettent de dÃ©finir les expressions
nÃ©cessaires pour calculer lâ€™ensemble des rÃ©fÃ©rents :
â€“ la partie numÃ©rique wrc du vecteur rÃ©fÃ©rent wc est le vecteur moyen dÃ©fini comme
suit :
wc =
âˆ‘
ziâˆˆA
K(Î´(c, Ï†(zi)))z
r
i
âˆ‘
ziâˆˆA
K(Î´(c, Ï†(zi)))
,
â€“ la partie binaire wbc du vecteur rÃ©fÃ©rent wc est le centre mÃ©dian de la partie binaire
des observations zi âˆˆ A pondÃ©rÃ©es par K(Î´(c, Ï†(zi))). Chaque composante wbc =
(wb1c , ..., w
bk
c , ..., w
m
c ) est calculÃ©e comme suit :
wkbc =
ï£±ï£²
ï£³
0 si
[âˆ‘
ziâˆˆA
K(Î´(c, Ï†(zi)))(1 âˆ’ z
bk
i )
]
â‰¥[âˆ‘
ziâˆˆA
K(Î´(c, Ï†(zi)))z
bk
i
]
1 sinon
,
La minimisation de la fonction de coÃ»t E(Ï†,W) sâ€™effectue par itÃ©ration successive des deux
phases jusquâ€˜Ã  stabilisation ou jusquâ€˜Ã  un nombre dâ€™itÃ©rations dÃ©finies Ã  lâ€™avance. A la fin de
lâ€™apprentissage, wc partage le mÃªme codage que les observations initiales, ce qui permet une
interprÃ©tation symbolique de la partie binaire du rÃ©fÃ©rent. La qualitÃ© de la partition rÃ©sultat de
la carte topologique ainsi que lâ€™ordre topologique fourni par la grille, dÃ©pend fortement de la
fonction voisinage K. Dans la pratique, comme dans le cas des cartes topologiques classiques,
nous utilisons une fonction noyau avec un paramÃ¨tre T pour contrÃ´ler la taille du voisinage
dÃ©finie par :KT (Î´(c, r)) = exp(âˆ’0.5Î´(c,r)
T
). Ainsi, par analogie avec lâ€™algorithme de Kohonen,
les deux itÃ©rations prÃ©cÃ©dentes sont rÃ©pÃ©tÃ©es en faisant dÃ©croÃ®tre le paramÃ¨tre T entre deux
valeurs Tmax et Tmin.
3 MÃ©thode hybride CT-SVM :Cartes topologiques et SVM
Pour certains problÃ¨mes de classement, il est prÃ©fÃ©rable de dÃ©composer le problÃ¨me global de
classement en sous-problÃ¨mes pour amÃ©liorer les performances en classement, (Platt (1999);
Gamma et al (2000); Kuncheva et al (2002); Lebrun et al (2004)). Par exemple, si lâ€™on
dispose dâ€™une base de donnÃ©es oÃ¹ certaines observations sont linÃ©airement sÃ©parables et les
autres sont non linÃ©airement sÃ©parables, alors il est possible de dÃ©composer la base entiÃ¨re en
deux sous-ensembles et dâ€™entraÃ®ner un classifieur SVM pour chacun des sous-ensembles. Ce
cas dâ€™utilisation des machines Ã  vecteurs de support dans le cas non linÃ©aire est le plus intÃ©-
ressant car la plupart des problÃ¨mes rÃ©els sont non linÃ©airement sÃ©parables. Il est Ã©vident que
la dÃ©termination du nombre dâ€™observations et par consÃ©quent la taille de la partition utilisÃ©e
pour lâ€™apprentissage de chaque SVM est important dâ€™un point de vue de la thÃ©orie de lâ€™ap-
prentissage, Vapnik (1995). Dans cette section, nous ne prÃ©sentons pas un indice permettant
RNTI - X - 7
Partitionnement par les cartes topologiques mixtes et classement par les SVM
dâ€™estimer la taille de la partition, mais nous allons prÃ©senter par la suite un modÃ¨le de classe-
ment qui permet dâ€™augmenter les performances en classement en utilisant le partitionnement
des observations.
Dans (Kuncheva , 2004, chapitre 6), lâ€™auteur fournit une dÃ©monstration pour ce type de mo-
dÃ¨le. Si lâ€™on considÃ¨re que lâ€™on dispose de S classifieurs notÃ©s Cla associÃ©s Ã  diffÃ©rents sous-
ensembles Pi et si on note par p(Clai/Pi) la probabilitÃ© du classement correct avec le classi-
fieur Clai dans le sous-ensemble Pi, alors la densitÃ© de probabilitÃ© du classement correct de
notre systÃ¨me de partitionnement et de classement sâ€™Ã©crit :
p(correct) =
Sâˆ‘
i=1
p(Pi)p(Clai/Pi)
OÃ¹ p(Pi) est la probabilitÃ© a priori que lâ€™observation soit gÃ©nÃ©rÃ©e dans le sous-ensemble
Pi. Pour maximiser ce mÃ©lange de probabilitÃ©, on choisit p(Clai/Pi) tel que p(Clai/Pi) â‰¥
p(Claj/Pj), j = 1..S.
Afin de simplifier le problÃ¨me de classement, notre approche consiste Ã  entraÃ®ner des SVMs
(Clai = SVM ) diffÃ©rents avec des sous-ensembles dâ€™une partition P de la base A. Ceci
permet de redÃ©finir des espaces de redescription diffÃ©rents (ou les mÃªmes) pour chaque sous-
ensemble Pc âˆˆ P , Lâ€™objectif de notre modÃ¨le CT-SVM est dâ€™amÃ©liorer la discrimination en
entraÃ®nant un SVM pour chaque sous-ensemble Pc âˆˆ P qui a plus dâ€™une classe (les sous-
ensembles non purs Â§1.1). Pour les sous-ensembles, qui sont composÃ©s dâ€™observation de la
mÃªme classe, aucun un SVM ne sera entraÃ®nÃ©. Lâ€™algorithme des cartes topologiques mixtes
dÃ©finit au paragraphe (Â§2) est utilisÃ© pour dÃ©finir une partition de la base dâ€™apprentissage.
Afin de rÃ©duire la partition et par consÃ©quent le nombre de SVMs entraÃ®nÃ©s, nous avons utilisÃ©
la classification hiÃ©rarchique (CAH), sur lâ€™ensemble des rÃ©fÃ©rentsW de la carte pour rÃ©duire la
partition ainsi le nombre de sous-ensembles,Yacoub et al (2001); Ripley (1996). Cette phase
de rÃ©duction de la partition, qui consiste Ã  fusionner certains sous-ensembles, est optionnelle
et, elle peut Ãªtre dÃ©terminÃ©e en interaction avec les experts et aprÃ¨s visualisation des cartes
topologiques.
Lâ€™algorithme de note modÃ¨le CT-SVM est le suivant :
Pour un nombre de sous-ensembles S fixÃ© faire :
â€“ Phase 1 : Construction dâ€™une partition P = {P1, ..., PNcell/ en utilisant les cartes topo-
logiques avec lâ€™algorithme dÃ©finit dans la section 2.1.
â€“ Phase 2 (optionnelle) : Si S < Ncell appliquer la classification hiÃ©rarchique (CAH)
pour construire la nouvelle partition P = {P1, ..., PS/1 â‰¤ S â‰¤ Ncell}
â€“ Phase 3 : DÃ©tecter lâ€™ensemble des indices Ip des sous-ensembles purs tel que Ip =
{c/âˆ€z âˆˆ Pc, Ï‡(Ï†(z)) = c, vote(Pc) = yc}. yc est lâ€™Ã©tiquette du vote majoritaire Ã  100%
RNTI - X - 8
Lebbah et al.
du sous-ensemble Pc.
â€“ Phase 4 : Apprentissage du SVM pour chaque sous-ensemble Pi tel que i /âˆˆ Ip.
Remarque :
Pour lâ€™apprentissage des cartes topologiques mixtes, nous avons utilisÃ© notre programme dÃ©-
veloppÃ© en C/C++. Nous avons aussi utilisÃ© les programmes et lâ€™heuristique dÃ©veloppÃ©e par
lâ€™Ã©quipe de Kohonen, Vesanto et al (2000), pour estimer la dimension de la carte. Pour lâ€™ap-
prentissage du modÃ¨le SVM dans le cas du multi-classe, nous avons utilisÃ© le modÃ¨le DAG-
SVM (Directed Acyclic Graph SVM) dÃ©veloppÃ© par Platt (1999); Platt et al (2000); Cawley
(2000).
Avec ce modÃ¨le CT-SVM, la topologie ou la forme des observations est prÃ©sentÃ©e par les
cartes topologiques. Lorsquâ€™on prÃ©sente une nouvelle observation qui nâ€™a pas participÃ© Ã  la
phase dâ€™apprentissage, elle sera projetÃ©e dâ€™abord sur la carte topologique avec la fonction dâ€™af-
fectation associÃ©e Ï† (formule 6), puis on utilisera la fonction dâ€™affectation Ï‡ (voir Â§1.1), pour
sÃ©lectionner le sous-ensemble qui va dÃ©terminer le classifieur SVM associÃ©. Cette mÃ©thode
dâ€™affectation de notre classement permet de comprendre le comportement dâ€™une observation
Ã  travers son rÃ©fÃ©rent wc. Si on note par svmr la fonction de classement du modÃ¨le SVM du
sous-ensemble Pr alors la fonction dâ€™affectation globale de notre systÃ¨me sâ€™Ã©crit comme suite :
yi =
{
svmÏ‡(Ï†(zi)) si Ï‡(Ï†(zi)) /âˆˆ Ip
vote(PÏ‡(Ï†(zi))) sinon
, (7)
oÃ¹ Ip est lâ€™ensemble des indice des sous-ensembles pures.Ï‡(c) = c siP = {P1, ..., Pc, ..., PNcell}
et Ï‡(c) = 1 si P = A (voir Â§1.1).
4 ExpÃ©rimentations
Dans la suite, nous avons illustrÃ© les performances obtenues par notre modÃ¨le en choisissant
quelques exemples de la littÃ©rature qui ont Ã©tÃ© traitÃ©s par les variantes de ce modÃ¨le. En outre,
une base rÃ©elle a Ã©tÃ© utilisÃ©e. Il sâ€™agit dâ€™un extrait dâ€™une base mÃ©dicale regroupant des donnÃ©es
clinico-biologiques portant sur lâ€™Ã©tude de lâ€™obÃ©sitÃ© (HÃ´pital HÃ´tel-Dieu, Paris). Cette base va
nous servir Ã  illustrer le potentiel des diffÃ©rentes visualisations des cartes topologiques mixtes
et Ã  montrer lâ€™intÃ©rÃªt de diviser le problÃ¨me global de classement pour augmenter les perfor-
mances en classement. Pour mesurer la robustesse de notre modÃ¨le, nous avons calculÃ© les taux
de bon classement.
4.1 Base rÃ©elle : prÃ©diction de perte de poids chez les obÃ¨ses
Cet exemple porte sur des donnÃ©es rÃ©elles, issues dâ€™une base de donnÃ©es caractÃ©risant 101
patients, massivement obÃ¨ses (BMI : Body Mass Index> 40kg/m2), recrutÃ©s et suivis dans
le service de Nutrition de lâ€™HÃ´tel Dieu dans le cadre dâ€™une chirurgie de lâ€™obÃ©sitÃ©, ( Crookes
(2006)). Ces donnÃ©es constituent une base difficile en classement. On retrouve un taux de bon
classement faible quelle que soit la mÃ©thode utilisÃ©e (46.8% avec "Random Forest", 50.9%
RNTI - X - 9
Partitionnement par les cartes topologiques mixtes et classement par les SVM
avec lâ€™arbre de dÃ©cision et 55% avec SVM). Cette base permet dâ€™illustrer le potentiel des
diffÃ©rentes visualisations des cartes topologiques mixtes et Ã  montrer lâ€™intÃ©rÃªt de diviser le
problÃ¨me global de classement pour augmenter les performances en classement du SVM. La
base de donnÃ©es comporte des variables cliniques et biologiques, recueillies avant lâ€™interven-
tion chirurgicale. Les patients sont classÃ©s en deux groupes (oui/non) suivant la mÃ©diane de
perte de poids observÃ©e 3 mois et 6 mois aprÃ¨s la chirurgie (gastroplastie par anneau ajustable
ou bypass gastrique). Si la perte de poids est supÃ©rieure Ã  la mÃ©diane, le patient est Ã©tiquetÃ©
"oui". Sinon il est Ã©tiquetÃ© par "non". Chaque patient est caractÃ©risÃ© par 37 variables rÃ©elles (par
exemple, le poids, le BMI, ALAT, ASAT, HDL, CRP...) et 13 variables qualitatives (exemple :
diabÃ¨te oui/non), caractÃ©risant lâ€™obÃ©sitÃ© et ses aspects cliniques et mÃ©taboliques ainsi que ses
complications multiples.
Pour Ã©tudier le comportement de notre modÃ¨le en classement, nous avons procÃ©dÃ© par une
validation croisÃ©e en variant le nombre de sous-ensembles de la partition et par consÃ©quent
le nombre dâ€™observations associÃ©es Ã  chaque apprentissage dâ€™un SVM. Ainsi, nous avons dÃ©-
coupÃ© la base complÃ¨te en trois sous bases de mÃªme taille, B1, B2, B3. On apprend sur deux
bases parmi les trois et on teste les performances en classement sur la troisiÃ¨me en utilisant les
deux Ã©tiquettes de perte de poids (oui/non) Ã  trois mois seulement. Ainsi, en utilisant le modÃ¨le
CT-SVM (Â§3), trois cartes topologiques sont construites de dimension 3Ã—4, ce qui fournit une
partition de 12 sous-ensembles. Pour montrer lâ€™importance de la taille de la partition, nous
avons calculÃ© les performances en classement en variant le nombre de sous-ensembles de 1 Ã 
12. Dans le premier cas, lâ€™application de notre modÃ¨le CT-SVM sur une partition avec un seul
sous-ensemble est Ã©quivalente Ã  entraÃ®ner un SVM binaire classique sur toute la base.
La figure 2 montre les trois variations du taux de bon classement des trois bases de test, en
fonction du nombre de sous- ensembles de la mÃªme partition. Dans le cas oÃ¹ la partition
contiendrait un seul sous-ensemble, un seul SVM est entraÃ®nÃ© sur toute la base. Ainsi dans ce
cas particulier, la fonction dâ€™affectation des cartes topologiques (formule 6), nâ€™influe pas sur la
fonction dâ€™affectation globale de notre modÃ¨le CT-SVM (formule 7). On observe aussi dans la
figure 2, que lâ€™augmentation du nombre de sous-ensembles de la partition permet dâ€™augmenter
les performances en classement sur les trois tests. Par contre, on constate aussi que lorsque la
taille de la partition est trÃ¨s grande les performances diminuent. La partition contenant plu-
sieurs sous-ensembles permet dâ€™apprendre autant de SVMs que de sous-ensembles. Ainsi, la
fonction dâ€™affectation globale de notre modÃ¨le (formule 7) utilise dâ€™abord la fonction dâ€™af-
fectation des cartes topologiques Ï† (formule 6 ) pour choisir le sous-ensemble, ainsi le SVM
associÃ© avec sa fonction dâ€™affectation svm.
Avec le premier test, on obtient au maximum 60.6% avec trois sous-ensembles ; avec le deuxiÃ¨me
test, on obtient 70.6% avec trois sous-ensembles. Finalement, avec le troisiÃ¨me test, on obtient
55.9 avec quatre sous-ensembles. Dans lâ€™entraÃ®nement des SVMs avec notre modÃ¨le CT-SVM,
nous avons utilisÃ© la mÃªme fonction noyau linÃ©aire.
Cette validation croisÃ©e avec une variation du nombre de sous-ensembles montre lâ€™intÃ©rÃªt et
la difficultÃ© de choisir la bonne partition pour une bonne discrimination. Cette partition est
dÃ©terminÃ©e dans notre cas par expÃ©rimentation et visualisation des cartes topologiques. Cette
RNTI - X - 10
Lebbah et al.
1 2 3 4 5 6 7 8 9 10 11 12
35
40
45
50
55
60
65
70
75
Nombre de sousâˆ’ensembles
Ta
ux
 d
e 
bo
nn
e 
cla
ss
em
en
t
CTâˆ’SVM1
CTâˆ’SVM2
CTâˆ’SVM3
FIG. 2 â€“ Taux de bon classement avec CT-SVM en fonction du nombre de sous-ensembles. 1 :
base dâ€™apprentissage : B1 et B2, base de test : B3 ; 2 : base dâ€™apprentissage : B1 et B3, base
de test : B2 ; 3 : base dâ€™apprentissage : B2 et B3, base de test : B1.
validation croisÃ©e montre aussi lâ€™intÃ©rÃªt de subdiviser le problÃ¨me de classement global en
sous-problÃ¨mes de classement pour amÃ©liorer les performances en classement. Afin de compa-
rer la robustesse en classement de notre modÃ¨le CT-SVM aux modÃ¨les classiques, nous allons
prÃ©senter par la suite dans lâ€™exemple 2 trois bases de donnÃ©es frÃ©quemment utilisÃ©es dans les
expÃ©rimentations.
4.1.1 Discussion
Puisque notre modÃ¨le utilise les cartes topologiques, on dispose dâ€™un pouvoir de visualisa-
tion de la partition. Lâ€™application dâ€™abord des cartes topologiques mixtes, va nous permettre
dâ€™analyser la rÃ©partition des observations et par consÃ©quent les sous-ensembles qui ont servi
au classement. Lâ€™apprentissage dâ€™une carte de dimension 3 Ã— 4 cellules effectuÃ© sur la base
entiÃ¨re des patients, avec lâ€™hyper-paramÃ¨treF = 0.01, fournie pour chaque cellule un rÃ©fÃ©rent
wc composÃ© de deux parties : la partie quantitativewrc et la partie qualitativewbc codÃ©e avec le
codage disjonctif binaire.
La figure 3.a prÃ©sente la rÃ©partition des observations. On observe que la partition obtenue
a permis de bien distribuer les observations sur 12 cellules de lâ€™ensemble de la partition
P = {P1, ..., P12}. La figure 3.b prÃ©sente la mÃªme rÃ©partition en distinguant ceux qui ont
perdu ou non du poids Ã  3 mois par rapport Ã  la mÃ©diane de lâ€™ensemble des patients. La figure
3.c prÃ©sente la mÃªme rÃ©partition de perte de poids Ã  6 mois. On constate que les sous-ensembles
sont mÃ©langÃ©s.
A lâ€™aide de cette carte topologique 3 Ã— 4, il est possible dâ€™effectuer un certain nombre dâ€™ana-
lyses de la base Ã©tudiÃ©e. Notre premier objectif est celui de partitionner les donnÃ©es, en prenant
en compte leurs spÃ©cificitÃ©s (donnÃ©es mixtes) pour augmenter les performances en classement.
En plus du classement, il est possible dâ€™utiliser le pouvoir de visualisation des cartes topo-
logiques. Pour visualiser la carte topologique, nous nous sommes limitÃ©s Ã  analyser les effets
dus Ã  quelques variables pour lesquels lâ€™exactitude des propriÃ©tÃ©smÃ©dicales retrouvÃ©es peuvent
RNTI - X - 11
Partitionnement par les cartes topologiques mixtes et classement par les SVM
Ãªtres vÃ©rifiÃ©es. En regardant Ã  la fois les trois figures 3.a, 3.b et 3.c le mÃ©decin a dÃ©tectÃ© globa-
lement trois grands groupes. Pour sâ€™approcher de la partition du mÃ©decin, nous avons appliquÃ©
la CAH avec les rÃ©fÃ©rents de la carte pour avoir 4 sous-ensembles, P = {P1, P2, P3, P4}.
La figure 8 prÃ©sente la partition avec 4 sous-ensembles numÃ©rotÃ©s de 1 Ã  4. Cette rÃ©partition
des donnÃ©es en quatre sous-ensembles et la rÃ©partition du mÃ©decin en trois sous-ensembles
correspondent Ã  la taille de la partition utilisÃ©e dans la phase de la validation croisÃ©e dÃ©crite ci-
dessous. En visualisant Ã  la fois les figures 3,4, 5, 6, 7 et la figure 8, il est possible de demander
au mÃ©decin de dÃ©finir des profils de patients. Ces profils vont servir Ã  dÃ©crire les paramÃ¨tres
(variables) liÃ©s Ã  la perte de poids et fournir des hypothÃ¨ses de travail sur la rÃ©sistance Ã  la perte
de poids fourni par le classifieur.
Trois grands profils de patients sont dÃ©finis selon la cinÃ©tique de perte de poids Ã  3 mois et Ã  6
mois. Le profil 1 est plutÃ´t un bon profil par rapport aux pertes de poids Ã  trois mois (figure 3.b)
et 6 mois (figure 3.c) et correspond aux deux sous-ensembles P1 et P2 de la CAH. Le profil 2
est caractÃ©risÃ© par une perte de poids moyenne Ã  3 mois et 6 mois et correspond approximati-
vement au sous ensemble P4 de la CAH. Enfin, le profil 3 est caractÃ©risÃ© par une perte de poids
mÃ©diocre Ã  3 mois dont lâ€™amplitude diminue Ã  6 mois, ce qui aboutit Ã  dÃ©nommer ce profil
comme un "mauvais" profil en terme de perte de poids. Ce profil correspond au sous-ensemble
P3 de la CAH. Nous dÃ©taillons par la suite les deux profils 1 et 3 par rapport aux diffÃ©rentes
variables clinico-biologiques.
Le profil 1 est caractÃ©risÃ© par un poids, un BMI (Body Mass Index) et une DÃ©pense EnergÃ©-
tique de Repos mesurÃ©e par calorimÃ©trie (DERm) Ã©levÃ©s. Les patients appartenant Ã  ce profil
ont une glycÃ©mie Ã  jeun et insulinÃ©mie Ã©levÃ©es (figure 4) sans Ãªtre diabÃ©tiques (figure 5). Il
sâ€™agit donc de patients insulinorÃ©sistants avant le stade de diabÃ¨te. Le reste du profil mÃ©ta-
bolique est caractÃ©risÃ© par des HDL plutÃ´t bas, des triglycÃ©rides (TG) et enzymes hÃ©patiques
(ASAT, ALAT et GGT) Ã©levÃ©s,(figure 4). Dans les classes qualitatives "HTA" (hypertension)
ou "SAS" (Syndrome dâ€™apnÃ©es du sommeil) ces patients sont classÃ©s "oui" (figures 6 et 7).
Dâ€™un point de vue inflammatoire, la CRP, la fÃ©rritinÃ©mie (FERR), la SAA et lâ€™orosomucoide
(ORO), toutes des protÃ©ines de la phase aiguÃ« de lâ€™inflammation, sont modÃ©rÃ©ment Ã©levÃ©es.
Sur le plan nutritionnel, la TSH est basse, le profil protÃ©ique (albumine, prÃ©albumine, RBP) et
vitaminique est favorable, sans dÃ©ficit. En conclusion pour ce profil, il sâ€™agit de patients avec
un poids trÃ¨s Ã©levÃ©, mais dont le profil mÃ©tabolique (figure 4) nâ€™est pas trop Ã©voluÃ© (sans dia-
bÃ¨te), sans inflammation importante et un bon profil nutritionnel.
Le profil 2 correspond Ã  des patients ayant un BMI Ã©levÃ© et une leptine Ã©levÃ©e (LEP, figure 4).
Ils sont insulinorÃ©sistants, mais pas diabÃ©tiques. Ils ont majoritairement une HTA et un SAS
(figures 6 et 7). Les paramÃ¨tres hÃ©patiques et mÃ©taboliques sont normaux. Lâ€™adiponectinÃ©mie
(ADIPO) est plutÃ´t basse. En revanche, les paramÃ¨tres inflammatoires (SAA et CRP) sont trÃ¨s
Ã©levÃ©s. Sur le plan nutritionnel, la TSH est normale haute et les marqueurs nutritionnels sont
bas (bilan protÃ©ique avec albumine, prÃ©albumine et RBP, fer, vitamines A, E, B1, B12). Le
profil 3 est un profil intermÃ©diaire en terme de paramÃ¨tres clinico-biologiques.
En conclusion les deux profils de patients 1 et 2 sont caractÃ©risÃ©s par des paramÃ¨tres clinico-
biologiques diffÃ©rents, notamment en terme de marqueurs dâ€™inflammation et nutritionnels et
RNTI - X - 12
Lebbah et al.
sont aussi diffÃ©rents en termes de profil de perte de poids Ã  3 mois et 6 mois. Nous pouvons
donc formuler lâ€™hypothÃ¨se que le statut nutritionnel et lâ€™Ã©tat dâ€™inflammation des patients avant
chirurgie pourraient Ãªtre des Ã©lÃ©ments liÃ©s Ã  la rÃ©sistance Ã  la perte de poids.
(a)
11 7 6 10
5 4 7 8
13 5 6 19
(b) (c)
1
2
1
2 1
2
1
2
1
2
1 2
1
2
1
2
1
2 1
2
1
2
1
2
FIG. 3 â€“ Cartes topologiques 3 Ã— 4 (P = {P1, P2, ..., P12}). (a) CardinalitÃ© des sous-
ensembles (b) et (C) RÃ©partition des pertes de poids respectivement Ã  3 mois et Ã  6 mois. .
1 : Pas de perte de poids ; 2 : perte de poids.
4.2 Bases issues de la littÃ©rature
Dans cet exemple, trois bases dâ€™apprentissage comportant un nombre variable dâ€™observations
ont Ã©tÃ© utilisÃ©es, (table 1) : Iris, Glass, Letter (Blake et al (1998)). Ces bases dâ€™apprentissage et
de test sont identiques Ã  ceux pris dans lâ€™article Benabdeslem (2006). Ceci va nous permettre
de comparer nos rÃ©sultats aux mÃ©thodes prÃ©sentÃ©s dans lâ€™article de Benabdeslem (2006).
nom/base #Apprentissage #Test #classe #variables
Iris 100 50 3 4
Glass 142 72 6 9
Letter 10000 5000 26 16
TAB. 1 â€“ Base dâ€™apprentissage et de test.
Puisque toutes les variables sont quantitatives, lâ€™utilisation des cartes topologiques mixtes se
rÃ©duit pour ces bases Ã  lâ€™application de cet algorithme avec lâ€™hyper-paramÃ¨tre F = 0 qui
correspond Ã  la version batch des cartes topologiques classiques de Kohonen. Afin de com-
prendre le dÃ©roulement de notre modÃ¨le CT-SVM, lâ€™application sur lâ€™exemple des Iris sera dÃ©-
taillÃ©e par la suite.
Lâ€™apprentissage dâ€™une carte avec 4Ã— 3, avec la base dâ€™apprentissage dâ€™Iris, permet dâ€™observer
sur la figure 9 une partition P = {P1, P2, ...P12}. AprÃ¨s application du vote majoritaire sur
chaque cellule, on observe que la carte est constituÃ©e de trois sous-ensembles. La partie en haut
de la carte est majoritairement dÃ©diÃ©e Ã  la classe 1, la partie centrale de la carte est dÃ©diÃ©e Ã  la
classe 2, et le reste est majoritairement de la classe 3 mais mÃ©langÃ© Ã  la classe 2.
RNTI - X - 13
Partitionnement par les cartes topologiques mixtes et classement par les SVM
(Taille)
 Taille cm pre
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
(Poids)
Poids kg preop
0.2
0.3
0.4
0.5
0.6
0.7
(BMI)
BMI Preop
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
(ASAT)
ASAT preop
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
(ALAT)
ALAT preop
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(GGT)
GGT preop
0.04
0.06
0.08
0.1
0.12
0.14
0.16
(INS)
Ins0 IRMA preop
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(GLY)
G0 gl preop
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
(HDL)
HDL mmol preop
0.46
0.47
0.48
0.49
0.5
0.51
0.52
0.53
0.54
0.55
(CRP)
CRP mgl preop
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
(SAA)
SAApop
0.05
0.1
0.15
0.2
0.25
(ORO)
oroso gl preop
0.4
0.45
0.5
0.55
(FERR)
ferr preop
0.05
0.1
0.15
0.2
0.25
(LEP)
Leptine preop
0.2
0.25
0.3
0.35
0.4
0.45
(ADIPO)
adiponectine preop
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
(PREALB)
Prealb mgl preop
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
(RBP)
RBP HD mgl preop
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
(A)
A umol preop
0.5
0.52
0.54
0.56
0.58
0.6
0.62
(E)
E umoll preop
0.2
0.25
0.3
0.35
0.4
(B1)
B1 nmoll preop
0.35
0.4
0.45
0.5
0.55
0.6
0.65
(B12)
B12 ngl preop
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0.34
FIG. 4 â€“ Cartes topologiques dÃ©crivant la variation sur les variables Taille,poids,BMI (Body
Mass Index), ALAT, ASAT,GGT, INS(insuline), GLY (glycÃ©mie),HDL, CRP,SAA,ORO (oro-
somucoide),FERR (fÃ©rritinÃ©mie),LEP (leptine),ADIPO (adiponectinÃ©mie),PREALB (prÃ©albu-
mine),RBP, A, E, B1, B12.
RNTI - X - 14
Lebbah et al.
(non)
DIAB preOP 1
0
1
(oui)
DIAB preOP 2
0
1
FIG. 5 â€“ Cartes topologiques reprÃ©sentant les deux modalitÃ©s non et oui de la variable quali-
tative DiabÃ¨te.1 et 0 reprÃ©sente respectivement la prÃ©sence ou lâ€™absence de la modalitÃ©.
(non)
SAS preop 1
0
1
(oui)
SAS preop 2
0
1
FIG. 6 â€“ Cartes topologiques reprÃ©sentant les deux modalitÃ©s non et oui de la variable quali-
tative SAS. 1 et 0 reprÃ©sentent respectivement la prÃ©sence ou lâ€™absence de la modalitÃ©.
(non)
HTA preop 1
0
1
(oui)
HTA preop 2
0
1
FIG. 7 â€“ Cartes topologiques reprÃ©sentant les deux modalitÃ©s non et oui de la variable quali-
tative HTA. 1 et 0 reprÃ©sentent respectivement la prÃ©sence ou lâ€™absence de la modalitÃ©.
FIG. 8 â€“ Carte topologique 3Ã— 4 aprÃ¨s le partitionnement de la CAH. P = {P1, P2, P3, P4}.
RNTI - X - 15
Partitionnement par les cartes topologiques mixtes et classement par les SVM
1
2
2
3
1
2
2
1
2
3
FIG. 9 â€“ Carte topologique 4 Ã— 3 Ã©tiquetÃ©e sur la base Iris. Les cellules non numÃ©rotÃ©es
reprÃ©sentent des cellules vides. Les numÃ©ro 1, 2,3 reprÃ©sente lâ€™Ã©tiquette du vote majoritaire de
chaque cellule. La partition P = {P1, P2, ...P12}.
Cette visualisation des donnÃ©es avec les cartes topologiques nous amÃ¨ne Ã  appliquer la CAH
(classification hiÃ©rarchique) sur lâ€™ensemble des rÃ©fÃ©rentsW = {w1,w2, ...,w12} pour avoir
trois grands sous-ensembles (S = 3). Ceci permet de dÃ©finir la nouvelle partition P =
{P1, P2, P3}. A partir de cette partition, nous avons calculÃ© la table de confusion 2, qui permet
de voir la rÃ©partition des classes dans chacun des sous-ensembles. Il est tout Ã  fait possible pour
cette base de chercher le nombre de sous-ensembles qui permet dâ€™augmenter les performances
en classement, comme nous lâ€™avons fait pour la validation croisÃ©e avec la base dâ€™obÃ©sitÃ© (Â§4.1).
On constate avec cet exemple que la visualisation de la carte topologique fournit une indication
du nombre de sous-ensembles.
Partition/Classe 1 2 3
P1 0 4 0
P2 0 34 33
P3 37 0 0
TAB. 2 â€“ Table de confusion de la partition P = {P1, P2, P3}, P1 est majoritairement de la
classe 3. P3 est majoritairement de la classe 1. Ip = {1, 3}.
La table 2 montre que la partition P contient deux sous-ensembles purs P1 et P3 qui sont
respectivement de la classe 3 et 1 (Ip = {1, 3}). A partir de cette table, on conclu que notre
partition nÃ©cessite lâ€™entraÃ®nement dâ€™un seul SVM binaire pour le sous-ensemble P2 avec les
deux classes Ã©tiquetÃ©es 2 et 3. Notre partitionnement par les cartes topologiques nous a permis
dâ€™avoir une projection des donnÃ©es en deux dimensions et de simplifier le SVM en passant
du SVM multi-classe, sur toute la base dâ€™apprentissage, Ã  un seul SVM binaire avec un sous-
ensemble de taille rÃ©duite (#P2 = 52 observations). Ceci permet un gain de temps et une
simplification du SVM. Pour ce sous-ensemble nous avons entraÃ®nÃ© un SVM avec une fonc-
RNTI - X - 16
Lebbah et al.
tion noyau de type RBF (Radial Basic Function).
Le mÃªme phÃ©nomÃ¨ne, de rÃ©duction du nombre de classe, a Ã©tÃ© observÃ© sur la base Glass.
Lâ€™apprentissage a Ã©tÃ© rÃ©alisÃ© avec une carte topologique de dimension 9 Ã— 7, puis le regrou-
pement avec la CAH pour avoir une partition P constituÃ©e de deux sous-ensembles P1 et P2.
Le premier sous-ensemble nÃ©cessite lâ€™apprentissage dâ€™un SVM multi-classe avec 5 classes et
le deuxiÃ¨me un SVM multi-classe avec 6 classes. Dans les deux cas, la taille de la base dâ€™ap-
prentissage est rÃ©duite. Pour lâ€™apprentissage des deux SVM, nous avons utilisÃ© une fonction
noyau de type RBF. Pour nos trois exemples, nous nous sommes basÃ©s pour trouver les hyper-
paramÃ¨tres du SVM sur les travaux de Hsu et al (2001).
Afin de mesurer la robustesse de notre systÃ¨me, lâ€™apprentissage de notre modÃ¨le CT-SVM est
rÃ©alisÃ© sur les bases dâ€™apprentissage prÃ©sentÃ©es dans la table 1. Lâ€™affectation des observations
de la base de test est rÃ©alisÃ©e Ã  lâ€™aide de la fonction dâ€™affectation de notre modÃ¨le CT-SVM,
prÃ©sentÃ©e par la formule (7).
Base/modÃ¨le one against one one against all MLP DHSVM CT-SVM
Iris 97.3 96.7 92.5 97.6 97.6
Glass 71.5 71.9 70.3 76.8 81.9
Letter 97.9 97.9 85.2 98.0 95.0
TAB. 3 â€“ Comparaison des performances en classement avec les algorithmes classiques. SVM
one against one, SVM one against all, MLP : Multi-Layer Perceptron, DHSVM :Descendant
Hierarchical Support Vector Machine.
La table 3 indique les performances atteintes avec notre modÃ¨le CT-SVM sur les bases de test
des trois exemples en rappelant ceux du SVM classiques et lâ€™algorithmeDHSVM (Descendant
Hierarchical Support Vector Machine). Dans la premiÃ¨re base "Iris", le taux de bon classement
est Ã©quivalent Ã  celui du DHSVM et il est de lâ€™ordre de 97.6%. Avec la deuxiÃ¨me base "Glass"
une nette amÃ©lioration du taux de bon classement est constatÃ©e. On passe dâ€™un taux de 71.5%
avec le SVM "one againt one" Ã  81.9% avec notre modÃ¨le CT-SVM. Avec la troisiÃ¨me base,
on constate que notre modÃ¨le CT-SVM arrive Ã  un taux de 95.0% qui mieux que le MLP qui
est de %85.2, mais moins bon que le SVM classique et le DHSVM qui a le meilleur taux de
98.0%.
5 Conclusion
Dans ce papier, nous avons prÃ©sentÃ© un modÃ¨le de classement hybride, associant une mÃ©thode
de partitionnement et une mÃ©thode de classement qui sont respectivement, les cartes topolo-
giques et les SVMs. Ce modÃ¨le utilise lâ€™organisation des donnÃ©es fournie par les cartes topolo-
giques mixtes pour subdiviser lâ€™espace des donnÃ©es afin dâ€™apprendre un SVM spÃ©cifique pour
chaque sous-espace des donnÃ©es. Notre modÃ¨le CT-SVM utilise la partition rÃ©sultat des cartes
topologiques, pour associer un SVM Ã  chaque sous-ensemble de la partition avec des hyper-
paramÃ¨tres diffÃ©rents si cela est nÃ©cessaire. Les expÃ©riences effectuÃ©es montrent la robustesse
RNTI - X - 17
Partitionnement par les cartes topologiques mixtes et classement par les SVM
de celui-ci Ã  traiter des bases classiques avec uniquement des donnÃ©es rÃ©elles ou des donnÃ©es
mixtes. Dâ€™autres part, dans le cadre dâ€™une application mÃ©dicale rÃ©elle, nous avons vu que la
quantitÃ© dâ€™information fournie par ce modÃ¨le CT-SVM Ã  travers les cartes topologiques mixtes
est trÃ¨s importante et le pouvoir de classement avec les SVMs est trÃ¨s performant. Nous avons
aussi constatÃ©, quâ€™il existe une base letter pour laquelle la mÃ©thode de classement DHSVM et
SVM, sont meilleurs, Wolpert et al (1997); Benabdeslem (2006). Ceci nous conduit Ã  rÃ©flÃ©chir
sur un indice permettant dâ€™estimer la capacitÃ© de notre approche Ã  traiter les problÃ¨mes de clas-
sement. Lâ€™autre amÃ©lioration qui peut Ãªtre apportÃ© est dâ€™estimer le nombre de sous-ensembles
et par consÃ©quent la partition idÃ©ale pour une bonne discrimination des donnÃ©es, Vesanto et al
(2000).
RÃ©fÃ©rences
Benabdeslem, K. (2006). Descendant hierarchical support vector machine for multi-class pro-
blems. International joint conference on neural network (IJCNN 2006) Vancouver .
Ben-Hur, A., D. Horn, H.T. Siegelmann, and V. Vapnik (2001). Support vector clustering,
Journal of Machine Learning Research, vol. 2, pp. 125.
Bishop, C. M., M. Svensen and C.K.I. Williams (1998). GTM : The Generative Topographic
Mapping. Neural Computation, 10(1), 215-234.
Blake C.L and C.J. Merz (1998). "UCI repository of machine learning databases". Technical
report. University of California, Department of information and Computer science, Irvine,
CA. available at : ftp ://ftp.ics.uci.edu/pub/machine-learning-databases.
Clancey, J.W. (1985). Heuristic Classification. Artificial Intelligence, 27 :p.289-350.
Cawley, G. C, (2000). MATLAB Support Vector Machine Toolbox (v0.55Î²) http ://theo-
val.sys.uea.ac.uk/gcc/svm/toolbox, University of East Anglia, School of Information Sys-
tems, Norwich, Norfolk, U.K. NR4 7TJ.
Crookes, P.F. (2006). Surgical treatment of morbid obesity. Vol. 57 : 243-264. annu-
rev.med.56.062904.144928.
Egmont-Petersen, M., W. R. M. Dassen, and J. H. C. Reiber (1999). Sequential selection of
discrete features for neural networks-A Bayesian approach to building a cascade. Pattern
Recognition Letters, 20(11-13) :1439-1448.
Gamma, J. and P. Brazdil (2000) Cascade generalization. Machine Learning, 41(3) :315-343.
Golub, T.R., D. K. Slonim, P. Tamayo, C. H. M. Gaasenbeek, J. P. Mesirov, H. Coller, M. L.
Loh, J. R. Downing, M. A. Caligiuri, C. D. Bloomfield, and E. S. Lander (1999). Molecular
classification of cancer : class discovery and class prediction by gene expression monitoring.
In Science, volume 286, p. 531-537.
Hsu, C-W. and C-J. Lin (2001). A comparison of methods for multi-class support vector ma-
chines. Technical report, Department of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan, 19.
Hyun-Chul. K, P. Shaoning, J. Hong-Mo, K. Daijin, Y.B. Sung (2003). Constructing support
vector machine ensemble. Pattern Recognition vol. 36, no. 12, pp. 2757-2767.
Kohonen, T. (1995). Self-Organizing Map. Springer, third edition Berlin.
RNTI - X - 18
Lebbah et al.
Kuncheva, L. I. (2004) Combining Pattern Classifiers, Methods and Algorithms. A Wiley-
Interscience publication. ISBN 0-471-21078-1.
Kuncheva, L. I. (2002). Switching between selection and fusion in combining classifiers : An
experiment. IEEE Transactions on Systems, Man, and Cybernetics, 32(2) :146-156.
Lebbah, M., Thiria. S, Badran. ESANN, Topological Map for Binary Data, ESANN 2000,
Bruges, April 26-27-28, 2000, Proceedings.
Lebbah, M., A. Chazottes, S. Thiria and F. Badran. ESANN (2005) Mixed Topological Map,
ESANN 2005, Bruges, April 26-27-28, Proceedings.
Lebrun, G., C. Charrier, O. Lezoray, H. Cardot (2004). RÃ©duction du temps dâ€™apprentissage
des SVM par Quantification Vectorielle . CORESA (COmpression et REprÃ©esentation des
signaux Audiovisuels), pp 223-226.
Liu, R. and B. Yuan.(2001). Multiple classifier combination by clustering and selection. Infor-
mation Fusion, 2 :163-168.
Peng, H., F. Long, C. Ding (2005). Feature Selection Based on Mutual Information : Criteria
of Max-Dependency, Max-Relevance, and Min-Redundancy, IEEE Transactions on Pattern
Analysis and Machine Intelligence, v.27 n.8, p.1226-1238.
Platt, J., N. Cristianini, J. Shawe-Taylor (2000) "LargeMargin DAGs for Multiclass Classifica-
tion", in Advances in Neural Information Processing Systems 12, pp. 547-553, MIT Press.
Platt, J. C. (1999). "Fast training of support vector machines using sequential minimal optimi-
zation", in Advances in Kernel Methods - Support Vector Learning, (Eds) B. Scholkopf, C.
Burges, and A. J. Smola, MIT Press, Cambridge, Massachusetts, chapter 12, pp 185-208.
Philip, M., L. Vinsensius B. Vega. (2003). Boosting and Microarray Data, Machine Learning,
v.52 n.1-2, p.31-44.
Ripley, B.D. Pattern Recognition and Neural networks. Cambridge University Press, Cam-
bridge, 1996.
Rybnik, M., A. Chebira, K. Madani (2003). Auto-adaptive Neural Network Tree Structure
Based on Complexity Estimator. IWANN (1) : 558-565.
Shaoning, P., D. Kim, S.Y. Bang (2005). Face Membership Authentication Using SVM Clas-
sification Tree Generated by Membershipbased LLE Data Partition, IEEE Trans. on Neural
Network, 16(2) 436-446.
Sungmoon, C., Sang Hoon Oh Soo-Young Lee (2004). Support Vector Machines with Binary
Tree Architecture. Neural Information Processing. Letters and Reviews Vol. 2, No. 3.
Tamayo, P., D. Slonim, J. Mesirov, Q. Zhu, S. Kitareewan, E. Dmitrovsky, E. S. Lander, T. R.
Golub (1999). Interpreting patterns of gene expression with self-organizing maps : Methods
and application to hematopoietic differentiation, Proc. National Academy Science of USA
96, p. 2907-2912.
Vapnik, V.N. (1995). "The Nature of Statistical Learning Theory", Springer-Verlag, New York,
ISBN 0-387-94559-8.
Vesanto, J., J. Himberg, E. Alhoniemi and J. Parhankangas (2000). "SOM Toolbox-
Team". Helsinki University of Technology. P.O.Box 5400, FIN-02015 HUT, FINLAND.
http ://www.cis.hut.fi/projects/somtoolbox/.
RNTI - X - 19
Partitionnement par les cartes topologiques mixtes et classement par les SVM
Vesanto, J., Alhoniemi, E.(2000). "Clustering of the Self-OrganizingMap", IEEE Transactions
on Neural Networks.
Weston, J., J. Cai and W.N. Grundy (2001). Gene functional classification from heterogeneous
data Paul Pavlidis. Proceedings of RECOMB.
Wolpert, D.H., and W.G. Macready (1997). No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation.
Wu,W., X. Liu,M. Xu, J. Peng, R. Setiono (2004)AHybrid SOM-SVMMethod for Analyzing
Zebra Fish Gene Expression. 17th ICPRâ€™04 - Volume 2 pp. 323-326.
Xing, E.P., M. I. Jordan, R.M. Karp (2001). Feature selection for high-dimensional genomic
microarray data, Proceedings of the Eighteenth International Conference on Machine Lear-
ning, p.601-608, June 28-July.
Yacoub, M., F. Badran and S. Thiria (2001). A Topological Hierarchical Clustering : Applica-
tion to Ocean Color Classification ICANN proceedings.
Summary
This paper introduces a classification model combining mixed topological map and support
vector machines. The non supervised model is dedicated for clustering and visualizing mixed
data. The supervised model is dedicated to classification task. In the present paper, we propose
a combination of two models performing a data visualization and classification. The task of
our model is to train topological map in order to cluster data set on organized subset. For each
subset, we propose to train a SVM model. The global classification problem is devided into
classification sub problem corresponding to the number of subset. The model is validated on
forth data bases. The first one is related to the obesity problem, which is provied by Nutri-
tion team located in hospital HÃ´tel-Dieu in Paris. The others are taken from public data set
repository.
RNTI - X - 20
