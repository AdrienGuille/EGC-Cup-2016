Le logiciel mixmod d'analyse de m√©lange pour
la classication et l'analyse discriminante
Christophe Biernacki1, Gilles Celeux2, Anwuli Echenim23, G√©rard Govaert4, Florent
Langrognet3
1 UMR 8524, CNRS & Universit√© de Lille 1, 59655 Villeneuve d'Ascq, France
2 NRIA Futurs, 91405 Orsay, France
3 UMR 6623, CNRS & Universit√© de Franche-Comt√©, 25030 Besan√ßon, France
4 UMR 6599, CNRS & UTC, 60205 Compi√®gne, France
E-mails : christophe.biernacki@math.univ-lille1.fr, gilles.celeux@inria.fr,
anwuli.echenim@univ-fcomte.fr, gerard.govaert@utc.fr, orent.langrognet@univ-fcomte.fr
R√©sum√© Le logiciel mixmod est d√©volu √† l'analyse de m√©langes de lois de probabi-
lit√© sur des donn√©es multidimensionnelles dans un but d'estimation de densit√©, de classi-
cation ou d'analyse discriminante. Il propose un choix important d'algorithmes pour
estimer les param√®tres d'un m√©lange (EM, Classication EM, Stochastic EM). Il est
possible de combiner ces algorithmes de multiples fa√ßons pour obtenir un maximum lo-
cal pertinent de la vraisemblance ou de la vraisemblance compl√©t√©e d'un mod√®le. Pour
des variables quantitatives, mixmod utilise des m√©langes de lois normales multidimen-
sionnelles. Il propose ainsi quatorze mod√®les gaussiens di√©rents selon des hypoth√®ses
faites sur les √©l√©ments spectraux des matrices de variance des composants. Pour des va-
riables qualitatives, mixmod utilise des m√©langes de lois multinomiales multidimension-
nelles sous une hypoth√®se d'ind√©pendance conditionnelle des variables sachant le com-
posant du m√©lange. Gr√¢ce √† une reparam√©trisation des probabilit√©s multinomiales, il
propose cinq mod√©lisations di√©rentes. Par ailleurs, di√©rents crit√®res d'information sont
propos√©s pour choisir un mod√®le parcimonieux et permettent notamment de choisir un
nombre de composants pertinents. L'emploi de l'un ou l'autre de ces crit√®res d√©pend
de l'objectif poursuivi (estimation de densit√©, classication supervis√©e ou non). √âcrit en
C++, mixmod poss√®de des interfaces avec Scilab et Matlab. Le logiciel, sa docu-
mentation statistique et son guide d'utilisation sont disponibles √† l'adresse suivante :
http ://www-math.univ-fcomte.fr/mixmod/index.php
Mots-cl√©s : mod√®les gaussiens, mod√®les multinomiaux, algorithmes de type EM, s√©lec-
tion de mod√®les.
1 Introduction
Par leur exibilit√©, les m√©langes nis de distributions de probabilit√© sont devenus un
outil populaire pour mod√©liser une grande vari√©t√© de ph√©nom√®nes al√©atoires. En parti-
culier, ils constituent un outil de choix pour l'estimation de densit√©, la classication et
l'analyse discriminante. Les mod√®les de m√©lange sont alors utilis√©s dans un nombre crois-
sant de disciplines comme l'astronomie, la biologie, la g√©n√©tique, l'√©conomie, les sciences
de l'ing√©nieur et le marketing. On a vu ainsi se d√©velopper plusieurs logiciels d√©di√©s √† ce
c Revue MODULAD, 2006 -25- Num√©ro 35
mod√®le. mixmod est l'un de ces logiciels et il a √©t√© essentiellement con√ßu pour traiter des
probl√®mes de classication supervis√©e ou non. Cet article vise √† d√©crire les caract√©ristiques
statistiques de ce logiciel d'analyse de donn√©es multidimensionnelles.
mixmod est un logiciel libre, disponible sous licence GPL pour les syst√®mes d'exploi-
tation Linux, Unix et Windows. Le noyau du logiciel, √©crit en C++, est disponible en
tant que biblioth√®que et ex√©cutable mais il est √©galement accessible via des interfaces avec
les logiciels Matlab et Scilab. Il a √©t√© d√©velopp√© conjointement par l'Inria, le labora-
toire de math√©matiques de l'universit√© de Besan√ßon, le laboratoire Heudiasyc de l'UTC
Compi√®gne et le laboratoire de math√©matiques de l'universit√© Lille 1.
Dans sa version actuelle, mixmod propose des mod√®les de m√©langes gaussiens multi-
dimensionnels ainsi que des m√©langes multinomiaux multidimensionnels (mod√®le dit des
classes latentes). Les principales caract√©ristiques de cette version sont les suivantes :
 trois niveaux d'utilisation du d√©butant √† l'expert ;
 quatorze mod√®les de m√©langes gaussiens tir√©s de param√©trisations di√©rentes des
matrices de variance des composants ;
 cinq mod√®les de m√©langes multinomiaux tir√©s de param√©trisations di√©rentes des
probabilit√©s multinomiales ;
 l'estimation des param√®tres du m√©lange par l'algorithme EM ou des variantes de cet
algorithme, permettant di√©rentes strat√©gies d'initialisation ;
 la mise √† disposition de nombreux crit√®res pour s√©lectionner un mod√®le able en
fonction de l'objectif d'estimation de densit√©, de classication ou de classement ;
 de nombreux graphiques 1D, 2D et 3D dont des densit√©s, des isodensit√©s, des des-
criptions d'un classieur dans un espace factoriel.
Cet article ne vise pas √† remplacer le guide d'utilisation ni la documentation statistique
de mixmod que l'on peut trouver sur le site web. Il a pour but de fournir une vue
synth√©tique des caract√©ristiques de mixmod en associant une pr√©sentation statistique
courte d'exemples d'utilisation.
Le premier jeu de donn√©es introduit √† cette n concerne la classication non supervis√©e.
La gure 1 (a) montre la logpopulation versus la logdensit√© (par habitants/km2) pour 312
villes de trois d√©partements fran√ßais (Biernacki et al., 2000), la Seine-Saint-Denis et les
Hauts de Seine, dans l'agglom√©ration parisienne et le d√©partement rural de Haute-Corse.
(a) (b) (c)
Fig. 1  Donn√©es s√©lectionn√©es : (a) Les d√©partements fran√ßais pour la classication, (b)
les oiseaux borealis pour l'analyse discriminante dans la cadre continu et (c) les oiseaux
puns pour l'analyse discriminante dans le cadre qualitatif.
c Revue MODULAD, 2006 -26- Num√©ro 35
Le deuxi√®me jeu de donn√©es a pour but d'illustrer les caract√©ristiques de mixmod dans
un contexte d'analyse discriminante. Il concerne 204 oiseaux de la sous-esp√®ce borealis
de la famille Procellaridae (P√©trel) dont cinq mesures morphologiques sont disponibles
(Biernacki et al., 2002) : culmen (longueur du cou), tarsus, taille des ailes et de la queue
et largeur du cou. La gure 1 (b) repr√©sente les m√¢les (55%) et les femelles (45%) dans le
premier espace 3D de l'analyse en composantes principales.
L'objectif du troisi√®me jeu de donn√©es est aussi d'illustrer l'analyse discriminante mais
cette fois sur des donn√©es qualitatives. Il s'agit de 132 puns issus de trois sous-esp√®ces
di√©rentes (dichrous, lherminieri et subalaris) sur lesquels six mesures morphologiques
ont √©t√© relev√©es : sexe, sourcil, collier, z√©brures, sous-caudales et liseret. La gure 1(c)
repr√©sente les individus de chaque sous-esp√®ce dans le premier plan de l'analyse des cor-
respondances multiples.
2 Caract√©ristiques techniques de mixmod
Le d√©veloppement du logiciel a commenc√© en 2001 et la derni√®re version de mixmod
(mixmod 2.0) est compos√© de 50 classes C++ (25000 lignes), et 20000 lignes de code
Scilab et Matlab. Sur le site web d√©di√© √† mixmod
(http ://www-math.univ-fcomte.fr/mixmod/index.php), on acc√®de √† l'ensemble
des ressources : t√©l√©chargement, documentations (userguide, statistique et logicielle), fo-
rum de discussion, nouvelles, ...
2.1 Modes op√©ratoires de mixmod
Le logiciel mixmod peut √™tre utilis√© de trois fa√ßons.
 mixmod par l'interm√©diaire d'une interface graphique.
La fonction mixmodGraph, disponible dans Scilab et dansMatlab, est le moyen le
plus simple d'acc√©der aux principales fonctionnalit√©s de mixmod (m√™me si certaines
d'entre elles ne sont pas disponibles via cette interface).
 mixmod en tant que fonction Scilab ou Matlab.
Gr√¢ce √† une syntaxe simple (deux entr√©es seulement sont obligatoires), cette fonc-
tion permet de r√©soudre l'ensemble des probl√©matiques de classication supervis√©e
ou non. Associ√©e √† d'autres fonctions √©crites pour Scilab et Matlab, comme la
fonction mixmodView permettant de visualiser les r√©sultats, elle repr√©sente un outil
de choix alliant performance et convivialit√©.
 mixmod en tant que biblioth√®que de calcul ou ex√©cutable.
Les fonctionnalit√©s de mixmod sont disponibles par l'interm√©diaire d'une biblio-
th√®que C++ que l'utilisateur peut int√©grer √† tout programme. De plus, celui-ci peut
√©galement lancer mixmod en ligne de commande (sous Linux, Unix ou Windows)
apr√®s avoir d√©ni un chier d'entr√©e. Les chiers r√©sultats pourront √™tre alors sau-
vegard√©s ou r√©utilis√©s par une autre application.
Dans cet article, les exemples sont pr√©sent√©s avec la fonction mixmod dans l'environnement
Scilab.
c Revue MODULAD, 2006 -27- Num√©ro 35
2.2 Repr√©sentation des donn√©es dans mixmod
mixmod accepte trois structures de donn√©es compl√©mentaires selon les donn√©es dis-
ponibles :
 repr√©sentation standard : chaque individu est repr√©sent√© par une ligne et chaque
variable par une colonne ;
 partition : chaque ligne est le vecteur indicateur d'appartenance d'un individu aux
di√©rentes classes. La coordonn√©e j est 1 si l'individu appartient √† la classe j, et 0
sinon. Une ligne de "0" indique un individu de classe inconnue ;
 poids : chaque ligne donne le poids d'un individu.
2.3 Performances de mixmod (vitesse d'ex√©cution)
Au-del√† des fonctionnalit√©s, l'objectif de mixmod est d'√™tre un outil de choix pour
les gros jeux de donn√©es. √Ä ce titre, un eort particulier et continu est engag√© pour
atteindre les meilleures performances possibles. Le choix du langage de programmation
du noyau mixmod (C++) a √©videmment √©t√© fait en ce sens. Notons √† titre d'illustration
que mixmod 2.0 est approximativement 10 fois plus rapide que la premi√®re version.
3 Les mod√®les statistiques
Nous d√©crivons maintenant les mod√®les de m√©lange disponibles dans mixmod. Nous
pr√©sentons d'abord les mod√®les de m√©lange gaussien pour le traitement de donn√©es quan-
titatives, puis le mod√®le des classes latentes pour le traitement des donn√©es qualitatives.
3.1 Quatorze mod√®les de m√©lange gaussien
3.1.1 Param√©trisation spectrale des matrices de variance
Dans mixmod, les observations continues 2 Rd, issues de d variables quantitatives
sont suppos√©es provenir d'un m√©lange de densit√© :
f(x; ) =
KX
k=1
pk'(x;k;k) (1)
o√π pk  0 pour k = 1; : : : ; K avec
PK
k=1 pk = 1 et repr√©sentent les proportions du m√©lange,
'(x;;) est la densit√© d'une distribution gaussienne multivari√©e de moyenne  et de
matrice de variance , et  = (p1; : : : ; pK;1; : : : ;K;1; : : : ;K) repr√©sente le vecteur
des param√®tres √† estimer.
Pour ce mod√®le, la densit√© du ke composant est la densit√© gaussienne
'(x;k;k) = (2)
 d=2jkj
 1=2 exp

 
1
2
(x  k)
0 1k (x  k)

: (2)
Cette densit√© gaussienne mod√©lise une classe ellipso√Ødale de centre k et dont les caract√©-
ristiques g√©om√©triques peuvent √™tre associ√©es √† la d√©composition spectrale de la matrice
de variance k.
c Revue MODULAD, 2006 -28- Num√©ro 35
Suivant Baneld and Raftery (1993) et Celeux and Govaert (1995), chaque matrice de
variance des composants du m√©lange peut s'√©crire :
k = kDkAkD
0
k (3)
avec k = jkj1=d; Dk √©tant la matrice des vecteurs propres de k et Ak √©tant une ma-
trice diagonale, telle que jAkj = 1, dont la diagonale est constitu√©e des valeurs propres
normalis√©es de k rang√©es en ordre d√©croissant. Le param√®tre k caract√©rise le volume
de la ke classe, Dk son orientation et Ak sa forme. En permettant √† ces param√®tres de
varier ou non entre les classes, on obtient des mod√®les plus ou moins parcimonieux, faciles
√† interpr√©ter et utiles pour appr√©hender des situations de classication pertinentes que
l'on soit dans un cadre supervis√© ou non. Ainsi supposer que les param√®tres k; Dk et Ak
d√©pendent ou non des classes conduit √† huit mod√®les g√©n√©raux. Par exemple, des volumes
di√©rents, des formes et des orientations √©gales s'obtiennent en supposant que Ak = A
(A inconnu) et Dk = D (D inconnu) pour chaque composant du m√©lange. Ce mod√®le est
not√© [kDAD0]. Avec cette convention, [DkAkD0k] indique un mod√®le dont les compo-
sants ont des volumes √©gaux, des formes et des orientations di√©rentes. D'autres familles
d'int√©r√™t se restreignent √† des matrices de variance k diagonales. Par la param√©trisation
(3), cela signie que les matrices d'orientation Dk sont des matrices de permutation. Dans
cet article, ces matrices de variance diagonales sont par convention not√©es k = kBk, Bk
√©tant une matrice diagonale avec jBkj = 1. Cette param√©trisation particuli√®re conduit √†
quatre mod√®les : [B], [kB], [Bk] et [kBk]. La derni√®re famille des mod√®les suppose des
formes sph√©riques, c'est-√†-dire telle que Ak = I, I d√©signant la matrice identit√©. Dans ce
cas, deux mod√®les parcimonieux peuvent √™tre consid√©r√©s : [I] et [kI]. Au total, quatorze
mod√®les sont ainsi obtenus.
Remarquons que, dans la suite, les mod√®les [DAD0] et [DkAkD0k] pourront aussi
√™tre d√©sign√©s sous une forme plus compacte [C] et [Ck] respectivement. De m√™me, les
mod√®les [kC] et [kCk] d√©signeront les mod√®les [kDAD0] et [kDkAkD0k] respectivement.
3.1.2 Contraintes sur les proportions
En dehors des caract√©ristiques g√©om√©triques, les proportions du m√©lange pk consti-
tuent des param√®tres importants. Deux hypoth√®ses les concernant sont consid√©r√©es dans
mixmod : soit, elles sont suppos√©es √©gales, soit elles d√©pendent des composants.
Combinant ces hypoth√®ses avec celles qui ont produit quatorze mod√®les, on obtient
vingt-huit mod√®les not√©s [pI], [pkI], [pkDAD0], etc., avec la m√™me convention. Tous ces
mod√®les, sch√©matis√©s dans le tableau 1, sont disponibles dans mixmod dans les contextes
non supervis√©s et supervis√©s.
3.1.3 Liens avec des crit√®res classiques de classication
Les di√©rents mod√®les pr√©sent√©s n'ont pas seulement une interpr√©tation g√©om√©trique
simple. Ils jettent aussi une lumi√®re nouvelle sur des crit√®res classiques de classication.
Par exemple, le crit√®re de l'algorithme des centres mobiles de Ward (1963) peut facilement
se d√©duire du mod√®le de m√©lange gaussien le plus simple [pI].
Le mod√®le [pDAD0] correspond au crit√®re sugg√©r√© par Friedman and Rubin (1967),
et les mod√®les [pkDAD0], [pkDAkD0] et [pkDkAkD0k] correspondent √† d'autres crit√®res
bien connus de classication (voir par exemple Scott and Symons (1971); Diday and Go-
vaert (1974); Maronna and Jacovkis (1974); Schroeder (1976)). En analyse discriminante,
c Revue MODULAD, 2006 -29- Num√©ro 35
Mod√®le Famille Prop. Volume Forme Orient.
[pDAD0] G√©n√©ral √âgal √âgal √âgal √âgal
[pkDAD
0] Variable √âgal √âgal
[pDAkD
0] √âgal Variable √âgal
[pkDAkD
0] Variable Variable √âgal
[pDkAD
0
k
] √âgal √âgal Variable
[pkDkAD
0
k
] Variable √âgal Variable
[pDkAkD
0
k
] √âgal Variable Variable
[pkDkAkD
0
k
] Variable Variable Variable
[pB] Diagonal √âgal √âgal √âgal Axes
[pkB] Variable √âgal Axes
[pBk] √âgal Variable Axes
[pkBk] Variable Variable Axes
[pI] Sph√©rique √âgal √âgal √âgal NA
[pkI] Variable √âgal NA
[pkDAD
0] G√©n√©ral Variable √âgal √âgal √âgal
[pkkDAD
0] Variable √âgal √âgal
[pkDAkD
0] √âgal Variable √âgal
[pkkDAkD
0] Variable Variable √âgal
[pkDkAD
0
k
] √âgal √âgal Variable
[pkkDkAD
0
k
] Variable √âgal Variable
[pkDkAkD
0
k
] √âgal Variable Variable
[pkkDkAkD
0
k
] Variable Variable Variable
[pkB] Diagonal Variable √âgal √âgal Axes
[pkkB] Variable √âgal Axes
[pkBk] √âgal Variable Axes
[pkkBk ] Variable Variable Axes
[pkI] Sph√©rique Variable √âgal √âgal NA
[pkkI] Variable √âgal NA
Tab. 1  Caract√©ristiques et identiants des vingt-huit mod√®les de m√©lange gaussien
disponibles dans mixmod.
les mod√®les [pC] et [pkCk] d√©nissent respectivement l'analyse discriminante lin√©aire et
l'analyse discriminante quadratique (voir par exemple McLachlan, 1992).
3.2 Cinq mod√®les de m√©langes multinomiaux
De la m√™me fa√ßon que le mod√®le gaussien est souvent retenu pour mod√©liser chaque
composant du m√©lange lorsque les variables sont continues, le choix du mod√®le loglin√©aire
(Agresti, 1990; Bock, 1986) s'impose assez naturellement lorsque les variables sont qua-
litatives. Le mod√®le loglin√©aire complet ou satur√© pour lequel chaque classe suit une dis-
tribution multinomiale √† 2d valeurs n'a pas d'int√©r√™t dans le cadre d'un m√©lange puisqu'il
conduit √† des mod√®les non identiables. Il faut se restreindre √† des mod√®les loglin√©aires
susamment contraints pour les rendre identiables. L'exemple le plus simple et le plus
r√©pandu est le mod√®le d'ind√©pendance qui suppose que conditionnellement √† l'apparte-
nance √† une classe, les variables qualitatives sont ind√©pendantes. Le mod√®le de m√©lange
associ√© √† cette distribution est appel√© mod√®le des classes latentes (Lazareld and Henry,
1968; Goodman, 1974). C'est le mod√®le retenu dans mixmod pour traiter les donn√©es
qualitatives.
On supposera dans ce paragraphe que toutes les variables sont des variables qualita-
tives √† mj modalit√©s et que les donn√©es sont constitu√©es d'un √©chantillon (x1; : : : ;xn) o√π
xi = (x
jh
i ; j = 1; : : : ; p; h = 1; : : : ; mj) avec
xjhi = 1 si i prend la modalit√© h pour la variable j
xjhi = 0 sinon.
c Revue MODULAD, 2006 -30- Num√©ro 35
3.2.1 Le mod√®le des classes latentes
Si on note jhk la probabilit√© que la variable x
j prenne la modalit√© h lorsque l'individu
est dans la classe k, la probabilit√© du m√©lange s'√©crit alors :
f(xi; ) =
X
k
pkf(xi;k) =
X
k
pk
Y
j;h
(jhk )
xjhi
o√π le param√®tre  est d√©ni par les proportions p = (p1; : : : ; pg) et par les param√®tres
k = (
jh
k ; j = 1; : : : ; p; h = 1; : : : ; mj), v√©riant 
jh
k 2]0; 1[ et
P
h 
jh
k = 1 8k; j.
3.2.2 Mod√®les parcimonieux
Le nombre de param√®tres n√©cessaires au mod√®le des classes latentes que l'on vient
d'√©tudier, √©gal √† (K   1) + K 
P
j(mj   1), est g√©n√©ralement beaucoup plus petit que
le nombre de param√®tres n√©cessaires au mod√®le loglin√©aire complet, √©gal √†
Q
jmj. Par
exemple, pour un nombre de classes K √©gal √† 5, un nombre de variables qualitatives d
√©gal √† 10 et si le nombre de modalit√©s mj est √©gal √† 4 pour toutes les variables, on obtient
respectivement 154 et 106 param√®tres. Ce nombre de param√®tres peut toutefois se r√©v√©ler
encore beaucoup trop grand et des mod√®les plus parcimonieux sont alors n√©cessaires.
Pour ceci, on contraint tous les vecteurs (j1k ; : : : ; 
jmj
k ) √† prendre la forme
(
"jk
mj   1
;
"jk
mj   1
; ::;
"jk
mj   1
; 1  "jk;
"jk
mj   1
; :::;
"jk
m  1
)
avec "jk <
mj 1
mj
(Celeux and Govaert, 1991). Les vecteurs des probabilit√©s sont alors
simplement caract√©ris√©s par une modalit√© majoritaire (le mode) √† laquelle est associ√© le
terme 1  "jk et un terme de dispersion "
j
k. Le mod√®le alors obtenu est not√© ["
j
k].
Comme pour le mod√®le de m√©lange gaussien, il est possible d'imposer des contraintes
suppl√©mentaires ; on obtient alors les mod√®les suivants :
 mod√®le ["k] : la dispersion ne d√©pend pas de la variable ;
 mod√®le ["j] : la dispersion ne d√©pend pas de la classe ;
 mod√®le ["] : la dispersion ne d√©pend ni de la variable, ni de la classe.
Enn, par analogie, le mod√®le des classes latentes initial, qui n'impose aucune contrainte
sur les probabilit√©s associ√©es √† chaque modalit√© sera not√© ["jhk ].
Un bilan du nombre de param√®tres associ√©s √† chacun de ces mod√®les disponibles dans
mixmod est donn√© dans le tableau 2.
Proportions √©gales Proportions libres
["jhk ] K
Pd
j=1(mj   1) (K   1) +K
Pd
j=1(mj   1)
["jk] Kd (K   1) +Kd
["k] K (K   1) +K
["j] d (K   1) + d
["] 1 (K   1) + 1
Tab. 2  Nombre de param√®tres des dix mod√®les des classes latentes disponibles dans
mixmod.
c Revue MODULAD, 2006 -31- Num√©ro 35
4 Mod√®le de m√©lange pour la classication
4.1 Le probl√®me de classication
Le contenu de cette section est g√©n√©ral mais, pour simplier l'expos√©, on se place
dans le cadre continu. Les donn√©es consid√©r√©es dans mixmod pour la classication sont
dans cette section n vecteurs x = fx1; : : : ;xng de Rd. Cela signie que l'on se place
dans ce paragraphe dans la situation de donn√©es d√©crites par des variables quantitatives.
Le cas o√π les donn√©es sont qualitatives ne sera pas d√©taill√© ici, mais il n'induit pas de
dicult√© particuli√®re. Le but de la classication est d'estimer une partition inconnue z
de x en K classes, z = fz1; : : : ; zng d√©signant n vecteurs indicateurs ou √©tiquettes zi =
(zi1; : : : ; ziK), i = 1; : : : ; n avec zik = 1 si xi appartient √† la ke classe et 0 sinon. La vision
du probl√®me de la classication par un mod√®le de m√©lange est d'associer chaque classe
√† un composant du m√©lange. En r√®gle g√©n√©rale, toutes les √©tiquettes zi sont inconnues.
Cependant, un √©tiquetage partiel des donn√©es est possible, et mixmod permet de traiter
les cas o√π l'ensemble de donn√©es x est divis√© en deux sous-ensembles x = fx`;xug o√π x` =
fx1; : : : ;xmg (1  m  n) sont des donn√©es aux √©tiquettes connues z` = fz1; : : : ; zmg,
tandis que xu = fxm+1; : : : ;xng sont d'√©tiquettes inconnues zu = fzm+1; : : : ; zng. De plus,
mixmod permet de sp√©cier un poids pour chaque unit√© statistique. Cette possibilit√© est
utile notamment pour le traitement de donn√©es group√©es ou des fr√©quences.
Dans le contexte du mod√®le de m√©lange de mixmod, les donn√©es compl√®tes (xi; zi) (i =
1; : : : ; n) sont suppos√©es provenir de la distribution de probabilit√©
QK
k=1 (pk'(xi;k;k))
zik .
Dans ce contexte statistique, mixmod consid√®re deux approches du maximum de vrai-
semblance (m. v.) : la premi√®re, dite approche de m√©lange, consiste √† maximiser en 
la densit√© des donn√©es observ√©es, et la seconde, l'approche de classication, consiste √†
maximiser en  et zu la densit√© des donn√©es compl√®tes.
4.2 Estimation par l'approche de m√©lange
Cette approche consiste √† maximiser en  = (p1; : : : ; pK;1; : : : ;K;1; : : : ;K) la
logvraisemblance observ√©e :
L(;x; z`) =
mX
i=1
KX
k=1
zik ln (pk'(xi;k;k)) +
nX
i=m+1
ln
 
KX
k=1
pk'(xi;k;k)
!
: (4)
Une partition z^u est d√©duite de l'estimateur m.v. ^ par une proc√©dure du Maximum A
Posteriori (MAP) qui aecte chaque xi de xu au composant k de probabilit√© condition-
nelle :
tk(xi; ^) =
p^k'(xi; ^k; ^k)PK
k0=1 p^k0'(xi; ^k0; ^k0)
(5)
la plus grande que xi soit issue de ce composant. La maximisation de L(;x; z`) peut
√™tre r√©alis√©e dans mixmod par l'algorithme EM de Dempster et al. (1977) ou par une
version stochastique de EM, l'algorithme SEM (voir par exemple Celeux and Diebolt,
1985; McLachlan and Krishnan, 1997). Dans la section 7, on d√©crit trois fa√ßons di√©rentes
de combiner ces algorithmes. Bien s√ªr, l'estimateur ^, et par cons√©quent z^u, d√©pend du
mod√®le de m√©lange consid√©r√© et du nombre de classes.
c Revue MODULAD, 2006 -32- Num√©ro 35
Exemple 1 (D√©partements fran√ßais) La gure 2 (a) d√©crit la partition et les isoden-
sit√©s des composants estim√©s par l'algorithme EM pour le m√©lange gaussien [pkkDAkD
0]
avec trois composants.
(a) (b)
Fig. 2  Partition estim√©e et isodensit√©s des composants pour les d√©partements fran√ßais :
(a) par EM et (b) par CEM.
4.3 Estimation par l'approche de classication
La seconde approche de mixmod est une approche de classication o√π les vecteurs
indicateurs zu, de l'origine inconnue du composant du m√©lange, sont trait√©s comme des
param√®tres. Cette approche vise √† maximiser la logvraisemblance compl√©t√©e :
CL(; zu;x; z`) =
nX
i=1
KX
k=1
zik ln (pk'(xi;k;k)) (6)
√† la fois en  et en zu. Le crit√®re CL peut √™tre maximis√© par une version de classication
de l'algorithme EM, l'algorithme CEM (Celeux and Govaert, 1992) qui inclut une √©tape
de classication (√©tape C) entre les √©tapes E et M. Dans la section 7, on d√©crit diverses
strat√©gies pour calculer l'estimateur de  utilisant cet algorithme.
Exemple 2 (D√©partements fran√ßais) La gure 2 (b) d√©crit la partition et les isoden-
sit√©s estim√©es par l'algorithme CEM pour le m√©lange gaussien [pkkDAkD
0] avec trois
composants. Cette solution est √† comparer √† celle obtenue par l'algorithme EM d√©crite
gure 2 (a).
5 Mod√®les g√©n√©ratifs d'analyse discriminante
Les donn√©es consid√©r√©es par mixmod pour l'analyse discriminante constituent un
√©chantillon d'apprentissage de n vecteurs (x; z) = f(x1; z1); : : : ; (xn; zn)g, o√π xi appar-
tient √† Rd, et zi est le vecteur indicateur de la classe de l'unit√© statistique i.
c Revue MODULAD, 2006 -33- Num√©ro 35
Le but est de construire √† partir de cet ensemble d'apprentissage, un classieur pour
pr√©dire la classe zn+1 d'une nouvelle observation d√©crite par le vecteur xn+1 de Rd et
d'origine inconnue. Notons qu'il est √©galement possible de pond√©rer les donn√©es dans le
contexte de l'analyse discriminante sous mixmod. Les hypoth√®ses statistiques sont les
m√™mes que celles utilis√©es pour la classication non supervis√©e.
Dans ce contexte supervis√©, le param√®tre  est estim√© par la maximisation de la vrai-
semblance compl√©t√©e (6). Comme z est parfaitement connu, l'obtention de l'estimation ^
du m.v. se r√©duit √† une √©tape de maximisation. Tout nouveau point x peut √™tre aect√© √†
l'une des K classes par la proc√©dure MAP avec ^.
En r√©sum√©, l'analyse discriminante est r√©alis√©e dans mixmod par les deux √©tapes
suivantes :
 √©tape M : Calcul de l'estimateur m.v. ^ de  par la maximisation de la logvrai-
semblance compl√©t√©e (6).
 √©tape MAP : Aectation de tout nouveau point x √† l'une des K classes par la
r√®gle suivante :
k(x) = argmax
k
tk(x; ^):
Exemple 3 (Oiseaux borealis) La gure 3 d√©crit les fronti√®res de classication, les iso-
densit√©s des composants et les individus dans le premier plan principal de l'ACP pour le
mod√®le de m√©lange gaussien le plus g√©n√©ral [pkkDkAkD
0
k].
Fig. 3  Limites des classes, isodensit√©s des composants et individus pour les donn√©es des
oiseaux avec le mod√®le [pkkDkAkD0k] dans le premier plan principal de l'ACP.
Exemple 4 (Oiseaux puns) La gure 4 repr√©sente la dispersion des individus au-
tour de chaque variable et de chaque modalit√© ainsi que la classication d'un individu
suppl√©mentaire dans le premier plan de l'ACM.
c Revue MODULAD, 2006 -34- Num√©ro 35
(a) (b)
Fig. 4  Illustration de l'analyse discriminante pour les donn√©es qualitatives : (a) disper-
sion et (b) nouvel individu dans le premier plan de l'ACM.
6 Les algorithmes de mixmod
6.1 L'algorithme EM
L'algorithme EM vise √† maximiser la vraisemblance du m√©lange dans un contexte non
supervis√©. Partant d'une valeur initiale arbitraire 0, la qe it√©ration de l'algorithme EM
consiste √† eectuer les deux √©tapes E et M maintenant d√©crites :
 √âtape E : Calcul des probabilit√©s conditionnelles tqik = tk(xi; 
q 1) que xi appar-
tienne √† la ke classe (i = m + 1; : : : ; n) en utilisant la valeur courante q 1 du
param√®tre.
 √âtape M : l'estimateur m.v. q de  est actualis√© en utilisant les probabilit√©s
conditionnelles tqik comme poids. Cette √©tape d√©pend bien s√ªr du mod√®le utilis√©. Les
formules d√©taill√©es pour les quatorze m√©langes gaussiens disponibles dans mixmod
sont donn√©es dans (Celeux and Govaert, 1995).
6.2 L'algorithme SEM
Dans la version stochastique SEM de l'algorithme EM, une √©tape S est incorpor√©e
entre les √©tapes E et M. Il s'agit d'une restauration al√©atoire des labels inconnus des
composants selon leur distribution conditionnelle courante. √Ä l'√©tape M, l'estimateur du
param√®tre  est actualis√© en maximisant la vraisemblance compl√©t√©e associ√©e √† cette
restauration des donn√©es manquantes. L'algorithme SEM n'assure pas une convergence
ponctuelle. Il engendre une cha√Æne de Markov dont la distribution stationnaire est plus ou
moins concentr√©e autour de l'estimateur du maximum de vraisemblance. Un estimateur
naturel tir√© de la suite (q)q=1;:::;Q engendr√©e par SEM est sa moyenne
PQ
q=r+1 
q=(Q  r)
(amput√©e des r premi√®res valeurs de chaue de l'algorithme). Un estimateur alternatif est
obtenu par s√©lection de la valeur de la suite SEM fournissant la plus grande vraisemblance.
c Revue MODULAD, 2006 -35- Num√©ro 35
6.3 L'algorithme CEM
L'algorithme Classication EM (CEM) incorpore une √©tape de classication entre les
√©tapes E et M de EM. Cette √©tape de classication consiste √† aecter chaque point √†
l'un des K composants par la proc√©dure MAP. Comme pour SEM, l'√©tape M consiste
alors √† actualiser l'estimateur du param√®tre  en maximisant la vraisemblance compl√©t√©e
associ√©e √† la restauration des donn√©es manquantes.
CEM est un algorithme de type K-means et, contrairement √† EM, il converge en un
nombre ni d'it√©rations. L'algorithme CEM ne maximise pas la vraisemblance observ√©e
L (4), mais vise √† maximiser la vraisemblance compl√©t√©e CL (6) en  et en zu. En
cons√©quence, CEM qui n'est pas destin√© √† maximiser la vraisemblance de , produit des
estimateurs biais√©s des param√®tres. Ce biais est d'autant plus fort que les composants du
m√©lange sont imbriqu√©s et que les proportions sont d√©s√©quilibr√©es (McLachlan and Peel,
2000, Section 2.21).
6.4 Fonctions M et MAP
Ces deux fonctions sont surtout utiles pour l'analyse discriminante. La fonction M est
d√©volue √† l'estimation du maximum de vraisemblance du param√®tre  d'un m√©lange dont
les labels z sont connus. Cette fonction de maximisation est simplement l'√©tape M utilis√©e
par les algorithmes SEM et CEM. La fonction MAP a √©t√© d√©crite dans la section 4.2.
7 Strat√©gies d'utilisation des algorithmes
7.1 Strat√©gies d'initialisation
Il y a cinq fa√ßons di√©rentes d'initialiser un algorithme dans mixmod. En dehors de
la premi√®re qui est d√©terministe, il est recommand√© de r√©p√©ter plusieurs fois le tandem
{strat√©gie d'initialisation/algorithme} an de s√©lectionner la meilleure solution possible
vis-√†-vis du crit√®re optimis√©, la vraisemblance observ√©e pour EM ou SEM, et la vraisem-
blance compl√©t√©e pour CEM :
 les trois algorithmes peuvent √™tre initialis√©s par une partition zu0 ou des valeurs des
param√®tres du m√©lange 0 sp√©ci√©s par l'utilisateur ;
 ils peuvent √™tre initialis√©s par une valeur al√©atoire de 0. Dans mixmod, ce d√©part
al√©atoire est obtenu en tirant au hasard la moyenne des composants parmi les obser-
vations, en xant des proportions √©gales et en choisissant une matrice de variance
commune et diagonale dont les √©l√©ments diagonaux sont √©gaux √† la variance empi-
rique de chaque variable. Cette strat√©gie, tr√®s utilis√©e, peut √™tre consid√©r√©e comme
une strat√©gie de r√©f√©rence ;
 l'algorithme EM peut √™tre initialis√© par la position produisant la plus grande valeur
de la vraisemblance compl√©t√©e obtenue apr√®s plusieurs lancements al√©atoires de l'al-
gorithme CEM. Le nombre de r√©plications de CEM est a priori inconnu et d√©pend de
la r√©partition entre les algorithmes du nombre total d'it√©rations disponibles fourni
par l'utilisateur (voir Biernacki et al., 2003) ;
 l'algorithme EM peut √™tre initialis√© par la position produisant la plus grande vrai-
semblance obtenue apr√®s le lancement al√©atoire de courtes et nombreuses ex√©cutions
de EM lui-m√™me. Par ex√©cution courte de EM, on entend que cet algorithme est
c Revue MODULAD, 2006 -36- Num√©ro 35
stopp√© d√®s que (Lq  Lq 1)=(Lq  L0)  10 2, Lq √©tant la vraisemblance observ√©e √†
la qe it√©ration. Ici 10 2 repr√©sente un seuil par d√©faut √† choisir au jug√©. Le nombre
de r√©plications d'ex√©cutions courtes de EM est a priori inconnu et d√©pend de la
r√©partition entre les algorithmes du nombre total d'it√©rations disponibles fourni par
l'utilisateur (voir Biernacki et al., 2003) ;
 l'algorithme EM peut √™tre d√©marr√© par la position fournissant la plus grande vrai-
semblance dans une suite d'estimations produites par l'algorithme SEM initialis√©
au hasard avec toujours une r√©partition des it√©rations choisie par l'utilisateur (voir
Biernacki et al., 2003).
7.2 R√®gles d'arr√™t
Dans mixmod, il y a trois fa√ßons d'arr√™ter un algorithme :
 les algorithmes EM, SEM et CEM peuvent √™tre arr√™t√©s par un nombre maximal
d'it√©rations ;
 un algorithme peut √™tre arr√™t√© par un seuil sur l'am√©lioration relative du crit√®re en
jeu (la logvraisemblance L (4) ou la logvraisemblance compl√©t√©e CL (6)). Pour l'al-
gorithme EM cette possibilit√© n'est pas recommand√©e car cet algorithme peut faire
face √† des situations de convergence lente. Il est recommand√© d'arr√™ter l'algorithme
CEM, qui converge en un nombre ni d'it√©rations, √† sa position stationnaire ;
 un algorithme peut √™tre arr√™t√© d√®s que l'une des r√®gles d'arr√™t pr√©c√©dentes est rem-
plie.
7.3 Cha√Ænage d'algorithmes
Dans mixmod il est facile de combiner les algorithmes EM, SEM et CEM selon sa fan-
taisie. Cette possibilit√© peut produire des strat√©gies d'initialisation originales et ecaces,
comme celles pr√©sent√©es dans Biernacki et al. (2003).
8 S√©lection de mod√®les
Il est bien s√ªr du plus haut int√©r√™t d'√™tre capable de s√©lectionner automatiquement un
mod√®le de m√©lange M et un nombre K de composants. Cependant, choisir un mod√®le de
m√©lange raisonnable d√©pend beaucoup du but de la mod√©lisation. C'est pourquoi, nous
distinguons, dans ce qui suit, les points de vue estimation de densit√©, classication et
analyse discriminante.
8.1 Les points de vue estimation de densit√© et classication
Dans mixmod trois crit√®res sont disponibles dans un contexte non supervis√© : BIC,
ICL et NEC. Lorsqu'aucune information n'est disponible sur K, il est recommand√© de
faire varier K entre K = 1 et le plus petit entier plus grand que n0:3 (voir Bozdogan,
1993).
Pour l'estimation de densit√©, BIC (Bayesian Information Criterion) doit √™tre pr√©f√©r√©.
Notant M;K le nombre de param√®tres ind√©pendants du mod√®le de m√©lange M avec K
c Revue MODULAD, 2006 -37- Num√©ro 35
composants, le crit√®re BIC s'√©crit comme un crit√®re de vraisemblance p√©nalis√©e :
BICM;K =  2LM;K + M;K ln(n): (7)
Le couple (M;K) conduisant √† la plus petite valeur de BIC est choisi. Bien que les
conditions de r√©gularit√© classiques justiant BIC (Schwarz, 1978) ne sont pas remplies
pour les m√©langes, on peut prouver que pour de nombreux m√©langes, BIC est un crit√®re
convergent (K√©ribin, 2000). De plus BIC s'av√®re ecace en pratique (voir par exemple
Fraley and Raftery, 1998).
Pour la classication, ICL et NEC peuvent choisir des mod√®les plus parcimonieux
et robustes. Pour prendre en compte la capacit√© d'un mod√®le de m√©lange √† r√©v√©ler une
structure en classes dans les donn√©es, on peut pr√©f√©rer au crit√®re BIC le crit√®re ICL
(Integrated Complete-data Likelihood) (Biernacki et al., 2000) qui s'√©crit :
ICLM;K = BICM;K   2
nX
i=m+1
KX
k=1
z^ik ln(tik); (8)
o√π tik = tk(xi; ^M;K) (avec ^M;K l'estimateur m.v. du param√®tre pour le mod√®le M et le
nombre de composants K) et o√π z^ = MAP(^M;K). Ce crit√®re √† minimiser est simplement
le crit√®re BIC p√©nalis√© par un terme d'entropie qui mesure le degr√© d'imbrication des
composants. Le crit√®re NEC (Normalized Entropy Criterion) propos√© par Celeux and
Soromenho (1996) utilise un terme d'entropie similaire EK =  
Pn
i=m+1
PK
k=1 tik ln(tik),
mais ce crit√®re est principalement destin√© √† √™tre utilis√© pour d√©terminer le nombre de
classes K, plut√¥t que la param√©trisation du mod√®le M (Biernacki and Govaert, 1999). Ce
crit√®re √† minimiser s'√©crit :
NECK =
EK
LK   L1
: (9)
On peut remarquer que NEC1 n'est pas d√©ni. Biernacki et al. (1999) ont propos√© la r√®gle
suivante, ecace pour lever cette ind√©termination : Soit K? minimisant NECK (2  K 
Ksup), Ksup √©tant un majorant du nombre de composants du m√©lange. On choisit K?
classes si NECK?  1, sinon on d√©cide qu'il n'existe pas de structure en classes dans les
donn√©es.
Exemple 5 (D√©partements fran√ßais) Cinq nombres de composants (K = 1   5) et
trois m√©langes gaussiens [pkkDAD
0], [pkkDAkD
0] et [pkkDkAkDk] sont consid√©r√©s.
L'algorithme EM est utilis√© pour chaque combinaison mod√®lenombre de composants. Les
gures 5 (a) et (b) donnent respectivement les valeurs de BIC pour chaque combinaison et
la partition associ√©e au choix de BIC. Les gures 5 (c) et (d) donnent les m√™mes choses
pour le crit√®re ICL.
c Revue MODULAD, 2006 -38- Num√©ro 35
(a) (b)
(c) (d)
Fig. 5  S√©lection d'une combinaison mod√®lenombre de composants pour les d√©parte-
ments fran√ßais : (a) valeurs de BIC ; (b) la partition optimale associ√©e ; (c) valeurs de ICL ;
(d) la partition optimale associ√©e.
c Revue MODULAD, 2006 -39- Num√©ro 35
8.2 Le point de vue de l'analyse discriminante
Dans ce cas, le mod√®le M doit √™tre s√©lectionn√©, mais le nombre de composants du
m√©lange est connu. Dans mixmod deux crit√®res sont propos√©s dans un contexte supervis√© :
BIC et le taux d'erreur √©valu√© par validation crois√©e (CV). Le crit√®re CV est sp√©cique √†
la classication supervis√©e. Il est d√©ni par :
CVM =
1
m
mX
i=1
(z^
(i)
i ; zi) (10)
o√π  repr√©sente le co√ªt 0-1 et z^(i)i le groupe d'aectation de xi lorsque le classieur est
construit √† partir de l'√©chantillon total (x; z) priv√© de (xi; zi). Des estimations rapides des
n r√®gles de discrimination sont implant√©es dans le cas gaussien (Biernacki and Govaert,
1999).
Dans mixmod, selon une approche d√©crite dans Bensmail and Celeux (1996), il est
possible de s√©lectionner l'un des quatorze m√©langes gaussiens par minimisation du crit√®re
CV. On doit, cependant, signaler que ce crit√®re fournit une estimation optimiste du vrai
taux d'erreur. En eet, c'est une situation o√π la m√©thode inclut la s√©lection d'un mod√®le
parmi plusieurs. Aussi le vrai taux d'erreur doit √™tre √©valu√© sur un √©chantillon ind√©pen-
dant. Typiquement, trois √©chantillons sont n√©cessaires : un √©chantillon d'apprentissage des
mod√®les, un √©chantillon de validation pour choisir l'un des mod√®les et un √©chantillon test
pour √©valuer le vrai taux d'erreur de la m√©thode compl√®te. Cela signie que lorsque l'on
utilise la validation crois√©e pour √©valuer les performances d'un mod√®le, on doit eectuer
une double validation crois√©e pour obtenir un estimateur sans biais du taux d'erreur. Cette
proc√©dure est implant√©e dans mixmod.
Exemple 6 (Oiseaux borealis) Nous avons consid√©r√© quatre m√©langes gaussiens [pDAD 0],
[pkDkAkD
0
k], [pkDAD
0] et [pkkDkAkD
0
k]. Les gures 6 (a) et (b) donnent respective-
ment les valeurs du crit√®re CV pour chaque mod√®le et le classieur associ√© au meilleur
mod√®le pour ce crit√®re.
(a) (b)
Fig. 6  S√©lection d'un m√©lange gaussien pour les oiseaux : (a) valeurs de CV et (b) r√®gle
de classement optimale associ√©e.
c Revue MODULAD, 2006 -40- Num√©ro 35
9 Fonctions associ√©es
Les environnements Matlab et Scilab fournissent des fonctions de haut niveau en
particulier pour les repr√©sentations graphiques.
9.1 Repr√©sentation graphique des crit√®res
L'une des sorties optionnelles de la fonction mixmod est un tableau de dimension quatre
fournissant les valeurs de tous les crit√®res demand√©s pour toutes les strat√©gies, tous les
nombres de classes et tous les mod√®les demand√©s. √Ä partir de ce tableau, des graphes de
variation peuvent √™tre dessin√©s dans mixmod. Des illustrations de cette possibilit√© sont
donn√©es dans les gures 5 (a), (c) et 6 (a).
(a) (b)
Fig. 7  Densit√© du m√©lange : (a) premier axe de l'ACP et (b) espace 2D d'origine.
c Revue MODULAD, 2006 -41- Num√©ro 35
9.2 La fonction mixmodView pour les graphiques
mixmod propose la fonction mixmodView de visualisation des r√©sultats. Cette fonction
permet de faire des graphiques g√©n√©r√©s √† partir des sorties de la fonction mixmod (densit√©s,
isodensit√©s, etc.) en dimension un, deux ou trois.
Les graphiques suivants sont disponibles (liste non exhaustive) :
 les isodensit√©s, la repr√©sentation des densit√©s des composants et du m√©lange sur le
premier axe de l'ACP ;
 le trac√© des limites de classes, les isodensit√©s des composants et la repr√©sentation
des individus dans le premier plan de l'ACP ;
 la densit√© du m√©lange dans le premier plan de l'ACP ;
 les individus et les labels dans le premier espace 3D de l'ACP.
Beaucoup de ces caract√©ristiques ont √©t√© illustr√©es dans les exemples pr√©c√©dents. L'exemple
suivant montre des graphiques de densit√©.
Exemple 7 (D√©partements fran√ßais) Les gures 7 (a) et (b) donnent respectivement
la densit√© du m√©lange sur le premier axe de l'ACP et dans l'espace 2D d'origine.
9.3 La fonction printMixmod pour des r√©sum√©s
La fonction printMixmod peut √™tre utilis√©e pour r√©sumer les r√©sultats de la fonction
mixmod. Elle fournit un r√©sum√© synth√©tique des r√©sultats (conditions d'entr√©e, valeur de
crit√®res, logvraisemblance, logvraisemblance compl√©t√©e, estimation des param√®tres, etc.)
9.4 La fonction inputMixmod pour les entr√©es
La fonction inputMixmod produit des structures Scilab ouMatlab qui peuvent √™tre
utilis√©es par la fonction mixmod. Elle permet de sp√©cier facilement les crit√®res, les mod√®les
de m√©lange, les algorithmes, leurs strat√©gies d'utilisation et leurs r√®gles d'arr√™t.
R√©f√©rences
Agresti, A. (1990). Categorical Data Analysis. Wiley, New York.
Baneld, J. D. and Raftery, A. E. (1993). Model-based Gaussian and non-Gaussian
clustering. Biometrics, 49 :803821.
Bensmail, H. and Celeux, G. (1996). Regularized Gaussian discriminant analysis through
eigenvalue decomposition. Journal of the American Statistical Association, 91(2) :1743
17448.
Biernacki, C., Beninel, F., and Bretagnolle, V. (2002). A generalized discriminant rule
when training population and test population dier on their descriptive parameters.
Biometrics, 58(2) :387397.
Biernacki, C., Celeux, G., and Govaert, G. (1999). An improvement of the NEC criterion
for assessing the number of clusters in a mixture model. Pattern Recognition Letters,
20 :267272.
c Revue MODULAD, 2006 -42- Num√©ro 35
Biernacki, C., Celeux, G., and Govaert, G. (2000). Assessing a mixture model for clustering
with the integrated completed likelihood. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(7) :719725.
Biernacki, C., Celeux, G., and Govaert, G. (2003). Choosing starting values for the EM
algorithm for getting the highest likelihood in multivariate gaussian mixture models.
Computational Statistics and Data Analysis, 41 :561575.
Biernacki, C. and Govaert, G. (1999). Choosing models in model-based clustering and
discriminant analysis. J. Statis. Comput. Simul., 64 :4971.
Bock, H. (1986). Loglinear models and entropy clustering methods for qualitative data.
In Gaull, W. and Schader, M., editors, Classication as a tool of research, pages 1926.
North-Holland, Amsterdam.
Bozdogan, H. (1993). Choosing the number of component clusters in the mixture-model
using a new informational complexity criterion of the inverse-sher information matrix.
In Opitz, O., Lauritzen, B., and Klar, R., editors, Information and Classication, pages
4054, Heidelberg. Springer-Verlag.
Celeux, G. and Diebolt, J. (1985). The SEM algorithm : A probabilistic teacher algorithm
derived from the EM algorithm for the mixture problem. Computational Statistics
Quarterly, 2 :7382.
Celeux, G. and Govaert, G. (1991). Clustering criteria for discrete data and latent class
models. Journal of Classication, 8(2) :157176.
Celeux, G. and Govaert, G. (1992). A classication EM algorithm for clustering and two
stochastic versions. Computational Statistics and Data Analysis, 14(3) :315332.
Celeux, G. and Govaert, G. (1995). Gaussian parsimonious clustering models. Pattern
Recognition, 28(5) :781793.
Celeux, G. and Soromenho, G. (1996). An entropy criterion for assessing the number of
clusters in a mixture model. Journal of Classication, 13 :195212.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical
Society, B 39 :138.
Diday, E. and Govaert, G. (1974). Classication avec distance adaptative. C. R. Acad.
Sc. Paris, s√©rie A, 278 :993995.
Fraley, C. and Raftery, A. E. (1998). How many clusters ? Which clustering method ?
Answers via model-based cluster analysis. Computer Journal, 41 :578588.
Friedman, H. P. and Rubin, J. (1967). On some invariant criteria for grouping data.
Journal of American Statistical Association, 62 :11591178.
Goodman, L. A. (1974). Exploratory latent structure models using both identiable and
unidentiable models. Biometrika, 61 :215231.
c Revue MODULAD, 2006 -43- Num√©ro 35
K√©ribin, C. (2000). Consistent estimation of the order of mixture models. Sankhy√£, Series
A, 1 :4966.
Lazareld, P. F. and Henry, N. W. (1968). Latent Structure Analysis. Houghton Miin
Company, Boston.
Maronna, R. and Jacovkis, P. (1974). Multivariate clustering procedure with variable
metrics. Biometrics, 30 :499505.
McLachlan, G. J. (1992). Discriminant Analysis and Statistical Pattern Recognition.
Wiley, New York.
McLachlan, G. J. and Krishnan, K. (1997). The EM Algorithm. Wiley, New York.
McLachlan, G. J. and Peel, D. (2000). Finite Mixture Models. Wiley, New York.
Schroeder, A. (1976). Analyse d'un m√©lange de distributions de probabilit√© de m√™me type.
Revue de Statistique Appliqu√©e, 24(1) :3962.
Schwarz, G. (1978). Estimating the number of components in a nite mixture model.
Annals of Statistics, 6 :461464.
Scott, A. J. and Symons, M. J. (1971). Clustering methods based on likelihood ratio
criteria. Biometrics, 27 :387397.
Ward, J. (1963). Hierarchical grouping to optimize an objective function. Journal of the
American Statistical Association, 58 :236244.
c Revue MODULAD, 2006 -44- Num√©ro 35
