  
Vers l'échantillonnage d'un entrepôt de données 
 
Raphaël Féraud, Fabrice Clérot 
 
France Télécom R&D, avenue Pierre Marzin, 22307 Lannion 
Contact : raphael.feraud@orange-ftgroup.com 
 
Résumé. L'afflux de données sur les usages des produits et services nécessite 
des traitements lourds pour les transformer en information. Or la capacité à 
traiter les données ne peut pas suivre l'augmentation exponentielle des volumes 
stockés. Avec les technologies actuelles, un difficile compromis doit être trou-
vé entre le coût de mise en œuvre et la qualité de l'information produite. Nous 
proposons une approche basée sur l'échantillonnage d'un entrepôt de données 
pour déployer à moindre coût un système d'information décisionnel utilisant 
tout notre potentiel d'information. La brique technologique essentielle pour 
construire ce système repose sur un opérateur d'échantillonnage des jointures. 
1 Introduction 
Une tendance lourde depuis la fin du siècle dernier est l'augmentation exponentielle du 
volume des données stockées. Les progrès de ces capacités de stockage ne se traduisent pas 
nécessairement par une meilleure compréhension de l'environnement. En effet, les données 
doivent être analysées afin de les transformer en connaissance or la capacité à traiter ces 
données n'a pas suivi cette augmentation exponentielle.  
La première raison est simple : le processus d'analyse des données requiert une interven-
tion humaine. Chaque augmentation de la couverture des domaines de données stockées 
induit une augmentation des équipes qui analysent ces données pour les transformer en in-
formation exploitable. 
La seconde raison est liée à la capacité de traitement des données. Même si la puissance 
de calcul des ordinateurs suivait une pente équivalente à celle de la capacité de stockage (la 
loi de Moore), il existerait toujours des barrières algorithmiques bien plus infranchissables. 
Par exemple, pour un algorithme dont la complexité est en O(n²), le doublement d'une puis-
sance de calcul ne permet d'augmenter que de √2 les possibilités de traitements. 
Dès lors calculer des indicateurs pour alimenter un datamart devient problématique et 
demande une architecture technique coûteuse. De plus les requêtes ad hoc sont susceptibles 
de bloquer l'entrepôt de données. Elles sont en général prohibées ou fortement administrées 
de manière à prévenir l'engorgement du système d'information. Les techniques OLAP sont 
efficaces pour traiter des requêtes d'agrégation sur des cubes de données prédéfinis. Ces 
techniques manquent de souplesse et d'expressivité pour répondre à l'ensemble des requêtes 
ad hoc. 
 
Avec nos technologies, il est difficile d'exploiter tout le potentiel de l'information contenu 
dans l'entrepôt. 
Echantillonnage d'un entrepôt de données 
 
2 Relever le défi de la volumétrie 
Pour effectuer une analyse exploratoire sur des données très volumineuses, une approche 
consiste à approximer le résultat d'une requête (Hellerstein J. M. et al 1997, Haas P. J. et al 
1999, Hellerstein J. M. et al 2000). La requête est lancée et elle s'arrête lorsque le degré de 
confiance est suffisant. Cette technique est souple et permet d'approximer toutes les requêtes. 
Elle présente néanmoins un défaut applicatif important : les requêtes sont lancées sur l'entre-
pôt de données. Lorsque quelques individus hors normes pèsent lourds dans le résultat d'une 
requête, comme une moyenne, il peut être nécessaire de parcourir toute la base pour obtenir 
un degré d'approximation satisfaisant. Cette limitation peut se révéler problématique lorsque 
l'entrepôt de données est bloqué par des requêtes exploratoires.  
Une approche plus générale pour analyser des données très volumineuses est de produire 
un synopsis des requêtes. Le synopsis consiste en une structure de données permettant d'ac-
célérer les traitements d'une requête. Le synopsis peut ensuite être interrogé de manière auto-
nome pour des requêtes exploratoires. Le bon fonctionnement du système d'information 
décisionnel est garanti, tout en donnant la souplesse nécessaire à l'exploration des données. 
L'histogramme des valeurs des attributs est le synopsis le plus courant et le plus simple à 
utiliser. Il permet d'accélérer le résultat d'une requête au prix d'un résultat approché (Ioanni-
dis Y. et al 1995, Ioannidis Y. et al 1999, Aboulnaga A. et al 1999, Poosla V. et al 1996, 
Poosla V. 1997). Les ondelettes peuvent être utilisées pour décomposer les attributs de ma-
nière hiérarchique pour mieux contrôler le compromis entre la précision et la rapidité (Vitter 
J. S. et al, 1999). Les limites de cette approche sont atteintes lorsque la requête porte sur 
plusieurs attributs. De plus, son implémentation nécessite des changements considérables sur 
les moteurs de base de données pour créer et maintenir les histogrammes ou les décomposi-
tions en ondelettes. 
Le projet AQUA développé par Bell propose un moteur d'approximation de requêtes basé 
sur des synopsis (Gibbons P. B. et al 1998, Gibbons P. B. et al 1999, Acharyal S. et al 1999-
1, Acharyal S. et al 1999-2). Le synopsis utilisé est une jointure échantillonnée. L'ensemble 
des synopsis correspond à un sous-ensemble des jointures de la base de données.  La princi-
pale limitation de cette approche est liée à la méthode d'échantillonnage : chaque synopsis 
doit nécessairement correspondre à une jointure sur clé étrangère, c'est-à-dire une jointure où 
la clé de jointure est une des clés primaires d'une des tables considérées. De plus, le graphe 
de connexion de la base de données ne doit pas avoir de cycles.  
 
Notre approche pour relever le défi de la volumétrie est d'utiliser un synopsis se basant 
sur un échantillon de l'entrepôt. Cet échantillon est alimenté au même rythme que l'entrepôt. 
Il n'a pas besoin d'être recalculé pour être mis à jour. Pour la plupart des requêtes, une ré-
ponse approchée pourra être obtenue à partir d'opérations sur cet  échantillon de l'entrepôt. 
L'échantillon de l'entrepôt sera alors interrogé de manière autonome. Pour les requêtes, plus 
complexes, plus rares, l'échantillon devra être complété en temps réel par une requête sur 
l'entrepôt. Nous garantissons ainsi le maximum de souplesse et de rapidité, tout en utilisant 
avec parcimonie les ressources de l'entrepôt. La brique technologique essentielle pour cons-
truire ce système repose sur un opérateur d'échantillonnage des jointures. Cet opérateur 
d'échantillonnage doit permettre d'échantillonner une jointure sans faire la moindre hypo-
thèse sur la nature de la clé de jointure, afin de pouvoir échantillonner n'importe quel entre-
pôt de données. Dans la partie suivante, nous allons présenter un opérateur d'échantillonnage 
de jointures. 
I. Féraud, R. et al. 
                                                                                                          
3 L'échantillonnage de jointures 
3.1 Echantillonnage d'une équijointure sur clé primaire 
Le cas le plus simple de l'échantillonnage d'une équijointure correspond au cas où la clé 
de jointure est la clé primaire d'une des deux tables. Dans ce cas, il existe une méthode per-
mettant d'échantillonner la jointure sans la calculer complètement (Olken F.1993) . Soit R1 et 
R2 les deux relations à échantillonner. La clé primaire de R2 est utilisée comme clé de join-
ture.  Pour ]1,0[∈λ  on notera )(RSλ l'échantillon réalisé en retenant les instances de R qui 
satisfont au test ]1,0[, ∈< λλALEA  où ALEA est un nombre aléatoire tiré pour chaque 
instance, uniformément dans [0,1] .La probabilité d'inclusion d'une instance dans l'échantil-
lon est λ. En notant ECHλ(J) l'échantillon recherché de la jointure, on a :  
)()()( 2121 RSRRRSJECH λλλ ><>< ==  
Dans la suite du document nous nous concentrerons sur le cas général, où la clé de join-
ture n'est pas la clé primaire d'une des deux relations. 
3.2 Echantillonnage d'une équijointure sur clé étrangère 
3.2.1 Difficultés 
L'approche triviale pour échantillonner une équijointure J consiste à calculer la jointure, 
puis à appliquer une fonction de tirage aléatoire : 
)()( 21 RRSJECH ><λλ =  
L'échantillon obtenu est le meilleur possible. Le temps de traitement est important puis-
qu'il correspond au calcul de l'équijointure.  
 
L'approche naïve pour échantillonner une équijointure J consiste à échantillonner chaque 
relation puis à les joindre : 
)()()( 21 RSRSJECH λλλ ><=  
Le temps de traitement est ici réduit puisque la jointure est faite sur des tables préalable-
ment échantillonnées. Qu'en est-il de la qualité de l'échantillon ? 
 
Afin de mettre en relief la difficulté l'échantillonnage d'un ensemble de relations, nous 
avons repris et développé un exemple donné dans (Chaudhuri S. et al, 1999-1). Soit deux 
relations R1 et R2 : { }),();...;,();,(1 11100 kbababaR =  
{ }),();...;,();,(2 01001 kcacacaR =  
 
Soit J l'équijointure de R1 et R2 avec la clé a. On a :  { }),,(),...,,,(),,,(),...,,,(21 0101100100 cbacbacbacbaRRJ kk== ><  
)( 21 RRS ><λ  contiendra en moyenne kλ2 éléments, avec une faible probabilité d'être égal 
à l'ensemble vide : 
{ } 0)1(0)( 221 →−=/= ∞→kkRRSP λλ ><  
Echantillonnage d'un entrepôt de données 
 
Il est en revanche beaucoup plus probable que ),2(),1( λλ RSRS ><  corresponde à l'en-
semble vide : 
{ } { } { })2(. )1(0)2()1( 10 RSaPRSaPRSRSP λλλλ ∉∉≥/=><  
{ } )²1(0)2()1( λλλ −≥/=⇔ RSRSP ><  
On a donc : 
{ } 00)2()1(
∞→
→//=
k
RSRSP λλ ><  
 
 
Figure 1 : l'approche triviale donne le meilleur résultat possible mais nécessite le calcul 
préalable de la jointure. L'approche naïve est rapide, mais peut conduire à une mauvaise 
estimation de la jointure. 
 
L'approche naïve, si elle est rapide, ne conduit pas nécessairement au même résultat que 
l'approche triviale. S n'est pas distributif sur l'équijointure. C'est ce qui rend difficile le pro-
blème de l'échantillonnage d'une équijointure de deux relations. 
3.2.2 Principales approches 
Olken et Rotem (1986,1993) ont proposé un algorithme permettant d'échantillonner une 
jointure sans la calculer préalablement dans le cas où la clé de jointure n'est pas la clé pri-
maire d'une des deux relations :  
















← )2)1((,
.2
.2
,)2,1( 11
max
RR
xR
xR
ACCEPTkWRRRS ik ><ψψ  
 
Ici, Sk(R1, R2) est un échantillon de la jointure comportant k instances. 
WR(k,<expression>) signifie que <expression> est évaluée jusqu'à ce qu'un ensemble de k 
éléments soit obtenu, avec remise des éléments sélectionnés. ACCEPT(α,<expression>) 
indique que l'élément évalué dans <expression> est accepté avec la probabilité α.  
ψ1(R) correspond au tirage d'une instance de R, xi est la valeur de la clé de jointure cor-
respondant à l'instance tirée, ixR. est la cardinalité de la valeur xi dans la relation R 
et ii xRxR .max. max = . 
I. Féraud, R. et al. 
                                                                                                          
Les auteurs ont montré que cette méthode d'échantillonnage est équivalente à calculer la 
jointure puis à l'échantillonner; on peut remarquer que seule la taille (k instances) de l'échan-
tillon est connue: pour connaître le taux d'échantillonnage, il faut au préalable connaître la 
taille de la jointure. Cette méthode d'échantillonnage nécessite de posséder des index sur R1 
et sur R2, ainsi que les fréquences de la clé de jointure pour la relation R2. En outre, la mé-
thode de Olken et Rotem (1986,1993), basée sur un algorithme de rejet, est relativement 
coûteuse. Le nombre de tirages et de tests pour chaque n-uplet de l'échantillon est de l'ordre 
de (Chaudhuri S. et al, 1999-2) : 
i
i
i xRxR
RxR
...
..
21
1max2
∑
 
 
Une approche pour tirer efficacement l'échantillon est d'effectuer un tirage biaisé sur le 
flux de données (Chaudhuri S. et al, 1999-2). L'algorithme se divise en deux phases : 
- La première phase utilise un réservoir avec des poids )(.2 txR  pour produire un 
échantillon S1 de R1 biaisé sur la fréquence de la clé de jointure de chaque n-uplet t 
dans R2.  
- La deuxième phase consiste à joindre chacun des n-uplets t1 de l'échantillon R1 avec 
un n-uplet t2 tiré aléatoirement tel que t2.x=t1.x. 
Chaque n-uplet de l'échantillon est tiré en une seule passe. L'index de la clé de jointure 
dans R2, et la connaissance des fréquences de la clé de jointure dans R2 sont nécessaires au 
fonctionnement de l'algorithme.  Les auteurs ont proposé des améliorations de cet algorithme 
pour tenir compte des individus hors norme (Chaudhuri S. et al, 1999-2). 
 
Les méthodes d'échantillonnage d'une équijointure peuvent être classées en trois catégo-
ries (Chaudhuri S. et al, 1999-2) : 
- Cas A : aucune information n'est utilisée sur R1 ou R2. 
- Cas B : un index et des statistiques sur la clé de jointures dans R2 sont utilisés. 
- Cas C : un index et des statistiques sur la clé de jointures dans R1 et R2 sont utilisés. 
Les méthodes appartenant à la catégorie B ou C utilisent la fréquence de la clé de jointure 
sur une table pour biaiser le tirage sur l'autre table. Ces méthodes sont difficilement générali-
sables à n équijointures. Considérons par exemple le cas d'un échantillon de la jointure de 
trois tables 321 RRR ><>< . Utiliser une méthode de la catégorie B ou C nécessite de disposer 
de statistiques et d'index sur 32 RR ><  ou de calculer au préalable la jointure 21 RR >< pour se 
ramener au cas à deux jointures. Ces approches sont difficilement applicables pour calculer 
des échantillons d'un nombre variable de jointures. Nous nous trouvons devant un paradoxe : 
pour échantillonner une équijointure, il est nécessaire de biaiser le tirage en fonction de la 
fréquence de la clé de jointure, mais pour échantillonner n équijointures, il n'est pas envisa-
geable de disposer d'index et des fréquences de la clé de jointures sur toutes les équijointures 
partielles. 
 
Nous proposons une approche permettant de dépasser ce paradoxe.  
Echantillonnage d'un entrepôt de données 
 
3.3 Echantillonnage d'équijointures par hachage 
3.3.1 Présentation de l'algorithme 
Soit X l'ensemble des valeurs de la clé de jointure x dans R1 et dans R2, et  h(x) une fonc-
tion de hachage bornée par N. Soit { }k.x h(tRtT k =∈= )que tels 1111 , l'ensemble des n-uplets de 
R1 dont le hachage suivant x a pour valeur k, et { }k.x) h(tRtT k =∈= 2222 que tels , l'ensemble des 
n-uplets de R2 dont le hachage suivant x a pour valeur k.   
Si la fonction de hachage choisie produit un hachage parfait de X, il n'y a pas de colli-
sions de clés différentes dans la table de hachage. En conséquence, quelque soit hx , il existe 
)h(xk h= tel que : hk xRT .11 =  et hk xRT .22 =  
)(
...
...
.
.
21
21
21
21
hX
i
ii
hh
N
j
jj
kk
xP
xRxR
xRxR
TT
TT
==⇒
∑∑
 
où P(xh) est la probabilité d'occurrence de la valeur xh dans X l'ensemble des valeurs pos-
sibles de la clé de jointure. Lorsqu'il n'y a pas de collisions, la construction des tables de 
hachage kk TT 21 ,  permet : 
- de calculer la probabilité de tirage suivant la fréquence des valeurs de la clé de join-
ture dans 21 RRJ ><= .  
- de fournir un index de 21 , RR permettant d'accéder par la clé de jointure aux n-uplets.  
Ces deux propriétés sont à la base de l'algorithme d'échantillonnage d'équijointure par 
hachage (Figure 2).  
 
0=S  
Pour k de 1 à N  
Pour i de 1 à m 
0=kiT  
Pour i de 1 à n 
Pour j de 1 à iR   
).( xthk ji=  
{ }jikiki tTT +=  
1+= ki
k
i TT  
Pour k de 1 à N, Calcul de 
∑∏
∏
= N
j
m
i
j
i
m
i
k
i
T
T
kP )(  
Tant que JS λ<  
On tire k suivant )(kP  
Pour i de 1 à m   
On tire kii Tt ∈  uniformément 
Si xtxtxt m ...... 21 ===  alors { }mtttSS ...21+=  
Figure 2 : Algorithme d'échantillonnage de n équijointures par hachage 
I. Féraud, R. et al. 
                                                                                                          
 
Si le hachage est parfait, l'échantillon ainsi obtenu contient un échantillon de J.λ  n-
uplets tirés suivant la fréquence de la clé de jointure dans 21 RRJ ><= . On peut noter que 
pour obtenir un échantillon correspondant à un taux d'échantillonnage fixé à l'avance, il faut 
connaître la taille de la jointure 21 RR >< ; il existe des techniques rapides permettant d'esti-
mer cette taille (Alon N. et al, 1999). Si on ne connaît pas la taille de la jointure, on doit se 
contenter de fixer à l'avance la taille de l'échantillon. Si des collisions se produisent, les n-
uplets sont tirés suivant une approximation de la fréquence de la clé de jointure. Dans ce cas, 
l'échantillon pourrait contenir des n-uplets ne faisant pas partie de la jointure. C'est pourquoi, 
un test a été rajouté avant l'inclusion du n-uplet dans l'échantillon (Figure 2).  
On remarquera que cet algorithme ne nécessite aucune connaissance sur les relations R1 
et R2. Si les relations R1 et R2 sont indexées par la clé de jointure, la fonction de hachage est 
remplacée par l'index afin d'obtenir un hachage parfait. Comme les statistiques sur la clé de 
jointure et l'index de la clé de jointure sont évalués par le hachage des relations, la méthode 
d'échantillonnage par hachage est naturellement généralisable à n équijointures (Figure 2) 
3.3.2 Complexité algorithmique 
La complexité de cet algorithme d'échantillonnage de m équijointures est de l'ordre de : 
))(1
..(.)(.).(..2
cP
Jm
ODNOBmnOAC
−
++=
λ
 
Le premier terme représente le nombre d'appels à la fonction de hachage. A représente le 
coût associé à la fonction de hachage. n représente le cardinal de la plus grande des relations 
de la jointure. m est le nombre d'équijointures à réaliser, on a : nm <<  
Le second terme correspond au calcul de la fonction de probabilité. B représente le coût 
du calcul de P(k). N est le nombre de cases de la fonction de hachage. Un choix judicieux de 
N correspond à un nombre premier de l'ordre du nombre de valeurs de la clé de jointure. On 
a donc nN < .  
Le dernier terme est le nombre de tirages dans les tables de hachage kiT . D est le coût de 
tirage d'un n-uplet suivant P(k). P(c) représente la probabilité de collisions dans les tables de 
hachage. Dans l'implémentation proposée, plus le nombre de collisions est important, moins 
l'algorithme est rapide. Une autre approche moins couteuse en temps de calcul consisterait à 
parcourir a posteriori l'échantillon pour supprimer les n-uplets ne faisant pas partie de la 
jointure. Dans ce cas la taille de l'échantillon généré et donc la qualité de l'estimation serait 
moindre. 
3.3.3 La fonction de hachage 
Nous considérerons que les clés de nos données sont des nombres entiers. Si tel n'est pas 
le cas, une transformation préalable en nombre entier est nécessaire.  Nous proposons d'utili-
ser comme fonction de hachage, le reste de la division entière par un entier N : 
Nxxh mod)( =  
Cette fonction de hachage a l'avantage d'être très rapide à calculer. Pour minimiser le ris-
que de collisions, nous choisirons N comme étant un nombre premier le plus proche possible 
de X , le nombre de clés différentes. Il existe des techniques rapides permettant d'estimer ce 
Echantillonnage d'un entrepôt de données 
 
nombre avec très peu de mémoire et en une passe sur les tables (Cormode et al, 2002). Si ce 
nombre ou plutôt son ordre de grandeur n'est pas connu, nous fixerons N à un très grand 
nombre premier. 
4 Comparaison des méthodes 
4.1 Introduction 
Nous avons choisi comme points de référence la méthode triviale et la méthode naïve. La 
méthode naïve consiste à échantillonner les deux tables puis de faire leur jointure afin de 
produire un échantillon de la jointure. Cette approche est assurément la plus rapide, mais 
comme nous l'avons vu précédemment sa précision peut être faible. La méthode triviale 
consiste à calculer la jointure puis tirer un échantillon de la jointure. C'est une méthode lente, 
lorsque les tables ne tiennent pas en mémoire, mais précise. Le positionnement de la mé-
thode par hachage par rapport à ces points de références permettra d'en évaluer le potentiel. 
4.2 Le coût de traitement 
Afin de ne pas comparer les implémentations des méthodes et les conditions expérimen-
tales, pour comparer le coût de traitement, nous nous appuierons sur la complexité algorith-
mique. La complexité algorithmique de la méthode triviale correspond au coût de calcul d'un 
tri : 
).())log((..2 111 JOBnnOAC λ+=  
A1 mesure le coût d'une opération de comparaison et B1 le coût d'une sélection d'une ins-
tance dans la jointure. Généralement le premier terme est grand devant le second terme de 
l'équation. 
 
La complexité algorithmique de la méthode naïve s'obtient de la même manière : 
).()).log(.(..2 112 JOBnnOAC λλλ +=  
On a C1 > C2. 
 
La complexité algorithmique de la méthode d'échantillonnage par hachage est : 
))(1
.(.)(.)(..2 2223
cP
J
ODNOBnOAC
−
++=
λ
 
Cette méthode d'échantillonnage a un coût qui dépend du paramètre N. Plus N est grand, 
plus le second terme de l'équation est important et plus la probabilité de collision tend vers 0. 
Nous avons tracé sur un exemple le temps de traitement nécessaire à l'échantillonnage d'une 
jointure en fonction du choix de N (Figure 3). On remarquera que le temps de traitement 
diminue à mesure que N augmente. Il n'y a pas de différence significative de temps de trai-
tement pour N=39 989 ou N=99 991. En effet, le nombre de collisions tend vers 0, alors que 
N est encore très faible devant n= 6 044 279. On peut donc en conclure que pour un choix 
judicieux de N, par exemple le premier nombre premier supérieur au nombre de valeurs 
distinctes de la clé de jointure, on a : 
KJODnOAC ++= )(.)(..2 223 λ  
I. Féraud, R. et al. 
                                                                                                          
K est le coût associé au calcul de la fonction de probabilité de tirage des valeurs de la clé 
de jointure. K est petit devant les deux premiers termes de l'équation. 
 
Le premier terme de C3 est toujours inférieur au premier terme de C1. Lorsque les clés 
primaires des tables tiennent en mémoire, on peut considérer que A1, qui représente le coût 
d'une opération de comparaison, et A2 qui représente l'appel à la fonction de hachage sont 
similaires. En revanche dans le cas de la méthode par hachage le nombre d'opérations est en 
O(n) alors que pour la méthode triviale le nombre d'opération est en O(n log (n)).  
Si les clés primaires des tables ne tiennent pas en mémoire, le premier terme de C1 de-
vient très grand devant C3 puisqu'il faut rajouter à A1 le coût d'un accès aléatoire aux données 
stockées sur un disque alors que pour A2 l'accès est séquentiel. 
 
Temps de traitement versus N 
échelle logarithmique
37 101 397 997 1999 3989 39989 99991
N
-100
0
100
200
300
400
500
600
Te
m
ps
 
en
 
s
 
Figure 3 : Influence de N sur le temps de traitement 
 
La comparaison du second terme des équations  C1 et  C3 dépend du nombre de clés de 
jointure. En effet, dans le cas de la méthode par hachage, il est indispensable de faire un 
accès aléatoire aux tableaux contenant les clés de jointure. Pour la méthode triviale un accès 
séquentiel est suffisant. Si les tableaux tiennent en mémoire les temps de traitements associés 
au second terme sont comparables. Si ces tableaux ne tiennent pas en mémoire, le temps de 
traitement associé au second terme sera beaucoup plus important pour la méthode de ha-
chage.  On remarquera néanmoins que si les clés de jointures ne tiennent pas en mémoire, les 
clés primaires ne peuvent tenir en mémoire. Dans ce cas, le premier terme de C1 devient 
grand devant C3. 
En conclusion, la méthode par hachage est moins couteuse en temps de traitement que la 
méthode triviale. Elle peut néanmoins demander un effort d'implémentation supérieur.  
Echantillonnage d'un entrepôt de données 
 
4.3 Expérimentations 
4.3.1 La base de données 
Pour tester les différentes méthodes d'échantillonnage, nous avons utilisé un extrait d'une  
base de données issue du domaine des télécommunications.  
La table des tiers représente la table des clients. Ces tiers sont liés à 1 à n utilisateurs d'un 
ou plusieurs services de télécommunication (email, téléphone fixe, mobile…). La table des 
éléments de parc contient la liste des services souscrits. Chaque client possède 0 à n services. 
La table des comptes-rendus d'usage contient les traces laissées par l'usage des services. 
Chaque utilisateur a généré 0 à n comptes-rendus d'usage. Chaque service ou élément de parc 
a généré 0 à n comptes-rendus d'usage. 
 
 
Figure 4 : Le modèle de données représente l'usage de services par des utilisateurs  
 
La table des comptes-rendus est la plus volumineuse. Elle regroupe de l'ordre de 6 mil-
lions d'enregistrements. La table des éléments de parc compte approximativement 80000 
enregistrements. Les utilisateurs sont un peu plus de 15000. Dans le Tableau 1 nous avons 
caractérisé chacune des jointures produites sur ces tables sur la clé de jointure du client 
ID_Tiers. Cette clé n'est pas la clé primaire des tables Identité Tiers, Compte Rendu d'Usage 
et Elément de parc. 
 
R1><R2 |R1| |R2| | R1><R2| F_In_Moy F_Out_Moy 
EDPxCRU 81579 6044279 39192792 486,2328 6,486334 
IT x CRU 14819 6044279 6043231 413,2689 1 
IT x EDP 14819 81579 81573 5,506480 1,002482 
Tableau 1 : Description des jointures. F_In indique le "fan-in", nombre d'enregistrements de 
R1 joints à un enregistrement de R2; F_Out indique le "fan-out", nombre d'enregistrements 
de R2 joints à un enregistrement de R1.  
 
I. Féraud, R. et al. 
                                                                                                          
La jointure EDP x CRU est la plus volumineuse. C'est aussi la plus éloignée d'une join-
ture sur clé primaire : chaque enregistrement de la table EDP correspond en moyenne à 486 
enregistrements de la table CRU, et inversement, à un n-uplet de la table CRU correspond en 
moyenne à 6.5 n-uplets dans la table EDP.  La jointure IT x CRU est une jointure équivalente 
à une jointure sur la clé primaire de IT. IT x EDP est une jointure très proche de la clé pri-
maire. Dans la suite nous exposerons les résultats des différentes méthodes sur la jointure 
EDP x CRU. 
4.3.2 Estimation des fréquences  
Nous avons choisi d'estimer les fréquences des valeurs de la clé de jointure obtenues à 
partir des différentes méthodes d'échantillonnage pour les comparer. La précision d'un esti-
mateur se décompose en deux grandeurs, respectivement la variance et le biais : 
)²)(()()²( θθ −+=− TETVarTE  
 
Nous avons construit dix échantillons de 5% de la jointure pour chacune des méthodes 
d'échantillonnage. Nous avons calculé la valeur moyenne de la précision, du biais et la va-
riance des estimateurs des fréquences des valeurs de la clé de jointure (Tableau 2).  
 
 Précision moyenne Biais moyen Variance moyenne 
Hachage 4.86 1.07 4.74 
Naïve 8.84 5.4 8.73 
Triviale 4.87 1.11 4.76 
Tableau 2 : Précision, biais et variance moyenne des estimateurs des fréquences de la clé 
de jointure. Afin de faciliter la lecture des résultats, nous avons appliqué la transformation 
suivante aux valeurs : )10log(' 9 xx = .  
 
La différence de qualité d'estimation est très nette entre la méthode naïve et les méthodes 
triviales et par hachage. Comme attendu, la méthode naïve est peu précise et peu robuste. La 
précision et la variance des estimateurs obtenus par la méthode par hachage ou triviale sont 
similaires.  Les estimateurs proposés sont tous sans biais. Néanmoins, les biais de ces estima-
teurs basés sur des méthodes d'échantillonnage différentes ne tendent pas à la même vitesse 
vers 0. La méthode naïve présente un biais supérieur et une vitesse de convergence inférieure 
aux méthodes d'échantillonnage par hachage et triviale (Figure 5). 
Nous avons construit dix échantillons de 5%, 0.5 %  et 0.05 % de la jointure afin de com-
parer les vitesses de convergence des estimateurs des fréquences en fonction de la taille des 
échantillons. La précision des estimateurs converge plus rapidement pour les méthodes tri-
viales et par hachage que pour la méthode naïve (Figure 6). 
 
Echantillonnage d'un entrepôt de données 
 
Vitesse de convergence du biais
taux 0.05 %
 Naïve
 Triviale
 Hachage
3 5 7 10
5.5
6.0
6.5
7.0
7.5
8.0
8.5
9.0
9.5
 
Figure 5 : Vitesse de convergence du biais en fonction du nombre d'échantillons 
 
 
Précision versus taux d'échantillonnage
 Naïve
 Triviale
 Hachage
0.05 % 0.5 % 5 %
4
5
6
7
8
9
10
11
 
Figure 6 : Vitesse de convergence de la précision en fonction du taux d'échantillonnage 
 
I. Féraud, R. et al. 
                                                                                                          
5 Conclusion 
Nous avons proposé une méthode d'échantillonnage d'une jointure par hachage. Contrai-
rement aux méthodes proposées dans l'état de l'art, cette méthode peut être étendue à n join-
tures. De plus, cette méthode ne nécessite aucune connaissance particulière sur les relations à 
échantillonner. Elle est utilisable comme opérateur élémentaire pour générer l'échantillon 
d'un entrepôt de données quelconque. 
Nos résultats expérimentaux montrent qu'elle permet de construire des estimateurs d'une 
précision comparable ceux obtenus en échantillonnant une jointure préalablement calculée. 
Le coût de traitement théorique de cette méthode d'échantillonnage est inférieur au calcul de 
la jointure. 
La méthode par hachage nécessite cependant un accès aléatoire aux clés de jointures des 
relations à échantillonner. Si ces clés ne tiennent pas en mémoire, dans une implémentation 
naïve, l'algorithme peut être considérablement ralenti. Une solution pour prendre en compte 
cette contrainte d'implémentation consiste à diviser par hachage de la clé de jointure le 
schéma d'échantillonnage en sous schémas traités indépendamment.  
Une autre voie intéressante serait d'adapter cette méthode à l'échantillonnage d'un flux de 
données avec une table en calculant en ligne la probabilité de tirage d'une clé. 
Références 
 Aboulnaga A. and Chaudhuri S. (1999), Self-Tuning Histograms: Building Histograms 
Without Looking at Data. ACM SIGMOD. 
Acharya S., Gibbons P. B., Poosala V., and Ramaswamy S. (1999-1), The Aqua approximate 
query answering system. ACM SIGMOD International Conf. on Management of Data 
(SIGMOD '99). 
Acharya S., Gibbons P. B., Poosala V., and Ramaswamy S. (1999-2), Join Synopses for 
Approximate Query Answering.  ACM SIGMOD International Conf. on Management of 
Data (SIGMOD '99), pp. 275-286. 
Alon N., Gibbons P., Matias Y., and Szegedy M. (1999). Tracking Join and Self-Join Sizes in 
Limited Storage. 18'th ACM Principles of Database Systems conference, ACM Press, 
New York, pages 10-20. 
Cormode G., Datar M., Indyk P., and Muthukrishnan S. (2002). Comparing data streams 
using hamming norms (how to zero in). 28th International Conf. on Very Large Data 
Bases (VLDB), pages 335-345. 
Chaudhuri S., Motwani R. (1999-1). On Sampling and Relational Operators. IEEE on Data 
Engineering, 1999 
Chaudhuri S., Motwani E., and Narasayya V. (1999-2).  On Random Sampling over Joins. 
ACM SIGMOD. 
 Gibbons P. B. and Matias Y. (1999). Synopsis data structures for massive data sets. 
DIMACS: Series in Discrete Mathematics and Theoretical Computer Science: Special Is-
sue on External Memory Algorithms and Visualization. 
Echantillonnage d'un entrepôt de données 
 
Gibbons P. B., Poosala V., Acharya S., Bartal Y., Matias Y., Muthukrishnan S.,  Ramas-
wamy S., and Suel T. (1998). AQUA: System and techniques for approximate query an-
swering. tech. rep. Bell Laboratories, Murray Hill, New Jersey, 
.http://citeseer.ist.psu.edu/gibbons98aqua.html. 
Haas P. J. and Hellerstein J. M. (1999). Ripple Joins for Online Aggregation. ACM 
SIGMOD. 
Hellerstein J. M., Avnur R., and Raman V. (2000). Informix under CONTROL: Online Query 
Processing. Data Mining and Knowledge Discovery Journal. 
 Hellerstein J. M., Haas P. J., Wang H. J. (1997). Online Aggregation. In Proc Of ACM 
SIGMOD. 
Ioannidis Y. and Poosala V. (1995). Balancing Histogram Optimality and Practicality for 
Query Result Size Estimation.  ACM SIGMOD. 
Ioannidis Y. and Poosala V. (1999).  Histogram-Based Approximation of Set-Valued Query 
Answers. VLDB. 
Olken F. (1993). Random Sampling from Databases.  PhD thesis, U.C. Berkeley. 
 
Olken F., Rotem D. (1986). Simple random sampling from relational databases. 12th VLDB, 
pages 160-169. 
Poosala V., Loannidis Y.,Haas P., and Shekita E. (1996). Improved Histograms for Selectiv-
ity Estimation of Range Predicates.  ACM SIGMOD. 
Poosala V. (1997). Histogram-Based Estimation Techniques in Database Systems. PhD The-
sis, Univ. of Wisconsin. 
Vitter J. S., and Wang M. (1999). Approximate Computation of Multidimensional Aggre-
gates of Sparse Data Using Wavelets. ACM SIGMOD. 
Summary 
Turning the wealth of customer usage data into valuable information requires very de-
manding treatments and the increase in processing power cannot cope with the dramatically 
increasing volumes of data. Even with state-of-the-art technologies, a difficult balance has to 
be found between the processing cost and the quality of the information. We propose an 
approach based on the sampling of the data-warehouse as a means to deploy a low-cost in-
formation system while using all our potential information. This approach leverages a joint 
sampling technique which is accurate while avoiding to build the joint itself. 
 
 
