Formulation Condorcéenne du critère de la modularité
Lazhar Labiod, Nistor Grozavu, Younès Bennani
LIPN UMR 7030, Université Paris 13
99, avenue Jean-Baptiste Clément, 93430 Villetaneuse
Prénom.Nom@lipn.univ-paris13.fr,
Résumé. La mesure de modularité a été utilisée récemment pour la classifica-
tion de graphes (Newman et Girvan, 2004), (Agarwal et Kempe, 2008). Dans
ce papier, nous montrons que la mesure de modularité peut être formellement
étendue pour la classification non supervisée des données catégorielles. Nous
établissons également des connexions entre le critère de modularité et celui de
l’analyse relationnelle qui est basé sur le critère de Condorcet. Nous dévelop-
pons ensuite un algorithme efficace inspiré de l’heuristique de l’analyse rela-
tionnelle pour trouver la partition optimale maximisant le critère de modularité.
Les résultats expérimentaux montrent l’efficacité de notre approche.
1 Introduction
La classification automatique est une méthode d’apprentissage non supervisé permettant
le partitionnement d’un ensemble d’observations en classes. les méthodes de classification
automatique conduisent à une partition de la population initiale en groupes disjoints, tels que,
selon un critère choisi a priori, deux individus d’un même groupe aient entre eux un maximum
d’affinité et deux individus de deux groupes différents aient entre eux un minimum d’affinité.
La classification automatique a été largement étudiée en apprentissage automatique, en bases
de données et en statistique de divers points de vue.
De nombreuses applications de la classification automatique ont été discutées et de nom-
breuses techniques ont été développées. Une étape importante dans la conception d’une tech-
nique de classification consiste à définir un critère pour mesurer la qualité de partitionnement
en termes des deux objectifs cités ci-dessus. Pour la classification des données numériques
continues, il est naturel de penser à utiliser une mesure basée sur une distance géométrique.
Étant donnée une telle mesure, une partition appropriée peut être calculée par l’optimisation de
certaines quantités (par exemple, la somme des distances des observations à leurs centroïdes).
Toutefois, si les vecteurs de données contiennent des variables catégorielles, le problème de la
classification devient plus difficile et d’autres stratégies doivent être développées. C’est sou-
vent le cas dans de nombreuses applications où les données sont décrites par un ensemble
d’attributs descriptifs ou binaires, dont beaucoup ne sont pas numériques. Des exemples de
tels attributs sont le pays d’origine et la couleur des yeux dans les données démographiques.
De nombreux algorithmes ont été développés pour la classification des données catégorielles,
par exemple, (Barbara et al, 2002), (Gibson et al, 1998), (Huang, 1998) et ( Ganti et al, 1999).
La mesure de modularité a été utilisée récemment pour la classification de graphes (Agar-
wal et Kempe, 2008), (Newman et Girvan, 2004) et (White et Smyth, 2005). Dans ce papier,
Formulation Condorcéenne du critère de la modularité
nous montrons que le critère de modularité peut être formellement etendu pour la classification
des données catégorielles. Nous avons également établi des liens entre le critère de modularité
et celui de l’analyse relationnelle (AR) (Marcotorchino et Michaud, 1978) (Marcotorchino,
2006), qui est basée sur le critère de Condorcet. Nous développons ensuite une procédure effi-
cace inspirée de l’heuristique de l’AR pour trouver la partition optimale maximisant le critère
de modularité. Les résultats expérimentaux montrent l’efficacité de notre approche. La pre-
mière contribution de ce papier est l’introduction d’une mesure de modularité étendue pour
la classification des données catégorielles. La deuxième contribution est la présentation de la
mesure de modularité étendue comme un critère de Condorcet modifié. En particulier, nous
montrons que notre nouveau critère de modularité introduit une pondération en fonction du
profil de chaque observation.
Le reste du papier est organisé comme suit: la section 2 introduit quelques notations et
définitions et nous présentons l’approche de l’analyse relationnelle dans la section 3. La section
4 présente deux variantes de la mesure de modularité étendue et sa connexion avec le critère
de l’AR. Des discussions sur la procédure d’optimisation proposée sont décrites à la section 5.
La section 6 montre nos résultats expérimentaux et enfin, la section 7 présente des conclusions
et certains travaux futurs.
2 Définitions et notations
Soit I un ensemble de données avec N objets {O1,O2,...,ON} décrit par l’ensemble V de
M attributs (ou variables catégorielles) {V 1,V 2,..,V m,...,VM} chacun ayant p1,..,pm,...,pM
catégories, respectivement, et soit P =
∑M
m=1 pm, désigne le nombre total de catégories de
toutes les variables. Chaque variable catégorielle peut être décomposée en une collection de
variables indicatrices. Pour chaque variable V m, considérons les pm valeurs qui correspondent
naturellement aux nombres de 1 à pm et V m1 ,V m2 ,...,V mpm sont des variables binaires telles que,
pour chaque j, 1 ≤ j ≤ pm, V mj = 1 si et seulement si V m prend la jieme valeur. Ainsi
la matrice de données peut être exprimée comme une collection de M matrices Km, (m =
1,..,M) de terme général kmij tel que :
kmij =
{
1 si l’objet i possède la catégorie j de V m
0 sinon (1)
ce qui donne la matrcie disjonctiveK de dimensions N × P ; K = (K1|K2|...|Km|...|KM ).
2.1 Représentation relationnelle des données
Si les données se composent de N objets {O1,O2,...,ON} décrits par M attributs (ou
variables) {V 1,V 2..,V k,...,VM} sur les quelles ont été mesurés, le principe de comparaison
par paires consiste à transformer les données, qui sont habituellement représentées par une
matrice rectangulaire de dimension N ×M en deux matrices carrées, S et S¯. La matrice S
est appelée la matrice de Condorcet de terme général sii′ , représentant la mesure de similarité
globale entre les deux objets Oi et Oi′ , mesurée sur tous les M attributs. La matrice S¯ de
terme général s¯ii′ représente la mesure de dissimilarité globale entre ces deux objets. Pour
obtenir la matrice S, chaque attribut V m est transformé en une matrice carré Sm de taille
L. Labiod et al.
N × N et de terme général smii′ , représentant la mesure de similarité entre deux objets Oi
et Oi′ pour l’attribut V m. Pour obtenir la matrice S¯, une mesure de dissimilarité s¯mii′ entre
les objets Oi et Oi′ pour l’attribut V m est alors calculée comme le complément à la mesure
de similarité maximale possible entre ces deux objets. Comme la similarité entre deux objets
différents est inférieure ou égale à leur auto-similarités: c.a.d smii′ ≤ min(smii ,smi′i′), alors il
vient, s¯mii′ = 12 (s
m
ii + s
m
i′i′) − smii′ . Cela nous amène à une matrice de dissimilarité S¯m. Les
matrices S et S¯ sont alors obtenues en additionnant, respectivement, toutes les matrices Sm et
S¯m, soit S =
∑M
m=1 S
m = KKt et S¯ =
∑M
m=1 S¯
m
. La similarité globale entre chaque deux
objets Oi et Oi′ est donc sii′ =∑Mm=1 smii′ et leur dissimilarité globale est s¯ii′ =∑Mm=1 s¯mii′ .
2.1.1 Graphe non orienté et matrice de similarité
La mesure de modularité a été utilisée pour la classification des graphes, un lien intéressant
entre une matrice de données et la théorie des graphes peut être établi ici. Une matrice de
similarité peut être représentée par un graphe non orienté pondéré G = (V,E), où V = I
représente l’ensemble des sommets et E l’ensemble des arêtes. La matrice de données S peut
être considérée comme une matrice de poids associée au graphe G où chaque noeud i dans V
correspond à une ligne. Le lien entre deux sommets i et i′ a le poids sii′ , désignant l’élément
de la matrice située à l’intersection entre la ligne i et la colonne i′.
2.1.2 Relation d’équivalence
Considérons un ensemble de données divisé en L classes C = {C1,C2,...CL}. On peut
modéliser la partition C dans un esapce relationnel par une relation d’équivalence X , qui doit
respecter les propriétés suivantes
xii′ ∈ {0,1},∀(i,i′) binarité
xii = 1, ∀ii réflexivité
xii′ − xi′i = 0,∀(i,i′) symétrie
xii′ + xi′i′′ − xii′′ ≤ 1,∀(i,i′,i′′) transitivité
(2)
3 Analyse relationnelle
L’Analyse Relationnelle a été développée en 1977 par Marcotorchino et Michaud, s’inspire
des travaux du Marquis de Condorcet, qui s’est intéressé au 18ème siècle au résultat collectif
d’un vote à partir de votes individuels. Cette méthodologie est basée sur la représentation rela-
tionnelle (comparaison par paires) des différentes variables et l’optimisation sous contraintes
du critère de Condorcet.
D’une manière générale, la fonction objective correspond au critère d’adéquation de la
solution aux données. Le choix de ce critère est un point fondamental puisque c’est lui qui
induit la nature de l’intensité des ressemblances que l’on veut mettre en évidence. L’approche
relationnelle permet de choisir parmi une vaste gamme de critères celui qui répond le mieux au
problème posé par les données en présence. Certains critères opèrent sur des données binaires,
d’autres sont plus appropriés à des données de fréquences; la plupart sont basés sur des règles
de majorité qui déterminent le niveau de relation seuil au delà duquel on considère que deux
objets sont regroupables. Rappelons que l’un des atouts majeurs de l’approche relationnelle
Formulation Condorcéenne du critère de la modularité
réside dans le fait que l’on ne doit pas fixer a priori le nombre de classes de la partition. Ce
paramètre caractéristique de la solution est directement issu du traitement, réflétant ainsi le
potentiel classificatoire inhérent aux données.
L’analyse relationnelle est utilisée pour résoudre de nombreux problèmes rencontrés dans
des domaines comme: le classement des préférences, les systèmes de vote, la classification,
etc. L’approche de l’analyse relationnelle est un modèle de classification non supervisée qui
fournit automatiquement le nombre approprié de classes et qui prend en entrée une matrice de
similarité. Dans notre contexte, nous voulons regrouper les objets de l’ensemble I en classes
disjointes, la matrice de similarité S est donnée, le but est donc de maximiser la fonction
objective suivante :
RAR(S,X) =
∑
i
∑
i′
(sii′ −mii′)xii′ (3)
OùM = [mii′ = sii+sii′4 = M2 ]i,i′=1,...,N est la matrice des valeurs seuils. Notons que
le critère de Condorcet repose sur la notion de majorité, c’est à dire deux individus i,i′ seront
a priori affectés dans une même classe si et seulement si leur similarité sii′ est supérieure ou
égale à la valeur de majorité M2 .X est la solution recherchée, elle modélise une partition dans
un espace relationnel (une relation d’équivalence), et doit vérifier les propriétés données en
(2).
4 Extension de la mesure de modularité à la classification
des données catégorielles
Cette section explique comment adapter la mesure de modularité à la classification des
données catégorielles.
4.1 Graphe et modularité
La modularité est une mesure récemment utilisée pour mesurer la qualité d’une classifica-
tion de graphes, elle a immédiatement reçu une attention considérable comme en témoignent
les articles (Newman et Girvan, 2004), (Agarwal et Kempe, 2008). Comme dans le cas de la
classification relationnelle, la maximisation de la mesure de modularité peut être exprimée sous
la forme d’un problème de programmation linéaire en nombres entiers. Étant donné le graphe
G = (V,E), soit A une matrice binaire et symétrique où chaque entrée aii′ = 1 s’il existe une
arête entre les noeuds i et i′, s’il n’y a pas de lien entre les noeuds i et i′, aii′ est égal à zéro. A
est une matrice contenant toutes les informations sur le graphe G, est souvent appelée matrice
d’adjacence. Trouver une partition de l’ensemble des noeuds V en sous-ensembles homogènes
conduit à la résolution du programme linéaire en variables bivalentes suivant :
max
X
Q(A,X) (4)
où
Q(A,X) =
1
2|E|
N∑
i,i′=1
(aii′ − ai.ai
′.
2|E| )xii′ (5)
L. Labiod et al.
avec, 2|E| = ∑i,i′ aii′ = a.. est le nombre totald’arêtes (liens) et ai. = ∑i′ aii′ le degré
de l’objet i.
La modularité évalue la densité des arêtes dans les classes de façon relative à la densité
attendue en cas d’indépendance entre les extrémités des arrêtes. Elle prend ses valeurs entre
−1 et 1 et des valeurs positives, quand les classes ont plus d’arêtes observées que dans le cas
d’indépendance des extrémités des arêtes. Ce critère vaut 0 dans les deux cas d’une partition
triviale; le cas d’une seule classe et le cas où chaque noeud est isolé dans une classe. La
modularité comme le critère de Condorcet possède une propriété intéressante : elle ne nécessite
aucun paramètre comme par exemple le nombre de classes.
4.2 Première extension : Intégration a priori
L’intégration a priori consiste en une combinaison directe de graphes obtenus à partir de
toutes les variables dans un seul ensemble de données (graphe) avant d’appliquer l’algorithme
d’apprentissage. Prenons la matrice de Condorcet S, (où chaque entrée sii′ =
∑M
m=1 s
m
ii′ ), qui
peut être considérée comme une matrice de poids associée au graphe G = (V,E), où chaque
arête eii′ a le poids sii′ . Par analogie avec la mesure de modularité classique, nous définissons
l’extension Q1(S,X) comme suit (voir figure 1) :
FIG. 1 – – Intégration a priori
Q1(S,X) =
1
2|E|
N∑
i,i′=1
(sii′ − si.si
′.
2|E| )xii′ (6)
où 2|E| =∑i,i′ sii′ = s.. est le poids total et si. =∑i′ sii′ le degré de l’objet i.
Formulation Condorcéenne du critère de la modularité
4.3 Deuxième extension : Intégration intermédiaire
Pour cette extension, l’idée principale est de calculer une mesure de modularité combinée
à partir des mesures de modularités claculées séparément sur chaque graphe, et d’appliquer
ensuite l’algorithme d’apprentissage (voir figure 2).
FIG. 2 – – Intégration intermédiaire
L’intégration intermédiaires peut être considérée comme une variante de la technique dite
"Ensemble Clustering". Considérons un ensemble V de N points. Un ensemble de classifica-
tions est une collection deM solutions de classification: C = {S1,S2,...,SM}. Chaque solu-
tion de classification Sm pour m = 1,...,M , est une partition (une relation d’équivalence) de
l’ensemble V, à savoir Cm = {Cm1 ,Cm2 ,...,CmL }, où
⋃
l C
m
l = V . Étant donné un ensemble de
solutions de classification C, l’objectif est de combiner les différentes solutions de classifica-
tion et de calculer une nouvelle partition de V en groupes disjoints. Le défi dans le problème dit
"Ensemble Clustering" est la construction d’une fonction de consensus appropriée qui combine
les différentes solutions de classification dans une seule solution finale la plus représentative de
la collection des différentes classifications. La fonction objective à maximiser est la suivante :
Q2(Sm{m=1,..,M},X) =
M∑
m=1
Q(Sm,X)
=
M∑
m=1
1
2|Em|
n∑
i,i′=1
(smii′ −
smi. s
m
i′.
2|Em| )xii′
=
M
H
n∑
i,i′=1
(sii′ −
∑M
m=1 s
m
i. s
m
i′.
Hii′ )xii
′ (7)
où 2|Em| =∑i,i′ smii′ = sm.. est le poids total dans le grapheGm et smi. =∑i′ smii′ le degré
de l’objet i
L. Labiod et al.
H andHii′ Sont les moyennes harmoniques des suites (s1..,s2..,..,sm.. ,..,sM.. ) et
( s
1
i.s
1
i′.
s1..
,...,
smi. s
m
i′.
sm..
,..,
sMi. s
M
i′.
sM..
) respectivement.
M
H =
M∑
m=1
1
sm..
(8)
et ∑M
m=1 s
m
i. s
m
i′.
Hii′ =
M∑
m=1
smi. s
m
i′.
sm..
(9)
4.4 Relation entre la mesure de Modularité et le critère de Condorcet
Nous pouvons établir une relation entre les deux extensions de la mesure de modularité et
le critère de l’analyse relationnelle, en effet les fonctions Q1(S,X) et Q2(S,X) peuvent être
exprimées comme étant un critère modifié de celui de l’AR de la façon suivante :
4.4.1 Q1(S,X) comme un critère de Condorcet modifié
Q1(S,X) =
1
2|E| (RAR(S,X) + ψ1(S,X)) (10)
et
ψ1(S,X) =
n∑
i=1
n∑
i′=1
(mii′ − si.si
′.
2|E| )xii′ (11)
est le terme de pondération qui dépend du profil de chaque paire d’objets (i,i′).
4.4.2 Q2(S,X) comme un critère de Condorcet modifié
De la même manière l’extension Q2(S,X) s’écrira
Q2(Sm{m=1,..,M},X) =
M
H (RAR(S,X) + ψ2(S,X)) (12)
où
ψ2(S,X) =
n∑
i=1
n∑
i′=1
(mii′ −
∑M
m=1 s
m
i. s
m
i′.
Hii′ )xii
′ (13)
Les deux extensions de la mesure modularité permettent d’introduire un système de pon-
dération en fonction du profil de chaque paire d’objets.
Formulation Condorcéenne du critère de la modularité
5 Procédure d’optimisation
Comme la fonction objective est linéaire par rapport àX et les contraintes queX doit res-
pecter sont des équations linéaires, théoriquement on peut résoudre le problème en utilisant un
solveur de programmation linéaire en nombres entiers. Toutefois, ce problème est NP-difficile.
En conséquence, dans la pratique, nous utilisons des heuristiques pour faire face aux grands
ensembles de données.
5.1 Décomposition de la mesure de modularité
Les deux extensions de la mesure modularité peuvent être décomposées en termes de la
contribution de chaque objet i dans chaque classe Cl de la partition recherchée de la manière
suivante :
Q1(S,X) =
L∑
l=1
N∑
i=1
contQ1(i,Cl) (14)
où
contQ1(i,Cl) =
1
2|E|
∑
i′∈Cl
(sii′ − si.si
′.
2|E| ) (15)
En utilisant les transformations : sii′ =< Ki,Ki′ > et si. =
∑
i′′ < Ki,Ki′′ > (où
Ki désigne la ieme ligne du tableau disjonctif complet K), l’expression de la formule de
contribution devient 1,
contQ1(i,Cl) =
1
2|E|
∑
i′∈Cl
(< Ki,Ki′ >
−
∑
i′′ < Ki,Ki′′ >
∑
i′′ < Ki′ ,Ki′′ >
2|E| ) (16)
=
1
2|E| < Ki,Pl > −
∑
i′∈Cl
δii′ (17)
où
Pl =
∑
i′∈Cl
Ki′ (18)
et
δii′ =
∑
i′′ < Ki,Ki′′ >
∑
i′′ < Ki′ ,Ki′′ >
2|E| (19)
De la même façon la contribution contQ2(i,Cl) peut être réécrite ;
1. Rappelons que cette nouvelle écriture de la formule de contribution permet de réduire considérablement le coût
de calcul lié à la matrice de similarité S et de caractériser chaque classe Cl avec son prototype Pl.
L. Labiod et al.
contQ2(i,Cl) =
M
H
∑
i′∈Cl
(< Ki,Ki′ >
−
∑M
m=1
∑
i′′ < K
m
i ,K
m
i′′ >
∑
i′′ < K
m
i′ ,K
m
i′′ >
Hii′ )
=
M
H (< Ki,Pl > −
∑
i′∈Cl
δ˜ii′) (20)
où
δ˜ii′ =
∑M
m=1
∑
i′′ < K
m
i ,K
m
i′′ >
∑
i′′ < K
m
i′ ,K
m
i′′ >
Hii′ (21)
La nouvelle formule de contribution introduit une pondération automatique, la valeur de la
nouvelle formule de contribution sera soit supérieure, inférieure ou bien égale à la contribution
de l’AR en fonction des poids (δii′ ou δ˜ii′). Les formules de contributions contQ1 et contQ2
peuvent être écrites en terme de contribution contAR par l’ajout d’un terme de pondération en
fonction du profil de chaque paire d’objets (i,i′) :
contQ1(i,Cl) =
1
2|E| [(< Ki,Pl > −
∑
i′∈Cl
mii′)
+
∑
i′∈Cl
(mii′ − δii′)] (22)
=
1
2|E| [contAR(i,Cl) +
∑
i′∈Cl
(mii′ − δii′)] (23)
Remarque : De la même manière, la contribution contQ2 peut être réécrite en fonction de la
contribution de l’AR, contAR =< Ki,Pl > −
∑
i′∈Cl mii′ .
Le changement dans la formule de contribution est intéressant car il introduit une pondé-
ration relative aux profils des objets de manière automatique sans nécessiter la présence d’un
expert. On distingue trois scénarios :
1. Prenant δii′ = mii′ , ∀i,i′, nous trouvons ainsi le cas de l’algorithme de l’AR
2. Si le poids δii′ est inférieure à mii′ , ∀i,i′, alors la valeur de contribution contQ1 est
supérieure à l’ancienne contribution contAR, et elle a donc plus de chance d’être positive
que contAR; l’observation i se trouvera alors affectée à une classe pré-existante. Ainsi,
le nombre de classes sera petit.
3. Si le poid δii′ est supérieur à mii′ , ∀i,i′, alors la valeur de la contribution contQ1 est
inférieure à l’ancienne contAR, et elle a donc plus de chance d’être négative que contAR;
l’observation i est alors affectée à une nouvelle classe. Ainsi, le nombre de classes sera
plus important.
Formulation Condorcéenne du critère de la modularité
5.2 Algorithme de l’Analyse Relationnelle
Le processus consiste à partir d’une classe initiale (une classe singleton) à construire de
façon incrémentale une partition de l’ensemble I en accentuant à chaque affectation la valeur
du critère de la modularité. Nous donnons ci-dessous la description de l’algorithme d’analyse
relationnelle qui a été utilisé par la méthodologie de l’analyse relationnelle (voir (Marcotor-
chino, 1978) pour plus de détails)
Algorithme1: Algorithme de l’AR
Inputs
Initialisation : Niter= nombre d’itérations. N= nombre d’individus (observations). Lmax =
N le nombre maximal de classes.
- Claculer les matrices des valeurs seuils (M,δ où δ˜)
- Prendre le premier individu comme étant le premier élément de la classe C1
- l = 1, où l est le nombre de classes
for t=1 to Niter do
for i = 1 to N do{Affectation}
for k = 1 to l do
Calculer la contribution cont(Ki,Pk)
end for
k∗ = argmaxk cont(Ki,Pk)
cont(Ki,Pk∗)← la contribution calculée
if cont(Ki,Pk∗) < 0 and l < Lmax then
Créer une nouvelle classe dont i est le premier individu affecté
à cette classe
l = l + 1
else
Affecter i à Ck∗
endif
endfor
endfor
Ouputs : au plus une parition de Lmax classes.
Nous devons produire un certain nombre d’itérations afin d’obtenir une solution approchée
dans un temps de traitement raisonnable. D’ailleurs, il est exigé un nombre maximal de classes
mais puisque nous n’avons pas besoin de fixer ce paramètre, nous avons pris Lmax = N par
défaut. Fondamentalement le coût de calcul de cet algorithme est en O(Niter × Lmax ×N) .
En général, on peut supposer queNiter << N , mais pas Lmax << N . Ainsi, dans le pire des
cas, l’algorithme a une complexité en O(Lmax ×N).
6 Expérimentation et validation
Afin de pouvoir évaluer la qualité de la classification obtenue, nous avons utilisé des bases
de données qualitatives UCI (Asuncion et Newman, 2007) comportant un nombre variable
d’observations (voir TAB. 1). Nous avons utilisé le taux de bonne classification (appelé aussi
L. Labiod et al.
pureté), les indices de Rand, de Jaccard et de Tanimoto en utilisant la classe connue de chaque
observation. L’évaluation de la pureté ou de la qualité d’une partition obtenue consiste à évaluer
si la partition résultat est cohérente par rapport à la connaissance disponible.
6.1 Mesures de performances
6.1.1 Indice de Pureté
Considérons L clusters de l’ensemble de données V et soit |Cl| la taille du cluster Cl .
La pureté de ce cluster est donnée par l’expression Pureté(Cl) = 1|Cl| maxk(|Cl|cluster=k) où
|Cl|cluster=k désigne le nombre d’objets de la classe k attribué au cluster l. La pureté globale
d’une partition résultat peut être exprimée comme une somme pondérée des puretés indivi-
duelles des clusters.
Pureté =
L∑
l=1
|Cl|
|V |Pureté(Cl) (24)
En général, plus la valeur de pureté est élevée, meilleure est la partition obtenue.
6.1.2 Indice de Rand (RI)
Indice de Rand (Rand, 1971) : mesure le nombre d’accords par paires entre une partition
obtenue U ′ et la vraie partition U d’un même ensemble d’objets, normalisé de sorte que la
valeur se situe entre 0 et 1 :
RI(U,U ′) =
a+ b
a+ b+ c+ d
(25)
Où a désigne le nombre de paires d’objets appartenant à la même classe de U et affectés au
même cluster de U ′, b désigne le nombre de paires dont les objets appartiennent à deux classes
différentes de U et à deux clusters différents de U ′, c désigne le nombre de paires d’objets
appartenants à la même classe de U et à deux clusters différents de U ′, et d désigne le nombre
de paires dont les objets appartiennent à deux classes différentes de U et affectées au même
cluster de U ′. Cet indice donne un résultat dans l’intervalle [0,1], où une valeur de 1 indique
que U et U ′ sont identiques.
6.1.3 Indice de jaccard(JI)
Indice de Jaccard (Jaccard, 1912) a été couramment utilisé pour évaluer la similarité entre
différentes partitions du même ensemble de données, le niveau d’accord entre la vraie partition
U et une partition résultat U ′ est déterminé par le nombre de paires de points attribués à une
même classe dans les deux partitions :
JI(U,U ′) =
a
a+ d+ c
(26)
L’indice de Jaccard donne un résultat dans l’intervalle [0,1], où une valeur de 1 indique que
U et U ′ sont identiques.
Formulation Condorcéenne du critère de la modularité
6.1.4 Indice de Tanimoto (TI)
La similarité entre différentes partitions d’un ensemble de données peut être mesurée par
le ratio de leurs éléments communs au nombre de tous les différents éléments,
TI(U,U ′) =
1
2 (a+ b)
1
2 (a+ b) + d+ c
(27)
Cet indice donne un résultat dans l’intervalle [0,1].
6.2 Bases de données pour la validation
Dans cette section, nous évaluons la performance de l’heuristique proposée sur plusieures
bases de données disponibles à l’UCI. La description des bases de données utilisées est donnée
dans TAB. 1 :
TAB. 1 – – Description des bases de données
Bases de données # d’objets # d’attributs # de classes
Soybean small 47 21 4
Zoo 101 16 7
Soybean large 307 35 19
SPECTF 267 22 2
Post-Operative 90 8 3
Balance Scale 625 4 3
Audiology Normalized 226 69 24
6.3 Résultats dans le cas d’une integration a priori
La méthode proposée est testée sur des bases de données obtenues à partir du référentiel de
données UCI. Comme la méthode proposée est une modification de l’approche de l’AR, nous
avons comparé les performances de l’algorithme proposé avec l’algorithme de l’AR. De la
table TAB. 2 et FIG. 3 , il est clair que la performance de la méthode proposée qui repose sur la
mesure modularité étendue est meilleure que l’approche AR pour toutes les bases de données.
Cela signifie que le système de pondération introduit améliore les résultats de la pureté du
clustering.
Afin de montrer la bonne performance de l’approche proposée, nous utilisons plusieures
bases de données catégorielles de différentes tailles et nous indiquons dans TAB. 4 les valeurs
des indices RI, JI et l’indice TI obtenus en utilisant le critère classique de l’AR et dans TAB.
3 les indices RI, JI et l’indice TI en utilisant la mesure de modularité étendue. Les résultats
montrent que l’approche proposée augmente la valeur des indices par rapport à l’AR classique
et permet d’introduire un système de pondération automatique relatif au profil de chaque objet
dans la base de données.
L. Labiod et al.
TAB. 2 – – Mesures de Pureté pourRAR(S,X) et Q1(S,X)
BD Taille RAR(S,X) Q1(S,X)
Soybean small 47x21 78 % 100 %
Zoo 101x16 83.17% 88.12 %
Soybean large 307x35 70 % 72.31 %
SPECTF 267x22 61.25 % 85 %
Post-Operative 90x8 71.11 % 73.33%%
Balance Scale 625x4 63.52 % 63.52 %
Audiology Normalized 226x69 50.50 % 58 %
0
0,2
0,4
0,6
0,8
1
1,2
So
yb
ea
n
sm
all Zo
o
So
yb
ea
n 
lar
ge
Sp
ec
tf
Po
st 
op
er
at
ive
Ba
lan
ce
sc
ale
Au
dil
og
y
Dataset
Pu
rit
y
RA
Q1
x
FIG. 3 – – Mesures de Pureté sur différentes bases de données
TAB. 3 – – Résultats sur différentes bases de données en utilisant Q1(S,X)
BD Taille RI JI TI
Soybean small 47x21 100 % 100 % 100 %
Zoo 101x16 94.2% 79.9 % 89.2 %
Soybean large 307x35 91.2 % 26.6 % 83.9 %
SPECTF 267x22 60.98 % 38.28 % 43.86 %
Post-Operative 90x8 50.75% 37.37% % 34.01%
Balance Scale 625x4 58 % 20 % 40 %
Audiology Normalized 226x69 82 % 20 % 69%
Formulation Condorcéenne du critère de la modularité
TAB. 4 – – Résultats sur différentes bases de données en utilisantRAR(S,X)
BD Taille RI JI TI
Soybean small 47x21 86.66 % 45.88 % 76.47 %
Zoo 101x16 72.9% 46.09 % 57.37 %
Soybean large 307x35 85.03 % 25.7 % 73.97 %
SPECTF 267x22 55.74 % 38.69 % 38.64 %
Post-Operative 90x8 54.44% 41.17% % 37.4%
Balance Scale 625x4 57 % 19 % 39 %
Audiology Normalized 226x69 82 % 20 % 69 %
7 Conclusions et perspectives
Dans ce papier, nous avons étudié deux extensions du critère de modularité pour la clas-
sification des données catégorielles et illustrer ses relations avec le critère de Condorcet. Une
procédure itérative efficace d’optimisation est présentée. Les résultats expérimentaux montrent
l’efficacité de la méthode de l’intégration a priori proposée par rapport à l’approche de l’ana-
lyse relationnelle classique. La validation de la démarche d’intégration intermédiaire sera éta-
blie dans des travaux futurs, une autre idée est d’adapter ces extensions pour la théorie des
graphes en utilisant les matrices d’adjacence.
Références
Agarwal, G. and Kempe, D. (2008). Modularity-maximizing graph communities via mathema-
tical programming. The European Physical Journal B 66:33, 409-418.
Asuncion, A. Newman, D.J. (2007). ”UCI Machine Learning Repository
[http://www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine”, CA: University of Califor-
nia, School of Information and Computer Science.
Barbara, D., Couto, J., Li, Y. (2002). COOLCAT: an entropy-based algorithm for categorical
clustering. Proceedings of the Eleventh ACM CIKM Conference (pp. 582-589).
Bock, H.-H. (1989). Probabilistic aspects in cluster analysis. In O. Opitz (Ed.), Conceptual and
numerical analysis of data, 12-44. Berlin: Springer-verlag .
Celeux, G., Govaert, G. (1991). Clustering criteria for discrete data and latent class models.
Journal of Classification, 8, 157-176.
Ganti, V., Gehrke, J., Ramakrishnan, R. (1999). CACTUS - clustering categorical data using
summaries. Proceedings of the Fifth ACM SIGKDD Conference (pp. 73- 83).
Gibson, D., Kleinberg, J., Raghavan, P. (1998). Clustering categorical data: An approach based
on dynamical systems. Proceedings of the 24rd VLDB Conference (pp. 311-322).
Guha, S., Rastogi, R., Shim, K. (2000). ROCK: A robust clustering algorithm for categorical
attributes. Information Systems, 25, 345-366.
L. Labiod et al.
Gyllenberg, M., Koski, T., Verlaan, M. (1997). Classification of binary vectors by stochastic
complexity. Journal of Multivariate Analysis, 47-72.
Huang, Z. (1998). Extensions to the k-means algorithm for clustering large data sets with
categorical values. Data Mining and Knowledge Discovery, 2, 283-304.
Li, T., Zhu, S., Ogihara, M. (2003). Efficient multi-way text categorization via generalized
discriminant analysis. Proceedings of Twelfth ACM CIKM Conference (pp. 317-324).
Marcotorchino, J. F. (2006). Relational analysis theory as a general approach to data analysis
and data fusion, in Cognitive Systems with interactive sensors, 2006.
Marcotorchino, J. F. and Michaud, P. Optimisation en analyse ordinale des données. (In Mas-
son, 1978.)
Newman, M. and Girvan, M.(2004). Finding and evaluating community structure in networks.
Physical Review E, 69, 026113.
Li, T. Ma, S., and Ogihara, M. (2004). Entropy-based criterion in categorical clustering. Pro-
ceedings of The 2004 IEEE International Conference on Machine Learning (ICML 2004).
536-543.
White, S. and Smyth, P. (2005). A spectral clustering approach to finding communities in
graphs. In SDM, pages 76-84.
Zhao, Y., Karypis, G. (2001). Criterion functions for document clustering: Experiments and
analysis (Technical Report). Department of Computer Science, University of Minnesota.
Summary
This paper studies the extension of the Modularity measure for categorical data clustering.
It first shows the relational data presentation and establishes the relationship between the ex-
tended Modularity and the Relational Analysis criterion. Two extensions are presented in this
work: the early integration and the intermediate integration approaches. The proposed Mod-
ularity measure introduces an automatic weighting scheme which takes in consideration the
profile of each data object. An iterative algorithm is then presented to search for the partitions
maximizing this criterion. This algorithm deals linearly with large data sets and allows natural
clusters identification, i.e. doesn’t require fixing the number of clusters and the size of each
cluster. For the early integration approach, several experiments are conducted in order to show
the effectiveness of the proposed approach.

