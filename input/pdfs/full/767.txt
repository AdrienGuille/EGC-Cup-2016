Chapitre 7 : Arbre de dÃ©cision pour donnÃ©es
dÃ©sÃ©quilibrÃ©es : sur la complÃ©mentaritÃ© de lâ€™intensitÃ©
dâ€™implication et de lâ€™entropie dÃ©centrÃ©e
Gilbert Ritschardâˆ—, Simon Marcellinâˆ—âˆ—, Djamel A. Zighedâˆ—âˆ—
âˆ—DÃ©partement dâ€™Ã©conomÃ©trie, UniversitÃ© de GenÃ¨ve
âˆ—âˆ—Laboratoire ERIC, UniversitÃ© de Lyon 2
gilbert.ritschard@unige.ch, {abdelkader.zighed,simon.marcellin}@univ-lyon2.fr
http://mephisto.unige.ch, http://eric.univ-lyon2.fr
RÃ©sumÃ©. Cet article porte sur lâ€™induction dâ€™arbres de classification pour des
donnÃ©es dÃ©sÃ©quilibrÃ©es, câ€™est-Ã -dire lorsque certaines catÃ©gories de la variable Ã 
prÃ©dire sont beaucoup plus rares que dâ€™autres. Plus particuliÃ¨rement nous nous
intÃ©ressons Ã  deux aspects: dâ€™une part, Ã  dÃ©finir des critÃ¨res de construction
de lâ€™arbre qui exploitent efficacement la nature dÃ©sÃ©quilibrÃ©e des donnÃ©es, et
dâ€™autre part la pertinence de la conclusion Ã  associer aux feuilles de lâ€™arbre.
Nous avons rÃ©cemment abordÃ© cette problÃ©matique sous deux angles indÃ©pen-
dants: lâ€™un Ã©tait axÃ© sur le recours Ã  des entropies dÃ©centrÃ©es, lâ€™autre sâ€™appuyant
sur des mesures dâ€™intensitÃ©s dâ€™implication issues de lâ€™ASI. Nous nous propo-
sons ici de comparer et dâ€™Ã©tablir les similaritÃ©s entre ces deux approches. Une
premiÃ¨re expÃ©rimentation sommaire est prÃ©sentÃ©e.
1 Introduction
Quâ€™il sâ€™agisse dâ€™induire un arbre, ou dâ€™associer une conclusion Ã  chacune de ses feuilles,
les critÃ¨res utilisÃ©s supposent en gÃ©nÃ©ral implicitement une importance Ã©gale des modalitÃ©s
de la variable Ã  prÃ©dire. Ainsi, des algorithmes comme CART (Breiman et al., 1984) ou C4.5
(Quinlan, 1993) utilisent comme critÃ¨re lâ€™amÃ©lioration dâ€™une entropie classique, câ€™est-Ã -dire
centrÃ©e sur la distribution uniforme correspondant Ã  lâ€™Ã©quiprobabilitÃ© des modalitÃ©s. Le rÃ©sultat
est quâ€™on obtient ainsi des segmentations en classes dont les distributions tendent Ã  sâ€™Ã©carter le
plus possible de la distribution uniforme. De mÃªme pour le choix de la conclusion, le critÃ¨re
communÃ©ment utilisÃ© est simplement la rÃ¨gle majoritaire qui nâ€™a Ã©videmment de sens que
si chaque modalitÃ© a la mÃªme importance. On le voit donc, cette distribution Ã©galitaire des
modalitÃ©s joue le rÃ´le de situation la moins dÃ©sirable. Mais est-ce vraiment le cas ? Et sinon, de
quelles solutions dispose-t-on pour dâ€™une part favoriser les Ã©carts Ã  une distribution non centrÃ©e
â€” reprÃ©sentative de la situation la moins dÃ©sirable â€” et dâ€™autre part choisir la conclusion la
plus pertinente par rapport Ã  cette rÃ©fÃ©rence la moins dÃ©sirable ?
Une premiÃ¨re solution nous est fournie par lâ€™indice dâ€™implication dont nous avons montrÃ©
dans Ritschard (2005) et Pisetta et al. (2007) comment il pouvait sâ€™utiliser avec les arbres de
dÃ©cision. En effet, cet indice est en fait un rÃ©sidu, soit un Ã©cart par rapport Ã  lâ€™indÃ©pendance qui
- 207 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
est caractÃ©risÃ©e dans les arbres par la distribution au nÅ“ud initial. Ainsi au lieu de mesurer
des Ã©carts par rapport Ã  la distribution uniforme, on mesure des Ã©carts par rapport Ã  cette
distribution initiale. Rien nâ€™empÃªche cependant de considÃ©rer des rÃ©sidus par rapport Ã  dâ€™autres
distributions. Voir Ã  ce sujet lâ€™indice dâ€™Ã©cart Ã  lâ€™Ã©quilibre de Blanchard et al. (2005) et sa
gÃ©nÃ©ralisation dans Lallich et al. (2005). Une seconde solution consiste Ã  utiliser des entropies
dÃ©centrÃ©es (Marcellin et al., 2006; Zighed et al., 2007; Lenca et al., 2008) qui gÃ©nÃ©ralisent les
entropies classiques en les paramÃ©trant par le point oÃ¹ elles prennent leur maximum, laissant
ainsi Ã  lâ€™utilisateur la possibilitÃ© de dÃ©terminer le point dâ€™incertitude maximale.
Nous nous proposons dans ce papier de comparer ces deux approches en discutant leurs
avantages respectifs comme critÃ¨re de construction de lâ€™arbre ainsi que comme critÃ¨re de choix
de la conclusion des rÃ¨gles. Notre discussion nous amÃ¨nera Ã  proposer une solution hybride
oÃ¹ lâ€™on utilise lâ€™entropie dÃ©centrÃ©e pour induire lâ€™arbre, et lâ€™indice dâ€™implication pour assigner
une dÃ©cision Ã  chaque feuille.
Lâ€™article est organisÃ© comme suit. La section 2 pose le cadre formel. Dans La section 3
nous introduisons un jeu de donnÃ©es qui nous servira dâ€™illustration et rappelons le principe
des arbres de dÃ©cision. A la section 4 nous rappelons les dÃ©finitions introduites dans Ritschard
(2005) sur la notion dâ€™indice dâ€™implication dans le contexte des arbres de dÃ©cision et examinons
la possibilitÃ© de lâ€™utiliser comme critÃ¨re dâ€™optimalitÃ© pour les Ã©clatements successifs lors de
la construction de lâ€™arbre. Nous rappelons aussi son intÃ©rÃªt pour lâ€™attribution de la conclusion
aux feuilles de lâ€™arbre. La section 5 quant Ã  elle rappelle la forme de lâ€™entropie dÃ©centrÃ©e
introduite dans Marcellin et al. (2006) et Zighed et al. (2007) et commente son usage, en
particulier comme critÃ¨re de dÃ©veloppement de lâ€™arbre. La discussion comparative fait lâ€™objet
de la section 6 tandis que la section 7 Ã©claire le propos avec des rÃ©sultats dâ€™expÃ©rimentations.
Enfin nous concluons Ã  la section 8.
2 Cadre formel et notations
On se place dans un cadre supervisÃ© oÃ¹ disposant dâ€™une variable dÃ©pendante y, dite aussi
variable rÃ©ponse ou Ã  prÃ©dire, on cherche Ã  caractÃ©riser une fonction f(x1, x2, . . .)â€” un arbre
de dÃ©cision dans notre cas â€” qui permette de prÃ©dire y Ã  partir dâ€™un ensemble x1, x2, . . .
de variables explicatives (prÃ©dicteurs) catÃ©gorielles, ordinales ou quantitatives. On sâ€™intÃ©resse
ici au cas oÃ¹ la variable rÃ©ponse est catÃ©gorielle avec ` modalitÃ©s y1, . . . , y`. Par exemple,
sâ€™agissant de diagnostiquer un cancer on aura y1 = â€˜a le cancerâ€™ et y2 = â€˜pas de cancerâ€™.
Notre propos concerne cependant plus particuliÃ¨rement les situations oÃ¹ la rÃ©ponse prend plus
de 2 modalitÃ©s, ce qui est par exemple le cas si lâ€™on retient une catÃ©gorie y3 = â€˜requiert une
analyse supplÃ©mentaireâ€™ en plus des deux classes prÃ©cÃ©dentes.
Comme y est catÃ©gorielle, la prÃ©diction de sa modalitÃ© est une classification. On assigne un
cas j Ã  la classe (modalitÃ©) de y que lâ€™on prÃ©dit Ã  partir des valeurs xj1, xj2, . . . que prennent
les prÃ©dicteurs pour ce cas j.
3 DonnÃ©es illustratives et principe des arbres de dÃ©cision
Pour illustrer notre propos, nous reprenons les donnÃ©es fictives utilisÃ©es dans Ritschard
(2005) et rÃ©capitulÃ©es au tableau 1. La variable Ã  prÃ©dire est lâ€™Ã©tat civil, le sexe et le secteur
- 208 -RNTI-E-16
G. Ritschard et al.
  
  
   
      
 	 

  
  
      
  
 	 
  
      
  

  
  
     
 	 
  
  
      
   
            
         
        
         
         
       	 
         
  
  
  

     

  
  
  
     

       	 
         
          
               
    !
" !   #      
$  %    ! & '   (
   
 
     #
FIG. 1 â€“ Arbre induit. Feuilles F1, F2, F3 avec indice implication, F1, F2, F3a, F3b avec
entropie dÃ©centrÃ©e.
dâ€™activitÃ© Ã©tant les prÃ©dicteurs disponibles.
homme femme
Ã©tat civil primaire secondaire tertiaire primaire secondaire tertiaire total
mariÃ© 50 40 6 0 14 10 120
cÃ©libataire 5 5 12 50 30 18 120
divorcÃ©/veuf 5 8 10 6 2 2 33
total 60 53 28 56 46 30 273
TAB. 1 â€“ DonnÃ©es illustratives.
Les arbres de classification sont des outils supervisÃ©s. Ils dÃ©terminent des rÃ¨gles de classi-
fication en deux temps. Dans une premiÃ¨re Ã©tape, une partition de lâ€™espace des prÃ©dicteurs (x)
est dÃ©terminÃ©e telle que la distribution de la variable (discrÃ¨te) Ã  prÃ©dire (y, lâ€™Ã©tat civil dans
notre exemple) diffÃ¨re le plus possible dâ€™une classe Ã  lâ€™autre de la partition. La partition se fait
successivement selon les valeurs des prÃ©dicteurs. On commence par partitionner les donnÃ©es
selon les modalitÃ©s de lâ€™attribut le plus discriminant, puis on rÃ©pÃ¨te lâ€™opÃ©ration localement sur
chaque nÅ“ud ainsi obtenu jusquâ€™Ã  la rÃ©alisation dâ€™un critÃ¨re dâ€™arrÃªt. Dans un second temps,
aprÃ¨s que lâ€™arbre ait Ã©tÃ© gÃ©nÃ©rÃ©, on dÃ©rive les rÃ¨gles de classification en choisissant la valeur
de la variable Ã  prÃ©dire la plus pertinente dans chaque feuille (nÅ“ud terminal) de lâ€™arbre. On
retient classiquement pour cela la valeur la plus frÃ©quente, mais nous reviendrons prÃ©cisÃ©ment
sur ce point.
Pratiquement, on relÃ¨ve dans chaque feuille j, j = 1, . . . , q, le nombre nij de cas qui sont
- 209 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
dans lâ€™Ã©tat yi. Ainsi, on peut rÃ©capituler les distributions au sein des feuilles sous forme dâ€™une
table de contingence croisant les Ã©tats de la variable y avec les feuilles (Tableau 2). On peut
noter que la marge de droite de ce tableau qui donne le total niÂ· des lignes correspond en fait Ã 
la distribution des cas dans le nÅ“ud initial de lâ€™arbre. Les nÂ·j dÃ©signent les totaux des colonnes.
feuille 1 Â· Â· Â· feuille j Â· Â· Â· feuille q Total
y1 n1Â·
...
...
yi nij niÂ·
...
...
y` n`Â·
Total nÂ·1 Â· Â· Â· nÂ·j Â· Â· Â· nÂ·q n
TAB. 2 â€“ Table de contingence croisant les Ã©tats de la rÃ©ponse y avec les feuilles de lâ€™arbre.
4 Indice dâ€™implication
Lâ€™indice dâ€™implication (voir par exemple Gras et al., 2004, p. 19) dâ€™une rÃ¨gle se dÃ©finit
Ã  partir des contre-exemples. Dans le cas des arbres de classification il sâ€™agit dans chaque
feuille (colonne du tableau 2) du nombre de cas qui ne sont pas dans la catÃ©gorie qui lui a
Ã©tÃ© attribuÃ©e. Ces cas vÃ©rifient en effet la prÃ©misse de la rÃ¨gle, mais pas sa conclusion. En
notant b la conclusion (ligne du tableau) 1 de la rÃ¨gle j et nbj le nombre de cas qui vÃ©rifient
cette conclusion dans la jÃ¨me colonne, le nombre de contre-exemples est nbÂ¯j = nÂ·j âˆ’ nbj .
Lâ€™indice dâ€™implication est une forme standardisÃ©e de lâ€™Ã©cart entre ce nombre et le nombre
espÃ©rÃ© de contre-exemples qui seraient gÃ©nÃ©rÃ©s en cas de rÃ©partition entre valeurs de la rÃ©ponse
indÃ©pendante de la condition de la rÃ¨gle.
Formellement, lâ€™hypothÃ¨se de rÃ©partition indÃ©pendante de la condition, que nous notons
H0, postule que le nombre NbÂ¯j de contre-exemples de la rÃ¨gle j rÃ©sulte du tirage alÃ©atoire
et indÃ©pendant dâ€™un groupe de nÂ·j cas vÃ©rifiant la prÃ©misse de la rÃ¨gle j et dâ€™un autre de
nbÂ¯Â· = nâˆ’nbÂ· cas qui ne vÃ©rifient pas la conclusion de la rÃ¨gle. SousH0 et conditionnellement
Ã  nbÂ· et nÂ·j , le nombre alÃ©atoireNbÂ¯j de contre-exemples est rÃ©putÃ© (Lerman et al., 1981) suivre
une loi de Poisson de paramÃ¨tre ne
bÂ¯j
= nbÂ¯Â·nÂ·j . Ce paramÃ¨tre nebÂ¯j est donc Ã  la fois lâ€™espÃ©rance
mathÃ©matique et la variance du nombre de contre-exemples sousH0. Il correspond au nombre
de cas de la feuille j qui seraient des contre-exemples si lâ€™on rÃ©partissait les nÂ·j cas de j selon
la distribution marginale, celle du nÅ“ud initial de lâ€™arbre (ou marge de droite du tableau 2).
Lâ€™indice dâ€™implication de Gras est lâ€™Ã©cart nbÂ¯j âˆ’ nebÂ¯j entre les nombres de contre-exemples
observÃ©s et attendus sous lâ€™hypothÃ¨se H0, standardisÃ© par lâ€™Ã©cart type, soit, en ajoutant la
correction pour la continuitÃ© en vue de la comparaison avec la loi normale
Imp(j) =
nbÂ¯j âˆ’ nebÂ¯j + .5âˆš
ne
bÂ¯j
(1)
1. Notons que b peut Ã©videmment varier dâ€™une colonne Ã  lâ€™autre.
- 210 -RNTI-E-16
G. Ritschard et al.
En termes de cas vÃ©rifiant la condition, cet indice sâ€™Ã©crit encore
Imp(j) =
âˆ’(nbj âˆ’ nebj) + .5âˆš
nÂ·j âˆ’ nebj
(2)
Une valeur positive de lâ€™indice indique que la rÃ¨gle fait moins bien que le hasard et nâ€™ap-
porte donc aucune information implicative. Seules les valeurs nÃ©gatives ont donc un intÃ©rÃªt.
Plus lâ€™indice â€” lâ€™Ã©cart par rapport au hasard â€” est grand (en valeur absolue), plus la force
implicative de la rÃ¨gle est forte.
Dans Ritschard (2005), nous avons proposÃ© des variantes inspirÃ©es des rÃ©sidus utilisÃ©s en
modÃ©lisation de tables de contingence multidimensionnelles. Il sâ€™agit du rÃ©sidu dÃ©viance, du
rÃ©sidu ajustÃ© dâ€™Haberman et du rÃ©sidu de Freeman-Tukey qui ont une variance plus proche de
1 que le rÃ©sidu standardisÃ© utilisÃ© par Gras de variance plus petite. Le premier a cependant
un comportement tendant vers 0 quand le nombre de contre-exemples sâ€™approche de 0 qui le
disqualifie (Pisetta et al., 2007). Les deux autres Ã©voluent de faÃ§on similaire Ã  lâ€™indice de Gras
tout au moins du point de vue qui nous intÃ©resse ici de lâ€™ordre de prÃ©fÃ©rence des conclusions
que suggÃ¨rent les valeurs de lâ€™indice. Nous nous contentons donc ci-aprÃ¨s de discuter lâ€™usage
de lâ€™indice de Gras.
4.1 Gain dâ€™implication comme critÃ¨re dâ€™optimalitÃ© des Ã©clatements
Lâ€™indice permet de mesurer la force implicative de la rÃ¨gle. On peut alors songer Ã  lâ€™ex-
ploiter comme critÃ¨re de dÃ©veloppement de lâ€™arbre. Lâ€™idÃ©e est de rechercher Ã  chaque nÅ“ud
lâ€™Ã©clatement qui produirait le meilleur gain en termes de force implicative des rÃ¨gles, en ad-
mettant Ã©videmment quâ€™on retienne Ã  chaque nÅ“ud la conclusion qui maximise lâ€™intensitÃ©
dâ€™implication. On se heurte cependant ici Ã  une difficultÃ© dâ€™agrÃ©gation. En effet, sâ€™il est aisÃ© de
calculer lâ€™indice dâ€™implication avant lâ€™Ã©clatement, on se retrouve aprÃ¨s lâ€™Ã©clatement avec plu-
sieurs nÅ“uds et donc un ensemble de valeurs dâ€™indices dâ€™implication quâ€™il nous faut synthÃ©tiser
en une seule valeur qui puisse Ãªtre comparÃ©e avec lâ€™indice dâ€™implication avant lâ€™Ã©clatement.
Une possibilitÃ© est de prendre simplement une moyenne pondÃ©rÃ©e par les effectifs des nÅ“uds
concernÃ©s. Une autre solution, qui ferait sens si lâ€™on est intÃ©ressÃ© en prioritÃ© Ã  obtenir quelques
rÃ¨gles trÃ¨s fortes tout en sâ€™accommodant de rÃ¨gles peu implicatives, est de retenir le maximum
des intensitÃ©s obtenues. Pour rester dans la logique de lâ€™indice dâ€™implication, une troisiÃ¨me
solution dâ€™indice dâ€™implication pour lâ€™ensemble S de sommets rÃ©sultant de lâ€™Ã©clatement est
(en incluant la correction pour la continuitÃ©)
ImpT(S) =
âˆ‘
jâˆˆS nbÂ¯j âˆ’
âˆ‘
jâˆˆS n
e
bÂ¯j
+ .5âˆšâˆ‘
jâˆˆS n
e
bÂ¯j
=
âˆ‘
jâˆˆS(nbÂ¯j âˆ’ nebÂ¯j) + .5âˆšâˆ‘
jâˆˆS n
e
bÂ¯j
(3)
soit lâ€™Ã©cart standardisÃ© entre le nombre total de contre-exemples observÃ©s des rÃ¨gles et le total
attendu.
Pour notre exemple, nous donnons au tableau 3 le gain de force implicative apportÃ© par les
diffÃ©rents Ã©clatements possibles au premier niveau. Le gain est la diffÃ©rence entre la valeur de
lâ€™indice au nÅ“ud que lâ€™on veut Ã©clater (soit 0 au nÅ“ud initial) et lâ€™indice synthÃ©tique pour les
nÅ“uds rÃ©sultant de lâ€™Ã©clatement. Le sexe sâ€™impose clairement comme meilleur attribut prÃ©dic-
tif. Il est intÃ©ressant de relever que le gain mesurÃ© avec lâ€™indice total est en rÃ¨gle gÃ©nÃ©rale plus
fort que lâ€™Ã©cart par rapport au maximum.
- 211 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
Attribut utilisÃ© nbre sommets moyenne pondÃ©rÃ©e maximum ImpT
sexe 2 4.17 4.59 5.94
secteur 3 0.82 1.34 1.50
primaire 2 0.31 0.44 0.46
tertiaire 2 0.79 0.82 1.09
TAB. 3 â€“ Gains de force implicative pour les Ã©clatements possibles au premier niveau.
Attribut utilisÃ© nbre sommets moyenne pondÃ©rÃ©e maximum ImpT
Sommet : Homme
secteur 3 -0.71 0.22 1.18
primaire 2 -1.30 -0.10 0
tertiaire 2 0.48 1.23 1.18
Sommet : Femme
secteur 3 -1.83 -0.15 0
primaire 2 -1.46 -0.15 0
tertiaire 2 -0.81 -0.01 0
TAB. 4 â€“ Gains de force implicative pour les Ã©clatements possibles au deuxiÃ¨me niveau.
On procÃ¨de donc Ã  lâ€™Ã©clatement selon le sexe, et lâ€™on donne au tableau 4 les gains pos-
sibles au niveau 2 pour chacun des sommets â€œHommeâ€ et â€œFemmeâ€. Pour les femmes, aucun
gain de force implicative nâ€™est possible avec la seule variable qui nous reste Ã  savoir le sec-
teur dâ€™activitÃ©. La raison en est simplement que quelque soit lâ€™Ã©clatement, la catÃ©gorie pour
laquelle on a lâ€™implication la plus forte reste la mÃªme (cÃ©libataire) dans tous les nÅ“uds quâ€™on
obtient. Pour les hommes, il en est de mÃªme si lâ€™on segmente entre le secteur primaire et le
reste. Par contre, une segmentation en deux, tertiaire contre le reste ou en trois, permet un
gain Ã©gal en termes dâ€™implication totale. Le partage en deux paraÃ®t cependant plus intÃ©ressant
puisquâ€™il se traduit, contrairement Ã  lâ€™Ã©clatement en 3, par un gain positif Ã©galement en termes
dâ€™implication moyenne.
4.2 Choix de la conclusion des rÃ¨gles
Chaque feuille (nÅ“ud terminal) de lâ€™arbre caractÃ©rise une rÃ¨gle dont la prÃ©misse est dÃ©finie
par les conditions dâ€™embranchement le long du chemin menant du nÅ“ud initial Ã  la feuille,
la conclusion de la rÃ¨gle correspondant Ã  la modalitÃ© assignÃ©e Ã  la feuille. Comme dÃ©jÃ  men-
tionnÃ©, le choix se porte de faÃ§on classique sur la modalitÃ© la plus frÃ©quente. Dans certaines
circonstances, il est plus pertinent de retenir la modalitÃ© assurant la plus forte implication. Il
en est en particulier ainsi dans le contexte du ciblage oÃ¹ il sâ€™agit de dÃ©terminer les profils types
de chaque modalitÃ© de la variable cible y, et non pas, comme en classification, de prÃ©voir la
modalitÃ© que prendra un individu avec un profil donnÃ©.
Notons que lâ€™usage de lâ€™indice dâ€™implication pour le dÃ©veloppement de lâ€™arbre suppose
implicitement que la conclusion attribuÃ©e est dans chaque feuille la modalitÃ© qui assure la plus
- 212 -RNTI-E-16
G. Ritschard et al.
forte valeur nÃ©gative de lâ€™indice dâ€™implication. La conclusion est ainsi dans ce cas automa-
tiquement dÃ©terminÃ©e. A titre dâ€™exemple, en induisant lâ€™arbre avec lâ€™indice dâ€™implication on
obtient les trois rÃ¨gles du tableau 5 qui correspondent aux feuilles F1, F2 et F3 dans la figure 1.
On notera en particulier que la conclusion attribuÃ©e Ã  la 2Ã¨me rÃ¨gle nâ€™est pas la modalitÃ© ma-
joritaire.
Le recours Ã  lâ€™indice dâ€™implication pour le choix des conclusions reste cependant Ã©galement
possible pour des arbres induits selon dâ€™autres critÃ¨res.
RÃ¨gle Condition Conclusion
R1 Homme et secteur primaire ou secondaire â†’ mariÃ©
R2 Homme et secteur tertiaire â†’ divorcÃ©
R3 Femme â†’ cÃ©libataire
TAB. 5 â€“ Meilleures rÃ¨gles en termes de force implicative.
5 Entropie dÃ©centrÃ©e
Les mesures dâ€™entropie ont Ã©tÃ© dÃ©finies mathÃ©matiquement par un ensemble dâ€™axiomes en
dehors du contexte de lâ€™apprentissage machine. On peut trouver des travaux dÃ©taillÃ©s dans RÃ©-
nyi (1960) et AczÃ©l et DarÃ³czy (1975). Leur transfert vers lâ€™apprentissage sâ€™est fait de maniÃ¨re
hÃ¢tive et sans prÃªter trop attention Ã  la pertinence de leurs axiomes fondateurs. Ainsi, nous
avons soulignÃ© dans Zighed et al. (2007) lâ€™intÃ©rÃªt de relÃ¢cher lâ€™axiome exigeant que lâ€™entropie
soit maximale Ã  la distribution uniforme, et par suite Ã©videmment lâ€™axiome de symÃ©trie stipu-
lant que lâ€™entropie doit Ãªtre insensible Ã  lâ€™ordre des probabilitÃ©s constituant la distribution. En
nous fondant sur une axiomatique plus gÃ©nÃ©rale, nous avons proposÃ© une entropie dÃ©centrÃ©e
dâ€™une distribution (p1, . . . , p`) gÃ©nÃ©ralisant lâ€™entropie quadratique dans le cas oÃ¹ ` = 2. Sa
forme thÃ©orique, standardisÃ©e pour que sa valeur maximale soit Ã©gale Ã  1, est :
hw(p1, p2, . . . , p`) =
1
`
âˆ‘`
i=1
pi(1âˆ’ pi)
(âˆ’2wi + 1)pi + w2i
(4)
oÃ¹ w = (w1, . . . , w`) est un vecteur de paramÃ¨tres caractÃ©risant la distribution dâ€™incertitude
maximale. On obtient une version empirique en remplaÃ§ant les pi par leurs estimations de
Laplace pË† = (ni + 1)/(n + `). Dâ€™autres formes dâ€™entropies dÃ©centrÃ©es ont Ã©galement Ã©tÃ©
proposÃ©es par Lallich et al. (2007) et Lenca et al. (2008).
5.1 Utilisation de lâ€™entropie dÃ©centrÃ©e
Par rapport Ã  lâ€™indice dâ€™implication qui compare le nÅ“ud obtenu au nÅ“ud initial en termes
de distribution entre exemples et contre-exemples, lâ€™entropie dÃ©centrÃ©e ne privilÃ©gie pas de
catÃ©gorie particuliÃ¨re et compare lâ€™ensemble de la distribution. Elle ne prÃ©juge donc pas de la
catÃ©gorie qui sera assignÃ©e au nÅ“ud.
Nous donnons au tableau 6 les gains dâ€™entropie pour les divers Ã©clatements possibles au
premier niveau. A titre de comparaison nous donnons les gains obtenus en termes de lâ€™indice de
- 213 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
entropie dÃ©centrÃ©e
Attribut utilisÃ© nbre sommets Gini thÃ©orique empirique
sexe 2 0.150 0.210 0.201
secteur 3 0.016 0.024 0.023
primaire 2 0.001 0.003 0.003
tertiaire 2 0.011 0.017 0.016
TAB. 6 â€“ Gains dâ€™entropie pour les Ã©clatements possibles au premier niveau.
Gini, qui est lâ€™entropie quadratique classique, et de lâ€™entropie dÃ©centrÃ©e thÃ©orique et empirique.
La version thÃ©orique est obtenue en remplaÃ§ant dans la formule (4) les pi par les frÃ©quences
observÃ©es, et la version empirique en les remplaÃ§ant par les estimations de Laplace.
Pour ce premier Ã©clatement, les entropies classiques et dÃ©centrÃ©es conduisent au mÃªme
rÃ©sultat. Le sexe est la variable Ã  retenir, tout comme il lâ€™Ã©tait avec le gain dâ€™implication. On
peut relever par ailleurs pour les entropies dÃ©centrÃ©es que le gain tend Ã  Ãªtre moins fort en
termes de mesure empirique que thÃ©orique.
entropie dÃ©centrÃ©e
Attribut utilisÃ© nbre sommets Gini thÃ©orique empirique
Sommet : Homme
secteur 3 0.084 0.111 0.089
primaire 2 0.020 0.025 0.012
tertiaire 2 0.082 0.106 0.098
Sommet : Femme
secteur 3 0.042 0.075 0.048
primaire 2 0.042 0.073 0.052
tertiaire 2 0.013 0.019 0.012
TAB. 7 â€“ Gains dâ€™entropie pour les Ã©clatements possibles au deuxiÃ¨me niveau.
Le tableau 7 propose la mÃªme comparaison pour les Ã©clatements possibles au second ni-
veau. Les rÃ©sultats divergent ici selon le type dâ€™entropie utilisÃ©. Pour ce qui est du sommet
â€œHommeâ€, Gini et lâ€™entropie dÃ©centrÃ©e thÃ©orique sÃ©lectionnerait lâ€™Ã©clatement en trois, tandis
que la version empirique de lâ€™entropie dÃ©centrÃ©e privilÃ©gie lâ€™Ã©clatement qui oppose le secteur
tertiaire aux deux autres secteurs. Il nâ€™y a donc que ce dernier indice qui donne un rÃ©sultat
concordant avec lâ€™optique implication discutÃ©e prÃ©cÃ©demment.
Pour le sommet â€œFemmeâ€, il y a Ã©galement divergence, lâ€™entropie empirique favorisant
Ã  nouveau un Ã©clatement en deux plutÃ´t quâ€™en trois, soit le secteur primaire contre les deux
autres.
Notons quâ€™Ã  nouveau les gains sont plus faibles avec la version empirique, les Ã©carts Ã©tant
ici plus importants en raison des effectifs plus faibles des nÅ“uds. Câ€™est la sensibilitÃ© aux effec-
tifs que nous souhaitions.
- 214 -RNTI-E-16
G. Ritschard et al.
5.2 Choix de la conclusion selon la contribution Ã  lâ€™entropie
Lâ€™entropie qui mesure lâ€™Ã©cart entre deux distributions ne se prÃªte pas en tant que telle Ã  la
mesure de lâ€™intÃ©rÃªt de chaque modalitÃ© dans la feuille. La contribution de chaque modalitÃ© Ã 
cette entropie nous donne par contre une information utile de ce point de vue. La seule valeur de
cette contribution nâ€™est cependant pas suffisante. Il nous faut tenir compte Ã©galement du signe
de lâ€™Ã©cart. En effet, une faible contribution Ã  lâ€™entropie indique une classe qui se dÃ©marque
fortement de sa proportion marginale, mais cet Ã©cart est pertinent seulement si lâ€™effectif observÃ©
dÃ©passe lâ€™effectif attendu en cas dâ€™indÃ©pendance. On propose alors de sÃ©lectionner dans chaque
feuille j la modalitÃ© qui maximise le critÃ¨re
max
i
Î·w,ij = signe(nij âˆ’ neij) (1âˆ’ hw,ij), j = 1, . . . , q (5)
oÃ¹ hw,ij est la contribution effective de la modalitÃ© i Ã  lâ€™entropie de la feuille j, nij le nombre
observÃ© de cas de modalitÃ© i dans la feuille j, et neij le nombre attendu sous lâ€™hypothÃ¨se dâ€™in-
dÃ©pendance. On retient ainsi la modalitÃ© dont la contribution hw,ij est la plus faible parmi
celles dont on observe plus de cas quâ€™attendus par hasard. Notons que, lâ€™Ã©cart nij âˆ’ neij Ã©tant
nÃ©cessairement non nÃ©gatif pour au moins un i, la fonction â€˜signeâ€™ pourrait tout aussi bien Ãªtre
remplacÃ©e par la fonction logique (nij âˆ’ neij > 0) qui prend la valeur 1 lorsque lâ€™Ã©cart est
positif et 0 sinon.
F1 F2 F3a F3b
mariÃ© 0.59 -0.79 -0.09 -0.93
cÃ©libataire -0.42 -1.00 0.39 0.88
dicorcÃ©/veuf -1.00 0.81 -1.00 -0.95
TAB. 8 â€“ Contributions Ã  lâ€™entropie dÃ©centrÃ©e des feuilles.
Le tableau 8 donne les valeurs de Î·w,ij pour les 4 feuilles de lâ€™arbre de la figure 1. On note
que les conclusions sÃ©lectionnÃ©es concordent avec celles dâ€™implication maximale.
6 Discussion
Nous avons vu que tant lâ€™indice dâ€™implication que lâ€™entropie dÃ©centrÃ©e pouvaient servir de
critÃ¨re de dÃ©veloppement de lâ€™arbre. Les deux approches fournissent Ã©galement des Ã©lÃ©ments
permettant dâ€™attribuer aux feuilles une conclusion appropriÃ©e dans un contexte de donnÃ©es
dÃ©sÃ©quilibrÃ©es oÃ¹ le rappel de catÃ©gories faiblement reprÃ©sentÃ©es est plus important que le
taux total dâ€™erreurs de classification. Les deux approches ne sont pas pour autant Ã©quivalentes.
Lâ€™indice dâ€™implication oppose la classe pour laquelle on a lâ€™implication maximale aux autres,
tandis que lâ€™entropie asymÃ©trique prend en compte tout le dÃ©tail de la distribution. Quels sont
alors les avantages et inconvÃ©nients respectifs ?
- 215 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
6.1 Avantages et limites
ConsidÃ©rons tout dâ€™abord lâ€™optique du dÃ©veloppement de lâ€™arbre. De ce point de vue, lâ€™in-
dice dâ€™implication a quelque analogie avec le critÃ¨re â€˜Twoingâ€™ de CART (Breiman et al., 1984)
qui pour chaque Ã©clatement possible cherche la partition en deux des valeurs de la variable
cible qui maximise lâ€™indice de Gini. Lâ€™avantage est quâ€™on a ainsi un critÃ¨re qui devient plus ro-
buste en se fondant sur des effectifs moins dispersÃ©s qui le rendent notamment moins sensible
aux variations Ã  lâ€™intÃ©rieur de chacune des deux classes. Le mÃªme argument vaut pour lâ€™indice
dâ€™implication bien que dans ce cas la premiÃ¨re classe nâ€™ait toujours quâ€™une seule catÃ©gorie.
Utiliser lâ€™indice dâ€™implication comme critÃ¨re dâ€™Ã©clatement prÃ©suppose que la catÃ©gorie
maximisant lâ€™implication sera assignÃ©e au nÅ“ud. Ceci assure Ã©videmment une cohÃ©rence Ã 
la procÃ©dure, mais limite Ã©videmment aussi lâ€™usage de lâ€™arbre obtenu au contexte oÃ¹ ce choix
de la conclusion selon la force implicative sâ€™avÃ¨re pertinent.
Pour ce qui est de lâ€™entropie dÃ©centrÃ©e, elle mesure la proximitÃ© Ã  la distribution de rÃ©fÃ©-
rence, proximitÃ© que lâ€™on cherche Ã  minimiser de sorte Ã  obtenir des distributions aussi dif-
fÃ©rentes que possible de la rÃ©fÃ©rence. On peut ici faire lâ€™analogie avec le critÃ¨re du khi-deux
utilisÃ© par lâ€™algorithme CHAID (Kass, 1980) qui conduit Ã©galement Ã  choisir la segmentation
pour laquelle les distributions sâ€™Ã©cartent le plus possible de celle dâ€™indÃ©pendance. La diffÃ©-
rence est que dans CHAID le rÃ©fÃ©rentiel change Ã  chaque nÅ“ud puisque le critÃ¨re consiste Ã 
sâ€™Ã©loigner le plus possible de la distribution du nÅ“ud quâ€™on Ã©clate, tandis quâ€™avec le gain dâ€™en-
tropie dÃ©centrÃ© on cherche Ã  se dÃ©marquer de la distribution du nÅ“ud initial qui reste la mÃªme
Ã  toutes les Ã©tapes du calcul.
Comme lâ€™illustre en particulier notre exemple, les deux approches conduisent Ã  des arbres
relativement semblables mÃªme sâ€™il lâ€™on peut imaginer des situations peu claires oÃ¹ les Ã©cla-
tements proposÃ©s peuvent diffÃ©rer. Remarquons tout de mÃªme que lâ€™indice dâ€™implication ne
propose pas dâ€™Ã©clatement lorsque les conclusions prÃ©vues pour les nÅ“uds qui en rÃ©sulteraient
sont les mÃªmes. Ainsi, dans la figure 1, le dÃ©veloppement sâ€™arrÃªte Ã  la feuille F3 avec lâ€™in-
dice dâ€™implication, alors mÃªme quâ€™on rÃ©alise un gain dâ€™entropie dÃ©centrÃ©e en Ã©clatant le nÅ“ud
en F3a et F3b. On peut donc sâ€™attendre Ã  obtenir des arbres moins complexes avec lâ€™indice
dâ€™implication quâ€™avec lâ€™entropie dÃ©centrÃ©e.
Sur le plan de la complexitÃ© de calcul, la mise en Å“uvre de lâ€™entropie dÃ©centrÃ©e semble un
peu plus immÃ©diate, lâ€™indice dâ€™implication nÃ©cessitant de tester Ã  chaque fois les diffÃ©rentes
possibilitÃ©s dâ€™opposer une catÃ©gorie aux autres. Ceci nâ€™affecte la complexitÃ© de lâ€™algorithme
que par un facteur multiplicatif c correspondant au nombre de catÃ©gories de la variable cible.
Enfin, dans une optique de gÃ©nÃ©ralisation, il est important pour assurer la robustesse des rÃ©-
sultats de disposer de critÃ¨res qui soient sensibles Ã  la taille des effectifs. Lâ€™entropie dÃ©centrÃ©e
lâ€™est dans sa forme empirique, la sensibilitÃ© Ã  lâ€™effectif dÃ©coulant de lâ€™utilisation des estima-
tions de Laplace des probabilitÃ©s. Quant Ã  lâ€™indice dâ€™implication, qui peut Ãªtre vu comme un
rÃ©sidu standardisÃ©, il est calculÃ© Ã  partir des effectifs et non des proportions et est donc sensible
aux effectifs par construction.
Si lâ€™on considÃ¨re Ã  prÃ©sent lâ€™attribution de la conclusion aux feuilles de lâ€™arbre, lâ€™indice
dâ€™implication prÃ©sente lâ€™avantage dâ€™avoir une interprÃ©tation claire abondamment discutÃ©e dans
la littÃ©rature : lâ€™implication statistique est dâ€™autant plus forte que la rÃ¨gle admet Ã©tonnamment
peu de contre-exemples.
Le critÃ¨re Î·w,ij , complÃ©mentaire Ã  un de la contribution Ã  lâ€™entropie dÃ©centrÃ©e, est moins
intuitif. Il mesure en quelque sorte lâ€™importance de lâ€™Ã©cart entre la frÃ©quence de la catÃ©gorie
- 216 -RNTI-E-16
G. Ritschard et al.
dans le feuille et la proportion avec laquelle cette mÃªme catÃ©gorie est observÃ©e dans lâ€™ensemble
de la population. Contrairement Ã  lâ€™indice dâ€™implication, il se fonde sur la frÃ©quence mÃªme
de la catÃ©gorie, et non sur ses contre-exemples. Le critÃ¨re Î·w,ij conduit ainsi Ã  privilÃ©gier la
catÃ©gorie dont la frÃ©quence domine relativement le plus fortement sa proportion marginale.
Les deux critÃ¨res trouvent leur justification dans une perspective de ciblage oÃ¹ lâ€™on sâ€™intÃ©-
resse Ã  savoir pour quelle valeur de la variable cible le profil dÃ©crit par la condition de la rÃ¨gle
est la plus typique. Par exemple, un mÃ©decin sera intÃ©ressÃ© en prioritÃ© Ã  savoir quelle est la
population la plus exposÃ©e au risque de dÃ©velopper un cancer. De mÃªme, il est naturel dâ€™axer
en prioritÃ© des actions de marketing, de prÃ©vention ou de contrÃ´le sur les groupes de population
qui seront les plus rÃ©ceptifs mÃªme lorsque ceux-ci ne sont pas majoritairement concernÃ©s par
les actions envisagÃ©es.
On peut noter que les deux indicateurs sÃ©lectionnent la mÃªme catÃ©gorie lorsquâ€™une seule
frÃ©quence de la feuille dÃ©passe la proportion marginale. Les choix peuvent cependant diverger
dans le cas contraire. A titre dâ€™exemple, nous donnons au tableau 9 la distribution dâ€™une feuille
j pour laquelle on obtient des conclusions non concordantes. On observe que lâ€™Ã©cart entre
effectifs observÃ©s (les nij) et attendus selon la distribution marginale est le mÃªme, âˆ’6, pour
les catÃ©gories A et B. Relativement, lâ€™Ã©cart est plus important pour la catÃ©gorie A que privilÃ©gie
la contribution Ã  lâ€™entropie. Lâ€™indice dâ€™implication privilÃ©gie par contre B, pour laquelle on a
moins de contre-exemples.
en tout distribution dans feuille j effectifs contre-exemples Indice Contrib. Î·w,ij
catÃ©gorie wi nij fij pË†ij attendus observÃ©s attendus implication Ã  lâ€™entropie
A 10% 12 .2 0.206 6 48 54 -0.75 0.064
B 20% 18 .3 0.302 12 42 48 -0.79 0.047
C 70% 30 .5 0.492 42 30 18 2.95 -0.147
Total 100% 60 1 60 1 - - - -
TAB. 9 â€“ Illustration de la diffÃ©rence entre indice dâ€™implication et contribution Ã  lâ€™entropie
dÃ©centrÃ©e.
Un avantage de lâ€™indice dâ€™implication est quâ€™il peut Ãªtre comparÃ© avec une distribution
normale, ce qui justifie dâ€™ailleurs la correction pour la continuitÃ© que nous lui avons apportÃ©.
Ceci permet de dire par exemple dans le cas du tableau 9 que le choix de la conclusion nâ€™est
pas solidement Ã©tabli statistiquement puisque la valeur de lâ€™indice et en deÃ§Ã  du seuil critique
de âˆ’1.645 pour un risque de 5%. Notons que pour la comparaison avec la loi normale il
serait prÃ©fÃ©rable dâ€™utiliser lâ€™une des variantes proposÃ©es dans Ritschard (2005), lâ€™indice de
Gras tendant Ã  avoir une variance infÃ©rieure Ã  1.
6.2 Vers une approche hybride
Au vu des remarques prÃ©cÃ©dentes nous proposons dâ€™exploiter de prÃ©fÃ©rence lâ€™entropie dÃ©-
centrÃ©e pour le dÃ©veloppement de lâ€™arbre et lâ€™indice dâ€™implication pour lâ€™attribution des conclu-
sions aux feuilles.
Pour le dÃ©veloppement de lâ€™arbre, lâ€™entropie dÃ©centrÃ©e est un peu plus simple Ã  mettre en
Å“uvre, mais nous semble surtout Ãªtre un instrument plus gÃ©nÃ©ral, lâ€™indice dâ€™implication Ã©tant
trop Ã©troitement liÃ© Ã  la procÃ©dure de choix de la conclusion et donc Ã  la seule Ã©laboration de
- 217 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
rÃ¨gles. Par exemple, le non Ã©clatement lorsque les rÃ¨gles obtenues prennent la mÃªme conclu-
sion peut Ãªtre un handicap dans la mesure oÃ¹ cela empÃªche de repÃ©rer des sous-groupes pour
lesquels la rÃ¨gle serait plus fiable que pour dâ€™autres. Lâ€™entropie dÃ©centrÃ©e nous semble de ce
point de vue permettre plus de nuances.
Une fois lâ€™arbre construit par contre, lâ€™indice dâ€™implication nous semble mieux indiquÃ©
pour le choix de la conclusion, de par lâ€™importance accordÃ©e aux contre-exemples et la possi-
bilitÃ© quâ€™il offre de juger de la signification statistique du lien entre prÃ©misse et conclusion de
la rÃ¨gle.
7 ExpÃ©rimentations
Le propos de cette section est de donner un Ã©clairage empirique sur lâ€™utilisation de lâ€™in-
dice dâ€™implication et de lâ€™entropie dÃ©centrÃ©e dans la construction dâ€™arbres de dÃ©cision. Il sâ€™agit
de vÃ©rifier empiriquement que ces critÃ¨res ont bien le comportement escomptÃ© en prÃ©sence
de donnÃ©es dÃ©sÃ©quilibrÃ©es, en particulier que le recours Ã  ces critÃ¨res permet dâ€™amÃ©liorer les
rÃ©sultats, notamment en termes de rappel de la ou des classes sous-reprÃ©sentÃ©es. Nous dis-
tinguons dans ces expÃ©rimentations lâ€™utilisation de chacun de ces indices comme critÃ¨re de
dÃ©veloppement de lâ€™arbre ainsi que comme critÃ¨re pour assigner la classe ou conclusion aux
feuilles.
Lâ€™objectif Ã©tant Ã©galement dâ€™illustrer les diffÃ©rences entre indice dâ€™implication et entropie
dÃ©centrÃ©e, nous retenons pour cette Ã©tude empirique des donnÃ©es oÃ¹ la variable rÃ©ponse a plus
de deux classes. En effet, dans le cas de deux classes, en opposant la catÃ©gorie sÃ©lectionnÃ©e aux
autres comme le fait le premier indice on exploite la mÃªme information que lâ€™entropie dÃ©cen-
trÃ©e qui prend en compte toute la distribution, et les rÃ©sultats devraient donc Ãªtre trÃ¨s similaires.
Nous retenons ainsi comme point de dÃ©part pour nos expÃ©rimentation des donnÃ©es rÃ©elles sur
la situation (rÃ©ussi, redouble, Ã©liminÃ©) aprÃ¨s leur premiÃ¨re annÃ©e dâ€™Ã©tudes des Ã©tudiants qui ont
commencÃ© leur cursus Ã  la FacultÃ© des sciences Ã©conomiques et sociales de lâ€™UniversitÃ© de Ge-
nÃ¨ve en 1998 (Petroff et al., 2001). Les donnÃ©es Ã©tant cependant peu dÃ©sÃ©quilibrÃ©es avec une
classe minoritaire (redouble) de 17%, nous avons forcÃ© le dÃ©sÃ©quilibre en gonflant la classe
majoritaire (rÃ©ussi) par sur-Ã©chantillonnage, câ€™est-Ã -dire en dupliquant alÃ©atoirement des cas
de ce groupe. Le tableau 7 indique comment les donnÃ©es finalement retenues se rÃ©partissent
selon les trois classes de la variable rÃ©ponse.
Classe Effectif Proportion
1. Ã©liminÃ© 209 0.06
2. redouble 130 0.03
3. rÃ©ussi 3384 0.91
TAB. 10 â€“ Distribution de la variable rÃ©ponse du jeu de donnÃ©es utilisÃ©es
Lâ€™expÃ©rimentation menÃ©e consiste Ã  gÃ©nÃ©rer des arbres en utilisant successivement le gain
dâ€™entropie classique de Shannon, le gain de force implicative et le gain de lâ€™entropie dÃ©centrÃ©e
comme critÃ¨re dâ€™Ã©clatement. Le dÃ©veloppement des arbres est arrÃªtÃ© lorsquâ€™on ne peut plus
obtenir de gain strictement positif. Aucun autre critÃ¨re dâ€™arrÃªt nâ€™est utilisÃ© et aucun Ã©lagage
- 218 -RNTI-E-16
G. Ritschard et al.
CritÃ¨re de dÃ©veloppement de lâ€™arbre
Shannon Implication Entropie dÃ©centrÃ©e
RÃ¨gle 1 2 3 1et2 1 2 3 1et2 1 2 3 1et2
Rappel
Majoritaire 25.4 15.6 100.0 46.6 18.6 16.7 99.9 33.3 15.3 10.8 100.0 25.1
Implication 35.4 26.1 82.6 57.8 39.2 20.0 86.9 51.3 40.2 26.2 73.1 61.4
Entropie dÃ©centrÃ©e 34.9 23.1 84.4 58.7 37.6 21.3 87.4 51.6 34.4 25.4 73.9 57.5
PrÃ©cision
Majoritaire 61.0 38.8 94.9 100.0 61.7 50.0 93.7 97.4 62.7 41.2 93.0 100.0
Implication 14.3 11.1 95.1 24.9 18.1 14.1 94.7 28.2 12.6 7.6 95.0 18.6
Entropie dÃ©centrÃ©e 15.9 9.9 95.3 27.3 18.4 15.1 94.7 29.1 12.7 6.4 94.6 18.1
TAB. 11 â€“ Rappel et prÃ©cision, 10-validation croisÃ©e
nâ€™est effectuÃ©. Il sâ€™agit lÃ  Ã©videmment dâ€™une premiÃ¨re expÃ©rimentation qui permettra de voir ce
quâ€™il se passe dans le cas extrÃªme oÃ¹ lâ€™on laisse lâ€™arbre se dÃ©velopper au maximum. Pour cette
expÃ©rimentation prÃ©liminaire, nous avons Ã©galement simplement utilisÃ© les simple frÃ©quences
observÃ©es sans correction de Laplace.
Le tableau 11 donne les taux de rappel et de prÃ©cision pour chacune des trois classes, ainsi
que pour le regroupement des deux classes sous-reprÃ©sentÃ©es. Il ressort trÃ¨s clairement de ces
rÃ©sultats que fonder le choix des conclusions sur lâ€™indice dâ€™implication ou la contribution Ã 
lâ€™entropie, permet dâ€™amÃ©liorer sensiblement le rappel des classes sous-reprÃ©sentÃ©es. Les diffÃ©-
rences entre les deux critÃ¨res restent cependant non significatifs. Pour ce qui est du critÃ¨re de
croissance de lâ€™arbre, les rÃ©sultats sont moins clairs. Il est surprenant que ni lâ€™indice dâ€™implica-
tion, ni lâ€™entropie dÃ©centrÃ©e ne domine lâ€™entropie centrÃ©e de Shannon. Lâ€™explication tient sans
doute Ã  lâ€™absence de critÃ¨re dâ€™arrÃªt et dâ€™Ã©lagage. En effet on tend ainsi Ã  Ã©puiser les prÃ©dicteurs
et donc Ã  gÃ©nÃ©rer une partition fine qui est sans doute assez semblable quelque soit le critÃ¨re
utilisÃ©.
8 Conclusion et perspectives
Nous nous sommes dans cet article intÃ©ressÃ©s Ã  la problÃ©matique des donnÃ©es dÃ©sÃ©qui-
librÃ©es dans le contexte des arbres de dÃ©cision. Nous avons prÃ©sentÃ© et discutÃ© avantages et
inconvÃ©nients de deux approches, lâ€™une fondÃ©e sur lâ€™indice dâ€™implication et lâ€™autre sur une en-
tropie dÃ©centrÃ©e. Il apparaÃ®t que ces deux approches conduisent Ã  des solutions semblables bien
quâ€™obÃ©issant Ã  des logiques totalement diffÃ©rentes. Lâ€™entropie dÃ©centrÃ©e semble Ãªtre un critÃ¨re
plus naturel pour le dÃ©veloppement de lâ€™arbre tandis que lâ€™indice dâ€™implication prÃ©sente des
avantages certains pour sÃ©lectionner la catÃ©gorie Ã  attribuer aux feuilles. Il sâ€™agit lÃ  cependant
dâ€™une conjecture que notre expÃ©rimentation prÃ©liminaire nâ€™a pas clairement confirmÃ©. Nous
travaillons actuellement Ã  la mise au point dâ€™un protocole dâ€™expÃ©rimentation plus Ã©laborÃ© qui
permettra de tester empiriquement notre hypothÃ¨se et dâ€™Ã©valuer les incidences des paramÃ¨tres
de contrÃ´le du dÃ©veloppement de lâ€™arbre. Lâ€™expÃ©rimentation devrait aussi porter sur un en-
semble de jeux de donnÃ©es benchmark. Enfin, il nous faudra encore populariser ces procÃ©dures
en les implÃ©mentant dans des plateformes aisÃ©ment accessibles.
- 219 - RNTI-E-16
Arbre sur donnÃ©es dÃ©sÃ©quilibrÃ©es, indice dâ€™implication et entropie dÃ©centrÃ©e
RÃ©fÃ©rences
AczÃ©l, J. et Z. DarÃ³czy (1975). On measures of information and their characterizations. New
York: Academic Press.
Blanchard, J., F. Guillet, H. Briand, et R. Gras (2005). Une version discriminante de lâ€™indice
probabiliste dâ€™Ã©cart Ã  lâ€™Ã©quilibre pour mesurer la qualitÃ© des rÃ¨gles. In Gras et al. (2005),
pp. 131â€“138.
Breiman, L., J. H. Friedman, R. A. Olshen, et C. J. Stone (1984). Classification And Regression
Trees. New York: Chapman and Hall.
Gras, R., R. Couturier, J. Blanchard, H. Briand, P. Kuntz, et P. Peter (2004). Quelques critÃ¨res
pour une mesure de qualitÃ© de rÃ¨gles dâ€™association. Revue des nouvelles technologies de
lâ€™information RNTI E-1, 3â€“30.
Gras, R., F. Spagnolo, et J. David (Eds.) (2005). Actes des TroisiÃ¨mes Rencontres Internatio-
nale ASI Analyse Statistique Implicative, Volume Secondo supplemento al N.15 ofQuaderni
di Ricerca in Didattica, Palermo. UniversitÃ  degli Studi di Palermo.
Kass, G. V. (1980). An exploratory technique for investigating large quantities of categorical
data. Applied Statistics 29(2), 119â€“127.
Lallich, S., P. Lenca, et B. Vaillant (2005). Variation autour de lâ€™intensitÃ© dâ€™implication. In
Gras et al. (2005), pp. 237â€“246.
Lallich, S., P. Lenca, et B. Vaillant (2007). Construction dâ€™une entropie dÃ©centrÃ©e pour lâ€™ap-
prentissage supervisÃ©. InQDC 2007, Actes du 3Ã¨me atelier QualitÃ©s des donnÃ©es et connais-
sances, EGC janvier 2007, Namur, pp. 45â€“54.
Lenca, P., S. Lallich, T.-N. Do, et N.-K. Pham (2008). A comparison of different off-centered
entropies to deal with class imbalance for decision trees. In Advances in Knowledge Dis-
covery and Data Mining, 12th Pacific-Asia Conference, PAKDD 2008, Osaka, Japan, May
20-23, pp. 634â€“643.
Lerman, I. C., R. Gras, et H. Rostam (1981). Elaboration dâ€™un indice dâ€™implication pour
donnÃ©es binaires I. MathÃ©matiques et sciences humaines (74), 5â€“35.
Marcellin, S., D. A. Zighed, et G. Ritschard (2006). Detection of breast cancer using an asym-
metric entropy measure. In A. Rizzi et M. Vichi (Eds.), COMPSTAT 2006 - Proceedings in
Computational Statistics, pp. 975â€“982. Berlin: Springer. (on CD).
Petroff, C., A.-M. Bettex, et A. Korffy (2001). ItinÃ©raires dâ€™Ã©tudiants Ã  la FacultÃ© des sciences
Ã©conomiques et sociales: le premier cycle. Technical report, UniversitÃ© de GenÃ¨ve, FacultÃ©
SES.
Pisetta, V., G. Ritschard, et D. A. Zighed (2007). Choix des conclusions et validation des
rÃ¨gles issues dâ€™arbres de classification. In M. Noirhomme et G. Venturini (Eds.), Extraction
et Gestion des Connaissances (EGC 2007), Volume E-9 of Revue des nouvelles technologies
de lâ€™information RNTI, pp. 485â€“496. CÃ©paduÃ¨s.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Mateo: Morgan Kaufmann.
RÃ©nyi, A. (1960). On measures of entropy and information. In Proceedings of the 4th Berkeley
Symposium on Mathematics, Statistics and Probability, Volume 1, Berkeley, pp. 547â€“561.
University of California Press.
- 220 -RNTI-E-16
G. Ritschard et al.
Ritschard, G. (2005). De lâ€™usage de la statistique implicative dans les arbres de classification.
In Gras et al. (2005), pp. 305â€“314.
Zighed, D. A., S. Marcellin, et G. Ritschard (2007). Mesure dâ€™entropie asymÃ©trique et consis-
tante. In M. Noirhomme et G. Venturini (Eds.), Extraction et Gestion des Connaissances
(EGC 2007), Volume E-9 of Revue des nouvelles technologies de lâ€™information RNTI, pp.
81â€“86. CÃ©paduÃ¨s.
Summary
This paper is concerned with the induction of classification trees for imbalanced data, i.e.
for the case where some categories of the target variable are much less frequent than other ones.
More specifically, we address two aspects. On the one hand, we look for growing criteria that
efficiently take into account the specific imbalanced nature of the data. On the other hand, we
deal with the relevance of the conclusion that should be assigned to the leaves of a grown tree.
We have recently considered two independent ways for dealing with these issues. The first
one consisted in defining and using out centered entropies, and the second one on relying on
measures of implication strength derived from implicative statistics. The aim of this paper is to
compare and establish the relationship between these two approaches. It presents a first rouph
experimentation.
- 221 - RNTI-E-16
  
- 222 -RNTI-E-16
