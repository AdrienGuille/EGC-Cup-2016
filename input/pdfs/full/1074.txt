Structure Inference of Bayesian Networks from Data: A New
Approach Based on Generalized Conditional Entropy
Dan A. Simoviciâˆ—, Saaid Baratyâˆ—
âˆ—Univ. of Massachusetts Boston, Massachusetts 02125, USA
{dsim,sbaraty}@cs.umb.edu
Abstract. We propose a novel algorithm for extracting the structure of a Bayesian
network from a dataset. Our approach is based on generalized conditional en-
tropies, a parametric family of entropies that extends the usual Shannon condi-
tional entropy. Our results indicate that with an appropriate choice of a general-
ized conditional entropy we obtain Bayesian networks that have superior scores
compared to similar structures obtained by classical inference methods.
1 Introduction
A Bayesian Belief Network (BBN) structure is a directed acyclic graph which represents
probabilistic dependencies among a set of random variables.
Inducing a BBN structure for the set of attributes of a dataset is a well known problem and
a challenging one due to enormity of the search space. The number of possible BBN structures
grows super-exponentially with respect to the number of the nodes.
In Cooper and Herskovits (1993), where the K2 heuristic algorithm is introduced, a mea-
sure of the quality of the structure is derived based on its posterior probability in presence of
a dataset. An alternative approach to compute a BBN structure is based on the Minimum De-
scription Length principle (MDL) first introduced in Rissanen (1978). The algorithms of Lam
and Bacchus (1994) and Suzuki (1999) are derived from this principle.
We propose a new approach to inducing BBN structures from datasets based on the notion
of Î²-generalized entropy (Î²-GE) and its corresponding Î²-generalized conditional entropy (Î²-
GCE) introduced in Havrda and Charvat (1967) and axiomatized in Simovici and Jaroszewicz
(2002) as a one-parameter family of functions defined on partitions (or probability distribu-
tions). The flexibility that ensues allows us to generate BBNs with better scores than published
results.
One important advantage of our approach is that, unlike Cooper and Herskovits (1993) it
is not based on any distributional assumption for developing the formula.
2 Generalized Entropy and Structure Inference
The set of partitions of a set S is denoted by PART(S). The trace of a partition pi on a
subset T of S is the partition piT = {T âˆ© Bi | i âˆˆ I and T âˆ© Bi 6= âˆ…} of T . The usual order
between set partitions is denoted by â€œâ‰¤â€. It is well-known that (PART(S),â‰¤) is a bounded
Inference of Bayesian Networks
lattice. The infimum of two partitions pi and piâ€² = {Bj |j âˆˆ J} on S, denoted with pi âˆ§ piâ€², is
the partition {Bi âˆ© Bj |i âˆˆ I, j âˆˆ J,Bi âˆ© Bj 6= âˆ…} on S . The least element of this lattice is
the partition Î±S = {{s} | s âˆˆ S}; the largest is the partition Ï‰S = {S}.
The notion of generalized entropy or Î²-entropy was introduced in Havrda and Charvat
(1967) and axiomatized for partitions in Simovici and Jaroszewicz (2002). If S is a finite
set and pi = {B1, . . . , Bm} is a partition of S, the Î²-entropy of pi is the number HÎ²(pi) =
1
1âˆ’21âˆ’Î²
(
1âˆ’
âˆ‘m
i=1
(
|Bi|
|S|
)Î²)
for Î² > 1. The Shannon entropy is obtained as limÎ²â†’1HÎ²(pi).
For Î² â‰¥ 1 the function HÎ² : PART(S) âˆ’â†’ Râ‰¥0 is anti-monotonic. Thus, HÎ²(pi) â‰¤
HÎ²(Î±S) =
1âˆ’nÎ²âˆ’1
(21âˆ’Î²âˆ’1)Â·nÎ²âˆ’1 , where n = |S|.
Let pi, Ïƒ âˆˆ PART(S) be two partitions, where pi = {B1, . . . , Bm} and Ïƒ = {C1, . . . , Cn}.
The Î²-conditional entropy of pi and Ïƒ isHÎ²(pi|Ïƒ) =
âˆ‘n
j=1
(
|Cj|
|S|
)Î²
HÎ²(piCj ). It is immediate
thatHÎ²(pi|Ï‰S) = HÎ²(pi) and thatH(pi|Î±S) = 0. Also, in Simovici and Jaroszewicz (2006) it
is shown thatHÎ²(pi|Ïƒ) = HÎ²(pi âˆ§ Ïƒ)âˆ’HÎ²(Ïƒ), a property that extends the similar property of
Shannon entropy.
When Î² â‰¥ 1 the Î²-GCE is dually anti-monotonic with respect to its first argument and is
monotonic with respect to its second argument. Moreover, we haveHÎ²(pi|Ïƒ) â‰¤ HÎ²(pi).
LetD be a dataset with set of attributesAttr(D). The domain of attribute Ai âˆˆ Attr(D)
isDom(Ai). The projection of a tuple t âˆˆ D onX is the restriction t[X ] of t to the setX . The
set of attributesX defines a partition piX onD, which groups together the tuples that have the
equal projections onX .
Let A be an attribute and let X be set of parents for A, where Dom(A) = {v1, v2, ..., vn}
and Dom(X) =
âˆ
BâˆˆX Dom(B) = {u1, u2, ..., um}. Define pij = P (t[A] = vi|t[X ] = uj).
We have 1
nÎ²âˆ’1
â‰¤
âˆ‘n
i=1 p
Î²
ij â‰¤ 1 for Î² â‰¥ 1. X is considered as a â€œgoodâ€ parent set for
A if knowing the its value enables us to predict the value of A with a high probability, that
is, if aj =
âˆ‘n
i=1 p
Î²
ij is close to 1 for every j where P (t[X ] = uj) is sufficiently large.
Clearly, X is a â€œperfectâ€ parent if
âˆ‘m
j=1 aj = m. The Î²-GCE captures exactly this parent-
hood quality measure. Indeed, suppose that piA = {Bi|1 â‰¤ i â‰¤ n} and piX = {Cj|1 â‰¤
j â‰¤ m}, where for t âˆˆ Bi we have t[A] = vi, and for s âˆˆ Cj we have s[X ] = uj .
Then, pij = P (t[A] = vi|t[X ] = uj) = P (t âˆˆ Bi|t âˆˆ Cj) = |Biâˆ©Cj ||Cj| , which implies
HÎ²(piA|piX) =
1
1âˆ’21âˆ’Î²
âˆ‘m
j=1 P
Î²(Cj) (1âˆ’ aj). Thus, minimizing HÎ²(piA|piX) amounts to
reducing the values of (1 âˆ’ aj) as much as possible for those jâ€™s where |Cj | is large, that is,
P (Cj) = P (t[X ] = uj) is non-trivial. We refer to quantity HÎ²(piA|piX) as the entropy of
node A in presence of set X . However, even if X = argminX(HÎ²(piA|piX)), the value of
the minimum itself may be too high to insure good predictability. An alternative is to mea-
sure the reduction of entropy of node A as a result of presence of set X as HÎ²(pi
A|piX)
HÎ²(piA)
. Since
0 â‰¤ HÎ²(piA|piX) â‰¤ HÎ²(piA) we have 0 â‰¤ HÎ²(pi
A|piX )
HÎ²(piA)
â‰¤ 1. If X is a perfect parent set for A,
then aj = 1 for 1 â‰¤ j â‰¤ m, soHÎ²(piA|piX) = 0.
Let  âˆˆ [0, 1] be a number referred to as prediction threshold. We regardX as a -suitable
parent of A if HÎ²(pi
A|piX)
HÎ²(piA)
â‰¤ .
To avoid cycles in the network we start from a sequence of attributes A1, A2, ..., Ap and
we seek the set of parents for Ai in the set Î¦(Ai) = {A1, . . . , Aiâˆ’1}, a frequent assumption
RNTI - X -
D. A. Simovici and S. Baraty
FIG. 1 â€“ Visualization of the Algorithm
(see Cooper and Herskovits (1993); Suzuki (1999)). In addition, we set a bound r on the
maximum number of parents. The set Î¦(Ai) may contain many subsets that are -suitable.
A possible solution is to choose an -suitable parent set X âŠ† Î¦(Ai) with minimum Î²-GCE
HÎ²(pi
A|piX). By the monotonicity property of Î²-GCE with respect to second argument we
have HÎ²(piAi |piÎ¦(Ai)) â‰¤ HÎ²(piAi |pi{A1,A2,...Aiâˆ’2}) â‰¤ Â· Â· Â· â‰¤ HÎ²(piAi |pi{A1}) â‰¤ HÎ²(piAi).
Then, for a given , if X has the minimum HÎ²(piAi |piX) among all -suitable parents of Ai,
thenX has the maximum possible size. To simplify the structure, we trade some predictability
for simplicity by adopting a heuristic approach which finds a minimal set of parents for a node
with highest possible reduction of entropy of that child node on its presence.
Define Î˜l (Ai) = {X âŠ† Î¦(Ai)|X is an -suitable parent of Ai and |X | = l} and Âµ =
min{n âˆˆ N|Î˜n(Ai) 6= âˆ…}. When Âµ â‰¤ r, we have the sequence of nonempty collections of
sets of attributes Î˜Âµ(Ai),Î˜Âµ+1(Ai), ...,Î˜r(Ai) by the monotonicity property of Î²-GCE.
Let X` = argminXâˆˆÎ˜
`
(Ai)(HÎ²(pi
Ai |piX)) be the first set of size ` (in lexicographical
order) that minimizes HÎ²(piAi |piX). We limit our parent search to the sequence of sets S =
(XÂµ, XÂµ+1, . . . , Xr), where the sets are listed in increasing order of size. For the sequence
S = (XÂµ, XÂµ+1, . . . , Xr) defined above we have HÎ²(piAi |piÂµ) â‰¥ HÎ²(piAi |piXÂµ+1) â‰¥ Â· Â· Â· â‰¥
HÎ²(piAi |piXr ). The set of points {(0,HÎ²(piAi))} âˆª {(p,HÎ²(piAi |piXp)) | Âµ â‰¤ p â‰¤ r} in
R
2 can be placed on a non-increasing curve with height h = HÎ²(piAi) âˆ’ HÎ²(piAi |piXr ) as
shown in Figure 1. We initialize the current parent set Xu to âˆ… and iterate over members of
S in increasing order of their size. The member Xv âˆˆ S leads to a nontrivial improvement in
predictability over Xu if HÎ²(pi
Ai |piXu )âˆ’HÎ²(pi
Ai |piXv )
HÎ²(piAi )âˆ’HÎ²(piAi |piXr )
â‰¥ vâˆ’u
r
. This happens if the decrease in
HÎ²(piAi |piX`) when the parent set of Ai is changed fromXu to Xv is greater than or equal to
linear decrease with respect to the two end points of the corresponding non-increasing curve as
shown in Figure 1. The end points of the curve are (0,HÎ²(piAi)) and (r,HÎ²(piAi |piXr )) and
the linear decrease with respect to two end points of the curve when we move from u to v on
x-axis which correspond to parent setsXu andXv is hÂ·(vâˆ’u)r =
(HÎ²(pi
Ai )âˆ’HÎ²(pi
Ai |piXr ))Â·(vâˆ’u)
r
.
Note that v = u + w where 1 â‰¤ w â‰¤ r âˆ’ u. This suggests that we do not stop the process
if Xu+1 does not satisfy the above inequality since there may be a parent set Xv âˆˆ S where
v > u+1 with non-trivial improvement in predictability with respect to current parent setXu.
RNTI - X -
Inference of Bayesian Networks
Algorithm 1: BuildBayesNet
input : DatasetD, Real Î², , r
//  âˆˆ [0, 1] is the prediction threshold.
// Î² â‰¥ 1 is the parameter for Î²-entropy.
// r is the maximum number of parents.
//Attr(D) is a list of attributes ofD where if
// 1 â‰¤ i < j â‰¤ |Attr(D)| the ith element of the list can
// be a parent of jth element, but not vice versa.
output : A Network Structure forD
NetworkStructure N
for iâ† |Attr(D)| to 1 do
NodeAi â† Attr(D)[i];
Integer Âµâ† 0, mâ† min(r, iâˆ’ 1)
RealH[m+ 1]
Set S[m+ 1]
H[0]â† HÎ²(pi
Ai )
for j â† m to 1 do
Compute Î˜j(Ai)
ifÎ˜j(Ai) = âˆ… then
break
else
S[j] â† argminxâˆˆÎ˜
j
(Ai)
(HÎ²(pi
Ai |pix))
H[j] â† HÎ²(pi
Ai |piS[j])
Âµâ† j
N.addNode(Ai)
if Âµ 6= 0 then
Integer uâ† 0
for v â† Âµ tom do
if H[u]âˆ’H[v]
vâˆ’u
â‰¥ H[0]âˆ’H[m]
m
then
uâ† v
forall x âˆˆ S[u] do
N.addEdge(xâ†’ Ai)
returnN; //end of algorithm
The increase in size of the parent set is penalized by making the condition stricter for larger
parent sets. Also, if none of the parent sets in S of size Âµ to r âˆ’ 1 satisfy the inequality, then
Xr will.
3 Experimental Results
We compared the generated results with well-known Bayesian structures in literature using
two scoring schemes, MDL used by Lam and Bacchus (1994) and Suzuki (1999) and the
scoring method of Cooper and Herskovits (1993). Experiments involved the Brain Tumor
dataset (Cooper (1984)), the Breast Cancer (Blake et al. (1998a)), ALARM (Beinlich et al.
(1989)), and IRIS (Blake et al. (1998b)). The experimental results are presented in Table 1. The
last row of each table contains the two scores for published structures (according to Williams
and Williamson (2006) and Beinlich et al. (1989)). We assume that the distribution on priors
of the structures for a given dataset is uniform Cooper and Herskovits (1993). Experiments
were performed on a machine with 64-bit Intel Xeon processor.
The scores for generated network structures depends on Î² and  and in many cases is better
than the scores for established structures (C-H scores are higher and MDL scores are lower).
Figure 2 represents four different structures for Brain Tumor dataset. Structure A is the one
RNTI - X -
D. A. Simovici and S. Baraty
TAB. 1 â€“ Experimental Results
Generated Structures 10000 rows
Î²  r log(C-H Score) MDL Score Time(ms)
1.0 1.0 3 -7483 13631.52 57
1.0 0.8 2 -7506 13474.37 51
1.6 0.7 2 -7588 13680.31 45
2.1 0.5 3 -7588 13693.21 55
Original Structure -8115 14410.10 -
Generated Structures 286 rows
Î²  r log(C-H Score) MDL Score Time(ms)
1.1 0.5 2 -1172 3210.22 144
1.0 0.6 3 -1197 8640.41 202
1.7 0.3 2 -1207 3669.88 121
1.8 0.7 3 -1214 3859.67 196
1.0 0.5 3 -1215 3511.35 202
1.2 0.4 2 -1224 4968.50 133
1.0 0.7 3 -1256 13667.40 202
Original Structure -1201 4142.03 -
Brain Cancer Results Breast Cancer Results
Generated Structures 20002 rows
Î²  r log(C-H Score) MDL Score Time(s)
1.2 0.5 3 -114931 270298.25 542
1.2 0.5 4 -114981 271590.92 12801
1.2 0.6 4 -116081 272665.06 12802
1.1 0.7 3 -116914 271469.89 546
Original Structure -159306 378518.37 -
Generated Structures 150 rows
Î²  r log(C-H Score) MDL Score Time(ms)
1.0 0.4 2 -902 127543.87 109
1.8 0.7 3 -905 13279.40 173
Original Structure -932 261481.02 -
Alarm Results Iris Results
introduced by G. F. Cooper. Structures B (Î² = 1.0, Î± = 1.0, r = 3), C(Î² = 1.0, Î± = 0.8, r =
2) and D(Î² = 2.1, Î± = 0.5, r = 3) are the ones generated by our approach.
4 Conclusions
We developed an approach for generating a Bayesian network structure from data based on
notion of generalized entropy.
The best parent-child relationships among attributes is obtained at values of Î² that are
highly dependent on the data set, a fact that suggests that the GCE approach is preferable to
using the Shannon entropy.
References
Beinlich, I., H. Suermondt, R. Chavez, and G. F. Cooper (1989). The ALARMmonitoring sys-
tem: A case study with two probabilistic inference techniques for belief networks. Technical
Report KSL-88-84, Stanford University, Knowledge System Laboratory.
Blake, C., D. Newman, S. Hettich, and C. Merz (1998a). UCI repository of machine learn-
ing databases. A dataset from Ljubljana Oncology Institute provided by UCI, available at
http://www.ics.uci.edu/ mlearn/MLRepository.html.
Blake, C., D. Newman, S. Hettich, and C. Merz (1998b). UCI repository of machine learning
databases. A dataset created by R.A. Fisher and donated by Michael Marshall, available at
http://www.ics.uci.edu/ mlearn/MLRepository.html.
Cooper, G. F. (1984). NESTOR: A computer-basedmedical diagnosis aid that integrates casual
and probabilistic knowledge. Ph. D. thesis, Stanford University.
Cooper, G. F. and E. Herskovits (1993). A Bayesian method for the induction of probabilistic
networks from data. Technical Report KSL-91-02, Stanford University, Knowledge System
Laboratory.
Havrda, J. H. and F. Charvat (1967). Quantification methods of classification processes: Con-
cepts of structural Î± entropy. Kybernetica 3, 30â€“35.
RNTI - X -
Inference of Bayesian Networks
FIG. 2 â€“ Brain Tumor Structures
Lam, W. and F. Bacchus (1994). Learning Bayesian belief networks: An approach based on
the MDL principle. Computational Intelligence 10, 269â€“293.
Rissanen, J. (1978). Modeling by shortest data description. Automatica 14, 456â€“471.
Simovici, D. A. and S. Jaroszewicz (2002). An axiomatization of partition entropy. Transac-
tions on Information Theory 48, 2138â€“2142.
Simovici, D. A. and S. Jaroszewicz (2006). A new metric splitting criterion for decision trees.
International Journal of Parallel, Emergent and Distributed Systems 21, 239â€“256.
Suzuki, J. (1999). Learning Bayesian belief networks based on theMDL principle: An efficient
algorithm using the branch and bound technique. IEICE Trans. Information and Systems,
356â€“367.
Williams, M. and J. Williamson (2006). Combining argumentation and Bayesian nets for breast
cancer prognosis. Journal of Logic, Language and Information 15, 155â€“178.
RÃ©sumÃ©
Nous proposons un nouvel algorithme pour extraire la structure dâ€™un rÃ©seau BayÃ©sien dâ€™un
ensemble de donnÃ©es. Notre approche est basÃ©e sur les entropies conditionnelles gÃ©nÃ©ralisÃ©es,
une famille conditionnelle dâ€™entropies qui Ã©tend lâ€™entropie conditionnelle de Shannon.Nos rÃ©-
sultats indiquent que, avec un choix appropriÃ© dâ€™une entropie conditionnelle gÃ©nÃ©ralisÃ©e, nous
obtenons des rÃ©seaux BayÃ©siens qui ont des scores supÃ©rieurs aux structures similaires obte-
nues par des mÃ©thodes classiques dâ€™infÃ©rence.
RNTI - X -
