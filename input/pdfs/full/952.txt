Analyse en composantes principales d’un flux de données
d’espérance variable dans le temps
Jean-Marie Monnez
Institut Elie Cartan UMR 7502 – Laboratoire de Mathématiques
Nancy-Université, CNRS, INRIA
BP 239 – F 54506 – Vandoeuvre-lès-Nancy Cedex
jean-marie.monnez@iecn.u-nancy.fr
Résumé. On considère un flux de données représenté par une suite de vecteurs
de données. On suppose que chaque vecteur de données est une réalisation d’un
vecteur aléatoire dont l’espérance mathématique varie dans le temps selon un
modèle linéaire pour chacune des composantes. On utilise des processus d’ap-
proximation stochastique pour estimer en ligne les paramètres des modèles li-
néaires et en même temps les facteurs de l’ACP du vecteur aléatoire.
1 Modèle de flux et plan d’étude
Soit un flux de données, représenté par une suite de vecteurs (z1, . . . , zn, . . .) dans Rp.
1.1 Modèle d’étude et formulation
On suppose que :
– pour tout n, zn est la réalisation d’un vecteur aléatoire Zn, d’espérance mathématique
variable dans le temps ;
– les vecteurs aléatoires Zn sont mutuellement indépendants ;
– pour tout n, on a la décompositionZn = θn+Rn, θn = (θ1n . . . θpn)′ étant un vecteur de
R
p
, la loi du vecteur aléatoire Rn ne dépendant pas de n, E [Rn] = 0, Covar [Rn] = Σ
(matrice de covariance deRn) ; ceci revient à supposer que lesRn = Zn−θn constituent
un échantillon i.i.d. d’un vecteur aléatoire R dans Rp tel que E [R] = 0, Covar [R] =
Σ ; on a alors E [Zn] = θn, Covar [Zn] = Σ ; rn = zn − E [Zn] représente la donnée
zn centrée ;
– pour i = 1, . . . , p, il existe un vecteur βi de Rni inconnu et , pour tout n, un vecteur
U in de Rni connu au temps n tels que θin =
〈
βi, U in
〉
, 〈., .〉 dé signant le produit scalaire
euclidien usuel dans Rni ; U in peut être un vecteur de fonctions connues du temps (θin
est alors une combinaison linéaire de fonctions connues du temps) ou un vecteur de
valeurs de variables explicatives contrôlées ; si l’on note Zin, respectivementRin, la ie`me
composante de Zn, respectivementRn, on a alors le modèle de régression linéaire
Zin =
〈
βi, U in
〉
+Rin, i = 1, . . . , p.
Analyse en composantes principales d’un flux de données
On pose le problème suivant : réaliser une analyse en composantes principales (ACP) du
vecteur aléatoire R dans Rp que l’on munit d’une métrique M ; on effectue ainsi une ACP
de données corrigées de l’effet du temps ou d’effets explicatifs. On étudie ici l’estimation des
facteurs de cette analyse.
On rappelle dans le paragraphe 2 une présentation de l’ACP d’un vecteur aléatoire (Mon-
nez, 2006).
1.2 Principe et plan de l’étude
Les facteurs de l’ACP du vecteur aléatoireR sont vecteurs propres de la matrice C =MΣ
associés aux valeurs propres rangées par ordre décroissant. On va en effectuer une estimation
en ligne, en actualisant au temps n, après avoir introduit l’observation zn, l’estimation d’un
facteur obtenue au temps n− 1. On fait cette estimation en parallèle avec celle des paramètres
βi, donc des composantes θin de θn. On utilise pour cela des processus d’approximation sto-
chastique de la famille de ceux de Robbins et Monro (1951), Benzécri (1969) et Krasulina
(1970).
On définit les processus dans le paragraphe 3, on donne les théorèmes de convergence
presque sûre dans le paragraphe 4, on étudie le cas particulier où l’on prend pour métriqueM
celle de l’ACP normée dans le paragraphe 5, on donne une conclusion dans le paragraphe 6 et
les démonstrations dans le paragraphe 7.
2 ACP d’un vecteur aléatoire
Soit un vecteur aléatoireR dansRp, défini sur un espace probabilisé (Ω,A, P ), de compo-
santes R1, R2, . . . , RP de carré intégrable. On note Σ la matrice de covariance de R.
OnmunitRp d’unemétriqueM ; on note ‖.‖ la norme associée : ‖R(ω)‖2 = R′(ω)MR(ω).
A partir deM est définie la distance entre deux réalisationsR(ω) et R(ω′) de R qui est la me-
sure de la différence vis-à-vis de R entre les éléments, ou individus, ω et ω′. Le choix de cette
métrique est primordial et conditionne les résultats de l’ACP.
On désigne par Fr un sous-espace affine de Rp de dimension r auquel appartient l’espé-
rance mathématique E [R] de R. On note ΠR le vecteur aléatoire dans Rp qui, à tout ω ∈ Ω,
fait correspondre la projection orthogonale, au sens de la métriqueM ,ΠR(ω) deR(ω) sur Fr .
On a E [R] = E [ΠR] et :
E
[
‖R− E [R]‖2
]
= E
[
‖R−ΠR‖2
]
+ E
[
‖ΠR − E [R]‖2
]
.
2.1 Etude géométrique
L’ACP du vecteur aléatoire R consiste à déterminer un sous-espace Fr qui restitue au
mieux en dimension r la dispersion deRmesurée parE
[
‖R− E (R)‖
2
]
, donc qui soit tel que
E
[
‖ΠR− E (R)‖2
]
soit maximale ouE
[
‖R−ΠR‖2
]
minimale. Si l’on note (u1, u2, . . . , ur)
une baseM -orthonormée de Fr, on a
E
[
‖ΠR− E [R]‖
2
]
=
r∑
k=1
u′kMΣMuk.
J.-M. Monnez
Pour k = 1, 2, . . . , r, on recherche alors un vecteur uk qui rend maximale la forme qua-
dratique u′MΣMu sous les contraintes d’être M -unitaire et M -orthogonal aux vecteurs uj ,
j = 1, . . . , k − 1 ; uk est vecteur propre de la matrice ΣM associé à la kie`me plus grande
valeur propre λk ; on a u′kMΣMuk = λk ; l’axe (E [R] , uk) est appelé le kie`me axe principal
de l’ACP de R.
2.2 Interprétation statistique
La formulation statistique, équivalente à la géométrique, est le cadre usuel de présenta-
tion de l’ACP d’un vecteur aléatoire. Soit l’élément ak = Muk du dual Rp∗ de Rp, ap-
pelé kie`me facteur principal de l’ACP de R. À partir du critère de détermination de uk, on
obtient que ak rend maximale la forme quadratique a′Σa sous les contraintes a′Σaj = 0,
j = 1, 2, . . . , k − 1 et a′M−1a = 1 ; la combinaison linéaire des composantes centrées de R,
Ck = a
′
k(R − E [R]), appelée kie`me composante principale, est donc de variance maximale
sous les contraintes d’être non corrélée aux composantes précédentes et que ak soit M−1-
unitaire. ak est vecteur propre associé à la kie`me plus grande valeur propre λk de la matrice
M−1-symétriqueMΣ et on a a′kΣak = λk.
3 Définition des processus d’approximation stochastique
SoitMn un estimateur deM au temps n.
On note 〈., .〉n le produit scalaire dans le dual Rp∗ de Rp au sens de la métriqueM−1n .
Soit (an) une suite de nombres réels positifs.
Pour i = 1, . . . , p, on définit le processus d’approximation stochastique (Bin) de βi tel que
Bin+1 = B
i
n − anU
i
n(U
i′
nB
i
n − Z
i
n).
On définit :
Θ̂in =
〈
Bin, U
i
n
〉
, Θ̂n = (Θ̂
1
n . . . Θ̂
p
n)
′;
Cn = Mn−1(ZnZ
′
n − Θ̂nΘ̂
′
n).
Soit r le nombre de facteurs à estimer. Pour i = 1, . . . , r, on définit les processus (X in)
d’estimation des facteurs tels que :
Fn(X
i
n) =
〈
CnX
i
n, X
i
n
〉
n−1
〈X in, X
i
n〉n−1
Y in+1 = X
i
n + an(Cn − Fn(X
i
n)I)X
i
n
X in+1 = orthM−1n (Y
i
n+1).
I désigne la matrice-identité d’ordre p.X in+1 = orthM−1n (Y
i
n+1) signifie que (X1n+1, . . . , X in+1)
est obtenu en orthogonalisant par rapport àM−1n au sens de Gram-Schmidt (Y 1n+1, . . . , Y in+1).
Analyse en composantes principales d’un flux de données
4 Théorèmes de convergence presque sûre
4.1 Principe de l’étude de la convergence des processus d’estimation des
facteurs
L’étude de la convergence du type précédent de processus a été faite par Bouamaine et
Monnez (1997, 1998) en se plaçant dans l’algèbre extérieure d’ordre j de Rp∗. On rappelle
d’abord quelques éléments théoriques relatifs à cette algèbre.
4.1.1 Algèbre extérieure d’ordre j de Rp∗
On note ∧ le produit extérieur de vecteurs de Rp∗ et pour j = 1, . . . , p, j∧Rp∗ l’algèbre
extérieure d’ordre j de Rp∗ : (e1, . . . , ep) étant une base de Rp∗, l’ensemble des Cjp produits
extérieurs ei1∧ . . .∧eij pour 1 ≤ i1 < . . . < ij ≤ p est une base de j∧Rp∗.
On définit un produit scalaire dans j∧Rp∗ à partir de celui dans Rp∗ induit par la métrique
M−1 ; dans cette définition, Gj est l’ensemble des permutations σ de {k1, . . . , kj}, s (σ) est
le nombre d’inversions de la permutation σ et ε (σ) = (−1)s(σ) :〈
ei1∧. . .∧eij , ek1∧. . .∧ekj
〉
=
∑
σ∈Gj
ε (σ)
〈
ei1 , eσ(k1)
〉
M−1
. . .
〈
eij , eσ(kj)
〉
M−1
.
On suppose que les r plus grandes valeurs propres de l’endomorphisme C dans Rp∗ sont
différentes : λ1 > . . . > λr. On définit pour j = 1, . . . , r, l’endomorphisme j1C dans j∧Rp∗
par
j1C(x1∧ . . .∧xj) =
j∑
h=1
x1∧ . . .∧Cxh∧ . . .∧xj , xl ∈ Rp∗, l = 1, . . . , j.
Si V 1, . . . , V j sont des vecteurs propres de C correspondant respectivement à λ1, . . . , λj ,
V 1∧. . .∧V j est vecteur propre de j1C correspondant à la plus grande valeur propre λ1j =∑j
l=1 λl. On note jS1 le sous-espace propre correspondant à λ1j et (jS1)⊥ son supplémentaire
orthogonal.
4.1.2 Convergence des processus
On effectue la démonstration de la convergence en deux étapes.
Soit le processus (jXn) dans l’algèbre extérieure d’ordre j de Rp∗ défini par : jXn =
X1n∧. . .∧X
j
n.
On démontre d’abord que, pour j = 1, . . . , r,
jXn
‖jXn‖
converge p.s. dans un ensemble jE
vers V 1∧. . .∧V j ∈ jS1 (on suppose les vecteurs V l normés). On démontre ensuite que, pour
l = 1, . . . , r,
Xln
‖Xln‖
converge p.s. dans ∩lj=1 jE vers V l. On donne ci-dessous la définition de
l’ensemble jE.
Dans le cas où l’on connaît C etM−1, on définit
hj(
jx) =
〈
j1C jx,j x
〉
〈jx,j x〉
,j x ∈ j∧Rp∗,
J.-M. Monnez
et le processus
(
jUn
)
dans j∧Rp∗ par
jUn+1 =
(
I + an
(
j1C − hj(
jUn)I
))
jUn.
Dans ce cas, jE est l’ensemble
{
jX1 /∈
(
jS1
)⊥}
; X11 , . . . , X
j
1 ne doivent pas être or-
thogonaux au sous-espace engendré par les vecteurs propres de B correspondant à ses j plus
grandes valeurs propres.
Le processus (jXn) = (X1n ∧ . . . ∧ Xjn) peut être considéré comme une perturbation
stochastique du processus
(
jUn
)
. On note :
∆nj = 1 + an(λ1j − hj(
jXn))
Qj = jX1 +
∞∑
n=1
jXn+1 −
(
I + an
(
j1C − hj(
jXn)I
))
jXn∏n
i=1∆ij
.
L’ensemble jE est
{
Qj /∈ (jS1)
⊥
}
. On remarque que Qj = jX1 pour (jXn) =
(
jUn
)
.
4.2 Hypothèses
On fait les hypothèses suivantes.
(H1) (a)maxi supn
∥∥U in∥∥ <∞.
(b) Pour i = 1, . . . , p, il existe un entier ri, un réel λi > 0, une suite croissante d’entiers
(nil, l ≥ 1) tels que ni1 = 1, ni,l+1 ≤ nil + ri, λmin(
∑
j∈Iil
U ijU
i′
j ) ≥ λi, avec
Iil = {nil, . . . , ni,l+1 − 1} .
(H2) Mn →M p.s.∑∞
1 an ‖Mn −M‖ <∞ p.s.
(H3) an = cnα , c > 0, 12 < α ≤ 1.
(H3’) an > 0,
∑∞
1 minj∈Il aj =∞,
∑∞
1 a
2
n <∞.
4.3 Théorèmes
On a les énoncés suivants.
Théorème 1. Sous H1 et H3’, pour i = 1, . . . , p, Bin → βi et Θ̂in − θin → 0 p.s.
Théorème 2. Sous H1 et H3, pour i = 1, . . . , p :
1. pour 12 < α ≤ 1 ou (α = 1 et
2λic
ri
> 1) :
lim nαE
[∥∥Bin − βi∥∥2] <∞ et lim nαE [∥∥∥Θ̂in − θin∥∥∥2] <∞ ;
Analyse en composantes principales d’un flux de données
2. pour α = 1 et 2λic
ri
= 1 :
lim
n
lnn
E
[∥∥Bin − βi∥∥2] <∞ et lim nlnnE
[∥∥∥Θ̂in − θin∥∥∥2] <∞ ;
3. pour α = 1 et 2λic
ri
< 1 :
lim n
2λic
ri E
[∥∥Bin − βi∥∥2] <∞ et lim n 2λicri E [∥∥∥Θ̂in − θin∥∥∥2] <∞.
Théorème 3. Sous H1, H2, H3, si l’on suppose que R admet des moments d’ordre 4r,
pourj = 1, . . . , r, Qj converge presque sûrement et, si l’on suppose que les r plus grandes
valeurs propres de C = MΣ sont distinctes, alors, pour i = 1, . . . , r, X in converge presque
sûrement dans ∩ij=1 jE vers un vecteur propre de C associé à sa ie`me plus grande valeur
propre.
5 Cas particulier de la métrique de l’ACP normée
Soit
(
σi
)2
= V ar
[
Ri
]
, i = 1, . . . , p. La métrique de l’ACP normée dans Rp est la
métrique diagonaleM des 1
(σi)2
.
On considère l’estimateur 1
Min
= 1
n
∑n
j=1
(
Zij − Θ̂
i
j
)2
de
(
σi
)2
et la métrique diagonale
Mn dans Rp desM in. On peut calculer 1Min de façon récursive.
Théorème 4. On suppose que R admet des moments d’ordre 4. Alors, sous H1 et H3 avec
α = 1,Mn →M et
∑∞
1 an ‖Mn −M‖ <∞ p.s. ; donc, l’hypothèse H2 est vérifiée.
6 Conclusion et extensions
On a traité ici l’estimation en ligne des facteurs de l’ACP d’un flux de données. On peut
également estimer en ligne les valeurs propres associées (Bouamaine et Monnez, 1998), les
corrélations entre les variables et les facteurs ; on peut aussi estimer en ligne la valeur d’un
facteur pour un individu et procéder éventuellement à une classification des individus.
Cette étude introductive sera développée dans les directions suivantes :
1. étude d’un modèle où l’espérance et la variance varient dans le temps ;
2. étude de modèles non linéaires de variation des paramètres ;
3. autres choix de métriquesM ;
4. application à d’autres méthodes d’analyse factorielle.
J.-M. Monnez
7 Démonstrations
7.1 Démonstration du théorème 1
On a défini : Bin+1 = Bin − anU in(U i′nBin − Zin).
Comme Zin = θin +Rin = U i′nβi +Rin, on a :
Bin+1 − β
i = Bin − β
i − anU
i
nU
i′
n (B
i
n − β
i) + anU
i
nR
i
n.
A i fixé, notons : Yn = Bin − βi, Vn = U in, Sn = Rin, nl = nil, r = ri. On a :
Yn+1 = Yn − anVnV
′
nYn + anVnSn.
Pour établir la convergencepresque sûre, on utilise le lemme suivant (Robbins et Siegmund,
1971).
Lemme 5. Soit (Ω, A, P ) un espace probabilisé, (Tn) une suite croissante de sous-tribus de
A. Soit, pour tout n, αn, βn, γn des variables aléatoires réelles Tn - mesurables, non négatives,
intégrables, telles que
E [αn+1 | Tn] ≤ αn(1 + βn) + γn − δn,
∞∑
1
βn < ∞,
∞∑
1
γn <∞ p.s.
Alors, la suite (αn) converge presque sûrement vers une variable aléatoire α finie et on a∑∞
1 δn <∞ p.s.
1. On a :
‖Yn+1‖
2
= ‖Yn‖
2
+ a2n ‖VnV
′
nYn − VnSn‖
2
+ 2an 〈Yn, VnSn〉
−2an 〈Yn, VnV
′
nYn〉
≤ ‖Yn‖
2
+ 2a2n ‖Vn‖
2
‖Yn‖
2
+ 2a2n ‖Vn‖
2
S2n + 2an 〈Yn, VnSn〉
−2an 〈Yn, VnV
′
nYn〉 .
Soit Tn la tribu du passé au temps n, par rapport à laquelle Y1, . . . , Yn sont mesurables.
On a E [Sn | Tn] = E
[
Ri
]
= 0, E
[
S2n | Tn
]
= E
[
(Ri)2
]
.
E
[
‖Yn+1‖
2 | Tn
]
≤ (1 + 2a2n ‖Vn‖
2) ‖Yn‖
2 + 2a2n ‖Vn‖
2E
[
S2
]
−2an 〈Yn, VnV
′
nYn〉 p.s.
Sous les hypothèses H1a et H3’, on a :
∑∞
1 a
2
n ‖Vn‖
2
<∞. D’après le lemme 5 :
∃T ≥ 0 : ‖Yn‖
2
→ T p.s.;
∞∑
1
an 〈Yn, VnV
′
nYn〉 <∞ p.s.
Analyse en composantes principales d’un flux de données
2. On a : ‖Yn+1 − Yn‖ ≤ an ‖Vn‖2 ‖Yn‖+ an ‖VnSn‖ .
E
[∑∞
1 a
2
n ‖VnSn‖
2
]
=
∑∞
1 a
2
n ‖Vn‖
2
E
[
S2
]
<∞.
Donc :
∑∞
1 a
2
n ‖VnSn‖
2 <∞ p.s. ; an ‖VnSn‖ → 0 p.s.
Sous H1a et H3, an ‖Vn‖2 → 0 p.s.
On en déduit que ‖Yn+1 − Yn‖ → 0 p.s.
3. On raisonne à ω fixé, appartenant à l’intersection des ensembles de convergence presque
sûre définis. Supposons T (ω) 6= 0.
On supprime dans la suite l’écriture de ω.
Alors : ∃0 < 1 < 1, ∃N(1) : ∀n > N(1), 1 < ‖Yn‖ < 11 .
Donc, sous H1b, à partir d’un certain rang L, on a :〈
Ynl ,
∑
j∈Il
VjV
′
j Ynl
〉
≥ λ21.
On en déduit qu’il existe un entierml ∈ Il tel que :〈
Ynl , VmlV
′
ml
Ynl
〉
≥
λ21
r
.
On considère la décomposition :〈
Yml , VmlV
′
ml
Yml
〉
=
〈
Yml + Ynl , VmlV
′
ml
(Yml − Ynl)
〉
+
〈
Ynl , VmlV
′
ml
Ynl
〉
.
Soit  > 0 tel que  ≤ λ
3
1
4r2C2 , avecC = supn ‖Vn‖ <∞ sous H1a. A partir d’un certain
rang : ‖Yn+1 − Yn‖ <  ; donc : ‖Yml − Ynl‖ < r ;
=⇒
∣∣〈Yml + Ynl , VmlV ′ml(Yml − Ynl)〉∣∣ < 2 11C2r ≤ λ
2
1
2r
=⇒
〈
Yml , VmlV
′
ml
Yml
〉
>
λ21
2r
.
Sous H3’, on a alors :
∑∞
l=1 aml
〈
Yml , VmlV
′
ml
Yml
〉
=∞. Donc :∑∞
n=1 an 〈Yn, VnV
′
nYn〉 =∞ ; il y a contradiction. Par conséquent : T (ω) = 0.
7.2 Démonstration du théorème 2
1. On reprend les notations du théorème 1. D’après la partie 1 de sa démonstration, on a :
E
[
‖Yn+1‖
2
]
≤ (1 + 2a2n ‖Vn‖
2
)E
[
‖Yn‖
2
]
+ 2a2n ‖Vn‖
2
E
[
S2
]
−2anE [〈Yn, VnV
′
nYn〉] .
D’après le lemme 5 :
∃t ≥ 0 : E
[
‖Yn‖
2
]
→ t ;
∞∑
1
anE [〈Yn, VnV
′
nYn〉] <∞.
J.-M. Monnez
2. Donc, il existe b > 0 tel que, avec µl = minj∈Il aj :
E
[
‖Yn+1‖
2
]
≤ E
[
‖Yn‖
2
]
+ ba2n ‖Vn‖
2
− 2anE [〈Yn, VnV
′
nYn〉]
E
[∥∥Ynl+1∥∥2] ≤ E [‖Ynl‖2]+ b∑
j∈Il
a2j ‖Vj‖
2
− 2µl
∑
j∈Il
E
[〈
Yj , VjV
′
j Yj
〉]
.
∑
j∈Il
E
[〈
Yj , VjV
′
j Yj
〉]
=
∑
j∈Il
E
[〈
Ynl , VjV
′
j Ynl
〉]
+
∑
j∈Il
E
[〈
Yj + Ynl , VjV
′
j (Yj − Ynl)
〉]
Sous H1b :
∑
j∈Il
E
[〈
Ynl , VjV
′
j Ynl
〉]
≥ λE
[
‖Ynl‖
2
]
.
On note C = supn ‖Vn‖. Il existe a > 0 tel que :∑
j∈Il
∣∣E [〈Yj + Ynl , VjV ′j (Yj − Ynl)〉]∣∣
≤
∑
j∈Il
E
[
‖Yj + Ynl‖ ‖Vj‖
2
‖Yj − Ynl‖
]
≤ C2
∑
j∈Il
(
E
[
‖Yj + Ynl‖
2
]) 1
2
(
E
[
‖Yj − Ynl‖
2
]) 1
2
≤ a2
∑
j∈Il
(
E
[
‖Yj − Ynl‖
2
]) 1
2
Or : Yj − Ynl =
∑j−1
k=nl
(−akVkV
′
kYk + akVkSk).
‖Yj − Ynl‖ ≤
∑j−1
k=nl
(akC
2 ‖Yk‖+ akC ‖Sk‖).
Il existe d > 0 tel que : E
[
‖Yj − Ynl‖
2
]
≤ d
∑
k∈Il
a2k ≤ drmaxk∈Il a
2
k.
Donc, il existe f > 0 tel que :∑
j∈Il
∣∣E [〈Yj + Ynl , VjV ′j (Yj − Ynl)〉]∣∣ ≤ f max
k∈Il
ak.
Par conséquent, il existe g > 0 tel que :
E
[∥∥Ynl+1∥∥2] ≤ (1 − 2λµl)E [‖Ynl‖2]+ brC2max
k∈Il
a2k + 2fµlmax
k∈Il
ak
≤ (1 − 2λµl)E
[
‖Ynl‖
2
]
+ gmax
k∈Il
a2k.
Or : µl = c(nl+1−1)α ≥
c
(lr)α ,maxk∈Il ak =
c
(nl)
α ≤ clα .
Donc, il existe h > 0 tel que :
E
[∥∥Ynl+1∥∥2] ≤ (1− 2λcrα 1lα
)
E
[
‖Ynl‖
2
]
+
h
l2α
.
Analyse en composantes principales d’un flux de données
3. Dans le cas 12 < α < 1, on applique un lemme de Schmetterer (1969) :
limlαE
[
‖Ynl‖
2
]
<∞.
Dans le cas α = 1, on applique un lemme de Venter (1966) :
pour
2λc
r
> 1, limlE
[
‖Ynl‖
2
]
<∞ ;
pour
2λc
r
= 1, lim
l
ln l
E
[
‖Ynl‖
2
]
<∞ ;
pour
2λc
r
< 1, liml
2λc
r E
[
‖Ynl‖
2
]
<∞.
Comme E
[
‖Yn+1‖
2
]
≤ E
[
‖Yn‖
2
]
+ ba2n ‖Vn‖
2
, on a pour n ∈ Il :
E
[
‖Yn‖
2
]
≤ E
[
‖Ynl‖
2
]
+
h
l2α
.
Comme l ≤ n ≤ lr, on obtient des résutats semblables aux précédents pour E
[
‖Yn‖
2
]
,
en remplaçant l et nl par n.
7.3 Démonstration du théorème 3
D’après le théorème 4 de Bouamaine et Monnez (1998), on a les conclusions de ce théo-
rème sous les hypothèses :
1.
∑∞
1 an ‖E [Cn | Tn]− C‖ <∞ p.s.
2. Pour j = 2, . . . , 2r,
∑∞
1 a
j
nE
[
‖Cn − C‖
j
| Tn
]
<∞ p.s.
3. Mn−1 est Tn-mesurable ;Mn →M p.s. ;
∑∞
1 an ‖Mn−1 −M‖ <∞ p.s.
4. an > 0,
∑∞
1 an =∞,
∑∞
1 a
2
n <∞.
On vérifie l’hypothèse 1.
Cn = Mn−1(ZnZ
′
n − Θ̂nΘ̂
′
n) ; C =MΣ =M(E [ZnZ
′
n]− θnθ
′
n).
Cn − C = Mn−1(ZnZ
′
n − E [ZnZ
′
n]) + (Mn−1 −M)(E [ZnZ
′
n]− θnθ
′
n)
−Mn−1(Θ̂n − θn)Θ̂
′
n −Mn−1θn(Θ̂n − θn)
′.
E [Cn | Tn]− C = (Mn−1 −M)Σ−Mn−1(Θ̂n − θn)Θ̂
′
n −Mn−1θn(Θ̂n − θn)
′.
D’après les conclusions du théorème 2, en utilisant la norme euclidienne usuelle dans Rp,
on a dans tous les cas :
E
[
∞∑
1
1
nα
∥∥∥Θ̂n − θn∥∥∥
]
≤
∞∑
1
1
nα
(
E
[∥∥∥Θ̂n − θn∥∥∥2]) 12
≤
p∑
i=1
∞∑
n=1
1
nα
(
E
[∥∥∥Θ̂in − θin∥∥∥2]) 12 <∞.
J.-M. Monnez
Donc :
∑∞
1
1
nα
∥∥∥Θ̂n − θn∥∥∥ <∞ p.s.
D’après le théorème 1, Θ̂n − θn → 0 p.s.
Sous H2, on a alors :
∞∑
1
1
nα
‖E [Cn | Tn]− C‖ <∞ p.s.
On vérifie l’hypothèse 2 en écrivant que :
‖Cn − C‖
j ≤ 4j−1(‖Mn−1‖
j ‖ZnZ
′
n − E [ZnZ
′
n]‖+ ‖Mn−1 −M‖
j ‖Σ‖j
+ ‖Mn−1‖
j
∥∥∥Θ̂n − θn∥∥∥j ∥∥∥Θ̂n∥∥∥j + ‖Mn−1‖j ‖θn‖j ∥∥∥Θ̂n − θn∥∥∥j),
∞∑
1
ajn ‖Mn −M‖
j
< ∞,
∞∑
1
ajn
∥∥∥Θ̂n − θn∥∥∥j <∞, ∞∑
1
ajn <∞,
Mn → M, Θ̂n − θn → 0 p.s.
7.4 Démonstration du théorème 4
1. Montrons que, pour i = 1, . . . , p :
1
n
n∑
j=1
(Zij − Θ̂
i
j)
2 → (σi)2 p.s.
1
n
n∑
j=1
(Zij − Θ̂
i
j)
2 =
1
n
n∑
j=1
((Zij − θ
i
j) + (θ
i
j − Θ̂
i
j))
2
=
1
n
n∑
j=1
(Zij − θ
i
j)
2 +
2
n
n∑
j=1
(Zij − θ
i
j)(θ
i
j − Θ̂
i
j) +
1
n
n∑
j=1
(θij − Θ̂
i
j)
2.
Les Zij − θij constituent un échantillon i.i.d. de Ri. Donc : 1n
∑n
j=1(Z
i
j − θ
i
j)
2 →
(σi)2 p.s.
On a : θij − Θ̂ij → 0 p.s. ; donc : 1n
∑n
j=1(θ
i
j − Θ̂
i
j)
2 → 0 p.s.
Notons V in = (Zin − θin)(θin − Θ̂in),W in+1 = 1n
∑n
j=1 V
i
j ,W
i
1 = 0. On a :
W in+1 = (1−
1
n
)W in +
1
n
V in
(W in+1)
2 = (1 +
1
n2
)(W in)
2 + 2(1−
1
n
)
1
n
V inW
i
n +
1
n2
(V in)
2 −
2
n
(W in)
2.
Analyse en composantes principales d’un flux de données
Soit Tn la tribu du passé au temps n.
E
[
V in | Tn
]
= E
[
Zin − θ
i
n | Tn
]
(θin − Θ̂
i
n) = E
[
Zin − θ
i
n
]
(θin − Θ̂
i
n) = 0.
E
[
(W in+1)
2 | Tn
]
= (1 +
1
n2
)(W in)
2 +
1
n2
E
[
(V in)
2 | Tn
]
−
2
n
(W in)
2 p.s.
Or : E
[
(V in)
2 | Tn
]
= E
[
(Zin − θ
i
n)
2 | Tn
]
(θin − Θ̂
i
n)
2 = E
[
(Ri)2
]
(θin − Θ̂
i
n)
2
.
Donc :
∑∞
1
1
n2
E
[
(V in)
2 | Tn
]
= E
[
(Ri)2
]∑∞
1
1
n2
(θin − Θ̂
i
n)
2 <∞ p.s.
En appliquant le lemme 5, on obtient :
∃T i ≥ 0 : (W in)
2 → T i p.s. ;
∞∑
1
1
n
(W in)
2 <∞ p.s. =⇒W in → 0 p.s.
Par conséquent : 1
n
∑n
j=1(Z
i
j − Θ̂
i
j)
2 → (σi)2 p.s.
On en déduit que :Mn →M p.s.
2. On utilise dans la suite le lemme suivant.
Lemme 6. Soit, pour tout n ≥ 1 : wn+1 = (1− an)wn+ anun, w1 = 0, un >
0, 0 < an < 1. Si
∑∞
1 an = ∞ et
∑∞
1 anun < ∞, alors
∑∞
1 anwn < ∞ et
wn → 0.
On remarque que, pour an = 1n , wn+1 =
1
n
∑n
j=1 uj .
Démonstration. Pour n > 1, wn > 0. D’après le lemme 5, il existe w ≥ 0 : wn → w
et
∑∞
1 anwn <∞ ; comme
∑∞
1 an =∞, on a w = 0.
3. Montrons que, pour i = 1, . . . , p,
∑∞
n=1
1
n
∣∣∣ 1n∑nj=1(Zij − Θ̂ij)2 − (σi)2∣∣∣ <∞ p.s.
On utilise la décomposition de 1
n
∑n
j=1(Z
i
j − Θ̂
i
j)
2 vue dans la première partie de la
démonstration. Soit µi4 le moment centré d’ordre 4 de Ri. On a :
E
( 1
n
n∑
j=1
(Zij − θ
i
j)
2 − (σi)2)2
 = 1
n2
n∑
j=1
V ar
[
(Zij − θ
i
j)
2
]
=
µi4 − (σ
i)4
n
,
∞∑
1
1
n
E

∣∣∣∣∣∣ 1n
n∑
j=1
(Zij − θ
i
j)
2 − (σi)2
∣∣∣∣∣∣
 <∞.
D’après le théorème 2, on a :
∑∞
1
1
n
E
[
(θin − Θ̂
i
n)
2
]
<∞. On déduit du lemme 6 que :∑∞
n=1
1
n
E
[
1
n
∑n
j=1(θ
i
j − Θ̂
i
j)
2
]
<∞.
∞∑
1
1
n
E
[∣∣∣(Zin − θin)(θin − Θ̂in)∣∣∣] ≤ ∞∑
1
1
n
(E
[
(Zin − θ
i
n)
2
]
)
1
2
(
E
[
(θin − Θ̂
i
n)
2
]) 1
2
≤ (E
[
(Ri)2
]
)
1
2
∞∑
1
1
n
(
E
[
(θin − Θ̂
i
n)
2
]) 1
2
<∞.
J.-M. Monnez
On déduit du lemme 6 que :
∞∑
n=1
1
n
E
 1
n
n∑
j=1
∣∣∣(Zij − θij)(θij − Θ̂ij)∣∣∣
 <∞ p.s.
On déduit des trois conclusions précédentes que :
∞∑
n=1
1
n
E

∣∣∣∣∣∣ 1n
n∑
j=1
(Zij − Θ̂
i
j)
2 − (σi)2
∣∣∣∣∣∣
 < ∞
∞∑
n=1
1
n
∣∣∣∣∣∣ 1n
n∑
j=1
(Zij − Θ̂
i
j)
2 − (σi)2
∣∣∣∣∣∣ < ∞ p.s.
Par conséquent, presque sûrement,
∑∞
n=1
1
n
∥∥M−1n −M−1∥∥ <∞ et
∞∑
n=1
1
n
‖Mn −M‖ =
∞∑
n=1
1
n
∥∥Mn(M−1n −M−1)M∥∥ <∞.
Références
Aguilar-Ruiz, J. (2006). Recent advances in data stream mining. In 38e`mes Journées de
Statistique de la SFDS, Clamart.
Benzécri, J. (1969). Approximation stochastique dans une algèbre normée non commutative.
Bull. Soc. Math. France 97, 225–241.
Bouamaine, A. (1996). Méthodes d’approximation stochastique en analyse des données. Ph.
D. thesis, thèse de doctorat d’Etat ès Sciences Appliquées, Université Mohammed V, EMI,
Rabat.
Bouamaine, A. et J. Monnez (1997). Convergence d’une classe de processus d’approximation
stochastique de vecteurs propres. Pub. Inst. Stat. Univ. Paris XXXXI(1-2), 97–117.
Bouamaine, A. et J. Monnez (1998). Approximation stochastique de vecteurs et valeurs
propres. Pub. Inst. Stat. Univ. Paris XXXXII(2-3), 15–38.
D’Aubigny, G. (2001). Data mining et statistique, discussion et commentaires. Journal de la
Société Française de Statistique 142(1), 37–52.
Krasulina, T. (1970). Method of stochastic approximation in the determination of the largest
eigenvalue of the mathematical expectation of random matrices. Automation and Remote
Control 2, 215–221.
Lebart, L. (1974). On the benzécri’s method for computing eigenvectors by stochastic approxi-
mation (the case of binary data). In P. Verlag (Ed.), Proceedings in Computational Statistics,
Vienne, pp. 202–211.
MacGregor, J. (1997). Using on-line process data to improve quality : challenges for statisti-
cians. International Statistical Review 65(3), 309–323.
Analyse en composantes principales d’un flux de données
Monnez, J. (1994). Convergence d’un processus d’approximation stochastique en analyse
factorielle. Pub. Inst. Stat. Univ. Paris XXXVIII(1), 37–56.
Monnez, J. (2006). Approximation stochastique en analyse factorielle multiple. Pub. Inst. Stat.
Univ. Paris L(3), 27–45.
Robbins, H. et S. Monro (1951). A stochastic approximation method. Ann. Math. Stat. 22,
400–407.
Robbins, H. et D. Siegmund (1971). A convergence theorem for nonnegative almost super-
martingales and some applications (Rustagi, J.S. ed.)., pp. 233–257. Academic Press, New
York.
Schmetterer, L. (1969). Multidimensional stochastic approximation. In A. Press (Ed.),Multi-
variate Analysis II, Proc. 2nd Int. Symp., Dayton, Ohio, pp. 443–460.
Venter, J. (1966). On dvoretzky stochastic approximation theorems. Ann. Math. Stat. 37,
1534–1544.
Summary
We consider a data stream and suppose that each data is a realization of a random vector
whose expectation varies in time according to a linear model for each component. We use
stochastic approximation processes to estimate on line the parameters of the linear models and
simultaneously the principal components of the data.
