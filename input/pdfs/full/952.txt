Analyse en composantes principales dâ€™un flux de donnÃ©es
dâ€™espÃ©rance variable dans le temps
Jean-Marie Monnez
Institut Elie Cartan UMR 7502 â€“ Laboratoire de MathÃ©matiques
Nancy-UniversitÃ©, CNRS, INRIA
BP 239 â€“ F 54506 â€“ Vandoeuvre-lÃ¨s-Nancy Cedex
jean-marie.monnez@iecn.u-nancy.fr
RÃ©sumÃ©. On considÃ¨re un flux de donnÃ©es reprÃ©sentÃ© par une suite de vecteurs
de donnÃ©es. On suppose que chaque vecteur de donnÃ©es est une rÃ©alisation dâ€™un
vecteur alÃ©atoire dont lâ€™espÃ©rance mathÃ©matique varie dans le temps selon un
modÃ¨le linÃ©aire pour chacune des composantes. On utilise des processus dâ€™ap-
proximation stochastique pour estimer en ligne les paramÃ¨tres des modÃ¨les li-
nÃ©aires et en mÃªme temps les facteurs de lâ€™ACP du vecteur alÃ©atoire.
1 ModÃ¨le de flux et plan dâ€™Ã©tude
Soit un flux de donnÃ©es, reprÃ©sentÃ© par une suite de vecteurs (z1, . . . , zn, . . .) dans Rp.
1.1 ModÃ¨le dâ€™Ã©tude et formulation
On suppose que :
â€“ pour tout n, zn est la rÃ©alisation dâ€™un vecteur alÃ©atoire Zn, dâ€™espÃ©rance mathÃ©matique
variable dans le temps ;
â€“ les vecteurs alÃ©atoires Zn sont mutuellement indÃ©pendants ;
â€“ pour tout n, on a la dÃ©compositionZn = Î¸n+Rn, Î¸n = (Î¸1n . . . Î¸pn)â€² Ã©tant un vecteur de
R
p
, la loi du vecteur alÃ©atoire Rn ne dÃ©pendant pas de n, E [Rn] = 0, Covar [Rn] = Î£
(matrice de covariance deRn) ; ceci revient Ã  supposer que lesRn = Znâˆ’Î¸n constituent
un Ã©chantillon i.i.d. dâ€™un vecteur alÃ©atoire R dans Rp tel que E [R] = 0, Covar [R] =
Î£ ; on a alors E [Zn] = Î¸n, Covar [Zn] = Î£ ; rn = zn âˆ’ E [Zn] reprÃ©sente la donnÃ©e
zn centrÃ©e ;
â€“ pour i = 1, . . . , p, il existe un vecteur Î²i de Rni inconnu et , pour tout n, un vecteur
U in de Rni connu au temps n tels que Î¸in =
âŒ©
Î²i, U in
âŒª
, ã€ˆ., .ã€‰ dÃ© signant le produit scalaire
euclidien usuel dans Rni ; U in peut Ãªtre un vecteur de fonctions connues du temps (Î¸in
est alors une combinaison linÃ©aire de fonctions connues du temps) ou un vecteur de
valeurs de variables explicatives contrÃ´lÃ©es ; si lâ€™on note Zin, respectivementRin, la ie`me
composante de Zn, respectivementRn, on a alors le modÃ¨le de rÃ©gression linÃ©aire
Zin =
âŒ©
Î²i, U in
âŒª
+Rin, i = 1, . . . , p.
Analyse en composantes principales dâ€™un flux de donnÃ©es
On pose le problÃ¨me suivant : rÃ©aliser une analyse en composantes principales (ACP) du
vecteur alÃ©atoire R dans Rp que lâ€™on munit dâ€™une mÃ©trique M ; on effectue ainsi une ACP
de donnÃ©es corrigÃ©es de lâ€™effet du temps ou dâ€™effets explicatifs. On Ã©tudie ici lâ€™estimation des
facteurs de cette analyse.
On rappelle dans le paragraphe 2 une prÃ©sentation de lâ€™ACP dâ€™un vecteur alÃ©atoire (Mon-
nez, 2006).
1.2 Principe et plan de lâ€™Ã©tude
Les facteurs de lâ€™ACP du vecteur alÃ©atoireR sont vecteurs propres de la matrice C =MÎ£
associÃ©s aux valeurs propres rangÃ©es par ordre dÃ©croissant. On va en effectuer une estimation
en ligne, en actualisant au temps n, aprÃ¨s avoir introduit lâ€™observation zn, lâ€™estimation dâ€™un
facteur obtenue au temps nâˆ’ 1. On fait cette estimation en parallÃ¨le avec celle des paramÃ¨tres
Î²i, donc des composantes Î¸in de Î¸n. On utilise pour cela des processus dâ€™approximation sto-
chastique de la famille de ceux de Robbins et Monro (1951), BenzÃ©cri (1969) et Krasulina
(1970).
On dÃ©finit les processus dans le paragraphe 3, on donne les thÃ©orÃ¨mes de convergence
presque sÃ»re dans le paragraphe 4, on Ã©tudie le cas particulier oÃ¹ lâ€™on prend pour mÃ©triqueM
celle de lâ€™ACP normÃ©e dans le paragraphe 5, on donne une conclusion dans le paragraphe 6 et
les dÃ©monstrations dans le paragraphe 7.
2 ACP dâ€™un vecteur alÃ©atoire
Soit un vecteur alÃ©atoireR dansRp, dÃ©fini sur un espace probabilisÃ© (â„¦,A, P ), de compo-
santes R1, R2, . . . , RP de carrÃ© intÃ©grable. On note Î£ la matrice de covariance de R.
OnmunitRp dâ€™unemÃ©triqueM ; on note â€–.â€– la norme associÃ©e : â€–R(Ï‰)â€–2 = Râ€²(Ï‰)MR(Ï‰).
A partir deM est dÃ©finie la distance entre deux rÃ©alisationsR(Ï‰) et R(Ï‰â€²) de R qui est la me-
sure de la diffÃ©rence vis-Ã -vis de R entre les Ã©lÃ©ments, ou individus, Ï‰ et Ï‰â€². Le choix de cette
mÃ©trique est primordial et conditionne les rÃ©sultats de lâ€™ACP.
On dÃ©signe par Fr un sous-espace affine de Rp de dimension r auquel appartient lâ€™espÃ©-
rance mathÃ©matique E [R] de R. On note Î R le vecteur alÃ©atoire dans Rp qui, Ã  tout Ï‰ âˆˆ â„¦,
fait correspondre la projection orthogonale, au sens de la mÃ©triqueM ,Î R(Ï‰) deR(Ï‰) sur Fr .
On a E [R] = E [Î R] et :
E
[
â€–Râˆ’ E [R]â€–2
]
= E
[
â€–Râˆ’Î Râ€–2
]
+ E
[
â€–Î R âˆ’ E [R]â€–2
]
.
2.1 Etude gÃ©omÃ©trique
Lâ€™ACP du vecteur alÃ©atoire R consiste Ã  dÃ©terminer un sous-espace Fr qui restitue au
mieux en dimension r la dispersion deRmesurÃ©e parE
[
â€–Râˆ’ E (R)â€–
2
]
, donc qui soit tel que
E
[
â€–Î Râˆ’ E (R)â€–2
]
soit maximale ouE
[
â€–Râˆ’Î Râ€–2
]
minimale. Si lâ€™on note (u1, u2, . . . , ur)
une baseM -orthonormÃ©e de Fr, on a
E
[
â€–Î Râˆ’ E [R]â€–
2
]
=
râˆ‘
k=1
uâ€²kMÎ£Muk.
J.-M. Monnez
Pour k = 1, 2, . . . , r, on recherche alors un vecteur uk qui rend maximale la forme qua-
dratique uâ€²MÎ£Mu sous les contraintes dâ€™Ãªtre M -unitaire et M -orthogonal aux vecteurs uj ,
j = 1, . . . , k âˆ’ 1 ; uk est vecteur propre de la matrice Î£M associÃ© Ã  la kie`me plus grande
valeur propre Î»k ; on a uâ€²kMÎ£Muk = Î»k ; lâ€™axe (E [R] , uk) est appelÃ© le kie`me axe principal
de lâ€™ACP de R.
2.2 InterprÃ©tation statistique
La formulation statistique, Ã©quivalente Ã  la gÃ©omÃ©trique, est le cadre usuel de prÃ©senta-
tion de lâ€™ACP dâ€™un vecteur alÃ©atoire. Soit lâ€™Ã©lÃ©ment ak = Muk du dual Rpâˆ— de Rp, ap-
pelÃ© kie`me facteur principal de lâ€™ACP de R. Ã€ partir du critÃ¨re de dÃ©termination de uk, on
obtient que ak rend maximale la forme quadratique aâ€²Î£a sous les contraintes aâ€²Î£aj = 0,
j = 1, 2, . . . , k âˆ’ 1 et aâ€²Mâˆ’1a = 1 ; la combinaison linÃ©aire des composantes centrÃ©es de R,
Ck = a
â€²
k(R âˆ’ E [R]), appelÃ©e kie`me composante principale, est donc de variance maximale
sous les contraintes dâ€™Ãªtre non corrÃ©lÃ©e aux composantes prÃ©cÃ©dentes et que ak soit Mâˆ’1-
unitaire. ak est vecteur propre associÃ© Ã  la kie`me plus grande valeur propre Î»k de la matrice
Mâˆ’1-symÃ©triqueMÎ£ et on a aâ€²kÎ£ak = Î»k.
3 DÃ©finition des processus dâ€™approximation stochastique
SoitMn un estimateur deM au temps n.
On note ã€ˆ., .ã€‰n le produit scalaire dans le dual Rpâˆ— de Rp au sens de la mÃ©triqueMâˆ’1n .
Soit (an) une suite de nombres rÃ©els positifs.
Pour i = 1, . . . , p, on dÃ©finit le processus dâ€™approximation stochastique (Bin) de Î²i tel que
Bin+1 = B
i
n âˆ’ anU
i
n(U
iâ€²
nB
i
n âˆ’ Z
i
n).
On dÃ©finit :
Î˜Ì‚in =
âŒ©
Bin, U
i
n
âŒª
, Î˜Ì‚n = (Î˜Ì‚
1
n . . . Î˜Ì‚
p
n)
â€²;
Cn = Mnâˆ’1(ZnZ
â€²
n âˆ’ Î˜Ì‚nÎ˜Ì‚
â€²
n).
Soit r le nombre de facteurs Ã  estimer. Pour i = 1, . . . , r, on dÃ©finit les processus (X in)
dâ€™estimation des facteurs tels que :
Fn(X
i
n) =
âŒ©
CnX
i
n, X
i
n
âŒª
nâˆ’1
ã€ˆX in, X
i
nã€‰nâˆ’1
Y in+1 = X
i
n + an(Cn âˆ’ Fn(X
i
n)I)X
i
n
X in+1 = orthMâˆ’1n (Y
i
n+1).
I dÃ©signe la matrice-identitÃ© dâ€™ordre p.X in+1 = orthMâˆ’1n (Y
i
n+1) signifie que (X1n+1, . . . , X in+1)
est obtenu en orthogonalisant par rapport Ã Mâˆ’1n au sens de Gram-Schmidt (Y 1n+1, . . . , Y in+1).
Analyse en composantes principales dâ€™un flux de donnÃ©es
4 ThÃ©orÃ¨mes de convergence presque sÃ»re
4.1 Principe de lâ€™Ã©tude de la convergence des processus dâ€™estimation des
facteurs
Lâ€™Ã©tude de la convergence du type prÃ©cÃ©dent de processus a Ã©tÃ© faite par Bouamaine et
Monnez (1997, 1998) en se plaÃ§ant dans lâ€™algÃ¨bre extÃ©rieure dâ€™ordre j de Rpâˆ—. On rappelle
dâ€™abord quelques Ã©lÃ©ments thÃ©oriques relatifs Ã  cette algÃ¨bre.
4.1.1 AlgÃ¨bre extÃ©rieure dâ€™ordre j de Rpâˆ—
On note âˆ§ le produit extÃ©rieur de vecteurs de Rpâˆ— et pour j = 1, . . . , p, jâˆ§Rpâˆ— lâ€™algÃ¨bre
extÃ©rieure dâ€™ordre j de Rpâˆ— : (e1, . . . , ep) Ã©tant une base de Rpâˆ—, lâ€™ensemble des Cjp produits
extÃ©rieurs ei1âˆ§ . . .âˆ§eij pour 1 â‰¤ i1 < . . . < ij â‰¤ p est une base de jâˆ§Rpâˆ—.
On dÃ©finit un produit scalaire dans jâˆ§Rpâˆ— Ã  partir de celui dans Rpâˆ— induit par la mÃ©trique
Mâˆ’1 ; dans cette dÃ©finition, Gj est lâ€™ensemble des permutations Ïƒ de {k1, . . . , kj}, s (Ïƒ) est
le nombre dâ€™inversions de la permutation Ïƒ et Îµ (Ïƒ) = (âˆ’1)s(Ïƒ) :âŒ©
ei1âˆ§. . .âˆ§eij , ek1âˆ§. . .âˆ§ekj
âŒª
=
âˆ‘
ÏƒâˆˆGj
Îµ (Ïƒ)
âŒ©
ei1 , eÏƒ(k1)
âŒª
Mâˆ’1
. . .
âŒ©
eij , eÏƒ(kj)
âŒª
Mâˆ’1
.
On suppose que les r plus grandes valeurs propres de lâ€™endomorphisme C dans Rpâˆ— sont
diffÃ©rentes : Î»1 > . . . > Î»r. On dÃ©finit pour j = 1, . . . , r, lâ€™endomorphisme j1C dans jâˆ§Rpâˆ—
par
j1C(x1âˆ§ . . .âˆ§xj) =
jâˆ‘
h=1
x1âˆ§ . . .âˆ§Cxhâˆ§ . . .âˆ§xj , xl âˆˆ Rpâˆ—, l = 1, . . . , j.
Si V 1, . . . , V j sont des vecteurs propres de C correspondant respectivement Ã  Î»1, . . . , Î»j ,
V 1âˆ§. . .âˆ§V j est vecteur propre de j1C correspondant Ã  la plus grande valeur propre Î»1j =âˆ‘j
l=1 Î»l. On note jS1 le sous-espace propre correspondant Ã  Î»1j et (jS1)âŠ¥ son supplÃ©mentaire
orthogonal.
4.1.2 Convergence des processus
On effectue la dÃ©monstration de la convergence en deux Ã©tapes.
Soit le processus (jXn) dans lâ€™algÃ¨bre extÃ©rieure dâ€™ordre j de Rpâˆ— dÃ©fini par : jXn =
X1nâˆ§. . .âˆ§X
j
n.
On dÃ©montre dâ€™abord que, pour j = 1, . . . , r,
jXn
â€–jXnâ€–
converge p.s. dans un ensemble jE
vers V 1âˆ§. . .âˆ§V j âˆˆ jS1 (on suppose les vecteurs V l normÃ©s). On dÃ©montre ensuite que, pour
l = 1, . . . , r,
Xln
â€–Xlnâ€–
converge p.s. dans âˆ©lj=1 jE vers V l. On donne ci-dessous la dÃ©finition de
lâ€™ensemble jE.
Dans le cas oÃ¹ lâ€™on connaÃ®t C etMâˆ’1, on dÃ©finit
hj(
jx) =
âŒ©
j1C jx,j x
âŒª
ã€ˆjx,j xã€‰
,j x âˆˆ jâˆ§Rpâˆ—,
J.-M. Monnez
et le processus
(
jUn
)
dans jâˆ§Rpâˆ— par
jUn+1 =
(
I + an
(
j1C âˆ’ hj(
jUn)I
))
jUn.
Dans ce cas, jE est lâ€™ensemble
{
jX1 /âˆˆ
(
jS1
)âŠ¥}
; X11 , . . . , X
j
1 ne doivent pas Ãªtre or-
thogonaux au sous-espace engendrÃ© par les vecteurs propres de B correspondant Ã  ses j plus
grandes valeurs propres.
Le processus (jXn) = (X1n âˆ§ . . . âˆ§ Xjn) peut Ãªtre considÃ©rÃ© comme une perturbation
stochastique du processus
(
jUn
)
. On note :
âˆ†nj = 1 + an(Î»1j âˆ’ hj(
jXn))
Qj = jX1 +
âˆâˆ‘
n=1
jXn+1 âˆ’
(
I + an
(
j1C âˆ’ hj(
jXn)I
))
jXnâˆn
i=1âˆ†ij
.
Lâ€™ensemble jE est
{
Qj /âˆˆ (jS1)
âŠ¥
}
. On remarque que Qj = jX1 pour (jXn) =
(
jUn
)
.
4.2 HypothÃ¨ses
On fait les hypothÃ¨ses suivantes.
(H1) (a)maxi supn
âˆ¥âˆ¥U inâˆ¥âˆ¥ <âˆ.
(b) Pour i = 1, . . . , p, il existe un entier ri, un rÃ©el Î»i > 0, une suite croissante dâ€™entiers
(nil, l â‰¥ 1) tels que ni1 = 1, ni,l+1 â‰¤ nil + ri, Î»min(
âˆ‘
jâˆˆIil
U ijU
iâ€²
j ) â‰¥ Î»i, avec
Iil = {nil, . . . , ni,l+1 âˆ’ 1} .
(H2) Mn â†’M p.s.âˆ‘âˆ
1 an â€–Mn âˆ’Mâ€– <âˆ p.s.
(H3) an = cnÎ± , c > 0, 12 < Î± â‰¤ 1.
(H3â€™) an > 0,
âˆ‘âˆ
1 minjâˆˆIl aj =âˆ,
âˆ‘âˆ
1 a
2
n <âˆ.
4.3 ThÃ©orÃ¨mes
On a les Ã©noncÃ©s suivants.
ThÃ©orÃ¨me 1. Sous H1 et H3â€™, pour i = 1, . . . , p, Bin â†’ Î²i et Î˜Ì‚in âˆ’ Î¸in â†’ 0 p.s.
ThÃ©orÃ¨me 2. Sous H1 et H3, pour i = 1, . . . , p :
1. pour 12 < Î± â‰¤ 1 ou (Î± = 1 et
2Î»ic
ri
> 1) :
lim nÎ±E
[âˆ¥âˆ¥Bin âˆ’ Î²iâˆ¥âˆ¥2] <âˆ et lim nÎ±E [âˆ¥âˆ¥âˆ¥Î˜Ì‚in âˆ’ Î¸inâˆ¥âˆ¥âˆ¥2] <âˆ ;
Analyse en composantes principales dâ€™un flux de donnÃ©es
2. pour Î± = 1 et 2Î»ic
ri
= 1 :
lim
n
lnn
E
[âˆ¥âˆ¥Bin âˆ’ Î²iâˆ¥âˆ¥2] <âˆ et lim nlnnE
[âˆ¥âˆ¥âˆ¥Î˜Ì‚in âˆ’ Î¸inâˆ¥âˆ¥âˆ¥2] <âˆ ;
3. pour Î± = 1 et 2Î»ic
ri
< 1 :
lim n
2Î»ic
ri E
[âˆ¥âˆ¥Bin âˆ’ Î²iâˆ¥âˆ¥2] <âˆ et lim n 2Î»icri E [âˆ¥âˆ¥âˆ¥Î˜Ì‚in âˆ’ Î¸inâˆ¥âˆ¥âˆ¥2] <âˆ.
ThÃ©orÃ¨me 3. Sous H1, H2, H3, si lâ€™on suppose que R admet des moments dâ€™ordre 4r,
pourj = 1, . . . , r, Qj converge presque sÃ»rement et, si lâ€™on suppose que les r plus grandes
valeurs propres de C = MÎ£ sont distinctes, alors, pour i = 1, . . . , r, X in converge presque
sÃ»rement dans âˆ©ij=1 jE vers un vecteur propre de C associÃ© Ã  sa ie`me plus grande valeur
propre.
5 Cas particulier de la mÃ©trique de lâ€™ACP normÃ©e
Soit
(
Ïƒi
)2
= V ar
[
Ri
]
, i = 1, . . . , p. La mÃ©trique de lâ€™ACP normÃ©e dans Rp est la
mÃ©trique diagonaleM des 1
(Ïƒi)2
.
On considÃ¨re lâ€™estimateur 1
Min
= 1
n
âˆ‘n
j=1
(
Zij âˆ’ Î˜Ì‚
i
j
)2
de
(
Ïƒi
)2
et la mÃ©trique diagonale
Mn dans Rp desM in. On peut calculer 1Min de faÃ§on rÃ©cursive.
ThÃ©orÃ¨me 4. On suppose que R admet des moments dâ€™ordre 4. Alors, sous H1 et H3 avec
Î± = 1,Mn â†’M et
âˆ‘âˆ
1 an â€–Mn âˆ’Mâ€– <âˆ p.s. ; donc, lâ€™hypothÃ¨se H2 est vÃ©rifiÃ©e.
6 Conclusion et extensions
On a traitÃ© ici lâ€™estimation en ligne des facteurs de lâ€™ACP dâ€™un flux de donnÃ©es. On peut
Ã©galement estimer en ligne les valeurs propres associÃ©es (Bouamaine et Monnez, 1998), les
corrÃ©lations entre les variables et les facteurs ; on peut aussi estimer en ligne la valeur dâ€™un
facteur pour un individu et procÃ©der Ã©ventuellement Ã  une classification des individus.
Cette Ã©tude introductive sera dÃ©veloppÃ©e dans les directions suivantes :
1. Ã©tude dâ€™un modÃ¨le oÃ¹ lâ€™espÃ©rance et la variance varient dans le temps ;
2. Ã©tude de modÃ¨les non linÃ©aires de variation des paramÃ¨tres ;
3. autres choix de mÃ©triquesM ;
4. application Ã  dâ€™autres mÃ©thodes dâ€™analyse factorielle.
J.-M. Monnez
7 DÃ©monstrations
7.1 DÃ©monstration du thÃ©orÃ¨me 1
On a dÃ©fini : Bin+1 = Bin âˆ’ anU in(U iâ€²nBin âˆ’ Zin).
Comme Zin = Î¸in +Rin = U iâ€²nÎ²i +Rin, on a :
Bin+1 âˆ’ Î²
i = Bin âˆ’ Î²
i âˆ’ anU
i
nU
iâ€²
n (B
i
n âˆ’ Î²
i) + anU
i
nR
i
n.
A i fixÃ©, notons : Yn = Bin âˆ’ Î²i, Vn = U in, Sn = Rin, nl = nil, r = ri. On a :
Yn+1 = Yn âˆ’ anVnV
â€²
nYn + anVnSn.
Pour Ã©tablir la convergencepresque sÃ»re, on utilise le lemme suivant (Robbins et Siegmund,
1971).
Lemme 5. Soit (â„¦, A, P ) un espace probabilisÃ©, (Tn) une suite croissante de sous-tribus de
A. Soit, pour tout n, Î±n, Î²n, Î³n des variables alÃ©atoires rÃ©elles Tn - mesurables, non nÃ©gatives,
intÃ©grables, telles que
E [Î±n+1 | Tn] â‰¤ Î±n(1 + Î²n) + Î³n âˆ’ Î´n,
âˆâˆ‘
1
Î²n < âˆ,
âˆâˆ‘
1
Î³n <âˆ p.s.
Alors, la suite (Î±n) converge presque sÃ»rement vers une variable alÃ©atoire Î± finie et on aâˆ‘âˆ
1 Î´n <âˆ p.s.
1. On a :
â€–Yn+1â€–
2
= â€–Ynâ€–
2
+ a2n â€–VnV
â€²
nYn âˆ’ VnSnâ€–
2
+ 2an ã€ˆYn, VnSnã€‰
âˆ’2an ã€ˆYn, VnV
â€²
nYnã€‰
â‰¤ â€–Ynâ€–
2
+ 2a2n â€–Vnâ€–
2
â€–Ynâ€–
2
+ 2a2n â€–Vnâ€–
2
S2n + 2an ã€ˆYn, VnSnã€‰
âˆ’2an ã€ˆYn, VnV
â€²
nYnã€‰ .
Soit Tn la tribu du passÃ© au temps n, par rapport Ã  laquelle Y1, . . . , Yn sont mesurables.
On a E [Sn | Tn] = E
[
Ri
]
= 0, E
[
S2n | Tn
]
= E
[
(Ri)2
]
.
E
[
â€–Yn+1â€–
2 | Tn
]
â‰¤ (1 + 2a2n â€–Vnâ€–
2) â€–Ynâ€–
2 + 2a2n â€–Vnâ€–
2E
[
S2
]
âˆ’2an ã€ˆYn, VnV
â€²
nYnã€‰ p.s.
Sous les hypothÃ¨ses H1a et H3â€™, on a :
âˆ‘âˆ
1 a
2
n â€–Vnâ€–
2
<âˆ. Dâ€™aprÃ¨s le lemme 5 :
âˆƒT â‰¥ 0 : â€–Ynâ€–
2
â†’ T p.s.;
âˆâˆ‘
1
an ã€ˆYn, VnV
â€²
nYnã€‰ <âˆ p.s.
Analyse en composantes principales dâ€™un flux de donnÃ©es
2. On a : â€–Yn+1 âˆ’ Ynâ€– â‰¤ an â€–Vnâ€–2 â€–Ynâ€–+ an â€–VnSnâ€– .
E
[âˆ‘âˆ
1 a
2
n â€–VnSnâ€–
2
]
=
âˆ‘âˆ
1 a
2
n â€–Vnâ€–
2
E
[
S2
]
<âˆ.
Donc :
âˆ‘âˆ
1 a
2
n â€–VnSnâ€–
2 <âˆ p.s. ; an â€–VnSnâ€– â†’ 0 p.s.
Sous H1a et H3, an â€–Vnâ€–2 â†’ 0 p.s.
On en dÃ©duit que â€–Yn+1 âˆ’ Ynâ€– â†’ 0 p.s.
3. On raisonne Ã  Ï‰ fixÃ©, appartenant Ã  lâ€™intersection des ensembles de convergence presque
sÃ»re dÃ©finis. Supposons T (Ï‰) 6= 0.
On supprime dans la suite lâ€™Ã©criture de Ï‰.
Alors : âˆƒ0 < 1 < 1, âˆƒN(1) : âˆ€n > N(1), 1 < â€–Ynâ€– < 11 .
Donc, sous H1b, Ã  partir dâ€™un certain rang L, on a :âŒ©
Ynl ,
âˆ‘
jâˆˆIl
VjV
â€²
j Ynl
âŒª
â‰¥ Î»21.
On en dÃ©duit quâ€™il existe un entierml âˆˆ Il tel que :âŒ©
Ynl , VmlV
â€²
ml
Ynl
âŒª
â‰¥
Î»21
r
.
On considÃ¨re la dÃ©composition :âŒ©
Yml , VmlV
â€²
ml
Yml
âŒª
=
âŒ©
Yml + Ynl , VmlV
â€²
ml
(Yml âˆ’ Ynl)
âŒª
+
âŒ©
Ynl , VmlV
â€²
ml
Ynl
âŒª
.
Soit  > 0 tel que  â‰¤ Î»
3
1
4r2C2 , avecC = supn â€–Vnâ€– <âˆ sous H1a. A partir dâ€™un certain
rang : â€–Yn+1 âˆ’ Ynâ€– <  ; donc : â€–Yml âˆ’ Ynlâ€– < r ;
=â‡’
âˆ£âˆ£âŒ©Yml + Ynl , VmlV â€²ml(Yml âˆ’ Ynl)âŒªâˆ£âˆ£ < 2 11C2r â‰¤ Î»
2
1
2r
=â‡’
âŒ©
Yml , VmlV
â€²
ml
Yml
âŒª
>
Î»21
2r
.
Sous H3â€™, on a alors :
âˆ‘âˆ
l=1 aml
âŒ©
Yml , VmlV
â€²
ml
Yml
âŒª
=âˆ. Donc :âˆ‘âˆ
n=1 an ã€ˆYn, VnV
â€²
nYnã€‰ =âˆ ; il y a contradiction. Par consÃ©quent : T (Ï‰) = 0.
7.2 DÃ©monstration du thÃ©orÃ¨me 2
1. On reprend les notations du thÃ©orÃ¨me 1. Dâ€™aprÃ¨s la partie 1 de sa dÃ©monstration, on a :
E
[
â€–Yn+1â€–
2
]
â‰¤ (1 + 2a2n â€–Vnâ€–
2
)E
[
â€–Ynâ€–
2
]
+ 2a2n â€–Vnâ€–
2
E
[
S2
]
âˆ’2anE [ã€ˆYn, VnV
â€²
nYnã€‰] .
Dâ€™aprÃ¨s le lemme 5 :
âˆƒt â‰¥ 0 : E
[
â€–Ynâ€–
2
]
â†’ t ;
âˆâˆ‘
1
anE [ã€ˆYn, VnV
â€²
nYnã€‰] <âˆ.
J.-M. Monnez
2. Donc, il existe b > 0 tel que, avec Âµl = minjâˆˆIl aj :
E
[
â€–Yn+1â€–
2
]
â‰¤ E
[
â€–Ynâ€–
2
]
+ ba2n â€–Vnâ€–
2
âˆ’ 2anE [ã€ˆYn, VnV
â€²
nYnã€‰]
E
[âˆ¥âˆ¥Ynl+1âˆ¥âˆ¥2] â‰¤ E [â€–Ynlâ€–2]+ bâˆ‘
jâˆˆIl
a2j â€–Vjâ€–
2
âˆ’ 2Âµl
âˆ‘
jâˆˆIl
E
[âŒ©
Yj , VjV
â€²
j Yj
âŒª]
.
âˆ‘
jâˆˆIl
E
[âŒ©
Yj , VjV
â€²
j Yj
âŒª]
=
âˆ‘
jâˆˆIl
E
[âŒ©
Ynl , VjV
â€²
j Ynl
âŒª]
+
âˆ‘
jâˆˆIl
E
[âŒ©
Yj + Ynl , VjV
â€²
j (Yj âˆ’ Ynl)
âŒª]
Sous H1b :
âˆ‘
jâˆˆIl
E
[âŒ©
Ynl , VjV
â€²
j Ynl
âŒª]
â‰¥ Î»E
[
â€–Ynlâ€–
2
]
.
On note C = supn â€–Vnâ€–. Il existe a > 0 tel que :âˆ‘
jâˆˆIl
âˆ£âˆ£E [âŒ©Yj + Ynl , VjV â€²j (Yj âˆ’ Ynl)âŒª]âˆ£âˆ£
â‰¤
âˆ‘
jâˆˆIl
E
[
â€–Yj + Ynlâ€– â€–Vjâ€–
2
â€–Yj âˆ’ Ynlâ€–
]
â‰¤ C2
âˆ‘
jâˆˆIl
(
E
[
â€–Yj + Ynlâ€–
2
]) 1
2
(
E
[
â€–Yj âˆ’ Ynlâ€–
2
]) 1
2
â‰¤ a2
âˆ‘
jâˆˆIl
(
E
[
â€–Yj âˆ’ Ynlâ€–
2
]) 1
2
Or : Yj âˆ’ Ynl =
âˆ‘jâˆ’1
k=nl
(âˆ’akVkV
â€²
kYk + akVkSk).
â€–Yj âˆ’ Ynlâ€– â‰¤
âˆ‘jâˆ’1
k=nl
(akC
2 â€–Ykâ€–+ akC â€–Skâ€–).
Il existe d > 0 tel que : E
[
â€–Yj âˆ’ Ynlâ€–
2
]
â‰¤ d
âˆ‘
kâˆˆIl
a2k â‰¤ drmaxkâˆˆIl a
2
k.
Donc, il existe f > 0 tel que :âˆ‘
jâˆˆIl
âˆ£âˆ£E [âŒ©Yj + Ynl , VjV â€²j (Yj âˆ’ Ynl)âŒª]âˆ£âˆ£ â‰¤ f max
kâˆˆIl
ak.
Par consÃ©quent, il existe g > 0 tel que :
E
[âˆ¥âˆ¥Ynl+1âˆ¥âˆ¥2] â‰¤ (1 âˆ’ 2Î»Âµl)E [â€–Ynlâ€–2]+ brC2max
kâˆˆIl
a2k + 2fÂµlmax
kâˆˆIl
ak
â‰¤ (1 âˆ’ 2Î»Âµl)E
[
â€–Ynlâ€–
2
]
+ gmax
kâˆˆIl
a2k.
Or : Âµl = c(nl+1âˆ’1)Î± â‰¥
c
(lr)Î± ,maxkâˆˆIl ak =
c
(nl)
Î± â‰¤ clÎ± .
Donc, il existe h > 0 tel que :
E
[âˆ¥âˆ¥Ynl+1âˆ¥âˆ¥2] â‰¤ (1âˆ’ 2Î»crÎ± 1lÎ±
)
E
[
â€–Ynlâ€–
2
]
+
h
l2Î±
.
Analyse en composantes principales dâ€™un flux de donnÃ©es
3. Dans le cas 12 < Î± < 1, on applique un lemme de Schmetterer (1969) :
limlÎ±E
[
â€–Ynlâ€–
2
]
<âˆ.
Dans le cas Î± = 1, on applique un lemme de Venter (1966) :
pour
2Î»c
r
> 1, limlE
[
â€–Ynlâ€–
2
]
<âˆ ;
pour
2Î»c
r
= 1, lim
l
ln l
E
[
â€–Ynlâ€–
2
]
<âˆ ;
pour
2Î»c
r
< 1, liml
2Î»c
r E
[
â€–Ynlâ€–
2
]
<âˆ.
Comme E
[
â€–Yn+1â€–
2
]
â‰¤ E
[
â€–Ynâ€–
2
]
+ ba2n â€–Vnâ€–
2
, on a pour n âˆˆ Il :
E
[
â€–Ynâ€–
2
]
â‰¤ E
[
â€–Ynlâ€–
2
]
+
h
l2Î±
.
Comme l â‰¤ n â‰¤ lr, on obtient des rÃ©sutats semblables aux prÃ©cÃ©dents pour E
[
â€–Ynâ€–
2
]
,
en remplaÃ§ant l et nl par n.
7.3 DÃ©monstration du thÃ©orÃ¨me 3
Dâ€™aprÃ¨s le thÃ©orÃ¨me 4 de Bouamaine et Monnez (1998), on a les conclusions de ce thÃ©o-
rÃ¨me sous les hypothÃ¨ses :
1.
âˆ‘âˆ
1 an â€–E [Cn | Tn]âˆ’ Câ€– <âˆ p.s.
2. Pour j = 2, . . . , 2r,
âˆ‘âˆ
1 a
j
nE
[
â€–Cn âˆ’ Câ€–
j
| Tn
]
<âˆ p.s.
3. Mnâˆ’1 est Tn-mesurable ;Mn â†’M p.s. ;
âˆ‘âˆ
1 an â€–Mnâˆ’1 âˆ’Mâ€– <âˆ p.s.
4. an > 0,
âˆ‘âˆ
1 an =âˆ,
âˆ‘âˆ
1 a
2
n <âˆ.
On vÃ©rifie lâ€™hypothÃ¨se 1.
Cn = Mnâˆ’1(ZnZ
â€²
n âˆ’ Î˜Ì‚nÎ˜Ì‚
â€²
n) ; C =MÎ£ =M(E [ZnZ
â€²
n]âˆ’ Î¸nÎ¸
â€²
n).
Cn âˆ’ C = Mnâˆ’1(ZnZ
â€²
n âˆ’ E [ZnZ
â€²
n]) + (Mnâˆ’1 âˆ’M)(E [ZnZ
â€²
n]âˆ’ Î¸nÎ¸
â€²
n)
âˆ’Mnâˆ’1(Î˜Ì‚n âˆ’ Î¸n)Î˜Ì‚
â€²
n âˆ’Mnâˆ’1Î¸n(Î˜Ì‚n âˆ’ Î¸n)
â€².
E [Cn | Tn]âˆ’ C = (Mnâˆ’1 âˆ’M)Î£âˆ’Mnâˆ’1(Î˜Ì‚n âˆ’ Î¸n)Î˜Ì‚
â€²
n âˆ’Mnâˆ’1Î¸n(Î˜Ì‚n âˆ’ Î¸n)
â€².
Dâ€™aprÃ¨s les conclusions du thÃ©orÃ¨me 2, en utilisant la norme euclidienne usuelle dans Rp,
on a dans tous les cas :
E
[
âˆâˆ‘
1
1
nÎ±
âˆ¥âˆ¥âˆ¥Î˜Ì‚n âˆ’ Î¸nâˆ¥âˆ¥âˆ¥
]
â‰¤
âˆâˆ‘
1
1
nÎ±
(
E
[âˆ¥âˆ¥âˆ¥Î˜Ì‚n âˆ’ Î¸nâˆ¥âˆ¥âˆ¥2]) 12
â‰¤
pâˆ‘
i=1
âˆâˆ‘
n=1
1
nÎ±
(
E
[âˆ¥âˆ¥âˆ¥Î˜Ì‚in âˆ’ Î¸inâˆ¥âˆ¥âˆ¥2]) 12 <âˆ.
J.-M. Monnez
Donc :
âˆ‘âˆ
1
1
nÎ±
âˆ¥âˆ¥âˆ¥Î˜Ì‚n âˆ’ Î¸nâˆ¥âˆ¥âˆ¥ <âˆ p.s.
Dâ€™aprÃ¨s le thÃ©orÃ¨me 1, Î˜Ì‚n âˆ’ Î¸n â†’ 0 p.s.
Sous H2, on a alors :
âˆâˆ‘
1
1
nÎ±
â€–E [Cn | Tn]âˆ’ Câ€– <âˆ p.s.
On vÃ©rifie lâ€™hypothÃ¨se 2 en Ã©crivant que :
â€–Cn âˆ’ Câ€–
j â‰¤ 4jâˆ’1(â€–Mnâˆ’1â€–
j â€–ZnZ
â€²
n âˆ’ E [ZnZ
â€²
n]â€–+ â€–Mnâˆ’1 âˆ’Mâ€–
j â€–Î£â€–j
+ â€–Mnâˆ’1â€–
j
âˆ¥âˆ¥âˆ¥Î˜Ì‚n âˆ’ Î¸nâˆ¥âˆ¥âˆ¥j âˆ¥âˆ¥âˆ¥Î˜Ì‚nâˆ¥âˆ¥âˆ¥j + â€–Mnâˆ’1â€–j â€–Î¸nâ€–j âˆ¥âˆ¥âˆ¥Î˜Ì‚n âˆ’ Î¸nâˆ¥âˆ¥âˆ¥j),
âˆâˆ‘
1
ajn â€–Mn âˆ’Mâ€–
j
< âˆ,
âˆâˆ‘
1
ajn
âˆ¥âˆ¥âˆ¥Î˜Ì‚n âˆ’ Î¸nâˆ¥âˆ¥âˆ¥j <âˆ, âˆâˆ‘
1
ajn <âˆ,
Mn â†’ M, Î˜Ì‚n âˆ’ Î¸n â†’ 0 p.s.
7.4 DÃ©monstration du thÃ©orÃ¨me 4
1. Montrons que, pour i = 1, . . . , p :
1
n
nâˆ‘
j=1
(Zij âˆ’ Î˜Ì‚
i
j)
2 â†’ (Ïƒi)2 p.s.
1
n
nâˆ‘
j=1
(Zij âˆ’ Î˜Ì‚
i
j)
2 =
1
n
nâˆ‘
j=1
((Zij âˆ’ Î¸
i
j) + (Î¸
i
j âˆ’ Î˜Ì‚
i
j))
2
=
1
n
nâˆ‘
j=1
(Zij âˆ’ Î¸
i
j)
2 +
2
n
nâˆ‘
j=1
(Zij âˆ’ Î¸
i
j)(Î¸
i
j âˆ’ Î˜Ì‚
i
j) +
1
n
nâˆ‘
j=1
(Î¸ij âˆ’ Î˜Ì‚
i
j)
2.
Les Zij âˆ’ Î¸ij constituent un Ã©chantillon i.i.d. de Ri. Donc : 1n
âˆ‘n
j=1(Z
i
j âˆ’ Î¸
i
j)
2 â†’
(Ïƒi)2 p.s.
On a : Î¸ij âˆ’ Î˜Ì‚ij â†’ 0 p.s. ; donc : 1n
âˆ‘n
j=1(Î¸
i
j âˆ’ Î˜Ì‚
i
j)
2 â†’ 0 p.s.
Notons V in = (Zin âˆ’ Î¸in)(Î¸in âˆ’ Î˜Ì‚in),W in+1 = 1n
âˆ‘n
j=1 V
i
j ,W
i
1 = 0. On a :
W in+1 = (1âˆ’
1
n
)W in +
1
n
V in
(W in+1)
2 = (1 +
1
n2
)(W in)
2 + 2(1âˆ’
1
n
)
1
n
V inW
i
n +
1
n2
(V in)
2 âˆ’
2
n
(W in)
2.
Analyse en composantes principales dâ€™un flux de donnÃ©es
Soit Tn la tribu du passÃ© au temps n.
E
[
V in | Tn
]
= E
[
Zin âˆ’ Î¸
i
n | Tn
]
(Î¸in âˆ’ Î˜Ì‚
i
n) = E
[
Zin âˆ’ Î¸
i
n
]
(Î¸in âˆ’ Î˜Ì‚
i
n) = 0.
E
[
(W in+1)
2 | Tn
]
= (1 +
1
n2
)(W in)
2 +
1
n2
E
[
(V in)
2 | Tn
]
âˆ’
2
n
(W in)
2 p.s.
Or : E
[
(V in)
2 | Tn
]
= E
[
(Zin âˆ’ Î¸
i
n)
2 | Tn
]
(Î¸in âˆ’ Î˜Ì‚
i
n)
2 = E
[
(Ri)2
]
(Î¸in âˆ’ Î˜Ì‚
i
n)
2
.
Donc :
âˆ‘âˆ
1
1
n2
E
[
(V in)
2 | Tn
]
= E
[
(Ri)2
]âˆ‘âˆ
1
1
n2
(Î¸in âˆ’ Î˜Ì‚
i
n)
2 <âˆ p.s.
En appliquant le lemme 5, on obtient :
âˆƒT i â‰¥ 0 : (W in)
2 â†’ T i p.s. ;
âˆâˆ‘
1
1
n
(W in)
2 <âˆ p.s. =â‡’W in â†’ 0 p.s.
Par consÃ©quent : 1
n
âˆ‘n
j=1(Z
i
j âˆ’ Î˜Ì‚
i
j)
2 â†’ (Ïƒi)2 p.s.
On en dÃ©duit que :Mn â†’M p.s.
2. On utilise dans la suite le lemme suivant.
Lemme 6. Soit, pour tout n â‰¥ 1 : wn+1 = (1âˆ’ an)wn+ anun, w1 = 0, un >
0, 0 < an < 1. Si
âˆ‘âˆ
1 an = âˆ et
âˆ‘âˆ
1 anun < âˆ, alors
âˆ‘âˆ
1 anwn < âˆ et
wn â†’ 0.
On remarque que, pour an = 1n , wn+1 =
1
n
âˆ‘n
j=1 uj .
DÃ©monstration. Pour n > 1, wn > 0. Dâ€™aprÃ¨s le lemme 5, il existe w â‰¥ 0 : wn â†’ w
et
âˆ‘âˆ
1 anwn <âˆ ; comme
âˆ‘âˆ
1 an =âˆ, on a w = 0.
3. Montrons que, pour i = 1, . . . , p,
âˆ‘âˆ
n=1
1
n
âˆ£âˆ£âˆ£ 1nâˆ‘nj=1(Zij âˆ’ Î˜Ì‚ij)2 âˆ’ (Ïƒi)2âˆ£âˆ£âˆ£ <âˆ p.s.
On utilise la dÃ©composition de 1
n
âˆ‘n
j=1(Z
i
j âˆ’ Î˜Ì‚
i
j)
2 vue dans la premiÃ¨re partie de la
dÃ©monstration. Soit Âµi4 le moment centrÃ© dâ€™ordre 4 de Ri. On a :
E
ï£®ï£°( 1
n
nâˆ‘
j=1
(Zij âˆ’ Î¸
i
j)
2 âˆ’ (Ïƒi)2)2
ï£¹ï£» = 1
n2
nâˆ‘
j=1
V ar
[
(Zij âˆ’ Î¸
i
j)
2
]
=
Âµi4 âˆ’ (Ïƒ
i)4
n
,
âˆâˆ‘
1
1
n
E
ï£®ï£°
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ 1n
nâˆ‘
j=1
(Zij âˆ’ Î¸
i
j)
2 âˆ’ (Ïƒi)2
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£
ï£¹ï£» <âˆ.
Dâ€™aprÃ¨s le thÃ©orÃ¨me 2, on a :
âˆ‘âˆ
1
1
n
E
[
(Î¸in âˆ’ Î˜Ì‚
i
n)
2
]
<âˆ. On dÃ©duit du lemme 6 que :âˆ‘âˆ
n=1
1
n
E
[
1
n
âˆ‘n
j=1(Î¸
i
j âˆ’ Î˜Ì‚
i
j)
2
]
<âˆ.
âˆâˆ‘
1
1
n
E
[âˆ£âˆ£âˆ£(Zin âˆ’ Î¸in)(Î¸in âˆ’ Î˜Ì‚in)âˆ£âˆ£âˆ£] â‰¤ âˆâˆ‘
1
1
n
(E
[
(Zin âˆ’ Î¸
i
n)
2
]
)
1
2
(
E
[
(Î¸in âˆ’ Î˜Ì‚
i
n)
2
]) 1
2
â‰¤ (E
[
(Ri)2
]
)
1
2
âˆâˆ‘
1
1
n
(
E
[
(Î¸in âˆ’ Î˜Ì‚
i
n)
2
]) 1
2
<âˆ.
J.-M. Monnez
On dÃ©duit du lemme 6 que :
âˆâˆ‘
n=1
1
n
E
ï£®ï£° 1
n
nâˆ‘
j=1
âˆ£âˆ£âˆ£(Zij âˆ’ Î¸ij)(Î¸ij âˆ’ Î˜Ì‚ij)âˆ£âˆ£âˆ£
ï£¹ï£» <âˆ p.s.
On dÃ©duit des trois conclusions prÃ©cÃ©dentes que :
âˆâˆ‘
n=1
1
n
E
ï£®ï£°
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ 1n
nâˆ‘
j=1
(Zij âˆ’ Î˜Ì‚
i
j)
2 âˆ’ (Ïƒi)2
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£
ï£¹ï£» < âˆ
âˆâˆ‘
n=1
1
n
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ 1n
nâˆ‘
j=1
(Zij âˆ’ Î˜Ì‚
i
j)
2 âˆ’ (Ïƒi)2
âˆ£âˆ£âˆ£âˆ£âˆ£âˆ£ < âˆ p.s.
Par consÃ©quent, presque sÃ»rement,
âˆ‘âˆ
n=1
1
n
âˆ¥âˆ¥Mâˆ’1n âˆ’Mâˆ’1âˆ¥âˆ¥ <âˆ et
âˆâˆ‘
n=1
1
n
â€–Mn âˆ’Mâ€– =
âˆâˆ‘
n=1
1
n
âˆ¥âˆ¥Mn(Mâˆ’1n âˆ’Mâˆ’1)Mâˆ¥âˆ¥ <âˆ.
RÃ©fÃ©rences
Aguilar-Ruiz, J. (2006). Recent advances in data stream mining. In 38e`mes JournÃ©es de
Statistique de la SFDS, Clamart.
BenzÃ©cri, J. (1969). Approximation stochastique dans une algÃ¨bre normÃ©e non commutative.
Bull. Soc. Math. France 97, 225â€“241.
Bouamaine, A. (1996). MÃ©thodes dâ€™approximation stochastique en analyse des donnÃ©es. Ph.
D. thesis, thÃ¨se de doctorat dâ€™Etat Ã¨s Sciences AppliquÃ©es, UniversitÃ© Mohammed V, EMI,
Rabat.
Bouamaine, A. et J. Monnez (1997). Convergence dâ€™une classe de processus dâ€™approximation
stochastique de vecteurs propres. Pub. Inst. Stat. Univ. Paris XXXXI(1-2), 97â€“117.
Bouamaine, A. et J. Monnez (1998). Approximation stochastique de vecteurs et valeurs
propres. Pub. Inst. Stat. Univ. Paris XXXXII(2-3), 15â€“38.
Dâ€™Aubigny, G. (2001). Data mining et statistique, discussion et commentaires. Journal de la
SociÃ©tÃ© FranÃ§aise de Statistique 142(1), 37â€“52.
Krasulina, T. (1970). Method of stochastic approximation in the determination of the largest
eigenvalue of the mathematical expectation of random matrices. Automation and Remote
Control 2, 215â€“221.
Lebart, L. (1974). On the benzÃ©criâ€™s method for computing eigenvectors by stochastic approxi-
mation (the case of binary data). In P. Verlag (Ed.), Proceedings in Computational Statistics,
Vienne, pp. 202â€“211.
MacGregor, J. (1997). Using on-line process data to improve quality : challenges for statisti-
cians. International Statistical Review 65(3), 309â€“323.
Analyse en composantes principales dâ€™un flux de donnÃ©es
Monnez, J. (1994). Convergence dâ€™un processus dâ€™approximation stochastique en analyse
factorielle. Pub. Inst. Stat. Univ. Paris XXXVIII(1), 37â€“56.
Monnez, J. (2006). Approximation stochastique en analyse factorielle multiple. Pub. Inst. Stat.
Univ. Paris L(3), 27â€“45.
Robbins, H. et S. Monro (1951). A stochastic approximation method. Ann. Math. Stat. 22,
400â€“407.
Robbins, H. et D. Siegmund (1971). A convergence theorem for nonnegative almost super-
martingales and some applications (Rustagi, J.S. ed.)., pp. 233â€“257. Academic Press, New
York.
Schmetterer, L. (1969). Multidimensional stochastic approximation. In A. Press (Ed.),Multi-
variate Analysis II, Proc. 2nd Int. Symp., Dayton, Ohio, pp. 443â€“460.
Venter, J. (1966). On dvoretzky stochastic approximation theorems. Ann. Math. Stat. 37,
1534â€“1544.
Summary
We consider a data stream and suppose that each data is a realization of a random vector
whose expectation varies in time according to a linear model for each component. We use
stochastic approximation processes to estimate on line the parameters of the linear models and
simultaneously the principal components of the data.
