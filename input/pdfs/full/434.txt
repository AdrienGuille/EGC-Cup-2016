Caract√©risation automatique des classes d√©couvertes en
classification non supervis√©e
Nistor Grozavu‚àó, Youn√®s Bennani‚àó
Mustapha Lebbah‚àó
‚àóLIPN UMR CNRS 7030, Universit√© Paris 13,
99, avenue Jean-Baptiste Cl√©ment, 93430 Villetaneuse
Pr√©nom.Nom@lipn.univ-paris13.fr
R√©sum√©. Dans cet article, nous proposons une nouvelle approche de classifi-
cation et de pond√©ration des variables durant un processus d‚Äôapprentissage non
supervis√©. Cette approche est bas√©e sur le mod√®le des cartes auto-organisatrices.
L‚Äôapprentissage de ces cartes topologiques est combin√© √† un m√©canisme d‚Äôesti-
mation de pertinences des diff√©rentes variables sous forme de poids d‚Äôinfluence
sur la qualit√© de la classification. Nous proposons deux types de pond√©rations
adaptatives : une pond√©ration des observations et une pond√©ration des distances
entre observations. L‚Äôapprentissage simultan√© des pond√©rations et des prototypes
utilis√©s pour la partition des observations permet d‚Äôobtenir une classification op-
timis√©e des donn√©es. Un test statistique est ensuite utilis√© sur ces pond√©rations
pour √©laguer les variables non pertinentes. Ce processus de s√©lection de variables
permet enfin, gr√¢ce √† la localit√© des pond√©rations, d‚Äôexhiber un sous ensemble
de variables propre √† chaque groupe (cluster) offrant ainsi sa caract√©risation.
L‚Äôapproche propos√©e a √©t√© valid√© sur plusieurs bases de donn√©es et les r√©sultats
exp√©rimentaux ont montr√© des performances tr√®s prometteuses.
1 Introduction
La classification automatique - clustering - est une √©tape importante du processus d‚Äôextraction
de connaissances √† partir de donn√©es. Elle vise √† d√©couvrir la structure intrins√®que d‚Äôun en-
semble d‚Äôobjets en formant des regroupements - clusters - qui partagent des caract√©ristiques
similaires (Fisher, 1996; Cheeseman et al., 1988). La complexit√© de cette t√¢che s‚Äôest forte-
ment accrue ces deux derni√®res d√©cennies lorsque les masses de donn√©es disponibles ont vu
leur volume exploser. En effet, le nombre d‚Äôobjets pr√©sents dans les bases de donn√©es a forte-
ment augment√© mais √©galement la taille de leur description. L‚Äôaugmentation de la dimension
des donn√©es a des cons√©quences non n√©gligeables sur les traitements classiquement mis en
oeuvre : outre l‚Äôaugmentation naturelle des temps de traitements, les approches classiques
s‚Äôav√®rent parfois inadapt√©es en pr√©sence de bruit ou de redondance.
La taille des donn√©es peut √™tre mesur√©e selon deux dimensions, le nombre de variables et
le nombre d‚Äôobservations. Ces deux dimensions peuvent prendre des valeurs tr√®s √©lev√©es, ce
qui peut poser un probl√®me lors de l‚Äôexploration et l‚Äôanalyse de ces donn√©es. Pour cela, il est
Caract√©risation automatique des groupes en classification non supervis√©e
fondamental de mettre en place des outils de traitement de donn√©es permettant une meilleure
compr√©hension des donn√©es. En effet, plus le nombre de dimensions d‚Äôune base de donn√©es
est important, plus les donn√©es sont dispers√©es dans l‚Äôespace de repr√©sentation et plus la dif-
f√©rences entre les deux donn√©es les plus similaires et les deux donn√©es les moins similaires
est r√©duit. Ainsi, dans un espace de grande dimensions, il est tr√®s difficile pour un algorithme
de classification de d√©tecter les variations de similarit√© qui d√©finissent les regroupements de
donn√©es. C‚Äôest ce qu‚Äôon appelle le "fl√©au de la dimension‚Äù. Pour contourner cette difficult√©,
on utilise souvent des techniques de r√©duction des dimensions afin de faciliter le processus
de l‚ÄôECD1. La r√©duction des dimensions permet d‚Äô√©liminer les informations non-pertinentes
et redondantes selon le crit√®re utilis√©. Cette r√©duction permet donc de rendre l‚Äôensemble des
donn√©es plus repr√©sentatif du ph√©nom√®ne √©tudi√©. Il s‚Äôagit d‚Äôun probl√®me complexe qui permet
d‚Äôoptimiser le volume d‚Äôinformations √† traiter et faciliter le processus de l‚Äôapprentissage. En
effet, les principaux objectifs de la r√©duction des dimensions sont :
‚Äì faciliter la visualisation et la compr√©hension des donn√©es,
‚Äì r√©duire l‚Äôespace de stockage n√©cessaire,
‚Äì r√©duire le temps d‚Äôapprentissage et d‚Äôutilisation,
‚Äì identifier les facteurs pertinents.
La r√©duction du nombre d‚Äôobservations peut se faire par quantification √† travers une classifi-
cation non supervis√©e ou par s√©lection d‚Äôexemples. Dans le cadre de cette √©tude, nous proc√©-
derons par une classification non supervis√©e permettant ainsi de calculer des prototypes (r√©f√©-
rents : moyennes locales) repr√©sentant l‚Äôensemble des donn√©es.
Les algorithmes d‚Äôapprentissage artificiel requi√®rent typiquement peu de traits (va-
riables/attributs) tr√®s significatifs caract√©risant le ph√©nom√®ne √©tudi√©. Dans la probl√©matique
de la classification non supervis√©e, il pourrait encore √™tre b√©n√©fique d‚Äôincorporer un module de
r√©duction du nombre de variables dans le syst√®me global avec comme objectif d‚Äôenlever toute
information incons√©quente et redondante. Cela a un effet important sur la qualit√© de la clas-
sification. En effet le nombre de caract√©ristiques utilis√©es est directement li√© √† l‚Äôerreur finale.
L‚Äôimportance de chaque caract√©ristique d√©pend de la taille de la base d‚Äôapprentissage : pour un
√©chantillon de petite taille, l‚Äô√©limination d‚Äôune caract√©ristique importante peut diminuer l‚Äôer-
reur. Il faut aussi noter que des caract√©ristiques individuellement peu pertinentes peuvent √™tre
tr√®s informatives si on les utilise conjointement. Pour la r√©duction du nombre de variables nous
pouvons proc√©der de plusieurs mani√®res :
‚Äì par s√©lection : qui consiste √† choisir un sous ensemble des caract√©ristiques initiales dans
l‚Äôespace de mesure,
‚Äì par transformation : qui vise √† construire de nouvelles caract√©ristiques dans un espace
transform√© - un espace de projection.
Dans cette √©tude, nous nous int√©ressons √† la r√©duction de dimension de l‚Äôespace de description
dans le cadre de la classification non supervis√©e par s√©lection √† travers la pond√©ration locale
des variables. Deux approches diff√©rentes, par la technique de pond√©ration locale et proche par
l‚Äôutilisation de la structure de la carte, seront pr√©sent√©es dans ce papier.
Pour la r√©duction de dimension de l‚Äôespace de description en apprentissage non supervis√©
les contributions sont plut√¥t moins cons√©quentes (Roth et Lange; Liu et al., 2005; Guyon et al.,
2006). Dans la litt√©rature nous trouvons g√©n√©ralement des approches bas√©es sur la pond√©ration
comme les travaux de Huang et al. (2005), Blansche et al. (2006), Gu√©rif et Bennani (2007),
1Extraction des Connaissance √† partir des Donn√©es
N. Grozavu et al.
Frigui et Nasraoui (2004) et Grozavu et al. (2008) et des approches de s√©lection de caract√©ris-
tiques comme les m√©thodes propos√©es par Basak et al. (1998), Bassiouny et al. (2004), Liu et al.
(2005), Questier et al. (2005) et Li et al. (2006). Nous trouvons aussi des m√©thodes permettant
la r√©duction simultan√©e des dimensions des donn√©es (exemples et variables). Ces m√©thodes
sont souvent appel√©es des techniques de "bi-classification" ou "classification crois√©e" ou bien
encore "Subspace clustering" (Parsons et al., 2004). Ces approches sont tr√®s s√©duisantes en
pratique car elles permettent, gr√¢ce √† une classification simultan√©e des observations et des va-
riables, de caract√©riser les groupes identifi√©s.
Les deux approches que nous proposons dans cet article peuvent √™tre vue comme proches,
mais pas identiques √† ces derni√®res techniques de classification crois√©e. Nos approches sont as-
soci√©es √† deux algorithmes d‚Äôapprentissage non supervis√© et simultan√© des observations et des
variables. La premi√®re approche est une technique compl√®tement nouvelle pour pond√©rer les
variables. Cette technique agit plus en amont en pond√©rant les caracteristiques des observations
au cours de l‚Äôapprentissage afin de d√©duire des pond√©rations locales. La deuxi√®me approche
n‚Äôest qu‚Äôune extension et une reformulation stochastique de l‚Äôapproche, de pond√©ration locale,
propos√©e dans Grozavu et al. (2008). Dans ce cas la pond√©ration des distances nous permet
de d√©duire les pond√©rations locales associ√©es √† chaque groupe "clusters". Dans le cadre de
notre √©tude, les deux formalismes de pond√©rations propos√©s sont associ√©s au mod√®le des cartes
auto-organisatrices. Les pond√©rations locales estim√©es sont utilis√©es pour la caract√©risation des
groupes de la partition obtenue avec la carte topologique. En effet, contrairement √† la pond√©-
ration globale, qui estime un seul vecteur de pond√©rations pour tout l‚Äôensemble des r√©f√©rents
(c‚Äôest √† dire toute la carte), la pond√©ration locale associe un vecteur de pond√©rations √† chaque
r√©f√©rent de la carte topologique. Ces valeurs sont simultan√©ment estim√©es au cours de l‚Äôap-
prentissage avec le partitionnement des observations. Par cons√©quent nous pouvons utiliser ces
pertinences, √† la fin de l‚Äôapprentissage, pour regrouper et caract√©riser les prototypes.
Le reste de cet article est organis√© comme suit. La Section 2 pr√©sente bri√®vement le mod√®le
de base des cartes auto-organisatrices suivi du d√©tail des deux approches propos√©es de pond√©-
ration locale adaptative : lwd-SOM2 (pond√©ration de la distance) et lwo-SOM3 (pond√©ration
des observations). La technique pour caracteriser automatiquement les groupes en utilisant nos
pond√©rations est pr√©sent√©e dans la section 5. Dans la section 6 nous pr√©sentons les r√©sultats des
exp√©rimentations. Une conclusion et des perspectives sont donn√©es dans la section 7.
2 Cartes auto-organisatrices traditionnelles
Les cartes auto-organisatrices (SOM4) pr√©sent√©es par Kohonen (2001) ont √©t√© largement uti-
lis√©es pour la classification et la visualisation des bases de donn√©es multidimensionnelles. On
trouve une grande vari√©t√© d‚Äôalgorithmes des cartes topologiques d√©riv√©e du premier mod√®le
original propos√© par Kohonen Bishop et al. (1998); Cottrell et al. (2004); Lebbah et al. (2007).
Ces mod√®les sont diff√©rents les uns des autres, mais partagent la m√™me id√©e de pr√©senter les
donn√©es de grande dimension en une simple relation g√©om√©trique sur une topologie r√©duite. Ce
mod√®le consiste en la recherche d‚Äôune classification non supervis√©e d‚Äôune base d‚Äôapprentissage
A = {xi ‚àà R
n, i = 1..N} o√π l‚Äôindividu xi = (xi1, xi2..., xij , ..., xin) est de dimension n.
2local weight distance using Self Organizing Map
3local weight observation using Self Organizing Map
4Self-Organizing Map
Caract√©risation automatique des groupes en classification non supervis√©e
Ce mod√®le classique se pr√©sente sous forme d‚Äôune carte poss√©dant un ordre topologique de C
cellules. Les cellules sont r√©parties sur les n≈ìuds d‚Äôun maillage. La prise en compte dans la
carte de taille C de la notion de proximit√© impose de d√©finir une relation de voisinage topo-
logique. Afin de mod√©liser la notion d‚Äôinfluence d‚Äôune cellule k sur une cellule l, qui d√©pend
de leur proximit√©, on utilise une fonction noyau K (K ‚â• 0 et lim
|y|‚Üí‚àû
K(y) = 0). L‚Äôinfluence
mutuelle entre deux cellules k et l est donc d√©finie par la fonction Kk,l(.). A chaque cellule k
de la grille est associ√©e un vecteur r√©f√©rent wk = (wk1, wk2..., wkj , ..., wkn) de dimension n.
On note par la suite parW = {wj ,wj ‚àà R
n}
|W |
j=1 l‚Äôensemble des r√©f√©rents associ√©s √† la carte.
Les phases principales de l‚Äôalgorithme d‚Äôapprentissage associ√© aux cartes auto-organisatrices
sont d√©finies dans la litt√©rature et consistent √† minimiser la fonction de co√ªt suivante Kohonen
(2001) :
RSOM(œá,W ) =
N‚àë
i=1
|W|‚àë
j=1
Kj,œá(xi)‚Äñxi ‚àíwj‚Äñ
2 (1)
o√π œá(xi) = argminj
(
‚Äñxi ‚àíwj‚Äñ
2
)
la fonction d‚Äôaffectation.
A la fin de l‚Äôapprentissage, la carte auto-organisatrice d√©termine une partition des donn√©es en
|W|5 groupes associ√©s √† chaque r√©f√©rent/prototype/repr√©sentant wk ‚àà R
n de la carte.
3 Apprentissage non supervis√© et pond√©ration des observa-
tions et des variables
Un des inconv√©nients des cartes auto-organisatrices (SOM) est qu‚Äôelles traitent avec √©galit√©
toutes les variables. Ceci n‚Äôest pas souhaitable dans de nombreuses applications de partitionne-
ment o√π les observations sont d√©crites avec un grand nombre de variables. Les groupes fournis
par SOM se caract√©risent souvent par un sous-ensemble de variables plut√¥t que l‚Äôensemble des
variables d√©finies. Par cons√©quent certaine variables peuvent occulter la d√©couverte de la struc-
ture sp√©cifique d‚Äôun groupe - cluster. La pertinence de chaque variable change d‚Äôun groupe √† un
autre. La pond√©ration des variables est une extension de la proc√©dure de s√©lection des variables
o√π les variables sont associ√©es √† un vecteur de poids qui peuvent √™tre consid√©r√©s comme des
degr√©s de pertinence.
La d√©marche propos√©e pour r√©aliser simultan√©ment le regroupement et la caract√©risa-
tion des groupes est con√ßue de telle mani√®re √† estimer les meilleurs prototypes, et les en-
semble optimaux de poids au cours de la phase d‚Äôapprentissage. Chaque prototype wj =
(wj1, wj2, ..., wjn) est associ√© √† un vecteur de poids pij = (pij1, pij2, ..., pijn). Nous notons par
la suite par Œ† = {pij , pij ‚àà <
n}
|pi|
j=1 l‚Äôensemble des vecteurs de poids. Dans ce qui suit, nous
pr√©sentons deux versions de pond√©ration locale des variables avec les cartes topologiques : une
nouvelle approche pour la pond√©ration des observations et une reformulation stochastique de
la pond√©ration des distances.
5|W| indique le nombre d‚Äô√©l√©ments de l‚ÄôensembleW
N. Grozavu et al.
3.1 Pond√©ration locale des observations : lwo-SOM
Cette technique de pond√©ration estime un vecteur de poids pour pond√©rer et filtrer les ob-
servations en les adaptant dans le processus d‚Äôapprentissage. Dans l‚Äôarchitecture propos√©e, qui
est similaire √† l‚Äôarchitecture supervis√©e ?, nous associons √† chaque r√©f√©rent wj un vecteur de
pond√©rations pij . Ainsi nous proposons de minimiser la nouvelle fonction de co√ªt suivante :
Rlwo(œá,W,Œ†) =
N‚àë
i=1
|w|‚àë
j=1
Kj,œá(xi)‚Äñpijxi ‚àíwj‚Äñ
2 (2)
La minimization de Rlwo(œá,W,Œ†) se fait it√©rativement, avec la descente du gradient, en
trois √©tapes jusqu‚Äô√† stabilisation. Apr√®s l‚Äô√©tape d‚Äôinitialisation de l‚Äôensemble des prototypes
W et l‚Äôensemble des pond√©rations associ√©s Œ†, √† chaque √©tape d‚Äôapprentissage (t + 1) nous
appliquons les √©tapes suivantes :
‚Äì Minimiser Rlwo(œá, WÀÜ, Œ†ÀÜ) par rapport √† œá en fixantW et Œ†. Chaque observation ponde-
r√©e (pijxi) est affect√©e au referent wj , dont elle est la plus proche au sens de la distance
euclidienne :
œá(xi) = argmin
j
(
‚Äñpijxi ‚àíwj‚Äñ
2
)
(3)
‚Äì Minimiser Rlwo(œáÀÜ,W, Œ†ÀÜ) par rapport √†W en fixant œá et Œ†. Les vecteurs r√©f√©rents sont
mis-√†-jour avec l‚Äôexpression suivante :
wj(t+ 1) = wj(t) + (t)Kj,œá(xi) (xi ‚àíwj(t)) (4)
‚Äì Minimiser Rlwo(œáÀÜ, WÀÜ ,Œ†) par rapport √† Œ† en fixant œá etW . L‚Äôexpression de mis-√†-jour
pour le vecteur des pond√©r√°tions pij(t+ 1) est :
pij(t+ 1) = pij(t) + (t)Kj,œá(xi) (pijxi ‚àíwj(t)) (5)
Comme l‚Äôalgorithme stochastique classique de Kohonen, SOM, on note par (t) le pas d‚Äôap-
prentissage au temps t. L‚Äôapprentissage est g√©n√©ralement r√©alis√© en deux phases. Dans la pre-
mi√®re phase, un grand pas d‚Äôapprentissage initial (0) et un grand rayon de voisinage Tmax
sont utilis√©s. Pendant la deuxi√®me phase,  et T d√©croient au cours du temps.
4 La pond√©ration locale de la distance : lwd-SOM
A partir de la version lwd-SOM analytique Grozavu et al. (2008), nous avons develop√© la
version stochastique de la pond√©ration de la distance. La fonction de co√ªt est d√©crite par la
formule suivante :
Rlwd(œá,W,Œ†) =
N‚àë
i=1
|w|‚àë
j=1
Kj,œá(xi)(pij)
Œ≤‚Äñxi ‚àíwj‚Äñ
2 (6)
o√π Œ≤ est le coefficient de discrimination.
Comme pour l‚Äôalgorithme lwo-SOM, la minimisation de la fonction de co√ªt se fait en trois
√©tapes :
Caract√©risation automatique des groupes en classification non supervis√©e
1. Minimiser Rlwd(œá, WÀÜ, Œ†ÀÜ) par rapport √† œá en fixantW et Œ†. L‚Äôexpression d‚Äôaffectation
est la suivante :
œá(xi) = argmin
j
(
(pij)
Œ≤
‚Äñxi ‚àíwj‚Äñ
2
)
(7)
2. MinimiserRlwd(œáÀÜ,W, Œ†ÀÜ) par rapport √†W en fixant œá etŒ†. Les vecteurs des prototypes
sont mis-√†-jour en utilisant l‚Äôexpression suivante :
wj(t+ 1) = wj(t) + (t)Kj,œá(xi)pi
Œ≤ (xi ‚àíwj(t)) (8)
3. Minimiser Rlwd(œáÀÜ, WÀÜ,Œ†) par rapport √† Œ† en fixant œá etW . La mis-√†-jour de ces vec-
teurs des pond√©rations pij(t+ 1) se fait d‚Äôapr√®s l‚Äôexpression :
pij(t+ 1) = pij(t) + (t)Kj,œá(xi)Œ≤pi
Œ≤‚àí1
j ‚Äñxi ‚àíwj(t)‚Äñ
2 (9)
De la m√™me mani√®re que la version lwo-SOM, on fait d√©cro√Ætre le pas et le rayon d‚Äôapprentis-
sage pour constituer deux phases : une phase d‚Äôauto-organisation associ√©e aux grandes valeurs
des parametres et une phase de quantification associ√©e aux petites valeurs.
5 Caracterisation automatique des groupes
Une proc√©dure de s√©lection de variables comporte trois √©l√©ments essentiels : une mesure
de pertinence, une proc√©dure de recherche et un crit√®re d‚Äôarr√™t. Nous distinguons trois types de
m√©thodologie :
‚Äì les approches filtres dont la mesure de pertinence est ind√©pendante de l‚Äôalgorithme qui
utilise ensuite les donn√©es ;
‚Äì les approches symbioses qui √©valuent la pertinence des sous-ensembles de variables √†
l‚Äôaide des performances du syst√®me que l‚Äôon construit ;
‚Äì les approches int√©gr√©es pour lesquelles la mesure de pertinence est directement incluse
dans la fonction de co√ªt optimis√©e par le syst√®me. Les approches int√©gr√©es sont de deux
types : (1) globale o√π la mesure de pertinence est calcul√©e globalement sur les individus ;
(2) locale pour la quelle chaque r√©f√©rent a son propre vecteur de mesures de pertinence ;
Nos approches lwo/d-SOM font partie des cat√©gories des approches integr√©es avec une mesure
localement adaptative de pertinence. Plusieurs crit√®res d‚Äôarr√™t ont √©t√© introduit dans la litt√©ra-
ture, mais souvent l‚Äôinconv√©nient de ces crit√®res est la fixation d‚Äôun seuil qui d√©pend de la
base des donn√©es. Dans notre cas, pour d√©tecter les variables pertinentes nous avons utilis√© un
test statistique propos√© par Cattell en 1960 Cattell (1966) appel√© "Scree Test". Ce test va nous
permettre une s√©lection des variables pertinentes d‚Äôune mani√®re automatique et sans d√©finir un
seuil d‚Äôarr√™t a priori.
5.1 Crit√®re de s√©lection : "Scree Acceleration Test"
L‚Äôutilisation initiale du test "Scree Test", Cattell (1966) √©tait la d√©termination visuelle du
nombre de valeurs propres √† prendre en compte lors d‚Äôune analyse en composantes principales.
L‚Äôid√©e de base est de repr√©senter graphiquement les valeurs propres et de trouver √† partir de
quelle valeur le graphique semble pr√©senter un changement brutal. Selon Cattell, nous devons
N. Grozavu et al.
trouver ce qui repr√©sente la ligne du changement brutal ‚ÄùScree‚Äù. Le nombre de composantes √†
garder correspond au nombre de valeurs propres pr√©c√©dant ce ‚ÄôScree‚Äô. Fr√©quement, ce ‚ÄôScree‚Äô
apparait l√† o√π la pente du graphe change radicalement. Ainsi, il s‚Äôagit de trouver la d√©c√©l√©ration
maximale dans ce graphique.
Par analogie, l‚Äôutilisation de ce test avec nos mod√®les de pond√©rations, consiste √† d√©-
tecter, par exemple, le changement brutal dans le graphique des pertinences pik =
(pik1, pik2..., pikj , ..., pikn). Il faudrait donc d√©tecter la plus forte d√©c√©l√©ration. La proc√©dure
de s√©lection est ainsi compos√©e des √©tapes suivantes :
1. Ordonner le vecteur des pond√©rations pik = (pik1, pik2..., pikj , ..., pikn) en suivant un
ordre d√©croissant. Le nouveau vecteur ordonn√© est not√© pik = (pi
1
.., pi
2
....., pi
i
.., ..., pi
n
..) ;
o√π l‚Äôexposant i de la pond√©ration pii indique l‚Äôordre.
2. Calculer les premi√®res diff√©rences dfi = pi
i
.. ‚àí pi
i+1
.. ;
3. Calculer les deuxi√®mes diff√©rences (l‚Äôacc√©l√©ration) acci = dfi ‚àí dfi+1
4. Chercher le changement brutal ‚Äôscree‚Äô √† l‚Äôaide de la fonction suivante :
maxi (abs(acci) + abs(acci+1))
Ce processus permet de s√©lectionner toutes les variables se trouvant avant le changement brutal.
6 Validation des approches propos√©es
Nous avons utilis√© diff√©rents jeux de donn√©es disponibles sur UCI Asuncion et Newman (2007)
de taille et de complexit√© variable pour √©valuer nos approches de pond√©ration locale adaptative
et de s√©lection. En particulier dans la partie validation, nous allons donner plus de d√©tails sur
la base des vagues de Breiman. Les bases utilis√©es sont :
‚Äì Vagues de Breiman bruit√©es : La base est compos√©e de 5000 exemples divis√©s en 3
classes. La base originale comportait 21 variables, mais 19 variables additionnelles dis-
tribu√©es selon une loi normale ont √©t√© rajout√©es sous forme de bruit. Chaque observation
a √©t√© g√©n√©r√© comme une combinaison de 2 sur 3 vagues.
‚Äì Base de cancers : "Wisconsin Diagnostic Breast Cancer (WDBC) : Ce jeu de donn√©es
contient 569 individus qui sont d√©crit par 32 variables. 357 individus sont atteint de
cancer b√©nigne et les 212 autres ont des cancers malignes. Les variables d√©crivent les
caract√©ristiques des noyaux de cellules pr√©sentes dans l‚Äôimage num√©riques.
‚Äì Jeu de donn√©es Isolet : Ces donn√©es ont √©t√© g√©n√©r√© comme suit : 150 sujets prononcent
chaque lettre de l‚Äôalphabet √† deux reprises. Ainsi, nous avons 52 exemples de formation
de chaque locuteur. Les donn√©es sont constitu√©es de 1559 individus et 617 variables.
Toutes les variables sont continues.
‚Äì La base Madelon : Ces donn√©es posent un probl√®me √† 2 classes propos√© √† l‚Äôorigine
pendant la comp√©tition sur la s√©lection de variables organis√©e lors de la conf√©rence
NIPS‚Äô2003, Guyon et al. (2006). Les exemples sont situ√©s sur les sommets d‚Äôun hyper-
cube en dimension 5, mais 15 variables redondantes et 480 dimensions bruit√©es ont √©t√©
ajout√©s. Le jeu de donn√©es original √©tait s√©par√© en trois parties (apprentissage, validation
et test) mais nous n‚Äôavons utilis√© que les 2600 observations de l‚Äôensemble d‚Äôapprentis-
sage et de validation pour lesquels les classes √©taient connues.
‚Äì La base "SpamBase" est un jeu de donn√©es compos√© de 4601 observations d√©crites par
57 variables, chacune decrivant un mail et sa categorie : spam ou non-spam. Les attributs
Caract√©risation automatique des groupes en classification non supervis√©e
descriptifs de ces mails sont les frequences d‚Äôapparition de certains mots ou caract√®res
ainsi que des informations sur la quantit√© de caract√®res mis en capitale.
Dans ces exp√©rimentations, la comparaison des diff√©rents r√©sultats est mesur√©e √† l‚Äôaide de deux
crit√®res externes. On peut utiliser ces indices lorsque la segmentation souhait√©e est connue, en
particulier sur nos jeux de donn√©es. Il s‚Äôagit de la comparaison entre la segmentation propos√©e
et une segmentation souhait√©e. Ainsi nous avons utilis√©, le taux de la puret√© et l‚Äôindice de
Rand, qui calcule le pourcentage du nombre de couples d‚Äôobservations ayant la m√™me classe
et se retrouvant dans le m√™me sous ensemble apr√®s segmentation de la carte Saporta (2006).
Nous avons lanc√© l‚Äôapprentissage avec nos algorithmes lwo/d-SOM et le test de s√©lection
de variables sur les cinq bases d√©crites ci-dessus. Nous calculons par la suite les valeurs des
indices de qualit√© pour les deux cartes (lwo/d‚àí SOM ).
(a) (W de SOM) (b) (W de lwo-SOM) (c)W de lwd-SOM
(d) Œ† de lwo-SOM (e) Œ† de lwd-SOM
FIG. 1 ‚Äì Carte topologique de taille 26√ó14 (364 neurones). Les graphiques a, b et c indiquent
l‚Äôensemble des r√©f√©rents W issus respectivement de SOM classique, lwo-SOM et lwd-SOM.
Les graphiques d et e repr√©sentent les pond√©rations locales estim√©es respectivement par lwo-
SOM et lwd-SOM.
6.1 D√©roulement de l‚Äôapproche sur un exemple : Vagues de Breiman
Nous utilisons ce jeu de donn√©es pour montrer l‚Äôint√©gralit√© du processus permettant la ca-
ract√©risation des groupes, √† partir de l‚Äôapprentissage avec les deux approches (lwo/d-SOM)
N. Grozavu et al.
en passant par la d√©tection et la s√©lection des variables pertinentes. L‚Äôapprentissage d‚Äôune
carte de dimension 26 √ó 14 pour toutes les observations permet de fournir pour chaque cel-
lule un vecteur r√©f√©rent wk = (wk1, wk2..., wkj , ..., wk40) et un vecteur de pond√©rations
pik = (pik1, pik2..., pikj , ..., pik40) de dimension n = 40.
La figure 1 (a, b, c) montre, avec une visualisation 3D, l‚Äôensemble des r√©f√©rentsW issus de
l‚Äôapprentissage des trois algorithmes : SOM classique, lwo-SOM et lwd-SOM. Les pond√©ra-
tions locales Œ† sont pres√©nt√©es dans la figure 1 (d, e) issues de l‚Äôapprentissage avec lwo-SOM
et lwd-SOM respectivement. Les axesX et Y indiquent respectivement les variables et les in-
dices des r√©f√©rents. L‚Äôamplitude indique la valeur estim√©e. Nous rappelons ici, que dans le cas
de l‚Äôalgorithme lwo-SOM, les r√©f√©rents calcul√©s W repr√©sentent des observations pond√©r√©es
(pix). En observant les trois graphiques (a, b, c) nous constatons que le bruit qui represente
les variables de 19 √† 40 est bien visible avec de faibles amplitudes. Cette analyse visuelle des
r√©sultats est plus claire avec la version nouvelle lwo-SOM que nous avons propos√©e. Les deux
graphiques representant les r√©f√©rents W et les pond√©rations Œ† montrent que les variables du
bruit ne sont pas pertinantes. Apr√®s l‚Äôanalyse des deux types de pond√©rations (figure 1 (d, e)),
nous constatons que les pond√©rations Œ† issues de lwo-SOM corespondent plus √† la structure
des donn√©es (pertinence des variables) que les pond√©rations de lwd-SOM. Ce ph√©nom√®ne a
lieu car les pond√©rations des observations r√©pr√©sentent un filtre pour les donn√©es et l‚Äôestima-
tion des r√©f√©rents tient compte de ce filtrage. Afin de v√©rifier qu‚Äôil est possible de s√©lectionner
les variables d‚Äôune mani√®re automatique avec nos algorithmes lwo-SOM, nous avons appliqu√©
la phase de s√©lection sur l‚Äôensemble des r√©f√©rents W pour la version lwo/d ‚àí SOM apr√®s
segmentation de la carte. Cette phase consiste √† d√©tecter les variations brutales pour chacun
des vecteurs en entr√©e. Pour la phase de segmentation nous avons utilis√© la classification hi√©-
rarchique Vesanto et Alhoniemi (2000).
En utilisant la version lwo-SOM la segmentation de la carte avec les r√©f√©rentsW , qui sont
TAB. 1 ‚Äì Les r√©sultats de la s√©l√©ction par groupe de la base vagues de Breiman (l‚Äôintervalle
[i‚àí j] indique les variables de i √† j)
Db # groupes r√©els. lwd-SOM : Œ†W lwo-SOM :W Classification crois√©e
wave- 3 cl1 : [6-15] cl1 : [3-8 ; 11-16] cl1 : [3-12]
form cl2 : [4-10] cl2 : [8-11 ; 14-19] cl2 : [7-15]
cl3 : [7-19] cl3 : [3-20] cl3 : [10-18]
cl4 : [5-17]
Puret√© 0,5374 0,5416
Rand 0,6068 0,6164
d√©j√† pond√©r√©s, permet d‚Äôobtenir du premier coup 3 groupes. Par contre avec la version lwd-
SOM, la segmentation de la carte utilisant les r√©f√©rentW fournie 6 groupes "clusters", mais la
segmentation √† partir du produit Œ†W permet d‚Äôoboutir √† 3 groupes ce qui est significatif pour
notre exemple (vagues de Breiman).
La caract√©risation des groupes avec l‚Äôalgorithme "ScreeTest" est fourni dans la Table 1. Pour
chaque technique nous montrons les variables s√©lectionn√©es et associ√©es √† chaque groupe.
Nous observons que les deux techniques fournissent 3 groupes caract√©ris√©s par des variables
Caract√©risation automatique des groupes en classification non supervis√©e
diff√©rentes, mais qui se recouvrent. Nous constatons que pour les groupes cl1, cl2 et cl3, les va-
riables d√©tect√©es avec la carte lwd-SOM sont inclues dans l‚Äôensemble des variables d√©tect√©es
avec la carte lwo-SOM. Nous observons aussi qu‚Äôaucune variable du bruit n‚Äôest s√©lection-
n√©e avec la m√©thode lwd-SOM, au contraire de la technique lwo-SOM qui d√©tecte une seule
variable du bruit 20. Ceci ne r√©duit pas la qualit√© de la segmentation puisque le calcul de la pu-
ret√© des deux partitions, confirme une meilleur segmentation avec la m√©thode lwo-SOM (Table
1). Cette augmentation est faible, mais elle est significatif puisqu‚Äôacune connaissance n‚Äôest a
priori utilis√©e pour cette t√¢che. En comparant ces r√©sultats avec des approches de classification
crois√©e, nous constatons qu‚Äôon selectionne les m√™mes variables.
6.2 R√©sultats sur d‚Äôautres bases de donn√©es
En ce qui concerne les autres bases de donn√©es, nous allons ce contenter dans cette sec-
tion d‚Äôindiquer les r√©sultats obtenus apr√®s la phase de s√©lection des variables. Pour la base
WDBC, l‚Äôapplication de nos approches lwo/d-SOM nous a permis d‚Äôobtenir les variables 4 et
24 comme variables pertinentes de la base avec une forte importance pour le premier groupe
et pour le 9ieme groupe (acune variable n‚Äôest selection√©es pour le rest des groupes). En ce qui
concerne la base Isolet nous avons constat√© un accord de la s√©lection des variables non perti-
nentes. Les algorithmes lwo/d-SOM associ√©s au test de s√©lection fournissent les variables non
pertinentes dont les indices appartiennent √† l‚Äôintervalle 300 √† 500. En comparant les indices de
qualit√© de partionement (Indice de puret√©), nous constatons une am√©lioration pour l‚Äôapproche
lwo-SOM par rapport √† lwd-SOM.
TAB. 2 ‚Äì D√©t√©ction des variables pertinantes par groupes pour les db. : wdbc, madelon, isolet
et spambase (l‚Äôintervalle [i‚àí j] indique les variables de i √† j ; cli : groupe i)
Db. # gr. lwd-SOM lwo-SOM Classification
r√©el s√©lection Œ†W Puret√© s√©lection surW Puret√© crois√©e
wdbc 2 cl1-cl9 : (4 ;24) 0,6274 cl1-cl9 : (4 ;24) 0,8682 cl1-cl5 (4 ;24)
Made- 2 cl1 :1 0,5242 cl1 :1 0,5347 cl1 :[445-450]
lon cl2 : (91, 281, cl2 : (242, cl2 :[450-460]
403-424) 417-452]
26 cl1-cl13 : 0,5242 cl1-cl13 : 0,5261 cl1-cl15 :
Isolet [1-330, [5-302, 434-488] [1-300,
450-617] [545-551,586-593] 450-620]
Spam 2 cl1 :56 ; cl2 :57 0,6103 cl1 : 56 ; cl2 : 57 0,6413 cl1 :56 ;cl2 :57
Apr√®s l‚Äôanalyse des exp√©rimentation nous pouvons d√©duire quelques caract√©ristiques par-
ticuli√®res et quelques comparaisons entre lwo-SOM et lwd-SOM :
‚Äì Les deux approches it√©ratives de pond√©rations lwo/d-SOM ne donne aucune importance
aux variables non-pertinentes √† l‚Äôoppos√© avec le SOM classique o√π nous avons des va-
leurs plus √©lev√©es pour les variables non pertinentes.
‚Äì Les deux m√©thodes stochastiques sont plus rapides en les comparant particuli√®rement √†
la version analytique ‚Äúbatch‚Äù de pond√©ration d√©j√† propos√© de Anonyme.
N. Grozavu et al.
‚Äì Les deux algorithmes (lwo/d-SOM) et la Scree Test fournissent une caract√©risation des
groupes plus adapt√© gr√¢ce √† l‚Äôutilisation des pond√©rations locales.
‚Äì Les indices de qualit√© de la segmentation (Indice de Puret√©) sont meilleurs pour les deux
approches avec un avantage pou l‚Äôalgorithme lwo-SOM.
‚Äì Les pond√©rations Œ† apr√®s l‚Äôapprentissage de lwo-SOM apporte plus d‚Äôinformation que
celles fournis par lwd-SOM du fait de l‚Äôadaptation des pond√©rations aux observations et
pas aux distances. L‚Äôavantage de la m√©thode lwo-SOM est obtenu gr√¢ce √† la pond√©ration
en amont des observations.
7 Conclusion
Dans ce papier, nous avons introduit deux approches pour caract√©riser les groupes "clusters"
en utilisant les cartes auto-organisatrices. La premi√®re est une nouvelle technique de caract√©ri-
sation en pond√©rant les observations (lwo-SOM). La deuxi√®me est une reformulation stochas-
tique de la pond√©ration classique des distances (lwd-SOM). Nous avons utilis√© un test statis-
tique original "Scree Test" qui nous a permis de d√©tecter automatiquement les variables les
plus pertinentes. Nous avons montr√© √† travers diff√©rents exemples l‚Äôint√©r√™t de l‚Äôestimation des
pertinences des variables pour la visualisation et la s√©lection des variables. Nous avons mon-
tr√© aussi que les deux approches peuvent √™tre consid√©r√©es comme une pseudo-classification
crois√©e ou simultan√©e des observations et des variables. Enfin, contrairement √† la classification
crois√©e, la m√©thode propos√©e dans cet article permet de caract√©riser les groupes d‚Äôune mani√®re
automatique. L‚Äôestimation du nombre correct de groupes est en relation avec la stabilit√© de
la segmentation et la validit√© des groupes g√©n√©r√©s. En perspective, nous allons mesurer cette
stabilit√© pour nos algorithmes par des techniques de sous-√©chantillonnage.
R√©f√©rences
Asuncion, A. et D. Newman (2007). UCI machine learning repository.
Basak, J., R. K. De, et S. K. Pal (1998). Unsupervised feature selection using a neuro-fuzzy
approach. Pattern Recogn. Lett. 19(11), 997‚Äì1006.
Bassiouny, S., M. Nagi, et M. F. Hussein (2004). Feature subset selection in som based text
categorization. In IC-AI, pp. 860‚Äì866.
Bishop, C. M., M. Svens√©n, et C. K. I.Williams (1998). Gtm : The generative topographic
mapping. Neural Comput 10(1), 215‚Äì234.
Blansche, A., P. Gancarski, et J. Korczak (2006). Maclaw : A modular approach for clustering
with local attribute weighting. Pattern Recognition Letters 27(11), 1299‚Äì1306.
Cattell, R. (1966). The scree test for the number of factors. MBR 1, 245‚Äì276.
Cheeseman, P., J. Kelly, M. Self, J. Stutz, W. Taylor, et D. Freeman (1988). Autoclass : A
bayesian classification system. In Fifth International Workshop on Machine learning.
Cottrell, M., S. Ibbou, et P. Letr√©my (2004). Som-based algorithms for qualitative variables.
Neural Netw. 17(8-9), 1149‚Äì1167.
Caract√©risation automatique des groupes en classification non supervis√©e
Fisher, D. (1996). Iterative optimization and simplification of hierarchical clusterings. Journal
of Artificial Intelligence Research. (JAIR) 4, 147‚Äì178.
Frigui, H. et O. Nasraoui (2004). Unsupervised learning of prototypes and attribute weights.
Pattern Recognition 37(3), 567‚Äì581.
Grozavu, N., Y. Bennani, et M. Lebbah (2008). Pond√©ration locale des variables en apprentis-
sage num√©rique non-supervis√©. Extraction et Gestion des Connaissances (EGC 08), 45‚Äì54.
Gu√©rif, S. et Y. Bennani (2007). Dimensionality reduction trough unsupervised features selec-
tion. International Conference on Engineering Applications of Neural Networks.
Guyon, I., S. Gunn, M. Nikravesh, et L. Zadeh (Eds.) (2006). FE, Found. and Appl. Springer.
Huang, J. Z., M. K. Ng, H. Rong, et Z. Li (2005). Automated variable weighting in k-means
type clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(5).
Kohonen, T. (2001). Self-organizing Maps. Springer Berlin.
Lebbah, M., N. Rogovschi, et Y. Bennani (2007). Besom : Bernoulli on self organizing map.
In International Joint Conferences on Neural Networks. IJCNN 2007, Orlando, Florida.
Li, Y., B.-L. Lu, et Z.-F. Wu (2006). A hybrid method of unsupervised feature selection based
on ranking. In ICPR ‚Äô06.
Liu, L., J. Kang, J. Yu, et Z. Wang (2005). A comparative study on unsupervised feature
selection methods for text clustering. pp. 597‚Äì601.
Parsons, L., E. Haque, et H. Liu (2004). Subspace clustering for high dimensional data : a
review. SIGKDD Explor. Newsl. 6(1), 90‚Äì105.
Questier, F., R. Put, D. Coomans, B. Walczak, et Y. V. Heyden (2005). The use of cart and
multivariate regression trees for supervised and unsupervised feature selection. pp. 45‚Äì54.
Roth, V. et T. Lange. Feature selection in clustering problems. In S. Thrun, L. Saul, et B. Sch√∂l-
kopf (Eds.), Advances in Neural Information Processing Systems 16.
Saporta, G. (2006). Probabilit√©s, analyse des donn√©es et statistiques. Editions Technip.
Vesanto, J. et E. Alhoniemi (2000). Clustering of the self-organizing map. Neural Networks,
IEEE Transactions on 11(3), 586‚Äì600.
Summary
We introduce a new approach, which provide simultaneously Self-Organizing Map (SOM)
and local weight vector for each cluster. The proposed approach is computationally simple,
and learns a different feature vector weights for each cluster. Clustering and feature weighting
offers two advantages: First, they guide the SOM process to cluster the data set into more
meaningful clusters; Second, they can be used to characterize a cluster using a feature selec-
tion method. Based on the Self-Organizing Map approach, we present two new simultaneously
clustering and weighting algorithms, called lwo-SOM and lwd-SOM respectively. These two
algorithms achieve the same goal, however, they minimize different objective functions. lwo-
SOM estimates the feature vector weights by weighting observations, while lwd-SOM esti-
mates the feature vector weights by weighting distance between observations and prototypes.
We illustrate the performance of the proposed approach using different data sets.
