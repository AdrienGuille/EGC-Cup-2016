 Scoring sur données d’entreprises : instrument de diagnostic 
individuel et outil d’analyse de portefeuille d’une clientèle 
 
Mireille Bardos 
Ancien chef de service 
de l’Observatoire des entreprises de la Banque de France 
mireille.bardos@wanadoo.fr
 
 
Mots clefs : prévision du risque de crédit, analyse discriminante, précision de la 
probabilité de défaillance, classes de risque, sélection de variables, choix de modèle  
 
Introduction 
La plupart des études statistiques sur le risque de crédit et l’analyse discriminante se 
concentrent sur les techniques de construction d’un score. Or la réalisation d’un outil efficace 
de détection du risque doit relier cette construction à l’utilisation future de cet instrument et 
les propriétés qu’il doit en conséquence nécessairement satisfaire. Les utilisateurs d’un 
scoring sur données d’entreprises seront des décideurs, pour la plupart experts en analyse 
financière, ou responsables du risque de crédit dans les banques, ou superviseurs bancaires. 
Le présent article s’efforce de relier construction et utilisation en mettant en exergue les 
propriétés requises pour les utilisateurs et les implications techniques qu’elles entraînent. Il en 
résultera une réflexion sur la sélection des données, sur le choix du modèle, sur l’estimation 
de la probabilité de défaut. Dès lors l’utilisation de l’outil sera approfondie sous deux 
aspects : le diagnostic individuel de l’entreprise et l’analyse du risque d’une population 
d’entreprises emprunteuses. Enfin le score comme indicateur probabilisé du risque de crédit 
joue un rôle important dans les recherches en économie. 
Beaucoup des problèmes abordés ici se rencontrent sur d’autres champs d’application 
de l’analyse discriminante. Toutefois il convient de s’adapter dans chaque cas aux spécificités 
de l’utilisation. 
1. Les enjeux de la prévision du risque de défaut de 
paiement  
La nécessité de mieux contrôler le risque de crédit dans les banques a conduit à 
l’intensification des travaux de credit scoring. Plusieurs types de techniques permettent la 
détection précoce des défauts paiement des entreprises. Elles relèvent de l’analyse 
discriminante. 
 
Un des objectifs majeurs pour les banques est d’estimer l’espérance de la perte globale 
attendue, Expected Loss, et la perte inattendue grâce à un quantile extrême, Unexpected Loss, 
sur une population d’entreprises, par exemple la clientèle d’une banque. Pour cela il est 
nécessaire de disposer au niveau de chaque entreprise d’une probabilité de défaut à un horizon 
donné (ou plusieurs). On peut alors déterminer des classes de risque homogène.  
 
D’un tel objectif découlent plusieurs questions sur les propriétés du score mis à 
disposition : 
- précision de l’estimation des probabilités et homogénéité des classes de risque 
- stabilité temporelle des classes de risque et de leurs propriétés 
© Revue MODULAD, 2008 - 159 -       Numéro 38 
 - dépendance à la conjoncture de la mesure du risque 
- stabilité des matrices de transitions 
- corrélation des risques 
 
La maîtrise de ces questions oblige à se pencher sur le processus de construction du score 
et d’examiner les stades sensibles de ce processus. La pratique de la construction et de 
l’utilisation conduit à une deuxième série de questions : 
- type de variables 
- longueur historique des fichiers de construction 
- processus de sélection des variables 
- choix de la technique d’analyse discriminante 
- horizon de la prévision 
- stabilité des entreprises dans les classes de risque 
- fréquence de révision de l’outil 
- articulation entre conjoncture, prévision et révision 
 
Ces questions constituent un enjeu important. De plus en plus étudiées elles se révèlent 
interdépendantes. Les phases successives de la construction d’un score ont une influence sur 
la robustesse et l’efficacité de l’outil obtenu. Diverses utilisations d’un tel outil seront 
envisagées. Les exemples présentés sont ceux des scores de la Banque de France. 
2. Construction d’un outil de détection précoce 
2.1 Les données 
 
La définition de l’événement à détecter constitue une première difficulté : procédure 
judiciaire ou défaut de paiement ? quelle gravité du défaut de paiement ? Le choix est souvent 
guidé par la disponibilité de l’information. Un banquier connaîtra les défauts de paiement et 
sera à même de qualifier leur gravité ; une institution publique est souvent mieux renseignée 
sur l’ouverture des procédures judiciaires. Ensuite se pose la question de la corrélation entre 
ces événements pour une même entreprise. 
La population d’entreprises cibles. La qualité du travail statistique requiert: la 
représentativité des échantillons et leur redressement éventuel. Une relative homogénéité de la 
population est également nécessaire,  pour que la variabilité liée aux difficultés des entreprises 
ne soit pas masquée par d’autres types de variabilité comme les différences structurelles des 
bilans entre grands secteurs. Dans ce dernier cas la construction d’un score par grand secteur 
est la solution. Au statisticien de décider à quel niveau de nomenclature d’activité sectoriel il 
doit travailler.  
L’horizon de la prévision est fixé par les besoins du décideur, mais conditionné par 
la fraîcheur des données, l’impact de la conjoncture et la prévisibilité du phénomène étudié 
(ici la défaillance). 
L’organisation des fichiers de données résultera d’un compromis.  
 Le schéma 1 décrit la façon dont les échantillons ont été organisés dans les études de 
la Banque de France. Huit années d’observations des comptes d’entreprises permettent de 
couvrir un cycle économique. Pour les entreprises défaillantes, la base de données est 
constituée des bilans des trois années précédant la défaillance. D indique le moment où celle-
ci intervient. Chaque rectangle représente les données bilancielles d’un exercice comptable. 
Les études statistiques sur la prévision de la défaillance sur données comptables ont révélé 
© Revue MODULAD, 2008 - 160 -       Numéro 38 
 que dès trois ans avant la défaillance des signes avant-coureurs  sont visibles dans beaucoup 
d’entreprises et ceux-ci s’accentuent à l’approche de l’événement. 
 La présence des bilans d’une entreprise dans les bases de données bancaires peut 
présenter une certaine discontinuité, en particulier chez les firmes en difficultés. Pour ne pas 
se priver d’un maximum d’observations les échantillons d’étude sont non constants. De tels 
fichiers correspondent d’ailleurs la réalité de l’utilisation future du scoring. 
 
Schéma 1 
ORGANISATION DES FICHIERS 
1995 1996 1997 1998 1999 2000 2001 2002 2003   
   D        
    D      Entreprises 
     D     défaillantes 
      K = 1 D     
     K = 2  D    
     K = 3   D   
     
 
     
     K = 4   Entreprises non défaillantes 
 
Légende:  K = 1: entreprises défaillantes observées 1 an avant la défaillance 
 K = 2: entreprises défaillantes observées 2 ans avant la défaillance 
 K = 3: entreprises défaillantes observées 3 ans avant la défaillance
 K = 4: entreprises non défaillantes jusqu’à la date de constitution des fichiers (été 2004) 
 
 
Schéma 2 
 
Sélection des variables une à une 
Discrimination linéaire Discrimination non linéaire 
Df  désigne la courbe de densité des entreprises défaillantes.  désigne la courbe de densité des entreprises non défaillantes Nf
  
Le choix des variables explicatives va également être conditionné par la disponibilité 
et la fiabilité. Les variables qualitatives sont particulièrement fragiles et souvent mieux 
adaptées à l’expertise. Parmi les variables quantitatives, le suivi du compte bancaire est 
probablement très révélateur à court terme, mais, n’est pas toujours disponible ou s’il est 
disponible il nécessite une exploitation difficile et complexe afin d’aboutir à des indicateurs 
synthétiques qu’il faudra correctement coder. 
Les ratios économiques et financiers construits sur les variables comptables sont 
largement disponibles et relativement homogènes grâce à l’existence d’un plan comptable. Ils 
reposent sur une théorie sous-jacente: l’analyse financière. La signification économique des 
© Revue MODULAD, 2008 - 161 -       Numéro 38 
 ratios et leur fiabilité statistique doivent faire l’objet d’une réflexion attentive. Elles 
conditionnent la qualité du résultat (Cf. Bardos (2001) chapitre 5 sur la préparation des 
données  et la sélection des variables) 
Leur préparation est délicate et longue. Sont examinés les valeurs aberrantes, 
extrêmes, ou encore « insolites » qui peuvent nécessiter une imputation de valeur, leur loi de 
probabilité, leur pouvoir discriminant, les corrélations, la linéarité ou non vis-à-vis du 
problème traité. Cette dernière caractéristique ne peut être connue que grâce à l’examen des 
distributions des ratios (Cf. schéma 2). Elle impose la cohérence entre les variables 
sélectionnées et le choix de la technique du modèle (linéaire ou non). 
La présence des bilans pouvant être irrégulière, les taux de variation ne peuvent être 
calculés pour toutes les entreprises. Si l’on souhaite que le champ d’application du score soit 
le plus large possible, les variables explicatives ne devront porter que sur une seule année. Les 
taux de variation sont d’ailleurs statistiquement très fragiles et peu discriminants : telle 
variable de très faible niveau initial chez une entreprise donnée, y restera faible même si elle 
double en montant. Ainsi on rencontrera des cas d’entreprises qui pour un même taux de 
variation se trouveront en situation de fait très différente, l’une à fort montant initial et l’autre  
à très faible montant initial. Le taux de variation dans ce cas les traitera sur un pied d’égalité 
et brouillera l’analyse statistique, même s’il y a dans le score d’autres indicateurs qui les 
départagent mieux.  
 
Une fois les bons ratios repérés le pouvoir discriminant est contrôlé par des tests sur 
les quantiles (Cf.  Vessereau (1987) and M.G. Kendall, A. Stuart (1961)). Un test non 
paramétrique efficace repose sur le calcul d’intervalle de confiance sur les quantiles comme suit. 
 
Tableau 1 : Intervalles de confiance des quartiles au niveau 95% pour le ratio délai fournisseurs 
Délai fournisseurs Nombre d’  
entreprises 
1er quartile médiane 3ème quartile 
Entreprises non défaillantes 11 011 73,0 
[71.9 ; 74.1] 
102,4 
[101.3 ; 103.3] 
132,2 
[130.9 ; 133.2] 
Entreprises défaillantes     
 3 ans avant 408 
 
 
79,4 
[73.1 ; 86.4] 
109,4 
[103.8 ; 117.2] 
142,1 
[137.5 ; 149.0] 
 2 ans avant 391 
 
 
84,3 
[74.5 ; 87.1] 
109,4 
[103.8 ; 117.2] 
145,5 
[137.5 ; 149.0] 
 1 an avant 177 
 
 
84,7 
[713 ; 90.7] 
106,9 
[101 ; 112.4] 
 
137,8 
[129.3 ; 153.0] 
Source : Banque de France 
 
Si la variable aléatoire X a une distribution non paramétrique, la probabilité que le  
quantile  soit compris entre les observations de rang r et s,  et , est calculée grâce à une 
loi Beta complète: 
thp
pX )(rx )(sx
)1,()1,()( )()( +−−+−=<< snsIrnrIxXxP ppspr  
où 0<p<1 et n est le nombre d’observations et ∫ −− −= P
à
vu
p dtttvuB
vuI 11 )1(
),(
1),( . 
© Revue MODULAD, 2008 - 162 -       Numéro 38 
 L’intervalle de confiance au niveau α est [ ])();( sxrx  tel que α−=<< 1)( )()( spr xXxP . 
Quelques résultats sont présentés tableau 1. 
 
Certains statisticiens utilisent le test de dominance stochastique (R. Davidson, Duclos 
Jean-Yves (1999), G. Barret, S.G. Donald (2002)). 
 
 
 
La sélection conjointe des ratios est une autre étape primordiale. Les ratios organisés 
par thème (endettement, rentabilité, gestion du cycle d’exploitation…etc) permettent de 
définir des jeux alternatifs de variables, un score étant estimé sur chaque jeu. Un jeu pourra 
être constitué d’un ratio par thème afin de prendre en compte tous les aspects de la vie de 
l’entreprise. Les variables finalement retenues doivent être peu corrélées entre elles. En effet,  
dans les méthodes linéaires des corrélations élevées peuvent entraîner des signes des 
coefficients a contrario de  ce que les statistiques descriptives ont révélé sur le sens de la 
discrimination pour chaque ratio1. De tels coefficients doivent être proscrits sous peine de 
graves erreurs de classement, très mal comprises par les utilisateurs futurs dans le diagnostic 
individuel (analystes financiers mais rarement statisticiens eux-mêmes). En conséquence les 
associations de ratios qui induisent de tels coefficients seront proscrites.  
 
Au-delà de ces considérations, trois techniques de sélection, souvent 
employées, apportent une aide bien qu’elles nécessitent des hypothèses sur la distribution des 
variables : le λ de Wilks (sélection pas à pas), l’algorithme de Furnival et Wilson (sélection 
optimale d’un ensemble de variables discriminantes), la statistique de Wald (test de la nullité 
des coefficients dans le cas de la régression logistique).  
 
2.2. Les modèles 
L’analyse discriminant, parfois appelée classification supervisée, couvre un large 
domaine de techniques. Des comparaisons détaillées de ces techniques ont été menées dans 
plusieurs études: Hand (2006), Hristache, Delcroix, Patiléa (2004), Baesens et alii (2003), 
Bardos (2001b), Bardos, Zhu (1998), Thiria et alii (1997), McLachlan (1992), Gnanadesikan 
and alii (1989). Des réflexions sur l’adéquation des modèles aux données économiques et 
financières  d’entreprises peuvent être trouvées dans cette littérature. Les principaux modèles 
étudiés son l’analyse discriminante de Fisher linéaire ou quadratique, la régression logistique, 
et quelques méthodes non paramétriques comme Disqual
2
, Arbres de décision (CART), 
Réseaux de Neurones, la méthode du Plus Proche Voisin, la méthode du Noyau, les Support 
Vector Machines. 
 
Le but de la construction d’un score peut se limiter à vouloir identifier les clignotants 
du risque. Mais si on veut obtenir un outil opérationnel, sa construction est fondée sur une 
règle de décision. Celle-ci est généralement définie à partir de la désignation d’un seuil de 
                                                 
1 Pour les méthodes conduisant à une fonction linéaire, seuls peuvent être retenus des ratios dont on peut dire « plus le risque de 
défaillance est élevé, plus cette variable est élevée » (par exemple le taux d’endettement) ou contraire « plus le risque est 
élevé, plus cette variable est faible » (par exemple le taux de marge).  
2 Cette méthode construit une fonction discriminante sur variables qualitatives. Elle a été créée par G. Saporta. Une récente 
application sur données de stratégie d’entreprises a été menée par L. Lelogeais (2003)  
© Revue MODULAD, 2008 - 163 -       Numéro 38 
 décision3 nécessaire à l’évaluation des taux de bons classements. Ceux-ci permettent de 
choisir la meilleure fonction discriminante parmi plusieurs estimations. 
 Sur données comptables d’entreprises, les méthodes conduisant à une combinaison 
linéaire de ratios sont de loin les plus robustes temporellement et sont facilement 
interprétables. Sont ainsi fréquemment mises oeuvre les techniques de l’analyse discriminante 
linéaire de Fisher (ADL) et les modèles LOGIT ou PROBIT. à la Banque de France c’est la 
très classique analyse discriminante linéaire de Fisher (ADL) qui est utilisée. Ce modèle, cas 
particulier de la régression logistique, a été choisi du fait des aides à l’interprétation qu’il 
fournit (Cf. §2.2.1). 
 Toutefois dans le domaine bancaire, plutôt que de classer les entreprises en 
« bonnes » et « mauvaises », il sera beaucoup plus utile de disposer d’une mesure du risque 
pour chaque entreprise, c’est-à-dire sa probabilité de défaut à un horizon fixé (Cf. §3). C’est 
elle qui motivera la décision finale du décideur.  
L’apport d’un score au décideur a pour avantage de fournir une synthèse de la situation 
de risque à partir du bilan et du compte de résultat. Toutefois ceci a des limites car la 
défaillance d’entreprise est un phénomène complexe dont les véritables variables causales 
sont peu disponibles et difficiles à identifier. Les fonctions scores utilisent donc des 
symptômes tels que des descripteurs de la situation de l’entreprise avant sa défaillance. Autant 
dire qu’il est souvent difficile de cerner avec précision le phénomène de défaillance par un 
score, à l’inverse de ce qui se produit sur d’autres champs d’application de l’analyse 
discriminante plus proches de la science physique, comme la reconnaissance des formes où le 
sur apprentissage est mieux maîtrisable, et des techniques telles que les réseaux de neurones 
s’appliquent avec succès. L’intérêt du score est de fournir une mesure du risque par la 
probabilité de défaillance à partir des variables mobilisées. Mais bien sûr il faut en mobiliser 
d’autres le plus possible, notamment celles de nature qualitative (pertinence de la politique 
commerciale, qualité du processus de production, clairvoyance des décisions 
d’investissement…) . 
  
2.2.1 L’analyse discriminant linéaire de Fisher (LDA) 
 
Deux règles de décision peuvent être implémentées pour estimer une LDA. 
  
On considère D le groupe des entreprises défaillantes, N le groupe des entreprises non défaillantes, 
 le vecteur des p ratios de l’entreprise,  et   les moyennes de X sur 
chaque groupe, T la matrice de variance totale. 
),...,,( 21 pXXXX = Nμ Dμ
 
La première règle de décision répond à un critère géométrique de comparaison de distance: 
),(),( ND XdXd μμ ≤ ⇔  “e est alloué au groupe D” 
Utilisant la métrique de matrice 1−T , la règle devient: 
f(X) est négative ⇔  “e est allouée au groupe D” 
où )
2
()'()( 1
DN
DN XTXf μμμμ +−−= −   est la fonction discriminante. Ce modèle ne demande pas 
d’hypothèses paramètriques, néanmoins la forme du nuage de données doit être assez régulière 
(Saporta (1990)). 
                                                 
3 Il y a plusieurs seuils dans le cas des arbres ou des réseaux de neurones ; pour certains modèles comme le Plus Proche Voisin 
ou le Noyau ou les SVM on définit des régions de décision. 
© Revue MODULAD, 2008 - 164 -       Numéro 38 
  
La seconde règle de  décision est la règle bayésienne qui minimise l’espérance  du coût 
d’erreur de classement. En cas de multinormalité et homoscédasticité des distributions de 
probabilité sur chacun des groupes à discriminer, cela conduit à la même fonction 
discriminante: )
2
()'()( 1
DN
DN XTXf μμμμ +−−= −   (1) 
Mais le seuil de décision devient  
22/1
11/2ln π
π
C
C    et non plus 0 comme pour la règle géométrique. 
 est le coût de l’erreur qui consiste à allouer au groupe i une entreprise qui en réalité 
appartient au groupe j, 
jiC /
iπ  est la probabilité a priori d’appartenir au groupe i. 
 
L’un des avantages majeurs de la fonction score  f  est de fournir la possibilité d’ 
interprétation grâce à la contribution de chaque ratio à la valeur du score. 
En effet il est possible de réécrire le score comme suit:  )()( ∑ −=
j
jjj pXXf α
où  est le k1)'( −−= TDN μμα ème coefficient de la fonction  f,  est la valeur  
pivot pour le j
2/)( Dj
N
jjp μμ +=
ème ratio. L’expression )( jjj pX −α  est la contribution du ratio j au score f(X).  
 
Pour un score construit de façon que plus le score est élevé meilleure est la situation de 
l’entreprise, les contributions négatives désignent les points faible de l’entreprise, tandis que 
les contributions positives mettent en évidence les points forts. 
 
La décomposition de la valeur du score en somme des contributions est extrêmement 
utile à l’analyste financier qui évalue l’entreprise (Cf. §5). Généralement, cet expert n’est pas 
statisticien. Il prend en compte beaucoup d’informations variées, la valeur du score est l’une 
d’entre elles. Les contributions l’aident à approfondir l’analyse de l’entreprise et lui signalent 
points faibles et points forts chaque année. Il a ainsi l’opportunité de suivre l’évolution de ces 
points. 
 
2.2.2 La régression logistique (LOGIT) 
 
La régression logistique estime la probabilité a posteriori sous les hypothèses suivantes:  
iXe
iXiYPip αβ −−+
===
1
1)/1(   (2) 
iXe
iXiYPip αβ ++
===−
1
1)/0(1   (3) 
avec  si l’entreprise  et  1=iY Ni∈ 0=iY  si  Di∈ . 
La vraisemblance est    où n est la taille de l’échantillon, . ii YiYi
n
i
pp −= −Π
1
1
)1( ND nnn +=
Les paramètres  α et β  sont estimés par la méthode du maximum de vraisemblance. 
ip est la probabilité a posteriori pour l’entreprise d’être saine. 
En conséquence, l i
i
i
i Xp
ppogit αβ +=−= 1ln , et la règle de décision  peut s’énoncer:  
“L’entreprise i est classée saine ”   ⇔ 00 log1 >+⇔>⇔−> iiii Xpitpp αβ  
© Revue MODULAD, 2008 - 165 -       Numéro 38 
 Une autre règle de décision pourrait être fondée sur KX i >+αβ . L’introduction d’un seuil K 
fournit l’opportunité d’accorder la décision aux objectifs de la banque, quantifiés par les coûts 
d’erreurs de classement (Hand (1981)). 
 
2.2.3 Comparaison entre les modèles LDA et LOGIT 
 
La comparaison très débattue entre les modèles LDA et LOGIT a donné lieu à 
plusieurs investigations – sur leurs propriétés théoriques [T. Amemiya, J. Powell (1983), 
A.W. Lo (1986), B. Efron (1975), Maddala (1999)], leur interprétabilité (le grand avantage de 
la LDA: contributions des variables à la valeur du score), la sensibilité au plan 
d’échantillonnage de LOGIT [G. Celeux, J.P. Nakache (1994)], l’estimation de la probabilité 
a posteriori (soit par la formule théorique, soit grâce au théorème de Bayes appliqué aux 
distributions empiriques). L’efficacité de ces deux modèles sur ratios économiques et 
financiers d’entreprise est généralement très proche. 
 
Dans le cadre paramétrique, la régression logistique a un champ d’hypothèses plus 
large que celui de l’analyse discriminant linéaire de Fisher. De fait, la linéarité du logit 
correspond à la linéarité du quotient des log vraisemblances de chaque groupe: 
βα  x   
)( 
)( +=
xL
xL
N
D  . C’est l’hypothèse fondamentale (H) de la régression logistique. 
 
Dans le cas de la multinormalité et l’homoscédasticité de la probabilité de la 
distribution des variables explicatives sur chaque groupe à discriminer, l’ADL peut être 
appliquée. Cela implique la linéarité du  quotient des log vraisemblances. En conséquence, 
l’ADL apparaît comme un cas particulier du modèle LOGIT. 
 
Si l’hypothèse (H) est vérifiée par les données, la probabilité a posteriori est calculée 
par les formules (2) et (3) qui sont donc les mêmes dans les deux modèles. 
 
Cependant, il se trouve que cette hypothèse n’est généralement pas vérifiée. 
L’utilisation de la formule théorique est alors dangereuse, car elle n’est pas adaptée aux 
données. Quand l’échantillon est suffisamment grand il est de beaucoup préférable d’estimer 
la probabilité par le théorème de Bayes appliqué sur les distributions empiriques du score sur 
chaque groupe comme cela est développé dans le §3. 
3. Probabilité de défaillance 
La probabilité de défaut fournit une mesure de l’intensité du risque. Elle est 
beaucoup plus informative qu’un seuil de décision. Plusieurs questions cruciales 
conditionnent la qualité de l’outil :  
a) L’horizon de la prévision doit être harmonisé avec la nature des données. 
Les variables de bilans sont, par définition, en décalage de plusieurs mois avec le moment 
d’examen de l’entreprise et décrivent ce qui est advenu au cours de l’année écoulée ; elles sont donc 
mieux adaptées à une prévision à moyen terme qu’à court terme. Les bilans apportent 
incontestablement une information utile et robuste à condition de bien harmoniser évaluation et 
horizon de la prévision. 
Avec un horizon d’un an, on pourrait penser être dans la position de créer un indicateur à court 
terme qu’il suffirait de ré-estimer suffisamment souvent pour suivre les conditions auxquelles sont 
© Revue MODULAD, 2008 - 166 -       Numéro 38 
 soumises les entreprises. Mais un tel indicateur suivrait alors de près la conjoncture. Or, une telle 
perspective est très difficile à mener à bien car la ré-estimation fréquente dans un environnement 
mouvant risque de conduire à des fonctions toujours en retard sur l’actualité. 
Le choix est donc fait de travailler sur un horizon de moyen terme avec des variables 
quantitatives reposant sur les bilans et sur lesquelles s’exerce une méthode d’analyse financière dont la 
qualité est reconnue de longue date. Les structures de bilan étant liées à l’appartenance sectorielle des 
entreprises, des scores sont créés par grands secteurs (industrie, commerce de gros, commerce de 
détail, transport, construction, services aux entreprises). 
b) Une estimation des probabilités a posteriori de défaillance bien adaptée aux 
données empiriques grâce au théorème de Bayes s’articule avec la détermination des classes 
de risque. La robustesse temporelle doit être assurée pour la probabilité moyenne par classe de 
risque. L’intervalle de confiance de cette moyenne indique la précision et fournit une mesure 
de ce qui peut se passer dans le pire des cas. 
 
Encadré 1 : Utilisation du  théorème de Bayes pour calculer la probabilité a posteriori 
La probabilité de défaillance d’une entreprise e dont la valeur du score appartient à l’ intervalle r peut s’écrire comme suit: 
)(
)/(
)/(
rsp
DersprsDeP D∈
∈∈=∈∈ π
NNDD
DD
pp
p
ππ
π
+=           (4) 
Les probabilités conditionnelles à l’appartenance à un des groupes sont: 
)/( DersppD ∈∈=  et )/( NersppN ∈∈=  
La probabilité a priori de défaillance est Dπ  et  DN ππ −=1  est la probabilité de non défaillance. 
 
Connaissant les fonctions de densité du score sur chacun des groupes fD  et fN  , la formule (4) devient : 
 
∫∫
∫
∈∈
∈
−+=∈∈
rz
ND
rz
DD
rz
DD
dzzfdzzf
dzzf
rsDeP
)()1()(
)(
)/( ππ
π
 
 
La probabilité peut être estimée en utilisant le modèle théorique ou être calculée sur la 
base des distributions empiriques du score par le théorème de Bayes. Le choix entre ces deux 
méthodes dépend de la représentativité des fichiers d’étude et de la vérification par les 
données des hypothèses nécessaires à l’application des formules théoriques (Cf. Bardos , Zhu 
(1997), Bardos (2001), Kendaoui (2007)). 
Du fait de l’importance et de la représentativité des fichiers de la Banque de France, 
les distributions empiriques peuvent être utilisées avec efficacité, car le résultat est plus 
proche de la réalités que ne serait celui issu de la formule théorique puisque les hypothèses 
sous-jacentes ne sont pas véritablement vérifiées. Comme on va le voir la méthode fondée sur 
les distributions empiriques permet de plus de contrôler la précision des probabilités estimées, 
et l’homogénéité des classes de risques.  
L’algorithme de cette estimation est le suivant : la probabilité a posteriori de 
défaillance est calculée sur des petits intervalles de score chaque année; pour chaque petit 
intervalle la moyenne et l’écart type des résultats annuels sont calculés. Les intervalles où les 
moyennes sont proches sont regroupés dans le but de réduire l’écart type et l’amplitude de 
l’intervalle de confiance de la nouvelle moyenne calculée sur le nouvel intervalle issu du 
regroupement des petits intervalles. Ce processus est renouvelé jusqu’à obtenir des intervalles 
de score où l’écart type est suffisamment réduit pour éviter le chevauchement des intervalles 
de confiance de la probabilité moyenne. Le tableau 2 présente les résultats pour le score 
BDFI2 qui concerne l’industrie manufacturière. Dans la première colonne sont présentés les 
intervalles qui définissent les classes de risque finalement retenues. Les colonnes 1998 à 2001 
© Revue MODULAD, 2008 - 167 -       Numéro 38 
 fournissent les probabilités a posteriori par intervalle calculées sur chacun des fichiers 
annuels.  Dans la colonne « moyenne » figure la probabilité de défaillance associée à chaque 
classe de risque calculée comme moyenne des résultats 1998 à 2001. Dans la dernière colonne 
la limite supérieure de l’intervalle de confiance représente la probabilité de défaillance dans le 
pire des cas au niveau de 99% (c'est-à-dire avec un risque de se tromper de 1%). Les 
intervalles de confiance liées aux  classes de risque voisines ne se chevauchent pas ce qui 
montre le degré d’homogénéité au sein de chaque classe de risque.  
 
Tableau 2: Probabilité de défaillance a posteriori par intervalle de score 
À l’ horizon de trois ans 
               Industrie 1998-2001 
Probabilité de défaillance a priori à l’ horizon de trois ans : 7.66% 
 
Probabilité a posteriori calculée chaque année Intervalle de 
confiance 
 
Intervalle de score  
 
Classe 
de 
risque   
1998 
  
1999 
  
2000 
  
2001 
moyenne 
μ 
Ecart-
type  
σ inf sup 
BDFI2 <-2.4 1 46.39 45.35 47.83 45.06 46.16 1.25 44.27 48.04 
-2.4 ≤ BDFI2<-1.8 2 31.98 34.62 33.43 35.54 33.89 1.54 31.58 36.20 
-1.8 ≤ BDFI2<-0.8 3 22.39 23.00 23.81 24.25 23.36 0.83 22.12 24.60 
-0.8 ≤ BDFI2<-0,3 4 16.08 17.64 18.56 19.49 17.94 1.45 15.76 20.13 
-0,3 ≤ BDFI2<0 5 10.89 11.19 13.04 15.89 12.75 2.30 9.31 16.20 
0 ≤ BDFI2<0,4 6 7.50 8.69 9.52 9.68 8.85 1.00 7.35 10.34 
0,4 ≤ BDFI2<1,2 7 3.46 3.67 4.45 4.54 4.03 0.55 3.21 4.85 
1,2 ≤ BDFI2<1,6 8 2.03 2.04 1.64 2.05 1.94 0.20 1.64 2.25 
1,6 ≤ BDFI2<2,4 9 0.84 0.68 0.80 0.91 0.81 0.10 0.67 0.95 
2,4 ≤ BDFI2 10 0.27 0.28 0.34 0.37 0.31 0.05 0.24 0.38 
Source : Banque de France – Fiben   mars 2005 
 
c) La stabilité des entreprises dans les classes de risque est étudiée grâce aux matrices 
de transition. Cette étude participe au débat, vif actuellement, sur l’estimation « à travers le 
cycle » c’est-à-dire peu sensible à la conjoncture avec un horizon de précision à long terme, 
ou l’estimation «  dans le contexte actuel » donc avec les critères du moment avec un horizon 
à court terme. 
4. Contrôles et maintenance 
4.1  Exemples de contrôles 
 
Une fois le score construit et validé sur des échantillons tests, sa qualité doit être 
contrôlée. Un des contrôles les plus nécessaires est d’examiner par sous population si le 
score a bien les propriétés attendues. On présente ici deux exemples de contrôle. 
Contrôle n°1 : Plus l’échéance de la défaillance se rapproche plus le score devient 
négatif. Le graphique 1 témoigne bien de cette caractéristique. 
Contrôle n°2 : Le score est discriminant quel que soit la taille de l’entreprise. Le 
graphique 2 montre que la fonction score des firmes défaillantes a la même distribution 
quelque soit la taille, de même pour les firmes non défaillantes. Cela montre que les 
ratios clignotants de la défaillance utilisés dans la fonction score BDFI2 sont pertinents 
quelque soit la taille et que leur importance relative mesurée par les coefficients de la 
fonction score est aussi la même quelle que soit la taille. Ce résultat est extrêmement 
important car il permet d’utiliser la même fonction pour différentes tailles d’entreprise. 
Par contre cela ne veut pas dire que la probabilité de défaillance a posteriori pour une 
© Revue MODULAD, 2008 - 168 -       Numéro 38 
 valeur de score donnée est la même quelque soit la taille. En effet la probabilité a 
posteriori dépend de la probabilité a priori qui elle diffère grandement selon la taille des 
entreprises. Ceci est du au fait que les grandes entreprises réagissent aux difficultés par 
divers moyens que n’ont pas souvent les PME  (par exemple en se restructurant).  
    
   Graphique 1  
Distribution du score BDFI2 selon l’horizon de la défaillance  
0
2
4
6
8
10
12
14
16
-5.1 -4.7 -4.3 -3.9 -3.5 -3.1 -2.7 -2.3 -1.9 -1.5 -1.1 -0.7 -0.3 0.1 0.5 0.9 1.3 1.7 2.1 2.5 2.9 3.3 3.7 4.1 4.5
1-yr horizon 2-yr horizon 3-yr horizon Non-failing
Scores
 
Source : Banque de France – Fiben  
 
 
Graphique 2 
Distribution du score BDFI2 par catégorie selon la taille de l’entreprise 
0
2
4
6
8
10
12
14
16
-5.1 -4.7 -4.3 -3.9 -3.5 -3.1 -2.7 -2.3 -1.9 -1.5 -1.1 -0.7 -0.3 0.1 0.5 0.9 1.3 1.7 2.1 2.5 2.9 3.3 3.7 4.1 4.5
Scores
Failing, staff count<20 Non-failing, staff count<20 Failing, 20<=staff count<50
Non-failing, 20<=staff count<50 Failing, 50<=staff count<500 Non-failing, 50<=staff count<500
 
Source : Banque de France – Fiben  
© Revue MODULAD, 2008 - 169 -       Numéro 38 
 4.2  La maintenance 
 La maintenance correspond à une autre nécessité. Une fois le score mis en place dans les 
programmes et utilisé, il est de la responsabilité du fournisseur de score de vérifier qu’il met à 
disposition un score dont la qualité ne se détériore pas. Le tracé des distributions de score par catégorie 
et les pourcentages de bon classement permettent ce contrôle. Trois situations peuvent se produire : 
- le score est stable et son pouvoir discriminant est inchangé. Le schéma 3 obtenu lors de la création 
du score reste toujours valable ; 
- le score se modifie par translation (par exemple vers les valeurs positives comme l’indique le 
schéma 4) mais son pouvoir discriminant traduit par la séparation des courbes reste intact. Il 
convient alors de ré-estimer les probabilités de défaillance et les classes de risques ; 
- le score n’est plus assez discriminant (schéma 5). Dans ce cas il faut ré-estimer le score lui-même 
bien avant d’en arriver à cette situation.      
Schéma 3:  Distribution des scores par catégorie au moment où le score est estimé (periode 1) 
 
Failing 
Period 1  
Score 
Non-failing 
0
Frequency
 
Schéma 4:  A la période 2 les distributions de scores par catégorie sont similaires mais 
translatées vers des valeurs plus positives 
 
Failing 
Period 2  
Score 
Non-failing  
0
Frequency
 
Schéma 5: A la période 3, plusieurs années après la construction, les distributions de score par 
catégorie se sont beaucoup rapprochées, le score n’est plus discriminant  
 
Failing
Score 
Non-failing 
0
FrequencyPeriod 3 
 
© Revue MODULAD, 2008 - 170 -       Numéro 38 
 5. Utilisation 
5.1 Diagnostic individuel 
 
Le credit scoring constitue un premier pas dans l’analyse des cas individuels. A la 
Banque de France, un dispositif complet est mis à disposition des analystes financiers : le 
score est suivi sur plusieurs années, les aides à l’interprétation qui l’accompagnent permettent 
un usage opérationnel éclairé : probabilités de défaillance, contributions des ratios au score, 
mise en situation par rapport au secteur. 
 
Par ailleurs, c’est un grand avantage pour le statisticien de pouvoir connaître les 
inadaptations de l’outil que lui signalent les experts utilisateurs. Ceux-ci indiquent sur les cas 
concrets les difficultés de mesure. Leurs remarques permettent d’améliorer la mesure 
statistique des concepts d’analyse financière et la compréhension des processus de 
défaillance. Ces renseignements pourront être mis en œuvre lors de révision de l’outil. 
 
Un exemple de cas est présenté ci-dessous tel qu’il apparaît aux analystes financiers de 
la Banque de France, comme aux experts des banques ayant adhéré aux services de FIBEN 
qui se présentent sous forme de modules accessibles via un lien internet sécurisé. Il s’agit 
d’une entreprise du secteur Commerce de détail sur laquelle le score BDFCD est calculé. 
  
Interprétation 
Encadré « INDICATEURS DE DEFAILLANCE » 
De 2000 à 2002, l’entreprise a embauché, notamment en 2002, où la consolidation de sa situation a été 
importante. La probabilité de défaillance à 3 ans, associée au score, passe de 3,9 % à 1,6 %. 
Parallèlement, l’entreprise a nettement amélioré sa situation puisque partant d’une classe risquée (probabilité 
de défaillance de 7,7 % > taux de défaillance de 2,7 %), elle est passée en classe neutre en 2001 et a atteint une 
classe favorable en 2002. Sans être encore dans les deux meilleures classes, l’entreprise a bien assaini sa 
position. 
 
Encadré « POSITIONNEMENT DE L’ENTREPRISE PAR RAPPORT AU SECTEUR » 
Comparativement aux firmes de son secteur d’activité, l’entreprise se situait en 2000 et 2001 dans le quart 
inférieur sans être toutefois parmi les 10 % les plus risquées. En 2002, son score se situe au-dessus du quart 
inférieur, sans atteindre toutefois la médiane (ce que confirme la dernière ligne du tableau : 0,412 < 1,498). 
 
Encadré « SCORE ET CONTRIBUTIONS DES RATIOS » 
En 2001, les contributions révèlent deux points faibles : des concours bancaires courants trop élevés et un délai 
fournisseurs trop long. Le premier point est à rapprocher du poids de l’endettement dont le niveau est 
conséquent, sans être excessif (contribution 5 proche de 0) : l’endettement comporte donc une part importante 
de court terme. La structure du haut bilan est d’ailleurs assez satisfaisante (contribution 6 positive). Toutefois, 
l’entreprise semble rechercher de la trésorerie avec des dettes fiscales et sociales conséquentes (contribution 1 
proche de 0 bien inférieure à la médiane du secteur 0,314) et des délais fournisseurs un peu longs. L’importance 
des charges financières vis-à-vis de l’excédent brut global est à un niveau correct. 
 
En 2002, toutes les contributions s’améliorent, sauf la première qui subit un très léger recul. Cette amélioration 
concerne surtout l’endettement financier qui s’allège grâce notamment à une réduction des crédits bancaires 
courants. Par ailleurs, la solvabilité à court terme se renforce (contribution 4). 
© Revue MODULAD, 2008 - 171 -       Numéro 38 
 Exemple d’une information sur le score accessible aux établissements de 
crédit via le Module 38 de FIBEN 
FIBEN       
CONFIDENTIEL      20/10/2003 
[Indicateurs de défaillance]   [Positionnement]   [Score et contributions] 
Analyse du risque 
Score Banque de France 
999 999 999  SOCIÉTÉ X  
Adresse   1 rue du Paradis. 
    CP VILLE 
Banque de France  SUCCURSALE Y 
 Secteur d’activité  503A     COMMERCE DE GROS D’ÉQUIPEMENTS AUTOMOBILE au   31/12/2002  
INDICATEURS DE DÉFAILLANCE 
 Date de clôture 12/2000  12/2001  2/2002  
 
 Durée 12 mois  12 mois  12 mois  
 Effectif 101  109  138  
 Probabilité de défaillance à 3 ans (%) 7,7  3,9  1,6  
 La probabilité est à relativiser en cas d’effectifs supérieurs à 500 
 Taux de défaillance sur 3 ans dans le commerce de détail et la réparation automobile  2,7 % 
POSITIONNEMENT DE L’ENTREPRISE PAR RAPPORT AU SECTEUR 
 Secteur d’activité  503A  
 Date de clôture 12/2000  12/2001  2/2002  
 
 Score supérieur au 9e décile  
 Score compris entre le 3e quartile et le 9e décile 
 Score compris entre le 2e et le 3e quartile    
 Score compris entre le 1er et le 2e quartile   X 
 Score compris entre le 1er décile et le 1er quartile  X X 
 Score inférieur au 1er décile     
 Plus le score est élevé, meilleur est le positionnement de l’entreprise par rapport à son secteur  
SCORE ET CONTRIBUTIONS DES RATIOS (comparaison aux médianes du secteur) 
 Contributions Individuelles Secteur 
 Date de clôture    12/2001   12/2002    2002  
 1. Poids des dettes fiscales et sociales -0,035 -0,060  0,314 
 2. Délai fournisseur  -0,171 -0,166  -0,128 
 3. Importance des CBC * -0,534 -0,325  0,367 
 4. Importance des charges financières dans l’EBG ** 0,346 0,488  0,672 
 5. Poids de l’endettement financier 0,038 0,137  0,256 
 6. Poids FRNG *** 0,323 0,338  0,437 
 SCORE BDFCD   -0,033 0,412  1,498 
Informations couvertes par le secret bancaire conformément aux dispositions du Contrat FIBEN 
 * CBC = concours bancaires courants                **EBG = excédent brut global    *** FRNG = fonds de roulement net global 
 
Interprétation du score BDFCD grâce aux contributions des ratios 
La contribution 1  est liée au poids des dettes fiscales et sociales relativement à l’activité ; quand celle-ci s’élève, ces dettes 
s’allègent. 
La contribution 2  est liée à l’importance des délais des dettes fournisseurs ; quand celle-ci s’élève, les délais des dettes 
fournisseurs raccourcissent. 
La contribution 3  est liée à l’importance des concours bancaires courants comparativement  au niveau de l’activité ; quand 
celle-ci s’élève, la part des concours bancaires diminue. 
La contribution 4  est liée à l’importance des charges financière vis-à-vis de l’excédent brut global ; quand celle-ci s’élève, 
l’entreprise renforce sa solvabilité à court terme. 
La contribution 5  est liée au poids de l’endettement financier (incluant le crédit-bail et les dettes groupe et associés) ; quand 
cette contribution augmente, l’importance de la dette diminue. 
La contribution 6 est liée à la structure du bilan ; quand elle s’élève, la structure s’améliore  
38 
© Revue MODULAD, 2008 - 172 -       Numéro 38 
  
5.2 Évaluation du risque sur une population 
 
Les tableaux de bord sur une clientèle sont recommandés par le comité de Bâle. 
La Banque de France en a élaborés quelques exemples pour assurer le suivi d’une 
population: méthode IRISK (Bardos M., Plihon D. (1999); impact économique de la 
défaillance (Nahmias L. (2005)). On présente ci-dessous quelques uns des indicateurs IRISK 
pour les PME du secteur Industrie.  
Un score probabilisé offre une mesure individualisée du risque de crédit. Disposant de 
cette information sur une population d’entreprises, des analyses statistiques en termes de suivi 
d’un portefeuille d’engagement peuvent être menées. 
La méthode IRISK , développée par la Banque de France, propose des tableaux de 
bord mettant en jeu plusieurs indicateurs de risque indépendants. Ils permettent une 
caractérisation du type de risque dont relève un portefeuille et son suivi sur plusieurs années. 
Cette méthode est mise en œuvre dans les études sectorielles de l’Observatoire des 
entreprises. Elle est de plus en plus développée et une implémentation annuelle complète sur 
tous les grands secteurs ayant un score est systématiquement réalisée. 
À titre d’exemple, un des tableaux de bord d’IRISK est présenté ci-dessous sur les 
PME de l’industrie. De tels tableaux peuvent être constitués sur des ensembles répondant à 
des critères souhaités (taille, secteur, liste d’entreprises constituant une clientèle). 
Tableau 3 
INDICATEURS IRISK 
PME : entreprises industrielles dont le chiffre d’affaires est inférieur à 50 000 K€ 
Industrie 
% des entrep. 
dans les 
classes 
risquées  
(1) 
% des effectifs 
dans les 
classes 
risquées 
(2) 
% du CA 
dans les 
classes 
risquées 
(3) 
Part risquée 
moyenne (%) de 
l'endettement 
bancaire 
(4) 
Part risquée 
max. (%) de 
l'endettement 
bancaire 
(5) 
Effectifs 
salariés 
moyens  
(6) 
Part de 
l'endet. banc. 
(%) des 1% 
les plus 
endettées 
(7) 
1998 18,95 19,28 16,05 8,89 10,08 39 19,85 
1999 17,48 18,87 15,45 8,55 9,71 39 19,80 
2000 16,07 18,68 15,33 8,39 9,52 39 19,87 
2001 15,77 19,26 16,03 8,48 9,62 39 19,49 
2002 16,96 19,69 16,13 8,77 9,90 39 19,62 
2003 17,57 20,20 16,39 8,38 9,48 38 19,46 
Source et réalisation : Banque de France – Observatoire des entreprises Mise à jour : août 2005 4
Légende : 
– la proportion  de firmes dans les classes les plus risquées, où les classes risquées prises en compte 
dans ce tableau sont  les classes 1, 2, 3, 4 (5 exclue) associées au score BDFI2 ; 
– la part des effectifs et du chiffre d’affaires des firmes des classes risquées ; ces pourcentages 
comparés au pourcentage de firmes risquées du secteur donnent des éléments sur la taille des entreprises 
risquées ;   
– la part risquée moyenne de l’endettement bancaire dans le secteur à l’horizon de trois 
ans : ∑∑ iii EEpx /100 , où pour chaque entreprise i, désigne sa probabilité de défaillance au cours 
des trois prochaines années, et le montant de son encours de crédit bancaire ;  
ip
iE
                                                 
4 En août 2005 un nombre important  de bilans clôturés au 31/12/2004 sont d’ores et déjà collectés et traités au plan individuel 
par les implantations de la Banque de France. Cependant, ce n’est généralement qu’en automne que les traitements de 
centralisation statistique sont totalement finalisés. C’est pourquoi le tableau 3 ne présente pas de données 2004. 
© Revue MODULAD, 2008 - 173 -       Numéro 38 
 – la part maximale de l’endettement bancaire dans le secteur à l’horizon de trois ans : 
, où  est la probabilité maximale qui représente le risque dans le pire des 
cas (elle est fournie par la borne supérieure de l’intervalle de confiance de la probabilité moyenne, 
cf. tableau 2 : « industrie – score BDFI2 ») ;  
∑∑ iii EEpmx /100 ipm
– les effectifs salariés moyens donne une première mesure de la concentration du secteur ;  
– l’indicateur de concentration : la part de l’endettement bancaire du secteur supporté par 1 % des 
entreprises les plus endettées.  
 
Le tableau 3 fait apparaître dans le secteur de l’industrie une diminution du risque de 
crédit des PME en début de période, comme le montrent la baisse du pourcentage 
d’entreprises risquées jusqu’en 2001 (colonne 1) et la réduction de la part risquée de 
l’endettement bancaire jusqu’en 2000 (colonnes 4 et 5). 
 
Après ces dates, chacun de ces indicateurs augmente. 
 
Toutefois, si en 2002, la part risquée de l’endettement bancaire atteignait un maximum 
anticipant la montée des risques l’année suivante (notamment avec la multiplication des 
procédures collectives), cette part se restreint en 2003, annonçant l’amélioration de la 
situation des entreprises en 2004. Est ainsi mis en valeur le caractère prédictif de ces 
indicateurs. 
 
Concernant les firmes dans les classes risquées, l’importance de leurs effectifs 
(colonne 2), mais la relative faiblesse de leur chiffre d’affaires (colonne 3) semblent indiquer 
que les entreprises sont de taille assez importante en termes d’effectifs salariés. 
 
Enfin, la concentration de l’endettement est relativement stable (colonne 7). 
 
6. Recherches sur le risque de crédit utilisant un 
score  
Les scores construits à la Banque de France couvrent un vaste champ de secteurs 
d’activité. Implémentés sur un ensemble représentatif des firmes dont le chiffre d’affaires 
excède 0,75 millions d’euros, ils permettent d’examiner de nombreuses questions liées au 
risque de crédit.  
La contagion du risque peut être étudiée via le fichier des impayés sur effets de 
commerce de la Banque de France (Stili D. ( 2003, 2005), Bardos M., Stili D. (2006)). 
La corrélation du risque entre entreprises a un impact important sur l’évaluation des 
pertes potentielles (S. Foulcher, C. Gouriéroux, A. Tiomo (2004) ).  
Le lien entre risque et conjoncture s’il peut être explicité permettrait de mieux 
anticiper le risque de défauts futurs au regard de variables macro économique ou de facteurs 
spécifiques (E. Bataille, C. Bruneau, F. Michaud (2005)).  
Les trajectoires des entreprises fournissent l’étude dynamique du risque (M. Bardos 
(1998) ). 
Les matrices de transitions entre classes de risque permettent d’étudier le caractère 
Markovien ou non des processus de défaillance.  
La concentration de l’endettement peut engendrer un risque majeur pour les banques 
(Bardos M., Plihon D. (1999) ). 
© Revue MODULAD, 2008 - 174 -       Numéro 38 
  
Par ailleurs, la réflexion sur les modèles de risque de crédit nécessite des comparaisons 
entre systèmes de notation des entreprises. Des travaux statistiques sur la simulation des 
distributions des taux de défaut par note permettent d’établir des échelles de référence dans 
ces comparaisons (S. Blockwitz, S. Hohl (2001) , A. Tiomo (2002)). 
 
 
 Ces exemples montrent l’intérêt d’un score qui mesure la probabilité de défaillance 
non seulement  pour les décideurs, et les pouvoirs publics, mais aussi pour les chercheurs en 
économie. De nombreuses questions liées au risque de crédit et à la stabilité financière 
peuvent ainsi être examinées.   
 
 
 
Bibliographie 
E. I. ALTMAN, A. SAUNDERS (1998) : Credit Risk Measurement: developments over the last 20 years, 
Journal of Banking and Finance 21, p. 1721-1742, North Holland ; 
T. AMEMIYA, J. POWELL (1983): A comparison of Logit model and normal discriminant analysis when the 
independent variables are binary, in Karlin, Amemiya, Goodman, edition Studies in econometrics, time 
series and multivariate statistics, Academic Press New York ; 
T.W. ANDERSON (1984) : An introduction to multivariate statistical analysis, Wiley, Chapter 6 “Classification 
of observations” ; 
BAESENS, T VAN GESTEL, S VIAENE, M STEPANOVA, J SUYKENS, J VANTHIENEN 
(2003) : Benchmarking state-of-art classification algorithms for credit scoring, Journal of the 
Operational Research Society . 
M.BARDOS (2007) : What is at Stake when Estimating the Probability of Default using a Scoring Function ?, 
Credit Risk Assessment Revisited: Methodological Issues and Practical Implications, European Committee 
of Central Balance Sheet data Offices Working Group on Risk Assessment, p. 95-118; 
M.BARDOS (2007) : What is at Stake in the Construction and the Use of Credit Scores ?, Computationnal 
Economics n°29, p. 159-172; 
M.BARDOS (2005) : Les scores de la Banque de France : leur développement, leurs applications, leur 
maintenance, Bulletin de la Banque de France n°144, décembre ; 
http://www.banque-france.fr/fr/publications/telechar/bulletin/etu144_6.pdf
 
M. BARDOS, S. FOULCHER, E. BATAILLE (2004) : “ Les scores de la Banque de France: méthode, 
résultats,applications ”, Banque de France, décembre; 
http://www.banque-france.fr/fr/publications/catalogue/dom_2i.htm
 
M. BARDOS (2001) : Analyse discriminante: application au risque et scoring financier, Dunod 
M. BARDOS (2001) : Développements récents de la méthode des scores de la Banque de France, Bulletin de la 
Banque de France n°90, juin ; http://www.banque-france.fr/fr/publications/telechar/bulletin/etud90_4.pdf
 
M. BARDOS, D. PLIHON (1999) : Détection des secteurs risqués – La méthode IRISK, Bulletin de la banque 
de France n°69, septembre 1999. 
http://www.banque-france.fr/fr/publications/telnomot/bulletin/etud69_2.pdf
 
© Revue MODULAD, 2008 - 175 -       Numéro 38 
 M. BARDOS (1998) : “ Detecting the risk of company failure ”, The Journal of Banking and Finance n° 22, 
1998 ; 
M. BARDOS (1998) : Le score BDFI : du diagnostic individuel à l’analyse de portefeuille, Les études 
de l’Observatoire des entreprises, Banque de France. 
http://www.banque-france.fr/fr/publications/catalogue/dom_2j.htm
 
M. BARDOS, D. STILI (2006) : La contagion du risque via les impayés sur effets de 
commerce, bulletin de la Banque de France n°  
 http://www.banque-france.fr/fr/publications/telechar/bulletin/etu148_3.pdf
 
M. BARDOS, W.H.ZHU (1997): Comparaison entre l’analyse discriminante linéaire et les réseaux de neurones, 
application à la détection des défaillance d’entreprises, Revue de Statistique Aplliquée, 1997, pages 77-
100 ; 
G. BARRET, S.G. DONALD : Consistent Tests for Stochastic Dominance, working paper march 
2002. 
 
E. BATAILLE, C. BRUNEAU, F. MICHAUD (2007) : Use of the principal components method to 
follow the link between business cycle and risk of companies’failure, Computationnal Economics 
n°29 (2007). 
 
S. BLOCKWITZ, S. HOHL (2001) : Reconciling Ratings, Risk Magazine, june 2001  
L. BREIMAN, J.H. FREIDMAN, R.A. OHLSON, C.J. STONE (1984): Classification and regression trees, 
Edition Wadsworth International Group, Californie ;  
G. CELEUX, J.P. NAKACHE (1994) : Analyse discriminante sur variables qualitatives, Polytechnica ; 
R. DAVIDSON, J.Y. DUCLOS : Statistical Infernece for Stochastic Dominance and for the 
Measurement of Poverty and Inequality, working paper november 1999 
B. EFRON (1975) : The efficiency of logistic regression compared to normal discriminant analysis, Journal 
American Statistical Society  n°70, pp.892-898; 
B. EFRON, R.J. TIBSHIRANI  (1993) : An introduction to the Bootstrap, ed. Chapman & Hall; 
S. FOULCHER, C. GOURIEROUX, A. TIOMO (2004) : La corrélation de migration : méthode 
d’estimation et application aux historiques de notation des entreprises françaises, Etudes et 
Recherches de l’Observatoire des entreprises, Banque de France 
http://www.banque-france.fr/fr/publications/observatoire/9.htm
 
S. FOULCHER, C. GOURIEROUX, A. TIOMO (2003) : La structure par terme des taux de défauts 
et rating, Etudes et Recherches de l’Observatoire des entreprises, Banque de France 
http://www.banque-france.fr/fr/publications/observatoire/1.htm
 
R. GNANADESIKAN and panel of authors (1989) : Discriminant Analysis and Clustering, Statistical Science, 
vol. 4, N° 1, p.34-69 ; 
C. GOURIEROUX (1989) : Économétrie des variables qualitatives, Économica ; 
D.J. HAND (1981): Discrimination and classification, Wiley series in probability and mathematical statistics ; 
HRISTACHE, DELCROIX, PATILEA (2004) on semi parametric m-estimation. Journal of Statistic 
Planning and Inference, in press 2004. 
 
M.G. KENDALL, A. STUART (1961) : The advanced Theory of Statistics, chapter 32, « Some Use of Order 
Statistics, Distribution free, Confidence Intervals for Quantiles », Griffin Londres 
 
L.KENDAOUI (2005): Risque et conjoncture, Mémoire de stage à la Banque de France   
 
© Revue MODULAD, 2008 - 176 -       Numéro 38 
 L.KENDAOUI (2007):          , Actes des journées de ASMDA2007   
G.J. Mc LACHLAN (1992) : Discriminant Analysis and Statistical Pattern Recognition, Wiley, New-York ; 
L. LEBART, A. MORINEAU, M. PIRON (1998) : Statistique Exploratoire Multidimensionnelle, Dunod ; 
L. LELOGEAIS (2003) : Un score sur variables qualitatives pour la détection précoce des défaillances 
d’entreprises, Bulletin de la Banque de France n°114, juin 2003. 
http://www.banque-france.fr/fr/publications/telechar/bulletin/etu114_1.pdf
  
A.W. LO (1986) : Logit versus discriminant analysis, Journal of Econometrics n°31, North Holland, 1986, pp. 
151-178 . 
G.S. MADDALA (1999): Limited-dependent and qualitative variables in econometrics, Cambridge University 
Press; 
F. MICHAUD (2004) : mémoire de stage à l’Observatoire des entreprises de la Banque de France. 
L. NAHMIAS (2005): Impact économique des défaillances d’entreprises, Bulletin de la Banque de France 
n°137, mai; http://www.banque-france.fr/fr/publications/telechar/bulletin/etu137_3.pdf
C. R. RAO (1973): Linear Statistical Inference and Its Applications, Wiley 
G. SAPORTA (1990) : Probabilités, analyse des données et statistique, Technip 
D. STILI (2003) : Les incidents de paiement sur effets de commerce, Etudes et Recherches de 
l’Observatoire des entreprises, Banque de France 
http://www.banque-
france.fr/fr/publications/telechar/observatoire/incidents_paiement_impossibilite.pdf
 
D. STILI (2005) : Détéction précoce des défaillances d’entreprises et contagion du risque, Thèse de 
doctorat, Université Paris I. 
S. THIRIA, Y. LECHEVALLIER, O. GASCUEL, S. CANU (1997) : Statistique et méthodes neuronales, 
Dunod ; 
A. TIOMO (2002) : Risque de crédit et variabilité des taux de défaut : une analyse empirique par 
simulation, Etudes et Recherches de l’Observatoire des entreprises, Banque de France 
http://www.banque-france.fr/fr/publications/telechar/observatoire/risque.pdf
 
VESSEREAU (1987) : « Une propriété peu connue : l’intervalle de confiance de la médiane » Revue 
de Staistique Appliquée XXXV (1), pp. 5-8 
 
WGRA : Credit Risk Assessment Revisited : Methodological Issues and Practical Implications (2007) 
http://www.banque-france.fr/fr/eurosys/bilans/wgra.htm
 
 
 
 
 
 
© Revue MODULAD, 2008 - 177 -       Numéro 38 
