K-moyennes contraintes par un classifieur
Application Ã  la personnalisation de scores de campagnes
Vincent Lemaireâˆ—, Nicolas Creffâˆ—âˆ—, Fabrice ClÃ©rotâˆ—
âˆ— Orange Labs, 2 avenue Pierre Marzin, 22300 Lannion
âˆ—âˆ—Epita 14-16 rue Voltaire 94276 Kremlin BicÃªtre Cedex
RÃ©sumÃ©. Lorsquâ€™on dÃ©sire contacter un client pour lui proposer un produit on
calcule au prÃ©alable la probabilitÃ© quâ€™il achÃ¨tera ce produit. Cette probabilitÃ©
est calculÃ©e Ã  lâ€™aide dâ€™un modÃ¨le prÃ©dictif pour un ensemble de clients. Le ser-
vice marketing contacte ensuite ceux ayant la plus forte probabilitÃ© dâ€™acheter le
produit. En parallÃ¨le, et avant le contact commercial, il peut Ãªtre intÃ©ressant de
rÃ©aliser une typologie des clients qui seront contactÃ©s. Lâ€™idÃ©e Ã©tant de proposer
des campagnes diffÃ©renciÃ©es par groupe de clients. Cet article montre comment
il est possible de contraindre la typologie, rÃ©alisÃ©e Ã  lâ€™aide des k-moyennes, Ã 
respecter la proximitÃ© des clients vis-Ã -vis de leur score dâ€™appÃ©tence.
1 Introduction
1.1 ProblÃ©matique industrielle
Le data mining consiste en lâ€™ensemble des mÃ©thodes et techniques qui permettent dâ€™extraire
des informations Ã  partir dâ€™une grande masse de donnÃ©es. Son utilisation permet dâ€™Ã©tablir des
relations entre ces donnÃ©es et par exemple de dÃ©finir des comportements type de clients dans
le cadre de la gestion de la relation client.
Lorsquâ€™on dÃ©sire contacter un client pour lui proposer un produit on calcule au prÃ©alable
son appÃ©tence Ã  ce produit. Il sâ€™agit lÃ  de calculer la probabilitÃ© quâ€™il achÃ¨tera ce produit. Cette
probabilitÃ© (encore appelÃ©e score) est calculÃ©e Ã  lâ€™aide dâ€™un modÃ¨le prÃ©dictif pour un ensemble
de clients (le pÃ©rimÃ¨tre de la campagne). Le calcul du score exploite un grand nombre de
variables explicatives issues du systÃ¨me dâ€™information.
Les clients sont ensuite triÃ©s dans lâ€™ordre dÃ©croissant de leur probabilitÃ© dâ€™appÃ©tence. Le
service marketing ne contacte ensuite que les plus appÃ©tents (nommÃ©s â€œtops scoresâ€), i.e. ceux
ayant la plus forte probabilitÃ© dâ€™acheter le produit. En parallÃ¨le et avant le contact commercial
il peut Ãªtre intÃ©ressant de rÃ©aliser une typologie des clients qui seront contactÃ©s. Lâ€™idÃ©e Ã©tant de
proposer des campagnes diffÃ©renciÃ©es par segment. Un argumentaire commercial est construit
pour chaque groupe de clients aprÃ¨s analyse des caractÃ©ristiques du groupe : Ã¢ge, CSP, offres
actuellement dÃ©tenues. Il est frÃ©quent (pour des raisons pratiques de temps dâ€™analyse) que
lâ€™analyse du groupe se rÃ©sume en lâ€™analyse du centre (ou reprÃ©sentant) du groupe.
Aujourdâ€™hui, cette typologie est trÃ¨s souvent rÃ©alisÃ©e de maniÃ¨re non supervisÃ©e Ã  lâ€™aide
dâ€™une technique de partitionnement de type k-moyennes. La valeur de k est prÃ©dÃ©finie et la
- 143 -
Personnalisation de score de campagnes
mÃ©trique utilisÃ©e ne tient pas compte du prÃ©dicteur (le modÃ¨le dÃ©livrant la probabilitÃ© dâ€™appÃ©-
tence) ce qui pose deux problÃ¨mes :
1. Les clients dans les clusters ne sont pas liÃ©s par leur probabilitÃ© dâ€™appÃ©tence : un cluster
peut contenir des clients trÃ¨s appÃ©tents et des clients peu appÃ©tents. Lâ€™analyse du centre
du groupe rend lâ€™argumentaire commercial erronÃ©.
2. Les segments crÃ©Ã©s ne sont pas stables dans le temps lorsque le classifieur est dÃ©ployÃ©
plusieurs mois consÃ©cutivement sur le mÃªme pÃ©rimÃ¨tre de campagne (voir les deux cri-
tÃ¨res section 4.3).
On pourrait essayer alors de poser ce problÃ¨me comme un problÃ¨me supervisÃ© en essayant
de construire un second modÃ¨le de classification (ou de rÃ©gression) avec pour variable cible
les scores issus du premier classifieur. On obtiendrait alors, par exemple dans le cas dâ€™un arbre
de dÃ©cision, les raisons qui impactent le plus les scores. Cette voie bien quâ€™intÃ©ressante ne
correspond pas Ã  notre souhait qui est bien plus de dÃ©couvrir la â€œstructureâ€ des scores plus
que de grouper les clients par puretÃ© des scores. En effet deux mÃªmes scores peuvent avoir
Ã©tÃ© â€œobtenusâ€ par des voies diffÃ©rentes et ce sont ces voies, ou raisons, que nous cherchons Ã 
dÃ©couvrir.
Aussi pour rÃ©soudre les problÃ¨mes citÃ©s ci-dessus cet article propose de rÃ©aliser une typo-
logie Ã  lâ€™aide dâ€™un algorithme de partitionnement. Cet algorithme sera contraint par la connais-
sance issue du classifieur qui calcule les scores dâ€™appÃ©tence. Il sâ€™agit de construire un clustering
qui conserve la proximitÃ© des clients ayant les mÃªmes scores dâ€™appÃ©tence.
La section 2 de cet article dÃ©crit le processus qui a conduit Ã  choisir lâ€™algorithme des k-
moyennes comme algorithme de partitionnement. Muni du choix de lâ€™algorithme la section 3
dÃ©taille comment il est possible dâ€™utiliser une mÃ©trique, au cours du calcul du partitionnement,
qui dÃ©pende du classifieur utilisÃ© pour calculer les scores dâ€™appÃ©tence. La section 4 prÃ©sentera
les rÃ©sultats obtenus avant de conclure au cours de la derniÃ¨re section.
2 Choix dâ€™une technique parmi les diffÃ©rentes mÃ©thodes de
clustering basÃ©es sur le partitionnement
Le clustering est un processus de partitionnement dâ€™un ensemble de donnÃ©es en un en-
semble significatif de groupes appelÃ©s clusters. Le but du regroupement est de trouver des
groupes dâ€™Ã©lÃ©ments similaires au sens dâ€™une mesure de similaritÃ© donnÃ©e. Il y a donc deux
Ã©lÃ©ments principaux Ã  choisir : la mÃ©thode de crÃ©ation des groupes et la mÃ©trique utilisÃ©e lors
de la crÃ©ation des groupes.
Les notations qui seront utilisÃ©es dans la suite de cet article sont les suivantes :
â€“ une base dâ€™apprentissage, D, comportant N instances, M attributs et une variable Ã 
prÃ©dire comportant J modalitÃ©s (les classes Ã  prÃ©dire sont notÃ©es Cj) ;
â€“ chaque instance, D, de donnÃ©es est un vecteur de valeurs (continues ou catÃ©gorielles)
D = (D1, D2, ..., DM ) ;
â€“ k est utilisÃ© pour dÃ©signer le nombre de classes souhaitÃ©es.
- 144 -
V. Lemaire et al.
2.1 Introduction
Il existe 4 grandes techniques de partitionnement qui peuvent Ãªtre utilisÃ©es pour regrouper
les Ã©lÃ©ments dâ€™un ensemble de donnÃ©e autour : dâ€™un centre de gravitÃ© (la moyenne empirique) :
les k-moyennes (MacQueen, 1967) ; dâ€™une mÃ©diane gÃ©omÃ©trique : les k-mÃ©dianes (Bradley
et al., 1997) ; dâ€™un centre contenant les modes les plus frÃ©quents : les k-modes (Huang, 1998) ;
dâ€™un medoid (lâ€™Ã©lÃ©ment dâ€™un ensemble qui minimise la somme des distances entre lui et chacun
des autres Ã©lÃ©ments de cet ensemble) : les k-medoids (Kaufman et Rousseeuw, 1990).
Le choix dâ€™un de ces algorithmes dÃ©pend de la nature des donnÃ©es sur lesquelles il devra
Ãªtre appliquÃ© ; du rÃ©sultat souhaitÃ© (moyenne, medoid, ...) ; du temps disponible et donc de
la complexitÃ© de lâ€™algorithme. De plus, chacun des ces algorithmes dÃ©pend des reprÃ©sentants
initialement choisis, de la valeur de k, de lâ€™indicateur de mesure qui Ã©valuera la qualitÃ© de la
partition (la cohÃ©sion des clusters obtenus), de la distance ou mesure de similaritÃ© utilisÃ©e, de
la reprÃ©sentation des donnÃ©es prÃ©sentÃ©es en entrÃ©e de lâ€™algorithme. Ces diffÃ©rents points sont
discutÃ©s ci-dessous et sont mis rÃ©guliÃ¨rement en relation avec le contexte industriel de lâ€™Ã©tude.
2.2 Influence de la nature des donnÃ©es initiales
On est ici dans un contexte industriel trÃ¨s prÃ©cis. Les donnÃ©es proviennent du systÃ¨me
dâ€™information dâ€™Orange. Les variables explicatives qui sont placÃ©es en entrÃ©e du classifieur
servant Ã  calculer les probabilitÃ©s dâ€™appÃ©tences sont numÃ©riques ou catÃ©gorielles (avec un
grand nombre de modalitÃ©s) et il existe des valeurs manquantes. Le lecteur pourra trouver
une description de ces donnÃ©es dans (Guyon et al., 2009). Si on reste Ã  une telle reprÃ©senta-
tion des donnÃ©es le choix de la technique de partition devrait se tourner vers la technique des
k-prototypes (Huang, 1997) qui est un mixage des k-moyennes et des k-modes.
Les donnÃ©es utilisÃ©es peuvent aussi contenir un certains nombre de clients atypiques (ou
des donnÃ©es erronÃ©es) ce qui dans ce cas amÃ¨nerait plus au choix des k-medoid qui sont par
nature moins sensibles aux valeurs aberrantes.
2.3 Influence du type de rÃ©sultat souhaitÃ©
Le rÃ©sultat du partitionnement doit permettre de construire un argumentaire commercial
par cluster. Un argumentaire de vente est un ensemble structurÃ© dâ€™arguments qui prÃ©sente les
caractÃ©ristiques dâ€™un produit / service comme autant dâ€™avantages pour le client. Il suppose une
connaissance approfondie du produit (caractÃ©ristiques), mais aussi des besoins et des motiva-
tions du client. Il doit Ãªtre adaptÃ© au client. Par consÃ©quent on souhaiterait que le â€œcentreâ€ des
clusters formÃ©s soit un â€œvraiâ€ client et non un client moyen. Il est en effet difficile de savoir ce
quâ€™est, par exemple, la moyenne de deux offres commerciales. Ce desideratum fait pencher le
choix de la technique de partition vers les k-medoid.
2.4 Influence de la mÃ©trique
Un certain nombre dâ€™Ã©lÃ©ments sont Ã  prendre en compte lors du choix de la mÃ©trique. Dâ€™un
cÃ´tÃ© la forme des clusters obtenus dÃ©pend de la norme utilisÃ©e. Dâ€™un autre cotÃ© chacun des
algorithmes dÃ©crits ci-dessus (k-moyennes, ...) vise Ã  minimiser une norme en particulier : les
- 145 -
Personnalisation de score de campagnes
k-moyennes la norme L2, les k-mÃ©dianes la norme L1 (Jajuga, 1987)... Bien que les algo-
rithmes de clustering basÃ©s sur le partitionnement fonctionnent Ã  peu prÃ¨s avec nâ€™importe quel
type de fonction de distance (ou mesure de similaritÃ©) on nâ€™obtient pas les mÃªmes garanties se-
lon la mÃ©trique que lâ€™on utilise. Par exemple le thÃ©orÃ¨me de Huygens qui montre que la somme
de lâ€™inertie intraclusters et lâ€™inertie interclusters est constante nâ€™est valable que si lâ€™on utilise
la distance euclidienne. Dans notre cas on dÃ©sire adapter la mÃ©trique Ã  celle naturellement in-
cluse dans le classifieur servant Ã  calculer les probabilitÃ©s dâ€™appÃ©tences. Cette adaptation sera
dÃ©crite au cours de la section 3 ci-dessous. On mentionne uniquement pour le moment, pour la
comprÃ©hension de la suite de cette section, que lâ€™on utilisera une norme L1 pondÃ©rÃ©eâ€.
2.5 Influence de la complexitÃ© algorithmique
Les complexitÃ©s algorithmiques des diffÃ©rentes techniques de partitionnement varient Ã©nor-
mÃ©ment selon la technique de partitionnement mais aussi selon lâ€™implÃ©mentation qui en est
faite. On trouve dans (Har-peled et Mazumdar, 2003) diffÃ©rentes implÃ©mentations des k-mÃ©-
dianes, dans (Kaufman et Rousseeuw, 1990) diffÃ©rentes implÃ©mentations des k-medoids (PAM
(Partitioning Around Medoids), CLARA (Clustering LARge Applications) et CLARANS
(Clustering Large Applications based upon RANdomized Search))... Si on classe ces algo-
rithmes de la complexitÃ© la plus faible Ã  la plus Ã©levÃ©e on trouve les k-moyennes, les k-modes,
les k-medoids et finalement les k-mÃ©dianes.
Les campagnes marketing concernÃ©es par cette Ã©tude utilisent des bases de donnÃ©es com-
portant des centaines de milliers de clients chacun dÃ©crit potentiellement par plusieurs (di-
zaines de) milliers de variables explicatives. AprÃ¨s la construction du classifieur (qui rÃ©alise
une sÃ©lection de variables) et en ne retenant que les clients ayant les plus fortes probabilitÃ©s
dâ€™appÃ©tences on obtient des bases de donnÃ©es de quelques dizaines de milliers de clients dÃ©crits
par une Ã  plusieurs centaines de variables explicatives. Ce sont ces bases de donnÃ©es qui sont
utilisÃ©es pour construire le partitionnement. On sâ€™aperÃ§oit donc que certains de ces algorithmes
seront difficilement utilisables (au-delÃ  dâ€™une certaine volumÃ©trie).
2.6 Influence du prÃ©traitement
Le classifieur utilisÃ© par Orange dans le cadre de cette Ã©tude, pour calculer les probabi-
litÃ©s dâ€™appÃ©tences est KhiopsTM(au sein de la plateforme PAC (FÃ©raud et al., 2010)). Khiops
incorpore un classifieur naÃ¯f de Bayes (Langley et al., 1992) qui est construit aprÃ¨s une Ã©tape
de prÃ©traitement des variables. Khiops va discrÃ©tiser les variables numÃ©riques et faire du grou-
page de modalitÃ©s pour les variables catÃ©gorielles. A la fin du processus de prÃ©traitement les
variables numÃ©riques et catÃ©gorielles sont donc recodÃ©es : chaque attribut m est recodÃ© en
un attribut qualitatif contenant Im valeurs de recodage. Chaque instance de donnÃ©es est alors
recodÃ©e sous forme dâ€™un vecteur de modalitÃ©s discrÃ¨tes D = D1i1 , D2i2 , ..., DMiM . Dmim
reprÃ©sente la valeur de recodage deDm sur lâ€™attributm, avec la modalitÃ© discrÃ¨te dâ€™indice im.
Ainsi les variables de dÃ©part sont alors toutes reprÃ©sentÃ©es sous une forme numÃ©rique, sur un
vecteur deM âˆ— J composantes : P (Dmim |Cj).
Ce prÃ©traitement rend inutile le choix dâ€™un algorithme comme celui des k-modes puisque
toutes les variables aprÃ¨s lâ€™Ã©tape de prÃ©traitement sont de type numÃ©rique. Il attÃ©nue Ã©galement
lâ€™intÃ©rÃªt des k-mÃ©dianes / k-medoid vis-Ã -vis des â€˜outliersâ€™ car aprÃ¨s ce type de prÃ©traitement
il nâ€™y a plus de valeurs aberrantes dans les donnÃ©es.
- 146 -
V. Lemaire et al.
2.7 Discussion
Les Ã©lÃ©ments prÃ©sentÃ©s ci-dessus montrent que de nombreuses contraintes influent sur le
choix dâ€™un algorithme de partitionnement qui soit adaptÃ© Ã  la problÃ©matique industrielle. A titre
dâ€™exemple, la complexitÃ© algorithmique et la nature des prÃ©traitements effectuÃ©s rend lâ€™algo-
rithme des k-moyennes trÃ¨s adaptÃ© Ã  notre problÃ©matique industrielle mais rend cet algorithme
moins adaptÃ© du fait de lâ€™usage dâ€™une norme L1 et du souhait dâ€™avoir de vrais clients comme
centres de cluster.
Lâ€™algorithme des k-mÃ©dianes est plus adaptÃ© vis-Ã -vis de la norme utilisÃ©e et de la nature
des donnÃ©es aprÃ¨s prÃ©traitement mais sa complexitÃ© algorithmique le rend inutilisable sur nos
donnÃ©es.
Lâ€™algorithme des k-medoid est lâ€™algorithme qui vient naturellement ensuite comme choix
mais sa complexitÃ© algorithmique bien que plus faible que celle des k-mÃ©dianes le rend diffi-
cilement exploitable car elle reste trop Ã©levÃ©e (plusieurs heures de calcul pour de petites bases
de donnÃ©es mÃªme avec des algorithmes de type CLARANS). Dâ€™autres algorithmes (Park et
Jun, 2009) modifient lÃ©gÃ¨rement lâ€™algorithme des k-medoid pour le rendre proche de celui des
k-moyennes en terme de complexitÃ© mais ils nÃ©cessitent de mettre en mÃ©moire la matrice des
distances entre les clients.
DÃ©cision a Ã©tÃ© prise alors dâ€™utiliser lâ€™algorithme des k-mÃ©dianes en prenant une approxima-
tion de la mÃ©diane comme prototype sous lâ€™hypothÃ¨se dâ€™indÃ©pendance des variables et en lui
ajoutant une Ã©tape finale aprÃ¨s convergence. Lâ€™hypothÃ¨se dâ€™indÃ©pendance des variables permet
dâ€™utiliser une version rapide du calcul de la mÃ©diane appelÃ©e â€œthe component-wise medianâ€
(Kashima et al., 2008). Lâ€™Ã©tape rÃ©alisÃ©e aprÃ¨s la convergence de lâ€™algorithme consiste Ã  rem-
placer chaque prototype par le â€œvraiâ€ client (au sein de ce cluster) qui est le plus proche du
prototype. La proximitÃ© entre le vrai client et le prototype du cluster est calculÃ©e Ã  lâ€™aide dâ€™une
distance en norme L1. Cette Ã©tape peut lÃ©gÃ¨rement dÃ©grader les rÃ©sultats du partitionnement
mais elle permet de rÃ©pondre Ã  lâ€™ensemble des souhaits Ã©noncÃ©s dans la section 1.1 ci-dessus.
3 K-moyennes basÃ©es sur la connaissance du classifieur
3.1 Introduction
Onmontre dans cette section quâ€™il est possible dâ€™insÃ©rer dans la mÃ©trique qui servira pour la
construction des k-moyennes une connaissance issue du classifieur : le naÃ¯ve Bayes moyennÃ©
pour le logiciel Khiops. Il sâ€™agit de construire une nouvelle reprÃ©sentation dite â€œsupervisÃ©eâ€
qui permet de construire une mÃ©trique pondÃ©rÃ©e telle que deux instances proches dans cette
reprÃ©sentation supervisÃ©e ont des scores proches.
La section suivante dÃ©crit comment on dÃ©finit la distance dÃ©pendante de la classe pour
un classifieur naÃ¯f de Bayes. La section 3.3 prÃ©sentera quant Ã  elle comment les variables
explicatives se voient attribuer des poids et comment ces poids pondÃ¨rent la distance.
3.2 Distance dÃ©pendante de la classe
Reprenant les notations introduites en 2.6, il est possible dâ€™Ã©crire une distance Bayesienne
dÃ©pendant de la classe Ã  prÃ©dire. Si on part du prÃ©dicteur Bayesien naÃ¯f et que lâ€™on passe au
- 147 -
Personnalisation de score de campagnes
log, on a pour chaque classe cible :
log(p(Cj |D)) =
Mâˆ‘
m=1
log (p(Dmim |Cj)) + log(p(Cj))âˆ’ log(p(D)) (1)
D = (Dm)m=1,...,M une instance
On rappelle que la dÃ©cision Bayesienne correspond Ã  la classe cible Cj maximisant la
formule prÃ©cÃ©dente. On dÃ©finit la distance entre deux instances, d1NB de la faÃ§on suivante :
d1NB(D,D
â€²) =
Mâˆ‘
m=1
Jâˆ‘
j=1
âˆ£âˆ£âˆ£log (p(Dmim |Cj))âˆ’ log ((p(Dâ€²mim |Cj))âˆ£âˆ£âˆ£ (2)
On peut alors coder chaque instance sur un vecteur deM âˆ—J composantes, comme illustrÃ©
dans 3 pour J = 2 :
(log(p(Di11 |C1)), log(p(Di11 |C2)), ..., ..., log(p(DMiM |C1)), log(p(DMiM |C2))) (3)
La distance proposÃ©e correspond Ã  la norme L1 pour ce codage. La distance entre deux
instances est donc dÃ©finie en fonction des recodages. Deux instances proches au sens de leur
recodage supervisÃ© seront proches au sens de leur comportement pour la classe Ã  prÃ©dire. En
effet si on dÃ©finit la distance entre les distributions de classes prÃ©dites de la faÃ§on suivante :
âˆ†1(D,Dâ€²) =
Jâˆ‘
j=1
|log(p(Cj |D))âˆ’ log(p(Cj |Dâ€²))| (4)
On a la majoration suivante :
âˆ†1(D,Dâ€²) â‰¤ [d1NB(D,Dâ€²) + J |log(p(D))âˆ’ log(p(Dâ€²))|] (5)
Donc deux instances de mÃªme probabilitÃ© globale proches au sens de d1NB seront proches
au sens de la prÃ©diction des probabilitÃ©s par classe cible (deux instances qui ont des recodages
proches dans lâ€™espace supervisÃ© auront des probabilitÃ©s proches dâ€™avoir Ã©tÃ© gÃ©nÃ©rÃ©es par le
modÃ¨le de recodage). Cette majoration est vraie aussi dans le cadre de la rÃ©gression linÃ©aire.
3.3 PondÃ©ration de la distance
Lâ€™Ã©tape de construction des poids des variables utilisÃ©es par le classifieur naÃ¯f de Bayes est
totalement dÃ©crit dans (BoullÃ©, 2007). Elle comprend deux Ã©tapes clefs : une Ã©tape de sÃ©lection
de variable dÃ©crite dans la section 3.5 de lâ€™article et une Ã©tape de moyennage de modÃ¨le dÃ©crite
dans la section 6.2 de cet article. Lâ€™Ã©tape de sÃ©lection de variable permet au classifieur dâ€™Ã©viter
dâ€™avoir des variables explicatives inutiles ou non liÃ©es au problÃ¨me de classification. Lâ€™Ã©tape de
moyennage de modÃ¨le permet de pondÃ©rer les variable de telle sorte que lâ€™Ã©quation 1 devient :
log(p(Cj |D) =
Mâˆ‘
m=1
Wmlog ((p(Dmim |Cj)) + log(p(Cj))âˆ’ log(p(D)) (6)
- 148 -
V. Lemaire et al.
oÃ¹Wm est le poids de la variablem quelle que soit la classe cible
Chaque instance est alors recodÃ©e sur un vecteur de M âˆ— J composantes mais oÃ¹ chaque
composante est pondÃ©rÃ©e par un poids. La distance (Ã©quation 2) est donc pondÃ©rÃ©e au sens des
poids des variables, la majoration prÃ©sentÃ©e Ã©quation 5 restant vraie.
3.4 Discussion - Algorithme modifiÃ© des K-moyennes
Dans ce qui suit on appellera â€œreprÃ©sentation supervisÃ©eâ€ la reprÃ©sentation issue du passage
de la base de donnÃ©es dâ€™apprentissage initiale vers une reprÃ©sentation oÃ¹ chaque instance est
reprÃ©sentÃ©e sur un vecteur deM âˆ— J composantes (comme illustrÃ© dans lâ€™Ã©quation 3) pour le
classifieur naÃ¯f de Bayes. Chaque variable est pondÃ©rÃ©e Ã  lâ€™aide dâ€™un poidsWm.
Le rÃ©sultat ci-dessus (Ã©quation 5) donne la garantie que si on utilise un algorithme de type
k-moyennes sur la reprÃ©sentation supervisÃ©e et Ã  lâ€™aide de la norme L1 on obtiendra des clusters
oÃ¹ deux individus proches au sens de la distance seront proches au sens de leur probabilitÃ©
dâ€™appartenance Ã  la classe cible.
Lâ€™algorithme des k-moyennes est dans la suite cet article est appelÃ© â€œmodifiÃ©â€ car il utilise
(i) une reprÃ©sentation supervisÃ©e des donnÃ©es, (ii) la norme L1, (iii) une approximation de la
mÃ©diane, (iv) une Ã©tape de post traitement de dÃ©signation de vrais clients comme centres. Ces 4
modifications devraient permettre dâ€™atteindre les objectifs initiaux de lâ€™Ã©tude tels que prÃ©sentÃ©s
dans lâ€™introduction de cet article.
4 RÃ©sultats expÃ©rimentaux
4.1 PrÃ©ambule
Initialisation : Lâ€™ensemble des mÃ©thodes dâ€™initialisation mentionnÃ©es dans (Meila et He-
ckerman, 1998) ont Ã©tÃ© testÃ©es. Nous nâ€™avons pas dans notre cas (prÃ©traitements supervisÃ©s
et/ou norme L1) mesurÃ© de diffÃ©rences significatives entre les rÃ©sultats obtenus. Les rÃ©sultats
prÃ©sentÃ©s dans cet article sont ceux obtenus Ã  lâ€™aide dâ€™une initialisation alÃ©atoire des proto-
types.
Validation croisÃ©e : Dans chacune des phases expÃ©rimentales (et pour toutes valeurs de
k) les bases de donnÃ©es ont Ã©tÃ© dÃ©coupÃ©es en 10 sacs afin de rÃ©aliser une validation croisÃ©e.
Dans ce qui suit ce sont les rÃ©sultats moyens en test et pour lâ€™AUC (Area Under ROC Curve)
qui sont indiquÃ©s. Le score dâ€™appartenance Ã  la classe cible dâ€™un exemple est dÃ©fini comme la
proportion dâ€™Ã©lÃ©ments de la classe cible du cluster de cet exemple. Dans le cas ou le nombre
de classes cibles est supÃ©rieur Ã  2 on donne lâ€™espÃ©rance de lâ€™AUC.
4.2 PremiÃ¨re phase expÃ©rimentale
Une premiÃ¨re phase expÃ©rimentale a Ã©tÃ© menÃ©e de maniÃ¨re Ã  (i) mesurer lâ€™impact de la
reprÃ©sentation supervisÃ©e sur les k-moyennes et Ã  (ii) mesurer lâ€™Ã©cart entre les rÃ©sultats obte-
nus entre lâ€™algorithme des k-medoid PAM (qui travaille directement sur les â€œvrais clientsâ€) et
lâ€™Ã©tape de post-dÃ©signation incluse dans lâ€™algorithme modifiÃ© des k-moyennes.
Le logiciel Khiops a Ã©tÃ© testÃ© Ã  lâ€™aide (i) des donnÃ©es natives et (ii) des donnÃ©es mises dans
leur reprÃ©sentation supervisÃ©e. Lâ€™ensemble des valeurs de k testÃ©es vont de 1 Ã 
âˆš
N par pas de
- 149 -
Personnalisation de score de campagnes
1, de 1 Ã  10, puis en doublant ensuite la valeur de k : k âˆˆ A = {1, 2, ..., 9, 10, 20, 40, ...,âˆšN}.
Pour pouvoir comparer les rÃ©sultats avec ceux obtenus par PAM la volumÃ©trie a Ã©tÃ© limitÃ©e Ã  de
â€œpetitesâ€ bases de donnÃ©es provenant de lâ€™UCI (Blake et Merz, 1998). La somme des erreurs
aux carrÃ©s (SSE) nâ€™a pas Ã©tÃ© utilisÃ©e pour Ã©valuer les rÃ©sultats obtenus car elle est ici inappro-
priÃ©e du fait quâ€™on travaille ici sur deux reprÃ©sentations diffÃ©rentes (native et supervisÃ©e). Le
critÃ¨re de lâ€™AUC a alors Ã©tÃ© choisi car il donne une indication de puretÃ© des clusters au sens
dâ€™une classe cible.
Le tableau 1 compare le gain entre les rÃ©sultats obtenus avec la reprÃ©sentation supervisÃ©e
par rapport Ã  la reprÃ©sentation native pour PAM et lâ€™algorithme modifiÃ© des k-moyennes sur
les bases Iris et PhonÃ¨me. Pour les bases Letter et Shuttle PAM nâ€™ayant pas abouti dans un
temps acceptable (pour les diffÃ©rentes valeurs de k testÃ©es et la 10-fold cross-validation) seuls
les rÃ©sultats pour les k-moyennes sont prÃ©sentÃ©s. Ce tableau prÃ©sente les rÃ©sultats en test et
en AUC. Il sâ€™agit lÃ  de rÃ©sultats moyens calculÃ©s Ã  lâ€™aide des valeurs individuelles obtenues
en fonction de k et du 10-fold (f ) cross validation (AUC = 1|A|10
âˆ‘
kâˆˆA
âˆ‘10
f=1AUC(k, f)).
On ne prÃ©sente ici que quelques rÃ©sultats reprÃ©sentatifs (croissants en taille (J,N,M )) des
tests effectuÃ©s mais le lecteur intÃ©ressÃ© pourra trouver plus de dÃ©tails dans (Creff, 2011). Nous
observons dans ce tableau que lâ€™utilisation dâ€™une reprÃ©sentation supervisÃ©e ne dÃ©grade pas les
rÃ©sultats voire permet une amÃ©lioration du critÃ¨re de lâ€™AUC.
Base Pam Pam â€œsupervisÃ©â€ Kmeans Kmeans â€œsupervisÃ©sâ€ J N M
Iris 0.959 0.951 0.946 0.966 2 150 4
PhonÃ¨me 0.926 0.935 0.910 0.919 5 2554 256
Shuttle - - 0.902 0.929 7 58000 9
Letter - - 0.711 0.787 26 20000 16
TAB. 1 â€“ Phase 1 - AUC : RÃ©sultats moyens en test entre une reprÃ©sentation native et une
reprÃ©sentation supervisÃ©e
Les figures 1 et 2 illustrent les rÃ©sultats obtenus sur les bases Abalone (N = 4177, J = 28)
et Titactoe (N = 958, J = 2) en utilisant (uniquement) la reprÃ©sentation supervisÃ©e. Dans ces
figures la courbe Rouge correspond Ã  PAM, la courbe Bleu correspond Ã  lâ€™algorithme modifiÃ©
des k-moyennes Ã  lâ€™aide de la mÃ©trique issue dâ€™un naÃ¯ve Bayes calculÃ© par Khiops, enfin la
courbe Noire correspond au classifieur naÃ¯f de Bayes calculÃ© par Khiops.
Ces rÃ©sultats illustratifs, ainsi que ceux prÃ©sentÃ©s dans (Creff, 2011), montrent que lâ€™algo-
rithme modifiÃ© des k-moyennes utilisant la reprÃ©sentation issue du naÃ¯ve Bayes Khiops est trÃ¨s
compÃ©titif. On observe aussi que, pour des valeurs Ã©levÃ©es de k, lâ€™algorithme modifiÃ© des k-
moyennes, utilisÃ© en tant que classifieur, peut atteindre des performances supÃ©rieures au naÃ¯ve
Bayes.
4.3 DeuxiÃ¨me phase
Plusieurs bases de donnÃ©es ont Ã©tÃ© mises Ã  notre disposition pour cette phase. Trois bases
de 200 000 clients datant de mars, mai, et aoÃ»t 2009 sur un problÃ¨me de churn Ã  lâ€™un des
produits dâ€™Orange ont Ã©tÃ© utilisÃ©es. Ces bases sont constituÃ©es dâ€™environ 1000 variables. La
base de donnÃ©es du mois de mars a Ã©tÃ© utilisÃ©e pour construire le classifieur. Ensuite les tops
scores de mars ont Ã©tÃ© utilisÃ©s pour rÃ©aliser la partition en k groupes Ã  lâ€™aide de lâ€™algorithme
- 150 -
V. Lemaire et al.
FIG. 1 â€“ AUC en Test pour Abalone FIG. 2 â€“ AUC en Test pour Titactoe
modifiÃ© des k-moyennes. Les bases de Mai et AoÃ»t correspondront Ã  des ensembles de tests.
Les critÃ¨res dâ€™Ã©valuation ont Ã©tÃ© calculÃ©s pour chacun des mois (mars, mai et aoÃ»t).
Dans notre contexte industriel, les utilisateurs de lâ€™algorithme de clustering basÃ© sur le
partitionnement souhaitent pouvoir choisir eux-mÃªmes la valeur de k. Ils appliqueront ensuite
Ã  certains groupes une campagne tÃ©lÃ©phonique, Ã  dâ€™autres une campagne de mailing, Ã  dâ€™autres
une campagne de courrier. Dans ce contexte, il est donc prÃ©fÃ©rable de laisser le choix de k
Ã  lâ€™utilisateur qui se basera sur son expertise, sur une connaissance Ã  priori quâ€™il a sur les
donnÃ©es, sur le fait quâ€™un nombre dâ€™argumentaires commerciaux maximal peut Ãªtre Ã©tabli.
AprÃ¨s consultation de lâ€™entitÃ© concernÃ©e, 3 valeurs de k ont Ã©tÃ© testÃ©es : 4, 10, 20. Pour des
considÃ©rations de place seuls les rÃ©sultats avec k=4 sont prÃ©sentÃ©s ci-dessous sachant que les
conclusions Ã©noncÃ©es restent valides pour k = 10 ou k = 20 (consultables dans (Creff, 2011)).
Au moment oÃ¹ les tests ont Ã©tÃ© effectuÃ©s il existait une solution logicielle au sein de lâ€™en-
treprise pour rÃ©aliser ce type de campagne. Mais cette solution nâ€™Ã©tait quasiment pas utilisÃ©e
car les groupes obtenus deviennent trop diffÃ©rents de mois en mois. Lâ€™algorithme modifiÃ© des
k-moyennes proposÃ© dans cet article a Ã©tÃ© donc Ã©tÃ© Ã©valuÃ© Ã  lâ€™aide dâ€™un critÃ¨re de stabilitÃ© dans
le temps des clusters trouvÃ©s. Ce critÃ¨re comprend deux axes.
Le premier axe est lâ€™Ã©volution du pourcentage dâ€™appartenance Ã  un cluster. Au mois T ,
on observe le pourcentage dâ€™Ã©lÃ©ments de lâ€™ensemble de donnÃ©es appartenant Ã  un cluster. On
recommence la mÃªme opÃ©ration aux mois suivants avec dâ€™autres ensembles de donnÃ©es. Dâ€™un
mois Ã  lâ€™autre les proportions dâ€™Ã©lÃ©ments appartenant Ã  un cluster devraient rester les mÃªmes
pour que lâ€™on considÃ¨re la solution comme stable par rapport Ã  ce critÃ¨re.
Le deuxiÃ¨me axe est lâ€™Ã©volution de la rÃ©partition des classes ou des valeurs cibles au sein
des clusters. En fait chaque client est associÃ© Ã  une classe, ces classes ne sont pas utilisÃ©es
pour rÃ©aliser le clustering. Au mois T , on observe dans les clusters la rÃ©partition des clients
appartenant Ã  une classe. On recommence lâ€™opÃ©ration les mois suivants et si la rÃ©partition des
clients reste la mÃªme dâ€™un mois Ã  lâ€™autre alors on pourra considÃ©rer la mÃ©thode de clustering
comme stable au cours du temps.
Les rÃ©sultats obtenus sur ces 2 axes de stabilitÃ© sont prÃ©sentÃ©s figures 3 Ã  6. Lâ€™axe des abs-
cisses reprÃ©sente les mois (T=1=mars, T=2=mai, T=3=aoÃ»t) et lâ€™axe des ordonnÃ©es un pour-
centage. Dans les figures 3 et 5 les pourcentages somment Ã  100% et correspondent aux tops
scores. Par contre dans les figures 4 et 6 le pourcentage ne somme pas Ã  1 puisquâ€™il reprÃ©sente
- 151 -
Personnalisation de score de campagnes
FIG. 3 â€“ Pourcentage dâ€™Ã©lÃ©ments par clus-
ter avec la solution actuelle.
FIG. 4 â€“ Proportion dâ€™Ã©lÃ©ments
(churn=1) avec la solution actuelle.
FIG. 5 â€“ Pourcentage dâ€™Ã©lÃ©ments par clus-
ter avec le k-moyennes â€œsupervisÃ©â€.
FIG. 6 â€“ Proportion dâ€™Ã©lÃ©ments
(churn=1) avec le k-moyennes â€œsu-
pervisÃ©â€.
la proportion dâ€™Ã©lÃ©ments de chaque cluster ayant lâ€™Ã©tiquette churn=1. On observe Ã  lâ€™aide de
ces 4 figures que le but est atteint : les clusters trouvÃ©s Ã  lâ€™aide de la reprÃ©sentation supervisÃ©e,
qui dÃ©pendent du classifieur construit au mois T , sont beaucoup plus stables dans le temps
(Figures 5 et 6 par comparaison aux Figures 3 et 4). On sait par ailleurs que les clients prÃ©sents
dans un cluster ont des scores de churn proches.
4.4 Discussion - Un clustering contraint par la proximitÃ© entre scores
Lâ€™utilisation dâ€™une reprÃ©sentation supervisÃ©e issue de discrÃ©tisation ou de groupage super-
visÃ© permet la majoration Ã©tablie Ã©quation 5. Cette majoration donne la garantie que si on
utilise lâ€™algorithme des k-moyennes, Ã  lâ€™aide de la norme L1 on obtiendra des clusters oÃ¹
deux individus proches dans la reprÃ©sentation supervisÃ©e seront proches au sens de leur pro-
babilitÃ© dâ€™appartenance Ã  la classe cible. Cependant cette majoration indique seulement que
âˆ†1(D,Dâ€²) â‰¤ d1NB(D,Dâ€²). Si donc deux instances D et Dâ€² sont trÃ¨s Ã©loignÃ©es dans lâ€™espace
supervisÃ© on a uniquement la garantie que la distance entre leurs scores sera plus petite. La
distance entre les scores de deux instances Ã©loignÃ©es dans la reprÃ©sentation supervisÃ©e peut
donc Ãªtre grande.
Il serait donc intÃ©ressant, dans la reprÃ©sentation supervisÃ©e, de contraindre lâ€™algorithme des
k-moyennes Ã  ne regrouper que des instances qui soient au plus Ã©loignÃ©es dâ€™une valeur seuil
- 152 -
V. Lemaire et al.
(notÃ©e ). Un algorithme de type Xmeans (Pelleg et Moore, 2000) pourrait Ãªtre utilisÃ© sous la
contrainte de recouper tout cluster oÃ¹ la distance maximale entre deux instances est supÃ©rieure
Ã  . Cette contrainte garantirait le fait de nâ€™avoir aucun cluster avec un diamÃ¨tre supÃ©rieur Ã  
et donc dâ€™instance qui a un score supÃ©rieur Ã   au score dâ€™une autre instance. Cette garantie
permettrait dâ€™amÃ©liorer lâ€™algorithme modifiÃ© des k-moyennes proposÃ© dans cet article.
On pourra aussi noter que la reprÃ©sentation supervisÃ©e construite avant lâ€™Ã©tape de clustering
pourrait Ãªtre utilisÃ©e avec dâ€™autres mÃ©thodes de clustering. Les cartes de Kohonen qui ont la
propriÃ©tÃ© de conservation de la proximitÃ© des donnÃ©es dans lâ€™espace initial pourraient Ãªtre
utilisÃ©es.
5 Conclusion
Cet article a montrÃ© comment il est possible de rÃ©aliser une typologie Ã  lâ€™aide dâ€™une tech-
nique de type partitionnement mais qui soit contrainte par la connaissance issue dâ€™un classi-
fieur. On a montrÃ© quâ€™il est possible de construire une reprÃ©sentation supervisÃ©e Ã  lâ€™aide dâ€™un
classifieur de type naÃ¯ve Bayes, dâ€™une rÃ©gression linÃ©aire ou dâ€™une rÃ©gression logistique. Cette
reprÃ©sentation supervisÃ©e permet de crÃ©er un partitionnement qui conserve la proximitÃ© des
exemples ayant les mÃªmes probabilitÃ©s dâ€™appartenance aux classes cibles. Cette technique a
Ã©tÃ© utilisÃ©e avec succÃ¨s dans un cadre applicatif de scoring de clients. Les rÃ©sultats expÃ©ri-
mentaux montrent son bon comportement en termes de mesure dâ€™AUC mais aussi vis-Ã -vis du
critÃ¨re imposÃ© de stabilitÃ© dans le temps.
RÃ©fÃ©rences
Blake, C. L. et C. J. Merz (1998). UCI Repository of machine learning databases. http:
//archive.ics.uci.edu/ml/ visitÃ© pour la derniÃ¨re fois : 15/09/2010.
BoullÃ©, M. (2007). Compression-based averaging of selective naive Bayes classifiers. Journal
of Machine Learning Research 8, 1659â€“1685.
Bradley, P. S., O. L. Mangasarian, et W. N. Street (1997). Clustering via concave minimization.
In Advances in Neural Information Processing Systems -9, pp. 368â€“374. MIT Press.
Creff, N. (2011). Clustering Ã  lâ€™aide dâ€™une reprÃ©sentation supervisÃ©e. Masterâ€™s thesis, Epita,
14-16 rue Voltaire 94276 Kremlin BicÃªtre Cedex.
FÃ©raud, R., M. BoullÃ©, F. ClÃ©rot, F. Fessant, et V. Lemaire (2010). The orange customer ana-
lysis platform. In Proceedings of the 10th Industrial Conference on Data Mining, Berlin,
Germany, pp. 584â€“594. Springer Verlag.
Guyon, I., V. Lemaire, M. BoullÃ©, G. Dror, et D. Vogel (2009). Analysis of the KDD cup
2009 : Fast scoring on a large orange customer database. JMLR : Workshop and Conference
Proceedings 7, 1â€“22. Data available on http://www.kddcup-orange.com.
Har-peled, S. et S. Mazumdar (2003). Coresets for k-means and k-median clustering and their
applications. In In Proc. 36th Annu. ACM Sympos. Theory Comput, Chicago, Illinois, USA,
pp. 291â€“300.
- 153 -
Personnalisation de score de campagnes
Huang, Z. (1997). Clustering large data sets with mixed numeric and categorical values. In
Pacific Asia Knowledge Discovery and Data Mining Conference, pp. 21â€“34. Singapore :
World Scientific.
Huang, Z. (1998). Extensions to the k-means algorithm for clustering large data sets with
categorical values. Data Min. Knowl. Discov. 2, 283â€“304.
Jajuga, K. (1987). A clustering method based on the l1-norm. Computational Statistics &
Data Analysis 5(4), 357â€“371.
Kashima, H., J. Hu, B. Ray, et M. Singh (2008). K-means clustering of proportional data using
l1 distance. In Pattern Recognition, 2008. ICPR 2008. 19th International Conference on,
pp. 1 â€“4.
Kaufman, L. et P. J. Rousseeuw (1990). Finding Groups in Data : An Introduction to Cluster
Analysis. John Wiley.
Langley, P., W. Iba, et K. Thompson (1992). An analysis of Bayesian classifiers. In Procee-
dings of the tenth National Conference on Artificial Intelligence, San Jose, California, USA,
pp. 223â€“228. MIT Press.
MacQueen, J. (1967). Some methods for classification and analysis of multivariate observa-
tions. In 5th Berkeley Symposium on Mathematical Statistics and Probability, Volume 1, pp.
281â€“297.
Meila, M. et D. Heckerman (1998). An experimental comparison of several clustering and
initialization methods. In Machine Learning, pp. 386â€“395.
Park, H.-S. et C.-H. Jun (2009). A simple and fast algorithm for k-medoids clustering. Expert
Syst. Appl. 36(2), 3336â€“3341.
Pelleg, D. et A. W. Moore (2000). X-means : Extending k-means with efficient estimation
of the number of clusters. In Proceedings of the Seventeenth International Conference on
Machine Learning, ICML â€™00, San Francisco, California, USA, pp. 727â€“734. Morgan Kauf-
mann Publishers Inc.
Summary
When the marketing service has to contact customers to propose them a product, the prob-
ability that these customers will buy this product is calculated beforehand. This probability
is calculated using a predictive model. The marketing service contacts then those having the
highest probability of buying the product, the strongest appetency. In parallel and before the
commercial contact it may be interesting to realize a typology of the customers who will be
contacted. The idea is to propose differentiated campaigns by group of customers. This article
shows how it is possible to force the typology, realized using a k-means type algorithm, to
respect the nearness of the customers as refers to their appetency score.
- 154 -
