Analyse en Facteurs : preÂ´sentation et
comparaison des logiciels
SAS, SPAD et SPSS
Marie Chavent1, Vanessa Kuentz1, JeÂ´roË†me Saracco1,2
1 UniversiteÂ´s Bordeaux 1 et 2,
IMB, UMR CNRS 5251,
351 Cours de la LibeÂ´ration, 33405 Talence Cedex, France
vanessa.kuentz,marie.chavent@math.u-bordeaux1.fr
2 UniversiteÂ´ Montesquieu - Bordeaux 4,
GREThA, UMR CNRS 5113,
Avenue LeÂ´on Duguit, 33608 Pessac Cedex, France
jerome.saracco@u-bordeaux4.fr
Abstract In data analysis, factorial methods are essential. These techniques can
be used as an end in themselves, seeking to highlight underlying common factors in
a group of variables. They can also be used as input to another analysis. Then, they
consist in data dimension reduction and operate by replacing the original variables,
sometimes highly correlated, by a smaller number of linearly independent variables.
Factor Analysis (F.A.) is one possible method for quantitative data. This article
aims at presenting in a synthetic way the F.A. model, rarely described in French
books, but frequent in the Anglo-Saxon literature, and often available in softwares.
The presentation of the estimation techniques for the F.A. model enables to estab-
lish the existing connection between Principal Component Analysis (P.C.A.) and
F.A. The usefulness of rotation techniques, which can facilitate the interpretation
of the results, will also be shown. An application on crime data of American cities
will be carried out and will allow to describe the results provided by three of the
most used statistical softwares : SAS, SPAD and SPSS. Then it will help to clarify
the vocabulary, sometimes confused for the user.
Keywords : Factor Analysis, Principal Component Analysis, Singular Value De-
composition, Rotation.
ReÂ´sumeÂ´ En analyse des donneÂ´es, les meÂ´thodes factorielles sont fondamentales. Ces
techniques peuvent eË†tre utiliseÂ´es comme but en soi, il sâ€™agit alors de faire ressortir des
facteurs sous-jacents communs a` un groupe de variables. Elles peuvent eÂ´galement
constituer une eÂ´tape preÂ´alable a` dâ€™autres eÂ´tudes. Elles consistent alors a` reÂ´duire
la dimension des donneÂ´es en remplacÂ¸ant les variables dâ€™origine, qui peuvent eË†tre
correÂ´leÂ´es, par un plus petit nombre de variables lineÂ´airement indeÂ´pendantes. Lorsque
les donneÂ´es sont quantitatives, lâ€™Analyse en Facteurs (A.F.) est une des meÂ´thodes
possibles. Lâ€™objectif de cet article est de dresser une preÂ´sentation syntheÂ´tique du
mode`le dâ€™A.F., peu deÂ´veloppeÂ´ dans les manuels francophones, mais freÂ´quent dans
la litteÂ´rature anglo-saxonne, et souvent preÂ´sent dans les logiciels statistiques. La
cÂ© Revue MODULAD, 2007 -1- NumeÂ´ro 37
preÂ´sentation des techniques dâ€™estimation du mode`le dâ€™A.F. permet dâ€™eÂ´tablir le lien
existant entre lâ€™Analyse en Composantes Principales (A.C.P.) et lâ€™A.F. Il sâ€™agit
eÂ´galement de montrer lâ€™utiliteÂ´ des techniques de rotation, qui peuvent faciliter lâ€™in-
terpreÂ´tation des reÂ´sultats. Un exemple dâ€™application sur des donneÂ´es de criminaliteÂ´
de villes ameÂ´ricaines permet enfin de deÂ´crire les reÂ´sultats fournis par trois des lo-
giciels statistiques les plus utiliseÂ´s : SAS, SPAD et SPSS, et ainsi de clarifier le
vocabulaire, parfois confus pour lâ€™utilisateur.
Mots-cleÂ´s : Analyse en Facteurs, Analyse en Composantes Principales, DeÂ´composition
en Valeurs Singulie`res, Rotation.
1 Introduction
Lâ€™A.F. trouve son origine en psychomeÂ´trie lorsquâ€™en 1904, Spearman deÂ´veloppe une
theÂ´orie psychologique selon laquelle lâ€™esprit humain sâ€™explique par un facteur commun a`
tous les individus et par plusieurs facteurs speÂ´cifiques a` chacun. Ce mode`le est geÂ´neÂ´raliseÂ´
pour plusieurs facteurs communs par Garnett en 1919. De nombreuses applications sont
alors reÂ´aliseÂ´es pour deÂ´terminer un nombre relativement faible de tests qui permettraient
de deÂ´crire lâ€™esprit humain de facÂ¸on aussi comple`te que possible.
Ainsi, lâ€™A.F. vise a` eÂ´crire chaque variable aleÂ´atoire du proble`me en fonction de facteurs
sous-jacents communs a` toutes les variables, et dâ€™un facteur speÂ´cifique ou unique a` la
variable aleÂ´atoire consideÂ´reÂ´e. Il repose sur diffeÂ´rentes hypothe`ses dont principalement la
non correÂ´lation des facteurs communs. DiffeÂ´rentes meÂ´thodes dâ€™estimation existent, les
plus courantes sont lâ€™estimation via les composantes principales, la meÂ´thode du facteur
principal et le maximum de vraisemblance. Lâ€™estimation du mode`le dâ€™A.F. via lâ€™A.C.P.
ne garantit pas que les hypothe`ses du mode`le soient veÂ´rifieÂ´es. Cependant cette technique
est la plus utiliseÂ´e car elle fournit souvent une approximation convenable.
Cet article met eÂ´galement en lumie`re un point essentiel de lâ€™A.F. : le choix du nombre q
de facteurs communs. DiffeÂ´rents crite`res empiriques et theÂ´oriques existent pour le choisir.
Nous insisterons sur le fait que ces re`gles sont une aide partielle qui ne doit pas se sub-
stituer a` une interpreÂ´tation rigoureuse des reÂ´sultats. Notons que lâ€™enjeu de ce choix est
majeur car la qualiteÂ´ des reÂ´sultats en deÂ´pend.
Suite a` lâ€™estimation du mode`le dâ€™A.F., la lecture des reÂ´sultats peut sâ€™aveÂ´rer deÂ´licate. Les
facteurs obtenus peuvent eË†tre difficiles a` interpreÂ´ter, sembler ne pas avoir dâ€™inteÂ´reË†t pour
lâ€™eÂ´tude, ou ne pas exliquer le pheÂ´nome`ne consideÂ´reÂ´, etc. Des reÂ´sultats sont pourtant parfois
preÂ´sents, mais leur lecture nâ€™est pas directe et intuitive. Lâ€™utilisateur peut alors passer a`
coË†teÂ´ de reÂ´sultats importants. Une rotation orthogonale des facteurs peut aider dans cette
phase. La justification de la possibiliteÂ´ dâ€™effectuer une rotation provient de la non-uniciteÂ´
de la solution du mode`le dâ€™A.F. Nous verrons de plus que la rotation est possible en
A.C.P. a` condition dâ€™effectuer convenablement la transformation. Bien que les techniques
de rotation peuvent faciliter de facÂ¸on significative la lecture des reÂ´sultats, elles sont peu
preÂ´senteÂ´es dans les ouvrages francophones, contrairement a` leurs voisins anglo-saxons.
Lâ€™utiliteÂ´ de la rotation des facteurs sera mise en exergue sur une application concernant la
criminaliteÂ´ de seize villes ameÂ´ricaines (donneÂ´es issues de U.S. Statistical Abstract, 1970).
cÂ© Revue MODULAD, 2007 -2- NumeÂ´ro 37
Enfin, lâ€™estimation du mode`le peut se faire a` lâ€™aide de logiciels statistiques, comme SAS,
SPAD et SPSS. Le vocabulaire employeÂ´ diffe`re dâ€™un logiciel a` lâ€™autre et peut rapidement
devenir source de confusion. Lâ€™exemple dâ€™application preÂ´cise ce vocabulaire et pourra ainsi
aider les utilisateurs dans la lecture des sorties numeÂ´riques des logiciels.
Le preÂ´sent article sâ€™articule autour de cinq parties. Le mode`le dâ€™A.F. est preÂ´senteÂ´ a` la
section 2. Lâ€™estimation des parame`tres du mode`le est ensuite deÂ´crite a` la section 3. Les
techniques de rotation des facteurs, facilitant la deÂ´tection de groupes de variables correÂ´leÂ´es,
sont preÂ´senteÂ´es a` la section 4. Enfin, a` la section 5, une application de ce mode`le dâ€™A.F.
est reÂ´aliseÂ´e sur des donneÂ´es de criminaliteÂ´ dans diffeÂ´rentes villes des Etats-Unis et permet
de comparer les reÂ´sultats fournis par les trois logiciels statistiques SAS, SPAD et SPSS.
2 Le mode`le dâ€™A.F.
Soit x = (x1, x2, . . . , xp)â€² un vecteur aleÂ´atoire de Rp dâ€™espeÂ´rance Âµ âˆˆ Rp. On note
xËœ = xâˆ’ Âµ la version centreÂ´e de x.
Le mode`le dâ€™A.F. sâ€™eÂ´crit :
xËœ = Aqf + e
(pÃ— 1) (pÃ— q)(q Ã— 1) (pÃ— 1) (1)
ou` :
â€“ Aq est une matrice (pÃ—q) de coefficients aÎ±j , j = 1, . . . , p, Î± = 1, . . . , q (â€loadingsâ€ en
anglais). Elle est appeleÂ´e matrice de saturation (â€factor loadings matrixâ€ ou â€factor
pattern matrixâ€).
â€“ f = (f 1, . . . , f q)â€² est un vecteur aleÂ´atoire de Rq, composeÂ´ des q facteurs communs
(â€common factorsâ€) aux p variables aleÂ´atoires xËœ1, xËœ2, . . . , xËœp.
â€“ e = (e1, . . . , ep)â€² est un vecteur aleÂ´atoire centreÂ´ de Rp, composeÂ´ des p facteurs
speÂ´cifiques (ou uniques) (â€unique factorsâ€) a` chaque variable xËœj, j = 1, . . . , p.
Il deÂ´coule de (1) et de E(e) = 0 la proprieÂ´teÂ´ suivante :
E(f) = 0. (2)
Pour tout j = 1, . . . , p, on a :
xËœj =
qâˆ‘
Î±=1
aÎ±j f
Î± + ej. (3)
Chaque variable xËœj sâ€™eÂ´crit comme la somme dâ€™une combinaison lineÂ´aire de facteurs f 1, . . . , f q
communs a` toutes les variables xËœ1, . . . , xËœp et dâ€™un facteur ej speÂ´cifique a` la variable
consideÂ´reÂ´e xËœj.
On insiste sur le fait que les facteurs communs f 1, . . . , f q sont aleÂ´atoires. Ainsi, le mode`le
dâ€™A.F. est souvent deÂ´signeÂ´ comme un mode`le a` effets aleÂ´atoires ou mode`le structurel
(Baccini et Besse, 2005).
Le mode`le (1) repose sur plusieurs hypothe`ses.
(H1) : E(ff â€²) = Iq, ou` Iq est la matrice identiteÂ´ (q Ã— q).
cÂ© Revue MODULAD, 2007 -3- NumeÂ´ro 37
(H2) : E(eeâ€²) = Î, ou` Î = diag(Î¾j, j = 1, . . . , p).
(H3) : E(ef â€²) = 0.
Lâ€™hypothe`se (H1) signifie que les facteurs communs f
Î±, Î± = 1, . . . , q, sont non correÂ´leÂ´s
et de variance 1. Cette hypothe`se de non correÂ´lation des facteurs sâ€™explique par le fait
que lâ€™on souhaite exprimer les variables aleÂ´atoires xËœj en fonction du plus petit nombre de
facteurs possible, et donc eÂ´viter des redondances.
Lâ€™hypothe`se (H2) signifie que les facteurs uniques e
j, j = 1, . . . , p, ne sont pas correÂ´leÂ´s. Ils
expriment pour chaque variable la part non expliqueÂ´e par les facteurs communs. Ils ont
chacun une variance speÂ´cifique Î¾j.
Lâ€™hypothe`se (H3) traduit le fait que chaque variable e
j, j = 1, . . . , p, traduit la part
speÂ´cifique a` la variable xËœj qui nâ€™a pu eË†tre exprimeÂ´e par les facteurs communs fÎ±, Î± =
1, . . . , q, donc les variables ej et fÎ±, ne sont pas correÂ´leÂ´es.
On note Î£ la matrice de variance covariance de x. On deÂ´duit du mode`le (1) que :
E(xËœxËœâ€²) = AqE(ff â€²)Aâ€²q + E(eeâ€²) et donc
Î£ = AqA
â€²
q + Î. (4)
Lâ€™eÂ´quation (4) est appeleÂ´e mode`le de structure de covariance.
Dâ€™apre`s (1) ou (3), on peut eÂ´crire pour tout j = 1, . . . , p :
V(xj) = (a1j)2 + (a2j)2 + . . .+ (a
q
j)
2 + Î¾j
=
qâˆ‘
Î±=1
(aÎ±j )
2 + Î¾j
= h2j + Î¾
j. (5)
De meË†me, pour j 6= k :
cov(xj, xk) =
qâˆ‘
Î±=1
aÎ±j a
Î±
k + 0. (6)
On voit ainsi que les covariances des variables aleÂ´atoires xj, j = 1, . . . , p, sont comple`tement
reconstitueÂ´es par la matrice de saturation Aq tandis que les variances se deÂ´composent en
une part due aux facteurs communs, appeleÂ´e communaliteÂ´ ou variance commune, et une
part due aux facteurs speÂ´cifiques, appeleÂ´e variance speÂ´cifique ou reÂ´siduelle.
On remarque eÂ´galement que Aq est la matrice des covariances entre les variables aleÂ´atoires
xj, j = 1, . . . , p, et les facteurs communs fÎ±, Î± = 1, . . . , q. En effet :
cov(x, f) = E(xf â€²) = E((Aqf + e+ Âµ)f â€²)
= AqE(ff â€²) + E(ef â€²) + ÂµE(f â€²)
= Aq. (7)
On travaille maintenant sur les variables standardiseÂ´es, câ€™est-a`-dire que xËœ correspond au
vecteur x centreÂ´ reÂ´duit : xËœ = Î£âˆ’1/2(xâˆ’ Âµ).
cÂ© Revue MODULAD, 2007 -4- NumeÂ´ro 37
Dans ce cas, la matrice Aq devient la matrice des correÂ´lations lineÂ´aires entre les variables
xj et les facteurs fÎ±, et lâ€™eÂ´quation (4) sâ€™eÂ´crit :
Î¥ = AqA
â€²
q + Î (8)
ou` Î¥ est la matrice de correÂ´lation lineÂ´aire de x.
De facÂ¸on analogue a` (5), on a :
1 = h2j + Î¾
j. (9)
Dans la suite de cet article, nous consideÂ´rons que le vecteur xËœ correspond au vecteur x
centreÂ´ reÂ´duit.
3 Estimation du mode`le
On veut estimer Aq et f dans le mode`le (1). Rigoureusement, on ne devrait pas parler
dâ€™estimation pour f car il sâ€™agit dâ€™un vecteur aleÂ´atoire, on va donc obtenir une reÂ´alisation
et non une estimation de f . Nous nous conformerons cependant a` cet abus de langage,
freÂ´quent dans la litteÂ´rature.
Pour cela, on dispose dâ€™un eÂ´chantillon {x1, . . . ,xn} de n reÂ´alisations indeÂ´pendantes et
identiquement distribueÂ´es du vecteur aleÂ´atoire x de Rp.
Dâ€™apre`s (1), on peut eÂ´crire pour tout i = 1, . . . , n :
xËœi = Aqfi + ei. (10)
On note :
â€“ XËœ la matrice (nÃ— p) des donneÂ´es centreÂ´es reÂ´duites.
â€“ Fq la matrice (nÃ— q) correspondant aux n reÂ´alisations des q facteurs communs. Elle
est appeleÂ´e matrice des scores des facteurs communs (â€factor scores matrixâ€).
â€“ Eq la matrice (nÃ— p) des erreurs speÂ´cifiques.
Le mode`le dâ€™A.F. sur eÂ´chantillon sâ€™eÂ´crit alors :
XËœ = FqA
â€²
q + Eq.
(nÃ— p) (nÃ— q)(q Ã— p) (nÃ— p) (11)
Nous preÂ´sentons ici trois meÂ´thodes dâ€™estimation de Aq et Fq. Pour toute meÂ´thode dâ€™es-
timation, il faut ensuite choisir le nombre q (avec q â‰¤ p) de facteurs communs que lâ€™on
retient. Quelques crite`res pour le choisir sont discuteÂ´s dans la section 3.4.
3.1 Estimation du mode`le via les composantes principales
Cette technique utilise lâ€™A.C.P. comme meÂ´thode dâ€™estimation du mode`le dâ€™A.F. Nous
rappelons donc dans un premier temps le principe de lâ€™A.C.P., puis nous expliquons com-
ment cette meÂ´thode est utiliseÂ´e pour estimer le mode`le dâ€™A.F.
cÂ© Revue MODULAD, 2007 -5- NumeÂ´ro 37
3.1.1 PreÂ´sentation de lâ€™A.C.P.
Lâ€™A.C.P est proposeÂ´e pour la premie`re fois par Pearson en 1901, elle est ensuite inteÂ´greÂ´e
a` la statistique matheÂ´matique par Hotelling en 1933. Lâ€™A.C.P. peut eË†tre consideÂ´reÂ´e selon
diffeÂ´rents points de vue. La preÂ´sentation la plus freÂ´quente dans la litteÂ´rature francophone
est geÂ´omeÂ´trique. Lâ€™A.C.P est alors vue comme une technique visant a` repreÂ´senter de
facÂ¸on optimale des donneÂ´es, selon certains crite`res geÂ´omeÂ´triques et algeÂ´briques. Le lecteur
pourra se reporter a` lâ€™ouvrage de Lebart et al. (1997). Lâ€™A.C.P peut eË†tre consideÂ´reÂ´e sur
un plan probabiliste, elle est alors un cas particulier du mode`le dâ€™A.F. ou` les variances
speÂ´cifiques sont nulles ou eÂ´gales, voir par exemple Tipping et Bishop (1999). Dans cet
article, nous adopterons une preÂ´sentation de lâ€™A.C.P. qui nous permettra de faire le lien
avec lâ€™A.F.
Lâ€™A.C.P preÂ´sente deux variantes, elle peut eË†tre reÂ´aliseÂ´e a` partir des donneÂ´es centreÂ´es ou
des donneÂ´es centreÂ´es reÂ´duites. Dans le premier cas, on parle dâ€™A.C.P. non normeÂ´e ou A.C.P
sur matrice des covariances. Dans le second cas, on parle dâ€™A.C.P normeÂ´e ou A.C.P. sur
matrice des correÂ´lations. Nous preÂ´sentons dans cet article lâ€™A.C.P. normeÂ´e, variante la
plus utiliseÂ´e.
Lâ€™objectif de lâ€™analyse du nuage des individus {x1, . . . , xn} en A.C.P. est de deÂ´terminer
q nouvelles variables Ïˆ1, . . . , Ïˆq avec q â‰¤ p, permettant de reÂ´sumer â€au mieuxâ€ les p
variables xËœ1, . . . , xËœp. Ces q nouvelles variables sont appeleÂ´es les composantes principales des
individus. Elles sont deÂ´finies comme des combinaisons lineÂ´aires des p variables xËœ1, . . . , xËœp.
On a donc, pour Î± = 1, . . . , q :
ÏˆÎ± = vÎ±1 xËœ
1 + . . .+ vÎ±p xËœ
p = XËœvÎ±. (12)
On suppose que lâ€™espace Rn est muni de la meÂ´trique M , matrice de dimension (n Ã— n),
avec M = diag(1/
âˆš
m, . . . , 1/
âˆš
m), ou` m = n ou m = n âˆ’ 1 selon lâ€™estimateur de la
variance choisi.
On veut que ces composantes soient de variance maximale et deux a` deux orthogonales.
Par construction, les colonnes de XËœ sont centreÂ´es et donc les composantes principales le
sont aussi. On a donc :
V(ÏˆÎ±) = (ÏˆÎ±)â€²MÏˆÎ± = (vÎ±)â€²RvÎ± (13)
ou` R = XËœ â€²MXËœ est la matrice des correÂ´lations empiriques entre les variables initiales
x1, . . . , xp.
En ajoutant la contrainte (vÎ±)â€²(vÎ±) = 1, on deÂ´montre, voir par exemple Lebart et al.
(1997), que pour Î± = 1, . . . , q, vÎ± est le vecteur propre associeÂ´ a` la Î±e`me plus grande
valeur propre de la matrice des correÂ´lations R.
On construit ainsi la matrice Î¨q dont les colonnes sont les composantes principales des
individus ÏˆÎ±, Î± = 1, . . . , q :
Î¨q = XËœVq (14)
ou` Vq est la matrice (p Ã— q) dont les colonnes sont les vecteurs propres vÎ±, Î± = 1, . . . , q,
associeÂ´s aux q plus grandes valeurs propres de la matrice R.
cÂ© Revue MODULAD, 2007 -6- NumeÂ´ro 37
3.1.2 Estimation du mode`le dâ€™A.F.
Lâ€™A.C.P. peut eË†tre utiliseÂ´e comme meÂ´thode dâ€™estimation du mode`le dâ€™A.F. Le lien entre
lâ€™A.C.P. et lâ€™A.F. sâ€™obtient facilement a` partir de la deÂ´composition en valeurs singulie`res
(D.V.S.) de la matrice Z =MXËœ.
On note r (avec r â‰¤ p < n) le rang de la matrice Z et on eÂ´crit sa D.V.S. :
Z = UÎ›V â€² (15)
ou` :
â€“ Î› = diag(
âˆš
Î»1, . . . ,
âˆš
Î»r) des valeurs singulie`res des matrices ZZ
â€² et Z â€²Z rangeÂ´es
par ordre deÂ´croissant (
âˆš
Î»1 â‰¥
âˆš
Î»2 â‰¥ . . . â‰¥
âˆš
Î»r > 0).
â€“ U est la matrice orthonormeÂ´e (nÃ— r) dont les colonnes sont les vecteurs propres de
ZZ â€² associeÂ´s aux r valeurs propres.
â€“ V est la matrice orthonormeÂ´e (pÃ— r) dont les colonnes sont les vecteurs propres de
Z â€²Z associeÂ´s aux r valeurs propres.
On a donc :
XËœ =Mâˆ’1Z =Mâˆ’1UÎ›V â€². (16)
On note Uq, Î›q et Vq les matrices contenant respectivement les q premie`res colonnes de
U , Î› et V .
â€¢ Avec q = r.
Pour se ramener au mode`le dâ€™A.F. (11), on pose :
FË†q = M
âˆ’1Uq (17)
AË†q = VqÎ›q. (18)
On note que dans ce cas EË†q = 0.
Comme U â€²qUq = Ir, on montre que :
AË†q = Z
â€²Uq = XËœ â€²Mâˆ’1Uq. (19)
Cette eÂ´criture est utiliseÂ´e pour deÂ´montrer que les eÂ´leÂ´ments de la matrice AË†q, noteÂ´s aË†
Î±
j ,
sont les correÂ´lations empiriques entre les variables xj et les facteurs fÎ± (deÂ´tails en annexe
7.1).
Comme VqV
â€²
q = Ip, on montre eÂ´galement que :
FË†q = XËœ VqÎ›
âˆ’1
qï¸¸ ï¸·ï¸· ï¸¸
V âˆ—q
. (20)
Cette eÂ´criture de FË†q en fonction de XËœ fait ainsi apparaitre la matrice V
âˆ—
q des coefficients
des scores des facteurs communs, calculeÂ´e par certains logiciels statistiques.
â€¢ Avec q < r.
En ne retenant que les vecteurs propres associeÂ´s aux q plus grandes valeurs propres, on a
lâ€™approximation de XËœ suivante :
XËœ = FË†qAË†q + EË†q (21)
ou` :
cÂ© Revue MODULAD, 2007 -7- NumeÂ´ro 37
â€“ FË†q contient les q premie`res colonnes de FË†q deÂ´finie dans (17).
â€“ AË†q contient les q premie`res colonnes de AË†q deÂ´finie dans (18).
â€“ EË†q est la matrice des erreurs associeÂ´e a` cette approximation.
Avec cette meÂ´thode dâ€™estimation, on montre facilement (voir les deÂ´tails en annexe 7.1)
que les facteurs communs estimeÂ´s posse`dent les bonnes proprieÂ´teÂ´s mais que les hypothe`ses
du mode`le ne sont pas neÂ´cessairement toutes veÂ´rifieÂ´es.
3.1.3 Lien avec lâ€™A.C.P.
Les facteurs communs estimeÂ´s par cette meÂ´thode correspondent aux composantes prin-
cipales des individus (trouveÂ´es en A.C.P.) standardiseÂ´es. En effet, dâ€™apre`s les eÂ´galiteÂ´s (14)
et (20), on voit que :
FË†q = Î¨qÎ›
âˆ’1
q (22)
De plus, la matrice de saturations AË†q est eÂ´gale a` la matrice des composantes princi-
pales des variables. En effet, si on preÂ´sente lâ€™A.C.P. dâ€™un point de vue geÂ´omeÂ´trique, on
reÂ´alise geÂ´neÂ´ralement non seulement lâ€™analyse des points-individus, comme preÂ´senteÂ´ ici,
mais eÂ´galement celle des points-variables. On montre que ces composantes correspondent
aux correÂ´lations entre les variables xj et les facteurs fÎ±, et donc aux saturations (voir
lâ€™ouvrage de Lebart et al., 1997).
3.1.4 Quelques eÂ´leÂ´ments de vocabulaire
On peut introduire, a` partir de ces premiers reÂ´sultats, le vocabulaire utiliseÂ´ en A.C.P.
et en A.F. (tableau 1).
Tab. 1 â€“ Quelques eÂ´leÂ´ments de vocabulaire en A.F. et A.C.P.
Matrices FrancÂ¸ais Anglais
ACP Î¨q Composantes principales Principal component scores
Vq Coefficient des composantes principales Principal component scoring coefficients
Fq Facteurs communs Factor scores
AF ou Standardized principal component scores
V âˆ—q Coefficients des facteurs communs Factor scoring coefficients
ou Standardized principal component scoring coefficients
3.2 MeÂ´thode du facteur principal
A partir de lâ€™eÂ´chantillon {x1, . . . ,xn}, on calcule la matrice des correÂ´lations empiriques
deÂ´finie par :
R = XËœ â€²MXËœ (23)
Lâ€™eÂ´quation (8) du mode`le de structure de covariance sur eÂ´chantillon sâ€™eÂ´crit alors :
R = AË†qAË†
â€²
q + ÎË† (24)
Il faut donc deÂ´terminer AË†q et ÎË†.
cÂ© Revue MODULAD, 2007 -8- NumeÂ´ro 37
Pour cela, la meÂ´thode du facteur principal estime Î (en fait Î¥ âˆ’ Î) et factorise R âˆ’ ÎË†
pour obtenir AË†qAË†
â€²
q en utilisant les valeurs propres et vecteurs propres de Râˆ’ ÎË†.
â€¢ Estimation de Î.
Dâ€™apre`s lâ€™eÂ´quation (9), un estimateur de Î¥âˆ’ Î est donneÂ´ par :
Râˆ’ ÎË† =
ï£«ï£¬ï£¬ï£¬ï£­
hË†21 r12 . . . r1p
r21 hË†
2
2 . . . r2p
...
...
...
...
rp1 rp2 . . . hË†
2
p
ï£¶ï£·ï£·ï£·ï£¸
ou` hË†2j est lâ€™estimation de la j
e`me communaliteÂ´ deÂ´finie par : hË†2j = 1âˆ’ Î¾Ë†j.
Dâ€™apre`s (5), la communaliteÂ´ h2j traduit la part commune entre x
j et les p âˆ’ 1 variables
restantes. Ainsi, une estimation courante pour la communaliteÂ´ h2j est R
2
j , le coefficient de
correÂ´lation multiple entre xj et les pâˆ’ 1 variables restantes.
Ainsi,
hË†2j = R
2
j = 1âˆ’
1
rjj
(25)
ou` rjj est le j e`me eÂ´leÂ´ment diagonal de Râˆ’1.
Pour effectuer ces estimations, R doit eË†tre reÂ´gulie`re. Si R est singulie`re, on utilise pour
hË†2j la valeur absolue (ou le carreÂ´) de la plus grande correÂ´lation de x
j avec les pâˆ’ 1 autres
variables. Un autre moyen de remeÂ´dier a` cette singulariteÂ´ est de remarquer que, puisque
R est singulie`re, cela signifie quâ€™il existe des combinaisons lineÂ´aires des variables. On peut
donc supprimer les redondances et rendre ainsi R inversible.
Notons t le rang de la matrice Râˆ’ ÎË†.
â€¢ Avec q = t.
- Estimation de Aq.
On eÂ´crit la deÂ´composition spectrale de la matrice Râˆ’ ÎË† :
Râˆ’ ÎË† = CDC â€² = (CD1/2)(CD1/2)â€² = AË†qAË†â€²q (26)
ou` :
â€“ D = diag(Î¸1, Î¸2, . . . , Î¸t) des valeurs propres non nulles de Râˆ’ ÎË†.
â€“ C est la matrice orthonormale dont les colonnes sont les vecteurs propres normeÂ´s
de Râˆ’ ÎË† associeÂ´s aux t valeurs propres non nulles.
- Estimation de Fq.
Apre`s avoir estimeÂ´ Aq, il faut â€estimerâ€ Fq. Une meÂ´thode possible est de choisir lâ€™estima-
teur lineÂ´aire fË† = LxËœ qui minimise lâ€™erreur quadratique moyenne :
E[â€–fË† âˆ’ fâ€–2] = E[â€–LxËœâˆ’ fâ€–2] = E[â€–LAqf + Leâˆ’ fâ€–2]. (27)
cÂ© Revue MODULAD, 2007 -9- NumeÂ´ro 37
Seber (1984) montre que (27) est eÂ´gale a` :
trace(Lâ€²LÎ£)âˆ’ 2trace(LAq) + q. (28)
En diffeÂ´renciant par rapport a` Aq, on obtient :
2LÎ£âˆ’ 2Aâ€²q = 0
L = Aâ€²qÎ£
âˆ’1 (29)
et donc :
fË† = Aâ€²qÎ£
âˆ’1xËœ. (30)
En remplacÂ¸ant Aq par son estimateur (26), et Î£ par la matrice de variance covariance
empirique S, on obtient :
FË†q = XËœS
âˆ’1AË†q
= XËœRâˆ’1AË†q car on travaille avec la matrice XËœ centreÂ´e reÂ´duite. (31)
Remarque. En deÂ´veloppant le calcul de Î£âˆ’1 = (AqAâ€²q+Î)
âˆ’1 dans (30), on montre (voir
lâ€™ouvrage de Seber, 1984) que fË† est lâ€™estimateur â€ridgeâ€ de f :
fË† = (Iq + A
â€²
qÎ
âˆ’1Aq)âˆ’1Aâ€²qÎ
âˆ’1xËœ. (32)
â€¢ Avec q < t.
Afin de retenir seulement q facteurs communs dans le mode`le dâ€™A.F., on ne conserve que
les q, premie`res colonnes des matrices AË†q et FË†q deÂ´finies respectivement dans (26) et (31).
â€¢ IteÂ´ration de la meÂ´thode.
Cette meÂ´thode du facteur principal peut facilement eË†tre iteÂ´reÂ´e afin dâ€™ameÂ´liorer lâ€™estima-
tion de Aq. Apre`s avoir estimeÂ´ Aq a` partir de (26), nous pouvons obtenir une nouvelle
estimation de la communaliteÂ´ en utilisant (5) :
hË†2j =
qâˆ‘
Î±=1
(aË†Î±j )
2.
Ces valeurs sont alors inseÂ´reÂ´es dans la diagonale de Râˆ’ÎË†, ce qui nous permet dâ€™obtenir une
nouvelle estimation de Aq a` partir de la deÂ´composition spectrale de la nouvelle matrice Râˆ’
ÎË†, comme dans lâ€™eÂ´quation (26). Ce processus est alors iteÂ´reÂ´ jusquâ€™a` ce que les estimations
de la communaliteÂ´ se stabilisent.
Cependant, un deÂ´faut majeur de la meÂ´thode iteÂ´reÂ´e est quâ€™elle ne converge pas toujours.
De plus, lors de ces iteÂ´rations, hË†2j peut devenir supeÂ´rieur a` 1, ce qui implique Î¾Ë†
j < 0. Or,
ceci est impossible car on ne peut pas avoir une variance speÂ´cifique estimeÂ´e neÂ´gative. Ce
proble`me est connu sous le nom de Heywood case (Heywood, 1931).
cÂ© Revue MODULAD, 2007 -10- NumeÂ´ro 37
3.3 Maximum de vraisemblance
On suppose que lâ€™eÂ´chantillon {x1, . . . ,xn} est issu dâ€™une loi multi-normale Np(Âµ,Î£).
Alors (nâˆ’ 1)S suit la loi de Wishart Wp(nâˆ’ 1,Î£) et la log-vraisemblance de lâ€™eÂ´chantillon
est donneÂ´e par :
logL(Aq,Î) = câˆ’ 1
2
(nâˆ’ 1)(ln|Î£|+ trace(Î£âˆ’1S)) (33)
ou` c est une constante et |M | deÂ´signe le deÂ´terminant de M .
Les parame`tres Aq et Î vont alors pouvoir eË†tre estimeÂ´s en maximisant logL(Aq,Î), sous
la contrainte Î£ = AqA
â€²
q + Î avec Î matrice diagonale. La condition suivante : A
â€²
qÎ
âˆ’1Aq
diagonale, est souvent rajouteÂ´e afin dâ€™avoir une solution unique.
Lâ€™eÂ´quation du maximum de vraisemblance nâ€™a pas de solution analytique, la reÂ´solution se
fait donc numeÂ´riquement par iteÂ´rations successives. Cependant, cette meÂ´thode ne converge
pas toujours. De plus, des cas de variances speÂ´cifiques estimeÂ´es neÂ´gatives peuvent la` encore
se produire (Heywood case).
Notons quâ€™avec cette meÂ´thode, les facteurs communs obtenus fË†Î± ne sont pas forceÂ´ment
ordonneÂ´s par variance expliqueÂ´e deÂ´croissante comme avec la meÂ´thode des composantes
principales et la meÂ´thode du facteur principal.
Apre`s avoir estimeÂ´ Aq et Î, il faut estimer la matrice Fq des scores des facteurs communs.
Pour cela, on utilise souvent la meÂ´thode des moindres carreÂ´s geÂ´neÂ´raliseÂ´s :
FË†q = (AË†
â€²
qÎË†
âˆ’1AË†q)âˆ’1AË†â€²qÎË†
âˆ’1XËœ. (34)
On ne retient ensuite que les q, avec q â‰¤ p, premie`res colonnes des matrices FË†q et AË†q.
Pour de plus amples deÂ´tails sur cette meÂ´thode dâ€™estimation, le lecteur pourra se reÂ´feÂ´rer a`
lâ€™ouvrage de Seber (1984).
3.4 Choix du nombre de facteurs
Comme dans toute meÂ´thode factorielle, une eÂ´tape importante de lâ€™A.F. est le choix
du nombre q de facteurs communs. La qualiteÂ´ des estimations du mode`le deÂ´pend de q.
En effet, si q est trop grand, certains facteurs speÂ´cifiques vont eË†tre meÂ´langeÂ´s aux facteurs
communs. A lâ€™inverse si q est trop petit, des facteurs communs importants risquent dâ€™eË†tre
oublieÂ´s. DiffeÂ´rents crite`res theÂ´oriques et empiriques peuvent eË†tre utiliseÂ´s pour choisir q.
Voici deux crite`res theÂ´oriques reposant sur la normaliteÂ´ de lâ€™eÂ´chantillon :
â€¢ Crite`re 1.
Ce crite`re consiste a` deÂ´terminer si les (p âˆ’ k) dernie`res valeurs propres de la matrice de
covariance Î£ sont significativement diffeÂ´rentes entre elles. On fait pour cela lâ€™hypothe`se
que les n observations sont les reÂ´alisations dâ€™un vecteur aleÂ´atoire gaussien dont les (pâˆ’k)
dernie`res valeurs propres Î»k+1, . . . , Î»p de la matrice Î£ sont eÂ´gales. Sous cette hypothe`se,
cÂ© Revue MODULAD, 2007 -11- NumeÂ´ro 37
la moyenne arithmeÂ´tique ma des (pâˆ’ k) dernie`res valeurs propres doit eË†tre peu diffeÂ´rente
de la moyenne geÂ´omeÂ´trique mg. On deÂ´finit :
T1 =
(
nâˆ’ 2p+ 11
6
)
(pâˆ’ k)log
(
ma
mg
)
. (35)
Sous H0, on peut montrer que T1 suit une loi du Khi-deux a` v1 =
(pâˆ’k+2)(pâˆ’kâˆ’1)
2
degreÂ´s de
liberteÂ´.
On rejette donc H0 au seuil de signification Î± si lâ€™ineÂ´galiteÂ´ suivante est veÂ´rifieÂ´e :
T1 > Ï‡
2
v1,1âˆ’Î± (36)
ou` Ï‡2v1,1âˆ’Î± est le fractile dâ€™ordre (1âˆ’ Î±) de la loi du Khi-deux a` v1 degreÂ´s de liberteÂ´.
Certains auteurs (voir par exemple Bouveyron, 2006) soulignent le fait que ce crite`re
surestime tre`s souvent le nombre de facteurs a` retenir.
â€¢ Crite`re 2.
Ce crite`re est utiliseÂ´ lorsque la meÂ´thode dâ€™estimation du mode`le dâ€™A.F. est le maximum
de vraisemblance.
On deÂ´sire tester lâ€™hypothe`se que q0 est le bon nombre de facteurs communs. Les hypothe`ses
sont donc : H0 : Î£ = Aq0(Aq0)
â€² + Î, ou` Aq0 est de dimension (p Ã— q0) contre H1 : Î£ =
AqA
â€²
q + Î, ou` Aq est de dimension (pÃ— q) avec q > q0.
La statistique de test est :
T2 =
(
nâˆ’ 2p+ 4q0 + 11
6
)
log
(
|AË†q0AË†â€²q0 + ÎË†|
|S|
)
(37)
ou` AË†q0 et ÎË† sont les estimateurs du maximum de vraisemblance de Aq et Î, obtenus avec
q0 facteurs communs.
Sous H0, on peut montrer que T2 suit une loi du Khi-deux a` v2 =
1
2
[(p âˆ’ q0)2 âˆ’ p âˆ’ q0]
degreÂ´s de liberteÂ´.
On rejette donc H0 au seuil de signification Î± si la condition (36) est veÂ´rifieÂ´e (ou` lâ€™on
aura preÂ´alablement substitueÂ´ v2 a` v1). Si H0 est rejeteÂ´e, cela signifie que le nombre q0 de
facteurs communs est trop petit.
En pratique, on commence souvent avec q0 = 1, et on ajoute des facteurs jusquâ€™a` trouver
la valeur q pour laquelle H0 est veÂ´rifieÂ´e. Ainsi, le risque associeÂ´ a` la proceÂ´dure pour trouver
le bon nombre de facteurs q0 est supeÂ´rieur a` Î±, du fait de la multipliciteÂ´ des tests.
Il faut noter que cette technique est souvent utiliseÂ´e pour fixer la borne supeÂ´rieure du
nombre de facteurs. En effet, on peut trouver dans la litteÂ´rature, voir par exemple Ren-
cher (2002), que lorsque le nombre dâ€™observations est grand, cette meÂ´thode a tendance a`
surestimer le nombre de facteurs communs.
DiffeÂ´rentes re`gles empiriques peuvent eÂ´galement eË†tre utiliseÂ´es pour choisir q. Dans la
deÂ´finition des crite`res ci-dessous, Î»i, i = 1, . . . , p, fait reÂ´feÂ´rence aux valeurs propres de R
cÂ© Revue MODULAD, 2007 -12- NumeÂ´ro 37
(ou S) ou bien Râˆ’ÎË† (ou Sâˆ’ÎË†), selon que la meÂ´thode dâ€™estimation utiliseÂ´e est la meÂ´thode
des composantes principales ou du facteur principal. Voici quelques exemples de crite`res
empiriques.
â€¢ Pourcentage de variance expliqueÂ´e.
On choisit q tel que le pourcentage de variance expliqueÂ´e par les q facteurs soit supeÂ´rieur
ou eÂ´gal a` un seuil fixeÂ´ par lâ€™utilisateur. Lâ€™appreÂ´ciation de ce pourcentage doit tenir compte
du nombre de variables p et du nombre dâ€™observations n. En effet, un pourcentage de 10%
peut eË†tre consideÂ´reÂ´ comme eÂ´leveÂ´ pour p = 100 variables et au contraire, faible pour p = 10.
Notre expeÂ´rience nous a montreÂ´ que ce crite`re a souvent tendance a` surestimer le nombre
de facteurs q.
â€¢ Le test du coude.
On utilise le test du coude de Cattell, ou scree-test, baseÂ´ sur lâ€™analyse des diffeÂ´rences
conseÂ´cutives entre les valeurs propres. On calcule les diffeÂ´rences premie`res :
i = Î»i âˆ’ Î»i+1
puis les diffeÂ´rences secondes :
Î´i = i âˆ’ i+1.
On retient alors les valeurs propres Î»1, Î»2, . . . , Î»k, Î»k+1 tels que Î´1, . . . , Î´k soient tous po-
sitifs.
Visuellement, ce crite`re revient a` deÂ´tecter un coude dans le graphe de lâ€™eÂ´boulis des valeurs
propres. En pratique, la deÂ´tection graphique de ce coude peut se reÂ´veÂ´ler difficile.
â€¢ La re`gle de Kaiser.
Sachant que la variance expliqueÂ´e par un facteur fË†Î± est Î»Î±, il sâ€™agit de retenir les facteurs
dont la variance expliqueÂ´e deÂ´passe la moyenne de la variance totale expliqueÂ´e : Î» =
pP
i=1
Î»i
p
.
Pour la matrice de correÂ´lation R, on a : Î» = 1. Cette valeur 1 peut aussi eË†tre vue comme
la variance de chaque variable xj et on retient donc un facteur sâ€™il explique au moins
autant de variance quâ€™une variable toute seule.
On insiste sur le fait que ces crite`res ne doivent pas se substituer a` une analyse approfondie
de lâ€™interpreÂ´tation des facteurs. Il est indispensable dâ€™examiner lâ€™information apporteÂ´e par
un facteur, et ainsi juger de sa pertinence et de son inteÂ´reË†t quant aux objectifs de lâ€™eÂ´tude.
Lâ€™utilisateur retiendra, par exemple, un facteur dont la part de variance expliqueÂ´e est
faible, mais dont lâ€™inteÂ´reË†t est significatif pour la probleÂ´matique traiteÂ´e. Au contraire, il
pourra rejeter un facteur qui posse`de une part de variance expliqueÂ´e eÂ´leveÂ´e, mais qui
nâ€™aide pas a` comprendre le pheÂ´nome`ne eÂ´tudieÂ´. Ainsi, on peut utiliser ces crite`res comme
valeur initiale du nombre q0 de facteurs, puis au vu de lâ€™interpreÂ´tation des reÂ´sultats et
des objectifs de lâ€™eÂ´tude, on peut augmenter ou diminuer ce nombre q0 afin de trouver une
interpreÂ´tation des reÂ´sultats satisfaisante.
3.5 Choix de la meÂ´thode dâ€™estimation
On trouve dans la litteÂ´rature (voir par exemple Rencher, 2002) que les solutions ob-
tenues avec la meÂ´thode des composantes principales et la meÂ´thode du facteur principal
(iteÂ´reÂ´e ou non) sont tre`s proches lorsque lâ€™une des deux conditions suivantes est veÂ´rifieÂ´e :
cÂ© Revue MODULAD, 2007 -13- NumeÂ´ro 37
â€“ Les correÂ´lations entre les variables xj, j = 1, . . . , p, sont eÂ´leveÂ´es.
â€“ Le nombre de variables p est grand.
Cependant, il est important de noter que la meÂ´thode dâ€™estimation la plus utiliseÂ´e est
celle des composantes principales. Câ€™est une technique qui fournit une approximation
convenable de la solution et qui est facile a` mettre en oeuvre. Ainsi, câ€™est la meÂ´thode
utiliseÂ´e par deÂ´faut lorsquâ€™on estime un mode`le dâ€™A.F. sous les logiciels SAS et SPSS.
Enfin, contrairement aux deux autres techniques, elle ne preÂ´sente pas le proble`me de
Heywood case.
4 La rotation des facteurs
Dans cette section, nous allons preÂ´senter les motivations de la rotation orthogonale
des facteurs estimeÂ´s, puis nous montrerons que cette rotation est justifieÂ´e car elle conserve
les proprieÂ´teÂ´s des facteurs. Quelques crite`res permettant la mise en place dâ€™une rotation
optimale seront ensuite discuteÂ´s. Enfin, nous montrerons brie`vement que la rotation est
possible en A.C.P., comme en A.F., a` condition dâ€™effectuer convenablement la transfor-
mation.
4.1 Motivations
Apre`s avoir estimeÂ´ le mode`le dâ€™A.F., on peut vouloir interpreÂ´ter les facteurs communs
obtenus en deÂ´tectant des groupes de variables correÂ´leÂ´es aux diffeÂ´rents facteurs. La matrice
de saturation Aq, repreÂ´sentant les correÂ´lations entre les variables x
j et les facteurs com-
muns fÎ±, il sâ€™agit de faire apparaË†Ä±tre des variables fortement correÂ´leÂ´es a` un meË†me facteur.
Il est donc souhaitable que pour chaque colonne de Aq les valeurs soient proches soit
de 0, soit de 1 et quâ€™il nâ€™y ait ainsi pas de valeur intermeÂ´diaire. Cela permet alors dâ€™as-
socier clairement des variables a` un facteur. Il faut eÂ´galement sâ€™assurer que sur chaque
ligne de Aq, il nâ€™y aura quâ€™une seule valeur proche de 1. En effet, si la correÂ´lation entre
une variable xj et un facteur fÎ± est proche de 1, les correÂ´lations de cette variable avec
les facteurs restants doivent eË†tre proches de 0, car les facteurs sont orthogonaux entre
eux. Cela se traduit par la condition dâ€™orthonormaliteÂ´ de la matrice de transformation T .
Ainsi, chaque variable xj ne pourra eË†tre parfaitement associeÂ´e quâ€™a` un seul facteur fÎ±.
Cependant, lâ€™estimation AË†q trouveÂ´e ne preÂ´sente pas toujours une telle structure. Afin
de se rapprocher de cette situation, il est possible de reÂ´aliser une rotation des facteurs.
La justification de la possibiliteÂ´ de faire une rotation provient de la non-uniciteÂ´ de la
solution du mode`le dâ€™A.F. Ainsi, il sâ€™agit de choisir la solution optimale du point de vue
de lâ€™interpreÂ´tation des reÂ´sultats.
Remarque. Il existe des rotations qui ne conservent pas la proprieÂ´teÂ´ dâ€™orthogonaliteÂ´
des facteurs communs. Ce type de rotation dites obliques ne sera pas abordeÂ´ dans cet
article. Pour plus de deÂ´tails, le lecteur pourra se reporter a` lâ€™ouvrage de Rencher (2002).
cÂ© Revue MODULAD, 2007 -14- NumeÂ´ro 37
4.2 Justifications de la rotation
La solution du mode`le dâ€™A.F. nâ€™est pas unique. En effet, soit T une matrice orthonor-
male de dimension (q Ã— q). On peut eÂ´crire :
XËœ = FË†qAË†
â€²
q + EË†q
= FË†qTT
â€²AË†â€²q + EË†q
= GË†qBË†
â€²
q + EË†q avec GË†q = FË†qT et BË†q = AË†qT. (38)
Ainsi GË†q est lâ€™estimation de la matrice des facteurs apre`s la rotation et BË†q est lâ€™estimation
de la matrice de saturation apre`s la rotation.
La transformation orthogonale entraË†Ä±ne une rotation â€rigideâ€ des q axes deÂ´finis par les
facteurs communs, câ€™est-a`-dire que les q nouveaux axes restent perpendiculaires apre`s la
rotation.
ProprieÂ´teÂ´ :
Les facteurs communs gÎ± et les saturations bË†Î±j veÂ´rifient toujours les proprieÂ´teÂ´s et hy-
pothe`ses du mode`le dâ€™A.F. apre`s la rotation.
La deÂ´monstration est disponible en annexe 7.2.
On montre en particulier que les saturations apre`s rotation, bË†Î±j , sont toujours les correÂ´lations
des variables dâ€™origine xj aux facteurs apre`s rotation, gÎ±.
Cependant, meË†me si suite a` la rotation, les communaliteÂ´s estimeÂ´es sont inchangeÂ´es et que
lâ€™on a :
hË†2j =
qâˆ‘
Î±=1
(bË†Î±j )
2 =
qâˆ‘
Î±=1
(aË†Î±j )
2, (39)
la variance expliqueÂ´e par chaque facteur fÎ± change lors de la rotation. En effet :
pâˆ‘
j=1
(bË†Î±j )
2 =
pâˆ‘
j=1
(aË†jt
Î±)2 ou` tÎ± est la Î±e`me colonne de T
=
pâˆ‘
j=1
(
qâˆ‘
k=1
aË†kj t
Î±
k )
2
6=
pâˆ‘
j=1
(aË†Î±j )
2. (40)
Apre`s la rotation, les facteurs ne sont donc plus forceÂ´ment rangeÂ´s par ordre de variance
expliqueÂ´e deÂ´croissante.
4.3 Comment faire la rotation ?
Afin dâ€™effectuer la rotation, il faut deÂ´terminer la matrice T qui fournit la â€meilleureâ€
interpreÂ´tation des reÂ´sultats, câ€™est-a`-dire telle que les eÂ´leÂ´ments bË†Î±j de la matrice BË†q = AË†qT
soient proches de 0 ou de 1, on parle de â€structure simpleâ€ de BË†q. DiffeÂ´rents crite`res
existent, le plus utiliseÂ´ est Varimax.
cÂ© Revue MODULAD, 2007 -15- NumeÂ´ro 37
4.3.1 Varimax
Comme toutes les variables nâ€™ont pas la meË†me commmunaliteÂ´, le crite`re Varimax est
souvent appliqueÂ´ sur les valeurs standardiseÂ´es de BË†q, obtenues en divisant chaque ligne
de la matrice BË†q par hË†j. On travaille avec le carreÂ´ de ces eÂ´leÂ´ments, (bË†
Î±
j /hË†j)
2, afin de se
ramener a` des valeurs comprises entre 0 et 1. On note BË†âˆ—q cette matrice. On veut que
ses eÂ´leÂ´ments soient aussi proches que possible de 0 ou de 1. Pour cela, il faut maximiser
la variance empirique de chaque colonne de BË†âˆ—q afin de donner plus de poids aux valeurs
extreË†mes 0 et 1.
Ceci eÂ´quivaut a` maximiser la somme sur lâ€™ensemble des q facteurs des variances empi-
riques de chaque colonne de BË†âˆ—q , câ€™est-a`-dire la quantiteÂ´ :
qâˆ‘
Î±=1
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³
pâˆ‘
j=1
((bË†Î±j )
2/hË†2j)
2
p
âˆ’
ï£«ï£¬ï£¬ï£­
pâˆ‘
j=1
((bË†Î±j )
2/hË†2j)
p
ï£¶ï£·ï£·ï£¸
2
ï£¼ï£´ï£´ï£´ï£½ï£´ï£´ï£´ï£¾ (41)
avec bË†Î±j = aË†jt
Î±, aË†j est la j
e`me ligne de la matrice AË†q, et t
Î± est la Î±e`me colonne de la matrice
T .
La maximisation de la quantiteÂ´ (41) se fait donc de facÂ¸on iteÂ´rative, par rapport a` tÎ±, Î± =
1, . . . , q, sous la contrainte tÎ±(tÎ±)â€² = 1 et tk(tl)â€² = 0 pour k 6= l.
4.3.2 Quartimax
Le crite`re Quartimax maximise la somme des variances des eÂ´leÂ´ments (bË†Î±j )
2 sur toute
la matrice BË†q, câ€™est-a`-dire la quantiteÂ´ :
qâˆ‘
Î±=1
pâˆ‘
j=1
(bË†Î±j )
4
pq
âˆ’
ï£«ï£¬ï£¬ï£­
qâˆ‘
Î±=1
pâˆ‘
j=1
(bË†Î±j )
2
pq
ï£¶ï£·ï£·ï£¸
2
. (42)
On trouve dans la litteÂ´rature (Jobson, 1992) que ceci eÂ´quivaut a` maximiser la quantiteÂ´
qâˆ‘
Î±=1
pâˆ‘
j=1
(bË†Î±j )
4. Il est de plus mentionneÂ´ que cette meÂ´thode a tendance a` produire un facteur
commun geÂ´neÂ´ral car elle maximise la variance des (bË†Î±j )
2 sur la totaliteÂ´ de la matrice de
saturation BË†âˆ—q et non sur chaque colonne, comme le crite`re Varimax.
4.3.3 Orthomax
Le crite`re Orthomax est une geÂ´neÂ´ralisation des crite`res de rotation orthogonale. Il
sâ€™agit de maximiser la quantiteÂ´ :
qâˆ‘
Î±=1
ï£±ï£²ï£³
pâˆ‘
j=1
(bË†Î±j )
4 âˆ’ Î´
p
(
pâˆ‘
j=1
(bË†Î±j )
2
)2ï£¼ï£½ï£¾ (43)
cÂ© Revue MODULAD, 2007 -16- NumeÂ´ro 37
Pour Î´ = 0 et Î´ = 1, on retrouve respectivement le crite`re Quartimax et la version non
standardiseÂ´e de Varimax. Pour Î´ = 0.5, le crite`re sâ€™appelle Biquartimax et pour Î´ = q
2
, ce
crite`re est connu sous le nom de Equamax.
4.4 Remarques sur la rotation et lâ€™A.C.P.
La rotation en A.C.P est possible, comme en A.F., mais il faut eË†tre prudent et appliquer
la transformation T aux bonnes matrices.
Si on applique la rotation directement sur les composantes principales Î¨q, les nouvelles
composantes apre`s rotation Î¨qT ne sont pas neÂ´cessairement non correÂ´leÂ´es. En effet :
(Î¨qT )
â€²MÎ¨qT = T â€²Î¨â€²qMÎ¨qT = T
â€²Î›2T. (44)
On en deÂ´duit que (Î¨qT )
â€²MÎ¨qT nâ€™est pas forceÂ´ment la matrice identiteÂ´.
Afin dâ€™effectuer convenablement une rotation en A.C.P. il faut donc introduire la matrice
T au bon endroit dans lâ€™eÂ´criture de XËœ.
Il ne faut pas eÂ´crire :
XËœ =
Î¨qï¸· ï¸¸ï¸¸ ï¸·
Mâˆ’1UÎ›TTâ€™V â€² (45)
mais :
XËœ =
FË†qï¸· ï¸¸ï¸¸ ï¸·
Mâˆ’1U TTâ€™Î›V â€². (46)
Ainsi les composantes obtenues apre`s rotation correspondent en fait aux facteurs communs
obtenus apre`s rotation GË†q = M
âˆ’1UT . Nous avons deÂ´ja` veÂ´rifieÂ´ en annexe 7.2 que ces
facteurs ne sont pas correÂ´leÂ´s.
On comprend ainsi la raison pour laquelle les logiciels qui calculent les composantes prin-
cipales ne proposent pas de rotation des composantes, celles-ci deviendraient en effet
correÂ´leÂ´es. A lâ€™inverse, les logiciels qui construisent les facteurs communs proposent une
rotation.
5 Un exemple dâ€™application sur des donneÂ´es de cri-
minaliteÂ´
5.1 ProbleÂ´matique
â€¢ DonneÂ´es.
Dans cette exemple, nous eÂ´tudions la criminaliteÂ´ de seize villes ameÂ´ricaines, donneÂ´es
eÂ´tudieÂ´es par de nombreux auteurs dont Rencher (2002) (extraites de U.S. Statistical Abs-
tract, 1970). Pour cela, sept types dâ€™effractions sont releveÂ´s et un taux pour 100 000
habitants est calculeÂ´ (tableau 2). Lâ€™objectif est de reÂ´sumer la criminaliteÂ´ de ces villes
a` lâ€™aide de facteurs communs. Nous allons eÂ´tudier les reÂ´sultats fournis par les logiciels
cÂ© Revue MODULAD, 2007 -17- NumeÂ´ro 37
SAS, SPAD et SPSS. Nous choisissons dâ€™estimer le mode`le dâ€™A.F. via les composantes
principales.
Il faut noter que le logiciel SPAD utilise la deÂ´finition de lâ€™estimateur biaiseÂ´ de lâ€™eÂ´cart-type
(m = n), les logiciels SAS et SPSS utilise au contraire la deÂ´finition de lâ€™estimateur non
biaiseÂ´ de lâ€™eÂ´cart-type (m = n âˆ’ 1). Il est cependant possible de preÂ´ciser au logiciel SAS
dâ€™utiliser lâ€™estimation biaiseÂ´e de lâ€™eÂ´cart-type avec lâ€™option â€vardef=nâ€.
Tab. 2 â€“ CriminaliteÂ´
Ville Meurtre Viol Vol Agression Cambriolage Vol avec effraction Vol de voiture
Atlanta 16.5 24.8 106 147 1112 905 494
Boston 4.2 13.3 122 90 982 669 954
Chicago 11.6 24.7 340 242 808 609 645
Dallas 18.1 34.2 184 293 1668 901 602
Denver 6.9 41.5 173 191 1534 1368 780
Detroit 13 35.7 477 220 1566 1183 788
Hartford 2.5 8.8 68 103 1017 724 468
Honolulu 3.6 12.7 42 28 1457 1102 637
Houston 16.8 26.6 289 186 1509 787 697
Kansas City 10.8 43.2 255 226 1494 955 765
Los Angeles 9.7 51.8 286 355 1902 1386 862
New Orleans 10.3 39.7 266 283 1056 1036 776
New York 9.4 19.4 522 267 1674 1392 848
Portland 5 23 157 144 1530 1281 488
Tucson 5.1 22.9 85 148 1206 756 483
Washington 12.5 27.6 524 217 1496 1003 793
On note X = (xji ), i = 1, . . . , n, j = 1, . . . , p, la matrice des donneÂ´es preÂ´senteÂ´es dans le
tableau 2 avec n = 16 observations et p = 7 variables.
â€¢ Choix de q.
Les diffeÂ´rents auteurs, qui ont eÂ´tudieÂ´s ces donneÂ´es, ont conserveÂ´ q = 3 facteurs communs
(voir par exemple Rencher, 2002). Nous veÂ´rifions, par une eÂ´tude approfondie, que la
valeur 3 permet une bonne interpreÂ´tation des reÂ´sultats. Pour cela, nous comparons les
valeurs de q proposeÂ´es par les crite`res empiriques discuteÂ´s dans la section 3.4 et examinons
attentivement les valeurs propres de la matrice des correÂ´lations R (figure 1). Dans cette
eÂ´tape, nous privileÂ´gions lâ€™interpreÂ´tation des facteurs et leur inteÂ´reË†t pour la probleÂ´matique
eÂ´tudieÂ´e. Nous veÂ´rifions ainsi que la valeur 3 permet des interpreÂ´tations inteÂ´ressantes pour
lâ€™eÂ´tude meneÂ´e. Dans la suite de cet article, la valeur q est donc choisi eÂ´gale a` 3, pour
lâ€™A.C.P. comme pour lâ€™A.F.
Fig. 1 â€“ Valeurs propres de la matrice R
cÂ© Revue MODULAD, 2007 -18- NumeÂ´ro 37
5.2 Logiciel SAS
La premie`re partie preÂ´sente la proceÂ´dure Princomp qui reÂ´alise une A.C.P. sur les
donneÂ´es. Dans un second temps, nous deÂ´crivons les reÂ´sultats de la proceÂ´dure Factor
avec pour meÂ´thode dâ€™estimation lâ€™A.C.P. et ainsi nous comparons les reÂ´sultats des deux
proceÂ´dures.
5.2.1 ProceÂ´dure PRINCOMP
Le code SAS de la proceÂ´dure Princomp est preÂ´senteÂ´ dans la figure 2. Lâ€™option â€n = 3â€
permet de reÂ´duire lâ€™affichage des reÂ´sultats a` 3 composantes principales.
Fig. 2 â€“ Code SAS de la proceÂ´dure Princomp
La proceÂ´dure propose comme reÂ´sultats la matrice des correÂ´lations empiriques R = XËœ â€²MXËœ,
ses valeurs propres Î»Î± et les vecteurs propres associeÂ´s, câ€™est-a`-dire la matrice Vq (fi-
gure 3). Cette matrice peut eÂ´galement eË†tre obtenue dans une table avec lâ€™option â€outs-
tat=loadACPâ€. Il sâ€™agit en fait de la matrice des coefficients des composantes principales.
Fig. 3 â€“ Sorties numeÂ´riques de la proceÂ´dure Princomp
Fig. 4 â€“ Matrice Vq des coefficients des composantes principales
Lâ€™option â€out=compâ€ permet dâ€™obtenir dans une table appeleÂ´e â€compâ€, la matrice Î¨q des
composantes principales (figure 5).
cÂ© Revue MODULAD, 2007 -19- NumeÂ´ro 37
Fig. 5 â€“ Matrice Î¨q des composantes principales
5.2.2 ProceÂ´dure FACTOR
La proceÂ´dure SAS permettant dâ€™estimer un mode`le dâ€™A.F. est Factor. Cette proceÂ´dure
nous propose diffeÂ´rentes meÂ´thodes dâ€™estimation dont la meÂ´thode des composantes princi-
pales que nous speÂ´cifions avec lâ€™option â€method=prinâ€ (figure 6). Lâ€™option â€nfactors=3â€
permet de fixer q.
Fig. 6 â€“ Code SAS de la proceÂ´dure Factor
La proceÂ´dure Factor fournit comme la proceÂ´dure Princomp les valeurs propres Î»Î± de
la matrice des correÂ´lations empiriques R = XËœ â€²MXËœ.
En preÂ´cisant lâ€™option â€out=factâ€, le logiciel calcule la reÂ´alisation des facteurs communs
fÎ±, Î± = 1, . . . , q, sur les n observations, câ€™est-a`-dire la matrice FË†q (figure 7).
Fig. 7 â€“ Matrice FË†q des facteurs communs
Le logiciel calcule la matrice V âˆ—q des coefficients des facteurs communs. Dans les sorties
du logiciel, elle est appeleÂ´e â€standardized scoring coefficentsâ€ (figure 8). Avec lâ€™option
â€outstat=loadAFâ€, on peut eÂ´galement obtenir cette matrice dans la table â€loadAFâ€.
cÂ© Revue MODULAD, 2007 -20- NumeÂ´ro 37
Fig. 8 â€“ Matrice V âˆ—q des coefficients des scores des facteurs communs
La matrice de saturation estimeÂ´e AË†q est preÂ´senteÂ´e sous le nom de â€factor patternâ€ (figure
9). Ses coefficients sont les correÂ´lations des variables dâ€™origine xj aux facteurs communs
fÎ±.
Fig. 9 â€“ Matrice AË†q de saturation
En rajoutant lâ€™option â€rotate=varimaxâ€ dans le code preÂ´senteÂ´ dans la figure 6, nous
demandons au logiciel dâ€™effectuer une rotation orthogonale avec le crite`re Varimax. La
matrice T de transformation orthogonale, estimeÂ´e selon ce crite`re, est preÂ´senteÂ´e dans la
figure 10.
Fig. 10 â€“ Matrice T de transformation orthogonale
En rajoutant lâ€™option â€out=factrotationâ€, on obtient la matrice GË†q = FË†qT des facteurs
communs apre`s la rotation (figure 11).
La matrice de saturation apre`s rotation, BË†q = AË†qT , est calculeÂ´e et nommeÂ´e â€rotated
factor patternâ€ (figure 12). Les valeurs de cette matrice sont les correÂ´lations des variables
dâ€™origine xj aux facteurs communs gÎ± apre`s la rotation, donneÂ´s en figure 11.
On voit tre`s clairement sur cet exemple que la rotation des â€loadingsâ€ facilite la lecture des
reÂ´sultats. Les valeurs de AË†q (figure 9) sont tre`s disperseÂ´es et rendent difficile la deÂ´tection
de groupes de variables correÂ´leÂ´es a` un meË†me facteur. Au contraire, la matrice BË†q (figure
12) posse`de beaucoup plus de valeurs soit proches de 1, soit proches de 0. On peut ainsi
associer clairement chaque variable a` un facteur.
cÂ© Revue MODULAD, 2007 -21- NumeÂ´ro 37
Fig. 11 â€“ Matrice GË†q des facteurs apre`s la rotation
Fig. 12 â€“ Matrice BË†q de saturation apre`s la rotation
On peut alors deÂ´crire les trois facteurs communs de la criminaliteÂ´ dans ces seize villes
ameÂ´ricaines. Le premier facteur fait reÂ´feÂ´rence aux crimes violents contre une personne :
meurtre, viol et agression. Le second facteur se rapporte aux crimes en rapport avec la
maison : cambriolage et vol avec effraction. Enfin le troisie`me facteur fait reÂ´feÂ´rence aux
vols commis a` lâ€™exteÂ´rieur : vol et vol de voiture.
5.3 Logiciel SPSS
Avec le logiciel SPSS, dans le menu â€Analyse â†’ Factorisation â†’ Analyse factorielleâ€,
on peut choisir diffeÂ´rentes meÂ´thodes dâ€™extraction des facteurs en cliquant sur le bouton
â€Extractionâ€ : Maximum de vraisemblance, Composantes principales, Factorisation en
axes principaux, etc (figure 13).
En cliquant sur le bouton â€Facteurâ€(figure 13), on peut demander au logiciel dâ€™afficher
lâ€™estimation de la matrice FË†q des scores des facteurs communs, et lâ€™estimation de la matrice
des coefficients des scores des facteurs communs V âˆ—q .
Lâ€™estimation de ces deux matrices FË†q et V
âˆ—
q est preÂ´senteÂ´e a` la figure 14. Nous retrouvons
les reÂ´sultats de la proceÂ´dure Factor de SAS, preÂ´senteÂ´s aux figures 7 et 8.
Le logiciel propose comme reÂ´sultats la matrice de saturation estimeÂ´e, AË†q, appeleÂ´e â€matrice
des composantesâ€ (figure 15). On voit la` lâ€™erreur commise par le logiciel car le terme
â€matrice des composantesâ€ est reÂ´serveÂ´ a` Î¨q. Ce proble`me de vocabulaire provient peut-
eË†tre dâ€™une mauvaise traduction francÂ¸aise de ce logiciel anglais.
Le logiciel nous propose eÂ´galement diffeÂ´rentes rotations. En choisissant le crite`re Varimax,
cÂ© Revue MODULAD, 2007 -22- NumeÂ´ro 37
Fig. 13 â€“ Estimation du mode`le dâ€™A.F. avec SPSS
Fig. 14 â€“ Matrice FË†q et V
âˆ—
q
nous retrouvons les matrices GË†q et BË†q (figure 16) de la proceÂ´dure Factor du logiciel SAS
(figures 11 et 12).
cÂ© Revue MODULAD, 2007 -23- NumeÂ´ro 37
Fig. 15 â€“ Matrice AË†q de saturation
Fig. 16 â€“ Matrice GË†q et BË†q apre`s la rotation
5.4 Logiciel SPAD
Parmi les meÂ´thodes dâ€™analyse factorielle du logiciel SPAD, lâ€™A.C.P. est proposeÂ´e mais
lâ€™A.F. nâ€™est pas disponible. Afin dâ€™effectuer une A.C.P., on inse`re la meÂ´thode des compo-
santes principales appeleÂ´e â€Copriâ€ (figure 17).
Le logiciel reÂ´alise alors lâ€™analyse des points-variables (voir lâ€™ouvrage de Lebart et al., 1997)
et propose comme reÂ´sultats les coordonneÂ´es des variables sur les composantes calculeÂ´es.
Cette matrice est eÂ´gale a` la matrice des correÂ´lations variable-facteur, il sâ€™agit de lâ€™esti-
mation de la matrice de saturation, noteÂ´e AË†q (figure 18). Sur cette figure, on retrouve
eÂ´galement la matrice des â€anciens axes unitairesâ€ qui correspond en fait a` la matrice Vq
des vecteurs propres de R.
Pour obtenir la matrice des composantes principales Î¨q (figure 19) , il faut demander a`
SPAD, lors du parameÂ´trage de la filie`re, dâ€™afficher les reÂ´sultats pour les individus.
cÂ© Revue MODULAD, 2007 -24- NumeÂ´ro 37
Fig. 17 â€“ Filie`re SPAD
Fig. 18 â€“ Matrice AË†q de saturation
Fig. 19 â€“ Matrice Î¨q des composantes principales
5.5 Tableau reÂ´capitulatif des diffeÂ´rents reÂ´sultats
Le tableau ci-dessous reÂ´capitule un certain nombre de reÂ´sultats et permet de faire le
lien entre les facteurs ou composantes, obtenus avec les trois logiciels. Pour faciliter la
cÂ© Revue MODULAD, 2007 -25- NumeÂ´ro 37
lecture, les matrices ne sont pas indiceÂ´es par q.
Tab. 3 â€“ Tableau comparatif des diffeÂ´rents reÂ´sultats
SAS SAS SPAD SPSS
Proc princomp Proc factor
(estimation par A.C.P.)
MeÂ´thode factorielle A.C.P. A.F. A.C.P. A.F.
Estimateur de la variance non biaiseÂ´ non biaiseÂ´ biaiseÂ´ non biaiseÂ´
Facteurs/composantes Î¨sas = XËœV Fsas = XËœV Î›
âˆ’1 Î¨spad = XËœV Fspss = XËœV Î›âˆ’1
Norme au carreÂ´ des facteurs (nâˆ’ 1)Î»Î± nâˆ’ 1 nÎ»Î± nâˆ’ 1
Variance des facteurs Î»Î± 1 Î»Î± 1
Matrice de saturation ? non fournie AË† = V Î› AË† = V Î› AË† = V Î›
Nom donneÂ´ par le logiciel â€factor patternâ€ â€correÂ´lations variables facteursâ€ â€matrice des composantesâ€
Rotation possible ? non oui non oui
Lien ÏˆÎ±sas f
Î±
sas =
ÏˆÎ±sasâˆš
Î»Î±
ÏˆÎ±spad =
q
n
nâˆ’1Ïˆ
Î±
sas f
Î±
spss = f
Î±
sas
Pour mettre en place un mode`le dâ€™A.F., on peut donc utiliser la proceÂ´dure factor de
SAS ou le logiciel SPSS.
Si on utilise la proceÂ´dure princomp de SAS ou le logiciel SPAD, la meÂ´thode reÂ´aliseÂ´e est
une A.C.P. et il faut standardiser les composantes principales pour obtenir lâ€™estimation
des facteurs communs. Ceci peut se faire facilement sous le logiciel SAS, en speÂ´cifiant
lâ€™option â€standardâ€ dans le code de la proceÂ´dure Princomp (figure 2). On obtient alors
la matrice des composantes principales standardiseÂ´es preÂ´senteÂ´e dans la figure (20), qui
correspond bien a` la matrice FË†q de la proceÂ´dure factor (figure 7).
Fig. 20 â€“ Matrice Î¨q des composantes principales standardiseÂ´es
De plus, la proceÂ´dure princomp de SAS ne fournit pas lâ€™estimation de la matrice de
saturation. Il faut la calculer : AË†q = VqÎ›q. Cependant, si elle contient des valeurs tre`s
disperseÂ´es entre 0 et 1, il sera difficile dâ€™associer des variables entre elles. Ce proble`me
peut eÂ´galement se rencontrer avec le logiciel SPAD qui ne propose de rotation.
On peut cependant souligner deux avantages du logiciel SPAD par rapport a` SAS et
SPSS : lâ€™interactiviteÂ´ et la possibiliteÂ´ de reÂ´aliser facilement des graphiques.
Ainsi, on voit lâ€™avantage dâ€™utiliser la proceÂ´dure factor de SAS ou le logiciel SPSS pour
estimer le mode`le dâ€™A.F., car ils fournissent lâ€™estimation de la matrice Aq de saturation
et proposent une rotation des facteurs.
cÂ© Revue MODULAD, 2007 -26- NumeÂ´ro 37
6 Conclusion
Le mode`le dâ€™A.F. est une meÂ´thode factorielle lineÂ´aire. Cette technique eÂ´crit un en-
semble de p variables aleÂ´atoires comme une combinaison lineÂ´aire de q facteurs non correÂ´leÂ´s,
communs a` toutes les variables, et de p facteurs speÂ´cifiques a` chaque variable. Lâ€™ensemble
de ces facteurs communs et uniques reproduit les covariances des variables aleÂ´atoires
initiales. Ainsi, le mode`le dâ€™A.F. permet de reÂ´sumer au â€mieuxâ€ lâ€™information contenue
dans p variables aleÂ´atoires, ou de deÂ´tecter des facteurs sous-jacents communs dans une
probleÂ´matique particulie`re. Comme toute meÂ´thode factorielle, le point strateÂ´gique du
mode`le dâ€™A.F. reÂ´side dans le choix du nombre q de facteurs communs, difficulteÂ´ a` laquelle
nous avons souhaiteÂ´ apporter une aide. Cet aide nâ€™est que partielle car nous avons vu
que seule une interpreÂ´tation attentive des reÂ´sultats et des objectifs de lâ€™eÂ´tude permet de
reÂ´pondre au proble`me du choix de q.
Apre`s une preÂ´sentation syntheÂ´tique du mode`le dâ€™A.F., nous avons deÂ´crit les techniques
dâ€™estimation et nous avons vu que lorsquâ€™on estime le mode`le dâ€™A.F. via les composantes
principales, cela revient a` faire une A.C.P.
Lâ€™accent a ensuite eÂ´teÂ´ mis sur les techniques de rotation des facteurs, qui peuvent sâ€™aveÂ´rer
tre`s utiles. Nous avons montreÂ´ que, contrairement a` ce quâ€™on peut lire dans certains tra-
vaux, la rotation en A.C.P. est eÂ´galement possible a` condition dâ€™effectuer convenablement
la transformation.
Une application numeÂ´rique a ensuite eÂ´teÂ´ mise en place sur des donneÂ´es concernant la
criminaliteÂ´ de villes ameÂ´ricaines. Ainsi, lâ€™estimation du mode`le dâ€™A.F. coupleÂ´e a` une ro-
tation de type Varimax nous a permis de reÂ´sumer la criminaliteÂ´ des villes ameÂ´ricaines a`
lâ€™aide de trois facteurs communs : les crimes violents contre la personne, les crimes en
rapport avec la maison et les crimes commis a` lâ€™exteÂ´rieur. De plus, cette application a
permis de clarifier le vocabulaire utiliseÂ´ par les logiciels SAS, SPAD et SPSS, reÂ´elle source
de confusion. Lâ€™exemple accompagneÂ´ de nombreuses illustrations pourra servir de guide,
tant pour lâ€™impleÂ´mentation que pour la lecture des reÂ´sultats numeÂ´riques.
7 Annexes
7.1 Annexe 1 : Etude des proprieÂ´teÂ´s des facteurs communs es-
timeÂ´s par la meÂ´thode des composantes principales
â€¢ Par construction, les facteurs communs estimeÂ´s sont centreÂ´s.
â€¢ Lâ€™hypothe`se (H1) est veÂ´rifieÂ´e car FË† â€²qMFË†q = U â€²qMâˆ’1Uq = Iq.
â€¢ Lâ€™hypothe`se (H2) nâ€™est pas neÂ´cessairement veÂ´rifieÂ´e. En effet, cette meÂ´thode dâ€™estimation
de la matrice Î ne garantit pas que ÎË† = EË†qEË†
â€²
q soit diagonale. Cependant, en pratique,
les termes en dehors de la diagonale de la matrice ÎË† sont souvent neÂ´gligeables. Ainsi, la
solution trouveÂ´e avec cette meÂ´thode est souvent une approximation convenable, ce qui
explique que cette meÂ´thode dâ€™estimation du mode`le dâ€™A.F. est tre`s utiliseÂ´e. On pourrait
preÂ´coniser a` lâ€™utilisateur dâ€™examiner attentivement la matrice EË†qEË†
â€²
q et de recommencer
cÂ© Revue MODULAD, 2007 -27- NumeÂ´ro 37
lâ€™estimation du mode`le avec une autre meÂ´thode si les valeurs en dehors de la diagonale de
cette matrice sont trop grandes.
â€¢ Lâ€™hypothe`se (H3) est veÂ´rifieÂ´e. En effet, en ne retenant que les vecteurs propres associeÂ´s
aux q plus grandes valeurs propres, on a :
XËœ =Mâˆ’1Uqï¸¸ ï¸·ï¸· ï¸¸
FË†q
Î›qV
â€²
qï¸¸ ï¸·ï¸· ï¸¸
AË†â€²q
+EË†q
ou` EË†q = M
âˆ’1UeÎ›eV â€²e , avec Ue, Î›e et Ve les matrices contenant respectivement les r âˆ’ q
dernie`res colonnes de U , Î› et V .
On a alors :
EË† â€²qMFË†q = EË†
â€²
qUq
= VeÎ›eU
â€²
eM
âˆ’1Uq
= 0 (47)
car la matrice U est orthonormeÂ´e.
â€¢ On peut veÂ´rifier que les valeurs de la matrice AË†q, noteÂ´s aË†Î±j sont les correÂ´lations empi-
riques entre les variables xj et les facteurs fÎ± :
aË†Î±j =
nâˆ‘
i=1
zji u
Î±
i
=
nâˆ‘
i=1
zji
1âˆš
m
fÎ±i
=
nâˆ‘
i=1
(
xji âˆ’ xjâˆš
msj
)(
fÎ±i âˆ’ f
Î±
âˆš
m
âˆš
var(fÎ±)
)
= corr(xj, fÎ±) (48)
7.2 Annexe 2 : DeÂ´monstration de la proprieÂ´teÂ´ des facteurs et
des â€loadingsâ€ apre`s rotation
â€¢ Les facteurs apre`s rotation sont toujours centreÂ´s.
â€¢ Lâ€™hypothe`se (H1) est veÂ´rifieÂ´e car GË†â€²qMGË†q = T â€²FË† â€²qMFË†qT = Iq.
â€¢ Lâ€™hypothe`se (H2) est veÂ´rifieÂ´e car la matrice des erreurs EË†q nâ€™est pas modifieÂ´e.
â€¢ Lâ€™hypothe`se (H3) est veÂ´rifieÂ´e car EË† â€²qMGË†q = EË† â€²qMFË†qT = 0.
â€¢ Les â€loadingsâ€ apre`s rotation, noteÂ´s bË†Î±j , repreÂ´sentent les correÂ´lations des variables xj
aux facteurs gÎ± apre`s la rotation.
cÂ© Revue MODULAD, 2007 -28- NumeÂ´ro 37
On a : BË†q = AË†qT = Z
â€²(UqT ) = Z â€²UË˜q ou` UË˜q = UqT .
De plus, on a :
gË†Î± =
âˆš
muË˜Î±
ou` gË†Î± est la Î±e`me colonne de la matrice GË†q, et uË˜
Î± est la Î±e`me colonne de la matrice UË˜q.
On en deÂ´duit :
bË†Î±j =
nâˆ‘
i=1
zji uË˜
Î±
i
=
nâˆ‘
i=1
zji
1âˆš
m
gÎ±i
=
nâˆ‘
i=1
(
xji âˆ’ xjâˆš
msj
)(
gÎ±i âˆ’ gÎ±âˆš
m
âˆš
var(gÎ±)
)
= corr(xj, gÎ±). (49)
â€¢ La matrice de saturation apre`s la rotation reproduit toujours le mode`le de structure
de covariance deÂ´fini par (8) :
BË†qBË†
â€²
q + ÎË† = AË†qTT
â€²AË†â€²q + ÎË† = AË†qAË†
â€²
q + ÎË† = R. (50)
â€¢ Les communaliteÂ´s sont inchangeÂ´es :
hË†2j =
qâˆ‘
Î±=1
(bË†Î±j )
2 =
qâˆ‘
Î±=1
(aË†Î±j )
2 (51)
car BË†qBË†
â€²
q = (AË†qT )(AË†qT )
â€² = AË†qAË†â€²q.
â€¢ La variance totale expliqueÂ´e par les q facteurs communs nâ€™est pas modifieÂ´e :
pâˆ‘
j=1
qâˆ‘
Î±=1
(bË†Î±j )
2 =
pâˆ‘
i=1
qâˆ‘
Î±=1
(aË†Î±j )
2. (52)
ReÂ´feÂ´rences
[1] Baccini A., Besse P. (2005), â€Data mining I, Exploration Statistiqueâ€
http ://www.lsp.ups-tlse.fr/Besse/pub/Explo stat.pdf.
[2] Bouveyron C., (2006),ModeÂ´lisation et classification des donneÂ´es de grande dimension -
Application a` lâ€™analyse dâ€™images, p 45-47, The`se, UniversiteÂ´ Joseph Fourier - Grenoble
1.
[3] Fine J. (1993), â€Proble`mes dâ€™indeÂ´termination en analyse en facteurs et analyse en
composantes principales optimaleâ€, Revue de Statistique AppliqueÂ´e, tome 41, nËš 4, p
45-72.
[4] Garnett J.-C.(1919), â€General ability, cleverness and purposeâ€, British Journal of
Psychiatry, 9, p 345-366.
cÂ© Revue MODULAD, 2007 -29- NumeÂ´ro 37
[5] Harman H. H. (1960), Modern Factor Analysis, University of Chicago Press.
[6] Heywood H.B. (1931), â€On finite sequences of real numbersâ€, Proceedings of the Royal
Society, Series A, 134, p 486-501.
[7] Hotelling H. (1933), â€Analysis of a complex of statistical variables into principal com-
ponentsâ€, Journal of Educational Psychology, 24, p 417-441.
[8] Jobson J.D. (1992), Applied Multivariate Data Analysis, Volume II : Categorical and
Multivariate Methods, Springer-Verlag.
[9] Lawley D.N., Maxwell A.E. (1963), Factor Analysis as a statistical method, Butter-
worths London.
[10] Lebart L., Morineau A., Piron M. (1997), Statistique exploratoire multidimension-
nelle, 2e cycle, 2e eÂ´dition, Editions Dunod.
[11] Pearson K. (1901), â€On lines and planes of closest fit to systems of points in spaceâ€,
Philosophical Magazine, 2, p 559-572.
[12] Rencher, A.C. (2002),Methods of Multivariate Analysis, Second Edition, Wiley Series
in Probability and Statistics.
[13] Seber G.A.F. (1984), Multivariate observations, Wiley Series in Probability and Ma-
thematical Statistics.
[14] Spearman C. (1904), â€General intelligence, objectively determined and measuredâ€,
American Journal of Psychology, 15, p 201-293.
[15] Tipping M.E, Bishop C.M. (1999), â€Probabilistic Principal Component Analysisâ€,
Journal of the Royal Statistical Society, Series B, 61, Part 3, p 611-622.
cÂ© Revue MODULAD, 2007 -30- NumeÂ´ro 37
