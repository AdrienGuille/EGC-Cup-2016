Analyse de la Vraisemblance des Liens Relationnels : Une
mÃ©thodologie dâ€™analyse classificatoire des donnÃ©es
IsraÃ«l-CÃ©sar Lerman
UniversitÃ© de Rennes 1-IRISA Projet Symbiose,
Campus Universitaire de Beaulieu, 35042 Rennes Cedex
lerman@irisa.fr
http://www.irisa.fr/symbiose/
RÃ©sumÃ©. La mÃ©thodologie de classification des donnÃ©es par lâ€™Analyse de la
Vraisemblance des Liens Relationnels a pris naissance vers la fin des annÃ©es
soixante. Elle sâ€™est trÃ¨s largement dÃ©veloppÃ©e. De nombreux chercheurs et pra-
ticiens ont pris part Ã  son dÃ©veloppement. De nombreuses applications dâ€™en-
vergure provenant des domaines les plus divers (Bioinformatique, Informatique,
Sciences sociales, Traitement dâ€™images, Traitement de langues naturelles, ...) ont
validÃ© cette mÃ©thodologie. Elle sâ€™adresse Ã  nâ€™importe quel type mathÃ©matico-
logique du tableau de description des donnÃ©es. Lâ€™objet de notre article est de
prÃ©senter de faÃ§on illustrÃ©e les principes fondamentaux de cette approche. Ces
principes se situent dâ€™une part au niveau de la reprÃ©sentation de la description et
dâ€™autre part, au niveau de lâ€™Ã©valuation quantifiÃ©e des ressemblances entre les
structures mathÃ©matiques Ã  comparer. Dans notre cas la reprÃ©sentation de la
description sera ensembliste et relationnelle et la ressemblance sera Ã©valuÃ©e au
moyen dâ€™une similaritÃ© qui se rÃ©fÃ¨re Ã  une Ã©chelle de probabilitÃ© Ã©tablie par rap-
port Ã  une hypothÃ¨se statistique dâ€™absence de liaison. Le texte ci-dessous se veut
le reflet de ma prÃ©sentation orale aux â€œ3-Ã¨mes JournÃ©es ThÃ©matiques Apprentis-
sage Artificiel et Fouille des DonnÃ©esâ€, 8-9 avril 2008.
1 PrÃ©ambule
Mon texte sâ€™apparente Ã  une introduction. Il se veut le reflet exact de ma prÃ©sentation orale
aux â€œ3-Ã¨mes JournÃ©es ThÃ©matiques Apprentissage Artificiel et Fouille des DonnÃ©esâ€, 8-9 avril
2008. Ce texte ne correspond pas Ã  la forme classique dâ€™un article.
Il sâ€™agit pour nous de prÃ©senter une mÃ©thodologie dâ€™analyse classificatoire des donnÃ©es sur
laquelle nous avons travaillÃ© de trÃ¨s longues annÃ©es. De nombreuses collaborations ont contri-
buÃ© de faÃ§on importante Ã  sa mise au point. Cette mÃ©thode nâ€™est pas vraiment connue dans ses
diffÃ©rentes facettes ; alors quâ€™elle repose sur deux principes fondamentaux. Le premier consiste
en la reprÃ©sentation ensembliste et relationnelle des variables (attributs) de description de lâ€™en-
semble des objets ou individus et le second, en lâ€™Ã©valuation probabiliste de la similaritÃ© entre
les structures mathÃ©matiques Ã  comparer (Indices de la vraisemblance des liens). Ces prin-
cipes sont concrÃ©tisÃ©s quelle que soit la complexitÃ© de la description des donnÃ©es. Il peut sâ€™agir
Classification par la vraisemblance des liens relationnels
dâ€™associer des descripteurs (resp., des objets), des classes de descripteurs (resp., des classes
dâ€™objets), il peut aussi sâ€™agir de reconnaÃ®tre des agrÃ©gats â€œintÃ©ressantsâ€ qui se distinguent, ...
Dans ma prÃ©sentation, je chercherai autant que possible Ã  insister sur les principes en mâ€™ap-
puyant sur des exemples.
Il est fondamental dâ€™avoir Ã  lâ€™esprit quâ€™une mÃ©thodologie dâ€™Analyse des DonnÃ©es se conÃ§oit
toujours par rapport Ã  une algorithmique dâ€™un type donnÃ©. Ã€ cet Ã©gard, les concepteurs de mÃ©-
thodes de classification ont vite tendance Ã  vouloir faire oublier lâ€™algorithmique qui a servi
de nid de fÃ©condation aux mÃ©thodes construites pour parler de â€œthÃ©orie gÃ©nÃ©raleâ€. Certes, il
est trÃ¨s important et trÃ¨s intÃ©ressant de vouloir transposer les acquis mÃ©thodologiques dans le
cadre dâ€™autres algorithmiques ; et dâ€™ailleurs, câ€™est une richesse de lâ€™approche initialement pro-
posÃ©e. Dans notre cas lâ€™algorithmique en question est fournie par la Classification Ascendante
HiÃ©rarchique, la CAH, comme on dit.
Il sera donc juste de rappeler le principe et les ingrÃ©dients de cette algorithmique pour
montrer comment ils se dÃ©clinent dans le cadre de lâ€™Analyse de la Vraisemblance des Liens
Relationnels AV LR. Le plan de notre prÃ©sentation est :
1. PrÃ©ambule
2. Introduction : principes gÃ©nÃ©raux et exemples
3. Le principe algorithmique et les Ã©lÃ©ments constitutifs dâ€™une CAH
4. La mÃ©thode de lâ€™Analyse de la Vraisemblance des Liens Relationnels (Une prÃ©sentation
Ã  partir dâ€™exemples)
5. Les grands types logico-mathÃ©matiques de donnÃ©es
6. Logiciels et Applications
7. RÃ©fÃ©rences
2 Introduction : principes gÃ©nÃ©raux et exemples
2.1 Le paradigme de la CAH, schÃ©ma dâ€™arbre et exemples
Le paradigme de la CAH remonte Ã  loin ...Michel Adanson, dans son Histoire naturelle du
SÃ©nÃ©gal (Bauche, Paris, 1757) Ã©crivait :
â€œJe me contenterai de rapprocher les objets suivant le plus grand nombre de degrÃ©s de leurs
rapports et de leurs ressemblances ... Les objets ainsi rÃ©unis formeront plusieurs petites fa-
milles que je rÃ©unirai encore afin dâ€™en faire un tout dont les parties soient unies et liÃ©es inti-
mement.â€
Je pourrais peut-Ãªtre ajouter : â€œliÃ©es intimementâ€ sur le plan de la cohÃ©rence.
Le schÃ©ma dâ€™arbre, tel que celui prÃ©sentÃ© ci-dessous (voir FIG. 1) sur un ensemble E de 10
Ã©lÃ©ments, donne prÃ©cisÃ©ment une image de la suite des rapprochements telle que dÃ©crite par
Michel Adanson ; en â€œpetitesâ€ familles puis ces derniÃ¨res en de plus â€œgrossesâ€. Le processus
de rapprochement est parfaitement conforme Ã  la construction ascendante hiÃ©rarchique dâ€™un
RNTI - G - 2
I.C. Lerman
arbre des classifications. Le niveau 0 est celui des feuilles de lâ€™arbre. Chaque feuille reprÃ©sente
- jâ€™allais dire un Ã©lÃ©ment - plus exactement, une partie comprenant exactement un Ã©lÃ©ment.
Entre les niveaux 0 et 1 on a les rapprochements suivants : {g} et {h}, {f} et {i} et {c} et {e}.
Entre les niveaux 1 et 2, on a dâ€™une part le rapprochement entre {b} et {g, h} et dâ€™autre part,
celui entre {a}, {j} et {c, e}. Ce dernier rapprochement donne lieu au noeud {a, c, e, j}. La
racine de lâ€™arbre correspond Ã  lâ€™ensemble plein E. AprÃ¨s une Ã©tape donnÃ©e on peut distinguer
la notion de classification (partition) produite Ã  un niveau donnÃ©. Ainsi, en est-il de la partition
{{b, g, h}, {d}, {f, i}, {a, c, e, j}} quâ€™on peut noter pi2, produite au niveau 2 de lâ€™arbre.
.    .    .    .    .    .    .    .    .    .
.       .      .       .      .       .      .
.          .       .              .
.              .                  .
.
{b} {g}{h}{d}{f}{i} {a}{c} {e}{j}
E = {a, b, c, d, e, f, g, h, i, j}
E = {a, b, c, d, e, f, g, h, i, j}
0
1
2
3
4
{a, c, e, j}
FIG. 1 â€“ Un exemple dâ€™arbre de classification.
Rentrons maintenant dans le vif du sujet et considÃ©rons une illustration rÃ©elle issue dâ€™un
travail de thÃ¨se citÃ©e dans les rÃ©fÃ©rences (M. Ouali-Allah). Il sâ€™agit dâ€™une partie dâ€™une enquÃªte
dâ€™opinion rÃ©alisÃ©e en 1989 par lâ€™institut dâ€™AgoramÃ©trie qui dÃ©pendait du consortium CEA-
EDF. Lâ€™objet de lâ€™enquÃªte globale portait sur les conflits qui pouvaient alors agiter lâ€™opinion
publique. Pour la partie concernÃ©e de lâ€™enquÃªte oÃ¹ un Ã©chantillon de 500 individus ont Ã©tÃ©
intÃ©rrogÃ©s, 19 questions, chacune associÃ©e Ã  un homme politique, de la forme suivante, ont Ã©tÃ©
introduites :
â€œSouhaitez-vous voir jouer un rÃ´le important Ã  tel homme politiqueâ€
Les codes des rÃ©sultats Ã  la question posÃ©e sont :
1. OUI
2. NON
3. SANS RÃ‰PONSE
Chaque question dÃ©finit ainsi une variable qualitative Ã  3 valeurs (on dit encore modalitÃ©s
ou catÃ©gories). Le plus classique consiste Ã  ne considÃ©rer aucune structure sur lâ€™ensemble des
RNTI - G - 3
Classification par la vraisemblance des liens relationnels
R. BARRE
S. VEIL
B. LALONDE
A. WAECHTER
L. FABIUS
L. JOSPIN
J. LANG
J. M. LEPEN
G. MARCHAIS
F. MITTERAND
M. ROCCARD
P. BEREGOVOY
J. DELORS
P. MEHAIGNERIE
M. SEGUIN
J. CHIRAC
V.G. Dâ€™ESTAING
F. LEOTARD
M. NOIR
6
3
1
7
11
16
 
 
12
5
2
13
*
4
10*
9
8
14
15
17*
18
FIG. 2 â€“ Un premier arbre organisant lâ€™opinion politique dans la France de 1989.
valeurs de la variable. Il sâ€™agit dans ce cas dâ€™une variable qualitative nominale. Une interprÃ©ta-
tion plus riche rÃ©sulte dâ€™une similaritÃ© ordinale, pouvant Ãªtre posÃ©e par lâ€™expert, sur lâ€™ensemble
des valeurs de la variable. Il sâ€™agira alors dâ€™une variable qualitative prÃ©ordonnance. Nous avons
retenu le prÃ©ordre total suivant sur lâ€™ensemble des paires de valeurs :
{1, 2} < {2, 3} < {1, 3} < {1, 1} âˆ¼ {2, 2} âˆ¼ {3, 3} (1)
oÃ¹ les catÃ©gories â€œOUIâ€ et â€œNONâ€ sont les plus dissemblables et oÃ¹, la ressemblance entre
une catÃ©gorie et elle mÃªme est maximale quelle que soit la catÃ©gorie. On comprendra Ã©gale-
ment la position des autres paires de catÃ©gories.
Câ€™est la mÃªme mÃ©thode, celle prÃ©cisÃ©ment de lâ€™AV LR qui a Ã©tÃ© appliquÃ©e pour chacun
des deux codages. Dans le premier rÃ©sultat, oÃ¹ le codage classique a Ã©tÃ© utilisÃ©, la droite et la
gauche politiques ont certes une certaine influence dans les regroupements ; mais, elles ne se
sÃ©parent pas vraiment. La tendance qui domine dans les associations ou sÃ©parations est celle
dâ€™une certaine rigueur, voire de sectarisme, face Ã  une certaine ouverture, autour du patrio-
tisme allant jusquâ€™au nationalisme, dâ€™attachement Ã  la terre, ... . Ainsi voit-on des associa-
tions telles que â€œLepen-Marchaisâ€, â€œMitterand-Rocardâ€, â€œMÃ©haignerie-SÃ©guinâ€, â€œBeregovoy-
Delorsâ€, â€œVeil-Barreâ€, â€œLalonde-Waechterâ€, ... .
ConsidÃ©rons Ã  prÃ©sent le second rÃ©sultat oÃ¹ le codage en termes de prÃ©ordonnance a Ã©tÃ© uti-
lisÃ©. Dans ce cas la droite et la gauche se sÃ©parent clairement. Dâ€™autre part, au sein de chacune
RNTI - G - 4
I.C. Lerman
R. BARRE
S. VEIL
J. CHIRAC
V.G. Dâ€™ESTAING
F. LEOTARD
M. NOIR
P. MEHAIGNERIE
M. SEGUIN
P. BEREGOVOY
J. DELORS
F. MITTERAND
M. ROCCARD
L. FABIUS
L. JOSPIN
J. LANG
B. LALONDE
A. WAECHTER
J.M. LEPEN
G. MARCHAIS
3
5
12
1
10   *
15*
8
2
13
*
6
4
11
16
18
17
9
7
14
FIG. 3 â€“ Un deuxiÃ¨me arbre organisant lâ€™opinion politique dans la France de 1989.
de ces deux formations politiques on distingue la vieille et traditionnelle gÃ©nÃ©ration et celle
montante. Ainsi, pour la droite, la gÃ©nÃ©ration la plus classique est reprÃ©sentÃ©e par {J.Chirac,
V.G. Dâ€™estaing, R. Barre, S. Veil} ; celle plus jeune et avec un accent nouveau, par {F. LÃ©otard,
M. Noir, M. SÃ©guin, P. MÃ©haignerie}. Pour la gauche, la gÃ©nÃ©ration classique est reprÃ©sentÃ©e
par {F. Mitterand, M. Roccard, P. Beregovoy, J. Delors} et celle, jeune et montante par {L.
Fabius, L. Jospin, J. Lang}. Une derniÃ¨re classe {J.M. Lepen, G. Marchais, B. Lalonde, J.
Waechter} regroupe les aspects les plus extrÃªmes du paysage politique franÃ§ais oÃ¹ le nationa-
lisme et lâ€™attachement Ã  la terre sont prÃ©sents.
La comparaison de ces deux rÃ©sultats illustre toute lâ€™importance de la prise en compte dâ€™une
structure relationnelle au niveau de lâ€™ensemble des valeurs dâ€™une mÃªme variable descriptive.
2.2 Le tableau des donnÃ©es et son analyse par la classification
2.2.1 Le tableau des donnÃ©es
Dans la trÃ¨s grande majoritÃ© des cas, mais pas toujours, les donnÃ©es peuvent Ãªtre reprÃ©sen-
tÃ©es au moyen dâ€™un tableau (voir FIG. 4).
La description peut concerner un ensemble O dâ€™objets Ã©lÃ©mentaires ou bien, un ensemble
C de catÃ©gories (on dit encore â€œconceptsâ€). Les colonnes du tableau des donnÃ©es sont indexÃ©es
par lâ€™ensemble A des attributs (variables) de description. En supposant que le nombre dâ€™attri-
buts est p et que le nombre dâ€™objets (resp., catÃ©gories) est n, on peut noter :
RNTI - G - 5
Classification par la vraisemblance des liens relationnels
aj
oi/ci
A
O
C
aj(oi/ci)
Valeur observÃ©e ou rÃ©sultat 
dâ€™une connaissance dâ€™expert
FIG. 4 â€“ Le tableau des donnÃ©es.
A = {aj |1 â‰¤ j â‰¤ p}
O = {oi|1 â‰¤ i â‰¤ n} , C = {ci|1 â‰¤ i â‰¤ n}
Si la description concerne un ensemble O dâ€™objets (resp., un ensemble C de catÃ©gories),
la i-Ã¨me ligne du tableau est indexÃ©e par lâ€™objet Ã©lÃ©mentaire oi (resp., par la catÃ©gorie ci),
1 â‰¤ i â‰¤ n. De toute faÃ§on, la j-Ã¨me colonne du tableau est indexÃ©e par lâ€™attribut aj , 1 â‰¤ j â‰¤ p.
Dâ€™autre part, quel que soit lâ€™ensemble dÃ©crit (un ensemble dâ€™objets ou un ensemble de catÃ©go-
ries), Ã  lâ€™intersection de la i-Ã¨me ligne et de la j-Ã¨me colonne se trouve une valeur observÃ©e ou
une connaissance dâ€™expert de lâ€™attribut aj sur lâ€™objet oi sâ€™il sâ€™agit dâ€™une description dâ€™objets,
ou sur la catÃ©gorie ci, sâ€™il sâ€™agit dâ€™une description de catÃ©gories.
Dans le cas des donnÃ©es ci-dessus, il sâ€™agit de la description dâ€™un ensemble O dâ€™objets
Ã©lÃ©mentaires qui est dÃ©fini par lâ€™Ã©chantillon formÃ© des 500 individus. Dâ€™autre part, comme cela
a dÃ©jÃ  Ã©tÃ© exprimÃ©, on dispose de 19 variables qualitatives. La valeur aj(oi) est reprÃ©sentÃ©e
par le code de la rÃ©ponse du i-Ã¨me individu Ã  la j-Ã¨me question, 1 â‰¤ i â‰¤ 500, 1 â‰¤ j â‰¤ 19.
Comme nous lâ€™avons vu, ce code est 1, 2 ou 3.
Il est important de noter que le tableau des donnÃ©es ne peut en aucun cas contenir lâ€™in-
formation concernant la structure relationnelle dont se trouve munie lâ€™ensemble des valeurs
dâ€™une mÃªme variable descriptive. Ainsi dans lâ€™exemple ci-dessus, on prÃ©cise la structure prÃ©-
ordonnance qui est supposÃ©e sur lâ€™ensemble des valeurs dâ€™une variable qualitative prÃ©ordon-
nance donnÃ©e. Dâ€™ailleurs, mÃªme dans le cas dâ€™une variable qualitative nominale, le calcul tient
compte de lâ€™absence de structure sur lâ€™ensemble des modalitÃ©s de la variable.
Les donnÃ©es de connaissance qui sont parfois appelÃ©es â€œsymboliquesâ€ peuvent parfaite-
ment rentrer dans le moule prÃ©cÃ©dent et Ãªtre traitÃ©es efficacement par lâ€™AV LR. Dâ€™ailleurs, nous
prÃ©senterons en section 5 notre expression formelle des grands types logico-mathÃ©matiques de
donnÃ©es.
RNTI - G - 6
I.C. Lerman
2.2.2 Analyse et rÃ©organisation par la classification hiÃ©rarchique du tableau des don-
nÃ©es
Les classifications proposÃ©es ci-dessus sont des classifications hiÃ©rarchiques dâ€™ensembles
de variables. Il est clair que - pour chacun des deux codages proposÃ©s ci-dessus (qualitatif
nominal et qualitatif prÃ©ordonnance) - nous aurions pu poursuivre par une classification de
lâ€™ensemble des 500 individus. Peut-Ãªtre que nous pouvons Ã©voquer ici la rÃ©alisation de ce type
de classification dans le cas dâ€™un codage pouvant Ãªtre apparentÃ© Ã  celui â€œprÃ©ordonnanceâ€.
Il sâ€™agissait dâ€™un ensemble dâ€™objets dÃ©finis par des sÃ©quences protÃ©iques. Une sÃ©quence pro-
tÃ©ique est une suite dâ€™acides aminÃ©s et peut Ãªtre reprÃ©sentÃ©e comme un mot dans un alphabet Ã 
20 lettres. La description relationnelle suppose prÃ©cisÃ©ment un graphe valuÃ© complet sur lâ€™en-
semble des 20 lettres.
Une Ã©tape ultime - nous y reviendrons - consistera Ã  situer des classes de variables par
rapport Ã  des classes dâ€™objets. En effet, la rÃ©organisation des lignes et des colonnes dâ€™un ta-
bleau de donnÃ©es conformÃ©ment Ã  un couple dâ€™arbres de classification (issus de la CAH), le
premier sur lâ€™ensemble des attributs descriptifs et le second sur lâ€™ensemble des objets, est trÃ¨s
riche dâ€™enseignement sur le plan de lâ€™interprÃ©tation des tendances comportementales. Consi-
dÃ©rons Ã  cet Ã©gard une illustration schÃ©matique trÃ¨s simplifiÃ©e oÃ¹ on considÃ¨re un trÃ¨s petit
tableau de donnÃ©es de prÃ©sence-absence, dÃ©crivant un ensemble de 5 objets au moyen se 4 at-
tributs boolÃ©ens (voir FIG. 5). Le croisement des deux arbres de classification (sur lâ€™ensemble
A des attributs de description et sur lâ€™ensembleO des objets dÃ©crits) donne la partie droite de la
figure, oÃ¹ une case noircie indique que lâ€™attribut est Ã  V RAI . La reconnaissance de niveaux in-
tÃ©ressants (nous disons â€œsignificatifsâ€) sur chacun des deux arbres permet de situer des classes
cohÃ©rentes dâ€™objets par rapport Ã  des classes cohÃ©rentes de variables. Nous avons repÃ©rÃ© sur
chacun des deux arbres un niveau â€œsignificatifâ€. Il sâ€™agit du deuxiÃ¨me (resp., troisiÃ¨me) niveau
pour lâ€™arbre de classification sur A (resp., O). Ces niveaux sont indiquÃ©s sur la partie droite
de la figure. La partition dÃ©terminÃ©e sur lâ€™ensemble A est pi = {{a1, a2}, {a3, a4}}, celle dÃ©-
terminÃ©e sur lâ€™ensemble O est Ï‡ = {{o1, o2, o4}, {o3, o5}}. Si la classe {o1, o2, o4} est en
correspondance avec la classe {a1, a2}, celle {o3, o5}, lâ€™est avec la classe {a3, a4}. Pour ce
qui est de la premiÃ¨re correspondance on constatera que lâ€™association la plus forte est relative Ã 
la sous classe {o1, o2}. Pour ce qui est de la deuxiÃ¨me correspondance, on peut aussi constater
que lâ€™association la plus forte concerne la sous-classe {o3}.
Il y a bien sÃ»r de nombreuses mÃ©thodes de classification conjointe des lignes et des co-
lonnes dâ€™un tableau de donnÃ©es boolÃ©ennes ou numÃ©riques, oÃ¹ on cherche Ã  optimiser au mieux
un critÃ¨re objectif. Cependant, il faut Ãªtre conscient que la philosophie qui prÃ©side au croise-
ment de deux arbres condensÃ©s Ã  leurs niveaux les plus â€œsignificatifsâ€ nâ€™est pas vraiment la
mÃªme. Deux questions importantes de complexitÃ© subsistent : la premiÃ¨re de nature statistique
et la seconde de nature formelle. Pour la premiÃ¨re, il sâ€™agit de savoir comment gÃ©rer un tel
croisement en cas dâ€™un â€œgrosâ€ tableau de donnÃ©es. Dâ€™autre part, comment gÃ©rer le cas oÃ¹ les
variables descriptives sont complexes ; par exemple le cas oÃ¹ lâ€™ensemble des valeurs dâ€™une
mÃªme variable est muni dâ€™une structure relationnelle.
RNTI - G - 7
Classification par la vraisemblance des liens relationnels
1 1 0 0
1 1 0 0
0 0 1 1
1 0 0 0
0 0 1 0
A
O
o1
o2
o3
o4
o5
a
1
a
2
a
3
a
4
A
O
o1
o2
o3
o4
o5
a
1
a
2
a
3
a
4
FIG. 5 â€“ La source (le tableau des donnÃ©es) et le but (deux arbres de classification croisÃ©s).
3 Le principe algorithmique et les Ã©lÃ©ments dâ€™une CAH
3.1 Une expression formalisÃ©e de la dÃ©marche dâ€™Adanson
La donnÃ©e est un couple de la forme (E, Î´) oÃ¹ E est lâ€™ensemble Ã  organiser par la Classifi-
cation Ascendante HiÃ©rarchique. Nous avons dÃ©jÃ  vu queE peut Ãªtre un ensembleA dâ€™attributs
(de variables) de description, un ensemble O dâ€™objets Ã©lÃ©mentaires ou un ensemble C de ca-
tÃ©gories. Î´ correspond Ã  une notion de dissimilaritÃ© quantifiÃ©e numÃ©riquement entre parties
disjointes de E.
Lâ€™Ã©tat initial se trouve dÃ©fini par lâ€™ensemble des parties singletons de E quâ€™on peut noter
P0 = {{e}|e âˆˆ E}. P0 dÃ©termine une partition en classes Ã  un Ã©lÃ©ment de E. La progression
de lâ€™algorithme est dÃ©finie comme suit :
Dâ€™un niveau au suivant de lâ€™arbre agrÃ©ger les paires de classes
â€œles plus prochesâ€ au sens de Î´
Dans cette expression on ne suppose pas nÃ©cessairement quâ€™il y a une seule fusion bi-
naire de classes qui est opÃ©rÃ©e en passant dâ€™un niveau de lâ€™arbre au suivant. Dâ€™ailleurs, dans
le schÃ©ma de la figure 1, trois fusions binaires de classes sont opÃ©rÃ©es â€œen mÃªme tempsâ€ pour
passer du niveau 1 au niveau 2.
La condition dâ€™arrÃªt est dÃ©finie par lâ€™obtention dâ€™une seule classe qui regroupe tous les Ã©lÃ©-
ments de E.
On comprend dans ces conditions que la question fondamentale est de savoir comment
Ã©laborer lâ€™indice numÃ©rique Î´ compte tenu de la nature de lâ€™ensemble Ã  organiser et de sa
description. Il est clair que la premiÃ¨re Ã©tape doit consister en la dÃ©finition dâ€™un indice de dis-
similaritÃ© d entre Ã©lÃ©ments de E. Cette dÃ©finition conduit Ã  un tableau carrÃ© de dissimilaritÃ© de
la forme :
{d(x, y)|(x, y) âˆˆ E Ã— E} (2)
RNTI - G - 8
I.C. Lerman
d est une fonction numÃ©rique Ã  valeurs positives qui, dans le cas classique, est symÃ©trique :
d(x, y) = d(y, x) pour tout (x, y) âˆˆ E Ã—E, pour laquelle on a : d(x, x) = 0 pour tout x âˆˆ E.
Maintenant, comment dÃ©finir d compte tenu de la nature deE et des caractÃ©ristiques mathÃ©-
matiques et statistiques du tableau des donnÃ©es (voir ci-dessus). Dâ€™autre part, comment opÃ©rer
le passage crucial entre lâ€™indice d entre Ã©lÃ©ments de E et celui Î´ entre parties disjointes de E.
3.2 Une expression formelle plus gÃ©nÃ©rale et formule de rÃ©actualisation
La donnÃ©e est maintenant un triplet de la forme (E,ÂµE , d) oÃ¹, avec les mÃªmes notations
que ci-dessus, ÂµE = {Âµx|x âˆˆ E} se trouve dÃ©fini par un systÃ¨me de poids ponctuels affec-
tÃ©s aux Ã©lÃ©ments de E. La dissimilaritÃ© d sur E, reprÃ©sentÃ©e toujours au moyen du tableau
{d(x, y)|(x, y) âˆˆ E Ã— E}, fait gÃ©nÃ©ralement intervenir ces poids.
Il sâ€™agit maintenant pour pouvoir construire une CAH de passer du triplet prÃ©cÃ©dent Ã  un
triplet de la forme : (P(E), ÂµP , Î´) oÃ¹ P(E) dÃ©signe lâ€™ensemble des parties de E et oÃ¹ ÂµP
dÃ©signe lâ€™extension additive de ÂµE sur P(E). Plus prÃ©cisÃ©ment,
âˆ€Z âˆˆ P(E) , ÂµZ =
âˆ‘
zâˆˆZ
Âµz (3)
Comme ci-dessus, Î´ dÃ©finit une notion de dissimilaritÃ© sur P(E) ; mais dÃ©pendant de ÂµP .
Î´ se prÃ©sente sous la forme de lâ€™application suivante :
Î´ :
(
P(E)Ã— P(E) , ÂµP
)
âˆ’â†’ R+ (4)
oÃ¹ R+ dÃ©signe lâ€™ensemble des nombres rÃ©els positifs. Plus prÃ©cisÃ©ment et de faÃ§on assez
gÃ©nÃ©rale - pour couvrir lâ€™ensemble de tous les critÃ¨res de fusion de paires de classes qui ont pu
Ãªtre proposÃ©s - la fonction Î´ peut sâ€™Ã©crire :
âˆ€(X,Y ) âˆˆ P(E)Ã— P(E) ,
Î´(X,Y ) = f [{d(x, y)|(x, y) âˆˆ (X âˆª Y )Ã— (X âˆª Y )} , {Âµz|z âˆˆ X âˆª Y }] (5)
oÃ¹ f est une fonction numÃ©rique Ã  valeurs positives dÃ©pendant de deux arguments. Le pre-
mier est le tableau des dissimilaritÃ©s sur lâ€™union X âˆª Y des deux classes X et Y Ã  comparer
et le second, est le systÃ¨me des masses ponctuelles surX âˆª Y . Cette Ã©quation exprime que les
classes formÃ©es dÃ©jÃ  en dehors deXâˆªY nâ€™interviennent pas dans la comparaison entreX et Y .
On peut voir la construction ascendante hiÃ©rarchique dâ€™un arbre de classification comme
lâ€™Ã©volution dâ€™un systÃ¨me. Si k est un niveau donnÃ© de lâ€™arbre, nous caractÃ©risons lâ€™Ã©tat du sys-
tÃ¨me par le couple (Tk, Âµk), oÃ¹ Tk est la matrice des dissimilaritÃ©s Î´ entre classes dÃ©jÃ  formÃ©es
au niveau k et oÃ¹ Âµk est la mesure (telle que ÂµP ci-dessus) sur lâ€™ensemble de ces classes.
Lâ€™Ã©tat initial (T0, Âµ0) est dÃ©fini par la matrice T0 des indices Î´ entre classes singletons (com-
prenant chacune exactement un Ã©lÃ©ment) et par le systÃ¨me initial de poids Âµ0 oÃ¹ chaque classe
singleton {x} est affectÃ©e par le poids de lâ€™Ã©lÃ©ment x concernÃ©, x âˆˆ E. En dÃ©signant par l
le dernier niveau de lâ€™arbre (l = 4 dans lâ€™exemple de la figure 1), les Ã©tats du systÃ¨me pour
0 â‰¤ k â‰¤ lâˆ’1 ont Ã  Ãªtre considÃ©rÃ©s. Dans ces conditions, la formule suivante appelÃ©e â€œformule
de rÃ©actualisationâ€ est de la premiÃ¨re importance :
RNTI - G - 9
Classification par la vraisemblance des liens relationnels
(Tk+1, Âµk+1) = Ï•(Tk, Âµk) (6)
oÃ¹ Ï• est une fonction Ã  dÃ©terminer compte tenu de lâ€™expression spÃ©cifique de lâ€™indice Î´
choisi de comparaison entre classes (critÃ¨re de fusion). Une telle Ã©quation dite aussi formule de
â€œrÃ©currenceâ€ permet aprÃ¨s la reconnaissance des paires de classes Ã©galement les plus proches
- et donc Ã  fusionner - de dÃ©terminer lâ€™Ã©tat du systÃ¨me au niveau k + 1, sachant son Ã©tat au
niveau k, 0 â‰¤ k â‰¤ l âˆ’ 1. Cette formule est trÃ¨s utilisÃ©e pour une forme dâ€™implÃ©mentation de
lâ€™algorithme de classification ascendante hiÃ©rarchique.
4 Lâ€™Analyse de la Vraisemblance des Liens Relationnels (Une
prÃ©sentation Ã  partir dâ€™exemples)
4.1 CaractÃ©ristiques principales de la mÃ©thode et principe de lâ€™indice de
fusion entre classes
Trois caractÃ©ristiques fondamentales conditionnent cette mÃ©thodologie :
1. ReprÃ©sentation ensembliste puis relationnelle des variables (attributs) de description
2. Notion trÃ¨s gÃ©nÃ©rale de similaritÃ© se rÃ©fÃ©rant Ã  une Ã©chelle probabiliste Ã©tablie par rap-
port Ã  une hypothÃ¨se dâ€™absence de liaison entre les variables (attributs) de description
â€œrespectantâ€ les distributions marginales observÃ©es des diffÃ©rentes variables
3. Guidage statistique dans lâ€™interprÃ©tation de lâ€™arbre de classification, conduisant Ã  la dÃ©-
termination - au moyen dâ€™un critÃ¨re objectif - dâ€™agrÃ©gats â€œintÃ©ressantsâ€ et de partitions
(classifications) â€œintÃ©ressantesâ€
La reprÃ©sentation ensembliste et relationnelle des variables descriptives entraÃ®ne une ex-
trÃªme gÃ©nÃ©ralitÃ© de lâ€™approche par rapport aux structures des donnÃ©es les plus complexes. Nous
le mentionnerons plus explicitement dans la section traitant des grands types mathÃ©matico-
logiques de donnÃ©es. La notion de similaritÃ© probabiliste va rejoindre la philosophie de la
thÃ©orie de lâ€™information ; mais oÃ¹, les Ã©vÃ¨nements observÃ©s sont des valeurs dâ€™indices de simi-
laritÃ© entre les structures de donnÃ©es Ã  comparer. Les partitions intÃ©ressantes vont se produire
aux niveaux que nous dirons â€œsignificatifsâ€ - dans un sens que nous prÃ©ciserons - de lâ€™arbre de
classification. Les agrÃ©gats â€œintÃ©ressantsâ€ vont correspondre Ã  des noeuds â€œsignificatifsâ€ - dans
un sens que nous prÃ©ciserons - de lâ€™arbre de classification.
Il y a une trÃ¨s forte cohÃ©rence dans la suite des diffÃ©rentes Ã©tapes de la mÃ©thode de classifi-
cation ascendante hiÃ©rarchique de lâ€™AVLR. Nous allons commencer par considÃ©rer une articu-
lation fondamentale concernant le critÃ¨re (indice) de fusion des paires de classes. Nous allons
en donner le principe dans le cadre dâ€™une illustration gÃ©omÃ©trique trÃ¨s simple alors que dans
notre cas, comme nous venons de lâ€™exprimer, câ€™est une reprÃ©sentation ensembliste et relation-
nelle des variables descriptives qui prÃ©vaut. Il sâ€™agit dans lâ€™exemple, de la classification dâ€™un
nuage de points unidimensionnel (voir FIG. 6). Ayons Ã  lâ€™esprit que lâ€™exigence premiÃ¨re dâ€™une
CAH consiste Ã  Ã©tablir un ordre des agrÃ©gations et que câ€™est un critÃ¨re numÃ©rique qui per-
met de sÃ©lectionner Ã  chaque Ã©tape la â€œmeilleureâ€ agrÃ©gation (ou les Ã©galement â€œmeilleuresâ€
agrÃ©gations). Câ€™est de ce critÃ¨re numÃ©rique dont il sâ€™agit. CommenÃ§ons par poser un indice Î´
RNTI - G - 10
I.C. Lerman
classique et naturel ici, dÃ©fini par la distance minimale et imaginons quâ€™Ã  une Ã©tape donnÃ©e de
lâ€™algorithme on ait Ã  choisir parmi deux agrÃ©gations candidates : C1 et C2 dâ€™une part etD1 etD2
dâ€™autre part. En se limitant au critÃ¨re dÃ©fini, compte tenu de ce que Î´(D1, D2) est strictement
infÃ©rieur Ã  Î´(C1, C2), la premiÃ¨re agrÃ©gation Ã  choisir est celle deD1 etD2. Cependant, on peut
remarquer que les classes D1 et D2 sont sensiblement plus denses que celles C1 et C2. Or, il
est â€œnormalâ€ que les deux extrÃ©mitÃ©s mutuellement les plus voisines soient plus proches pour
deux classes fortement densesD1 etD2 que pour deux classes faiblement denses C1 et C2. Dans
ces conditions, pour le critÃ¨re de lâ€™AVLR, la fusion choisie est celle de C1 et C2. Ainsi, câ€™est
lâ€™exceptionnalitÃ© de la petitesse de Î´ qui guide vers le â€œmeilleurâ€ choix. On se rend compte
quâ€™ainsi, nous rejoignons la philosophie de la thÃ©orie de lâ€™information au niveau des relations
observÃ©es.
C1 C2 D1 D2
 (C1, C2)  (D1,D2)>
.  â€¦ ..   . .. .              â€¦â€¦. â€¦..
FIG. 6 â€“ Principe de lâ€™agrÃ©gation AVL.
ConsidÃ©rons Ã  prÃ©sent une deuxiÃ¨me illustration gÃ©omÃ©trique supposant un nuage de points
planaire (voir FIG. 7). C1 et C2 sont deux classes fortement denses ; alors que D1 et D2 sont
deux classes faiblement denses. Pour le critÃ¨re de la distance minimale Î´ on a clairement
Î´(C1, C2) < Î´(D1,D2). Avec le mÃªme raisonnement intuitif que ci-dessus, entre les deux
fusions ; celle de C1 et C2 ou celle de D1 et D2, AV LR choisit celle de D1 et D2.
Profitons de signaler ici que lâ€™effet bien connu de â€œchaÃ®nageâ€ du critÃ¨re de la distance
minimale, disparaÃ®t dÃ¨s lors quâ€™on lui substitue la vraisemblance de la petitesse de la valeur de
cette distance minimale. Dâ€™autre part et enfin, il est clair quâ€™on peut prendre dâ€™autres indices
de base (nous disons â€œbrutsâ€) que la distance minimale et suivre la mÃªme dÃ©marche.
4.2 Les comparaisons par paires dans la mÃ©thode AV LR en cas de don-
nÃ©es boolÃ©ennes
4.2.1 ReprÃ©sentation ensembliste de lâ€™ensemble des attributs de description
Nous noterons de faÃ§on conforme Ã  ci-dessus par A lâ€™ensemble des attributs boolÃ©ens de
description et par O lâ€™ensemble des objets dÃ©crits. Nous reprÃ©sentons un attribut boolÃ©en a
(a âˆˆ A) par le sous ensemble O(a) des objets oÃ¹ il est Ã  V RAI (oÃ¹ il est prÃ©sent) (voir FIG.
8). Ainsi, lâ€™ensemble de reprÃ©sentation est lâ€™ensemble des parties de lâ€™ensemble O des objets.
Câ€™est le cas de reprÃ©sentation le plus simple puisque lâ€™attribut boolÃ©en dÃ©finit une variable
relationnelle unaire. Signalons ici quâ€™un autre cas trÃ¨s important de variable relationnelle unaire
est fourni par la variable quantitative qui elle, dÃ©finit une valuation sur lâ€™ensembleO des objets.
Il est clair que sâ€™il sâ€™agit dâ€™attributs de mÃªme type dont lâ€™ensemble des valeurs peut Ãªtre
structurÃ© de faÃ§on plus ou moins complexe, la reprÃ©sentation fidÃ¨le dâ€™un mÃªme attribut ne peut
plus se faire au niveau de O. Il faudra travailler Ã  un niveau supÃ©rieur : lâ€™ensemble des paires
RNTI - G - 11
Classification par la vraisemblance des liens relationnels
C1 C2
D1 D2
 (C1, C2)
 (D1,D2)
FIG. 7 â€“ Comparaison entre deux dissimilaritÃ©s entre classes.
O(a)
O
FIG. 8 â€“ ReprÃ©sentation dâ€™un attribut boolÃ©en.
RNTI - G - 12
I.C. Lerman
11001011
00111001
11001100
01110001
00011001
00010101
11000100
00011011
11001000
10101100
a1 a2 a3 a4 a5 a6 a7 a8
o1
o2
o3
o4
o5
o8
o6
o7
o9
o10
O(a5) = {o3, o5, o6, o7, o9}
O(a1) = {o3, o5, o6, o7, o9, o10}
O(a2) = {o3, o10}
O(a3) = {o1, o4, o5, o8}
O(a4) = {o1, o2, o3, o6, o8, o9, o10}
O(a7) = {o2, o4, o7, o8, o10}
O(a6) = {o1, o7, o9}
O(a8) = {o1, o2, o4, o8, o10}
FIG. 9 â€“ ReprÃ©sentation de la description dâ€™un tableau de donnÃ©es boolÃ©ennes.
dâ€™objets ou celui des couples de paires dâ€™objets, ou ...
La figure 9 donne lâ€™exemple de la reprÃ©sentation de lâ€™ensemble des attributs de descrip-
tion dans le cas dâ€™un petit tableau de donnÃ©es boolÃ©ennes (10 objets et 8 attributs). Il sâ€™agit de
faÃ§on gÃ©nÃ©rale dâ€™un Ã©chantillon dâ€™Ã©lÃ©ments dans lâ€™ensemble des parties de O.
4.2.2 Comparaison entre attributs boolÃ©ens
En vue de lâ€™organisation par la classification de lâ€™ensembleA des attributs boolÃ©ens consi-
dÃ©rons le problÃ¨me de la comparaison deux Ã  deux de ces attributs boolÃ©ens. Ã€ cet effet, com-
menÃ§ons par considÃ©rer une paire {a, b} dâ€™attributs faisant partie de A. La reprÃ©sentation en-
sembliste conduit Ã  une paire de parties de O de la forme {O(a),O(b)}. La situation relative
entreO(a) etO(b) est schÃ©matisÃ©e dans la figure 10 oÃ¹ on introduit les paramÃ¨tres s, u, v et t.
On a avec des notations que lâ€™on comprend :ï£±ï£´ï£´ï£²ï£´ï£´ï£³
s = n(a âˆ§ b) = card(O(a) âˆ© O(b))
u = n(a âˆ§ Â¬b) = card(O(a) âˆ© O(Â¬b))
v = n(Â¬a âˆ§ b) = card(O(Â¬a) âˆ© O(b))
t = n(Â¬a âˆ§ Â¬b) = card(O(Â¬a) âˆ© O(Â¬b))
On a s + u + v + t = n. De nombreux indices (nous dirons aussi â€œcoefficientsâ€) de
similaritÃ© (nous dirons aussi dâ€™â€œassociationâ€) entre deux attributs boolÃ©ens ont Ã©tÃ© proposÃ©s.
Ils se prÃ©sentent tous respectivement sous la forme, de fonctions des trois paramÃ¨tres s, u et v,
strictement croissantes par rapport Ã  s, symÃ©triques en u et v et dÃ©croissantes par rapport Ã  u.
Dans ces fonctions on cherche dâ€™une faÃ§on ou dâ€™une autre et de faÃ§on implicite Ã  neutraliser
lâ€™influence dans la ressemblance des tailles n(a) = card(O(a)) et n(b) = card(O(b)). Ainsi
en est-il du fameux indice de Jaccard (1908) qui se met sous la forme s/(s+ u+ v) que nous
RNTI - G - 13
Classification par la vraisemblance des liens relationnels
considÃ©rerons ci-dessous pour notre illustration. Notre approche constructive est diffÃ©rente.
Nous commenÃ§ons bien par le choix initial dâ€™une fonction de similaritÃ©. Mais, compte tenu
de lâ€™invariance du rÃ©sultat par rapport Ã  ce choix, nous partirons le plus â€œnaturellementâ€ de
lâ€™indice â€œbrutâ€ de ressemblance s qui reprÃ©sente le nombre dâ€™objets oÃ¹ les deux attributs a et b
sont Ã  V RAI . Il sâ€™agit alors dâ€™Ã©valuer la grandeur relative de s compte tenu du contexte dÃ©fini
â€œlocalementâ€ par le couple (n(a), n(b)). Cette Ã©valuation se fera par rapport Ã  une HypothÃ¨se
dâ€™Absence de Liaison (H.A.L.) oÃ¹ au couple de parties observÃ©es (O(a)),O(b))), on associe
un couple de parties alÃ©atoires et indÃ©pendantes (O(aâˆ—)),O(bâˆ—)) â€œrespectantâ€ respectivement
les cardinalitÃ©s n(a) et n(b). DiffÃ©rents modÃ¨les alÃ©atoires peuvent Ãªtre considÃ©rÃ©s. La valeur
observÃ©e s est alors situÃ©e par rapport Ã  la distribution probabiliste de lâ€™indice brut alÃ©atoire
S = card(O(aâˆ—))âˆ©O(bâˆ—)). Dans ces conditions, la valuation de la â€œressemblanceâ€ entre a et b
est dâ€™autant plus grande que s apparaÃ®t â€œinvraisemblablementâ€ grand eu Ã©gard Ã  la distribution
de S. On introduit ainsi une notion de â€œvraisemblanceâ€ dans la notion de â€œressemblanceâ€. Dans
la figure 11 qui schÃ©matise la construction, Pl(a, b) est lâ€™indice probabiliste dâ€™association.
su
v
tO(a) O(b)
O
FIG. 10 â€“ ReprÃ©sentation ensembliste de la comparaison entre deux attributs boolÃ©ens.
(O(a),O(b))
H.A.L.
O(a ) O(b
 )
S = card(O(a )  O(b ))
Pl(a, b) = Pr{S ! s}
FIG. 11 â€“ Indice probabiliste local.
Signalons ici lâ€™indice statistiquement normalisÃ© suivant qui permet le calcul de lâ€™indice
probabiliste via la fonction de rÃ©partition de la loi normale centrÃ©e rÃ©duite Î¦ :
Q(a, b) =
(
sâˆ’ E(S))/âˆšvar(S)
RNTI - G - 14
I.C. Lerman
Pl(a, b) = Î¦
(
Q(a, b)
)
(7)
oÃ¹ E(S) et var(S) dÃ©signent lâ€™espÃ©rance mathÃ©matique et la variance de lâ€™indice brut alÃ©a-
toire S.
Lâ€™indice Pl(a, b) a un caractÃ¨re local, circonscrit aux deux attributs Ã  comparer. Le contexte
dÃ©finitif de comparaison deux Ã  deux pour lâ€™Ã©tablissement de lâ€™indice probabiliste sur A sera
global.
Les coefficients dâ€™association (indices de similaritÃ©) que nous considÃ©rons ont essentiel-
lement un caractÃ¨re symÃ©trique. Ainsi, relativement Ã  un couple dâ€™attributs boolÃ©ens (a, b),
lâ€™indice Pl(a, b) cherche Ã  â€œmesurerâ€ le degrÃ© dâ€™Ã©quivalence entre a est Ã  V RAI et b est Ã 
V RAI . RÃ©gis Gras a eu lâ€™idÃ©e dâ€™adapter ce type dâ€™indice de la Vraisemblance du Lien pour
â€œmesurerâ€ une relation dissymÃ©trique de la forme â€œcombien a Ã  V RAI implique b Ã  V RAIâ€.
Il en est rÃ©sultÃ© un ensemble important de travaux - Ã©voquÃ©s dans les rÃ©fÃ©rences - autour de la
recherche dans les donnÃ©es de structures implicatives.
6 0.89 0.27 0.58 0.95 0.74 0.38 0.15
2 2 0.45 0.84 0.74 0.55 0.74 0.74
1 0 4 0.44 0.38 0.66 0.68 0.88
4 2 2 7 0.51 0.65 0.51 0.75
5 1 1 3 5 0.82 0.24 0.06
2 0 1 2 2 3 0.54 0.54
2 1 2 3 1 1 5 0.92
1 1 3 4 0 1 4 5
A
a
1
a
1
a
2
a
2
a
3
a
3
a
4
a
4
a
5
a
5
a
6
a
6
a
7
a
7
a
8
a
8
s Pl
FIG. 12 â€“ Tableau des similaritÃ©s pour le tableau des donnÃ©es boolÃ©ennes ci-dessus.
Exemple du tableau prÃ©cÃ©dent des donnÃ©es boolÃ©ennes Le tableau de la figure 12 est rela-
tif aux donnÃ©es boolÃ©ennes du tableau de la figure 9. Les valeurs de lâ€™indice brut s se trouvent
sous la diagonale principale (au sens large). Ainsi, s(a4, a7) = 3, s(a5, a6) = 2, ... . Le j-Ã¨me
Ã©lÃ©ment de la diagonale tel que s(aj , aj) nâ€™est autre que n(aj) = card(O(aj)), 1 â‰¤ j â‰¤ 8.
Les valeurs dâ€™un indice probabiliste local Pl - conformÃ©ment Ã  un modÃ¨le alÃ©atoire dâ€™ab-
sence de liaison - sont placÃ©es strictement en dessous de la diagonale principale. On a ainsi
Pl(a4, a7) = 0.51 et Pl(a5, a6) = 0.82.
RNTI - G - 15
Classification par la vraisemblance des liens relationnels
On constate quâ€™il peut exister des inversions entre dâ€™une part lâ€™indice brut s et dâ€™autre
part, lâ€™indice probabiliste local Pl. On a par exemple :
s(a4, a7) = 3 > s(a5, a6) = 2 et Pl(a4, a7) = 0.51 < Pl(a5, a6) = 0.82
Des inversions peuvent mÃªme avoir lieu entre lâ€™indice de Jaccard J mentionnÃ© ci-dessus et
lâ€™indice probabiliste local Pl. On a ainsi :
J(a1, a6) =
2
7
< J(a4, a7) =
3
9
et Pl(a1, a6) = 0.74 > Pl(a4, a7) = 0.51
RÃ©duction globale des similaritÃ©s Le contexte local permet avec Pl des comparaisons bien
diffÃ©renciÃ©es pourvu que le nombre dâ€™objets ne soit pas â€œtrop grandâ€. Autrement, Pl a ten-
dance Ã  tendre soit vers 1 (resp., 0) en cas dâ€™association positive (resp., nÃ©gative) par rapport Ã 
lâ€™indÃ©pendance probabiliste. Câ€™est quâ€™en fait, pour lâ€™organisation classificatoire de lâ€™ensemble
A des attributs boolÃ©ens, le problÃ¨me consiste en la comparaison relative deux Ã  deux des Ã©lÃ©-
ments de A. Câ€™est pour cette raison que dans le contexte global on rapporte la comparaison
entre deux attributs aux comparaisons mutuelles deux Ã  deux entre attributs. Dans ces condi-
tions, Ã  la suite (a1, a2, ..., aj , ..., ap) des attributs observÃ©s on associe dans une hypothÃ¨se
dâ€™absence de liaison respectant de faÃ§on marginale la configuration du tableau des donnÃ©es,
une suite (a1âˆ—, a2âˆ—, ..., ajâˆ—, ..., apâˆ—) dâ€™attributs alÃ©atoires mutuellement indÃ©pendants. Lâ€™indice
Q(aj , ak) (cf. (7)) entre deux attributs de A, statistiquement localement normalisÃ©, est lui
mÃªme Ã  nouveau globalement normalisÃ© par rapport Ã  sa distribution empirique sur lâ€™ensemble
P2(A), 1 â‰¤ j < k â‰¤ p. Ainsi, on substitue Ã  lâ€™indice Q(aj , ak), celui Qg(aj , ak) oÃ¹ (voir
FIG. 13) oÃ¹ me(Q) et var(Q) sont respectivement la moyenne et la variance empirique de
Q(aj , ak) sur P2(A). Câ€™est la loi asymptotiquement normale de Qg(ajâˆ—, akâˆ—) qui conduit Ã 
lâ€™indice probabiliste global Pg(aj , ak), 1 â‰¤ j < k â‰¤ p. La figure 13 donne le schÃ©ma gÃ©nÃ©ral
de la procÃ©dure.
(a1, a2, ..., aj , ..., ap) (a1 , a2 , ..., aj , ..., ap )
H.A.L.
P2(A) = {{a
j , ak}|1  j < k  p}
Q(aj, ak)!" Qg(a
j , ak) = (Q(aj , ak)"moye(Q))/
p
vare(Q)
Pg(a
j , ak) = Pr{Qg(a
j , ak )  Qg(a
j , ak)} =  (Qg(a
j , ak))
me(Q) et vare(Q) sur P2(A)
FIG. 13 â€“ SimilaritÃ© probabiliste globale.
RNTI - G - 16
I.C. Lerman
Cette rÃ©fÃ©rence Ã  la loi normale est justifiÃ©e dans les rÃ©fÃ©rences DaudÃ© (1992) et Lerman
(1984) de la section 7.2. Lâ€™approximation normale devient dans la plupart des cas, trÃ¨s bonne
dÃ¨s lors que la taille n de lâ€™ensemble des objets dÃ©passe lâ€™ordre de la dizaine dâ€™unitÃ©s.
4.2.3 Comparaison entre objets dÃ©crits par des attributs
Il sâ€™agit Ã  prÃ©sent dâ€™adresser le problÃ¨me de la classification de lâ€™ensemble O des objets
dÃ©crits par des attributs. Pour se fixer les idÃ©es on peut imaginer que les attributs sont boolÃ©ens.
Cependant, la procÃ©dure est dâ€™une extrÃªme gÃ©nÃ©ralitÃ© par rapport Ã  la nature des attributs. Ainsi,
comment effectuer de faÃ§on cohÃ©rente les comparaisons mutuelles entre objets pour aboutir au
mÃªme type dâ€™indice probabiliste que dans le cas de la comparaison entre attributs. La procÃ©dure
se dÃ©compose en la suite des Ã©tapes suivantes :
(i) Contribution brute dâ€™un attribut Ã  la comparaison de deux objets ConsidÃ©rons le
couple formÃ© par un attribut aj et une paire dâ€™objets {oi, oiâ€²}, 1 â‰¤ j â‰¤ p, 1 â‰¤ i < iâ€² â‰¤ n.
On dÃ©finit la contribution â€œbruteâ€ sj(oi, oiâ€²) de lâ€™attribut aj Ã  la ressemblance entre les deux
objets oi et oiâ€² . Un exemple en cas dâ€™attributs boolÃ©ens - que nous ne pouvons ici justifier - est
donnÃ© par :
sj(oi, oiâ€²) =
1
p
âˆ’ 1
2
(Î·ji âˆ’ Î·jiâ€²)2 (8)
oÃ¹
Î·ji = 
j
i/
âˆš( âˆ‘
1â‰¤kâ‰¤p
ki
)
(9)
et oÃ¹ ji = 1(resp.,0) selon que lâ€™attribut a
j est prÃ©sent (Ã  V RAI) (resp., absent (Ã FAUX))
chez lâ€™objet oi.
(ii) Normalisation statistique de la contribution â€œbruteâ€ de similaritÃ© sj(oi, oiâ€²) est nor-
malisÃ© par rapport Ã  la distribution statistique de la contribution â€œbruteâ€ du j-Ã¨me attribut sur
lâ€™ensemble O Ã—O des couples dâ€™objets. Il sâ€™agit nommÃ©ment de la distribution :
{sj(ol, olâ€²)|1 â‰¤ l, lâ€² â‰¤ n}
La contribution normalisÃ©e de lâ€™attribut aj Ã  la comparaison des deux objets oi et oiâ€² se
met dans ces conditions sous la forme :
Sj(oi, oiâ€²) =
(
sj(oi, oiâ€²)âˆ’me(sj)
)
/
âˆš
vare(sj) (10)
oÃ¹ me(sj) et vare(sj) sont la moyenne et la variance de la prÃ©cÃ©dente distribution, 1 â‰¤
i < iâ€² â‰¤ n.
RNTI - G - 17
Classification par la vraisemblance des liens relationnels
(iii) Somme des contributions normalisÃ©es On dÃ©finit lâ€™indice totalisant lâ€™ensemble des
contributions normalisÃ©es sous la forme :
S(oi, oiâ€²) =
âˆ‘
1â‰¤jâ‰¤p
Sj(oi, oiâ€²) (11)
1 â‰¤ i < iâ€² â‰¤ n. Par rapport au cas dual de la construction dâ€™un indice dâ€™association entre
attributs de description, on peut ici considÃ©rer quâ€™on se trouve au mÃªme niveau que celui de la
dÃ©finition du coefficient Q (cf. (7)). Il nous reste donc Ã  procÃ©der Ã  la
(iv) Normalisation statistique globale de S(oi, oiâ€²) par rapport Ã  lâ€™ensemble P2(O) des
paires dâ€™objets distincts Lâ€™indice ainsi normalisÃ© conduit via la fonction de rÃ©partition de la
loi normale centrÃ©e et rÃ©duite, Ã  lâ€™indice probabiliste de vraisemblance du lien. La rÃ©fÃ©rence Ã 
la loi normale dans lâ€™hypothÃ¨se probabiliste dâ€™absence de liaison mutuelle entre les diffÃ©rents
attributs de description, pour la loi de lâ€™indice alÃ©atoire associÃ© Ã  S(oi, oiâ€²) se justifie en faisant
appel au thÃ©orÃ¨me central limite. En effet, cette variable alÃ©atoire est formÃ©e dâ€™une somme de
contributions normalisÃ©es alÃ©atoires et indÃ©pendantes. Il importe dans ces conditions que le
nombre p de variables de descriptions ne soit pas trop petit. Ce qui est le cas le plus souvent.
Toutefois, quelle que soit la valeur de p, il nâ€™est pas interdit de vouloir se positionner, pour la
loi de lâ€™indice alÃ©atoire globalement normalisÃ© dans lâ€™hypothÃ¨se dâ€™absence de liaison, relative-
ment Ã  la loi normale centrÃ©e et rÃ©duite.
Signalons ici que lâ€™Ã©laboration dâ€™une matrice de similaritÃ©s probabilistes sur un ensemble
C de catÃ©gories dÃ©crits par des attributs boolÃ©ens obÃ©it Ã  la mÃªme dÃ©marche que ci-dessus.
4.2.4 Cas gÃ©nÃ©ral
Lâ€™Ã©tablissement dâ€™une matrice dâ€™indices probabilistes de la vraisemblance du lien telle
quâ€™elle a Ã©tÃ© introduite pour la classification dâ€™un ensemble A dâ€™attributs, respectivement pour
la classification dâ€™un ensemble O dâ€™objets, dans le cas dâ€™un tableau de donnÃ©es boolÃ©ennes,
a Ã©tÃ© Ã©tendu dans le cas dâ€™un tableau trÃ¨s gÃ©nÃ©ral de donnÃ©es (voir la sous section 2.2.1 et la
section suivante). Cette gÃ©nÃ©ralisation mathÃ©matico-statistique a Ã©tÃ© validÃ©e sur le plan expÃ©-
rimental par de nombreuses, difficiles et trÃ¨s intÃ©ressantes applications. La figure 14 donne le
schÃ©ma gÃ©nÃ©ral. Lâ€™ensemble E Ã  organiser par la classification peut Ãªtre soit un ensemble A
de variables descriptives, soit un ensemble O dâ€™objets dÃ©crits (resp., un ensemble C de catÃ©go-
ries dÃ©crites). Dans lâ€™un ou lâ€™autre de ces deux cas duaux, on aboutit Ã  une matrice dâ€™indices
probabilistes {P (x, y)|{x, y} âˆˆ P2(E)} telle quâ€™elle est indiquÃ©e dans le schÃ©ma. P2(E) est
lâ€™ensemble des paires (parties Ã  deux Ã©lÃ©ments) de E.
Nous proposons de substituer Ã  la table dâ€™indices probabilistes la table suivante que nous
appelons â€œMatrice des dissimilaritÃ©s â€œinformationnellesâ€â€ :{
âˆ†(x, y) = âˆ’Log2(P (x, y))|{x, y} âˆˆ P2(E)
}
âˆ’Log2(P (x, y)) est la quantitÃ© dâ€™information de lâ€™Ã©vÃ¨nement dont la probabilitÃ© estP (x, y).
On remarquera que lorsque P (x, y) varie entre 1 et 0, âˆ†(x, y) varie entre 0 et lâ€™infini. La ma-
trice prÃ©cÃ©dente est ce que la mÃ©thode AV LR peut transmettre aux algorithmes qui travaillent
avec des dissimilaritÃ©s.
RNTI - G - 18
I.C. Lerman
{P (x, y)|{x, y}  P2(E)}
O( resp., C)A
E
FIG. 14 â€“ Table de similaritÃ©s probabilistes quel que soit lâ€™ensemble organisÃ©.
4.3 Famille de critÃ¨res de la vraisemblance du lien maximal et construc-
tion de lâ€™arbre
Il sâ€™agit maintenant, selon la dÃ©marche dâ€™Adanson, de rapprocher des â€œpetites famillesâ€ qui
reprÃ©sentent dans notre formalisme des parties disjointes, des classes, de lâ€™ensemble Ã  organi-
serE. Nous avons vu (section 3.2) que cela passait par la dÃ©finition dâ€™un indice de dissimilaritÃ©
- ou de faÃ§on Ã©quivalente - de similaritÃ© entre parties disjointes de E. E est maintenant muni
dâ€™un indice de similaritÃ© probabiliste (section 4.2.4 ci-dessus). Dans ces conditions, soient C
et D deux parties disjointes (deux classes) de lâ€™ensemble E. Nous allons partir dâ€™un indice de
comparaison qui est dÃ©fini par le lien â€œmaximalâ€ (on dit encore lien â€œsimpleâ€ ou â€œsingle lin-
kageâ€) mais dans le contexte de la matrice des indices probabilistes Ã©tablie. Plus prÃ©cisÃ©ment,
nous dÃ©finissons :
p(C,D) = max{P (c, d)|(c, d) âˆˆ C Ã—D} (12)
ConformÃ©ment au principe de fusion des classes dans lâ€™AV LR (voir section 4.1), nous
associons dans une hypothÃ¨se dâ€™absence de liaison, au couple de parties (C,D) un couple
(Câˆ—, Dâˆ—) de parties alÃ©atoires indÃ©pendantes et formÃ©es respectivement dâ€™Ã©lÃ©ments mutuelle-
ment indÃ©pendants. La forme pure du critÃ¨re â€œvraisemblance du lien maximalâ€ prend alors la
forme :
P (C,D) = Pr{p(Câˆ—, Dâˆ—) â‰¤ p(C,D)} = (p(C,D))(|C|Ã—|D|) (13)
oÃ¹ |C| (resp., |D|) dÃ©signe le cardinal de C (resp.,D). Une famille plus large et paramÃ©trÃ©e
de critÃ¨res est dÃ©finie par lâ€™Ã©quation :
P(C,D) =
(
p(C,D)
)(|C|Ã—|D|)
(14)
Pour des raisons de discrimination au niveau du calcul numÃ©rique ; mais avec un rÃ©sultat
identique on utilise la fonction strictement croissante :
S(C,D) = âˆ’Log2[âˆ’Log2(P(C,D))] (15)
Nous avions introduit cette famille Ã  lâ€™occasion de travaux avec F. Costa NicolaÃ¼ et H.
Bacelar-NicolaÃ¼. En effet, F. Costa NicolaÃ¼ a cherchÃ© Ã  introduire une certaine souplesse en
RNTI - G - 19
Classification par la vraisemblance des liens relationnels
Ã©tudiant dâ€™autres fonctions que celles associÃ©es Ã  (12) ou (13). Remarquons que pour  = 0,
on a le lien simple et que pour  = 1 on a la forme pure du critÃ¨re de la vraisemblance du lien
maximal. Signalons que dÃ¨s que  se dÃ©tache de la valeur 0, lâ€™effet bien connu de â€œchaÃ®nageâ€
du â€œsingle linkageâ€ disparaÃ®t. Une valeur trÃ¨s utilisÃ©e pour  est 0.5.
Signalons enfin quâ€™on dispose clairement dâ€™une formule de rÃ©actualisation pour S.
4.3.1 Les deux arbres de classification issus de lâ€™exemple
Relativement au tableau de donnÃ©es boolÃ©ennes de la figure 9, nous avons obtenu par lâ€™ap-
plication de la mÃ©thode les deux arbres des figures 15 et 16. Le premier (FIG. 15) porte sur
lâ€™ensemble A des 8 attributs boolÃ©ens et le second (FIG. 16) porte sur lâ€™ensemble O des 10
objets. Dans un sens que nous prÃ©ciserons ci-dessous, le niveau 5 est distinguÃ© dans le premier
arbre (sur A), comme Ã©tant un niveau â€œsignificatifâ€. Le niveau 6 est Ã©galement intÃ©ressant.
Dans le second arbre (sur O) câ€™est le niveau 9 qui est distinguÃ© comme â€œsignificatifâ€. Nous
prÃ©ciserons Ã©galement bientÃ´t la notion de noeuds â€œsignificatifâ€. Les noeuds significatifs sont
marquÃ©s par une Ã©toile âˆ—.
 
 
 
0
1
2
3
4
5
6
7
a
1
a
5
a
6
a
2
a
4
a
7
a
3
a
8
FIG. 15 â€“ Arbre de classification sur A ; niveaux et noeuds â€œsignificatifsâ€.
1
2
3
4
5
6
7
8
9
o1o2o9 o8o4o10o3o6o7o5
 
 
 
FIG. 16 â€“ Arbre de classification sur O ; niveaux et noeuds â€œsignificatifsâ€.
RNTI - G - 20
I.C. Lerman
4.4 Niveaux et Noeuds les plus â€œsignificatifsâ€ dâ€™un arbre de classification
CommenÃ§ons par donner un sens intuitif aux notions dâ€™un niveau â€œsignificatifâ€ et dâ€™un
noeud â€œsignificatifâ€ dâ€™un arbre de classification. Reprenons ici lâ€™image donnÃ©e au paragraphe
3.2 de la construction de lâ€™arbre en termes dâ€™un processus de synthÃ¨se automatique. Un niveau
donnÃ©, dÃ©fini par un Ã©tat du processus, dÃ©termine une partition (classification) sur lâ€™ensemble
total E. Un niveau â€œsignificatifâ€ correspond Ã  un Ã©tat dâ€™Ã©quilibre dans la synthÃ¨se automa-
tique. Les classes obtenues quâ€™achÃ¨vent des noeuds qui sont en dessous (au sens large) du
niveau concernÃ©, sont ces familles qui, selon lâ€™expression dâ€™Adanson, forment un â€œtout dont
les parties sont liÃ©es intimementâ€, sur le plan de la cohÃ©rence, avions nous prÃ©cisÃ©. La liaison
est dâ€™autant plus intime sur le plan de la cohÃ©rence que le noeud est â€œsignificatifâ€.
Il y a lieu maintenant de prÃ©ciser comment objectivement, nous dÃ©terminons les niveaux
et les noeuds les plus â€œsignificatifsâ€ dâ€™un arbre de classification. La base est un critÃ¨re qui
se prÃ©sente comme un coefficient dâ€™association obÃ©issant au principe de la dÃ©marche AV LR,
entre une partition et une similaritÃ© sur E. Ce coefficient a une nature combinatoire et non
paramÃ©trique. Le point de dÃ©part consiste en la reprÃ©sentation de la partition et de la similaritÃ©
sur E. Elles seront vues comme deux relations binaires symÃ©triques sur E. La reprÃ©sentation
se fait au niveau de lâ€™ensemble F des paires ou parties Ã  deux Ã©lÃ©ments de E :
F = P2(E) =
{{x, y}|x âˆˆ E, y âˆˆ E, x 6= y}
Pour notre problÃ¨me de dÃ©tection de partitions intÃ©ressantes, posons :
{pi1, pi2, ..., pik, ..., pil}
la suite des partitions produites aux diffÃ©rents niveaux de lâ€™arbre des classifications. Indi-
quons pik sous la forme :
pik = {Ek1, Ek2, ..., Eki, ..., Ekck}
Nous reprÃ©sentons pik au niveau de F par le sous ensemble des paires quâ€™elle rÃ©unit ; nom-
mÃ©ment :
R(pik) =
âˆ‘
1â‰¤iâ‰¤ck
P2(Eki),
oÃ¹ la somme dâ€™ensembles indique une rÃ©union dâ€™ensembles disjoints.
Prenons lâ€™exemple du premier arbre de la figure 15 oÃ¹ E = A. On a la partition du niveau
5, pi5 =
{{1, 5, 6}, {2, 4}, {3, 7, 8}} quâ€™on reprÃ©sente comme suit :
R(pi5) =
{{1, 5}, {1, 6}, {5, 6}, {2, 4}, {3, 7}, {3, 8}, {7, 8}}
PrÃ©cisons maintenant la construction du coefficient dâ€™association entre une partition pi et
une similaritÃ© Q sur lâ€™ensemble E. Nous proposons ici de prendre une similaritÃ© numÃ©rique.
Cependant, il existe une version du critÃ¨re oÃ¹ une similaritÃ© ordinale (prÃ©ordonnance sur E)
est prise en compte. La similaritÃ© numÃ©rique considÃ©rÃ©e dans nos programmes est celle Qg
pour la classification de lâ€™ensembleA des variables descriptives et celle duale correspondante,
obtenue aprÃ¨s (11), pour la classification de lâ€™ensemble O des objets.
RNTI - G - 21
Classification par la vraisemblance des liens relationnels
Le point de dÃ©part consiste en la dÃ©finition de lâ€™indice â€œbrutâ€
s(Q, pi) =
âˆ‘
{x,y}âˆˆR(pi)
Q(x, y) (16)
qui reprÃ©sente la somme des similaritÃ©s des paires rÃ©unies par la partition pi. Introduisons
ici lâ€™ensemble P(n; t(pi)) qui est lâ€™ensemble des partitions de E de mÃªme type t(pi) (i.e. ayant
la mÃªme famille de cardinaux de classes) que la partition donnÃ©e pi. Lâ€™hypothÃ¨se dâ€™absence de
liaison associe Ã  la partition observÃ©e pi une partition alÃ©atoire piâˆ— qui reprÃ©sente un Ã©lÃ©ment
alÃ©atoire dans lâ€™ensemble P(n; t(pi)) muni dâ€™une probabilitÃ© uniformÃ©ment rÃ©partie. Dans ces
conditions, lâ€™indice brut alÃ©atoire s(Q, piâˆ—) suit asymptotiquement (lâ€™approximation est excel-
lente) une loi normale dont la moyenneM et la variance V sont calculÃ©es mathÃ©matiquement.
Le critÃ¨re (coefficient dâ€™association) est alors calculÃ© par la formule :
C(Q, pi) =
s(Q, pi)âˆ’Mâˆš
V
(17)
Lorsque ce critÃ¨re est appliquÃ© Ã  la suite dÃ©croissante en finesse, de lâ€™arbre de classifi-
cation, nous lâ€™appelons â€œStatistique globale des niveauxâ€. PrÃ©cisÃ©ment, nous considÃ©rons la
distribution suivante de la Statistique globale des niveaux :{
C(Q, pii)|1 â‰¤ i â‰¤ l
}
(18)
De la sorte un niveau â€œsignificatifâ€ correspond Ã  un maximum local de cette distribution.
Câ€™est de cette faÃ§on que nous avons repÃ©rÃ© le niveau 5 de lâ€™arbre de classification sur A de la
figure 15, ainsi que celui 9 de la figure 16 sur O. Nous avons pu exprimer que le niveau 6 du
premier arbre restait intÃ©ressant, parce que la valeur du critÃ¨re C(Q, pi) gardait une certaine
force. Le graphique de gauche de la figure 17 donne dans un cas rÃ©el, le diagramme de la dis-
tribution le long de la suite des niveaux, de la Statistique globale des niveaux. Chaque bÃ¢tonnet
vertical est associÃ© Ã  un niveau. Sa hauteur est proportionnelle Ã  la valeur de cette statistique.
Les niveaux â€œsignificatifsâ€ sont marquÃ©s Ã  la base par une âˆ—. Ces niveaux permettent, pour un
ordre donnÃ© du nombre de classes de faire le bon choix dâ€™une partition de lâ€™ensemble organisÃ©.
Maintenant, relativement Ã  deux niveaux consÃ©cutifs i âˆ’ 1 et i, on associe le taux dâ€™ac-
croissement Statistique globale des niveaux ; câ€™est-Ã -dire :
Ï„(Q, pii) = C(Q, pii)âˆ’ C(Q, piiâˆ’1) , 1 â‰¤ i â‰¤ l,
Ï„(Q, pii) dÃ©finit la Statistique locale attachÃ©e au niveau i. Lâ€™importance de ce taux tÃ©moigne
du â€œgain en cohÃ©renceâ€ obtenu en passant du niveau i âˆ’ 1 au niveau i. On constate dâ€™ailleurs
dans la pratique expÃ©rimentale que ce taux augmente graduellement dÃ¨s lors quâ€™une classe
en cours de formation se confirme. Ã€ lâ€™achÃ¨vement Ã  un niveau de synthÃ¨se dâ€™une classe, la
valeur de ce taux retombe. Il est donc naturel de considÃ©rer la distribution le long de la suite
des niveaux de la Statistique locale des niveaux :{
Ï„(Q, pii)|1 â‰¤ i â‰¤ l
}
(19)
Dans ces conditions, un noeud â€œsignificatifâ€ est dÃ©fini par un maximum local de cette
distribution. Le graphique de droite de la figure 17 donne la dsitribution de la Statistique locale
RNTI - G - 22
I.C. Lerman
des niveaux associÃ©e Ã  celle globale figurÃ©e Ã  gauche. Lâ€™amplitude du bÃ¢tonnet vertical pour
le niveau i est proportionelle Ã  la valeur Ï„(Q, pii). Les noeuds â€œsignificatifsâ€ sont marquÃ©s Ã  la
base par une âˆ—. Les noeuds et niveaux â€œsignificatifsâ€ permettent une interprÃ©tation dynamique
et ascendante (des feuilles vers la racine) de lâ€™arbre de classification.
niveau
niveau
FIG. 17 â€“ Distributions des statistiques â€œglobaleâ€ (en haut Ã  gauche) et â€œlocaleâ€ (en bas et Ã 
droite) des niveaux.
4.5 Croisement entre deux classifications duales
Nous en sommes maintenant - dans le cadre de lâ€™AV LR - au niveau du paragraphe 2.2.2 et
en particulier de la figure 5 de lâ€™introduction gÃ©nÃ©rale. ConsidÃ©rons dans le cadre de lâ€™exemple
traitÃ© le couple dâ€™arbres de classification sur A et sur O (voir FIG. 15 et FIG. 16). Retenons la
partition la plus â€œsignificativeâ€ sur A de niveau 5. Cette derniÃ¨re est en 3 classes et peut Ãªtre
notÃ©e {A1, A2, A3}. De mÃªme, retenons la partition la plus â€œsignificativeâ€ sur O de niveau 9,
elle est en deux classes et peut Ãªtre notÃ©e {C1, C2}. Le croisement de ces deux partitions donne
le tableau de la figure 18.
Ce tableau est obtenu par permutation des lignes et des colonnes du tableau initial des
donnÃ©es (voir FIG. 9), conformÃ©ment au couple de classifications retenues. Les cases noircies
sont celles qui contiennent la valeur logique 1 (attribut Ã  V RAI). On constate que la classe
C1 se rÃ©fÃ¨re dans une forte mesure Ã  la classe A3 et sâ€™oppose Ã  la classe A1. La rÃ©fÃ©rence Ã 
la classe A2 est plus partielle. Au contraire, la classe C2 se rÃ©fÃ¨re Ã  la classe A1 et sâ€™oppose Ã 
la classe A3. LÃ  encore, C2 se rÃ©fÃ¨re de faÃ§on partielle mais diffÃ©rente de celle de C1, Ã  A2.
On peut certes considÃ©rer la partition de niveau 6 de lâ€™arbre sur A oÃ¹ la partition obtenue est
RNTI - G - 23
Classification par la vraisemblance des liens relationnels
C1
C2
A1 A2 A3
A
O
a1 a2 a3a5 a6 a4 a7 a8
o1
o2
o8
o4
o10
o6
o9
o3
o7
o5
FIG. 18 â€“ Croisement entre deux partitions significatives duales.
{A1, A2 âˆª A3}. Dans ce cas, C1 se rÃ©fÃ¨re Ã  A2 âˆª A3 et C2 se rÃ©fÃ¨re Ã  A1 ; mais, lâ€™opposition
de C2 par rapport Ã  A2 âˆªA3 est moins nette que celle de C2 par rapport Ã  A3.
Reprenons une des deux questions posÃ©es Ã  la fin du paragraphe 2.2.2 Ã  savoir : sâ€™il sâ€™agit
dâ€™un â€œgrandâ€ tableau de donnÃ©es boolÃ©ennes, comment faire ? Ã€ cet Ã©gard, nous avons bien
une solution qui a Ã©tÃ© de nombreuses fois expÃ©rimentÃ©e avec beaucoup dâ€™intÃ©rÃªt. Elle relÃ¨ve
dâ€™un dÃ©veloppement que nous avons effectuÃ© de la thÃ©orie du Ï‡2 attachÃ© Ã  un tableau de contin-
gence. Ce dÃ©veloppement nous permet de croiser deux classes dâ€™attributs boolÃ©ens A et B qui
sont logiquement indÃ©pendants ; mais, bien sÃ»r, statistiquement dÃ©pendants. Plus prÃ©cisÃ©ment,
A âˆ© B = âˆ…, dâ€™autre part, dans A âˆª B, il nâ€™y a pas un attribut et sa nÃ©gation. La notion dâ€™at-
tribut boolÃ©en Ã  V RAI chez un objet donnÃ© x est remplacÃ©e, relativement Ã  un ensemble A
dâ€™attributs boolÃ©ens (logiquement indÃ©pendants), par la proportion dans A dâ€™attributs boolÃ©ens
Ã  V RAI chez x. Nous noterons Î¦A(x) cette proportion. Elle sâ€™Ã©crit :
Î¦A(x) =
1
c(A)
âˆ‘
aâˆˆA
Î¦a(x)
oÃ¹ Î¦a(x) = 1( resp. 0) si lâ€™attribut a est Ã  V RAI (resp., FAUX) chez x. Dâ€™autre part,
c(A) dÃ©signe le cardinal de A.
Associons Ã  lâ€™ensemble A dâ€™attributs lâ€™ensemble AÂ¯ des attributs obtenus en remplaÃ§ant
chaque attribut de A par sa nÃ©gation. On a dans ces conditions :
Î¦AÂ¯(x) =
1
c(A)
âˆ‘
aâˆˆA
Î¦aÂ¯(x)
On a alors lâ€™Ã©quation de â€œcohÃ©renceâ€ :
RNTI - G - 24
I.C. Lerman
Î¦A(x) + Î¦AÂ¯(x) = 1 (20)
Relativement au couple (A,B) de classes dâ€™attributs boolÃ©ens tel que nous venons de le dÃ©-
crire, la dÃ©marche de lâ€™AV LR est toujours la mÃªme. Nous introduisons lâ€™indice brut Î½(A,B)
suivant :
Î½(A,B) = Î½(A âˆ§B) =
âˆ‘
xâˆˆO
Î¦A(x)Ã— Î¦B(x) (21)
Maintenant, on considÃ¨re lâ€™association
(A,B) âˆ’â†’ (Aâˆ—, Bâˆ—)
oÃ¹ au couple (A,B) de classes dâ€™attributs boolÃ©ens on associe un couple (Aâˆ—, Bâˆ—) de
classes dâ€™attributs boolÃ©ens mutuellements indÃ©pendants dans une hypothÃ¨se dâ€™absence de liai-
son adÃ©quate. Câ€™est lâ€™indice centrÃ© et rÃ©duit
Ï‡(A,B) =
1âˆš
nâˆ’ 1
(
Î½(A âˆ§B)âˆ’ E [Î½(Aâˆ— âˆ§Bâˆ—)])/âˆšvar[Î½(Aâˆ— âˆ§Bâˆ—)] (22)
qui dÃ©finit lâ€™intensitÃ© de lâ€™association entre les deux classes dâ€™attributs A et B.
Relativement Ã  lâ€™exemple ci-dessus introduisons les attributs boolÃ©ens c1 et c2, oÃ¹ c1 (resp.,
c2) dÃ©finit la fonction indicatrice de la classe C1 (rep., C2). c2 est lâ€™attribut boolÃ©en opposÃ© Ã 
c1. Le tableau des coefficients
{Ï‡(ci, Aj)|i = 1, 2, 1 â‰¤ j â‰¤ 3}
est donnÃ© dans le tableau de la figure 19. Ce tableau synthÃ©tise bien les conclusions que
nous avons dÃ©jÃ  apportÃ©es.
De tels coefficients peuvent aisÃ©ment Ãªtre Ã©tendus dans le cas oÃ¹ les attributs sont quan-
titatifs. Cependant la mÃ©thode sâ€™applique dans le cas trÃ¨s gÃ©nÃ©ral de variables relationnelles,
conformÃ©ment au schÃ©ma de la figue 20. La question se pose alors de savoir comment faire
correspondre les deux arbres de classification duaux surA etO (Croiser(CAHA,CAHB)).
Dâ€™autre part, dans la mesure oÃ¹ il peut Ãªtre rÃ©alisÃ© sous une certaine forme, quel type dâ€™in-
terprÃ©tation peut-on tirer dâ€™un tel croisement. Ã€ cet Ã©gard, des coefficients que nous avons
expÃ©rimentÃ©s avec beaucoup dâ€™intÃ©rÃªt entre dâ€™une part une variable relationnelle quelconque
et dâ€™autre part, une classification sur O ou une classe de O, joueront certainement un rÃ´le
important.
5 Les grands types mathÃ©matico-logiques de donnÃ©es
Selon notre point de vue les grands types mathÃ©matico-logiques de donnÃ©es se formalisent
au moyen de deux systÃ¨mes que nous noterons T et S. Nous allons les prÃ©senter ci-dessous en
cherchant Ã  les illustrer.
RNTI - G - 25
Classification par la vraisemblance des liens relationnels
 0.833
+0.833 0.833
+0.833
+0.143
 0.143
A1 A2 A3
c1
c2
FIG. 19 â€“ Croisement statistique entre deux partitions duales.
E  A;
CAHA E.CAH;
E  O( resp., E  C);
CAHB  E.CAH;
Croiser(CAHA,CAHB)
FIG. 20 â€“ SchÃ©ma gÃ©nÃ©ral de la biclassification ascendante hiÃ©rarchique.
5.1 Le systÃ¨me T
T est un systÃ¨me de Tarsky. Il se prÃ©sente sous la forme :
T = (O;R1, R2, ..., Rj , ..., Rp) (23)
oÃ¹ O est un ensemble dâ€™objets Ã©lÃ©mentaires et oÃ¹ R1, R2, ..., Rj , ..., Rp sont p relations
dâ€™aritÃ©s respectives quelconques. En analyse des donnÃ©es la relation Rj se trouve dÃ©finie par
le j-Ã¨me attribut aj , 1 â‰¤ j â‰¤ p. CommenÃ§ons par nous rÃ©fÃ©rer au tableau des donnÃ©es du
paragraphe 2.2.1 et donnons quelques exemples typiques. Dans le cas dâ€™un attribut boolÃ©en
aj , la relation Rj est unaire et lâ€™attribut est reprÃ©sentÃ© au niveau de O de faÃ§on ensembliste
(voir paragraphe 4.2.1). Rj est toujours une relation unaire mais valuÃ©e (valuation surO) dans
le cas dâ€™un attribut quantitatif-numÃ©rique. Dans le cas oÃ¹ lâ€™attribut aj est qualitatif nominal
Rj est une relation binaire symÃ©trique quâ€™on reprÃ©sente par un sous ensemble au niveau de
lâ€™ensemble P2(O) des parties Ã  deux Ã©lÃ©ments de O. Si aj est un attribut qualitatif ordinal
la reprÃ©sentation ensembliste est au niveau du produit cartÃ©sien O Ã— O. Si aj est un attribut
qualitatif prÃ©ordonnance, la reprÃ©sentation est au niveau de lâ€™ensemble des couples de couples
(O)2 Ã— (O)2 ou, si lâ€™on veut, de O4. Cependant, un codage en termes dâ€™une valuation sur O2
est le plus souvent proposÃ© pour des raisons de simplification de la complexitÃ©. La variable
taxinomique est un cas particulier de la variable prÃ©ordonnance. Une variable qualitative aj
RNTI - G - 26
I.C. Lerman
dont lâ€™ensemble des valeurs, codÃ© par {1, 2, ..., i, ..., vj}, est muni dâ€™un graphe valuÃ© de si-
milaritÃ©, peut Ãªtre interprÃ©tÃ©e, sous un certain point de vue, comme une gÃ©nÃ©ralisation dâ€™une
variable qualitative prÃ©ordonnance. Une telle variable dÃ©finit un graphe valuÃ© sur lâ€™ensemble
O des objets. Une telle variable peut Ãªtre reprÃ©sentÃ©e par la j-Ã¨me colonne du tableau des
donnÃ©es. Cette derniÃ¨re comprendra des codes issus de {1, 2, ..., i, ..., vj}. Il importe dans ces
conditions, dâ€™avoir sÃ©parÃ©ment la matrice donnant le graphe valuÃ© sur {1, 2, ..., i, ..., vj}.
Un cas dâ€™importance est celui oÃ¹ aj dÃ©finit directement un graphe valuÃ© sur lâ€™ensemble
O des objets. Le support en termes dâ€™une colonne du tableau des donnÃ©es pour reprÃ©senter
aj , ne peut plus alors se conÃ§evoir. NÃ©anmoins, le problÃ¨me bien connu de la classification
dâ€™un ensemble de graphes valuÃ©s sur un ensemble dâ€™objets, peut Ãªtre abordÃ©, dans le cadre de
lâ€™AV LR, comme la classification dâ€™un ensemble de variables relationnelles.
Maintenant, il existe des situations oÃ¹ le tableau des donnÃ©es ne peut pas directement re-
prÃ©senter lâ€™information descriptive. Il sâ€™agit notamment des donnÃ©es de type â€œsÃ©quencesâ€ de
longueurs inÃ©gales (e.g. sÃ©quences gÃ©nÃ©tiques). On ne pourra passer Ã  la description au moyen
dâ€™un tableau de donnÃ©es quâ€™aprÃ¨s avoir dÃ©fini le mÃªme ensemble de variables de description
pour lâ€™ensemble des sÃ©quences. Câ€™est ce que font la grande majoritÃ© des mÃ©thodes.
Un dernier point concerne le mÃ©lange des types de variables dans le cadre de la classi-
fication AV LR. Il sâ€™agit plus prÃ©cisÃ©ment du cas oÃ¹ les diverses relations Rj , 1 â‰¤ j â‰¤ p ne
sont pas de mÃªme aritÃ©. La classification de lâ€™ensemble O des objets est formellement indiffÃ©-
rente Ã  ce mÃ©lange. Toutefois, il importe que les diffÃ©rentes variables correspondent au mÃªme
ordre de finesse structurelle et statistique. Il est ainsi difficile de traiter sur le mÃªme plan un
attribut boolÃ©en et un attribut taxinomique trÃ¨s structurÃ©. Pour ce qui est de la classification
AV LR des variables de description, il importe quâ€™elles soient toutes dâ€™un mÃªme type. Ã€ cet
Ã©gard et dans les cas classiques et assez gÃ©nÃ©ralement, oÃ¹ les variables induisent sur O des
relations soit unaires, soit binaires, on peut de faÃ§on naturelle dÃ©terminer un codage uniforme
en termes de variables prÃ©ordonnances ou graphes valuÃ©s (thÃ¨se M. Ouali-Allah).
5.2 Le systÃ¨me S
Le systÃ¨me S se prÃ©sente sous la forme :
S = (C; distC(R1), distC(R2), ..., distC(Rj), ..., distC(Rp)) (24)
C est un ensemble de catÃ©gories (on dit encore â€œconceptsâ€). R1, R2, ..., Rj , ... et Rp sont
comme ci-dessus, p relations dâ€™aritÃ©s respectives quelconques, sur O. distC(Rj) reprÃ©sente la
famille des distributions statistiques de Rj sur chacune des catÃ©gories c de C.
Pour se rendre compte du degrÃ© de gÃ©nÃ©ralitÃ© de ce systÃ¨me, considÃ©rons quelques types
de donnÃ©es classiques qui sâ€™y inscrivent. Un tableau de contingence constitue un cas trÃ¨s par-
ticulier de ce systÃ¨me. Il se met sous la forme :
(C; distC(R))
RNTI - G - 27
Classification par la vraisemblance des liens relationnels
oÃ¹ R est une relation dâ€™Ã©quivalence associÃ©e Ã  une variable qualitative nominale. Mainte-
nant, une juxtaposition â€œhorizontaleâ€ de tableaux de contingence sâ€™exprime au moyen de :(C; distC(R1), distC(R2), ..., distC(Rj), ..., distC(Rp))
oÃ¹ R1, R2, ..., Rj , ... et Rp sont p relations dâ€™Ã©quivalence respectivement associÃ©es Ã  p
variables qualitatives nominales. Plus gÃ©nÃ©ralement, les donnÃ©es de type â€œhistogrammesâ€ oÃ¹
les pieds dâ€™un mÃªme histogramme sont munis dâ€™une relation, correspondent Ã  ce systÃ¨me.
Signalons pour terminer que cette formalisation mathÃ©matico-logique des donnÃ©es en les
deux systÃ¨mes T et S - qui nous Ã©tait apparue vers la fin des annÃ©es 80 - intÃ¨gre bien les don-
nÃ©es dites â€œsymboliquesâ€ et que nous prÃ©fÃ©rons appeler â€œdonnÃ©es de connaissaceâ€ (â€œknowledge
dataâ€, en anglais). AV LV R traite efficacement ces donnÃ©es.
6 Logiciels et Applications
6.1 Les programmes CHAVLh, AVARE et LLAhclust
Nous allons mentionner succintement les produits logiciels qui ont Ã©tÃ© Ã©laborÃ©s pour la
mise en oeuvre de la mÃ©thodologie AV LR. Ces programmes ont Ã©tÃ© Ã©crits en Fortran 77,
conformÃ©ment aux normes rigoureuses de Modulad Ã©tablies par Henri Leredde (maÃ®tre de
confÃ©rences Ã  lâ€™UniversitÃ© de Paris Nord).
Le programme le plus important est CHAV Lh (Classification HiÃ©rarchique par Analyse
de la Vraisemblance des Liens en cas de donnÃ©es hÃ©tÃ©rogÃ¨ne) (P. Peter, H. Leredde et I-C.
Lerman). La derniÃ¨re version de ce programme a Ã©tÃ© dÃ©posÃ©e lâ€™Agence de Protection des Pro-
grammes (APP) en dÃ©cembre 2005. Philippe Peter (maÃ®tre de confÃ©rences Ã  lâ€™Ã‰cole Polytech-
nique de lâ€™UniversitÃ© de Nantes) a Ã©tÃ© lâ€™artisan de cette derniÃ¨re version. Il a Ã©tÃ© installÃ© par le
projet Symbiose sur la plateforme logicielle de la gÃ©nopole Ouest. Les structures de donnÃ©es
prises en compte sont les suivantes :
1. NumÃ©riques-Quantitatives
2. BoolÃ©ennes
3. Qualitatives nominales
4. Qualitatives ordinales
5. Qualitatives prÃ©ordonnances ou graphes valuÃ©s binaires
6. Juxtaposition â€œhorizontaleâ€ de tableaux de contingence
7. Tableau de dissimilaritÃ©s fourni par lâ€™expert
Le programme permet la classification dâ€™un ensemble O dâ€™objets dÃ©crits par des variables
toutes dâ€™un mÃªme type ou par des variables pouvant Ãªtre de types respectifs diffÃ©rents.
Par ailleurs, le programme permet la classification de lâ€™ensemble V des variables de des-
cription, toutes dâ€™un mÃªme type. Il peut sâ€™agir de variables numÃ©riques, boolÃ©ennes, qualita-
tives nominales ou qualitatives ordinales.
RNTI - G - 28
I.C. Lerman
Un deuxiÃ¨me programme important AV ARE (Association entre VAriables qualitatives
pREordonnance) a Ã©tÃ© Ã©laborÃ© par M. Ouali-Allah dans le cadre de sa thÃ¨se. Il permet la clas-
sification AV LR dâ€™un ensemble de variables qualitatives de diffÃ©rents types codÃ©es en termes
de prÃ©ordonnances. Rappelons Ã  cet Ã©gard que la variable taxinomique reprÃ©sente un cas trÃ¨s
spÃ©cifique dâ€™une variable prÃ©ordonnance.
Signalons ici quâ€™une version ergonomique et donc simplifiÃ©e du programme CHAV Lh
a Ã©tÃ© implantÃ©e (juillet 2007) dans lâ€™environnement logiciel dit R (I. Kojadinovic (Ã‰cole Po-
lytechnique de lâ€™UniversitÃ© de Nantes), I.C. Lerman et P. Peter). Bien que ne comportant pas
certaines options de CHAV Lh, cette version reste assez riche. Le programme est intitulÃ©
LLAhclust (Likelihood Linkage Analysis hierarchical clustering)
6.2 Quelques applications rÃ©centes dâ€™envergure
Nous nous contenterons de mentionner les thÃ©matiques dans lesquelles sâ€™inscrivent ces
applications et les rÃ©fÃ©rences des travaux menÃ©s dans diffÃ©rents projets ; mais oÃ¹, Ã  chaque
fois, la mÃ©thode AV LR et donc le programme CHAV Lh a jouÃ© un rÃ´le essentiel.
6.2.1 Simulation du comportement de lâ€™exÃ©cution de â€œtrÃ¨s grosâ€ programmes Ã  partir
dâ€™un Ã©chantillonnage â€œreprÃ©sentatifâ€
T. Lafage ; â€œÃ‰tude, rÃ©alisation et application dâ€™une plate-forme de collecte
de traces dâ€™exÃ©cution de programmesâ€, ThÃ¨se de doctorat, UniversitÃ© de Rennes
1, DÃ©cembre 2000.
6.2.2 DÃ©termination de classes sÃ©mantiques dans le Traitement Automatique de Langues
Naturelles
M. Rossignol ; â€œAcquisition automatique de lexiques sÃ©mantiques pour la
recherche dâ€™informationâ€, ThÃ¨se de doctorat, UniversitÃ© de Rennes 1, Octobre
2005.
6.2.3 Correspondance entre profils gÃ©notypiques et profils phÃ©notypiques dans lâ€™hÃ©mo-
chromatose
I.-C. Lerman ; â€œCoefficient numÃ©rique gÃ©nÃ©ral de discrimination de classes
dâ€™objets par des variables de types quelconques. Application Ã  des donnÃ©es
gÃ©notypiquesâ€, Revue de Statistique AppliquÃ©e, 2006, vol. 2, pp. 33-63.
RNTI - G - 29
Classification par la vraisemblance des liens relationnels
6.2.4 Structuration dâ€™une famille spÃ©cifique de sÃ©quences dâ€™ADN
S. Tempel ; â€œDynamique des hÃ©litrons dans le gÃ©nÃ´me dâ€™Arabidopsis tha-
liana : dÃ©veloppement de nouvelles stratÃ©gies dâ€™analyse des Ã©lÃ©ments transpo-
sablesâ€, ThÃ¨se de doctorat, UniversitÃ© de Rennes 1, juin 2007.
6.2.5 Organisation dâ€™espÃ¨ces de â€œphlÃ©botomesâ€ pour une description biologique trÃ¨s
complexe
I.-C. Lerman, P. Peter ; â€œRepresentation of Concept Description by Multiva-
lued Taxonomic Preordonance Variablesâ€, in : Selected Contributions in Data
Analysis, and Knowledge organization, P. Brito, P. Bertrand, G. Cucumel, F.
de Carvalho (editors), Studies in Classification, Data Analysis, and Knowledge
organization, Springer, 2007, pp. 271-284.
6.2.6 Segmentation dâ€™images numÃ©risÃ©s
I.-C. Lerman, K. Bachar ; â€œComparaison de deux critÃ¨res en Classification
Ascendante HiÃ©rarchique sous contrainte de contiguÃ¯tÃ©. Application en ima-
gerie numÃ©riqueâ€, Journal de la SociÃ©tÃ© FranÃ§aise de Statistique et Revue de
Statistique AppliquÃ©e, tome 149, nâ—¦ 2, 2008, pp. 45-74.
7 RÃ©fÃ©rences importantes pour le fondement et lâ€™Ã©laboration
de la mÃ©thodologie
Nous organisons ces rÃ©fÃ©rences par thÃ¨mes. Ã€ lâ€™intÃ©rieur de chacun des
thÃ¨mes, les rÃ©fÃ©rences seront ordonnÃ©es chronologiquement. Elles concernent
nos publications ainsi que celles de chercheurs de notre environnement le plus
immÃ©diat.
7.1 RÃ©fÃ©rences gÃ©nÃ©rales
I.C. Lerman ;Classification et Analyse Ordinale des DonnÃ©es, Dunod, Paris,
1981.
I.C. Lerman ; Foundations of the Likelihood Linkage Analysis (LLA) Clas-
sification Method, Stochastic Models and Data Analysis, vol. 7, ] 1, march
1991, pp. 63-76.
I.C. Lerman ; Likelihood linkage analysis (LLA) classification method (Around
an example treated by hand) Biochimie 75, Elsevier editions, 1993, pp. 379-
397.
RNTI - G - 30
I.C. Lerman
7.2 Coefficients dâ€™association (de similaritÃ©) entre variables relationnelles
I.C. Lerman, R. Gras, H. Rostam ; Ã‰laboration et Ã©valuation dâ€™un indice
dâ€™implication pour des donnÃ©es binaires I : Revue MathÃ©matiques et Sciences
Humaines, 19Ã¨me annÃ©e, nâ—¦ 74, 1981, pp. 5-35, II : Revue MathÃ©matiques et
Sciences Humaines, 19Ã¨me annÃ©e, nâ—¦ 75, 1981, pp. 5-47.
I.C. Lerman ; Association entre variables qualitatives ordinales nettes ou
floues, Statistique et Analyse des DonnÃ©es, 1983, vol. 8 nâ—¦ 7, pp. 41-73.
I.C. Lerman ; Indices dâ€™association partielle entre variables qualitatives no-
minales, RAIRO sÃ©rie verte, vol. 17, nâ—¦ 3, aoÃ»t 1983, pp. 213-259.
I.C. Lerman ; Indices dâ€™association partielle entre variables qualitatives or-
dinales, Publications Institut Statistique de Paris, XXVIII, fasc. 1, 2, 1983, pp.
7-46.
I.C. Lerman ; Justification et validitÃ© statistique dâ€™une Ã©chelle [0,1] de frÃ©-
quence mathÃ©matique pour une structure de proximitÃ© sur un ensemble de va-
riables observÃ©es, Publications Institut Statistique UniversitÃ©s de Paris, XXIX,
fasc. 3-4, 1984, pp. 27-57.
I.C. Lerman ; Comparing partitions. Mathematical and Statistical aspects,
1Ã¨re confÃ©rence internationale des fÃ©dÃ©rations des sociÃ©tÃ©s de classification.
Aix-la-Chapelle, juin 1987, in Classification and related methods of data ana-
lysis, Edited by H. H. Bock, North Holland, 1988, pp. 121-132.
M. Ouali-Allah ; Analyse en prÃ©ordonnance des donnÃ©es qualitatives. Ap-
plication aux donnÃ©es numÃ©riques et symboliques, thÃ¨se de doctorat de lâ€™Uni-
versitÃ© de Rennes 1, dÃ©cembre 1991.
F. DaudÃ© ; Analyse et justification de la notion de ressemblance entre va-
riables qualitatives dans lâ€™optique de la classification hiÃ©rarchique par AVL,
thÃ¨se de doctorat de lâ€™UniversitÃ© de Rennes 1, juin 1992.
I.C. Lerman ; Conception et analyse de la forme limite dâ€™une famille de
coefficients statistiques dâ€™association entre variables relationnelles, I : MathÃ©-
matique (, Informatique) et Sciences Humaines, 30iÃ¨me annÃ©e, nâ—¦ 118, 1992,
pp. 35-52, II : MathÃ©matique (, Informatique) et Sciences Humaines, 30iÃ¨me
annÃ©e, nâ—¦ 119, 1992, pp. 75-100, Paris.
I.C. Lerman ; Comparing classification tree structures : a special case of
comparing q-ary relations, RAIRO-Operations Research, vol. 33, 1999, sept.,
pp. 339-365.
I.C. Lerman, F. Rouxel ; Comparing classification tree structures : a spe-
cial case of comparing q-ary relations II, RAIRO-Operations Research, vol. 34,
2000, pp. 251-281.
RNTI - G - 31
Classification par la vraisemblance des liens relationnels
I.C. Lerman ; Comparing taxonomic data, Revue MathÃ©matiques et Sciences
Humaines, 38-Ã¨me annÃ©e, nâ—¦ 1510, 2000, pp. 37-51.
I.C. Lerman, J. AzÃ© ; A New Probabilistic Measure of Interestingness for
Association Rules, Based on the Likelihood of the Link, in Quality Measures
in Data Mining. Studies in Computational Intelligence. F. Guillet and H. Ha-
milton (eds). 2006. Springer. pp. 207-236.
R. Gras, P. Kuntz ; An overview of the Statistical Implicative Analysis (SIA)
development, in R. Gras, F. Guillet, F. Spagnolo, E. Suzuki ; Statistical Impli-
cative Analysis, Studies in Computational Intelligence, nâ—¦ 27, Springer, 2008.
7.3 Indices de similaritÃ© entre objets dÃ©crits par des variables relation-
nelles de types quelconques
I.C. Lerman et Ph. Peter ; Organisation et consultation dâ€™une banque de pe-
tites annonces Ã  partir dâ€™une mÃ©thode de classification hiÃ©rarchique en paral-
lÃ¨le, Data Analysis and Informatics IV, North Holland, 1986, pp. 121-136.
Ph. Peter ;MÃ©thodes de classification hiÃ©rarchique et problÃ¨mes de structu-
ration et de recherche dâ€™informations assistÃ©e par ordinateur, thÃ¨se de doctorat
de lâ€™UniversitÃ© de Rennes 1, mars 1987.
I.C. Lerman ; Construction dâ€™un indice de similaritÃ© entre objets dÃ©crits par
des variables dâ€™un type quelconque. Application au problÃ¨me de consensus en
Classification, Revue de Statistique AppliquÃ©e, XXXV (2), 1987, pp. 39-60.
I.C. Lerman, Ph. Peter et J.L. Risler ; Matrices AV L pour la classification
et lâ€™alignement de sÃ©quences protÃ©iques, Publication Interne IRISA nâ—¦ 866,
septembre 1994, Rapport de Recherche INRIA nâ—¦ 2466.
I.C. Lerman, Ph. Peter ; Indice probabiliste de vraisemblance du lien entre
objets quelconques : analyse comparative entre deux approches, Revue de Sta-
tistique AppliquÃ©e, volume LI(1), 2003, pp. 5-35.
I.C. Lerman and Ph. Peter ; Representation of Concept Description by Mul-
tivalued Taxonomic Preordonance Variables, in Selected Contributions in Data
Analysis and Classification, Studies in Classification, Data Analysis, and Know-
ledge Organization, P. Brito, P. Bertrand, G. Cucumel and F. de Carvalho, edi-
tors, Springer 2007, pp. 271-284.
7.4 Tableaux de contingence
I.C. Lerman et B. Tallur ; Classification des Ã©lÃ©ments constitutifs dâ€™une jux-
taposition de tableaux de contingence, Revue de Statistique AppliquÃ©e, nâ—¦ 28,
3, 1980, pp. 5-28.
RNTI - G - 32
I.C. Lerman
I.C. Lerman ; InterprÃ©tation non linÃ©aire dâ€™un coefficient dâ€™association entre
modalitÃ©s dâ€™une juxtaposition de tables de contingence,MathÃ©matique et Sciences
Humaines, 21Ã¨me annÃ©e, nâ—¦ 83, 1983, pp. 5-30.
I.C. Lerman ; Analyse classificatoire dâ€™une correspondance multiple, typo-
logie et rÃ©gression, in Data Analysis and Informatics, III, E. Diday et al. (edi-
tors), North Holland, 1984, pp. 193-221.
B. Tallur ; Contribution Ã  lâ€™analyse exploratoire de tableaux de contingence
par la classification, thÃ¨se de doctorat Ã¨s sciences, UniversitÃ© de Rennes 1,
septembre 1988.
7.5 CritÃ¨res de fusion entre classes
I.C. Lerman ; Formules de rÃ©actualisation en cas dâ€™agrÃ©gations multiples,
RAIRO, sÃ©rie R.O. vol 23, nâ—¦ 2, 1989, pp. 151-163.
F. Costa Nicolau and H. Bacelar-Nicolau ; Some trends in the Classification
of Variables, inData Science, Classification, and Related Methods,C. Hayashi,
N. Ohsumi, K. Yajima, Y. Tanaka, H.-H. Bock, Y. Baba Editors, 1998, pp. 89-
98.
7.6 Niveaux et noeuds â€œsignificatifsâ€
I.C. Lerman ; Sur la signification des classes issues dâ€™une classification au-
tomatique, in Numerical Taxonomy, NATO ASI Series, vol. G1. Edited by J.
Felsenstein, Springer-Verlag (1983), pp. 179-198.
I.C. Lerman, N. Ghazzali ; What do we retain from a classification tree ? an
experiment in image coding, in Symbolic-Numeric data analysis and learning,
edited by E. Diday and Y. Lechevallier, Nova Science Publishers, Proceedings
of the conference of Versailles, september 18-20, 1991, pp. 27-42.
7.7 Croisement de classifications
I.C. Lerman ; Croisement de classifications â€œflouesâ€, Publications de lâ€™Ins-
titut de Statistique des UniversitÃ©s de Paris, 1979, XXIV, fasc. 1-2, pp. 13-46.
I.C. Lerman, M. Hardouin et T. Chantrel ; Analyse de la situation relative
entre deux classifications â€œflouesâ€, Secondes JournÃ©es Internationales Analyse
des DonnÃ©es et Informatique, inData Analysis and Informatics, North Holland,
1980, pp. 523-533.
I.C. Lerman ; Association entre variables qualitatives ordinales â€œnettesâ€ ou
â€œflouesâ€, Statistique et Analyse des DonnÃ©es, nâ—¦ 7, 1983, pp. 41-73.
RNTI - G - 33
Classification par la vraisemblance des liens relationnels
I-C. Lerman ; Coefficient numÃ©rique gÃ©nÃ©ral de discrimination de classes
dâ€™objets par des variables de types quelconques. Application Ã  des donnÃ©es
gÃ©notypiques, Revue de Statistique AppliquÃ©e, 2006, LIV (2), pp. 33-63.
7.8 Logiciels
P. Peter, H. Leredde et I.C. Lerman ; Notice du programme CHAVLh (Clas-
sification HiÃ©rarchique par Analyse de la Vraisemblance des Liens en cas de
variables hÃ©tÃ©rogÃ¨nes),DÃ©pÃ´t APP (Agence pour la Protection des Programmes)
IDDN.FR.001.240016.000.S.P.2006.000.20700, UniversitÃ© de Rennes 1, DÃ©-
cembre 2005.
M. Ouali Allah ; Programme pour le calcul de coefficients dâ€™association
entre variables relationnelles, La revue de modulad nâ—¦ 25, juin 2000, pp. 63-73.
I. Kojadinovic, I.C. Lerman and P. Peter ; LLAhclust dans R :
http ://cran.rproject.org/src/contrib/Descriptions/LLAhclust.html.
Summary
The edification of the Likelihood Linkage Relational Analysis classification
method began at the end of nineteen sixties. From that period this methodology
has been extensively developed. Many researchers and practitioners have con-
tributed to its development. Many application works on a large scale, provided
by various fields (Bioinformatics, Informatics, Social sciences, Image process-
ing, Natural language processing, ...) have been performed and then, have
validated this approach. Any logical or mathematical type of data description
can be handled in an accurate fashion. The aim of this paper consists of pre-
senting in an illustrated way the fundamental principles of this methodology.
Two conception levels are associated with these principles. The first is defined
by the mathematical representation of the data description, while the second
level is concerned by the problem of measuring the resemblances between the
mathematical structures to be compared. In our case, the description repre-
sentation will have a set theoretic and relational nature. Besides, quantifying
the resemblance will be done by means of a probabilistic similarity. The lat-
ter is established with respect to a statistical hypothesis of independence. The
following text corresponds to our oral presentation at the â€œ3-Ã¨mes JournÃ©es
ThÃ©matiques Apprentissage Artificiel et Fouille des DonnÃ©esâ€, 2008 April 8-9.
RNTI - G - 34
