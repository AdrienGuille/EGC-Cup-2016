De l’importance du pre´-traitement des donne´es pour
l’utilisation de l’infe´rence grammaticale en
Web Usage Mining
Thierry Murgue
Eurise – Universite´ Jean Monnet
23 rue du docteur Paul Michelon
42023 Saint-E´tienne Cedex 2
thierry.murgue@univ-st-etienne.fr
Re´sume´. LeWeb Usage Mining est un processus d’extraction de connais-
sance qui permet la de´tection d’un type de comportement usager sur un
site internet. Cette taˆche rele`ve de l’extraction de connaissances a` par-
tir de donne´es : plusieurs e´tapes sont ne´cessaires a` la re´alisation du pro-
cessus complet. Les donne´es brutes, utilise´es et souvent incomple`tes cor-
respondent aux requeˆtes enregistre´es par un serveur. Le pre´-traitement
ne´cessaire de ses donne´es brutes pour les rendre exploitables se situe en
amont du processus et est donc tre`s important. Nous voulons travailler sur
des mode`les structure´s, issus de l’infe´rence grammaticale. Nous de´taillons
un ensemble de techniques de traitement des donne´es brutes et l’e´valuons
sur des donne´es artificielles. Nous proposons, enfin, des expe´rimentations
mettant en e´vidence l’affectation des algorithmes classiques d’infe´rence
grammaticale par la mauvaise qualite´ des logs bruts.
1 Introduction
Le Web Usage Mining a e´te´ introduit pour la premie`re fois en 1997 (Cooley et al.
1997). Dans cet environnement, la taˆche est d’extraire de manie`re automatique la fac¸on
dont les utilisateurs naviguent sur un site web. Depuis 1995, Catledge et Pitkow ont e´tu-
die´ la manie`re de cate´goriser les comportements utilisateurs sur un site web (Catledge
1995). Le processus d’extraction de connaissance – pre´-traitement, fouille, interpre´ta-
tion – est base´ sur la disponibilite´ de donne´es fiables : divers travaux on e´te´ mene´s sur la
fac¸on de traiter les donne´es re´cupe´rables depuis un site web (Cooley et al. 1999, Pitkow
1997, Chevalier et al. 2003). Une grande majorite´ de chercheurs utilisent de manie`re
syste´matique les informations contenues dans les enregistrements du serveur (fichiers
de logs), mais ces donne´es, sous forme brute, ne sont pas comple`tes : un pre´-traitement
est donc ne´cessaire. L’e´tape suivante du Web Usage Mining consiste a` apprendre des
mode`les de comportement utilisateurs depuis ces donne´es. Ainsi, ces dernie`res anne´es,
de nombreuses me´thodes de traitement (Tanassa et al. 2004) et d’apprentissage ont
e´te´ utilise´es dans ce domaine : recherche de se´quences fre´quentes (Frias-Martinez 2002,
Gery 2003), travaux sur l’utilisation de mode`les structure´s de type chaˆıne de Markov ou
mode`le de Markov cache´ (Hmm) (Pitkow 1999, Bidel et al. 2003). Certains chercheurs
ont notamment travaille´s sur des mode`les grammaticaux : certains (Borges 1999) en uti-
lisant des n-grams, d’autres (Karampatziakis et al. 2004) en e´tudiant le comportement
- 113 - RNTI-E-5
Pre´-traitement des donne´es pour l’utilisation de l’infe´rence grammaticale en WUM
d’une me´thode classique d’infe´rence grammaticale sur des automates probabilistes.
Dans cet article, nous pre´sentons tout d’abord les deux types de proble`mes lie´s a`
l’architecture des re´seaux pouvant provoquer une corruption des donne´es. Nous pre´-
sentons dans la section 3, nos me´thodes de reconstruction des logs, en donnant des
re´sultats montrant la capacite´ de notre me´thode a` ame´liorer les donne´es initiales. Ces
expe´rimentations sont mene´es sur des donne´es ge´ne´re´es artificiellement pour posse´-
der un e´chantillon de logs de re´fe´rence. Nous pre´sentons, ensuite, nos travaux actuels
concernant un protocole d’expe´rimentations ayant pour but de faire e´merger la re´elle
ne´cessite´ de la reconstruction des logs pour l’utilisation de l’infe´rence grammaticale en
comparant cette me´thodes d’apprentissage a` de l’extraction de se´quences fre´quentes.
Enfin nous concluons sur les travaux restant a` mener.
2 Proble`mes lie´s aux fichiers de logs
La quasi-totalite´ des travaux sur le Web Usage Mining utilisent les fichiers de logs
enregistre´s par le serveur de manie`re chronologique. Le protocole Http, utilise´ dans les
navigations web, est dissyme´trique : en re`gle ge´ne´rale, un client demande une page que
lui renvoie ou non (page inexistante, droits insuffisants, . . . ) le serveur. A` chaque re´-
ponse le serveur enregistre un log fre´quemment compose´ (Luotonen 1999) de l’adresse
de la machine d’ou` provient la demande (adresse du client), un champ (souvent vide)
concernant l’identification de l’utilisateur sur la machine cliente, un champ d’authen-
tification dans le cas de requeˆte (( se´curise´e )), la date de la requeˆte, la page demande´e
(la requeˆte elle-meˆme), le type de re´ponse du serveur (re´ussi ou non), la taille de la re´-
ponse. On trouve souvent deux champs supple´mentaires : la page re´fe´rente (l’Url de la
page vue par l’utilisateur, lorsqu’il fait la requeˆte), et le type de client utilise´. Dans un
cas ide´al, ou` chaque utilisateur utilise une machine diffe´rente et ou` toutes les machines
sont relie´es directement au serveur, ce type de donne´es permet d’extraire de manie`re
line´aire des sessions ou visites utilisateur 1. Lorsqu’un utilisateur navigue sur un site
dans un but pre´cis, il effectue un ensemble de requeˆtes que l’on appellera visite ; s’il
navigue a` nouveau sur le meˆme site dans un autre but, une nouvelle visite doit eˆtre
cre´e. Il suffit, dans une premie`re approximation, de regrouper toutes les requeˆtes d’une
meˆme provenance dans une feneˆtre temporelle de´termine´e pour cre´er la visite.
L’architecture hie´rarchique des re´seaux et les faculte´s de cache et proxy rendent la
taˆche beaucoup plus difficile.
Dans (Pitkow, 97), la mise en cache est de´finie comme un me´canisme visant a`
restreindre le temps de re´cupe´ration d’une ressource en sauvegardant une copie de
celle-ci localement. Ainsi, lors de la demande d’une page :
– Soit elle n’est pas en cache : une requeˆte est effectue´e au serveur, une copie de la
re´ponse est copie´e dans le cache, et la re´ponse est envoye´e au client.
– Soit elle est en cache : la copie pre´ce´demment cre´e´e est envoye´e au client en guise
de re´ponse. Aucune requeˆte n’aboutit au serveur.
Souvent, le client (navigateur) utilise´ par l’utilisateur re´serve un espace disque de la
machine pour pouvoir stocker une quantite´ de donne´es en cache. Ce processus permet
1. ensemble se´mantique de requeˆtes d’un meˆme utilisateur
- 114 -RNTI-E-5
Murgue
d’acce´le´rer la navigation en ne redemandant au serveur que les choses non de´ja` vues. En
contrepartie, du coˆte´ du serveur, tout se passe comme si l’utilisateur ne redemandait
jamais la meˆme page, ce qui du point de vue du comportement n’est pas re´aliste.
Un proxy est une machine interme´diaire place´e entre le client et le serveur, qui
permet d’acheminer indirectement les requeˆtes de l’un vers l’autre. Le proxy, lorsqu’il
est couple´ au cache, corrompt encore plus les donne´es.
Lorsqu’il existe un cache sur le proxy, on parle de cache global : en effet, les caches
locaux permettent d’e´conomiser du trafic re´seau pour un utilisateur faisant plusieurs
fois la meˆme requeˆte, les caches globaux e´tendent ce concept a` toutes les requeˆtes
de tous les utilisateurs des machines clientes du proxy. On conside`re qu’un cache est
performant a` partir de 15% de de´bit sauvegarde´. Usuellement, le taux de requeˆtes
perdues par les serveurs a` causes de proxy-cache est de l’ordre de 40%. Dans la re´alite´,
les re´seaux sont conc¸us de manie`res hie´rarchiques, et il y a souvent plusieurs machines
interme´diaires entre clients et serveur.
En re´sume´, deux types de donne´es peuvent eˆtre perdus : les requeˆtes manquantes
car de´ja` en cache, les adresses de provenance errone´e car correspondant a` des requeˆtes
passe´es via un proxy.
Sur une suite de requeˆtes simples nous pouvons perdre une grande partie des donne´es
initiales ; nous pre´sentons dans la section suivante la reconstruction des donne´es.
3 Reconstruction des donne´es
Plusieurs travaux parlent du pre´-traitement de donne´es et de la reconstruction des
donne´es manquantes. Pour aller plus loin, nous avons voulu estimer empiriquement
quelle quantite´ de donne´es notre reconstruction permettait de retrouver.
Pour cela, nous nous plac¸ons du coˆte´ serveur et nous supposons la connaissance du
site web. Nous mode´lisons ce dernier par un graphe ou` les nœuds repre´sentent les pages
et les arcs, les liens hypertextes. L’analyse des fichiers de logs se fait chronologiquement,
le but e´tant pour chaque enregistrement, de l’associer a` la bonne visite utilisateur. Pour
cela, nous faisons des distinctions sur les adresses de provenance de la requeˆte, sur le
type de client utilise´ (s’il est renseigne´), et enfin nous de´tectons les incohe´rences dans
la navigation. La de´tection de ces incohe´rences requiert la me´morisation de l’historique
de chaque visite en cours. Soit s une visite ouverte, on note hs(i) la iie´me page dans
l’historique de la visite s, i.e. h0(1) correspond a` la dernie`re page vue par l’utilisateur
associe´ a` la visite 0. Nous pre´sentons avec la fonction Associe comment
1. nous associons l’enregistrement courant h(0) a` la visite la mieux adapte´e ;
2. nous corrigeons le fichier de logs.
Pour re´sumer, nous associons au log courant la visite la plus adapte´e, soit ayant
un lien direct depuis la dernie`re page vue, soit dont la remonte´e dans l’historique pour
trouver une page compatible est la plus courte et dans ce dernier cas, nous ajoutons la
liste des logs de l’historique comme logs manquants.
Pour pouvoir e´valuer la pertinence de la reconstruction des logs avec cette me´thode,
nous avons mene´ une se´rie d’expe´rimentations sur des donne´es artificielles. Des pages
d’un site web, nous ge´ne´rons des navigations virtuelles. Nous se´lectionnons une page
- 115 - RNTI-E-5
Pre´-traitement des donne´es pour l’utilisation de l’infe´rence grammaticale en WUM
Fonction Associe.
Data : h(0) //le log a` classer
pour chaque k ∈ visitess ouvertes faire
si hk(1)→ h(0) alors
Associe(k, h(0)) /*associe h(0) avec la visite k */
finsi
finprch
min historique = ∞;
pour chaque k ∈ visitess ouvertes faire
pour 2 ≤ i ≤ |hk| faire
si hk(i)→ h(0) alors
si min historique > i alors
min historique = i ;
min visite = k ; break;
finsi
finsi
finpour
finprch
si min historique <∞ alors
/*inconsistance trouve´e : il faut ajouter ce qui manque */
Ajouter_Logs_Manquants(min visite, min historique);
Associe(min visite,h(0));
sinon
/*pas d’historique pour se raccrocher : nouvelle visite */
l = Nouvelle_Visite();
Associe(l,h(0));
finsi
ale´atoirement dans le site, et d’une page initiale nous calculons un plus court chemin,
que nous adaptons ensuite avec la possibilite´ de revenir en arrie`re ou de ne pas prendre
la bonne direction. Les visites utilisateurs obtenues sont donc celles de re´fe´rence. Nous
simulons ensuite l’existence de proxy-cache sur les enregistrements pour obtenir un
fichier de logs incomplet. Nous extrayons la` encore des visites sans la de´tection des
incohe´rences. Puis nous appliquons notre me´thode sur les meˆme donne´es. Les nombres
de visites de´tecte´es lors des phases (( de´grade´e )) et (( de´grade´e et reconstruite )) sont
diffe´rents. Pour pouvoir comparer les deux ensembles de visites extraites nous calculons
la diffe´rence de visites extraites correctement, d’une part, puis apre`s re´-organisation des
logs suivant leur visite originale, nous calculons une distance d’e´dition (Levenshtein,
1966) pour comparer chaque visite individuellement. La distance que nous donnons ici
est la somme des distances pour toutes les visites. Les re´sultats sont pre´sente´s en Fig. 1.
Les expe´rimentations ont e´te´ mene´es sur 6 ensembles de logs artificiels diffe´rents.
Les deux indicateurs de qualite´ se comportent bien : en effet, le nombre de changement
de visites est plus faible dans la partie reconstruite et la distance d’e´dition entre les
visites prises deux a` deux est aussi plus faible. Ceci veut dire que la reconstruction
- 116 -RNTI-E-5
Murgue
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 0  200  400  600  800  1000  1200  1400  1600  1800  2000
D
if
fé
re
n
c
e
 #
s
e
s
s
io
n
s
Nombre de sessions originales
Données corrompues
Données reconstruites
(a) Diffe´rence #visites.
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 4500
 5000
 0  200  400  600  800  1000  1200  1400  1600  1800  2000
D
is
ta
n
c
e
 d
’é
d
it
io
n
 t
o
ta
le
Nombre de sessions originales
Données corrompues
Données reconstruites
(b) Distance d’e´dition totale.
Fig. 1 – Influence de la reconstruction des donne´es.
permet de de´tecter plus de visites, et que les visites reconstruites sont plus proches des
visites originales. Les donne´es ainsi extraites sont donc plus proches des donne´es de
re´fe´rence, ce qui implique des re´sultats meilleurs en apprentissage.
Les re´sultats pre´sente´s sont simule´s sur 10 machines dont 2 derrie`re le meˆme proxy.
En faisant augmenter ce dernier nombre, le gain en distance devient plus faible mais
reste strictement positif. Dans le meˆme temps, le gain sur le nombre de visites de´croit
mais beaucoup moins rapidement.
L’infe´rence grammaticale peut remplir la taˆche d’apprentissage dans le processus
d’extraction de connaissances, cette technique est connue pour une faible robustesse
face au bruit. Nous travaillons actuellement a` mettre en place un protocole d’expe´rimen-
tations permettant de montrer le re´el impact du bruit du aux re´seaux dans les donne´es
de type web. Des travaux (Frias-Martinez 2002, Gery 2003), utilisent des se´quences
fre´quentes comme support a` la pre´diction de nouvelles pages demande´es dans la navi-
gation d’un utilisateur. Nous voulons apprendre de notre coˆte´ des mode`les structure´s
stochastiques (automates finis de´terministes) et comparer les re´sultats obtenus avec la
me´thode d’extraction de se´quences fre´quentes largement re´pandue. Nous pensons que
les automates appris peuvent eˆtre plus pre´cis en pre´diction que les simples mode`les de
se´quences, mais sont moins robustes si les donne´es en entre´e sont peu fiables.
4 Conclusion
Nous avons pre´sente´ dans cet article une me´thode de reconstruction de donne´es
que nous avons e´value´e sur des donne´es artificielles. Les re´sultats obtenus, bien que
pre´liminaires, sont encourageants. Nous voulons continuer dans ce domaine en compa-
rant deux me´thodes d’apprentissage sur les donne´es brutes et celles reconstruites pour
montrer que l’infe´rence grammaticale peut eˆtre utilise´e sur ce type de donne´es avec de
bons re´sultats.
- 117 - RNTI-E-5
Pre´-traitement des donne´es pour l’utilisation de l’infe´rence grammaticale en WUM
Re´fe´rences
Bidel S., Lemoine L., Piat F., Artie`res T. et Gallinary P. (1999), Statistical Machine
Learning for Tracking Hypermedia User Behaviour, MLIRUM.
Borges J. et LeveneM. (1999), Data Mining of User Navigation Patterns,WEBKDD’99.
Catledge L.D. et Pitkow J.E. (1995), Characterizing Browsing Strategies in the World-
Wide Web, Computer Networks and ISDN Systems, 27(6), pp 1065–1073.
Chevalier K., Bothorel C., Corruble V. (2003), Discovering Rich Navigation Pattern on
a Web Site in Discovery Science, Proc. of 6th International Conference, DS 2003.
Cooley R., Mobasher B. et Srivastava J. (1999), Data Preparation for Mining World
Wide Web Browsing Patterns, Knowledge and Information Systems, 1(1), pp 5–32.
Cooley R., Srivastava J. et Mobasher B. (1997), WEBMining: Information and Pattern
Discovery on the WWW, Proceedings of ICTAI’97.
Frias-Martinez E. et Karamcheti V. (2002), A Prediction Model for User Access Se-
quences, WEBKDD Workshop: Web Mining for Usage Patterns and User Profiles.
Gery M. et Haddad H. (2003), Evaluation of Web Usage Mining Approaches for User’s
Next Request Prediction, Proceedings of WIDM’03.
Karampatziakis N., Paliouras G., Pierrakos D., Stamatopoulos P. (2004) Navigation
Pattern Discovery Using Grammatical Inference in Grammatical Inference: Algo-
rithms and Applications, Proc. of ICGI 2004.
Levenshtein V.I. (1966), Binary codes capable of correcting deletions, insertions and
reversals. Soviet Physics–Doklady, 6:707-710, 1966.
Luotonen A. (1995), The Common Log File Format, W3C Recommandation, http://-
www.w3.org/Daemon/User/Config/Logging.html.
Pitkow J.E. (1997), In Search of Reliable Usage Data on the WWW, Proceedings of
the Sixth International WWW Conference.
Pitkow J.E. et Pirolli P. (1999), Mining Longest Repeating Subsequences to Predict
WWW Surfing, Proc. of USITS’99.
Tanasa D. et Trousse B. (2004), Advanced Data Preprocessing for Intersites Web
Usage Mining, IEEE Intelligent Systems, Vol. 19(2):59–65.
Summary
Web Usage Mining is used to detect types of users during an internet navigation.
This task can be viewed as Knowledge Discovering process: some steps are required in
order to accomplish the whole process. Data used in this kind of task is almost always
log server files. Data pre-processing is the first step, so it is an important one because of
its outcome on the next learning step. Some works show different methods to extract
web data, and recently structured models have been used to learn users navigation
models. We propose in this paper to continue research on Web Usage Mining using
Grammatical Inference. To avoid weak robustness, we need to use reliable data. We
present here methods which process raw data and create reliable users visits. Finally,
we describe an experimental protocol in order to show why pre-processing is important
when the learning method is Grammatical Inference.
- 118 -RNTI-E-5
