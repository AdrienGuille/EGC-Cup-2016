Méthodes à noyaux appliquées aux textes structurés
Sujeevan Aseervatham, Emmanuel Viennet
Université de Paris-Nord, LIPN - UMR CNRS 7030
99, avenue Jean-Baptiste Clément
93430 Villetaneuse, France
{Prénom.Nom}@lipn.univ-paris13.fr
Résumé. Cet article ébauche un état de l’art sur l’utilisation des noyaux pour le
traitement des données structurées. Les applications modernes de la fouille de
données sont de plus en plus confrontés à des données structurées, notamment
textuelles. Les algorithmes d’apprentissage doivent donc être capables de tirer
parti des informations apportées par la structure, ce qui pose d’intéressants pro-
blèmes de représentation des données. L’une des approches possibles consiste
à utiliser les noyaux de Mercer. Ces noyaux permettent de calculer la similarité
entre deux données de type quelconque, et peuvent être utilisés par une large
gamme d’algorithmes d’apprentissage (Machines à Vecteur de Support, ACP,
Analyse Discriminante, Perceptron, etc). Nous présentons dans cet article les
principaux noyaux proposés ces dernières années pour le traitement des struc-
tures telles que les séquences, les arbres et les graphes.
1 Introduction
Les techniques d’apprentissage statistique sont généralement conçues pour travailler sur
des données vectorielles ; chaque mesure est représentée par un ensemble de données numé-
riques de taille fixe. Pendant plusieurs décennies, les recherches en statistique se sont centrées
sur des problèmes comme la normalisation des données, le traitement des valeurs manquantes,
etc. Depuis une dizaine d’années, sous la pression des applications, nous sommes confrontés à
des problème dans lesquels la structure des données porte une information essentielle : textes
en langage naturel, documents XML, séquences biologiques, analyse de scènes (images), ana-
lyse des réseaux sociaux. Pour attaquer ces problèmes, il est nécessaire trouver un moyen de
traiter l’information structurelle, par exemple en calculant une mesure de similarité entre deux
structures.
De nombreux systèmes d’apprentissage numérique des données textuelles utilisent une
représentation du texte en "sac de mot". Ce type de codage, qui a l’avantage de la simplicité,
n’utilise que les fréquences d’apparition des mots dans les documents et perd toute information
liée à l’ordre des éléments (ordre des mots, structure en paragraphes ou sections, etc).
Depuis une petite dizaine d’années, une nouvelle famille d’algorithmes d’apprentissage
basés sur la notion de noyaux, fait l’objet d’intenses recherches. Les noyaux, proposés par V.
Vapnik pour les machines à vecteur de support (SVM) (Vapnik, 1995), permettent de définir des
mesures de similarité non linéaires. En simplifiant, la fonction noyau calcule un produit scalaire
Méthodes à noyaux appliquées aux textes structurés
(utilisé commemesure de similarité) entre deux éléments à traiter. Or, de nombreuses méthodes
d’apprentissage statistique peuvent se formuler en ne recourant qu’à des calculs de produits
scalaires entre les éléments (exemples d’aprentissage ou nouveaux points). L’utilisation d’une
fonction noyau permet alors d’utiliser ces méthodes bien connues et performantes (analyse
discriminante, analyse en composantes principales, perceptron, etc) avec un traitement intégré
qui peut être non linéaire et incorporer des connaissances sur l’application (par exemple sur la
structure des données). Récemment, l’utilisation de noyaux spécifiques pour le traitement de
données textuelles structurées a commencé à faire l’objet de recherches.
Dans le cadre du projet InfoM@gic (pôle IMVN Cap Digital), nous travaillons sur l’ap-
plication des méthodes à noyaux au traitement de données textuelles structurées, et cet article
présente un bref état de l’art dans ce domaine. Nous passons en revue les principaux types de
noyaux proposés ces dernières années pour le traitement des séquences et plus généralement
des données structurées (arbres, graphes, etc).
2 Le noyau de convolution
Le noyau de convolution appelé R-noyau (Haussler, 1999) permet de définir un cadre gé-
néral pour les noyaux appliqués aux données structurées telles que les arbres et les graphes.
Les données structurées sont des objets pouvant être décomposés en sous-objets jusqu’à at-
teindre une unité atomique. L’idée du R-noyau est de calculer la similarité entre deux éléments
x et y en effectuant une décomposition de x et de y. Plus formellement, soit x un élément
appartenant à un ensemble X d’éléments structurés de même type, x peut être décomposé en
sous éléments (x1, . . . , xD) où xd peut être un élément structuré ou non appartenant à Xd. On
définit la relation binaire
R : X1 × . . .×XD → X
qui associe les parties d’un élément x à x et la relation inverse
R−1 : {x = (x1, . . . , xD)|R(x, x)}
qui retourne pour x l’ensemble de toutes les décompositions possibles. Le noyau de convolu-
tion (R-noyau) pour deux éléments x et y de X est alors :
kR(x, y) =
∑
x∈R−1(x)
∑
y∈R−1(y)
d∏
i=1
ki(xi, yi)
avec ki un noyau calculant la similarité entre les éléments xi et yi de même structure i.e.
xi, yi ∈ Xi. Il est facile de montrer que si les ki sont des noyaux valides alors kR est valide.
En effet, si ki est valide alors sa matrice de Gram est semi-définie positive et le produit et la
somme de matrices semi-définies positives sont des matrices semi-définies positives.
De plus, le R-noyau peut être défini par récurrence lorsque les parties xi d’un élément
x sont de même type de structure (xi, x ∈ X ). Le critère d’arrêt est défini pour l’élément
atomique (par exemple une feuille pour une structure arborescente).
La possibilité de décomposer le calcul de la similarité d’éléments permet de traiter aisément
des structures complexes. Toutefois, elle nécessite, en contre-partie, un temps de calcul non
S. Aseervatham et E. Viennet
négligeable. Ainsi, il est nécessaire de spécialiser ce noyau selon le type de structure afin de
réduire la complexité.
3 Les noyaux pour les séquences de caractères
Les séquences de caractères sont considérées comme faisant partie des données structurées
car d’une part une séquence peut être décomposée en sous-partie ainsi les séquences possèdent
la propriété des données structurées vue dans le paragraphe précédent et d’autre part les carac-
tères de la séquence sont ordonnés.
Les séquences sont généralement rencontrées dans les domaines liés à la bioinformatique mais
aussi dans les documents en langage naturel. En effet, on peut considérer tout un document
comme étant une séquence. Ainsi, deux documents peuvent être considérés comme proches
s’ils partagent un nombre important de sous-séquences identiques. Cette approche permet alors
de tenir compte des mots composés comme par exemple “économie” et “microéconomie” qui
ne peuvent être traités par l’approche en sac de mots. De plus, les mots composés sont très
présents dans le domaine de la chimie et les domaines connexes où il n’est pas rare d’avoir des
noms de molécules composées.
Bien que cette approche donne de meilleurs résultats que l’approche classique en sac de
mots, elle n’en reste pas moins coûteuse en temps de calcul.
3.1 Le noyau p-Spectrum
Le noyau p-spectrum (ou n-gram) (Leslie et al., 2002; Lodhi et al., 2002) est le noyau le
plus simple pour le traitement de séquences. Il permet d’évaluer le nombre de sous-séquences
contiguës de taille p (ou n) que deux documents ont en communs. Plus le nombre de sous-
séquences en commun est important et plus la similarité des deux documents sera importante.
Soit l’alphabet Σ, l’espace associé au noyau p-spectrum sera de dimension card(Σ)p. Le uème
composant du vecteur Φp(s) associé à la séquence s avec u ∈ ΣP est :
Φpu(s) = card({(v1, v2)|s = v1uv2})
Avec v1uv2 désignant la concaténation des séquences v1, u et v2.
Le noyau p-spectrum est alors :
kp(s1, s2) = 〈Φp(s1),Φp(s2)〉
La complexité de ce noyau est O(p|s1||s2|). Toutefois, il est possible de réduire cette com-
plexité à O(p×max(|s1|, |s2|)) en utilisant la programmation dynamique et des structures de
données appropriées comme les arbres de suffixes (Shawe-Taylor et Cristianini, 2004).
Le noyau p-Spectrum a été utilisé par Leslie et al. (2002) pour la classification de séquences
de protéines avec l’algorithme SVM (Séparateur à Vaste Marge). Les résultats obtenus sur la
base de données SCOP (Structural Classification of Proteins) ont montré que la classification
par SVM avec le noyau p − Spectrum donne des résultats semblables aux méthodes géné-
ratives basées sur les modèles de Markov cachés. Cependant, la méthode SVM avec le noyau
Fisher reste la plus performante pour la classification de séquences de protéines.
Méthodes à noyaux appliquées aux textes structurés
3.2 Le noyau All-SubSequences
Le noyau All-SubSequences (Shawe-Taylor et Cristianini, 2004) permet de tenir compte
des sous-séquences non contiguës de toutes tailles. Ainsi, la fonction ΦA(s) permet de plon-
ger la séquence s dans un espace vectoriel dans lequel le uème composant de ΦA(s) indique
la fréquence d’occurrence de la séquence u dans s. On dira que u est une sous-séquence non
contiguë de s, s’il existe un ensemble I = {i1, . . . , i|u|} tel que ∀j ∈ {2, . . . , |u|}, ij−1 < ij
et ∀j ∈ {1, . . . , |u|}, u(j) = s(ij) (u(j) désignant le jème élément de u). On notera s[I] = u
pour désigner le fait que chaque élément de u(j) est identique à l’élément s(ij) avec ij ∈ I .
ΦAu (s) =
∑
I:s[I]=u
1
D’où :
kA(s, t) =
∑
(I1,I2):s[I1]=t[I2]
1
Il est possible de définir ce noyau de manière récursive. En effet, il suffit de remarquer
que toute sous-séquence de s contenant le dernier caractère a de s, tel que s = s′a, ne peut
apparaître dans t qu’entre le premier caractère de t et la dernière occurrence de a dans t. Ainsi,
on a :
kA(s, ) = 1
kA(s′a, t) = kA(s′, t) +
∑
k:t[k]=a
kA(s′, t[1 . . . k − 1])
L’avantage de ce noyau est qu’il est capable de capturer tous les motifs communs à deux
séquences. Le désavantage est que l’espace de projection est de très haute dimension entraînant
un temps de calcul important.
3.3 Le noyau p-Fixed length SubSequence
Le noyau p-Fixed length SubSequence (Shawe-Taylor et Cristianini, 2004) est un compro-
mis entre le noyau p-Spectrum et le noyau All-SubSequences. Il permet de limiter la recherche
de sous-séquences à des sous-séquences non contiguës de taille p. Ainsi, la fonction de projec-
tion ΦF (s) sera composée des éléments ΦAu (s) tels que |u| = p.
De même que précédemment, on pourra définir ce noyau par récurrence en notant que la récur-
rence sera définie sur la séquence, en retirant à chaque étape le dernier élément de la séquence,
mais aussi sur la taille p du motif. En effet, si le dernier caractère du motif a été fixé, le préfixe
du motif ne peut être constitué que de p− 1 éléments.
k0(s, t) = 1
kp(s, ) = 0 pour p > 0
kp(s′a, t) = kp(s′, t) +
∑
k:t[k]=a
kp−1(s′, t[1 . . . k − 1])
S. Aseervatham et E. Viennet
3.4 Le noyau String Subsequence (SSK)
L’un des inconvénients des noyaux traitant les sous-séquences non contiguës vus précé-
demment est qu’ils ne tiennent pas compte de la distance séparant les éléments non contiguë.
En effet, prenons l’exemple de deux séquences "aaab" et "aab", la séquence "ab" est une sous-
séquence des deux premières mais elle est plus similaire à la deuxième qu’à la première. Or,
les noyaux All-SubSequences et p-Fixed length SubSequence attribueront la même valeur aux
couples ("aaab", "ab") et ("aab", "ab").
Le noyau String Subsequence (Lodhi et al., 2002) permet de tenir compte de la disconti-
nuité dans le calcul de la similarité en pondérant les séquences en fonction de leur taille. Pour
une séquence s, la fonction de projection ΦSSK(s) sera défini pour tout u ∈ Σn par :
ΦSSKu (s) =
∑
I:u=s[I]
λcard(I)
Le noyau SSK, de paramètre n, pour deux séquences s et t est alors :
knSSK(s, t) =
∑
u∈Σn
∑
I:u=s[I]
∑
J:u=t[J]
λcard(I)+card(J)
Comme pour les noyaux précédents, en utilisant la programmation dynamique, on peut ré-
duire la complexité à O(n.|s|.|t|).
Des expérimentations ont été menées dans (Lodhi et al., 2002) pour évaluer les noyaux
SSK, p-Spectrum et le noyau standard Bag Of Words (Joachims, 2002). La base de données
utilisée est la base Reuters-21578 contenant des documents en langage naturel. L’expérience
consistait à effectuer un classement binaire des documents après avoir effectué un apprentis-
sage sur les données prévues à cet effet. Les documents ont été pré-traités en éliminant les
mots d’arrêts et les signes de ponctuations. Les résultats ont montrés que les string kernels
sont plus performants que l’approche standard du Bag Of Words. De plus, le noyau SSK est le
plus performant lorsque la valeur de pondération est choisie judicieusement. De même, lorsque
la taille p est choisie convenablement, le noyau p-Spectrum donne les meilleurs résultats.
Les deux inconvénients pour l’utilisation de ces noyaux sont d’une part le temps de calcul et
d’autre part le choix des paramètres qui doit être fait de manière spécifique à chaque applica-
tion.
3.5 Le noyau Séquence marginalisée
Le noyau marginalisé pour les séquences a été introduit par Kashima et Tsuboi (2004) afin
d’étiqueter des structures complexes tels que les séquences, les arbres et les graphes.
Le problème de l’étiquetage consiste à affecter à une donnée x = (x1, . . . , xT ) un groupe
d’étiquettes y = (y1, . . . , yT ) ∈ ΣTy . Par exemple, en langage naturel x peut représenter une
phrase, et le problème consiste à attribuer à chaque mot de x une étiquette désignant son groupe
grammatical (Part Of Speech tagging).La figure 1(a) montre un exemple de couple (x, y) où
les nœuds noirs représentent les éléments xi et les nœuds blancs les étiquettes yi associées à
xi.
Méthodes à noyaux appliquées aux textes structurés
Pour résoudre le problème de l’étiquetage, l’approche standard consiste à attribuer à x la sé-
quence d’étiquettes y tel que : y=argmaxy
∑
i logP (yi|xi). Il s’agit ici de maximiser la pro-
babilité que la séquence entière soit correcte. Une autre approche consiste à maximiser indivi-
duellement la probabilité qu’une étiquette yi corresponde à l’élément xi. Soit :
yi = argmaxyP (yi = y|xi) = argmaxy
∑
y:yi=y
P (y|x)
Partant de cette dernière approche, Kashima et Tsuboi (2004) proposent une méthode
pour étiqueter une séquence. Cette méthode se base sur l’utilisation d’un perceptron à noyau
(Shawe-Taylor et Cristianini, 2004).
Afin d’étiqueter les éléments individuellement, une séquence (x,y) est décomposée en triplets
(x, t, yt) où yt est le tème élément de x. Pour cela, le perceptron à noyau est entraîné avec des
séquences étiquetées auxquelles ont été ajoutés des exemples négatifs. Ces exemples négatifs
sont obtenus pour chaque couple (x,y) en modifiant les valeurs de y. Ainsi pour ce couple
on obtient un ensemble de positifs {(x, i, yi) : 1 ≤ i ≤ dim(x)} et un ensemble de négatifs
{(x, i, z) : 1 ≤ i ≤ dim(x) ∀z ∈ Σy}. Après apprentissage, on affecte à un élément ut d’une
séquence u l’étiquette y qui maximise le score calculé par le perceptron avec un noyau margi-
nalisé.
Le noyau marginalisé proposé est le suivant :
k(x,x′, τ, t, yτ , y′t) =
∑
z:zτ=yτ
∑
z′:z′t=y
′
t
P (z|x)P (z′|x′) 〈Φ(x, z; τ),Φ(x′, z′; t)〉 (1)
yτ et y′t étant les étiquettes respectives des éléments xτ et x
′
t ; φf (x,y; t) indiquant la fré-
quence d’occurrence de f dans (x,y) incluant la tème position.
La combinatoire induite par le noyau de l’équation 1 peut-être diminuée en effectuant une re-
cherche bidirectionnelle au sein d’une séquence. En effet, la composante φf (x,y; t) du vecteur
Φ(x,y; t) indique le nombre d’occurrence de f dans (x,y) incluant la tème position de (x,y)
((xt, yt)). Il est alors possible de décomposer f en fu et fd tel que ((xt, yt)) soit le dernier
élément de fu et le premier élément de fd. Le calcul se limitera alors à évaluer les deux en-
sembles Fu = {(xi, xi+1, . . . , xt)|1 ≤ i ≤ t} et Fd = {(xt, xt+1, . . . , xk)|t ≤ k ≤ dim(x)}.
Les autres composantes de l’espace seront obtenues par combinaison de Fu et de Fd i.e.
F = Fu × Fd.
La figure 1 illustre ce principe : le couple de séquences (a) peut être obtenu en combinant (b)
et (c).
Afin d’effectuer la décomposition d’une séquence en fonction de la position t, on supposera
que :
P (y|x) =
∏
t
P (yt|xt)
On posant xu(t) = (x0, . . . , xt) et xd(t) = (xt, . . . , xT ), de même pour yu(t) et yd(t), on
obtient :
P (y|x) = P (yu(t)|xu(t)). P (yt|xt)(P (yt|xt))2 .P (yd(t)|xd(t))
S. Aseervatham et E. Viennet
   N      V    Det   N      N      V        V    Det   N
  Jeff  mange  la  pomme   Jeff  mange    mange  la  pomme
           (a)                 (b)              (c)
t t t
FIG. 1 – (a) Exemple de couple de séquences (x, y). (b) sous-séquences de (a) où les xi avec
i > t ont été éliminés. (c) sous-séquences de (a) obtenues en éliminant les xi de (a) pour i < t.
En décomposant, l’équation 1 on obtient :
k(x,x′, τ, t, yτ , y′t) = ku(x,x
′, τ, t).kp(x,x′, τ, t, yτ , y′t).kd(x,x
′, τ, t)
avec :
ku(x,x′, τ, t) =
∑
yu(τ)
∑
y′u(t)
P (yu(τ)|xu(τ))P (y′u(t)|x′u(t))
. 〈Φ(xu(τ),yu(τ); τ),Φ(x′u(t),y′u(t); t)〉
kd(x,x′, τ, t) =
∑
yd(τ)
∑
y′d(t)
P (yd(τ)|xd(τ))P (y′d(t)|x′d(t))
. 〈Φ(xd(τ),yd(τ); τ),Φ(x′d(t),y′d(t); t)〉
kp(x,x′, τ, t, yτ , y′t) =
P (yτ |xτ )P (y′t|x′t). 〈Φ(xτ , yτ ; τ),Φ(x′t, y′t; t)〉
(
∑
zτ
∑
z′t
P (zτ |xτ )P (z′t|x′t). 〈Φ(xτ , zτ ; τ),Φ(x′t, z′t; t)〉)2
(2)
La complexité pour l’évaluation de ces noyaux peut être ramenée à O(T.T ′) avec T et T ′
la taille des séquences en utilisant la programmation dynamique. On obtient alors les noyaux
suivants :
ku(x,x′, τ, t) =
{
0 si τ = 0 ou t = 0
c2k(xτ , x′t)(ku(x,x
′, τ − 1, t− 1) + 1)
kd(x,x′, τ, t) =
{
0 si τ > dim(x) ou t > dim(x′)
c2k(xτ , x′t)(kd(x,x
′, τ + 1, t+ 1) + 1)
k(xτ , x′t) =
{
0 si xτ 6= x′t∑
y P (y|xτ )P (y|x′t)
kp(x,x′, τ, t, yτ , y′t) =
{
0 si (xτ , yτ ) 6= (x′t, y′t)
cP (yτ |xτ )P (y′t|x′t)
(c2k(xτ ,x′t)2)
La constante c est utilisée pour pondérer les termes φf selon la taille de f . Il est aussi
possible de permettre des discontinuités dans les sous-séquences comme dans le cas du noyau
String Subsequence. Il suffit alors simplement de modifier ku et kd (voir (Kashima et Tsuboi,
2004) pour plus de détails).
Méthodes à noyaux appliquées aux textes structurés
Ce noyau, combiné au noyau polynomial de degré deux, a été utilisé avec un perceptron
pour résoudre un problème de reconnaissance d’entités nommées et un problème d’extraction
d’information. La loi uniforme est utilisé pour modéliser P (yt|xt). Les expériences ont été
menées en utilisant la validation croisée à 3 blocs.
En outre, pour chaque expérience le noyau marginalisé est comparé à un perceptron utilisant
le modèle de Markov caché (Collins, 2002). L’idée de base de cet algorithme est d’utiliser
l’algorithme de Viterbi sur un modèle de Markov caché afin d’attribuer la meilleure séquence
d’étiquettes à une séquence de termes. Le perceptron est utilisé pour attribuer un score à une
séquence étiquetée. Ce score sera utilisé par l’algorithme de Viterbi pour trouver la séquence
d’étiquettes optimale.
Pour la reconnaissance d’entités nommées, les données utilisées sont un sous-ensemble
d’un corpus espagnol fourni par CoNLL2002. Ce corpus est composé de 300 phrases com-
prenant au total 8541 termes. L’objectif est d’attribuer à chaque terme l’un des neuf labels
désignant le type d’entité nommée (l’un des neufs labels correspond au type "non-entité nom-
mée"). Les résultats montrent que le noyau marginalisé a un taux de reconnaissance, tant au
niveau de la précision que du rappel, supérieur à celui du modèle de Markov caché.
La deuxième expérience consistait à extraire des informations concernant l’utilisation de
produits. A partir d’une base de 184 phrases (soit 3570 termes) en japonais, l’objectif est
de reconnaître le nom du produit, le vendeur, le nombre de produits achetés, les raisons de
l’achat etc. Ainsi, il s’agit d’attribuer à chaque terme l’une des 12 étiquettes correspondants
aux informations citées.
Les termes ont été annotés en effectuant une analyse lexicale. En outre, un noyau marginalisé
sur les arbres (voir la section 4) a été utilisé. Pour ce noyau, les données ont été structurées
en arbre lexical de dépendance représentant la structure linguistique de la phrase en terme de
dépendance entre les mots.
Les expériences montrent que les noyaux marginalisés sont plus performants que le perceptron
utilisant le modèle de Markov caché. De plus, le noyau marginalisé sur les arbres obtient de
meilleurs résultats que le noyau sur les séquences. Ce résultat peut être expliqué par le fait que
le noyau sur les arbres tire avantage de l’information structurelle contrairement au noyau sur
les séquences.
4 Les noyaux pour les arbres
Les arbres sont des structures de données permettant de représenter efficacement des don-
nées organisées de manière hiérarchique. Ainsi, ils sont communément utilisés dans de nom-
breux domaines.
La majorité des documents structurés et semi-structurés, tels que les documents XML, sont
représentés de manière arborescente. Ainsi, il peut être intéressant de tenir compte de cette
structure dans l’évaluation des critères de similarités entre ces différents documents.
S. Aseervatham et E. Viennet
4.1 Le noyau Tree kernel
Le noyau Tree Kernel (appelé aussi parse tree kernel) (Collins et Duffy, 2002) a été défini
pour le calcul de similarité entre les arbres grammaticaux (ou arbres syntaxiques). Un arbre
syntaxique est obtenu à partir d’une phrase en la décomposant en groupe grammatical. La fi-
gure 2 montre l’arbre syntaxique associé à la phrase “Jeff mange la pomme”.
Phrase
GN
VN
Jeff
GV
GN
mange Det N
la pomme
FIG. 2 – Arbre syntaxique pour la phrase "Jeff mange la pomme"
Definition 1 (arbre propre) Un arbre propre est un arbre ayant une racine et au moins un
nœud fils.
Definition 2 (Sous-arbre complet) Un arbre S est dit sous-arbre complet d’un arbre T si et
seulement si il existe un nœud n tel que l’arbre induit par n (de racine n) est égal à S. On
notera τT (n) l’arbre complet induit par le nœud n de T .
Definition 3 (Sous-arbre co-enraciné) Un arbre S est dit sous-arbre co-enraciné d’un arbre
propre T si et seulement si S les propriétés suivantes sont vérifiées :
1. S est un arbre propre,
2. la racine de S (rac(S)) est identique à la racine de T (rac(T )) (si S et T sont des arbres
dont les nœuds sont étiquetés alors les étiquettes de rac(S) et de rac(T ) doivent être
identiques),
3. ∀i, filsi(rac(S)) = filsi(rac(T )),
4. S peut être obtenu à partir de T en supprimant des sous-arbres de filsi(rac(T )).
La notion de sous-arbre co-enraciné permet de garantir la consistance des règles gramma-
ticales. Par exemple, pour l’arbre T de la figure 2, il existe des arbres co-enracinés de T qui
produisent : "N V la N", "Jeff mange GN", "GN V GN", ... Toutefois, il n’existe pas d’arbres
co-enracinés de T produisant "Jeff mange Det”, "GN mange N", ...
Soit Γ l’ensemble de tous les arbres propres possibles, un arbre T peut être plongé, par une
fonction Φr, dans un espace vectoriel de caractéristique (feature-space). Le uème composant
Méthodes à noyaux appliquées aux textes structurés
de Φr, associé à Su ∈ Γ, donne le nombre de nœud n de T tel que Su est un sous-arbre
co-enraciné de τT (n) (RfreqT (Su)), soit :
ΦrSu(T ) = RfreqT (Su) =
∑
n∈T
ISu(τT (n)) (3)
Avec ISu(τT (n)) = 1 si l’arbre Su est un sous-arbre co-enraciné de τT (n) et 0 sinon.
Le noyau permettant de calculer la similarité entre deux arbres T1 et T2 est :
ktree(T1, T2) = 〈Φr(T1),Φr(T2)〉
=
∑
Su∈Γ
ΦrSu(T1).Φ
r
Su(T2)
=
∑
Su∈Γ
(
∑
n1∈T1
ISu(τT1(n1))).(
∑
n2∈T2
ISu(τT2(n2)))
=
∑
n1∈T1
∑
n2∈T2
∑
Su∈Γ
ISu(τT1(n1)).ISu(τT2(n2))
=
∑
n1∈T1
∑
n2∈T2
krtree(τT1(n1), τT2(n2)) (4)
krtree(T1, T2) indique le nombre de sous-arbres co-enracinés qu’ont en commun les arbres T1
et T2. Cette fonction retourne 0 si 1) les racines sont différentes, ou 2) si le nombre de fils
de T1 et de T2 ne correspondent pas ou 3) si ∃i, filsi(T1) 6= filsi(T2). Dans les autres cas,
on peut définir krtree(T1, T2) par récurrence. En effet, cette fonction sera égale au produit des
nombres de sous-arbres co-enracinés communs à chacun des fils de T1 et de T2. Il est à noter
que si krtree(T1, T2) 6= 0mais que krtree(τT1(filsi(rac(T1)), filsi(rac(T2))) = 0, il existe un
unique sous-arbre co-enraciné commun à T1 et T2 pour la partie du sous-arbre induit par filsi.
On peut ainsi définir krtree(T1, T2) dans le cas non nul :
krtree(T1, T2) =
∏
i
(krtree(τT1(filsi(rac(T1))), τT2(filsi(rac(T2)))) + 1)
La complexité temporelle du noyau ktree(T1, T2) est O(|T1||T2|) (Collins et Duffy, 2002)
avec |T | le nombre de nœuds dans T .
Collins et al. ont utilisé ce noyau pour associer à une phrase l’arbre syntaxique le plus plausible
(parsing) (Collins et Duffy, 2002). La décomposition d’une phrase en arbre syntaxique est
un problème difficile. En effet, l’ambiguïté sous-jacente au langage naturel entraîne plusieurs
décompositions possibles pour une même phrase. L’objectif proposé par Collins et al. est de
sélectionner l’arbre le plus probable par une approche discriminante.
Soit F une fonction permettant de générer un ensemble d’arbres syntaxiques pour une phrase,
les données sont représentés par un couple (s, F (s)). Pour l’ensemble d’apprentissage, l’arbre
syntaxique correct pour chaque s est connu dans F (s). Un séparateur est alors "appris" en
utilisant un perceptron. Pour une phrase s, on lui associe l’arbre Tsi de F (s) tel que :
Tsi = argmaxT∈F (s)(w
∗.Φr(T ))
S. Aseervatham et E. Viennet
Avec w∗ le vecteur optimal associé à :
w =
∑
s,j>1
αs,j(Φr(Tsi)− Φr(Tsj ))
Avec si une phrase d’apprentissage, s1 ∈ F (s) l’arbre syntaxique correct de s et sj ∈ F (s).
Les expériences sur le corpus Penn treebank ATIS, qui est un corpus anglais annoté sous forme
arborescente, ont montré que l’utilisation de cette méthode améliore de près de 4% les résultats
obtenus par une méthode conventionnelle stochastique (Probabilistic Context Free Grammar).
4.2 Le noyau Tree kernel généralisé
Le noyau Tree Kernel a été essentiellement développé pour traiter des arbres spécifiques
telles que les arbres syntaxiques ; il se base sur les propriétés suivantes :
1. les descendants d’un nœud n’ont jamais les mêmes étiquettes que les nœuds ancêtres
2. le noyau utilise la notion de sous-arbre co-enraciné pour calculer la similitude entre deux
arbres.
Une généralisation de ce noyau a été proposée dans (Kashima et Koyanagi, 2002) pour
traiter des arbres complexes tels que les arbres XML et HTML. Toutefois, on impose que
l’arbre soit étiqueté et ordonné (tel que c’était le cas pour les arbres syntaxiques).
Le cadre général du noyau Tree Kernel reste valide. En effet, pour généraliser le noyau, il suffit
de modifier la fonction IS(T ) et de changer le noyau spécifique krtree(T1, T2). Dans le cas
d’un arbre quelconque étiqueté et ordonné T , IS(T ) retournera la fréquence d’occurrence du
sous-arbre S dans T .
Definition 4 (Sous-arbre) Un arbre S (possédant au moins un nœud) est un sous-arbre de T
si et seulement si il existe un nœud n de T tel que l(n) = l(rac(S)) (l(n) correspondant à l’éti-
quette du nœud n) et une liste ordonnée d’indexes {j1, . . . , jk} tel que ∀i, l(filsi(rac(S))) =
l(filsji(τT (n))) avec i < ji et τS(filsi(rac(S))) soit, soit une feuille soit un sous-arbre de
τT (filsji(τT (n))) partageant la même racine.
Étant donnée cette définition de sous-arbre, le noyau krtree(T1, T2) peut être défini comme
étant la fonction qui retourne le nombre de sous-arbres commun à T1 et T2 avec pour racine
rac(T1), en tenant compte de la fréquence d’occurrence dans chaque arbre. Autrement dit, il
s’agit de la somme, pour chaque sous-arbre possible S de racine rac(T1), des produits des
nombres d’occurrences de S dans T1 et dans T2.
Les arbres étant ordonnés, le noyau sur T1 et T2 peut être calculé en introduisant une récur-
rence sur le nombre de fils de T1 (nf(rac(T1))) et le nombre de fils de T2. Ainsi, la fonction
ST1,T2(i, j) est introduite pour calculer k
r
tree(T1i, T2j) tel que T1i est le sous-arbre de T1 ob-
tenu en supprimant tous les fils d’index supérieurs à i, ainsi que leurs descendants, de même
pour T2j .
krtree(T1, T2) =
{
0 si l(rac(T1)) 6= l(rac(T2))
ST1,T2(nf(rac(T1)), nf(rac(T2)))
(5)
Méthodes à noyaux appliquées aux textes structurés
avec
ST1,T2(0, 0) = ST1,T2(i, 0) = ST1,T2(0, j) = 1
ST1,T2(i, j) = ST1,T2(i− 1, j) + ST1,T2(i, j − 1) (6)
−ST1,T2(i− 1, j − 1)
+ST1,T2(i− 1, j − 1).krtree(τT1(filsi(rac(T1))), τt2(filsj(rac(T2))))
On généralise ce noyau en s’appuyant sur deux idées ; d’une part utiliser la similarité entre
les étiquettes, d’autre part représenter les sous-arbres non-contigus (Kashima et Koyanagi,
2002).
Soit Σ l’ensemble de toutes les étiquettes et f : Σ × Σ → [0, 1] indiquant un score de "mu-
tation" entre deux étiquettes tel que f(e, a) indique la probabilité d’acceptation de la mutation
de l’étiquette a vers e, la fonction de similarité entre deux étiquettes de deux nœuds n1 et n2
est :
Sim(l(n1), l(n2)) =
∑
a∈Σ
f(l(n1), a).f(l(n2), a)
En introduisant la fonction de similarité dans l’équation 5, on obtient :
krtree(T1, T2) = Sim(l(n1), l(n2)).ST1,T2(nf(rac(T1)), nf(rac(T2))) (7)
On peut encore généraliser en utilisant, au niveau du noyau, de la notion d’élasticité des
sous-arbres. Ainsi, la définition de sous-arbre se voit élargie en permettant la non contiguïté
au niveau des nœuds d’un chemin. Il n’est ici plus nécessaire qu’un chemin d’un sous-arbre
apparaisse de manière contiguë dans un arbre. Cependant, la contrainte d’arbre ordonné reste
valable.
Definition 5 (Sous-arbre non contigu) Un arbre S (possédant au moins un nœud) est un
sous-arbre de T si et seulement si il existe un nœud n de T tel que l(n) = l(rac(S)) (l(n) et
une liste ordonnée d’indexes {j1, . . . , jk} tel que ∀i, τS(filsi(rac(S))) soit un sous arbre de
τT (filsji(τT (n))).
Afin de prendre en compte la définition de sous-arbre non-contigu, il est nécessaire de gé-
néraliser la formule de krtree(T1, T2) (équations 5 et 7). En effet, cette formule retourne une
valeur nulle (ou faible selon la similarité) si les étiquettes des racines sont différentes. En
d’autre terme, tout sous-arbre commun à T1 et T2 doit être enraciné aux nœuds racines de T1
et T2 impliquant ainsi que les arbres possèdent la même racine. Or, dans le cas des sous-arbres
non-contigus, un sous-arbre commun à T1 et T2 peut être construit par combinaison (ordonnée)
à partir de sous-arbres enracinés à n’importe quels nœuds descendants de T1 et T2.
Soit krold le noyau défini par l’équation 5, le noyau spécifique pour les arbres élastiques
utilisés par le noyau Tree Kernel (équation 4) est :
krtree(T1, T2) =
∑
n1∈T1
∑
n2∈T2
krold(τT1(n1), τT2(n2))
S. Aseervatham et E. Viennet
Cette formule peut être calculée efficacement de manière récursive :
krtree(T1, T2) = k
r
old(T1, T2) +
∑
ni=filsi(T1)
krtree(τT1(ni), T2)
+
∑
nj=filsj(T2)
krtree(T1, τT2(nj))
−
∑
ni=filsi(T1)
∑
nj=filsj(T2)
krtree(τT1(ni), τT2(nj))
De même que dans le cas du noyau parse tree kernel vu dans la section précédente, la com-
plexité du noyau tree kernel est O(|T1|.|T2|) dans les différents cas de généralisation.
Kashima et Koyanagi (2002) ont utilisé les noyaux généralisés élastiques et non élastiques,
sans tenir compte des mutations d’étiquettes, pour la classification et l’extraction d’information
à partir de documents HTML. Les performances ont été évaluées en utilisant la méthode de
validation croisée leave-one-out. L’apprentissage a été effectué en utilisant l’algorithme du
perceptron "kernelisé".
Pour la classification de documents, une base de données comprenant 30 documents HTML
en japonais et 30 documents HTML en anglais a été construite en extrayant les documents
aléatoirement sur les sites web américains et japonais d’IBM. La classification devant être
purement structurelle, seules les balises HTML ont été préservées, éliminant ainsi les attributs
et les données. De plus, les noyaux sur arbres ont été combinés avec le noyau polynomial. Les
résultats ont montrés que le noyau non élastique était de 12% plus performant que le noyau
élastique en atteignant prés de 80% de bon classement. Ces résultats ont été obtenus avec un
noyau polynomial d’ordre 4 (resp. 3).
L’extraction d’information consiste à apprendre et à reconnaître une information précise
dans des documents HTML. Il s’agit ici du marquage des nœuds pertinents. Le problème
du marquage consiste à apprendre à partir d’arbres correctement marqués puis à marquer les
nœuds pertinents des arbres non traités. Ce problème peut être ramené à un problème de clas-
sification en effectuant une transformation du marquage. En effet, pour un nœud marqué, on
insère entre le nœud concerné et le nœud père, un nœud portant une étiquette appropriée pour
signaler le marquage. Puis, un ensemble d’arbres négatifs est générés à partir des arbres cor-
rects en retirant le marquage et en les plaçant sur des nœuds non initialement marqués. Un
apprentissage peut ensuite être effectué sur ces données. Pour le marquage sur un arbre, on
effectue pour chacun de ses nœuds un marquage puis on le classe.
Pour l’expérimentation, une base a été créée à partir de 54 pages HTML extraites d’un cata-
logue de vente d’ordinateurs portables d’IBM Japon. L’objectif de l’expérience était de retrou-
ver l’image de l’ordinateur à vendre. Pour cela, les nœuds contenants les images pertinentes
ont été marqués et les données textuelles présentes dans les pages ont été éliminées. Les ré-
sultats ont montré que le noyau élastique a permis d’extraire l’information avec une précision
de 99.3% et un rappel de 68.6% contre une précision de 11.9% et un rappel de 79.6% pour le
noyau non élastique. Ces résultats ont été obtenus sans la combinaison avec le noyau polyno-
mial. En effet, ce dernier n’a pas permis d’améliorer les résultats.
Méthodes à noyaux appliquées aux textes structurés
4.3 Le noyau Tree kernel marginalisé
Le noyau marginalisé pour les arbres a été introduit par Kashima et Tsuboi (2004) pour
répondre aux problèmes d’étiquetages (voir la section sur le noyau marginalisé sur les sé-
quences).
L’objectif de ce noyau est de permettre de calculer la similarité entre deux arbres étiquetés. Un
arbre étiqueté étant simplement un arbre où chaque nœud représente un élément observable
(un terme) et à chaque nœud est associé une étiquette. L’avantage de résoudre un problème
d’étiquetage en utilisant un modèle de donnée arborescent, plutôt que séquentiel, est qu’il est
possible d’exploiter l’information structurelle pour améliorer la discrimination.
Le noyau marginalisé sur les arbres est obtenu en intégrant le noyau Tree kernel généralisé
dans le cadre théorique du noyau marginalisé défini par la série d’équation 2.
Ainsi, kd(T1, T2, τ, t) est le noyau ne prenant en compte que les sous-arbres ayant la même
racine rac(T1τ ) (T1τ indique ici le sous-arbre de T1 induit par le nœud d’index τ ). De même,
ku(T1, T2, τ, t) ne prend en compte que les sous-arbres ayant une feuille correspondant au
nœud d’index τ de T1 (rac(T1τ )).
Pour le calcul de kd, on se base sur les équations 5 et 6. L’équation 6 calcule la somme
des contributions des sous-arbres communs à T1 et T2 en explorant à chaque niveau les fils de
droite à gauche. En modifiant cette équation on obtient :
SF (T1, T2, τ, t, 0, 0) = SF (T1, T2, τ, t, i, 0) = SF (T1, T2, τ, t, 0, j) = 1
SF (T1, T2, τ, t, i, j) = SF (T1, T2, τ, t, i− 1, j) + SF (T1, T2, τ, t, i, j − 1)
−SF (T1, T2, τ, t, i− 1, j − 1)
+SF (T1, T2, τ, t, i− 1, j − 1).kd(T1, T2, ch(T1, τ, i), ch(T2, t, j))
Avec ch(T1, τ, i) l’index, dans T1, du ième fils de la racine de T1τ .
Le noyau kd devient alors :
kd(T1, T2, τ, t) = c2k(T1τ , T2t).(1 + SF (T1, T2, τ, t, nf(rac(T1τ )), nf(rac(T2t))))
Pour ku, le calcul s’effectue de la feuille vers la racine. Ainsi, on utiliser la fonction
pa(T1, τ) qui calculera l’index du père du τ ème nœud dans T1. En outre, il faut aussi tenir
compte des frères gauches et des frères droits du τ ème nœud de T1. Pour la contribution des
frères gauches, la fonction SF pourra être utilisée. Quant aux frères droits, il faudra les explo-
rer de la gauche vers la droite. On utilisera la fonction n = chID(T1, τ) pour indiquer que le
nœud d’index τ est le nème fils de son père. On définit donc, une fonction symétrique à SF :
SB(T1, T2, τ, t, i, j) = 1 si i ≥ nf(rac(T1τ )) ou si j ≥ nf(rac(T2t))
SB(T1, T2, τ, t, i, j) = SB(T1, T2, τ, t, i+ 1, j) + SB(T1, T2, τ, t, i, j + 1)
− SB(T1, T2, τ, t, i+ 1, j + 1)
+ SB(T1, T2, τ, t, i+ 1, j + 1).kd(T1, T2, ch(T1, τ, i), ch(T2, t, j))
S. Aseervatham et E. Viennet
L’expression de ku est :
ku(T1, T2, τ, t) = c2k(T1τ , T2t).(1 + ku(T1, T2, τ, t))
. SF (pa(T1, τ), pa(T2, t), τ, t, chID(T1, τ)− 1, chID(T2, t)− 1)
. SB(pa(T1, τ), pa(T2, t), τ, t, chID(T1, τ) + 1, chID(T2, t) + 1)
Les expériences menées sur ce noyau sont décrites dans la section sur le noyau marginalisé
pour les séquences.
5 Les noyaux pour les graphes
Le graphe est une structure de donnée très utilisée dans le domaine informatique pour
modéliser des informations structurées complexes. Les séquences et les arbres étudiés précé-
demment peuvent être vus comme des graphes acycliques orientés (dans le cas d’une séquence,
le graphe est de degré maximum 1).
Nous définirons un graphe étiquetéG par le triplé (V,E, σ) où V est l’ensemble des nœuds de
G, σ l’ensemble des étiquettes tel que σi représente l’étiquette du nœud i (il est aussi possible
d’étiqueter les arcs : on notera σ(i,j) l’étiquette de l’arc reliant le nœud i à j) et E, une matrice
d’adjacence tel que Eij = 1 si et seulement si il existe un arc reliant le nœud i au nœud j (afin
de simplifier l’écriture on utilisera la même notation i, j pour désigner les indexes dans E que
pour designer les nœuds de V ). L’une des propriétés de la matrice d’adjacence est que [En]ij
indique le nombre de chemins de longueur n reliant le nœud i au nœud j.
La conception d’un noyau nécessite une définition de la similarité entre deux graphes.
Pour cela, deux approches ont été proposées (Gärtner, 2003; Gärtner et al., 2006). La première
consiste à déterminer si les deux graphes sont isomorphes (ils ne se distinguent que par l’ordre
des nœuds) ou à déterminer le nombre de sous-graphes isomorphes communs. Cependant, il
est connu que ce problème est fortement combinatoire.
La deuxième approche consiste à projeter le graphe G dans un espace vectoriel où chaque
dimension est indexée par un graphe H tel que la valeur de la projection de G sur cet axe
représente la fréquence d’occurrence de H , en tant que sous-graphe, dans G. Il est alors pos-
sible de concevoir un noyau qui identifie certaines propriétés dans les sous-graphes H . En
particulier, ce noyau peut effectuer le produit scalaire dans l’espace vectoriel en se limitant
aux sous-graphes qui sont des chemins hamiltoniens (H est un chemin hamiltonien de G si
et seulement si H est un sous-graphe de G et si H est de même ordre que G i.e. H contient
tous les nœuds de G exactement une fois). Comme pour la première approche, le problème du
chemin hamiltonien est NP-difficile.
Afin de réduire la complexité dans l’évaluation de la similarité d’autres approches ont
été explorées. L’approche la plus répandue consiste à calculer la similarité en se basant sur
les chemins parcourus (Gärtner et al., 2003, 2006; Kashima et Inokuchi, 2002; Kashima et al.,
2003). L’un des problèmes principaux de cette approche est qu’il existe une infinité de chemins
possibles dès lors qu’il existe dans le graphe un cycle. On a alors le noyau :
Méthodes à noyaux appliquées aux textes structurés
k(G,G′) = lim
n→∞
n∑
i=1
∑
p∈Pi(G)
∑
p′∈Pi(G′)
λi.kp(p, p′) (8)
Avec Pl(G) l’ensemble des chemins de G de longueur l, λl un réel pondérant les chemins
de longueur l (on fixera λl = λl) et kp un noyau défini sur les chemins. Il existe plusieurs
façons de définir kp selon qu’on veuille tenir compte des étiquettes sur les nœuds, sur les arcs
ou encore permettre des discontinuités (gap).
Dans (Gärtner et al., 2003, 2006), kp est défini sur des chemins contiguës en tenant compte
des étiquettes sur les nœuds et sur les arcs. Ainsi, le noyau sur les arcs revient à énumérer le
nombre de chemins communs aux deux graphes.
De plus, l’équation 8 est réécrite plus élégamment en utilisant la propriété de la matrice d’adja-
cence. Pour cela, un nouveau graphe G× : (V×, E×, σ×) est introduit en effectuant le produit
direct des graphes G : (V,E, σ) et G′ : (V ′, E′, σ′) :
V× = {(v, v′) ∈ V × V ′|σv = σv′}
σ×k=(v,v′) = σv
Pour (i, j) correspondant à ((u, u′), (v, v′)) ∈ V 2×
[E×]i,j =
{
1 si [E]u,v = [E′]u,v′ = 1 et σ(u,v) = σ′(u′,v′)
0 sinon
σ×(i,j) = σ(u,v)
L’équation 8 devient :
k(G,G′) = lim
n→∞
n∑
i=1
|V×|∑
u,v
λi.[Ei×]u,v
L’expression Ei× peut être simplifiée si la matrice E× est diagonalisable. Dans le cadre
d’un graphe non-orienté, la matrice E× étant une matrice réelle et symétrique, elle peut être
diagonalisée.
Ainsi, si E× peut être exprimé sous la forme T−1.D.T alors E×i = T−1.Di.T . On peut alors
réécrire le noyau sous la forme :
k(G,G′) =
|V×|∑
u,v
(T−1.( lim
n→∞
n∑
i=1
λi.Di).T )u,v
Pour calculer la limite, Gärtner et al. (2003) propose d’utiliser une décomposition en série
exponentielle ou en série géométrique.
La décomposition en série exponentielle se base sur l’égalité
eβ.E = lim
n→∞
n∑
i=0
(βE)i
i!
S. Aseervatham et E. Viennet
Ainsi, en fixant λi = β
i
i! , on obtient :
k(G,G′) =
|V×|∑
u,v
(T−1.eβ.D.T )u,v
De même, la décomposition en série géométrique se base sur, pour γ < 1 :
lim
n→∞
n∑
i=0
γi =
1
1− γ
En fixant γ = λ.D et en veillant à ce que λ.D < I, on a :
k(G,G′) =
|V×|∑
u,v
(T−1.(I− λ.D)−1.T )u,v
Dans Kashima et Inokuchi (2002), les auteurs ont décomposé la similarité de deux graphes
en une somme de similarité entre nœuds :
k(G,G′) =
1
|V |.|V ′|
∑
vi∈V,vj∈V ′
kn(vi, vj)
Avec V et V ′ l’ensemble des nœuds de G et respectivement de G′.
La similarité entre deux nœuds sera d’autant plus importante qu’il existera des chemins longs
communs aux deux graphes issus de ces nœuds. Pour assurer la terminaison du calcul, une
probabilité 1− λ est fixée pour terminer le chemin et une probabilité λ pour continuer vers un
successeur du nœud courant. Ainsi, plus le chemin sera long et plus la probabilité de terminer
le chemin deviendra importante. On obtient, ainsi, le noyau suivant :
kn(u, u′) = I(u, u′).((1− λ) + λ.
∑
(v,v′)∈AG(u)×AG′ (u′)
IA((u, v), (u′, v′))
|AG(u)|.|AG′(u′)| .kn(v, v
′))
Avec AG(u) = {v ∈ V |Euv = 1}, I(u, u′) = 1 si les étiquettes des nœuds u de G et u′ de G′
sont identiques ou 0 sinon et de même pour IA((u, v), (u′, v′)) qui retourne 1 si l’étiquette de
l’arc reliant u et v de G est identique à l’étiquette de l’arc reliant u′ et v′ de G′.
Dans Kashima et al. (2003), un noyau marginalisé sur tous les chemins possible est proposé
en ne considérant que des graphes orientés. Ainsi, le noyau est défini par :
k(G,G′) = lim
L→∞
L∑
l=1
∑
h
∑
h′
kz(G,G′,h,h′).P (h|G).P (h′|G′)
La probabilité a posteriori d’avoir un chemin h de G de longueur l (P (h|G)) est défini en
fonction de la probabilité de débuter un chemin par un nœud h1(Ps(h1)), la probabilité de
Méthodes à noyaux appliquées aux textes structurés
terminer ce chemin par un nœud hl (Pq(hl)) et les probabilités d’effectuer une transition d’un
nœud hi vers un nœud hi+1 (Pt(hi+1|hi)). D’où :
P (h|G) = Ps(h1).
l=|h|∏
i=2
Pt(hi|hi−1).Pq(hl)
Le noyau kz effectue la comparaison des deux chemins h et h′ des graphes respectifs G et
G′. Pour cela, le noyau calcule le produit des similarités entre les étiquettes des nœuds et des
arcs du chemins :
kz(G,G′,h,h′) =
{
0 si |h| 6= |h′|
ke(σh1 , σ
′
h′1
)
∏l
i=2 ke(σ(hi−1,hi), σ
′
(h′i−1,h
′
i
).ke(σhl , σ
′
h′l
)
On rappelle que σhi indique l’étiquette du nœud hi de G et que σ(hi,hi+1) indique l’éti-
quette de l’arc reliant le nœud hi à hi+1. Étant données deux étiquettes e et e′, on peut définir
ke comme étant un noyau retournant 1 si e = e′ ou 0 sinon. Toutefois, on peut définir un noyau
plus complexe si les étiquettes sont des réelles avec une certaine métrique. On pourrait alors
définir un noyau gaussien qui tolérerait certaines différences entre les étiquettes.
Outre les graphes que nous venons de voir, Suzuki et al. ont introduit dans (Suzuki et al.,
2003a,b) la notion de graphe acyclique orienté hiérarchique (HDAG). Les HDAG sont des
graphes dont certains nœuds contiennent des graphes acycliques orientés. Cette structure a
été proposée pour permettre la représentation de documents textuels ainsi que d’informations
connexes. En effet, un document textuel peut subir de multiple pré-traitement et des infor-
mations grammaticales et sémantiques peuvent lui être ajoutées. Ces informations combinées
entre elles forment des structures hiérarchiques complexes.
En outre, un noyau a été proposé pour calculer la similarité entre les HDAG. Ce noyau a été
évalué sur un problème de classification multi-classe avec l’algorithme SVM et la méthode "un
contre tous" (un classifieur SVM par classe). Une base de données de 3000 questions divisées
en 148 classes a été utilisée pour l’expérimentation. Les questions ont été pré-traitées à l’aide
d’un parser. En outre, les entités nommées ont été étiquetées et les informations sémantiques
ajoutés.
Les résultats ont montrés que le noyau HDAG était plus performant que le noyau SubString
Kernel et le noyau "sac de mots".
6 Conclusion
Nous avons présenté une collection de méthodes assez variées, adaptées aux différents cas
rencontrés lors du traitement des données structurées. La diversité de ces méthodes rend pour
l’instant délicate toute évaluation comparative de leurs performances respectives. Le caractère
très récent de ces travaux fait qu’il n’existe pas pour l’instant d’étude expérimentale compara-
tive sérieuse des comportements de ces différents algorithmes sur des données issues du monde
réel et plus particulièrement du langage naturel.
Insistons sur le fait que la souplesse des méthodes à noyaux facilite la construction de méthodes
ad-hoc adaptées à la structure du problème à traiter.
S. Aseervatham et E. Viennet
Références
Collins, M. (2002). Discriminative training methods for hidden markov models : Theory and
experiments with perceptron algorithms. In Proceedings of EMNLP.
Collins, M. et N. Duffy (2002). Convolution kernels for natural language. In NIPS : Advances
in Neural Information Processing Systems 14, pp. 625–632. MIT Press.
Gärtner, T. (2003). A survey of kernels for structured data. SIGKDD Explor. Newsl. 5(1),
49–58.
Gärtner, T., K. Driessens, et J. Ramon (2003). Graph kernels and gaussian processes for rela-
tional reinforcement learning. In ILP, pp. 146–163.
Gärtner, T., Q. V. Le, et A. J. Smola (2006). A short tour of kernel methods for graphs.
Haussler, D. (1999). Convolution kernels on discrete structures. Technical Report UCS-CRL-
99-10, UC Santa Cruz.
Joachims, T. (2002). Learning to Classify Text Using Support Vector Machines : Methods,
Theory and Algorithms. Norwell, MA, USA : Kluwer Academic Publishers.
Kashima, H. et A. Inokuchi (2002). Kernels for graph classification. In ICDM ’02 : Procee-
dings of the First International Conference On Data Mining, Workshop on Active Mining.
Kashima, H. et T. Koyanagi (2002). Kernels for semi-structured data. In ICML ’02 : Procee-
dings of the Nineteenth International Conference on Machine Learning, San Francisco, CA,
USA, pp. 291–298. Morgan Kaufmann Publishers Inc.
Kashima, H. et Y. Tsuboi (2004). Kernel-based discriminative learning algorithms for labeling
sequences, trees, and graphs. In ICML ’04 : Proceedings of the twenty-first international
conference on Machine learning, New York, NY, USA, pp. 58. ACM Press.
Kashima, H., K. Tsuda, et A. Inokuchi (2003). Marginalized kernels between labeled graphs.
In T. Faucett et N. Mishra (Eds.), Proceedings of the 20th International Conference on Ma-
chine Learning, pp. 321–328. AAAI Press.
Leslie, C. S., E. Eskin, et W. S. Noble (2002). The spectrum kernel : A string kernel for svm
protein classification. In Pacific Symposium on Biocomputing, pp. 566–575.
Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, et C. J. C. H. Watkins (2002). Text
classification using string kernels. Journal of Machine Learning Research 2, 419–444.
Shawe-Taylor, J. et N. Cristianini (2004). Kernel Methods for Pattern Analysis. Cambridge
University Press.
Suzuki, J., T. Hirao, Y. Sasaki, et E. Maeda (2003a). Hierarchical directed acyclic graph kernel :
methods for structured natural language data. In ACL ’03 : Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics, Morristown, NJ, USA, pp. 32–39.
Association for Computational Linguistics.
Suzuki, J., Y. Sasaki, et E. Maeda (2003b). Kernels for structured natural language data. In
NIPS : Advances in Neural Information Processing Systems 16. MIT Press.
Vapnik, V. N. (1995). The nature of statistical learning theory. New York, NY, USA : Springer-
Verlag New York, Inc.
Méthodes à noyaux appliquées aux textes structurés
Summary
This paper review the application of kernel methods to the mining of structured data. Mod-
ern applications of data mining must handle structured data, e.g. for text mining, and learning
algorithms should benefit of the use of this structural information, which is an interesting chal-
lenge. One of the possible approach to this problem is the use of Mercer’s kernels. These
kernels compute a similarity measure between complex data, and can be used in a lot of learn-
ing algorithms (Support Vector Machines, PCA, Discriminant Analysis, Perceptron, etc). We
present the most important kernels proposed during the last years to handle structured data like
sequences, trees and graphs.
