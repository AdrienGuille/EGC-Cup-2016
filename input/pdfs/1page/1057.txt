Optimisation incrémentale de réseaux de neurones RBF pour
la régression via un algorithme évolutionnaire : RBF-Gene
Virginie LEFORT ∗, Guillaume BESLON ∗∗
∗Laboratoire ERIC, Université Lumière Lyon 2, 5 avenue Pierre Mendès-France,
69676 Bron Cedex, France, virginie.lefort@univ-lyon2.fr
∗∗Laboratoire LIRIS, UMR 5205 CNRS, Bâtiment Blaise Pascal, INSA de Lyon,
69621 Villeurbanne Cedex FRANCE, guillaume.beslon@liris.cnrs.fr
Résumé. Les réseaux de neurones RBF sont d’excellents régresseurs. Ils sont
cependant difficiles à utiliser en raison du nombre de paramètres libres : nombre
de neurones, poids des connexions, ... Des algorithmes évolutionnaires permettent
de les optimiser mais ils sont peu nombreux et complexes. Nous proposons ici un
nouvel algorithme, RBF-Gene, qui permet d’optimiser la structure et les poids
du réseau, grâce à une inspiration biologique. Il est compétitif avec les autres
techniques de régression mais surtout l’évolution peut choisir dynamiquement
le nombre de neurones et la précision des différents paramètres.
1 Introduction
Les réseaux de neurones supervisés sont d’excellents régresseurs permettant à la fois de
classer des données ou de trouver des relations entre des entrées et des sorties (régression).
Cependant ils sont bien souvent durs à mettre en œuvre de part le nombre important de para-
mètres. L’utilisation d’algorithmes évolutionnaires (en particulier les algorithmes génétiques)
permet de faciliter la mise en œuvre de ces réseaux, en définissant leur structure ou les poids
des connexions. Grâce à une inspiration fortement biologique, nous proposons un nouvel al-
gorithme, RBF-Gene, qui permet une optimisation incrémentale d’un réseau RBF (structure et
connexions) de manière efficace.
2 Algorithmes évolutionnaires et réseaux de neurones
Dans un réseau de neurones chaque neurone réalise un traitement simple et ce sont le
nombre de neurones et leur connectivité qui vont faire toute la puissance du réseau. Dans
les réseaux de neurones dits "en couche", le réseau est constitué de trois sous-ensembles de
neurones : les neurones d’entrée, les neurones cachés (complètement connectés aux neurones
d’entrée ou, si le réseau possède plus d’une couche cachée, à la couche précédente) et les
neurones de sortie connectés à la dernière couche de neurones cachés.
Les réseaux RBF (Poggio et Girosi, 1989), pour “Radial Basis Function”, sont des réseaux
à une couche cachée dont les neurones cachés utilisent une fonction de transfert gaussienne
tandis que les neurones de sortie réalisent une "simple" somme pondérée des réponses des
