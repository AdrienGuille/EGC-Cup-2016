Probabilistic Multi-classifier by SVMs
from voting rule to voting features
Anh Phuc TRINH, David BUFFONI, Patrick GALLINARI∗
∗Laboratoire d’Informatique de Paris 6
104, avenue du Président Kennedy, 75016 Paris.
{anh-phuc.trinh,david.buffoni,patrick.gallinari}@lip6.fr,
1 Probabilistic multi-classifier by SVMs
Definition of the posterior probabilities for multiclass problem
Let S = {(x1, y1), (x2, y2), . . . , (xm, ym)} be a set of m training examples. We assume
that each example xi is drawn from a domain X ∈ R
n and each class yi is an integer from
the set Y = {1, . . . , k} with k > 2. The posterior probabilities of multiclass problem is a
conditional probability of each class y ∈ Y given an instance x
P (y = i|x) = pi (1)
subject to
k∑
i=1
pi = 1 pi > 0 ∀i (2)
There are two approaches, either one-vs-one or one-vs-rest, in solving the multi-class pro-
blem by SVMs. Following the setting of the one-vs-one approach, we have the voting method
proposed by (Tax, 2002) using decision values fij(x) of SVMs to estimate the posterior proba-
bilities. Another method of (Wu T-F, 2004) obtains pi from the pairwise probability of (Platt,
2000).
2 From voting rule to voting features
Definition of the voting features
Suppose that S = {(x1, y1), (x2, y2), . . . , (xm, ym)} is the set of m training examples
drawn from an independent and identical distribution. A voting feature representation Θ :
C(fij(x))×Y → B
d is a functionΘ that maps a configuration of decision values c(fij(x)) ⊂
C(fij(x)) and a class yi ∈ Y to a d-dimensional feature vector, thus the set of voting features
is denoted by VF.
The posterior probabilities definied on the set of voting features VF pi = P (y = i|x, λ) =
exp(
Pd
l=1
λl×Θl(x,y=i))P
k
y=1
exp(
P
d
l=1
λl×Θl(x,y))
is estimated in maximizing the logarithm of the conditional likeli-
hood (Nigam et McCallum, 1999) and is solved by unconstrained optimization problem.
