L’apprentissage statistique a` grande e´chelle
Le´on Bottou1, Olivier Bousquet2
1 NEC Labs America, Princeton, USA
2 Google, Zurich, Suisse
Re´sume´ Depuis une dizaine d’anne´es, la taille des donne´es croit plus vite que la
puissance des processeurs. Lorsque les donne´es disponibles sont pratiquement infi-
nies, c’est le temps de calcul qui limite les possibilite´s de l’apprentissage statistique.
Ce document montre que ce changement d’e´chelle nous conduit vers un compromis
qualitativement diffe´rent dont les conse´quences ne sont pas e´videntes. En particu-
lier, bien que la descente de gradient stochastique soit un algorithme d’optimisation
me´diocre, on montrera, en the´orie et en pratique, que sa performance est excellente
pour l’apprentissage statistique a` grande e´chelle.
1 Introduction
La the´orie de l’apprentissage statistique prend rarement en compte le couˆt des algo-
rithmes d’apprentissage. Vapnik [1] ne s’y inte´resse pas. Valiant [2] exclue les algorithmes
d’apprentissage dont le couˆt croit exponentiellement. Cependant, malgre´ de nombreux
progre`s sur les aspects statistiques [3, 4], peu de re´sultats concerne la complexite´ des
algorithmes d’apprentissage (e.g., [5].)
Ce document de´veloppe une ide´e simple : une optimisation approximative est souvent
suffisante pour les besoins de l’apprentissage. La premie`re partie reprend la de´composition
de l’erreur de pre´vision propose´e dans [6] dans laquelle un terme supple´mentaire qui de´crit
les conse´quences de l’optimisation approximative. Dans le cas de l’apprentissage a` petite
e´chelle, cette de´composition de´crit le compromis habituel entre approximation et esti-
mation. Dans le cas de l’apprentissage a` grande e´chelle, elle de´crit une situation plus
complexe qui de´pend en particulier du couˆt de calcul associe´ a` l’algorithme d’apprentis-
sage. La seconde partie explore les proprie´te´s asymptotiques de l’apprentissage a` grande
e´chelle lorsque l’on utilise diverses me´thodes d’optimisation. Ces re´sultats montrent clai-
rement que le meilleur algorithme d’optimisation n’est pas ne´cessairement le meilleur
algorithme d’apprentissage. Finalement, cette analyse est confirme´e par quelques compa-
raisons expe´rimentales.
2 Optimisation approximative
Comme [7, 1], conside´rons un espace de paires entre´es-sorties (x, y) ∈ X × Y e´quippe´
d’une loi jointe de probabilite´ P (x, y). La loi conditionelle P (y|x) repre´sente la relation
inconnue qui lie entre´es et sorties. Une fonction de perte `(yˆ, y) mesure l’e´cart entre la
sortie pre´dite yˆ et la sortie observe´e y. Notre objectif est la fonction f ∗ qui minimise le
risque moyen
E(f) =
∫
`(f(x), y) dP (x, y) = E [`(f(x), y)],
c© Revue MODULAD, 2010 -61- Nume´ro 42
