La place de l’a priori dans l’analyse des données économiques  
ou le programme fort des  méthodes inductives au service de l’hétérodoxie 
Dominique DESBOIS 
 
 
Selon Jean-Paul Benzécri, dans une introduction à un volume présentant les travaux de 
son équipe appliquées aux données économiques [Benzécri, 1986], «  …il est réservé à 
l’Analyse des Données d’exprimer adéquatement les lois de ce qui, complexe par essence 
(être vivant, corps social, écosystème) ne peut être disséqué sans perdre sa nature ». Une 
telle affirmation qui semble vouloir ériger ce champ de savoirs et de pratiques au rang d’une 
véritable discipline scientifique s’oppose à la fois dans la forme et dans l’esprit à la 
formulation utilisée par Michel Volle dans l’avant-propos d’un ouvrage didactique [Volle, 
1978] qui définit l’analyse des données comme une collection d’instruments de statistique 
descriptive lui niant explicitement le statut de « théorie logiquement articulée et cohérente en 
toutes ses parties ».  
Le positionnement épistémologique de Jean-Paul Benzécri a été explicité dans un article 
écrit pour l’Encyclopedia Universalis [Benzécri, 1973] qui fait figure de véritable manifeste 
car il en appelle à refonder la pratique des statisticiens sur la base d’un « Novius Organum » 
qui serait basé sur l’analyse des correspondances, technique d’analyse factorielle dont il fût un 
des principaux concepteurs s’engouffrant dans la voie frayée par Ronald Fisher, Louis 
Guttman et Chikio Hayashi : « Cet outil nouveau qu'est l'ordinateur électronique peut 
permettre de substituer à des notions qualitatives du sens commun des quantités définies 
statistiquement ; en sorte que l'édifice, fondé sur une ample base de faits, s'affranchira, dans 
sa structure définitive, de l'arbitraire échafaudage des idées a priori ». 
L’hypothèse que nous formulons est que ce « Novius Organum » que Jean-Paul 
Benzécri appelle de ses vœux a fonctionné dans les faits plutôt comme un outil de 
déconstruction au service des tendances hétérodoxes de l’époque en économie, et plus 
généralement dans les sciences sociales. 
 
Le programme inductiviste fort d’une économétrie sans modèle aléatoire 
Lorsque Jean-Paul Benzécri  déclare « Le modèle doit suivre les donnés et non 
l’inverse ! » G. Morlat répond dans une préface à l’ouvrage de [Caillez & Pages, 1976], 
« l’analyse des données n’est pas autre chose que la forme moderne de la statistique 
descriptive ». En effet « Contrairement à la statistique néobayésienne1, qui vise à consolider 
la formalisation de l’induction, en consolidant le ‘sable’ sur lequel s’appuyait la théorie 
statistique, on peut dire qu’on a retiré, en analyse des données, non seulement le sable mais 
tout ce qui reposait dessus, c'est-à-dire le modèle probabiliste ». L’analyse des données est 
donc située très explicitement dans le cadre d’une statistique mathématique sans modèle 
aléatoire. 
Sur la base du corpus de travaux délimité par ceux présentés dans les Cahiers de 
l’Analyse des données (1976-1997), revue de l’Institut de Statistiques de l’Université de Paris 
(ISUP) dirigée par Jean-Paul Benzécri, il est manifeste que les travaux effectués se situent 
dans le cadre de ce qu’Edmond Malinvaud a qualifié d’Econométrie sans modèle aléatoire 
([Malinvaud, 1964], cf. chapitre 1). Dans la littérature économique, l’économétrie 
multidimensionnelle sans modèle aléatoire remonte au moins jusqu’à la méthode des 
faisceaux (« bunch map ») proposée par Ragnar Frisch [Frisch, 1934]. Ce point de vue est 
                                                 
1 G. Morlat attire l’attention sur le développement des méthodes bayésiennes impulsé par l’exploitation du 
théorème de Richard Threlkeld Cox (1946)  qu’introduisit Edwin Thompson Jaynes, dont l’importance fût 
vraisemblablement sous-estimée en France à cette époque. 
© Revue MODULAD, 2009 - 176 -       Numéro 39 
