series	booktitle	year	title	abstract	authors	pdf1page	pdfarticle	id	lang_title	lang_abstract	affiliations	long_content	short_content	n_citations	
Revue des Nouvelles Technologies de l'Information	EGC	2015	A Clustering Based Approach for Type Discovery in RDF Data Sources	RDF(S)/OWL data sources are not organized according to a predefined schema, as they are structureless by nature. This lack of schema limits their use to express queries or to understand their content. Our work is a contribution towards the inference of the structure of RDF(S)/OWL data sources. We present an approach relying on density-based clustering to discover the types describing the entities of possibly incomplete and noisy data sets.	Kenza Kellou-Menouer, Zoubida Kedad	http://editions-rnti.fr/render_pdf.php?p1&p=1002113		15	en	en	@prism.uvsq.fr, @prism.uvsq.fr	cluster base approach type discovery rdf data source rdf s owl data source organize accord predefined schema structureless nature lack schema limit use express query understand content work contribution towards inference structure rdf s owl data source present approach rely density base cluster discover type describe entity possibly incomplete noisy datum set	A Clustering Based Approach for Type Discovery in RDF Data Sources	2
Revue des Nouvelles Technologies de l'Information	EGC	2015	A Framework for Mesh Segmentation and Annotation using Ontologies	La segmentation et annotation de maillages utilisant la sémantique a été l'objet d'un intérêt grandissant avec la démocratisation des techniques de reconstruction 3D. Une approche classique consiste à réaliser cette tâche en deux étapes, tout d'abord en segmentant le maillage, puis en l'annotant. Cependant, cette approche ne permet pas à chaque étape de profiter de l'autre. En traitement d'images, quelques méthodes combinent la segmentation et l'annotation, mais ces approches ne sont pas génériques, et nécessitent des ajustements d'implémentation ou des réécritures pour chaque modification des connaissances expertes. Dans ce travail, nous décrivons un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre d'étapes de segmentation, et nous présentons des résultats préliminaires qui montrent la faisabilité de l'approche.Notre système fournit une ontologie générique qui décrit sous forme de concepts les propriétés d'un objet (géométrie, topologie, etc.), ainsi que des algorithmes permettant de détecter ces concepts. Cette ontologie peut être étendue par un expert pour décrire formellement un domaine spécifique. La description formelle du domaine est alors utilisée pour réaliser automatiquement l'assemblage de la segmentation et de l'annotation d'objets et de leurs propriétés, en sélectionnant à chaque étape l'algorithme le plus pertinent, étant données les information sémantiques déjà détectées. Cette approche originale comporte plusieurs avantages. Tout d'abord, elle permet de segmenter et d'annoter des objets sans aucune connaissance en traitement d'images ou de maillages, en décrivant uniquement les propriétés de l'objet en terme de concepts ontologiques. De plus, ce cadre de fontionnement peut facilement être réutilisé et appliqué à différents contextes, dès lors qu'une ontologie de domaine a été définie. Finalement, la réalisation conjointe de la segmentation et de l'annotation permet d'utiliser d'une manière efficace la connaissance experte, en réduisant les erreurs de segmentation et le temps de calcul, en lançant toujours l'algorithme le plus pertinent.	Thomas Dietenbeck, Ahlem Othmani, Marco Attene, Jean-Marie Favreau	http://editions-rnti.fr/render_pdf.php?p1&p=1002088		16	en	fr		le segmentation et annotation de maillage utiliser le sémantique avoir être le objet d' un intérêt grandissant avec le démocratisation des technique de reconstruction 3D. un approche classique consister à réaliser ce tâche en deux étape , tout d' abord en segmenter le maillage , puis en l' annoter . cependant , ce approche ne permettre pas à chaque étape de profiter de le autre . En traitement d' image , quelque méthode combiner le segmentation et le annotation , mais ce approche ne être pas générique , et nécessiter un ajustement d' implémentation ou un réécriture pour chaque modification des connaissance expert . Dans ce travail , nous décrire un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre d' étape de segmentation , et nous présenter un résultat préliminaire qui montrer le faisabilité de le approche . son système fournir un ontologie générique qui décrire sous forme de concept le propriété d' un objet ( géométrie , topologie , etc. ) , ainsi que un algorithme permettre de détecter ce concept . ce ontologie pouvoir être étendre par un expert pour décrire formellement un domaine spécifique . le description formel du domaine être alors utiliser pour réaliser automatiquement le assemblage de le segmentation et de le annotation d' objet et de son propriété , en sélectionner à chaque étape le algorithme le plus pertinent , être donner le information sémantique déjà détecter . ce approche original comporter plusieurs avantage . tout d' abord , elle permettre de segmenter et d' annoter un objet sans aucun connaissance en traitement d' image ou de maillage , en décrire uniquement le propriété de le objet en terme de concept ontologique . De plus , ce cadre de fontionnement pouvoir facilement être réutiliser et appliquer à différents contexte , dès lors qu' un ontologie de domaine avoir être définir . finalement , le réalisation conjoindre de le segmentation et de le annotation permettre d' utiliser d' un manière efficace le connaissance expert , en réduire le erreur de segmentation et le temps de calcul , en lancer toujours le algorithme le plus pertinent . 	A Framework for Mesh Segmentation and Annotation using Ontologies	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse des paramètres de recherche d'information: Etude de l'influence des paramètres sur les résultats	Cet article présente une analyse détaillée d'un ensemble de 2 millions de résultats de recherche d'information obtenus par différents paramétrages de systèmes de recherche d'information. Plus spécifiquement, nous avons utilisé la plateforme Terrier et l'interface RunGeneration pour créer différentes exécutions (run en anglais) en modifiant les modèles d'indexation et de recherche. Nous avons ensuite évalué chacun des résultats obtenus selon différentes mesures de performance de recherche d'information. Une analyse systématique a été menée sur ces données afin de déterminer d'une part quels étaient les paramètres qui ont le plus d'influence, d'autre part quels étaient les valeurs de ces paramètres les plus susceptibles de conduire à de bonnes performances du système.	Josiane Mothe, Marion Moulinou	http://editions-rnti.fr/render_pdf.php?p1&p=1002059		17	fr	fr	@irit.fr	analyse des paramètre de recherche d' information : Etude de le influence des paramètre sur le résultats  ce article présenter un analyse détailler d' un ensemble de 2 million de résultat de recherche d' information obtenir par différent paramétrages de système de recherche d' information . plus spécifiquement , nous avoir utiliser le plateforme Terrier et le interface RunGeneration pour créer différent exécution ( run en anglais ) en modifier le modèle d' indexation et de recherche . Nous avoir ensuite évaluer chacun des résultat obtenir selon différentes mesure de performance de recherche d' information . un analyse systématique avoir être mener sur ce donnée afin de déterminer d' un part quel étayer le paramètre qui avoir le plus d' influence , d' autre part quel étayer le valeur de ce paramètre le plus susceptible de conduire à un bon performance du système . 	Analyse des paramètres de recherche d'information: Etude de l'influence des paramètres sur les résultats	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse et visualisation d'opinions dans un cadre de veille sur leWeb	L'analyse d'opinions est une tâche qui consiste en l'identification et la classification de textes subjectifs. Dans ce travail, nous nous intéressons au problème d'analyse d'opinions dans un contexte de veille sur le Web. Nous proposons une approche pour visualiser les résultats d'analyse d'opinions, basée sur l'utilisation de termes clés. Nous décrivons également la plateforme de veille sur leWeb AMIEI, au sein de laquelle notre approche a été implémentée. La démonstration consistera en une expérimentation de la plateforme de veille AMIEI et du module d'analyse d'opinions sur un corpus de tweets politiques.	Mohamed Dermouche, Leila Khouas, Sabine Loudcher, Julien Velcin, Eric Fourboul	http://editions-rnti.fr/render_pdf.php?p1&p=1002110		18	fr	fr	@univ-lyon2.fr, @amisw.com	analyse et visualisation d' opinion dans un cadre de veille sur leWeb  le analyse d' opinion être un tâche qui consister en le identification et le classification de texte subjectif . Dans ce travail , nous clr intéresser au problème d' analyse d' opinion dans un contexte de veille sur le Web . Nous proposer un approche pour visualiser le résultat d' analyse d' opinion , baser sur le utilisation de terme clé . Nous décrire également le plateforme de veille sur leWeb AMIEI , au sein de laquelle son approche avoir être implémenter . le démonstration consister en un expérimentation de le plateforme de veille AMIEI et du module d' analyse d' opinion sur un corpus de tweets politique . 	Analyse et visualisation d'opinions dans un cadre de veille sur leWeb	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse OLAP sur des tweets et des blogs : un retour d'expérience	Le projet ANR IMAGIWEB dans lequel s'inscrit ce travail s'est donné pour mission d'étudier les images véhiculées sur Internet en se basant sur la détection d'opinions. Deux cas d'étude ont été définis : (1) le premier vise à répondre aux besoins d'analyse de chercheurs en science politique grâce à des données issues de Twitter durant la campagne présidentielle de 2012 ; (2) le second doit permettre à l'entreprise française EDF d'évaluer l'opinion du public en matière de sécurité, d'emploi et de prix à partir de billets de blogs. Dans cet article, nous présentons un retour d'expérience sur l'usage de l'analyse en ligne OLAP (OnLine Analytical Processing) pour des données textuelles, mettant en avant l'intérêt de ce type d'analyse pour les membres du projet.	Brice Olivier, Cécile Favre, Sabine Loudcher	http://editions-rnti.fr/render_pdf.php?p1&p=1002106		19	fr	fr	@univ-lyon2.fr	analyse OLAP sur un tweets et des blog : un retour d' expérience  le projet ANR IMAGIWEB dans lequel clr inscrire ce travail clr être donner pour mission d' étudier le image véhiculer sur Internet en clr baser sur le détection d' opinion . Deux cas d' étude avoir être définir : ( 1 ) le premier viser à répondre aux besoin d' analyse de chercheur en science politique grâce à un donnée issu de Twitter durant le campagne présidentiel de 2012 ; ( 2 ) le second devoir permettre à le entreprise français EDF d' évaluer le opinion du public en matière de sécurité , d' emploi et de prix à partir de billet de blog . Dans ce article , nous présenter un retour d' expérience sur le usage de le analyse en ligne OLAP ( OnLine Analytical Processing ) pour un donnée textuel , mettre en avant le intérêt de ce type d' analyse pour le membre du projet . 	Analyse OLAP sur des tweets et des blogs : un retour d'expérience	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse visuelle pour la détection des intrusions	"La démocratisation d'Internet, couplée à l'effet de la mondialisation, a pour résultat d'interconnecter les personnes, les états et les entreprises. Le côté déplaisant de cette interconnexion mondiale des systèmes d'information réside dans un phénomène appelé ""Cybercriminalité"". Nous proposons une méthode de visualisation de grands ""graphes"" et l'exploitation d'analyses statiques des flux permettant de détecter les comportements anormaux et dangereux afin d'appréhender les risques d'une façon compréhensible par tous les acteurs."	David Pierrot, Nouria Harbi	http://editions-rnti.fr/render_pdf.php?p1&p=1002079		20	fr	fr	@univ-lyon2.fr	analyse visuel pour le détection des intrusions  " le démocratisation d' Internet , coupler à le effet de le mondialisation , avoir pour résultat d' interconnecter le personne , le état et le entreprise . . le côté déplaisant de ce interconnexion mondial des système d' information résider dans un phénomène appeler " " Cybercriminalité " " . . Nous proposer un méthode de visualisation de grand " " graphe " " et le exploitation d' analyse statique des flux permettre de détecter le comportement anormal et dangereux afin d' appréhender le risque d' un façon compréhensible par tout le acteur . " 	Analyse visuelle pour la détection des intrusions	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Approche d'extraction de classes interlangues à partir de documents multilingues à base de Concepts Fermés	In this article, we highlight the interest and usefulness of Formal Concept Analysis (FCA) in multilingual document clustering. We propose a statistical approach for clustering multilingual documents based on Closed Concepts and vector model partition the documents of one or more collections.An experimental evaluation was conducted on the collection of bilingual documents French-English of CLEF' 2 2003 and showed the merits of this method and the interesting degree of comparability of the obtained bilingual classes.	Mohamed Chebel, Chiraz Latiri	http://editions-rnti.fr/render_pdf.php?p1&p=1002119		21	fr	en	@gmail.com, @gnet.tn	approche d' extraction de classe interlangues à partir de document multilingue à base de concept Fermés 	Approche d'extraction de classes interlangues à partir de documents multilingues à base de Concepts Fermés	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Approche relationnelle de l'apprentissage de séquences	We observe an increasing amount of sequential data, for instance open data sources provide real-time information. In order to apply classical learning algorithms, sequential data are often modelled in an attribute-value setting using a sliding window. In this paper, we propose a relational approach. A first advantage is to let the relational algorithm choose the length of the window. A second advantage is to allow to consider conditions based on the existential quantifier and aggregates. A third advantage is to be able to consider several granularities at the same time.	Clément Charnay, Nicolas Lachiche, Agnès Braud	http://editions-rnti.fr/render_pdf.php?p1&p=1002118		22	fr	en	@unistra.fr	approche relationnel de le apprentissage de séquences 	Approche relationnelle de l'apprentissage de séquences	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Big Data and the Dawn of Algorithms in Everything	The mainstream adoption of the internet as a source for knowledge and interaction for the past decades has given rise to new data sources that are characterized by large sizes and rapid creation. In addition, sensory data from mobile devices and machinery are on the rise with similar characteristics. All these sources have the commonality that they will tell us something new or something more detailed than before. From a business standpoint these data sources holds the opportunity to create more customized services and improved products in practically anything, however, they also present a challenge since they are big and typically residing outside the traditional server structure of organizations. This talk will explore the challenges of integrating these new, so-called Big Data, in decision processes. Specifically, we will explore the paradigm shifts when external data become equally or more important than internal data. We will also explore the emerging shift in decision making becoming algorithmic as opposed to human discovery driven.	Morten Middelfart	http://editions-rnti.fr/render_pdf.php?p1&p=1002057		23	en	en	@targit.com	big datum dawn algorithms everything mainstream adoption internet source knowledge interaction past decade give rise new data source characterize large size rapid creation addition sensory datum mobile device machinery rise similar characteristic source commonality tell us something new something detail before business standpoint data source hold opportunity create customize service improve product practically anything however also present challenge since big typically reside outside traditional server structure organization talk explore challenge integrate new so call big datum decision process specifically explore paradigm shift external datum become equally important internal datum also explore emerge shift decision make become algorithmic opposed human discovery drive	Big Data and the Dawn of Algorithms in Everything	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Big Data is all about data that we don't have	Big Data is now becoming a buzz word in information technology industry and research. Is Big Data only about large volume of data?, and if it is yes, why is it suddenly becoming a trend. Hasn't the growth of data volume been gigantic in the last decade? From a research point of view, it is not surprising to see researchers from all walks of computer science are trying to align their research to Big Data for the sake of being trendy. The question remains whether it tackles the real Big Data problems. In this talk, I will describe the misconceptions of Big Data, present motivating cases, and discuss the unavoidable challenges faced by industry and research.	David Taniar	http://editions-rnti.fr/render_pdf.php?p1&p=1002055		24	en	en	@monash.edu	big datum datum don t big datum become buzz word information technology industry research big datum large volume datum yes suddenly become trend hasn t growth data volume gigantic last decade research point view surprising see researcher walk computer science try align research big datum sake trendy question remain whether tackle real big datum problem talk describe misconception big datum present motivate case discuss unavoidable challenge face industry research	Big Data is all about data that we don't have	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Challenges and Opportunities in HCI, Visual Analytics and Knowledge Management for the development of Sustainable Cities	While overtly exposed in the media, the challenges faced by our societies to transition towards sustainable energy use are quite formidable. A simple visual refresher of the cold hard facts should amply reveal the importance of visualization to assess the situation. Private companies, such as IBM, and public research centers are joining forces and investing to design and evaluate novel approaches to build and manage Cities, defined as the rational organisation of dense human habitat. Information and Communication technologies are certainly part of the answers, in particular in areas related to knowledge management, data mining, HCI and social computing. Illustrated with telltaling examples of research work carried at IBM, the CSTB and the Efficacity Institute, I will argue that Interactive Information Technologies can help managing the energy transition of cities in 3 key aspects:   -- to support the city design process, notably computer supported tooling and information infrastructure that help taming the complexity of the intertwinning actors and interests at play,   -- to help understand better the city's dynamics, identifiy inefficiencies and reveal optimization opportunities, where knowledge management and extraction is crucial,   -- and foremost, to ease the necessary changes that will have to happen in our mobility and housing habits with novel tools and services that alleviate our energy needs.	Thomas Baudel	http://editions-rnti.fr/render_pdf.php?p1&p=1002058		25	en	en	@fr.ibm.com	challenge opportunity hci visual analytic knowledge management development sustainable city overtly exposed medium challenge face society transition towards sustainable energy use quite formidable simple visual refresher cold hard fact amply reveal importance visualization assess situation private company ibm public research center join force invest design evaluate novel approach build manage city define rational organisation dense human habitat information communication technology certainly part answer particular area related knowledge management data mining hci social compute illustrated telltaling example research work carry ibm cstb efficacity institute argue interactive information technology help manage energy transition city 3 key aspect support city design process notably computer support tool information infrastructure help tame complexity intertwinning actor interest play help understand better city s dynamic identifiy inefficiency reveal optimization opportunity knowledge management extraction crucial foremost ease necessary change happen mobility housing habit novel tool service alleviate energy need	Challenges and Opportunities in HCI, Visual Analytics and Knowledge Management for the development of Sustainable Cities	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Choix d'une mesure de proximité discriminante dans un contexte topologique	"Les résultats de toute opération de classification ou de classement d'objets dépendent fortement de la mesure de proximité choisie. L'utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d'équivalence topologique fait appel à la structure de voisinage local.Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la ""meilleure"" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la ""meilleure"" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d'apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique.Le principe de l'approche proposée est illustré à partir d'un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d'évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la ""meilleur"" mesure de proximité discriminante."	Fatima-Zahra Aazi, Rafik Abdesselam	http://editions-rnti.fr/render_pdf.php?p1&p=1002069		26	fr	fr	@gmail.com, @univ-lyon2.fr	choix d' un mesure de proximité discriminant dans un contexte topologique  " le résultat de tout opération de classification ou de classement d' objet dépendre fortement de le mesure de proximité choisir . . le utilisateur être amener à choisir un mesure parmi le nombreux mesure de proximité existant . . Or , selon le notion d' équivalence topologique choisir , certains être plus ou moins équivalent . . Dans ce article , nous proposer un nouveau approche de comparaison et de classement de mesure de proximité , dans un structure topologique et dans un objectif de discrimination . . le concept d' équivalence topologique faire appel à le structure de voisinage local . . Nous proposer alors de définir le équivalence topologique entre deux mesure de proximité à travers le structure topologique induire par chaque mesure dans un contexte de discrimination . . Nous proposer également un critère pour choisir le " " meilleur " " mesurer adapter aux donnée considérer , parmi quelque mesure de proximité le plus utiliser dans le cadre de donnée quantitatif . . le choix de le " " meilleur " " mesure de proximité discriminant pouvoir être vérifier avoir posteriori par un méthode d' apprentissage superviser de type SVM , analyse discriminant ou encore régression Logistique , appliquer dans un contexte topologique . . le principe de le approche proposer être illustrer à partir d' un exemple de donnée quantitatif réel avec huit mesure de proximité classique de le littérature . . un expérimentation avoir permettre d' évaluer le performance de ce approche topologique de discrimination en terme de taille et de dimension des donnée considérer et de sélection de le " " meilleur " " mesure de proximité discriminant . " 	Choix d'une mesure de proximité discriminante dans un contexte topologique	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Classification évidentielle avec contraintes d'étiquettes	Ce papier propose une version améliorée de l'algorithme de classification automatique évidentielle semi-supervisée SECM. Celui-ci bénéficie de l'introduction de données étiquetées pour améliorer la pertinence de ses résultats et utilise la théorie des fonctions de croyance afin de produire une partition crédale qui généralise notamment les concepts de partitions dures et floues. Le pendant de ce gain d'expressivité est une complexité qui est exponentielle avec le nombre de classes, ce qui impose en retour l'utilisation de schémas efficaces pour optimiser la fonction objectif. Nous proposons dans cet article une heuristique qui relâche la contrainte classique de positivité liée aux masses de croyances des méthodes évidentielles. Nous montrons sur un ensemble de jeux de données de test que notre méthode d'optimisation permet d'accélérer sensiblement l'algorithme SECM avec un schéma d'optimisation classique, tout en améliorant également la qualité de la fonction objectif.	Violaine Antoine, Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1002071		27	fr	fr	@univ-bpclermont.fr, @univ-tours.fr	classification évidentielle avec contrainte d' étiquettes  ce papier proposer un version améliorer de le algorithme de classification automatique évidentielle semi-supervisée SECM . celui _-ci bénéficier de le introduction de donnée étiqueter pour améliorer le pertinence de son résultat et utiliser le théorie des fonction de croyance afin de produire un partition crédale qui généraliser notamment le concept de partition dur et flou . le pendant de ce gain d' expressivité être un complexité qui être exponentiel avec le nombre de classe , ce qui imposer en retour le utilisation de schéma efficace pour optimiser le fonction objectif . Nous proposer dans ce article un heuristique qui relâche le contrainte classique de positivité lier aux masse de croyance des méthode évidentielles . Nous montrer sur un ensemble de jeu de donnée de test que son méthode d' optimisation permettre d' accélérer sensiblement le algorithme SECM avec un schéma d' optimisation classique , tout en améliorer également le qualité de le fonction objectif . 	Classification évidentielle avec contraintes d'étiquettes	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Classification multi-label par raisonnement logique pour l'indexation sémantique de documents	Cet article présente une solution centrée sur les ontologies pour la classification multi-label automatique d'information nécessaire à un système de recommandation d'informations économiques.	David Werner, Christophe Cruz, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1002114		28	fr	fr	@u-bourgogne.fr, @u-bourgogne.fr, @u-bourgogne.fr	classification multi-label par raisonnement logique pour le indexation sémantique de documents  ce article présenter un solution centrer sur le ontologie pour le classification multi-label automatique d' information nécessaire à un système de recommandation d' information économique . 	Classification multi-label par raisonnement logique pour l'indexation sémantique de documents	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Clustering topologique pour le flux de données	Actuellement, le clustering de flux de données devient le moyen le plus efficace pour partitionner un très grand ensemble de données. Dans cet article, nous présentons une nouvelle approche topologique, appelée G-Stream, pour le clustering de flux de données évolutives. La méthode proposée est une extension de l'algorithme GNG (Growing Neural Gas) pour gérer le flux de données. G-Stream permet de découvrir de manière incrémentale des clusters de formes arbitraires en ne faisant qu'une seule passe sur les données. Les performances de l'algorithme proposé sont évaluées à la fois sur des données synthétiques et réelles.	Mohammed Ghesmoune, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1002072		29	fr	fr	@univ-paris13.fr	Clustering topologique pour le flux de données  actuellement , le clustering de flux de donnée devenir le moyen le plus efficace pour partitionner un très grand ensemble de donnée . Dans ce article , nous présenter un nouveau approche topologique , appeler G-Stream , pour le clustering de flux de donnée évolutif . le méthode proposer être un extension de le algorithme GNG ( Growing Neural Gas ) pour gérer le flux de donnée . G-Stream permettre de découvrir de manière incrémentale des clusters de forme arbitraire en ne faire qu' un seul passe sur le donnée . le performance de le algorithme proposer être évaluer à le foi sur un donnée synthétique et réel . 	Clustering topologique pour le flux de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Cohérence des données de bases RDF en évolution constante	Le maintien de la qualité et de la fiabilité de bases de connaissances RDF du Web Sémantique est un problème courant. De nombreuses propositions pour l'intégration de « bonnes » données ont été faites, se basant soit sur les ontologies de ces bases, soit sur des méta-données additionnelles. Dans cet article, nous proposons une approche originale, basée exclusivement sur l'étude des données de la base. Le principe est de déterminer si les modifications apportées par la mise à jour candidate rendent la partie ciblée de la base plus similaire - selon certains critères - à d'autres parties existantes dans la base. La mise à jour est considérée cohérente avec cette base et peut être appliquée.	Pierre Maillot, Thomas Raimbault, David Genest	http://editions-rnti.fr/render_pdf.php?p1&p=1002085		30	fr	fr	@devinci.fr, @univ-angers.fr	cohérence des donnée de base RDF en évolution constante  le maintien de le qualité et de le fiabilité de base de connaissance RDF du Web Sémantique être un problème courant . un nombreux proposition pour le intégration de « bon » donner avoir être faire , clr baser être sur le ontologie de ce base , soit sur un méta-données additionnel . Dans ce article , nous proposer un approche original , baser exclusivement sur le étude des donnée de le base . le principe être de déterminer si le modification apporter par le mise à jour candidat rendre le partie cibler de le base plus similaire - selon certain critère - à un autre partie existant dans le base . le mise à jour être considérer cohérent avec ce base et pouvoir être appliquer . 	Cohérence des données de bases RDF en évolution constante	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Comparison of linear modularization criteria using the relational formalism, an approach to easily identify resolution limit	La modularisation de grands graphes ou recherche de communautés est abordée comme l'optimisation d'un critère de qualité, l'un des plus utilisés étant la modularité de Newman-Girvan. D'autres critères, ayant d'autres propriétés, aboutissent à des solutions différentes. Dans cet article, nous présentons une réécriture relationnelle de six critères linéaires: Zahn-Condorcet, Owsi´nski- Zadro&#729;zny, l'Ecart à l'Uniformité, l'Ecart à l'Indétermination et la Modularité Equilibrée. Nous utilisons une version générique de l'algorithme d'optimisation de Louvain pour approcher la partition optimale pour chaque critère sur des réseaux réels de différentes tailles. Les partitions obtenues présentent des caractéristiques différentes, concernant notamment le nombre de classes. Le formalisme relationnel nous permet de justifier ces différences d'un point de vue théorique. En outre, cette notation permet d'identifier facilement les critères ayant une limite de résolution (phénomène qui empêche en pratique la détection de petites communautés sur de grands graphes). Une étude de la qualité des partitions trouvées dans les graphes synthétiques LFR permet de confirmer ces résultats.	Patricia Conde-Céspedes, Jean-François Marcotorchino, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1002080		31	en	fr	@univ-paris13.fr, @thalesgroup.com	le modularisation de grand graphe ou recherche de communauté être aborder comme le optimisation d' un critère de qualité , le un des plus utiliser être le modularité de Newman-Girvan . un autre critère , avoir un autre propriété , aboutir à un solution différent . Dans ce article , nous présenter un réécriture relationnel de six critère linéaire : Zahn-Condorcet , Owsi'nski-Zadro˙zny , le Ecart à le Uniformité , le Ecart à le indétermination et le modularité Equilibrée . Nous utiliser un version générique de le algorithme d' optimisation de Louvain pour approcher le partition optimal pour chaque critère sur un réseau réel de différent taille . le partition obtenir présenter un caractéristique différent , concerner notamment le nombre de classe . le formalisme relationnel nous permettre de justifier ce différence d' un point de vue théorique . En outre , ce notation permettre d' identifier facilement le critère avoir un limite de résolution ( phénomène qui empêcher en pratique le détection de petit communauté sur un grand graphe ) . un étude de le qualité des partition trouver dans le graphe synthétique LFR permettre de confirmer ce résultat . 	Comparison of linear modularization criteria using the relational formalism, an approach to easily identify resolution limit	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Compromis précision-rappel dans l'évaluation des performances 	Dans de nombreux problèmes d'apprentissage automatique la performance des algorithmes est évaluée à l'aide des mesures précision et rappel. Or ces deux mesures peuvent avoir une importance très différente en fonction du contexte. Dans cet article nous étudions le comportement des principaux indices de performance en fonction du couple précision-rappel. Nous proposons un nouvel outil de visualisation de performances et définissons l'espace de compromis qui représente les différents indices en fonction du compromis précision-rappel. Nous analysons les propriétés de ce nouvel espace et mettons en évidence ses avantages par rapport à l'espace précision-rappel.	Blaise Hanczar, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002070		32	fr	fr	@parisdescartes.fr	compromettre précision-rappel dans le évaluation des performances  Dans un nombreux problème d' apprentissage automatique le performance des algorithme être évaluer à le aide des mesure précision et rappel . Or ce deux mesure pouvoir avoir un importance très différent en fonction du contexte . Dans ce article nous étudier le comportement des principal indice de performance en fonction du couple précision-rappel . Nous proposer un nouveau outil de visualisation de performance et définir le espace de compromis qui représenter le différent indice en fonction du compromis précision-rappel . Nous analyser le propriété de ce nouveau espace et mettre en évidence son avantage par rapport à le espace précision-rappel . 	Compromis précision-rappel dans l'évaluation des performances 	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Contribution au calcul du skyline par réduction de l'espace candidat	L'opérateur skyline est devenu un paradigme dans les bases de données. Il consiste à localiser Sky l'ensemble des points d'un espace vectoriel qui ne sont pas dominés. Cet opérateur est utile lorsqu'on n'arrive pas à se décider dans les situations conflictuelles. Le calcul des requêtes skyline est pénalisé par le nombre de points que peuvent contenir les bases de données. Dans ce papier, nous présentons une solution analytique pour la réduction de l'espace candidat et nous proposons une méthode efficace pour le calcul de ce type de requêtes	Lougmiri Zekri, Hadjer Belaicha	http://editions-rnti.fr/render_pdf.php?p1&p=1002082		33	fr	fr	@gmail.com	contribution au calcul du skyline par réduction de le espace candidat  le opérateur skyline être devenir un paradigme dans le base de donnée . Il consister à localiser Sky le ensemble des point d' un espace vectoriel qui ne être pas dominer . ce opérateur être utile lorsqu' on n' arriver pas à clr décider dans le situation conflictuel . le calcul des requête skyline être pénaliser par le nombre de point que pouvoir contenir le base de donnée . Dans ce papier , nous présenter un solution analytique pour le réduction de le espace candidat et nous proposer un méthode efficace pour le calcul de ce type de requêtes 	Contribution au calcul du skyline par réduction de l'espace candidat	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	D113 : une plateforme open-source dédiée à l'analyse des flux et à la détection des intrusions	"Ce travail se situe dans le domaine de la ""Cybersécurité"", le projet ""D113"" permet de visualiser en temps réel les flux transitant sur des équipements de filtrage sans avoir recours au traitement manuel des journaux d'événements. Nous centrerons notre démonstration sur la visualisation de grands ""graphes"" et l'exploitation d'analyses statiques des flux."	David Pierrot, Nouria Harbi	http://editions-rnti.fr/render_pdf.php?p1&p=1002108		34	fr	fr	@univ-lyon2.fr	D113 : un plateforme open-source dédier à le analyse des flux et à le détection des intrusions  " ce travail clr situer dans le domaine de le " " Cybersécurité " " , le projet " " D113 " " permettre de visualiser en temps réel le flux transiter sur un équipement de filtrage sans avoir recours au traitement manuel des journal d' événement . . Nous centrer son démonstration sur le visualisation de grand " " graphe " " et le exploitation d' analyse statique des flux . " 	D113 : une plateforme open-source dédiée à l'analyse des flux et à la détection des intrusions	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Découverte de proportions analogiques dans les bases de données : une première approche	Cet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue unCet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue une première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».e première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».	William Correa Beltran, Hélène Jaudoin, Olivier Pivert	http://editions-rnti.fr/render_pdf.php?p1&p=1002075		35	fr	fr	@irisa.fr, @irisa.fr, @irisa.fr	découverte de proportion analogique dans le base de donnée : un premier approche  ce article présenter un nouveau cadre pour le découverte de connaissance baser sur le notion de proportion analogique qui exprimer le égalité des rapport entre le attribut de deux paire d' élément . ce notion être développer dans le contexte des base de donnée pour découvrir un parallèle dans le donnée . Dans un premier temps , nous donner un définition formel des proportion analogique dans le cadre des base de donnée relationnel , puis nous étudier le problème de le extraction des proportion analogique . Nous montrer qu' il être possible de suivre un approche de clustering pour découvrir le classe d' équivalence de paire de n-uplets dans le même rapport de proportion analogique . ce travail constituer unCet article présenter un nouveau cadre pour le découverte de connaissance baser sur le notion de proportion analogique qui exprimer le égalité des rapport entre le attribut de deux paire d' élément . ce notion être développer dans le contexte des base de donnée pour découvrir un parallèle dans le donnée . Dans un premier temps , nous donner un définition formel des proportion analogique dans le cadre des base de donnée relationnel , puis nous étudier le problème de le extraction des proportion analogique . Nous montrer qu' il être possible de suivre un approche de clustering pour découvrir le classe d' équivalence de paire de n-uplets dans le même rapport de proportion analogique . ce travail constituer un premier étape vers le extension des langage d' interrogation de base de donnée avec un requête « analogique » . e premier étape vers le extension des langage d' interrogation de base de donnée avec un requête « analogique » . 	Découverte de proportions analogiques dans les bases de données : une première approche	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Détection automatique de reformulations - Correspondance de concepts appliquée à la détection du plagiat	Dans le cadre de la détection du plagiat, la phase de comparaison de deux documents est souvent réduite à une comparaison mot à mot, une recherche de « copier/coller ». Dans cet article, nous proposons une approche naïve de comparaison de deux documents dans le but de détecter automatiquement aussi bien les phrases copiées de l'un des textes dans l'autre que les paraphrases et reformulations, ceci en se focalisant sur l'existence des mots porteurs de sens, ainsi que sur leurs mots de substitution possibles. Nous comparons trois algorithmes utilisant cette approche afin de déterminer la plus efficace pour ensuite l'évaluer face à des méthodes existantes. L'objectif est de permettre la détection des similitudes entre deux textes en utilisant uniquement des mots clefs. L'approche proposée permet de détecter des reformulations non paraphrastiques impossibles à détecter avec des approches conventionnelles faisant appel à une phase d'alignement.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002089		36	fr	fr	@compilatio.net, @compilatio.net	détection automatique de reformulation - correspondance de concept appliquer à le détection du plagiat  Dans le cadre de le détection du plagiat , le phase de comparaison de deux document être souvent réduire à un comparaison mot à mot , un recherche de « copier  coller » . Dans ce article , nous proposer un approche naïf de comparaison de deux document dans le but de détecter automatiquement aussi bien le phrase copier de le un des texte dans le autre que le paraphrase et reformulation , ceci en clr focaliser sur le existence des mot porteur de sens , ainsi que sur son mot de substitution possible . Nous comparer trois algorithme utiliser ce approche afin de déterminer le plus efficace pour ensuite l' évaluer face à un méthode existant . le objectif être de permettre le détection des similitude entre deux texte en utiliser uniquement un mot clef . le approche proposer permettre de détecter un reformulation non paraphrastiques impossible à détecter avec un approche conventionnel faire appel à un phase d' alignement . 	Détection automatique de reformulations - Correspondance de concepts appliquée à la détection du plagiat	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Détection et regroupement automatique de style d'écriture dans un texte	La détection de plagiat extrinsèque devient vite inefficace lorsque l'on n'a pas accès aux documents potentiellement sources du plagiat ou lorsque l'on se confronte à un espace aussi vaste que leWeb, ce qui est souvent le cas dans les logiciels anti-plagiat actuels. Dès lors la détection intrinsèque devient nettement plus efficace. Dans cet article, nous traitons justement de la détection automatique d'auteurs qui permet de savoir si un passage d'un texte n'appartient pas au même auteur que le reste du texte et donc en théorie de repérer les passages plagiés d'un document. Nous expliquons notre contribution aux procédures déjà existantes et évaluons les limites de notre approche. L'objectif est de permettre la détection et le regroupement de passages d'un document par auteur.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002060		37	fr	fr	@compilatio.net, @compilatio.net	détection et regroupement automatique de style d' écriture dans un texte  le détection de plagiat extrinsèque devenir vite inefficace lorsque le on n' avoir pas accès aux document potentiellement source du plagiat ou lorsque le on clr confronter à un espace aussi vaste que leWeb , ce qui être souvent le cas dans le logiciel anti- plagiat actuel . Dès lors le détection intrinsèque devenir nettement plus efficace . Dans ce article , nous traiter justement de le détection automatique d' auteur qui permettre de savoir si un passage d' un texte n' appartenir pas au même auteur que le reste du texte et donc en théorie de repérer le passage plagier d' un document . Nous expliquer son contribution aux procédure déjà existant et évaluer le limite de son approche . le objectif être de permettre le détection et le regroupement de passage d' un document par auteur . 	Détection et regroupement automatique de style d'écriture dans un texte	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Deux approches pour catégoriser le risque	Le risque chimique ou alimentaire couvre les situations où les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l'environnement. Les experts qui assurent le contrôle et la gestion de ces substances se retrouvent face à de gros volumes de littérature scientifique, qui doit être analysée pour appuyer la prise de décisions. Nous proposons une aide automatique pour l'analyse de cette littérature. Nous abordons la tâche comme une problématique de catégorisation: il s'agit de catégoriser les phrases des textes dans les classes du risque lié aux substances. Nous utilisons deux approches: par apprentissage supervisé et la recherche d'information. Les résultats obtenus avec l'apprentissage supervisé (toute classe confondue, F-mesure autour de 0,8 pour le risque alimentaire, entre 0,61 et 0,64 pour le risque chimique) sont meilleurs que ceux obtenus avec par recherche d'information (toute classe confondue, F-mesure entre 0,18 et 0,226 pour le risque alimentaire, entre 0,20 et 0,32 pour le risque chimique). Le rappel est compétitif avec les deux approches.	Natalia Grabar, Niña Kerry	http://editions-rnti.fr/render_pdf.php?p1&p=1002067		38	fr	fr	@univ-lille3.fr, @boujut.com	Deux approche pour catégoriser le risque  le risque chimique ou alimentaire couvrir le situation où le produit chimique être dangereux pour le santé et consommation humain ou animal , et pour le environnement . le expert qui assurer le contrôle et le gestion de ce substance clr retrouver face à un gros volume de littérature scientifique , qui devoir être analyser pour appuyer le prise de décision . Nous proposer un aide automatique pour le analyse de ce littérature . Nous aborder le tâche comme un problématique de catégorisation : il clr agir de catégoriser le phrase des texte dans le classe du risque lier aux substance . Nous utiliser deux approche : par apprentissage superviser et le recherche d' information . le résultat obtenir avec le apprentissage superviser ( tout classe confondre , F-mesure autour de 0_,_8 pour le risque alimentaire , entre 0_,_61 et 0_,_64 pour le risque chimique ) être meilleur que celui obtenir avec par recherche d' information ( tout classe confondre , F-mesure entre 0_,_18 et 0_,_226 pour le risque alimentaire , entre 0_,_20 et 0_,_32 pour le risque chimique ) . le rappel être compétitif avec le deux approche . 	Deux approches pour catégoriser le risque	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Échantillonnage de flux de données sémantiques : Une approche orientée graphe	Nowadays, processing online massive data streams with special techniques like load shedding is an unavoidable alternative to optimize system resources use. In this paper, we propose a graph-oriented approach for load shedding semantic data streams. Our approach, unlike the RDF triple based one, preserves the semantic level of the data streams, which improves the responses quality of the RDF data stream processing systems.	Fethi Belghaouti, Amel Bouzeghoub, Zakia Kazi-aoul, Raja Chiky	http://editions-rnti.fr/render_pdf.php?p1&p=1002120		39	fr	en	@telecom-sudparis.eu, @isep.fr	Échantillonnage de flux de donnée sémantique : un approche orienter graphe 	Échantillonnage de flux de données sémantiques : Une approche orientée graphe	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Etude de La Pertinence lors de La Sélection de Collections dans les Systèmes Distribués	This paper presents a new function of collection selection. Our function is free of any extracollection parameter and is based on the documents relevance. The ranking of a collection is proportional to its number of relevant documents.	Kheira Mechach, Lougmiri Zekri, Mustapha Kamel Abdi	http://editions-rnti.fr/render_pdf.php?p1&p=1002122		40	fr	en	@gmail.com, @gmail.com, @univ-oran.dz	Etude de le pertinence lors de le sélection de collection dans le système Distribués 	Etude de La Pertinence lors de La Sélection de Collections dans les Systèmes Distribués	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Extraction complète efficace de chemins pondérés dans un a-DAG	Un nouveau domaine de motifs appelé chemins pondérés condensés a été introduit en 2013 lors de la conférence IJCAI. Le contexte de fouille est alors un graphe acyclique orienté (DAG) dont les sommets sont étiquetés par des attributs. Nous avons travaillé à une implémentation efficace de ce type de motifs et nous montrons que l'algorithme proposé était juste mais incomplet. Nous établissons ce résultat d'incomplétude et nous l'expliquons avant de trouver une solution pour réaliser une extraction complète. Nous avons ensuite développé des structures complémentaires pour calculer efficacement tous les chemins pondérés condensés. L'algorithme est amélioré en performance de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l'appliquons à des données réelles pour motiver qualitativement l'usage des chemins pondérés.	Nazha Selmaoui-Folcher, Frédéric Flouvat, Chengcheng Mu, Jérémy Sanhes, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002077		41	fr	fr	@univ-nc.nc, @insa-lyon.fr	extraction compléter efficace de chemin pondérer dans un a-DAG  un nouveau domaine de motif appeler chemin pondérer condenser avoir être introduire en 2013 lors de le conférence IJCAI . le contexte de fouille être alors un graphe acyclique orienter ( DAG ) dont le sommet être étiqueter par un attribut . Nous avoir travailler à un implémentation efficace de ce type de motif et nous montrer que le algorithme proposer être juste mais incomplet . Nous établir ce résultat d' incomplétude et nous le expliquons avant de trouver un solution pour réaliser un extraction complet . Nous avoir ensuite développer un structure complémentaire pour calculer efficacement tout le chemin pondérer condenser . le algorithme être améliorer en performance de plusieurs ordre de magnitude sur un jeu de donnée artificiel et nous l' appliquer à un donnée réel pour motiver qualitativement le usage des chemin pondérer . 	Extraction complète efficace de chemins pondérés dans un a-DAG	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Extraction de l'intérêt implicite des utilisateurs dans les attributs des items pour améliorer les systèmes de recommandations	Les systèmes de recommandation ont pour objectif de sélectionner et présenter d'abord les informations susceptibles d'intéresser les utilisateurs. Ce travail expose un système de recommandation qui s'appuie sur deux concepts: des relations sémantiques sur les données et une technique de filtrage collaboratif distribué basée sur la factorisation des matrices (MF). D'une part, les techniques sémantiques peuvent extraire des relations entre les données, et par conséquent, améliorer la précision des recommandations. D'autre part, MF donne des prévisions très précises avec un algorithme facilement parralélisable. Notre proposition utilise cette technique en ajoutant des relations sémantiques au processus. En effet, nous analysons en profondeur les intérêts cachés des utilisateurs dans les attributs des items à recommander. Nous utilisons dans nos expérimentations le jeu de données MovieLens enrichi par la base de données IMDb. Nous comparons notre travail à une technique MF classique. Les résultats montrent une précision dans les recommandations, tout en préservant un niveau élevé d'abstraction du domaine. En outre, nous améliorons le passage à l'échelle du système en utilisant des techniques parallélisables.	Manuel Pozo, Raja Chiky, Elisabeth Métais	http://editions-rnti.fr/render_pdf.php?p1&p=1002093		42	fr	fr	@isep.fr, @cnam.fr	extraction de le intérêt implicite des utilisateur dans le attribut des item pour améliorer le système de recommandations  le système de recommandation avoir pour objectif de sélectionner et présenter d' abord le information susceptible d' intéresser le utilisateur . ce travail exposer un système de recommandation qui clr appuyer sur deux concept : un relation sémantique sur le donnée et un technique de filtrage collaboratif distribuer baser sur le factorisation des matrice ( MF ) . D' un part , le technique sémantique pouvoir extraire un relation entre le donnée , et par conséquent , améliorer le précision des recommandation . D' autre part , MF donner un prévision très précis avec un algorithme facilement parralélisable . son proposition utiliser ce technique en ajouter un relation sémantique au processus . En effet , nous analyser en profondeur le intérêt cacher des utilisateur dans le attribut des item à recommander . Nous utiliser dans son expérimentation le jeu de donnée MovieLens enrichir par le base de donnée IMDb . Nous comparer son travail à un technique MF classique . le résultat montrer un précision dans le recommandation , tout en préserver un niveau élever d' abstraction du domaine . En outre , nous améliorer le passage à le échelle du système en utiliser un technique parallélisables . 	Extraction de l'intérêt implicite des utilisateurs dans les attributs des items pour améliorer les systèmes de recommandations	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Feedback - Study and Improvement of the Random Forest of the Mahout library in the context of marketing data of Orange	L'apprentissage automatique a fait son apparition dans l'écosystème Hadoop créant, de par la puissance promise, une opportunité sans précédent pour ce domaine. Dans cet écosystème, Apache Mahout est une réponse à la question du temps de calcul et/ou de la volumétrie: il consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous portés afin de s'exécuter sur Map/Reduce. Ce rapport se concentre sur le portage et l'utilisation de l'algorithme des Random Forest dans Mahout. Il montre à travers notre retour d'expérience les difficultés qui peuvent être rencontrées tant pratiques que théoriques et suggère une piste d'amélioration.	C. Thao, Nicolas Voisine, Vincent Lemaire, R. Trinquart	http://editions-rnti.fr/render_pdf.php?p1&p=1002104		43	en	fr		le apprentissage automatique avoir faire son apparition dans le écosystème Hadoop créer , de par le puissance promettre , un opportunité sans précédent pour ce domaine . Dans ce écosystème , Apache Mahout être un réponse à le question du temps de calcul et de le volumétrie : il consister en un entrepôt d' algorithme d' apprentissage automatique , tout porter afin de clr exécuter sur Map  Reduce . ce rapport clr concentrer sur le portage et le utilisation de le algorithme des Random Forest dans Mahout . Il montrer à travers son retour d' expérience le difficulté qui pouvoir être rencontrer tant pratique que théorique et suggérer un piste d' amélioration . 	Feedback - Study and Improvement of the Random Forest of the Mahout library in the context of marketing data of Orange	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	gapIT : Un outil visuel pour l'imputation de valeurs manquantes en hydrologie	Les données manquantes sont problématiques en hydrologie, car elles gênent le calcul de statistiques interannuelles et sur de longues périodes, ainsi que l'analyse et l'interprétation de la variabilité des données. Dans cet article, nous présentons gapIT, une plateforme d'analyse de données permettant d'inspecter visuellement les données manquantes et ensuite de choisir la méthode de correction adéquate. Nous avons utilisé l'outil pour estimer les données manquantes dans des séries temporelles correspondant aux débits mesurés par des stations hydrométriques du Luxembourg.	Olivier Parisot, Laura Giustarini, Olivier Faber, Renaud Hostache, Ivonne Trebs, Mohammad Ghoniem	http://editions-rnti.fr/render_pdf.php?p1&p=1002107		44	fr	fr	@lippmann.lu	gapIT : un outil visuel pour le imputation de valeur manquant en hydrologie  le donnée manquant être problématique en hydrologie , car elles gêner le calcul de statistique interannuelles et sur un long période , ainsi que le analyse et le interprétation de le variabilité des donnée . Dans ce article , nous présenter gapIT , un plateforme d' analyse de donnée permettre d' inspecter visuellement le donnée manquant et ensuite de choisir le méthode de correction adéquat . Nous avoir utiliser le outil pour estimer le donnée manquant dans un série temporel correspondre aux débit mesurer par un station hydrométrique du Luxembourg . 	gapIT : Un outil visuel pour l'imputation de valeurs manquantes en hydrologie	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Gestion de l'incertitude dans le cadre d'une extraction des connaissances à partir de texte	The knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.	Fadhela Kerdjoudj, Olivier Curé	http://editions-rnti.fr/render_pdf.php?p1&p=1002116		45	fr	en	@univ-mlv.fr, @univ-mlv.fr	gestion de le incertitude dans le cadre d' un extraction des connaissance à partir de texte 	Gestion de l'incertitude dans le cadre d'une extraction des connaissances à partir de texte	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Heuristiques pour l'adaptation des mappings entre ontologies dynamiques	Les correspondances sémantiques entre ontologies (mappings) jouent un rôle essentiel dans les systèmes d'information. Cependant, en vertu de l'évolution des connaissances, les éléments ontologiques sont sujets à modification invalidant potentiellement les alignements préalablement établis. Des techniques de maintenance sont donc nécessaires pour maintenir la validité des mappings. Dans cet article, nous présentons un ensemble d'heuristiques guidant leur adaptation. Notre approche s'appuie sur l'explication des mappings existants, les informations provenant de l'évolution des ontologies ainsi que les adaptations possibles applicables aux mappings. Nous proposons une validation expérimentale à partir d'ontologies du domaine médical et des mappings qui leur sont associés.	Julio Cesar Dos Reis, Cédric Pruski, Chantal Reynaud-Delaître	http://editions-rnti.fr/render_pdf.php?p1&p=1002084		46	fr	fr	@tudor.lu, @lri.fr	heuristique pour le adaptation des mappings entre ontologie dynamiques  le correspondance sémantique entre ontologie ( mappings ) jouer un rôle essentiel dans le système d' information . cependant , en vertu de le évolution des connaissance , le élément ontologique être sujet à modification invalider potentiellement le alignement préalablement établir . un technique de maintenance être donc nécessaire pour maintenir le validité des mappings . Dans ce article , nous présenter un ensemble d' heuristique guider son adaptation . son approche clr appuyer sur le explication des mappings existant , le information provenir de le évolution des ontologie ainsi que le adaptation possible applicable aux mappings . Nous proposer un validation expérimental à partir d' ontologie du domaine médical et des mappings qui leur être associer . 	Heuristiques pour l'adaptation des mappings entre ontologies dynamiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2015	Identification des utilisateurs atypiques dans les systèmes de recommandation sociale	Malgré des performances très satisfaisantes, l'approche sociale de la recommandation ne fournit pas de bonnes recommandations à un sous-ensemble des utilisateurs. Nous supposons ici que certains de ces utilisateurs ont des préférences différentes de celles des autres, nous les qualifions d'atypiques. Nous nous intéressons à leur identification, en amont de la tâche de recommandation, et proposons plusieurs mesures représentant l'atypicité des préférences d'un utilisateur. L'évaluation de ces mesures sur un corpus de l'état de l'art montre qu'elles permettent d'identifier de façon fiable des utilisateurs recevant de mauvaises recommandations.	Benjamin Gras, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1002097		47	fr	fr	@loria.fr	identification des utilisateur atypique dans le système de recommandation sociale  Malgré un performance très satisfaisant , le approche social de le recommandation ne fournir pas un bon recommandation à un sous-ensemble des utilisateur . Nous supposer ici que certains de ce utilisateur avoir un préférence différent de celui des autre , nous les qualifier d' atypique . Nous nous intéresser à son identification , en amont de le tâche de recommandation , et proposer plusieurs mesure représenter le atypicité des préférence d' un utilisateur . le évaluation de ce mesure sur un corpus de le état de le art montrer qu' elles permettre d' identifier de façon fiable des utilisateur recevoir un mauvais recommandation . 	Identification des utilisateurs atypiques dans les systèmes de recommandation sociale	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Identification d'auteurs par apprentissage automatique	Etant donné un ensemble de documents rédigés par un même auteur, le problème d'authentification d'auteurs consiste à décider si un nouveau texte a été rédigé ou non par cet auteur. Pour résoudre ce problème, nous avons proposé et implémenté différentes approches : comptage de similarité, techniques de vote et apprentissage supervisé qui exploitent différents modèles de représentation des documents. Les expérimentations réalisées à partir des collections de la compétition PAN-CLEF 2013 et 2014 ont confirmé l'intérêt de nos approches et leur performance en termes de temps de traitement.	Jordan Frery, Christine Largeron, Mihaela Juganaru-Mathieu	http://editions-rnti.fr/render_pdf.php?p1&p=1002062		48	fr	fr	@univ-st-etienne.fr, @emse.fr	identification d' auteur par apprentissage automatique  Etant donner un ensemble de document rédiger par un même auteur , le problème d' authentification d' auteur consister à décider si un nouveau texte avoir être rédiger ou non par ce auteur . Pour résoudre ce problème , nous avoir proposer et implémenter différent approche : comptage de similarité , technique de vote et apprentissage superviser qui exploiter différent modèle de représentation des document . le expérimentation réaliser à partir un collection de le compétition PAN-CLEF 2013 et 2014 avoir confirmer le intérêt de son approche et son performance en terme de temps de traitement . 	Identification d'auteurs par apprentissage automatique	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	LeveragingWeb 2.0 for Informed Real-Estate Services	"The perception about real estate properties, both for individuals and agents, is not formed exclusively by their intrinsic characteristics, such as surface and age, but also from property externalities, such as pollution, traffic congestion, criminality rates, proximity to playgrounds, schools and stimulating social interactions that are equally important. In this paper, we present the Real-Estate 2.0 System that in contrary to existing Real-Estate e-services and applications, takes also into account important externalities. By leveraging Web 2.0 (content from Social Networks, POI listings) applications and Open Data enables the thorough analysis of the current physical and social context of the property, the context-based objective valuation of RE properties, along with an advanced property search and selection experience that unveils otherwise ""hidden"" property features and significantly reduces user effort and time spent in their RE quest. The system encompasses the above to provide services which assist individuals and agents in making more informed and sound RE decisions."	Papantoniou Katerina, Athanasiadis Marios - Lazaros, Fundulaki Irini, Georgis Christos, Stavrakas Yannis, Troullinos Michalis, Tsitsanis Anastasios	http://editions-rnti.fr/render_pdf.php?p1&p=1002105		49	en	en	@ics.forth.gr, @imis.athena-innovation.gr, @trek.gr	leveragingweb 2 0 informed real estate service the perception real estate property individual agent form exclusively intrinsic characteristic surface age also property externality pollution traffic congestion criminality rate proximity playground school stimulate social interaction equally important paper present real estate 2 0 system contrary existing real estate e service application take also account important externality leverage web 2 0 content social network poi listing application open datum enable thorough analysis current physical social context property context base objective valuation re property along advanced property search selection experience unveil otherwise hidden property feature significantly reduce user effort time spend re quest system encompass provide service assist individual agent make informed sound re decision	LeveragingWeb 2.0 for Informed Real-Estate Services	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Linked Data Annotation and Fusion driven by Data Quality Evaluation	Dans cet article nous présentons une approche de fusion de données fondée sur l'utilisation d'informations sur la qualité des données pour résoudre les éventuels conflits entre valeurs.	Ioanna Giannopoulou, Fatiha Saïs, Rallou Thomopoulos	http://editions-rnti.fr/render_pdf.php?p1&p=1002086		50	en	fr		Dans ce article nous présenter un approche de fusion de donnée fonder sur le utilisation d' information sur le qualité des donnée pour résoudre le éventuel conflit entre valeur . 	Linked Data Annotation and Fusion driven by Data Quality Evaluation	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	L'apport d'une approche symbolique pour le repérage des entités nommées en langue amazighe	Le repérage des Entités Nommées (REN) en langue amazighe est un prétraitement éventuellement essentiel pour de nombreuses applications du traitement automatique des langues (TAL), en particulier pour la traduction automatique. Dans cet article, nous présentons une chaîne de repérage des entités nommées en amazighe fondée sur une étude synthétique des spécificités de la langue et des entités nommées en amazighe. L'article met l'accent sur les choix méthodologiques à résoudre les ambiguïtés dues à la langue, en exploitant les technologies existantes pour d'autres langues.	Meryem Talha, Siham Boulaknadel, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1002061		51	fr	fr	@gmail.com, @fsr.ac.ma, @ircam.ma	le apport d' un approche symbolique pour le repérage des entité nommer en langue amazighe  le repérage des entité Nommées ( REN ) en langue amazighe être un prétraitement éventuellement essentiel pour un nombreux application du traitement automatique des langue ( TAL ) , en particulier pour le traduction automatique . Dans ce article , nous présenter un chaîne de repérage des entité nommer en amazighe fonder sur un étude synthétique des spécificité de le langue et des entité nommer en amazighe . le article mettre le accent sur le choix méthodologique à résoudre le ambiguïté devoir à le langue , en exploiter le technologie existant pour un autre langue . 	L'apport d'une approche symbolique pour le repérage des entités nommées en langue amazighe	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Managing Big Multidimensional Data	"Multidimensional database concepts such as cubes, dimensions with hierarchies, and measures have been a cornerstone of analytical business intelligence tools for decades. However, the standard data models and system implementations (OLAP) for multidimensional databases cannot handle ""Big Multidimensional Data"", very large amounts of complex and highly dynamic multidimensional data that occur in a number of emerging domains such as energy, transport, logistics, as well as science. This talk will discuss similarities and differences between traditional Business Intelligence (BI) and Big Data, present examples of Big Multidimensional data with the characteristics of large volume, high velocity (fast data), and/or high variety (complex data) and discuss how to manage Big Multidimensional Data, including modeling, algorithmic, implementation, as well as practical, issues."	Torben Bach Pedersen	http://editions-rnti.fr/render_pdf.php?p1&p=1002056		52	en	en	@cs.aau.dk	manage big multidimensional data multidimensional database concept cube dimension hierarchy measure cornerstone analytical business intelligence tool decade however standard data model system implementation olap multidimensional databasis cannot handle big multidimensional datum large amount complex highly dynamic multidimensional data occur number emerge domain energy transport logistic well science talk discuss similarity difference traditional business intelligence bi big datum present example big multidimensional data characteristic large volume high velocity fast data and or high variety complex data discuss manage big multidimensional data include modeling algorithmic implementation well practical issue	Managing Big Multidimensional Data	1
Revue des Nouvelles Technologies de l'Information	EGC	2015	Mesure d'influence via les indicateurs de centralité dans les réseaux sociaux	For social network analysis, existing centrality measures emphasize the importance of an actor considering only the structural position in the network regardless of a priori information on this actors such as popularity, accessibility or behavior. In this study new variants of centrality measures are proposed operating both the network structure and the specific attributes of an actor. Experiments have validated the contribution of valuations especially for the detection of broadcasters in social networks.	Oualid Benyahia, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1002112		53	fr	en	@univ-st-etienne.fr	mesure d' influence via le indicateur de centralité dans le réseau sociaux 	Mesure d'influence via les indicateurs de centralité dans les réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Méthode alternative à la détection de « copier/coller » : intersection de textes et construction de séquences maximales communes 	La détection du plagiat passe le plus souvent par la phase de recherche de similitudes la plus naïve, la détection de « copier/coller ». Dans cet article, nous proposons une méthode alternative à l'approche standard de comparaison mot à mot. Le principe étant d'effectuer une intersection des deux textes à comparer, récupérant ainsi un tableau des mots qu'ils ont en commun et de ne conserver que les séquences maximales des mots se suivant dans l'un des textes et existant également dans l'autre. Nous montrons que cette méthode est plus rapide et moins coûteuse en ressources que les méthodes de parcours de textes habituellement utilisées. L'objectif étant de détecter les passages identiques entre deux textes plus rapidement que les méthodes de comparaison mot à mot, tout en étant plus efficace que les méthodes n-grammes.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002065		54	fr	fr	@compilatio.net, @compilatio.net	méthode alternatif à le détection de « copier  coller » : intersection de texte et construction de séquence maximal communes  le détection du plagiat passer le plus souvent par le phase de recherche de similitude le plus naïf , le détection de « copier  coller » . Dans ce article , nous proposer un méthode alternatif à le approche standard de comparaison mot à mot . le principe être d' effectuer un intersection des deux texte à comparer , récupérer ainsi un tableau des mot qu' ils avoir en commun et de ne conserver que le séquence maximal des mot clr suivre dans le un des texte et existant également dans le autre . Nous montrer que ce méthode être plus rapide et moins coûteux en ressource que le méthode de parcours de texte habituellement utiliser . le objectif être de détecter le passage identique entre deux texte plus rapidement que le méthode de comparaison mot à mot , tout en être plus efficace que le méthode n-gramme . 	Méthode alternative à la détection de « copier/coller » : intersection de textes et construction de séquences maximales communes 	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Mining Classes by Multi-label Classification	We propose a new approach to mine potential classes in news documents by examining close relationship between new classes and probability vectors of multiple labeling of the documents. Using EM algorithm to obtain the distribution over linear mixture models, we make clustering and mine classes.	Yuichiro Kase, Takao Miura	http://editions-rnti.fr/render_pdf.php?p1&p=1002066		55	en	en	@hosei.ac.jp, @hosei.ac.jp	mining class multus label classification propose new approach mine potential class news document examine close relationship new class probability vector multiple labele document used em algorithm obtain distribution linear mixture model make cluster mine class	Mining Classes by Multi-label Classification	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	"Modèle de Biclustering dans un paradigme ""Mapreduce"""	Biclustering is a main task in a variety of areas of machine learning providing simultaneous observations and features clustering. Biclustering approches are more complex compared to the traditional clustering particularly those requiring large dataset and Mapreduce platforms. We propose a new approach of biclustering based on popular self-organizing maps for cluster analysis of large dataset. We have designed scalable implementations of the new biclustering algorithm using MapReduce with the Spark platform. We report the experiments and demonstrated the performance public dataset using different cores. Using practical examples, we demonstrate that our algorithm works well in practice. The experimental results show scalable performance with near linear speedups across different data and 120 cores.	Tugdual Sarazin, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1002111		56	fr	en	@univ-paris13.fr	" modèle de Biclustering dans un paradigme " " Mapreduce " " " 	"Modèle de Biclustering dans un paradigme ""Mapreduce"""	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Nouvelle approche de contextualisation de tweets basée sur les règles d'association inter-termes	Tweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet's vocabulary by a set of thematically related words using mining association rules between terms.	Meriem Amina Zingla, Mohamed Ettaleb, Chiraz Latiri, Yahia Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1002121		57	fr	en		nouveau approche de contextualisation de tweets baser sur le règle d' association inter-termes 	Nouvelle approche de contextualisation de tweets basée sur les règles d'association inter-termes	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Pour une meilleure exploitation de la classification croisée dans les systèmes de filtrage collaboratif	Pour la prédiction automatique des items préférés par des utilisateurs sur le Web, différents systèmes de filtrage collaboratif ont été proposés. La plupart d'entre eux sont basés sur la factorisation matricielle et les approches de type k plus proches voisins. Malheureusement ces deux approches requièrent un temps de calcul important. Une partie de ces problèmes a pu être surmontée par la classification croisée ou co-clustering qui s'avère pertinente du fait qu'elle permet par nature une gestion simultanée des ensembles correspondant aux utilisateurs et aux items. Cependant, des travaux doivent encore être menés pour une meilleure prise en compte des données manquantes. Dans ce travail, nous proposons donc une gestion efficace des données non observées permettant une meilleure exploitation du potentiel de la classification croisée dans le domaine des systèmes de recommandation. Nous montrons de plus qu'elle permet d'obtenir des représentations à base de graphes bipartis facilitant l'interprétation interactive des affinités entre des groupes d'utilisateurs et des groupe d'items.	Aghiles Salah, Nicoleta Rogovschi, François Role, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002095		58	fr	fr	@parisdescartes.fr	Pour un meilleur exploitation de le classification croiser dans le système de filtrage collaboratif  Pour le prédiction automatique des item préférer par un utilisateur sur le Web , différent système de filtrage collaboratif avoir être proposer . le plupart d' entre lui être baser sur le factorisation matriciel et le approche de type k plus proche voisin . malheureusement ce deux approche requérir un temps de calcul important . un partie de ce problème avoir pouvoir être surmonter par le classification croiser ou co- clustering qui clr avérer pertinent du fait qu' elle permettre par nature un gestion simultané des ensemble correspondre aux utilisateur et aux item . cependant , un travail devoir encore être mener pour un meilleur prise en compte des donnée manquant . Dans ce travail , nous proposer donc un gestion efficace des donnée non observer permettre un meilleur exploitation du potentiel de le classification croiser dans le domaine des système de recommandation . Nous montrer de plus qu' elle permettre d' obtenir un représentation à base de graphe biparti faciliter le interprétation interactif des affinité entre un groupe d' utilisateur et des groupe d' item . 	Pour une meilleure exploitation de la classification croisée dans les systèmes de filtrage collaboratif	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Proposition d'outil de clustering visuel et interactif	Cet article présente un nouvel outil visuel de clustering interactif. Il utilise une technique de réduction de dimensionnalité pour permettre une représentation 2D des données et des classes associées, initialement établies de manière non-supervisée. L'originalité de l'outil consiste à autoriser des modifications itératives à la fois du clustering et de la projection 2D. Grâce à des contrôles adaptés, l'utilisateur peut ainsi injecter ses préférences, et observer le changement induit en temps réel. La méthode de projection utilisée suit une métaphore physique, qui facilite le suivi des changements par l'utilisateur. Nous montrons un exemple illustrant l'intérêt pratique de l'outil.	Pierrick Bruneau, Philippe Pinheiro, Bertjan Broeksema, Benoît Otjacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002073		59	fr	fr	@lippmann.lu	proposition d' outil de clustering visuel et interactif  ce article présent un nouveau outil visuel de clustering interactif . Il utiliser un technique de réduction de dimensionnalité pour permettre un représentation 2D des donnée et des classe associer , initialement établir de manière non- superviser . le originalité de le outil consister à autoriser un modification itératif à le foi du clustering et de le projection 2D. Grâce à un contrôle adapter , le utilisateur pouvoir ainsi injecter son préférence , et observer le changement induire en temps réel . le méthode de projection utiliser suivre un métaphore physique , qui faciliter le suivi des changement par le utilisateur . Nous montrer un exemple illustrer le intérêt pratique de le outil . 	Proposition d'outil de clustering visuel et interactif	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Qualité et complexité en évaluation des mesures d'intérêt	Remplacer des hypothèses sur le modèle de données par des informations mesurées sur les données réelles est l'une des forces de la fouille de données. Cet article étudie cet ajustement entre les données et les méthodes de découverte de motifs pour en évaluer la qualité et la complexité. Nous formalisons ce lien entre données et mesures d'intérêt en identifiant les motifs liés qui sont ceux nécessaires pour l'évaluation d'une mesure ou d'une contrainte. Nous formulons alors trois axiomes que devraient satisfaire ces motifs liés pour qu'une méthode d'extraction se comporte bien. En outre, nous définissons la complexité en évaluation qui quantifie finement l'interrelation entre les motifs au sein d'une méthode d'extraction. A la lumière de ces axiomes et de cette complexité en évaluation, nous dressons une typologie de multiples méthodes de découverte de motifs impliquant la fréquence.	Bruno Crémilleux, Arnaud Giacometti, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002094		60	fr	fr	@unicaen.fr, @univ-tours.fr	qualité et complexité en évaluation des mesure d' intérêt  remplacer un hypothèse sur le modèle de donnée par un information mesurer sur le donnée réel être le un des force de le fouille de donnée . ce article étudier ce ajustement entre le donnée et le méthode de découverte de motif pour en évaluer le qualité et le complexité . Nous formaliser ce lien entre donnée et mesure d' intérêt en identifier le motif lier qui être celui nécessaire pour le évaluation d' un mesure ou d' un contrainte . Nous formuler alors trois axiome que devoir satisfaire ce motif lier pour qu' un méthode d' extraction clr comporter bien . En outre , nous définir le complexité en évaluation qui quantifier finement le interrelation entre le motif au sein d' un méthode d' extraction . A le lumière de ce axiome et de ce complexité en évaluation , nous dresser un typologie de multiple méthode de découverte de motif impliquer le fréquence . 	Qualité et complexité en évaluation des mesures d'intérêt	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	RankMerging: Apprentissage supervisé de classements pour la prédiction de liens dans les grands réseaux sociaux	Trouver les liens manquants dans un grand réseau social est une tâche difficile, car ces réseaux sont peu denses, et les liens peuvent correspondre à des environnements structurels variés. Dans cet article, nous décrivons RankMerging, une méthode d'apprentissage supervisé simple pour combiner l'information obtenue par différentes méthodes de classement. Afin d'illustrer son intérêt, nous l'appliquons à un réseau d'utilisateurs de téléphones portables, pour montrer comment un opérateur peut détecter des liens entre les clients de ses concurrents. Nous montrons que RankMerging surpasse les méthodes à disposition pour prédire un nombre variable de liens dans un grand graphe épars.	Lionel Tabourier, Anne-Sophie Libert, Renaud Lambiotte	http://editions-rnti.fr/render_pdf.php?p1&p=1002102		61	fr	fr		RankMerging : apprentissage superviser de classement pour le prédiction de lien dans le grand réseau sociaux  trouver le lien manquant dans un grand réseau social être un tâche difficile , car ce réseau être peu dense , et le lien pouvoir correspondre à un environnement structurel varier . Dans ce article , nous décrire RankMerging , un méthode d' apprentissage superviser simple pour combiner le information obtenir par différentes méthode de classement . Afin d' illustrer son intérêt , nous l' appliquer à un réseau d' utilisateur de téléphone portable , pour montrer comment un opérateur pouvoir détecter un lien entre le client de son concurrent . Nous montrer que RankMerging surpasser le méthode à disposition pour prédire un nombre variable de lien dans un grand graphe épars . 	RankMerging: Apprentissage supervisé de classements pour la prédiction de liens dans les grands réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Réduction de la complexité spatiale et temporelle du Compact Prediction Tree pour la prédiction de séquences	La prédiction de séquences de symboles est une tâche ayant de multiples applications. Plusieurs modèles de prédiction ont été proposés tels que DG, All-k-order markov et PPM. Récemment, il a été montré qu'un nouveau modèle nommé Compact Prediction Tree (CPT) utilisant une structure en arbre et un algorithme de prédiction plus complexe, offre des prédictions plus exactes que plusieurs approches de la littérature. Néanmoins, une limite importante de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ce problème en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact et est 4.5 fois plus rapide que CPT, tout en conservant une exactitude très élevée par rapport à All-K-order Markov, DG, Lz78, PPM et TDAG.	Ted Gueniche, Philippe Fournier-Viger	http://editions-rnti.fr/render_pdf.php?p1&p=1002064		62	fr	fr	@gmail.com, @umoncton.ca	réduction de le complexité spatial et temporel du Compact Prediction Tree pour le prédiction de séquences  le prédiction de séquence de symbole être un tâche avoir de multiple application . plusieurs modèle de prédiction avoir être proposer tel que DG , All-k-order markov et PPM . récemment , il avoir être montrer qu' un nouveau modèle nommer Compact Prediction Tree ( CPT ) utiliser un structure en arbre et un algorithme de prédiction plus complexe , offre des prédiction plus exact que plusieurs approche de le littérature . néanmoins , un limite important de CPT être son complexité temporel et spatial élever . Dans ce article , nous pallier ce problème en proposer trois stratégie pour réduire le taille et le temps de prédiction de CPT . le résultat expérimental sur 7 jeu de donnée réel montrer que le modèle résulter nommer CPT plus être jusqu' à 98 foi plus compact et être 4.5 foi plus rapide que CPT , tout en conserver un exactitude très élever par rapport à All-K-order Markov , DG , Lz78 , PPM et TDAG . 	Réduction de la complexité spatiale et temporelle du Compact Prediction Tree pour la prédiction de séquences	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Regroupement d'attributs par règles d'association dans les systèmes d'inférence floue	Dans les systèmes d'apprentissage supervisé par construction de règles de classification floues, un nombre élevé d'attributs descriptifs conduit à une explosion du nombre de règles générées et peut affecter la précision des algorithmes d'apprentissage. Afin de remédier à ce problème, une solution est de traiter séparément des sous-groupes d'attributs. Cela permet de décomposer le problème d'apprentissage en des sous-problèmes de complexité inférieure, et d'obtenir des règles plus intelligibles car de taille réduite. Nous proposons une nouvelle méthode de regroupement des attributs qui se base sur le concept des règles d'association. Ces règles découvrent des relations intéressantes entre des intervalles de valeurs des attributs. Ces liaisons locales sont ensuite agrégées au niveau des attributs mêmes en fonction du nombre de liaisons trouvées et de leur importance. Notre approche, testée sur différentes bases d'apprentissage et comparée à l'approche classique, permet d'améliorer la précision tout en garantissant une réduction du nombre de règles.	Ilef Ben Slima, Amel Borgi	http://editions-rnti.fr/render_pdf.php?p1&p=1002092		63	fr	fr	@fst.rnu.tn, @insat.rnu.tn	regroupement d' attribut par règle d' association dans le système d' inférence floue  Dans le système d' apprentissage superviser par construction de règle de classification flou , un nombre élever d' attribut descriptif conduire à un explosion du nombre de règle générer et pouvoir affecter le précision des algorithme d' apprentissage . Afin de remédier à ce problème , un solution être de traiter séparément un sous-groupes d' attribut . cela permettre de décomposer le problème d' apprentissage en un sous-problèmes de complexité inférieur , et d' obtenir un règle plus intelligible car de taille réduire . Nous proposer un nouveau méthode de regroupement des attribut qui clr baser sur le concept des règle d' association . ce règle découvrir un relation intéressant entre un intervalle de valeur des attribut . ce liaison local être ensuite agréger au niveau des attribut même en fonction du nombre de liaison trouver et de son importance . son approche , tester sur différent base d' apprentissage et comparer à le approche classique , permettre d' améliorer le précision tout en garantir un réduction du nombre de règle . 	Regroupement d'attributs par règles d'association dans les systèmes d'inférence floue	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Régularisation de noyaux temporellement élastiques et analyse en composantes principales non-linéaire pour la fouille de séries temporelles 	Dans le domaine de la fouille de séries temporelles, plusieurs travaux récents exploitent des noyaux construits à partir de distances élastiques de type Dynamic Time Warping (DTW) au sein d'approches à base de noyaux. Pourtant les matrices, apparentées aux matrices de Gram, construites à partir de ces noyaux n'ont pas toujours les propriétés requises ce qui peut les rendre in fine impropres à une telle exploitation. Des approches émergeantes de régularisation de noyaux élastiques peuvent être mises à profit pour répondre à cette insuffisance. Nous présentons l'une de ces méthodes, KDTW, pour le noyau DTW, puis, autour d'une analyse en composantes principales non-linéaire (K-PCA), nous évaluons la capacité de quelques noyaux concurrents (élastiques v.s non élastiques, définis v.s. non définis) à séparer les catégories des données analysées tout en proposant une réduction dimensionnelle importante. Cette étude montre expérimentalement l'intérêt d'une régularisation de type KDTW.	Pierre-François Marteau	http://editions-rnti.fr/render_pdf.php?p1&p=1002063		64	fr	fr		régularisation de noyau temporellement élastique et analyser en composante principal non- linéaire pour le fouille de série temporelles  Dans le domaine de le fouille de série temporel , plusieurs travail récent exploiter un noyau construire à partir de distance élastique de type Dynamic Time Warping ( DTW ) au sein d' approche à base de noyau . pourtant le matrice , apparenter aux matrice de Gram , construire à partir de ce noyau n' avoir pas toujours le propriété requérir ce qui pouvoir les rendre in fin impropre à un tel exploitation . un approche émergeantes de régularisation de noyau élastique pouvoir être mettre à profit pour répondre à ce insuffisance . Nous présenter le un de ce méthode , KDTW , pour le noyau DTW , puis , autour d' un analyse en composante principal non- linéaire ( K-PCA ) , nous évaluer le capacité de quelque noyau concurrent ( élastique vers s non élastique , définir vers s . non définir ) à séparer le catégorie des donnée analyser tout en proposer un réduction dimensionnel important . ce étude montrer expérimentalement le intérêt d' un régularisation de type KDTW . 	Régularisation de noyaux temporellement élastiques et analyse en composantes principales non-linéaire pour la fouille de séries temporelles 	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Requêtes Skyline en présence des données évidentielles	Dans cet article, nous nous intéressons à la recherche des points les plus intéressants au sens de l'ordre de Pareto, dans les bases de données évidentielles. Nous présentons le modèle skyline évidentiel qui est adapté à la nature des données incertaines. Ensuite, nous présentons une évaluation expérimentale de notre approche.	Sayda Elmi, Karim Benouaret, Allel HadjAli, Mohamed Anis Bach Tobji, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1002081		65	fr	fr	@gmail.com, @isg.rnu.tn, @ihec.rnu.tn, @liris.cnrs.fr, @ensma.fr	requête Skyline en présence des donnée évidentielles  Dans ce article , nous clr intéresser à le recherche des point le plus intéressant au sens de le ordre de Pareto , dans le base de donnée évidentielles . Nous présenter le modèle skyline évidentiel qui être adapter à le nature des donnée incertain . ensuite , nous présenter un évaluation expérimental de son approche . 	Requêtes Skyline en présence des données évidentielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	To initiate a corporate memory with a knowledge compendium: ten years of learning from experience with the Ardans method	Ardans method ArdansSas (2006b) and technology ArdansSas (2006a) of knowledge capitalization and structuration are used with different industries (automotive, aerospace, energy, defence, steel, health, etc.) for more than a decade in France and Europe.The proposed solutions in knowledge management and especially in expertise capitalisation have set a lot of feedback over time. With a view toward ongoing improvement, what are the impacts of these feedbacks on the method nowadays? Put into practice into the industry, the return of investment of a capitalization campaign is inferred from the quality of the knowledge base delivered at the end of the campaign. Therefore, the method and the technology are intrinsically connected. How IT tools can assist with the quality diagnosis of the knowledge base?A comparative study was conducted on the basis of the method Mariot et al. (2007) exposed at EGC'2007. This article sets out the results of the changes and improvements of the method, in conjunction with the latest technical and scientific development on the one hand, and the change of the industry needs on the other hand.	Vincent Besson, Alain Berger	http://editions-rnti.fr/render_pdf.php?p1&p=1002103		66	en	en	@ardans.fr	initiate corporate memory knowledge compendium ten year learn experience ardan method ardans method ardanssas 2006b technology ardanssas 2006a knowledge capitalization structuration used different industry automotive aerospace energy defence steel health etc decade france europe the proposed solution knowledge management especially expertise capitalisation set lot feedback time view toward ongoing improvement impact feedback method nowadays put practice industry return investment capitalization campaign infer quality knowledge base deliver end campaign therefore method technology intrinsically connected tool assist quality diagnosis knowledge base a comparative study conduct basis method mariot et al 2007 expose egc 2007 article set result change improvement method conjunction latest technical scientific development one hand change industry need hand	To initiate a corporate memory with a knowledge compendium: ten years of learning from experience with the Ardans method	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Towards Linked Data Extraction From Tweets	Millions of Twitter users post messages every day to communicate with other users in real time information about events that occur in their environment. Most of the studies on the content of tweets have focused on the detection of emerging topics. However, to the best of our knowledge, no approach has been proposed to create a knowledge base and enrich it automatically with information coming from tweets. The solution that we propose is composed of four main phases: topic identification, tweets classification, automatic summarization and creation of an RDF triplestore. The proposed approach is implemented in a system covering the entire sequence of processing steps from the collection of tweets written in English language (based on both trusted and crowd sources) to the creation of an RDF dataset anchored in DBpedia's namespace.	Manel Achichi, Zohra Bellahsene, Dino Ienco, Konstantin Todorov	http://editions-rnti.fr/render_pdf.php?p1&p=1002100		67	en	en	@lirmm.fr, @teledetection.fr	towards link data extraction tweet million twitter user post message every day communicate user real time information event occur environment study content tweet focuse detection emerge topic however best knowledge approach propose create knowledge base enrich automatically information come tweet solution propose compose four main phase topic identification tweet classification automatic summarization creation rdf triplestore proposed approach implemented system cover entire sequence process step collection tweet write english language base trusted crowd source creation rdf dataset anchor dbpedia s namespace	Towards Linked Data Extraction From Tweets	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Ultrametricity of Dissimilarity Spaces and Its Significance for Data Mining	Nous introduisons une mesure d'ultramétricité pour les dissimilaritées et examinons les transformations des dissimilaritées et leurs impact sur cette mesure. Ensuite, nous étudions l'influence de l'ultramétricité sur la comportement de deux classes d'algorithmes d'exploration de données (le kNN algorithme de classification et l'algorithme de regroupement PAM) appliqués sur les espaces de dissimilarité. On montre qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour les clusters, une augmentation d'ultramétricité genere regroupements avec une meilleure séparation. Une diminution de la ultramétricité produit groupes plus compacts.	Dan Simovici, Rosanne Vetro, Kaixun Hua	http://editions-rnti.fr/render_pdf.php?p1&p=1002068		68	en	fr	@cs.umb.edu, @cs.umb.edu, @cs.umb.edu	Nous introduire un mesure d' ultramétricité pour le dissimilaritées et examiner le transformation des dissimilaritées et son impact sur ce mesure . ensuite , nous étudier le influence de le ultramétricité sur le comportement de deux classe d' algorithme d' exploration de donnée ( le kNN algorithme de classification et le algorithme de regroupement PAM ) appliquer sur le espace de dissimilarité . On montrer qu' il exister un variation inverse entre ultramétricité et le performance des classificateur . Pour le clusters , un augmentation d' ultramétricité genere regroupement avec un meilleur séparation . un diminution de le ultramétricité produire groupe plus compact . 	Ultrametricity of Dissimilarity Spaces and Its Significance for Data Mining	1
Revue des Nouvelles Technologies de l'Information	EGC	2015	Un algorithme EM pour une version parcimonieuse de l'analyse en composantes principales probabiliste	Nous considérons une version parcimonieuse de l'analyse en composantes principales probabiliste. La pénalité `1 imposée sur les composantes principales rend leur interprétation plus aisée en ne faisant dépendre ces dernières que d'un nombre restreint de variables initiales. Un algorithme EM, simple de mise en oeuvre, est proposé pour l'estimation des paramètres du modèle. La méthode de l'heuristique de pente est finalement utilisée pour choisir le coefficient de pénalisation.	Charles Bouveyron, Julien Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002074		69	fr	fr	@parisdescartes.fr, @univ-lyon2.fr	un algorithme EM pour un version parcimonieux de le analyse en composante principal probabiliste  Nous considérer un version parcimonieux de le analyse en composante principal probabiliste . le pénalité ` 1 imposer sur le composante principal rendre son interprétation plus aisé en ne faire dépendre ce dernier que d' un nombre restreindre de variable initial . un algorithme EM , simple de mise en oeuvre , être proposer pour le estimation des paramètre du modèle . le méthode de le heuristique de pente être finalement utiliser pour choisir le coefficient de pénalisation . 	Un algorithme EM pour une version parcimonieuse de l'analyse en composantes principales probabiliste	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Un algorithme ICM basé sur la compacité pour la segmentation des images satellites à très haute résolution	"Dans cet article nous proposons une modification pour l'algorithme ""Iterated Conditional Modes"" (ICM) appliqué à la segmentation d'images à très haute résolution. Pour ce faire, nous introduisons un nouveau critère de convergence basé sur la compacité des clusters et qui repose sur une fonction d'énergie adaptée aux modèles de voisinages irréguliers de ce type d'images. Grâce à cette méthode, nos premières expériences ont montré que nous obtenons des résultats plus fiables en terme de convergence et de meilleure qualité qu'en utilisant l'énergie globale comme critère d'arrêt."	Jérémie Sublime, Younès Bennani, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1002078		70	fr	fr	@agroparistech.fr, @agroparistech.fr, @univ-paris13.fr	un algorithme ICM baser sur le compacité pour le segmentation des image satellite à très haut résolution  " Dans ce article nous proposer un modification pour le algorithme " " Iterated Conditional Modes " " ( ICM ) appliquer à le segmentation d' image à très haut résolution . . Pour ce faire , nous introduire un nouveau critère de convergence baser sur le compacité des clusters et qui reposer sur un fonction d' énergie adapter aux modèle de voisinage irrégulier de ce type d' image . . Grâce à ce méthode , son premier expérience avoir montrer que nous obtenir un résultat plus fiable en terme de convergence et de meilleur qualité qu' en utiliser le énergie global comme critère d' arrêt . " 	Un algorithme ICM basé sur la compacité pour la segmentation des images satellites à très haute résolution	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Un langage d'interrogation à la SPARQL pour les graphes conceptuels	Cet article propose un langage générique d'interrogation pour le modèle des graphes conceptuels. D'abord, nous introduisons les graphes d'interrogation. Un graphe d'interrogation est utilisé pour exprimer un « ou » entre deux sous-graphes, ainsi qu'une « option » sur un sous-graphe optionnel. Ensuite, nous proposons quatre types de requêtes (interrogation, sélection, description et construction) en utilisant les graphes d'interrogation. Enfin, les réponses à ces requêtes sont calculées à partir d'une opération basée sur l'homomorphisme de graphe.	Marc Legeay, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1002083		71	fr	fr	@univ-angers.fr	un langage d' interrogation à le SPARQL pour le graphe conceptuels  ce article proposer un langage générique d' interrogation pour le modèle des graphe conceptuel . D' abord , nous introduire le graphe d' interrogation . un graphe d' interrogation être utiliser pour exprimer un « ou » entre deux sous-graphes , ainsi qu' un « option » sur un sous-graphe optionnel . ensuite , nous proposer quatre type de requête ( interrogation , sélection , description et construction ) en utiliser le graphe d' interrogation . enfin , le réponse à ce requête être calculer à partir d' un opération baser sur le homomorphisme de graphe . 	Un langage d'interrogation à la SPARQL pour les graphes conceptuels	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une approche centrée graine pour la détection de communautés dans les réseaux multiplexes	Nous nous intéressons dans ce travail au problème de détection de communautés dans les réseaux multiplexes. Le modèle de réseau multiplexe a été récemment introduit afin de faciliter la modélisation des réseaux multirelationnels, des réseaux dynamiques et/ou des réseaux attribués. Les approches existantes pour la détection de communautés dans ce genre de graphes sont, pour la plupart, basées sur des schémas d'agrégation de couches ou d'agrégation de partitions. Nous proposons ici une nouvelle approche centrée graine qui permet de prendre en compte directement la nature multi-couche d'un réseau multiplexe. Des expérimentations effectuées sur différents réseaux multiplexes montrent que notre approche surpasse les approches de l'état de l'art en termes de qualité des communautés identifiées.	Issam Falih, Manel Hmimida, Rushed Kanawati	http://editions-rnti.fr/render_pdf.php?p1&p=1002099		72	fr	fr	@univ-paris13.fr	un approche centrer graine pour le détection de communauté dans le réseau multiplexes  Nous nous intéresser dans ce travail au problème de détection de communauté dans le réseau multiplexe . le modèle de réseau multiplexe avoir être récemment introduire afin de faciliter le modélisation des réseau multirelationnels , un réseau dynamique et des réseau attribuer . le approche existant pour le détection de communauté dans ce genre de graphe être , pour le plupart , baser sur un schéma d' agrégation de couche ou d' agrégation de partition . Nous proposer ici un nouveau approche centrer graine qui permettre de prendre en compte directement le nature multi-couche d' un réseau multiplexe . un expérimentation effectuer sur différents réseau multiplexe montrer que son approche surpasser le approche de le état de le art en terme de qualité des communauté identifier . 	Une approche centrée graine pour la détection de communautés dans les réseaux multiplexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une approche de visualisation analytique pour comparer les modèles de propagation dans les réseaux sociaux	Les modèles de propagation d'informations, d'influence et d'actions dans les réseaux sociaux sont nombreux et diversifiés rendant le choix de celui approprié à une situation donnée potentiellement difficile. La sélection d'un modèle pertinent pour une situation exige de pouvoir les comparer. Cette comparaison n'est possible qu'au prix d'une traduction des modèles dans un formalisme commun et indépendant de ceux-ci. Nous proposons l'utilisation de la réécriture de graphes afin d'exprimer les mécanismes de propagation sous la forme d'un ensemble de règles de transformation locales appliquées selon une stratégie donnée. Cette démarche prend tout son sens lorsque les modèles ainsi traduits sont étudiés et simulés à partir d'une plate-forme de visualisation analytique dédiée à la réécriture de graphe. Après avoir décrit les modèles et effectué différentes simulations, nous exhibons comment la plate-forme permet d'interagir avec ces formalismes, et comparer interactivement les traces d'exécution de chaque modèle grâce à diverses mesures soulignant leurs différences.	Jason Vallet, Bruno Pinaud, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1002098		73	fr	fr	@labri.fr	un approche de visualisation analytique pour comparer le modèle de propagation dans le réseau sociaux  le modèle de propagation d' information , d' influence et d' action dans le réseau social être nombreux et diversifier rendre le choix de celui approprier à un situation donner potentiellement difficile . le sélection d' un modèle pertinent pour un situation exiger de pouvoir les comparer . ce comparaison n' être possible qu' au prix d' un traduction des modèle dans un formalisme commun et indépendant de celui _-ci . Nous proposer le utilisation de le réécriture de graphe afin d' exprimer le mécanisme de propagation sous le forme d' un ensemble de règle de transformation local appliquer selon un stratégie donner . ce démarche prendre tout son sens lorsque le modèle ainsi traduire être étudier et simuler à partir d' un plate-forme de visualisation analytique dédier à le réécriture de graphe . Après avoir décrire le modèle et effectuer différent simulation , nous exhiber comment le plate-forme permettre d' interagir avec ce formalisme , et comparer interactivement le trace d' exécution de chaque modèle grâce à divers mesure souligner son différence . 	Une approche de visualisation analytique pour comparer les modèles de propagation dans les réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une nouvelle formalisation des changements ontologiques composés et complexes	L'évolution d'une ontologie est un processus indispensable dans son cycle de vie. Elle est exprimée et définie par des changements ontologiques de différents types : élémentaires, composés et complexes. Les changements complexes et composés sont très utiles dans le sens où ils aident l'utilisateur à adapter son ontologie sans se perdre dans les détails des changements élémentaires. Cependant, ils cachent derrière une formalisation sophistiquée puisqu'ils affectent, à la fois, plusieurs entités ontologiques et peuvent causer des inconsistances à l'ontologie évoluée. Pour adresser cette problématique, cet article présente une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés. Cette formalisation s'appuie sur l'approche algébrique Simple Pushout (SPO) de transformation de graphes et possède deux principaux avantages : (1) fournir une nouvelle formalisation permettant de contrôler les transformations de graphes et éviter les incohérences d'une manière a priori, (2) simplifier la définition des changements composés et complexes en réduisant le nombre de changements élémentaires nécessaires à leur application.	Mariem Mahfoudh, Laurent Thiry, Germain Forestier, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1002087		74	fr	fr	@uha.fr	un nouveau formalisation des changement ontologique composer et complexes  le évolution d' un ontologie être un processus indispensable dans son cycle de vie . Elle être exprimer et définir par un changement ontologique de différent type : élémentaire , composer et complexe . le changement complexe et composer être très utile dans le sens où ils aider le utilisateur à adapter son ontologie sans clr perdre dans le détail des changement élémentaire . cependant , ils cacher derrière un formalisation sophistiquer puisqu' ils affecter , à le foi , plusieurs entité ontologique et pouvoir causer un inconsistance à le ontologie évolué . Pour adresser ce problématique , ce article présenter un nouveau formalisation des changement ontologique composer et complexe baser sur le grammaire de graphe typer . ce formalisation clr appuyer sur le approche algébrique simple Pushout ( SPO ) de transformation de graphe et posséder deux principal avantage : ( 1 ) fournir un nouveau formalisation permettre de contrôler le transformation de graphe et éviter le incohérence d' un manière avoir priori , ( 2 ) simplifier le définition des changement composer et complexe en réduire le nombre de changement élémentaire nécessaire à son application . 	Une nouvelle formalisation des changements ontologiques composés et complexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une nouvelle méthode de Web Usage Mining basée sur une analyse sémiotique du comportement de navigation	L'objectif de nos travaux est de proposer une méthode d'analyse automatique du comportement des utilisateurs à des fins de prédiction de leur propension à réaliser une action suggérée. Nous proposons dans cet article une nouvelle méthode de Web Usage Mining basée sur une étude sémiotique des styles perceptifs, considérant l'expérience de l'utilisateur comme élément déterminant de sa réaction à une sollicitation. L'étude de ces styles nous a amené à définir de nouveaux indicateurs (des descripteurs sémiotiques) introduisant un niveau supplémentaire à l'approche sémantique d'annotation des sites. Nous proposons ensuite un modèle neuronal adapté au traitement de ces nouveaux indicateurs. Nous expliquerons en quoi le modèle proposé est le plus pertinent pour traiter ces informations.	Sandra Mellot, Tony Bourdier, Moez Baccouche	http://editions-rnti.fr/render_pdf.php?p1&p=1002090		75	fr	fr	@iraiser.eu	un nouveau méthode de Web Usage Mining baser sur un analyse sémiotique du comportement de navigation  le objectif de son travail être de proposer un méthode d' analyse automatique du comportement des utilisateur à un fin de prédiction de son propension à réaliser un action suggérer . Nous proposer dans ce article un nouveau méthode de Web Usage Mining baser sur un étude sémiotique des style perceptif , considérant le expérience de le utilisateur comme élément déterminant de son réaction à un sollicitation . le étude de ce style nous avoir amener à définir un nouveau indicateur ( des descripteur sémiotique ) introduire un niveau supplémentaire à le approche sémantique d' annotation des site . Nous proposer ensuite un modèle neuronal adapter au traitement de ce nouveau indicateur . Nous expliquer en quoi le modèle proposer être le plus pertinent pour traiter ce information . 	Une nouvelle méthode de Web Usage Mining basée sur une analyse sémiotique du comportement de navigation	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une Plateforme ETL parallèle et distribuée pour l'intégration de données massives	Nous nous intéressons, dans ce papier, à l'impact des données massives dans un environnement décisionnel et plus particulièrement sur la phase d'intégration des données. Dans ce contexte, nous avons développé une plateforme, baptisée P-ETL (Parallel-ETL), destinée à l'entreposage de données massives selon le paradigme MapReduce. P-ETL permet le paramétrage de processus ETL (workflow) et un paramétrage avancé relatif à l'environnement parallèle et distribué. Ce papier décrit la plateforme P-ETL en vue d'une démonstration. Face à des jeux de données allant de 244 * 106 à 7, 317 * 109 tuples, les expérimentations menées ont montré l'amélioration significative des performances de P-ETL lorsque la taille du cluster et le nombre des tâches parallèles augmentent.	Mahfoud Bala, Oussama Mokeddem, Omar Boussaid, Zaia Alimazighi	http://editions-rnti.fr/render_pdf.php?p1&p=1002109		76	fr	fr	@gmail.com, @univ-lyon2.fr, @usthb.dz	un plateforme ETL parallèle et distribuer pour le intégration de donnée massives  Nous nous intéresser , dans ce papier , à le impact des donnée massif dans un environnement décisionnel et plus particulièrement sur le phase d' intégration des donnée . Dans ce contexte , nous avoir développer un plateforme , baptiser P-ETL ( Parallel-ETL ) , destiner à le entreposage de donnée massif selon le paradigme MapReduce . P-ETL permettre le paramétrage de processus ETL ( workflow ) et un paramétrage avancer relatif à le environnement parallèle et distribuer . ce papier décrire le plateforme P-ETL en vue d' un démonstration . face à un jeu de donnée aller de 244 * 106 à 7_,_317 * 109 tuples , le expérimentation mener avoir montrer le amélioration significatif des performance de P-ETL lorsque le taille du cluster et le nombre des tâche parallèle augmenter . 	Une Plateforme ETL parallèle et distribuée pour l'intégration de données massives	1
Revue des Nouvelles Technologies de l'Information	EGC	2015	Using Social Conversational Context For Detecting Users Interactions on Microblogging Sites	Dans ce travail, nous proposons une nouvelle méthode de détection des conversations sur les sites des réseaux sociaux. Cette méthode est basée sur l'analyse et l'enrichissement de contenu dans le but de présenter un résultat informatif basé sur les interactions des utilisateurs. Nous avons évalué notre méthode sur corpus recueillis de réseau social lié à des sujets spécifiques, et nous avons obtenu des bons résultats.	Rami BELKAROUI, Rim Faiz, Aymen Elkhlifi	http://editions-rnti.fr/render_pdf.php?p1&p=1002101		77	en	fr	@gmail.com, @ihec.rnu.tn, @paris4.sorbonne.fr	Dans ce travail , nous proposer un nouveau méthode de détection des conversation sur le site des réseau social . ce méthode être baser sur le analyse et le enrichissement de contenu dans le but de présenter un résultat informatif baser sur le interaction des utilisateur . Nous avoir évaluer son méthode sur corpus recueillir de réseau social lier à un sujet spécifique , et nous avoir obtenir un bon résultat . 	Using Social Conversational Context For Detecting Users Interactions on Microblogging Sites	1
Revue des Nouvelles Technologies de l'Information	EGC	2015	Utilisation des pyramides pour visualiser la contamination des manuscrits	In this paper we present a new codicum stemma visualization method. Don Quentin's modeling is usec to classify the textual tradition.We supplement the genealogical editor's information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets	Marc Le Pouliquen	http://editions-rnti.fr/render_pdf.php?p1&p=1002115		78	fr	en	@univ-brest.fr	utilisation des pyramide pour visualiser le contamination des manuscrits 	Utilisation des pyramides pour visualiser la contamination des manuscrits	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Vers la découverte de modèles exceptionnels locaux : des règles descriptives liant les molécules à leurs odeurs	"Issue d'un phénomène complexe partant d'une molécule odorante jusqu'à la perception dans le cerveau, l'olfaction reste le sens le plus difficile à appréhender par les neuroscientifiques. L'enjeu principal est d'établir des règles sur les propriétés physicochimiques des molécules (poids, nombre d'atomes, etc.) afin de caractériser spécifiquement un sous-ensemble de qualités olfactives (fruité, boisé, etc.). On peut trouver de telles règles descriptives grâce à la découverte de sous-groupes (""subgroup discovery""). Cependant les méthodes existantes permettent de caractériser soit une seule qualité olfactive ; soit toutes les qualités olfactives à la fois (""exceptional model mining"") mais pas un sousensemble. Nous proposons alors une approche de découverte de sous-groupes caractéristiques de seulement certains labels, par une nouvelle technique d'énumération, issue de la fouille de redescriptions. Nous avons expérimenté notre méthode sur une base de données d'olfaction fournie par des neuroscientifiques et pu exhiber des premiers sous-groupes intelligibles et réalistes."	Guillaume Bosc, Mehdi Kaytoue, Marc Plantevit, Fabien De Marchi, Moustafa Bensafi, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002091		79	fr	fr	@insa-lyon.fr	Vers le découverte de modèle exceptionnel local : un règle descriptif lier le molécule à son odeurs  " issu d' un phénomène complexe partir d' un molécule odorant jusqu' à le perception dans le cerveau , le olfaction rester le sens le plus difficile à appréhender par le neuroscientifiques . . le enjeu principal être d' établir un règle sur le propriété physicochimiques des molécule ( poids , nombre d' atome , etc. ) afin de caractériser spécifiquement un sous-ensemble de qualité olfactif ( fruité , boiser , etc. ) . . On pouvoir trouver de tel règle descriptif grâce à le découverte de sous-groupes ( " " subgroup discovery " " ) . . cependant le méthode existant permettre de caractériser être un seul qualité olfactif ; ; soit tout le qualité olfactif à le foi ( " " exceptional model mining " " ) mais pas un sousensemble . . Nous proposer alors un approche de découverte de sous-groupes caractéristique de seulement certain label , par un nouveau technique d' énumération , issue de le fouille de redescriptions . . Nous avoir expérimenter son méthode sur un base de donnée d' olfaction fournir par un neuroscientifiques et pouvoir exhiber un premier sous-groupes intelligible et réaliste . " 	Vers la découverte de modèles exceptionnels locaux : des règles descriptives liant les molécules à leurs odeurs	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	Visualizing Shooting Spots using Geo-tagged Photographs from Social Media Sites	Hotspots, à laquelle de nombreuses photographies ont été prises, pourraient être des lieux intéressants pour beaucoup de gens faire du tourisme. Visualisation des hotspots révèle les intérêts des utilisateurs, ce qui est important pour les industries telles que la recherche et du marketing touristiques. Bien que plusieurs techniques basées sociaux-pour hotspots extraction indépendamment ont été proposés, un hotspot a une relation à d'autres hotspots dans certains cas. Pour organiser ces hotspots, nous proposons une méthode pour détecter et de visualiser les relations entre les hotspots. Notre méthode proposée détecte et évalue les relations de taches de tir et sujets photographiques. Notre approche extrait les relations à l'aide de sous-hotspots, qui sont fendus d'un hotspot qui comprend des photographies de différents types.	Masaharu Hirota, Masaki Endo, Shohei Yokoyama, Hiroshi Ishikawa	http://editions-rnti.fr/render_pdf.php?p1&p=1002076		80	en	fr	@sd.tmu.ac.jp, @inf.shizuoka.ac.jp	Hotspots , à laquelle un nombreux photographie avoir être prendre , pouvoir être un lieu intéressant pour beaucoup de gens faire du tourisme . visualisation des hotspots révéler le intérêt des utilisateur , ce qui être important pour le industrie tel que le recherche et du marketing touristique . bien que plusieurs technique baser sociaux-pour hotspots extraction indépendamment avoir être proposer , un hotspot avoir un relation à un autre hotspots dans certain cas . Pour organiser ce hotspots , nous proposer un méthode pour détecter et de visualiser le relation entre le hotspots . son méthode proposer détecter et évaluer le relation de tache de tir et sujet photographique . son approche extraire le relation à le aide de sous-hotspots , qui être fendre d' un hotspot qui comprendre un photographie de différents type . 	Visualizing Shooting Spots using Geo-tagged Photographs from Social Media Sites	0
Revue des Nouvelles Technologies de l'Information	EGC	2015	XEWGraph : Outil de Visualisation et Analyse des Hypergraphes pour un Système d'Intelligence Economique	The Competitive Intelligence System Xplor EveryWhere helps searching, visualizing, and sharing useful data. In this paper, we will intorduce Xplor EveryWhere and its newest feature called XEWGraph, which is dedicated to the analysis of massive data and visualization of hypergraphs.	Zakaria Boulouard, Amine El Haddadi, Anass El Haddadi, Lahcen Koutti, Abdelhadi Fennan	http://editions-rnti.fr/render_pdf.php?p1&p=1002117		81	fr	en	@gmail.com, @yahoo.fr, @gmail.com, @gmail.com	XEWGraph : outil de visualisation et analyse des Hypergraphes pour un système d' Intelligence Economique 	XEWGraph : Outil de Visualisation et Analyse des Hypergraphes pour un Système d'Intelligence Economique	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	1d-SAX : une nouvelle représentation symbolique pour les séries temporelles	SAX (Symbolic Aggregate approXimation) est une des techniquesmajeures de symbolisation des séries temporelles. La non prise en compte destendances dans la symbolisation est une limitation bien connue de SAX. Cet articleprésente 1d-SAX, une méthode pour représenter une série temporelle parune séquence de symboles contenant des informations sur la moyenne et la tendancedes fenêtres successives de la série segmentée. Nous comparons l'efficacitéde 1d-SAX vs SAX dans une tâche de classification de séries temporellesd'images satellites. Les résultats montrent que 1d-SAX améliore les taux de classificationpour une quantité d'information identique utilisée.	Simon Malinowski, Thomas Guyet, Rene Quiniou, Romain Tavenard	http://editions-rnti.fr/render_pdf.php?p1&p=1001930		122	fr	fr	@agrocampus-ouest.fr, @agrocampus-ouest.fr, @inria.fr, @idiap.ch	1d : un nouveau représentation symbolique pour le série temporelles  SAX ( Symbolic Aggregate approXimation ) être un des techniquesmajeures de symbolisation des série temporel . le non prendre en compte destendances dans le symbolisation être un limitation bien connaître de SAX . ce articleprésente 1d , un méthode pour représenter un série temporel parune séquence de symbole contenir un information sur le moyenne et le tendancedes fenêtre successif de le série segmenter . Nous comparer le efficacitéde 1d vs SAX dans un tâche de classification de série temporellesd'images satellite . le résultat montrer que 1d améliorer le taux de classificationpour un quantité d' information identique utiliser . 	1d-SAX : une nouvelle représentation symbolique pour les séries temporelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Agrégation de sac-de-sacs-de-mots pour la recherche d'information par modèles vectoriels	Cet article étudie l'intérêt de représenter les documents textuels nonplus comme des sacs-de-mots, mais comme des sacs-de-sacs-de-mots. Au coeurde l'utilisation de cette représentation, le calcul de similarité entre deux objetsnécessite alors d'agréger toutes les similarités entre sacs de chacun des objets.Nous évaluons cette représentation dans un cadre de recherche d'information,et étudions les propriétés attendues de ces fonctions d'agrégation. Les expériencesrapportées montrent l'intérêt de cette représentation lorsque les opérateursd'agrégation respectent certaines propriétés, avec des gains très importantspar rapport aux représentations standard.	Vincent Claveau	http://editions-rnti.fr/render_pdf.php?p1&p=1001925		123	fr	fr	@irisa.fr	agrégation de sac-de-sacs-de-mots pour le recherche d' information par modèle vectoriels  ce article étudier le intérêt de représenter le document textuel nonplus comme un sacs-de-mots , mais comme un sacs-de-sacs-de-mots . Au coeurde le utilisation de ce représentation , le calcul de similarité entre deux objetsnécessite alors d' agréger tout le similarité entre sac de chacun des objet . Nous évaluer ce représentation dans un cadre de recherche d' information , et étudier le propriété attendre de ce fonction d' agrégation . le expériencesrapportées montrer le intérêt de ce représentation lorsque le opérateursd'agrégation respecter certain propriété , avec un gain très importantspar rapport aux représentation standard . 	Agrégation de sac-de-sacs-de-mots pour la recherche d'information par modèles vectoriels	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Alignement d'ontologies : exploitation des ontologies liées sur le web de données	Nous proposons dans cet article une méthode d'alignement d'une ontologiesource avec des ontologies cibles déjà publiées et liées sur le web dedonnées. Nous présentons ensuite un retour d'expérience sur l'alignement d'uneontologie dans le domaine des sciences du vivant et de l'environnement avecAGROVOC et NALT.	Thomas Hecht, Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu, Cássia Trojahn dos Santos	http://editions-rnti.fr/render_pdf.php?p1&p=1001911		124	fr	fr	@risk, @supagro.inra.fr, @agroparistech.fr, @irit.fr	alignement d' ontologie : exploitation des ontologie lier sur le web de données  Nous proposer dans ce article un méthode d' alignement d' un ontologiesource avec un ontologie cible déjà publier et lier sur le web dedonnées . Nous présenter ensuite un retour d' expérience sur le alignement d' uneontologie dans le domaine des science du vivre et de le environnement avecAGROVOC et NALT . 	Alignement d'ontologies : exploitation des ontologies liées sur le web de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Annotation sémantique de documents administratifs	La numérisation de documents administratifs est un enjeu économiqueet écologique prioritaire dans le contexte sociétal actuel. La dématérialisationmassive de document n'est pas sans conséquence et soulève les problèmes d'organisation,de stockage et d'accès à l'information. Le défi n'est donc plus la numérisationdu document, mais l'extraction des informations qu'ils contiennent.Les documents sont produits par l'Homme et pour l'Homme. Cette propriétépermet de localiser des informations dans les zones saillantes du document (logos).La saillance et la reconnaissance sont deux éléments essentiels pour laclassification rapide de documents. A l'opposé, la recherche d'un document oud'un ensemble de documents repose presque toujours sur le texte brut, il estdonc nécessaire de faire une correspondance entre une requête textuelle et ledocument. Cet article présente une nouvelle approche d'annotation automatiquede documents administratifs qui utilise une approche visuel et une approche defouille de texte.	Benjamin Duthil, Mickaël Coustaty, Vincent Courboulay, Jean-Marc Ogier	http://editions-rnti.fr/render_pdf.php?p1&p=1001913		125	fr	fr	@univ-lr.fr	annotation sémantique de document administratifs  le numérisation de document administratif être un enjeu économiqueet écologique prioritaire dans le contexte sociétal actuel . le dématérialisationmassive de document n' être pas sans conséquence et soulever le problème d' organisation , de stockage et d' accès à le information . le défi n' être donc plus le numérisationdu document , mais le extraction des information qu' ils contenir . le document être produire par le Homme et pour le Homme . ce propriétépermet de localiser un information dans le zone saillant du document ( logo ) . le saillance et le reconnaissance être deux élément essentiel pour laclassification rapide de document . A le opposer , le recherche d' un document oud'un ensemble de document reposer presque toujours sur le texte brut , il estdonc nécessaire de faire un correspondance entre un requête textuel et ledocument . ce article présenter un nouveau approche d' annotation automatiquede document administratif qui utiliser un approche visuel et un approche defouille de texte . 	Annotation sémantique de documents administratifs	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Application du paradigme MapReduce aux données ouvertes Cas : Accessibilité des personnes à mobilité réduite aux musées	Le modèle MapReduce est aujourd'hui l'un des modèles de programmationparallèle les plus utilisés. Définissant une architecture Maître-Esclave,il permet le traitement parallèle de grandes masses de données. Dans ce papier,nous proposons un algorithme basé sur MapReduce qui permet, à partir des donnéespubliques du Ministère Français de la Communication et de la Culture, dedéfinir un classement des galeries et musées nationaux selon leurs degré d'accessibilitéaux personnes handicapées. Tout en profitant de la puissance et de laflexibilité du paradigme MapReduce, les décideurs pourront mettre en place desstratégies efficaces à moindre coût et avoir ainsi une vision plus précise sur lesétablissements culturels et leurs limites relatives à cette catégorie de personnes.L'algorithme que nous proposons peut être exploité et appliqué à d'autres casd'études avec des jeux de données plus volumineux.	Billel Arres, Nadia Kabachi, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001963		126	fr	fr	@univ-lyon2.fr	application du paradigme MapReduce aux donnée ouvert cas : accessibilité des personne à mobilité réduire aux musées  le modèle MapReduce être aujourd' hui le un des modèle de programmationparallèle le plus utiliser . définir un architecture Maître-Esclave , il permettre le traitement parallèle de grand masse de donnée . Dans ce papier , nous proposer un algorithme baser sur MapReduce qui permettre , à partir un donnéespubliques du ministère français de le communication et de le Culture , dedéfinir un classement des galerie et musée national selon son degré d' accessibilitéaux personne handicaper . tout en profiter de le puissance et de laflexibilité du paradigme MapReduce , le décideur pouvoir mettre en place desstratégies efficace à moindre coût et avoir ainsi un vision plus précis sur lesétablissements culturel et son limite relatif à ce catégorie de personne . le algorithme que nous proposer pouvoir être exploiter et appliquer à un autre casd'études avec un jeu de donnée plus volumineux . 	Application du paradigme MapReduce aux données ouvertes Cas : Accessibilité des personnes à mobilité réduite aux musées	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage de fonctions de tri pour la prédiction d'interactions protéine-ARN	Les fonctions biologiques dans la cellule mettent en jeu des interactions3D entre protéines et ARN. Les avancées des techniques exérimentalesrestent insuffisantes pour de nombreuse applications. Il faut alors pouvoir prédirein silico les interactions protéine-ARN. Dans ce contexte, nos travaux sontfocalisés sur la construction de fonctions de score permettant d'ordonner les solutionsgénérées par le programme d'amarrage protéine-ARN RosettaDock. Laméthodologie d'évaluation utilisée par RosettaDock impose de trouver une fonctionde score s'exprimant comme une combinaison linéaire de mesures physicochimiques.Avec une approche d'apprentissage supervisé par algorithme génétique,nous avons appris différentes fonctions de score en imposant descontraintes sur la nature des poids recherchés. Les résultats obtenus montrentl'importance de la signification des poids à apprendre et de l'espace de rechercheassocié.	Adrien Guilhot-Gaudeffroy, Jérôme Azé, Julie Bernauer, Christine Froidevaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001960		127	fr	fr	@lri.fr	apprentissage de fonction de tri pour le prédiction d' interaction protéine-ARN  le fonction biologique dans le cellule mettre en jeu des interaction 3D entre protéine et ARN . le avancée des technique exérimentalesrestent insuffisant pour un nombreux application . Il faillir alors pouvoir prédirein silico le interaction protéine-ARN . Dans ce contexte , son travail sontfocalisés sur le construction de fonction de score permettre d' ordonner le solutionsgénérées par le programme d' amarrage protéine-ARN RosettaDock . Laméthodologie d' évaluation utiliser par RosettaDock imposer de trouver un fonctionde score clr exprimer comme un combinaison linéaire de mesure physicochimiques . Avec un approche d' apprentissage superviser par algorithme génétique , nous avoir apprendre différent fonction de score en imposer descontraintes sur le nature des poids rechercher . le résultat obtenir montrentl'importance de le signification des poids à apprendre et de le espace de rechercheassocié . 	Apprentissage de fonctions de tri pour la prédiction d'interactions protéine-ARN	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage incrémental anytime d'un classifieur Bayésien naïf pondéré	Nous considérons le problème de classification supervisée pour desflux de données présentant éventuellement un très grand nombre de variablesexplicatives. Le classifieur Bayésien naïf se révèle alors simple à calculer etrelativement performant tant que l'hypothèse restrictive d'indépendance des variablesconditionnellement à la classe est respectée. La sélection de variables etle moyennage de modèles sont deux voies connues d'amélioration qui reviennentà déployer un prédicteur Bayésien naïf intégrant une pondération des variablesexplicatives. Dans cet article, nous nous intéressons à l'estimation directe d'untel modèle Bayésien naïf pondéré. Nous proposons une régularisation parcimonieusede la log-vraisemblance du modèle prenant en compte l'informativité dechaque variable. La log-vraisemblance régularisée obtenue étant non convexe,nous proposons un algorithme de gradient en ligne qui post-optimise la solutionobtenue afin de déjouer les minima locaux. Les expérimentations menéess'intéressent d'une part à la qualité de l'optimisation obtenue et d'autre part auxperformances du classifieur en fonction du paramétrage de la régularisation.	Carine Hue, Marc Boullé, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001939		128	fr	fr		apprentissage incrémental anytime d' un classifieur Bayésien naïf pondéré  Nous considérer le problème de classification superviser pour desflux de donnée présenter éventuellement un très grand nombre de variablesexplicatives . le classifieur Bayésien naïf clr révéler alors simple à calculer etrelativement performant tant que le hypothèse restrictif d' indépendance des variablesconditionnellement à le classe être respecter . le sélection de variable etle moyennage de modèle être deux voie connaître d' amélioration qui reviennentà déployer un prédicteur Bayésien naïf intégrer un pondération des variablesexplicatives . Dans ce article , nous clr intéresser à le estimation direct d' untel modèle Bayésien naïf pondérer . Nous proposer un régularisation parcimonieusede le log-vraisemblance du modèle prendre en compte le informativité dechaque variable . le log-vraisemblance régulariser obtenir être non convexe , nous proposer un algorithme de gradient en ligne qui post-optimise le solutionobtenue afin de déjouer le minimum local . le expérimentation menéess'intéressent d' un part à le qualité de le optimisation obtenir et d' autre part auxperformances du classifieur en fonction du paramétrage de le régularisation . 	Apprentissage incrémental anytime d'un classifieur Bayésien naïf pondéré	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage non supervisé de dépendances syntaxiques à partir de texte étiqueté, plusieurs variantes de PCFG légères	L'apprentissage de dépendances est une tâche consistant à établir, àpartir des phrases d'un texte, un modèle de construction d'arbres traduisant unehiérarchie syntaxique entre les mots. Nous proposons un modèle intermédiaireentre l'analyse syntaxique complète de la phrase et les sacs de mots. Il est basésur une grammaire stochastique hors-contexte se traduisant par des relations dedépendance entre les catégories grammaticales d'une phrase. Les résultats expérimentauxobtenus sur des benchmarks attestés dépassent pour cinq langues surdix les scores de l'algorithme de référence DMV, et pour la première fois desscores sont obtenus pour le français. La très grande simplicité de la grammairepermet un apprentissage très rapide, et une analyse presque instantanée.	Marie Arcadias, Guillaume Cleuziou, Edmond Lassalle, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001924		129	fr	fr	@orange.com, @univ-orleans.fr	apprentissage non superviser de dépendance syntaxique à partir de texte étiqueter , plusieurs variante de PCFG légères  le apprentissage de dépendance être un tâche consister à établir , àpartir un phrase d' un texte , un modèle de construction d' arbre traduire unehiérarchie syntaxique entre le mot . Nous proposer un modèle intermédiaireentre le analyse syntaxique complet de le phrase et le sac de mot . Il être basésur un grammaire stochastique hors-contexte clr traduire par un relation dedépendance entre le catégorie grammatical d' un phrase . le résultat expérimentauxobtenus sur un benchmark attester dépasser pour cinq langue surdix le score de le algorithme de référence DMV , et pour le premier foi desscores être obtenir pour le français . le très grand simplicité de le grammairepermet un apprentissage très rapide , et un analyse presque instantané . 	Apprentissage non supervisé de dépendances syntaxiques à partir de texte étiqueté, plusieurs variantes de PCFG légères	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Approche formelle de fusion d'ontologies à l'aide des grammaires de graphes typés	L'article propose une approche formelle de fusion d'ontologies se reposantsur les grammaires de graphes typés. Elle se décompose en trois étapes :1) la recherche de similarités entre concepts ; 2) la fusion des ontologies parl'approche algébrique SPO (Simple Push Out) ; 3) l'adaptation d'une ontologieglobale par le biais de règles de réécriture de graphes. Contrairement aux solutionsexistantes, cette méthode offre une représentation formelle de la fusiond'ontologies ainsi qu'une implémentation fonctionnelle basée sur l'outil AGG.	Mariem Mahfoudh, Germain Forestier, Laurent Thiry, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1001980		130	fr	fr	@uha.fr	approche formel de fusion d' ontologie à le aide des grammaire de graphe typés  le article proposer un approche formel de fusion d' ontologie clr reposantsur le grammaire de graphe typer . Elle clr décomposer en trois étape : 1 ) le recherche de similarité entre concept ; 2 ) le fusion des ontologie parl'approche algébrique SPO ( simple Push Out ) ; 3 ) le adaptation d' un ontologieglobale par le biais de règle de réécriture de graphe . contrairement aux solutionsexistantes , ce méthode offrir un représentation formel de le fusiond'ontologies ainsi qu' un implémentation fonctionnel baser sur le outil AGG . 	Approche formelle de fusion d'ontologies à l'aide des grammaires de graphes typés	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Approche par motifs pour l'analyse de données multi-résolution	Dans cet article nous nous intéressons aux approches pour l'analysede graphes pouvant évoluer dans le temps et tel qu'un sommet à un temps donnépeut correspondre à plusieurs sommets au temps suivant et où les sommets sontassociés à un ensemble d'attributs catégoriels. Dans ce type de données, nousproposons une nouvelle classe de motifs basée sur des contraintes permettant dedécrire l'évolution de structures homogènes. Ce type d'approche est particulièrementadaptée pour l'analyse d'images multi-résolution sans perte d'information.Nous présentons un résultat qualitatif dans ce domaine.	Pierre-Nicolas Mougel, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001952		131	fr	fr	@univ-nc.nc	approche par motif pour le analyse de donnée multi-résolution  Dans ce article nous nous intéresser aux approche pour le analysede graphe pouvoir évoluer dans le temps et tel qu' un sommet à un temps donnépeut correspondre à plusieurs sommet au temps suivant et où le sommet sontassociés à un ensemble d' attribut catégoriel . Dans ce type de donnée , nousproposons un nouveau classe de motif baser sur un contrainte permettre dedécrire le évolution de structure homogène . ce type d' approche être particulièrementadaptée pour le analyse d' image multi-résolution sans perte d' information . Nous présenter un résultat qualitatif dans ce domaine . 	Approche par motifs pour l'analyse de données multi-résolution	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Automatic correction of SVM for drifted data classification	Concept drift is an important feature of real-world data streams thatcan make usual machine learning techniques rapidly become unsuitable. Thispaper addresses the problem of sudden concept drift in classification problemsfor which standard techniques may fail. To this end, support vector machines(SVMs) are automatically corrected to cope with a new suddenly drifted dataset.Results on real-world datasets with several types of sudden drift indicate that themethod is able to correct the SVM in order to better classify the new data afterthe concept drift, using a correction based on the difference between the initialdataset and the new drifted dataset, even when the new dataset is small.	Alexandra Degeest, Benoît Frénay, Michel Verleysen	http://editions-rnti.fr/render_pdf.php?p1&p=1001942		132	en	en	@uclouvain.be, @isib.be	automatic correction svm drift datum classification concept drift important feature real world data stream thatcan make usual machine learn technique rapidly become unsuitable thispaper address problem sudden concept drift classification problemsfor standard technique may fail end support vector machine svms automatically correct cope new suddenly drift dataset result real world dataset several type sudden drift indicate themethod able correct svm order better classify new data afterthe concept drift used correction base difference initialdataset new drift dataset even new dataset small	Automatic correction of SVM for drifted data classification	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Broad Data: What happens when the Web of Data becomes real?	"Big Data is used to refer to the very large datasets generated by scientists, to the manypetabytes of data held by companies like Facebook and Google, and to analyzing real-time dataassets like the stream of twitter messages emerging from events around the world. Key areasof interest include technologies to manage much larger datasets (cf. NoSQL), technologies for the visualization and analysis of databases, cloud-based data management and dataminingalgorithms.Recently, however, we have begun to see the emergence of another, and equally compellingdata challenge - that of the ""Broad data"" that emerges from millions and millions of rawdatasets available on the World Wide Web. For broad data the new challenges that emerge includeWeb-scale data search and discovery, rapid and potentially ad hoc integration of datasets,visualization and analysis of only-partially modeled datasets, and issues relating to the policiesfor data use, reuse and combination. In this talk, we present the broad data challenge anddiscuss potential starting points for solutions. We illustrate these approaches using data froma ""meta-catalog"" of over 1,000,000 open datasets that have been collected from about twohundred governments from around the world."	Jim Hendler	http://editions-rnti.fr/render_pdf.php?p1&p=1001905		133	en	en	@cs.rpi.edu	broad datum happen web datum become real big datum used refer large dataset generate scientist manypetabyte data held company like facebook google analyze real time dataasset like stream twitter message emerge event around world key areasof interest include technology manage much larger dataset cf nosql technology visualization analysis databasis cloud base datum management dataminingalgorithms recently however begin see emergence another equally compellingdata challenge broad datum emerge million million rawdataset available world wide web broad datum new challenge emerge includeweb scale datum search discovery rapid potentially ad hoc integration dataset visualization analysis only partially modeled dataset issue relate policiesfor data use reuse combination talk present broad data challenge anddiscus potential start point solution illustrate approach used datum froma meta catalog 1 000 000 open dataset collect twohundred government around world	Broad Data: What happens when the Web of Data becomes real?	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Classification des actions humaines basée sur les descripteurs spatio-temporels	Dans cet article, nous proposons un nouveau descripteurspatio-temporel appelé ST-SURF pour l'analyse et la reconnaissance d'actionsdans des flux vidéo. L'idée principale est d'enrichir le descripteur Speed UpRobust Feature (SURF) en intégrant l'information de mouvement issue du flotoptique. Seuls les points d'intérêts qui ont subi un déplacement sont pris encompte pour générer un dictionnaire de mots visuels (DMV) robuste basé surl'algorithme des k-moyennes (K-means). Le dictionnaire est utilisé lors du processusd'apprentissage et de reconnaissance d'actions basé sur la méthode desmachines à vecteurs supports (SVM). Les résultats obtenus confirment l'intérêtdu descripteur proposé ST-SURF pour l'analyse de scènes et en particulierpour la reconnaissance d'actions. La méthode atteind une précision de reconnaissancede l'ordre de 80.7%, équivalente aux performances des descripteursspatio-temporels de l'état de l'art.	Sameh Megrhi, Azeddine Beghdadi, Wided Souidène	http://editions-rnti.fr/render_pdf.php?p1&p=1001945		134	fr	fr		classification des action humain baser sur le descripteur spatio- temporels  Dans ce article , nous proposer un nouveau descripteurspatio- temporel appeler ST-SURF pour le analyse et le reconnaissance d' actionsdans des flux vidéo . le idée principal être d' enrichir le descripteur Speed UpRobust Feature ( SURF ) en intégrer le information de mouvement issue du flotoptique . seul le point d' intérêt qui avoir subir un déplacement être prendre encompte pour générer un dictionnaire de mot visuel ( DMV ) robuste baser surl'algorithme des k-moyennes ( K-means ) . le dictionnaire être utiliser lors du processusd'apprentissage et de reconnaissance d' action baser sur le méthode desmachines à vecteur support ( SVM ) . le résultat obtenir confirmer le intérêtdu descripteur proposer ST-SURF pour le analyse de scène et en particulierpour le reconnaissance d' action . le méthode atteind un précision de reconnaissancede le ordre de 80 .7 \% , équivalent aux performance des descripteursspatio- temporel de le état de le art . 	Classification des actions humaines basée sur les descripteurs spatio-temporels	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Classification et prédiction du flux solaire	La prédiction du rayonnement solaire horaire dans une journée estun enjeu primordial pour la production d'énergie de type photovoltaïque. Nousprésentons deux stratégies de classification des jours selon leurs rayonnementssolaires puis une méthode de prédiction du flux solaire cohérente avec la classification.	Henri Ralambondrainy, Yves Lechevallier, Jean Daniel Lan-Sun-Luk, Jean-Pierre Chabriat	http://editions-rnti.fr/render_pdf.php?p1&p=1001962		135	fr	fr	@univ-reunion.fr, @inria.fr, @univ-reunion.fr	classification et prédiction du flux solaire  le prédiction du rayonnement solaire horaire dans un journée estun enjeu primordial pour le production d' énergie de type photovoltaïque . Nousprésentons deux stratégie de classification des jour selon son rayonnementssolaires puis un méthode de prédiction du flux solaire cohérent avec le classification . 	Classification et prédiction du flux solaire	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Classifieur naïf de Bayes pondéré pour flux de données	Un classifieur naïf de Bayes est un classifieur probabiliste basé surl'application du théorème de Bayes avec l'hypothèse naïve, c'est-à-dire que lesvariables explicatives (Xi) sont supposées indépendantes conditionnellement àla variable cible (C). Malgré cette hypothèse forte, ce classifieur s'est avéré trèsefficace sur de nombreuses applications réelles et est souvent utilisé sur les fluxde données pour la classification supervisée. Le classifieur naïf de Bayes nécessitesimplement en entrée l'estimation des probabilités conditionnelles parvariable P(Xi|C) et les probabilités a priori P(C). Pour une utilisation sur lesflux de données, cette estimation peut être fournie à l'aide d'un « résumé superviséen-ligne de quantiles ». L'état de l'art montre que le classifieur naïf de Bayespeut être amélioré en utilisant une méthode de sélection ou de pondération desvariables explicatives. La plupart de ces méthodes ne peuvent fonctionner quehors-ligne car elles nécessitent de stocker toutes les données en mémoire et/oude lire plus d'une fois chaque exemple. Par conséquent, elles ne peuvent être utiliséessur les flux de données. Cet article présente une nouvelle méthode baséesur un modèle graphique qui calcule les poids des variables d'entrée en utilisantune estimation stochastique. La méthode est incrémentale et produit un classifieurNaïf de Bayes Pondéré pour flux de données. Cette méthode est comparéeau classique classifieur naïf de Bayes sur les données utilisées lors du challenge« Large Scale Learning ».	Christophe Salperwyck, Vincent Lemaire, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1001938		136	fr	fr		classifieur naïf de Bayes pondérer pour flux de données  un classifieur naïf de Bayes être un classifieur probabiliste baser surl'application du théorème de Bayes avec le hypothèse naïf , c' est-à-dire que lesvariables explicatives ( Xi ) être supposer indépendant conditionnellement àla variable cible ( C ) . Malgré ce hypothèse fort , ce classifieur clr être avérer trèsefficace sur un nombreux application réel et être souvent utiliser sur le fluxde donner pour le classification superviser . le classifieur naïf de Bayes nécessitesimplement en entrée le estimation des probabilité conditionnel parvariable P ( Xi|C ) et le probabilité avoir priori P ( C ) . Pour un utilisation sur lesflux de donnée , ce estimation pouvoir être fournir à le aide d' un « résumé superviséen-ligne de quantiles » . le état de le art montrer que le classifieur naïf de Bayespeut être améliorer en utiliser un méthode de sélection ou de pondération desvariables explicatif . le plupart de ce méthode ne pouvoir fonctionner quehors-ligne car elles nécessiter de stocker tout le donnée en mémoire et  oude lire plus d' un foi chaque exemple . Par conséquent , elles ne pouvoir être utiliséessur le flux de donnée . ce article présenter un nouveau méthode baséesur un modèle graphique qui calculer le poids des variable d' entrée en utilisantune estimation stochastique . le méthode être incrémentale et produire un classifieurNaïf de Bayes Pondéré pour flux de donnée . ce méthode être comparéeau classique classifieur naïf de Bayes sur le donnée utiliser lors du challenge « Large Scale Learning » . 	Classifieur naïf de Bayes pondéré pour flux de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Clustering de données relationnelles pour la structuration de flux télévisuels	Les approches existantes pour structurer automatiquement un flux detélévision (i.e. reconstituer un guide de programme exact et complet), sont supervisées.Elles requièrent de grandes quantités de données annotées manuellement,et aussi de définir a priori les types d'émissions (publicités, bandes annonces,programmes, sponsors...). Pour éviter ces deux contraintes, nous proposonsune classification non supervisée. La nature multi-relationnelle de nosdonnées proscrit l'utilisation des techniques de clustering habituelles reposantsur des représentations sous forme attributs-valeurs. Nous proposons et validonsexpérimentalement une technique de clustering capable de manipuler ces donnéesen détournant la programmation logique inductive (PLI) pour fonctionnerdans ce cadre non supervisé.	Vincent Claveau, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1001934		137	fr	fr	@irisa.fr, @inria.fr	Clustering de donnée relationnel pour le structuration de flux télévisuels  le approche existant pour structurer automatiquement un flux detélévision ( id est reconstituer un guide de programme exact et complet ) , être superviser . Elles requérir un grand quantité de donnée annoter manuellement , et aussi de définir avoir priori le type d' émission ( publicité , bande annonce , programme , sponsor ... ) . Pour éviter ce deux contrainte , nous proposonsune classification non superviser . le nature multi-relationnelle de nosdonnées proscrire le utilisation des technique de clustering habituel reposantsur des représentation sous forme attributs-valeurs . Nous proposer et validonsexpérimentalement un technique de clustering capable de manipuler ce donnéesen détourner le programmation logique inductif ( pli ) pour fonctionnerdans ce cadre non superviser . 	Clustering de données relationnelles pour la structuration de flux télévisuels	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Clustering de séquences d'évènements temporels	Nous proposons une nouvelle méthode de clustering et d'analyse deséquences temporelles basée sur les modèles en grille à trois dimensions. Lesséquences sont partitionnées en clusters, la dimension temporelle est discrétiséeen intervalles et la dimension évènement est partitionnée en groupes. La grille decellules 3D forme ainsi un estimateur non-paramétrique constant par morceauxde densité jointe des séquences et des dimensions des évènements temporels.Les séquences d'un cluster sont ainsi groupés car elles suivent une distributionsimilaire d'évènements au cours du temps. Nous proposons aussi une méthoded'exploitation du clustering par simplification de la grille ainsi que des indicateurspermettant d'interpréter les clusters et de caractériser les séquences quiles composent. Les expériences sur des données artificielles ainsi que sur desdonnées réelles issues de DBLP démontrent le bien-fondé de notre approche.	Romain Guigourès, Dominique Gay, Marc Boullé, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1001929		138	fr	fr	@orange.com, @zalando.de	Clustering de séquence d' évènements temporels  Nous proposer un nouveau méthode de clustering et d' analyse deséquences temporel baser sur le modèle en grille à trois dimension . Lesséquences être partitionnées en clusters , le dimension temporel être discrétiséeen intervalle et le dimension évènement être partitionnée en groupe . le grille decellules 3D former ainsi un estimateur non- paramétrique constant par morceauxde densité joindre des séquence et des dimension des évènements temporel . le séquence d' un cluster être ainsi grouper car elles suivre un distributionsimilaire d' évènements au cour du temps . Nous proposer aussi un méthoded'exploitation du clustering par simplification de le grille ainsi que un indicateurspermettant d' interpréter le clusters et de caractériser le séquence quiles composer . le expérience sur un donnée artificiel ainsi que sur desdonnées réel issue de DBLP démontrer le bien-fondé de son approche . 	Clustering de séquences d'évènements temporels	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Clusters dans les réseaux sociaux : intersections entre liens conceptuels fréquents et communautés	La recherche de liens conceptuels fréquents (FCL) est une nouvelleapproche de clustering de réseaux, qui exploite à la fois la structure et les attributsdes noeuds. Bien que les travaux récents se soient déjà intéressés à l'optimisationdes algorithmes de recherche des FCL, peu de travaux sont aujourd'huimenés sur la complémentarité qui existe entre les liens conceptuels et l'approcheclassique de clustering qui consiste en l'extraction de communautés. Ainsi dansce papier, nous nous intéressons à ces deux approches. Notre objectif est d'évaluerles relations potentiellement existantes entre les communautés et les FCLpour comprendre la façon dont les motifs obtenus par chacune des méthodespeuvent correspondre ou s'intersecter ainsi que la connaissance utile résultantde la prise en compte de ces deux types de connaissance. Nous proposons pourcela un ensemble de mesures originales, basées sur la notion d'homogénéité, visantà évaluer le niveau d'intersection des FCL et des communautés lorsqu'ilssont extraits d'un même jeu de données. Notre approche est appliquée à deuxréseaux et démontre l'importance de considérer simultanément plusieurs typesde connaissance et leur intersection.	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001920		139	fr	fr	@univ-ag.fr	Clusters dans le réseau social : intersection entre lien conceptuel fréquent et communautés  le recherche de lien conceptuel fréquent ( FCL ) être un nouvelleapproche de clustering de réseau , qui exploiter à le foi le structure et le attributsdes noeud . bien que le travail récent clr être déjà intéresser à le optimisationdes algorithme de recherche des FCL , peu de travail être aujourd' huimenés sur le complémentarité qui exister entre le lien conceptuel et le approcheclassique de clustering qui consister en le extraction de communauté . ainsi dansce papier , nous clr intéresser à ce deux approche . son objectif être d' évaluerles relation potentiellement existant entre le communauté et le FCLpour comprendre le façon dont le motif obtenir par chacun des méthodespeuvent correspondre ou clr intersecter ainsi que le connaissance utile résultantde le prise en compte de ce deux type de connaissance . Nous proposer pourcela un ensemble de mesure original , baser sur le notion d' homogénéité , visantà évaluer le niveau d' intersection des FCL et des communauté lorsqu' ilssont extrait d' un même jeu de donnée . son approche être appliquer à deuxréseaux et démontrer le importance de considérer simultanément plusieurs typesde connaissance et son intersection . 	Clusters dans les réseaux sociaux : intersections entre liens conceptuels fréquents et communautés	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Comment Devenir Cybercondriaque ?	"Nous avons tous déjà eu l'occasion d'effectuer des recherches d'ordremédical sur Internet. Si certains sites spécialisés se refusent à tout diagnosticen ligne, préférant le renvoi vers des professionnels de santé, d'autres en revancheconduisent souvent à des déclarations alarmistes faisant état de situationshumaines difficiles. Dans ce travail, nous étudions l'ampleur de ce phénomèneet montrons que quel que soit le syndrome recherché, les résultats obtenusconduisent toujours à l'énoncé des mots ""cancer"" ou ""tumeur""."	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001986		140	fr	fr	@univ-ag.fr	Comment Devenir Cybercondriaque ?  " Nous avoir tout déjà avoir le occasion d' effectuer un recherche d' ordremédical sur Internet . . Si certain site spécialiser clr refuser à tout diagnosticen ligne , préférer le renvoi vers un professionnel de santé , un autre en revancheconduisent souvent à un déclaration alarmiste faire état de situationshumaines difficile . . Dans ce travail , nous étudier le ampleur de ce phénomèneet montrer que quel que être le syndrome rechercher , le résultat obtenusconduisent toujours à le énoncer des mot " " cancer " " ou " " tumeur " " . " 	Comment Devenir Cybercondriaque ?	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Comparaison de bornes théoriques pour l'accélération du clustering incrémental en une passe	Le clustering incrémental en une passe repose sur l'affectation efficacede chaque nouveau point aux clusters existants. Dans le cas général, où lesclusters ne peuvent être représentés par une moyenne, la détermination exhaustivedu cluster le plus proche possède une complexité quadratique avec le nombrede données. Nous proposons dans ce papier une nouvelle méthode d'affectationstochastique à chaque cluster qui minimise le nombre de comparaisons à effectuerentre la donnée et chaque cluster pour garantir, étant donné un taux d'erreuracceptable, l'affectation au cluster le plus proche. Plusieurs bornes théoriques(Bernstein, Hoeffding et Student) sont comparées dans ce papier. Les résultatssur des données artificielles et réelles montrent que la borne de Bernstein donneglobalement les meilleurs résultats (notamment lorsqu'elle est réduite) car ellepermet une accélération forte du processus de clustering, tout en conservant unnombre très faible d'erreurs.	Nicolas Labroche, Marcin Detyniecki, Thomas Baerecke	http://editions-rnti.fr/render_pdf.php?p1&p=1001959		141	fr	fr	@lip6.fr	comparaison de borne théorique pour le accélération du clustering incrémental en un passe  le clustering incrémental en un passe reposer sur le affectation efficacede chaque nouveau point aux clusters existant . Dans le cas général , où lesclusters ne pouvoir être représenter par un moyenne , le détermination exhaustivedu cluster le plus proche posséder un complexité quadratique avec le nombrede donner . Nous proposer dans ce papier un nouveau méthode d' affectationstochastique à chaque cluster qui minimiser le nombre de comparaison à effectuerentre le donner et chaque cluster pour garantir , être donner un taux d' erreuracceptable , le affectation au cluster le plus proche . plusieurs borne théorique ( Bernstein , Hoeffding et Student ) être comparer dans ce papier . le résultatssur des donnée artificiel et réel montrer que le borner de Bernstein donneglobalement le meilleur résultat ( notamment lorsqu' elle être réduire ) car ellepermet un accélération fort du processus de clustering , tout en conserver unnombre très faible d' erreur . 	Comparaison de bornes théoriques pour l'accélération du clustering incrémental en une passe	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Comparaison des chemins de Hilbert adaptatif et des graphes de voisinage pour la caractérisation d'un parcellaire agricole	Cet article compare deux représentations de données spatiales, lesgraphes de voisinages et les chemins de Hilbert-Peano, utilisées par des algorithmesde fouille. Cette comparaison s'appuie sur la mise en oeuvre d'une méthoded'énumération de « sacs de noeuds », qui permet d'obtenir des caractérisationshomogènes à partir des deux représentations. La méthode est appliquée àla caractérisation de parcellaires agricoles et les résultats tendent à montrer quela linéarisation de l'espace capte la majorité de l'information, à l'exception deséléments rares, sur cet exemple particulier.	Thomas Guyet, Sébastien Da Silva, Claire Lavigne, Florence Le Ber	http://editions-rnti.fr/render_pdf.php?p1&p=1001974		142	fr	fr		comparaison des chemin de Hilbert adaptatif et des graphe de voisinage pour le caractérisation d' un parcellaire agricole  ce article comparer deux représentation de donnée spatial , lesgraphes de voisinage et le chemin de Hilbert-Peano , utiliser par un algorithmesde fouille . ce comparaison clr appuyer sur le mise en oeuvre d' un méthoded'énumération de « sac de noeud » , qui permettre d' obtenir un caractérisationshomogènes à partir un deux représentation . le méthode être appliquer àla caractérisation de parcellaires agricole et le résultat tendre à montrer quela linéarisation de le espace capter le majorité de le information , à le exception deséléments rare , sur ce exemple particulier . 	Comparaison des chemins de Hilbert adaptatif et des graphes de voisinage pour la caractérisation d'un parcellaire agricole	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Compréhension de recettes de cuisine utilisateurs par extraction de connaissances intrinsèques	Sur les sites Web communautaires, les utilisateurs échangent des connaissances,en étant à la fois auteurs et lecteurs. Nous présentons une méthodepour construire notre propre compréhension de la sémantique de la communauté,sans recours à une base de connaissances externe. Nous effectuons une extractionde la connaissance présente dans les contributions analysées. Nous proposonsune évaluation de la confiance imputable à cette compréhension déduite,afin d'évaluer la qualité du contenu, avec application à un site Web de partagede recettes de cuisine.	Damien Leprovost, Thierry Despeyroux, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001984		143	fr	fr	@inria.fr	compréhension de recette de cuisine utilisateur par extraction de connaissance intrinsèques  Sur le site Web communautaire , le utilisateur échanger un connaissance , en être à le foi auteur et lecteur . Nous présenter un méthodepour construire son propre compréhension de le sémantique de le communauté , sans recours à un base de connaissance externe . Nous effectuer un extractionde le connaissance présent dans le contribution analyser . Nous proposonsune évaluation de le confiance imputable à ce compréhension déduire , afin d' évaluer le qualité du contenu , avec application à un site Web de partagede recette de cuisine . 	Compréhension de recettes de cuisine utilisateurs par extraction de connaissances intrinsèques	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Construction de cube OLAP à partir d'un entrepôt de données orienté colonnes	L'optimisation de la construction de cubes OLAP 1 a été jusqu'à présentaxée sur le développement d'algorithmes de calcul performants. Ces derniersopèrent sur des données extraites de l'entrepôt de données qui est généralementimplémenté selon le modèle relationnel qui adopte l'architecture orientéelignes. Or, pour les requêtes décisionnelles, l'architecture orientée colonnes offrede meilleures performances. Cependant, les SGBDR 2 selon cette architecture nedisposent pas d'opérateurs appropriés pour le calcul de cube OLAP. Nous proposonsdans cet article une nouvelle méthode de calcul de cube OLAP. Les résultatsobtenus à partir des expérimentations que nous avons menées démontrentque notre approche optimise considérablement le temps de construction de cubeOLAP et réduit le temps de réponse relatif à l'exploitation du cube comparé àl'approche orientée lignes.	Khaled Dehdou, Fadila Bentayeb, Nadia Kabachi, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001970		144	fr	fr	@univ-lyon2.fr	construction de cube OLAP à partir d' un entrepôt de donnée orienter colonnes  le optimisation de le construction de cube OLAP 1 avoir être jusqu' à présentaxée sur le développement d' algorithme de calcul performant . ce derniersopèrent sur un donnée extraire de le entrepôt de donnée qui être généralementimplémenté selon le modèle relationnel qui adopter le architecture orientéelignes . Or , pour le requête décisionnel , le architecture orienter colonne offrede meilleur performance . cependant , le SGBDR 2 selon ce architecture nedisposent pas un opérateur approprier pour le calcul de cube OLAP . Nous proposonsdans ce article un nouveau méthode de calcul de cube OLAP . le résultatsobtenus à partir un expérimentation que nous avoir mener démontrentque son approche optimiser considérablement le temps de construction de cubeOLAP et réduire le temps de réponse relatif à le exploitation du cube comparer àl'approche orienter ligne . 	Construction de cube OLAP à partir d'un entrepôt de données orienté colonnes	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Construction de profils de préférences contextuelles basée sur l'extraction de motifs séquentiels	L'utilisation de préférences suscite un intérêt croissant pour personnaliserdes réponses et effectuer des recommandations. En amont, l'étape essentielleest l'élicitation des préférences qui consiste à construire un profil depréférences en sollicitant le moins possible l'utilisateur. Dans cet article, nousprésentons une méthode basée sur l'extraction de motifs séquentiels afin de générerdes règles de préférences contextuelles à partir d'une base de paires detransactions. À partir de ces règles générées, qui ont une expressivité plus richeque celle des approches existantes, nous montrons comment construire et utiliserun profil modélisant les préférences de l'utilisateur. De plus, notre approchea l'avantage de bénéficier des nombreux algorithmes efficaces d'extraction deséquences fréquentes. L'évaluation de notre méthode sur des données réellesmontre que les modèles de préférences construits permettent d'effectuer des recommandationsjustes à un utilisateur.	Arnaud Giacometti, Dominique Haoyuan Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001954		145	fr	fr	@univ-tours.fr	construction de profil de préférence contextuel baser sur le extraction de motif séquentiels  le utilisation de préférence susciter un intérêt croissant pour personnaliserdes réponse et effectuer un recommandation . En amont , le étape essentielleest le élicitation des préférence qui consister à construire un profil depréférences en solliciter le moins possible le utilisateur . Dans ce article , nousprésentons un méthode baser sur le extraction de motif séquentiel afin de générerdes règle de préférence contextuel à partir d' un base de paire detransactions . À partir de ce règle générer , qui avoir un expressivité plus richeque celui des approche existant , nous montrer comment construire et utiliserun profil modéliser le préférence de le utilisateur . De plus , son approchea le avantage de bénéficier un nombreux algorithme efficace d' extraction deséquences fréquent . le évaluation de son méthode sur un donnée réellesmontre que le modèle de préférence construire permettre d' effectuer un recommandationsjustes à un utilisateur . 	Construction de profils de préférences contextuelles basée sur l'extraction de motifs séquentiels	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	De l'ombre à la lumière : plus de visibilité sur l'Eclipse	L'extraction de connaissances à partir de données issues du génie logicielest un domaine qui s'est beaucoup développé ces dix dernières années, avecnotamment la fouille de référentiels logiciels (Mining Software Repositories) etl'application de méthodes statistiques (partitionnement, détection d'outliers) àdes thématiques du processus de développement logiciel. Cet article présente ladémarche de fouille de données mise en oeuvre dans le cadre de Polarsys, ungroupe de travail de la fondation Eclipse, de la définition des exigences à la propositiond'un modèle de qualité dédié et à son implémentation sur un prototype.Les principaux concepts adoptés et les leçons tirées sont également passés enrevue.	Boris Baldassari, Flavien Huynh, Philippe Preux	http://editions-rnti.fr/render_pdf.php?p1&p=1001967		146	fr	fr	@squoring.com, @inria.fr	De le ombre à le lumière : plus de visibilité sur le Eclipse  le extraction de connaissance à partir de donnée issu du génie logicielest un domaine qui clr être beaucoup développer ce dix dernier année , avecnotamment le fouille de référentiel logiciel ( Mining Software Repositories ) etl'application de méthode statistique ( partitionnement , détection d' outliers ) àdes thématique du processus de développement logiciel . ce article présent ladémarche de fouille de donnée mettre en oeuvre dans le cadre de Polarsys , ungroupe de travail de le fondation Eclipse , de le définition des exigence à le propositiond'un modèle de qualité dédier et à son implémentation sur un prototype . le principal concept adopter et le leçon tirer être également passer enrevue . 	De l'ombre à la lumière : plus de visibilité sur l'Eclipse	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	De nouvelles pondérations adaptées à la classification de petits volumes de données textuelles	Un des défis actuels dans le domaine de la classification supervisée dedocuments est de pouvoir produire un modèle fiable à partir d'un faible volumede données. Avec un volume conséquent de données, les classifieurs fournissentdes résultats satisfaisants mais les performances sont dégradées lorsque celui-cidiminue. Nous proposons, dans cet article, de nouvelles méthodes de pondérationsrésistant à une diminution du volume de données. Leur efficacité, évaluéeen utilisant des algorithmes de classification supervisés existants (Naive Bayeset Class-Feature-Centroid) sur deux corpus différents, est supérieure à celle desautres algorithmes lorsque le nombre de descripteurs diminue. Nous avons étudiéen parallèle les paramètres influençant les différentes approches telles que lenombre de classes, de documents ou de descripteurs.	Flavien Bouillot, Pascal Poncelet, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001922		147	fr	fr	@lirmm.fr	un nouveau pondération adapter à le classification de petit volume de donnée textuelles  un des défi actuel dans le domaine de le classification superviser dedocuments être de pouvoir produire un modèle fiable à partir d' un faible volumede donner . Avec un volume conséquent de donnée , le classifieur fournissentdes résultat satisfaisant mais le performance être dégrader lorsque celui-cidiminue . Nous proposer , dans ce article , un nouveau méthode de pondérationsrésistant à un diminution du volume de donnée . son efficacité , évaluéeen utiliser un algorithme de classification superviser existant ( Naive Bayeset Class-Feature-Centroid ) sur deux corpus différent , être supérieur à celui desautres algorithme lorsque le nombre de descripteur diminuer . Nous avoir étudiéen parallèle le paramètre influencer le différent approche tel que lenombre de classe , de document ou de descripteur . 	De nouvelles pondérations adaptées à la classification de petits volumes de données textuelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Des humanités au numérique : interdisciplinarité et réciprocité	Les Humanités Numériques, aussi contestable et critiquable que soit le terme, font maintenantpartie du paysage de la recherche en sciences humaines, institutionnalisées par la TrèsGrande Infrastructure de Recherche Huma-Num du CNRS. Elles sont généralement définiescomme la convergence de disciplines autour d'un matériau numérique, matériau inévitablementaccompagné d'un outillage tout aussi numérique. Ce matériau, suivant la discipline quil'observe pourra être considéré comme un objet éditorial, un objet analysable ou un objetcalculable. Nous tenterons de montrer que ce matériau peut aussi être perçu, voire construit,comme un dépôt voire un entrepôt de connaissances. Notre présentation s'appuiera sur diversprojets de recherche en humanités numériques auxquels nous contribuons afin de mettre enexergue le lien qui peut être fait entre extraction et gestion de connaissances d'une part ethumanités numériques d'autre part : le premier peut trouver un terrain expérimental dans lesecond tandis que le second peut tirer profit des méthodes et outils développés par le premier.Nous égrainerons par ailleurs d'autres problématiques inhérentes aux Humanités numériques :de la constitution à l'analyse du corpus en passant par la formalisation et la normalisationdes données. Enfin, nous tenterons de montrer par l'exemple que les questions posées par leshumanités numériques ne sont pas sans rappeler celles des industries de la connaissance.	Thomas Lebarbé	http://editions-rnti.fr/render_pdf.php?p1&p=1001907		148	fr	fr	@me.com	un humanité au numérique : interdisciplinarité et réciprocité  le humanité Numériques , aussi contestable et critiquable que être le terme , faire maintenantpartie du paysage de le recherche en science humain , institutionnaliser par le TrèsGrande infrastructure de recherche Huma-Num du CNRS . Elles être généralement définiescomme le convergence de discipline autour d' un matériau numérique , matériau inévitablementaccompagné d' un outillage tout aussi numérique . ce matériau , suivant le discipline quil'observe pouvoir être considérer comme un objet éditorial , un objet analysable ou un objetcalculable . Nous tenter de montrer que ce matériau pouvoir aussi être percevoir , voire construire , comme un dépôt voire un entrepôt de connaissance . son présentation clr appuyer sur diversprojets de recherche en humanité numérique auxquels nous contribuer afin de mettre enexergue le lien qui pouvoir être faire entre extraction et gestion de connaissance d' un part ethumanités numérique d' autre part : le premier pouvoir trouver un terrain expérimental dans lesecond tandis que le second pouvoir tirer profit des méthode et outil développer par le premier . Nous égrainer par ailleurs d' autre problématique inhérent aux humanité numérique : de le constitution à le analyse du corpus en passer par le formalisation et le normalisationdes donner . enfin , nous tenter de montrer par le exemple que le question poser par leshumanités numérique ne être pas sans rappeler celui des industrie de le connaissance . 	Des humanités au numérique : interdisciplinarité et réciprocité	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection de changements dans des flots de données qualitatives	Pour mieux analyser et extraire de la connaissance de flots de données,des approches spécifiques ont été proposées ces dernières années. L'un deschallenges auquel elles doivent faire face est la détection de changement dansles données. Alors que de plus en plus de données qualitatives sont générées,peu de travaux de recherche se sont intéressés à la détection de changement dansce contexte et les travaux existants se sont principalement focalisés sur la qualitéd'un modèle appris plutôt qu'au réel changement dans les données. Danscet article nous proposons une nouvelle méthode de détection de changementnon supervisée, appelée CDCStream (Change Detection in Categorical DataStreams), adaptée aux flux de données qualitatives.	Dino Ienco, Albert Bifet, Bernhard Pfahringer, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001968		149	fr	fr	@irstea.fr, @lirmm.fr, @yahoo-inc.com, @cs.waikato.ac.nz	détection de changement dans un flot de donnée qualitatives  Pour mieux analyser et extraire de le connaissance de flot de donnée , un approche spécifique avoir être proposer ce dernier année . le un deschallenges auquel elles devoir faire face être le détection de changement dansles donner . alors que de plus en plus de donnée qualitatif être générer , peu de travail de recherche clr être intéresser à le détection de changement dansce contexte et le travail existant clr être principalement focaliser sur le qualitéd'un modèle apprendre plutôt qu' au réel changement dans le donnée . Danscet article nous proposer un nouveau méthode de détection de changementnon superviser , appeler CDCStream ( Change Detection in Categorical DataStreams ) , adapter aux flux de donnée qualitatif . 	Détection de changements dans des flots de données qualitatives	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection de situations à risque basée sur des détecteurs de mouvement à domicile pour les personnes dépendantes	Avec le vieillissement de la population dans les décennies à venir, laprise en charge de la dépendance est devenu un enjeu majeur. Les nouvellestechnologies permettent d'améliorer le confort et la sécurité des personnes dépendantesà domicile. Dans cet article nous proposons une méthode de détectionde situations à risques basée sur le seuillage automatique des intervalles d'inactivitédes capteurs de mouvement de type infrarouge passif. Notre contributionconsiste à apprendre de façon automatique la durée maximale d'inactivité, parpièce et par plage horaire. La méthode est évaluée sur des données réelles provenantde l'activité d'une personne réelle dans un appartement équipé de capteursdomotiques. Notre approche permet de réduire le temps d'appel des secours.	Alban Meffre, Nicolas Lachiche, Pierre Gançarski, Christophe Collet	http://editions-rnti.fr/render_pdf.php?p1&p=1001983		150	fr	fr	@unistra.fr, @unistra.fr, @unistra.fr, @unistra.fr	détection de situation à risque baser sur un détecteur de mouvement à domicile pour le personne dépendantes  Avec le vieillissement de le population dans le décennie à venir , laprise en charge de le dépendance être devenir un enjeu majeur . le nouvellestechnologies permettre d' améliorer le confort et le sécurité des personne dépendantesà domicile . Dans ce article nous proposer un méthode de détectionde situation à risque baser sur le seuillage automatique des intervalle d' inactivitédes capteur de mouvement de type infrarouge passif . son contributionconsiste à apprendre de façon automatique le durée maximal d' inactivité , parpièce et par plage horaire . le méthode être évaluer sur un donnée réel provenantde le activité d' un personne réel dans un appartement équiper de capteursdomotiques . son approche permettre de réduire le temps d' appel des secours . 	Détection de situations à risque basée sur des détecteurs de mouvement à domicile pour les personnes dépendantes	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection d'opinions dans des tweets	Twitter est à l'heure actuelle un des réseau sociaux les plus utilisé aumonde et analyser les opinions qui y sont contenues permet de fournir de précieusesinformations notamment aux entreprises commerciales. Dans cet article,nous décrivons une méthode permettant de déterminer l'opinion d'un tweet endétectant dans un premier temps sa subjectivité, puis sa polarité.	Caroline Collet, Alexandre Pauchet, Laurent Vercouter, Khaled Khelif	http://editions-rnti.fr/render_pdf.php?p1&p=1001964		151	fr	fr		détection d' opinion dans un tweets  twitter être à le heure actuel un des réseau social le plus utiliser aumonde et analyser le opinion qui y être contenir permettre de fournir de précieusesinformations notamment aux entreprise commercial . Dans ce article , nous décrire un méthode permettre de déterminer le opinion d' un tweet endétectant dans un premier temps son subjectivité , puis son polarité . 	Détection d'opinions dans des tweets	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Du texte à la base de données géographiques	Avec la prolifération des données géographiques, il y a un fort besoinde concevoir des outils automatiques pour l'exploitation des connaissances géographiquesincarnées dans les documents textuels. C'est dans ce contexte, quenous proposons une approche permettant de générer une base de données géographiques(BDG) à partir de textes. Notre approche s'articule autour de deuxgrandes phases : la génération du schéma de la BDG et la détermination desdonnées qui serviront au remplissage de cette base. L'implémentation de notreapproche a donné naissance à un outil que nous avons baptisé GDB Generatoret que nous avons intégré dans le SIG : OpenJUMP.	Nesrine Hassini, Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001972		152	fr	fr	@yahoo.fr, @yahoo.fr, @insat.rnu.tn	Du texte à le base de donnée géographiques  Avec le prolifération des donnée géographique , il y avoir un fort besoinde concevoir un outil automatique pour le exploitation des connaissance géographiquesincarnées dans le document textuel . C' être dans ce contexte , quenous proposer un approche permettre de générer un base de donnée géographique ( BDG ) à partir de texte . son approche clr articuler autour de deuxgrandes phase : le génération du schéma de le BDG et le détermination desdonnées qui servir au remplissage de ce base . le implémentation de notreapproche avoir donner naissance à un outil que nous avoir baptiser GDB Generatoret que nous avoir intégrer dans le SIG : OpenJUMP . 	Du texte à la base de données géographiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Dynamique des communautés par prédiction d'interactions dans les réseaux sociaux	Dans cet article, nous proposons une approche générale de prédictiondes communautés basée sur un modèle d'apprentissage automatique pour la prédictiondes interactions. En effet, nous pensons que, si on peut prédire avec précisionla structure du réseau, alors on a juste à rechercher les communautés surle réseau prédit. Des expérimentations sur des jeux de données réels montrent lafaisabilité de cette approche.	Blaise Ngonmang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001977		153	fr	fr	@univ-paris13.fr	dynamique des communauté par prédiction d' interaction dans le réseau sociaux  Dans ce article , nous proposer un approche général de prédictiondes communauté baser sur un modèle d' apprentissage automatique pour le prédictiondes interaction . En effet , nous penser que , si on pouvoir prédire avec précisionla structure du réseau , alors on avoir juste à rechercher le communauté surle réseau prédire . un expérimentation sur un jeu de donnée réel montrer lafaisabilité de ce approche . 	Dynamique des communautés par prédiction d'interactions dans les réseaux sociaux	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Evaluation de la pertinence dans un système de recommandation sémantique de nouvelles économiques	De nos jours dans les secteurs commerciaux et financiers la veille estcruciale et complexe, car la charge d'informations est importante. Pour répondreà cette problématique, nous proposons un système novateur de recommandationd'articles basé sur une modélisation ontologique des connaissances. Nous présentonségalement une nouvelle méthode d'évaluation de la pertinence utilisantle modèle vectoriel intrinsèquement efficace et adapté afin de pallier la confusionnative de ces modèles entre les notions de similarité et de pertinence.	David Werner, Christophe Cruz, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001976		154	fr	fr	@u-bourgogne.fr	Evaluation de le pertinence dans un système de recommandation sémantique de nouveau économiques  De son jour dans le secteur commercial et financier le veille estcruciale et complexe , car le charge d' information être important . Pour répondreà ce problématique , nous proposer un système novateur de recommandationd'articles baser sur un modélisation ontologique des connaissance . Nous présentonségalement un nouveau méthode d' évaluation de le pertinence utilisantle modèle vectoriel intrinsèquement efficace et adapter afin de pallier le confusionnative de ce modèle entre le notion de similarité et de pertinence . 	Evaluation de la pertinence dans un système de recommandation sémantique de nouvelles économiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Exploration d'une collection de chansons à partir d'une interface de visualisation basée sur une analyse des paroles	Dans cet article, nous présentons une approche de fouille de textesainsi qu'une interface de visualisation afin d'explorer une large collection dechansons frana¸ises à partir des paroles. Dans un premier temps, nous collectonsparoles et métadonnées de différentes sources sur leWeb. Nous utilisons une approchecombinant clustering et analyse sémantique latente afin d'identifier différentesthématiques et de déterminer différents descripteurs significatifs. Noustransformons par la suite le modèle afin d'obtenir une visualisation interactivepermettant d'explorer la collection de chansons	Rémy Kessler, Audrey Laplante, Dominic Forest	http://editions-rnti.fr/render_pdf.php?p1&p=1001946		155	fr	fr	@umontreal.ca	exploration d' un collection de chanson à partir d' un interface de visualisation baser sur un analyse des paroles  Dans ce article , nous présenter un approche de fouille de textesainsi qu' un interface de visualisation afin d' explorer un large collection dechansons frana¸ises à partir un parole . Dans un premier temps , lui collectonsparoles et métadonnées de différent source sur leWeb . Nous utiliser un approchecombinant clustering et analyse sémantique latent afin d' identifier différentesthématiques et de déterminer différents descripteur significatif . Noustransformons par le suite le modèle afin d' obtenir un visualisation interactivepermettant d' explorer le collection de chansons 	Exploration d'une collection de chansons à partir d'une interface de visualisation basée sur une analyse des paroles	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extension de l'étiquetage géographique des pixels d'une image par fouille de données	Les techniques de classification modernes permettent d'étiqueter leszones non couvertes des bases de données cartographiques, mais souffrent d'unmanque de robustesse important. Dans cet article, nous proposons une méthoderobuste d'extension d'étiquetage sur l'emprise d'une image satellite, par analysehiérarchique des données existantes. Notre approche est fondée sur une sélectiond'attributs par thème de la base de données, une sélection des pixels d'apprentissageet des classifications par objet de chaque thème. La décision finaled'étiquetage est prise après fusion des classifications par thème. Notre méthodeest appliquée avec succès et comparée à plusieurs méthodes de classification,couplant données d'occupation du sol et imagerie spatiale très haute résolution.	Adrien Gressin, Nicole Vincent, Clément Mallet, Nicolas Paparoditis	http://editions-rnti.fr/render_pdf.php?p1&p=1001966		156	fr	fr	@ign.fr, @parisdescartes.fr	extension de le étiquetage géographique des pixel d' un image par fouille de données  le technique de classification moderne permettre d' étiqueter leszones non couvert des base de donnée cartographique , mais souffrir d' unmanque de robustesse important . Dans ce article , nous proposer un méthoderobuste d' extension d' étiquetage sur le emprise d' un image satellite , par analysehiérarchique des donnée existant . son approche être fonder sur un sélectiond'attributs par thème de le base de donnée , un sélection des pixel d' apprentissageet des classification par objet de chaque thème . le décision finaled'étiquetage être prendre après fusion des classification par thème . son méthodeest appliquer avec succès et comparer à plusieurs méthode de classification , coupler donner d' occupation du sol et imagerie spatial très haut résolution . 	Extension de l'étiquetage géographique des pixels d'une image par fouille de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraction de motifs dans des graphes orientés attribués en présence d'automorphisme	Les graphes orientés attribués sont des graphes orientés dans lesquelsles noeuds sont associés à un ensemble d'attributs. De nombreuses données, issuesdu monde réel, peuvent être représentées par ce type de structure, maisencore peu d'algorithmes sont capables de les traiter directement. La fouille desgraphes attribués est difficile, car elle nécessite de combiner l'exploration de lastructure du graphe avec l'identification d'itemsets fréquents. De plus, du fait del'explosion combinatoire des itemsets, les isomorphismes de sous-graphes, dontla présence impacte énormément les performances des algorithmes de fouille,sont beaucoup plus nombreux que dans les graphes étiquetés.Dans cet article, nous présentons une nouvelle méthode de fouille de donnéesqui permet d'extraire des motifs fréquents à partir d'un ou de plusieurs graphesorientés attribués. Nous montrons comment réduire l'explosion combinatoireprovoquée par les isomorphismes de sous-graphes en traitant de manière particulièreles motifs automorphes.	Claude Pasquier, Frédéric Flouvat, Jérémy Sanhes, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001949		157	fr	fr	@univ-nc.nc, @unice.fr	extraction de motif dans un graphe orienter attribuer en présence d' automorphisme  le graphe orienter attribuer être un graphe orienter dans lesquelsles noeud être associer à un ensemble d' attribut . un nombreux donnée , issuesdu monde réel , pouvoir être représenter par ce type de structure , maisencore peu d' algorithme être capable de les traiter directement . le fouille desgraphes attribuer être difficile , car elle nécessiter de combiner le exploration de lastructure du graphe avec le identification d' itemsets fréquent . De plus , du fait del'explosion combinatoire des itemsets , le isomorphisme de sous-graphes , dontla présence impacter énormément le performance des algorithme de fouille , être beaucoup plus nombreux que dans le graphe étiqueter . Dans ce article , nous présenter un nouveau méthode de fouille de donnéesqui permettre d' extraire un motif fréquent à partir d' un ou de plusieurs graphesorientés attribuer . Nous montrer comment réduire le explosion combinatoireprovoquée par le isomorphisme de sous-graphes en traiter de manière particulièreles motif automorphes . 	Extraction de motifs dans des graphes orientés attribués en présence d'automorphisme	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraction de règles d'épisodes minimales dans des séquences complexes	Les messages déposés quotidiennement sur les réseaux sociaux et lesblogs sont très nombreux et constituent une source d'informations précieuse.Leur fouille peut être utilisée dans un but de prédiction d'informations. Notreobjectif dans cet article est de proposer un algorithme permettant la prédictiond'informations au plus tôt et de façon fiable, par le biais de l'identification derègles d'épisodes.	Lina Fahed, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001975		158	fr	fr	@loria.fr	extraction de règle d' épisode minimal dans un séquence complexes  le message déposer quotidiennement sur le réseau social et lesblogs être très nombreux et constituer un source d' information précieux . son fouille pouvoir être utiliser dans un but de prédiction d' information . Notreobjectif dans ce article être de proposer un algorithme permettre le prédictiond'informations au plus tôt et de façon fiable , par le biais de le identification derègles d' épisode . 	Extraction de règles d'épisodes minimales dans des séquences complexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraire les motifs minimaux efficacement et en profondeur	Les représentations condensées ont fait l'objet de nombreux travauxdepuis 15 ans. Tandis que les motifs maximaux des classes d'équivalence ontreçu beaucoup d'attention, les motifs minimaux sont restés dans l'ombre notammentà cause de la difficulté de leur extraction. Dans ce papier, nous présentonsun cadre générique concernant l'extraction de motifs minimaux en introduisantla notion de système minimisable d'ensembles. Il permet de considérer des langagesvariés comme les motifs ensemblistes ou les chaînes de caractères, maisaussi différentes métriques dont la fréquence. Ensuite, pour n'importe quel systèmeminimisable d'ensembles, nous introduisons un test de minimalité rapidepermettant d'extraire en profondeur les motifs minimaux. Nous démontrons quel'algorithme proposé est polynomial-delay et polynomial-space. Des expérimentationssur les benchmarks traditionnels complètent notre étude.	Arnaud Soulet, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001950		159	fr	fr	@univ-tours.fr, @unicaen.fr	extraire le motif minimal efficacement et en profondeur  le représentation condenser avoir faire le objet de nombreux travauxdepuis 15 an . Tandis que le motif maximal des classe d' équivalence ontreçu beaucoup d' attention , le motif minimal être rester dans le ombre notammentà cause de le difficulté de son extraction . Dans ce papier , nous présentonsun cadre générique concernant le extraction de motif minimal en introduisantla notion de système minimisable d' ensemble . Il permettre de considérer un langagesvariés comme le motif ensembliste ou le chaîne de caractère , maisaussi différent métrique dont le fréquence . ensuite , pour n' importer quel systèmeminimisable d' ensemble , nous introduire un test de minimalité rapidepermettant d' extraire en profondeur le motif minimal . Nous démontrer quel'algorithme proposer être polynomial-delay et polynomial-space . un expérimentationssur le benchmark traditionnel compléter son étude . 	Extraire les motifs minimaux efficacement et en profondeur	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Fouille de données par programmation visuelle structurée avec KD-Ariane	Nous présentons ici la plate-forme KD-Ariane, un déploiement d'outilspour la fouille de données dans l'environnement de programmation visuelleAriane. Ce déploiement facilite la conception de chaînes structurées de traitementspour l'extraction de connaissance dans les données	Régis Clouard, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001989		160	fr	fr	@ensicaen.fr, @unicaen.fr	fouille de donnée par programmation visuel structurer avec KD-Ariane  Nous présenter ici le plate-forme KD-Ariane , un déploiement d' outilspour le fouille de donnée dans le environnement de programmation visuelleAriane . ce déploiement faciliter le conception de chaîne structurer de traitementspour le extraction de connaissance dans le données 	Fouille de données par programmation visuelle structurée avec KD-Ariane	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Fouille de motifs séquentiels pour l'élicitation de stratégies à partir de traces d'interactions entre agents en compétition	Pour atteindre un but, tout agent en compétition élabore inévitablementdes stratégies. Lorsque l'on dispose d'une certaine quantité de traces d'interactionsentre agents, il est naturel d'utiliser la fouille de motifs séquentielspour découvrir de manière automatique ces stratégies. Dans cet article, nous proposonsune méthodologie qui permet l'élicitation de stratégies et leur capacité àdiscriminer une réussite ou un échec. La méthodologie s'articule en trois étapes :(i) les traces brutes sont transformées en une base de séquences selon des choixqui permettent, (ii) l'extraction de stratégies fréquentes, (iii) lesquelles sont muniesd'une mesure originale d'émergence. C'est donc une méthodologie de découvertede connaissances que nous proposons. Nous montrons l'intérêt des motifsextraits et la faisabilité de l'approche à travers des expérimentations quantitativeset qualitatives sur des données réelles issues du domaine émergent dusport électronique.	Guillaume Bosc, Mehdi Kaytoue-Uberall, Chedy Raïssi, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001948		161	fr	fr	@insa-lyon.fr, @inria.fr	fouille de motif séquentiel pour le élicitation de stratégie à partir de trace d' interaction entre agent en compétition  Pour atteindre un but , tout agent en compétition élaborer inévitablementdes stratégie . Lorsque le on disposer d' un certain quantité de trace d' interactionsentre agent , il être naturel d' utiliser le fouille de motif séquentielspour découvrir de manière automatique ce stratégie . Dans ce article , lui proposonsune méthodologie qui permettre le élicitation de stratégie et son capacité àdiscriminer un réussite ou un échec . le méthodologie clr articuler en trois étape : ( i ) le trace brut être transformer en un base de séquence selon un choixqui permettre , ( ii ) le extraction de stratégie fréquent , ( iii ) lesquelles être muniesd'une mesure original d' émergence . C' être donc un méthodologie de découvertede connaissance que nous proposer . Nous montrer le intérêt des motifsextraits et le faisabilité de le approche à travers des expérimentation quantitativeset qualitatif sur un donnée réel issir du domaine émerger dusport électronique . 	Fouille de motifs séquentiels pour l'élicitation de stratégies à partir de traces d'interactions entre agents en compétition	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Généralisation des k-moyennes pour produire des recouvrements ajustables	La recherche de groupes non-disjoints à partir de données non-étiquetéesest une problématique importante en classification non-supervisée. Laclassification recouvrante (Overlapping clustering) contribue à la résolution deplusieurs problèmes réels qui nécessitent la détermination de groupes qui se chevauchent.Cependant, bien que les recouvrements entre groupes soient tolérésvoire encouragés dans ces applications, il convient de contrôler leur importance.Nous proposons dans ce papier des généralisations de k-moyennes offrant lecontrôle et le paramétrage des recouvrements. Deux principes de régulation sontmis en place, ils visent à contrôler les recouvrements relativement à leur tailleet à la dispersion des classes. Les expérimentations réalisées sur des jeux dedonnées réelles, montrent l'intérêt des principes proposés.	Chiheb-Eddine Ben N'Cir, Guillaume Cleuziou, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001932		162	fr	fr	@isg.rnu.tn, @isg.rnu.tn, @univ-orleans.fr	généralisation des k-moyennes pour produire un recouvrement ajustables  le recherche de groupe non- disjoindre à partir de donnée non- étiquetéesest un problématique important en classification non- superviser . Laclassification recouvrante ( Overlapping clustering ) contribuer à le résolution deplusieurs problème réel qui nécessiter le détermination de groupe qui clr chevaucher . cependant , bien que le recouvrement entre groupe être tolérésvoire encourager dans ce application , il convenir de contrôler son importance . Nous proposer dans ce papier des généralisation de k-moyennes offrir lecontrôle et le paramétrage des recouvrement . Deux principe de régulation sontmis en place , ils viser à contrôler le recouvrement relativement à son tailleet à le dispersion des classe . le expérimentation réaliser sur un jeu dedonnées réel , montrer le intérêt des principe proposer . 	Généralisation des k-moyennes pour produire des recouvrements ajustables	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Génération d'un extrait textuel à partir de bases de données	Dans ce papier, nous présentons une approche dédiée à la transformationd'une base de données en un extrait textuel. L'idée sous-jacente à notreproposition est d'apporter plus de sémantique aux données de la base. Cet objectifest atteint moyennant l'utilisation des ontologies comme ressources sémantiques.Notre approche prend comme input un ensemble de bases de donnéeset associe à chacune une ontologie. Une ontologie globale est générée, à partirde laquelle des règles d'association sont proposées pour mieux expliciter sasémantique. Enfin, la génération d'un extrait textuel prend lieu.	Ghada Landoulsi, Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001971		163	fr	fr	@yahoo.fr, @laposte.net, @insat.rnu.tn	génération d' un extraire textuel à partir de base de données  Dans ce papier , nous présenter un approche dédier à le transformationd'une base de donnée en un extraire textuel . le idée sous-jacent à notreproposition être d' apporter plus un sémantique aux donnée de le base . ce objectifest atteindre moyennant le utilisation des ontologie comme ressource sémantique . son approche prendre comme input un ensemble de base de donnéeset associer à chacun un ontologie . un ontologie global être générer , à partirde laquelle un règle d' association être proposer pour mieux expliciter sasémantique . enfin , le génération d' un extraire textuel prendre lieu . 	Génération d'un extrait textuel à partir de bases de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Granularité des motifs de co-variations dans des graphes attribués dynamiques	Découvrir des connaissances dans des graphes qui sont dynamiqueset dont les sommets sont attribués est de plus en plus étudié, par exemple dansle contexte de l'analyse d'interactions sociales. Il est souvent possible d'expliciterdes hiérarchies sur les attributs permettant de formaliser des connaissancesa priori sur les descriptions des sommets. Nous proposons d'étendre destechniques de fouille sous contraintes récemment proposées pour l'analyse degraphes attribués dynamiques lorsque l'on exploite de telles hiérarchies et doncle potentiel de généralisation/spécialisation qu'elles permettent. Nous décrivonsun algorithme qui calcule des motifs de co-évolution multi-niveaux, c'est-à-diredes ensembles de sommets qui satisfont une contrainte topologique et qui évoluentde la même façon selon un ensemble de tendances et de pas de temps. Nosexpérimentations montrent que l'utilisation d'une hiérarchie permet d'extrairedes collections de motifs plus concises sans perdre d'information.	Elise Desmier, Marc Plantevit, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001955		164	fr	fr		granularité des motif de co- variation dans un graphe attribuer dynamiques  découvrir un connaissance dans un graphe qui être dynamiqueset dont le sommet être attribuer être de plus en plus étudier , par exemple dansle contexte de le analyse d' interaction social . Il être souvent possible d' expliciterdes hiérarchie sur le attribut permettre de formaliser un connaissancesa priori sur le description des sommet . Nous proposer d' étendre destechniques de fouille sous contrainte récemment proposer pour le analyse degraphes attribuer dynamique lorsque le on exploiter de tel hiérarchie et doncle potentiel de généralisation  spécialisation qu' elles permettre . Nous décrivonsun algorithme qui calculer un motif de co- évolution multi-niveaux , c' est-à-diredes ensemble de sommet qui satisfaire un contrainte topologique et qui évoluentde le même façon selon un ensemble de tendance et de pas de temps . Nosexpérimentations montrer que le utilisation d' un hiérarchie permettre d' extrairedes collection de motif plus concis sans perdre d' information . 	Granularité des motifs de co-variations dans des graphes attribués dynamiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Identification de classes non-disjointes ayants des densités différentes	La classification recouvrante correspond à un enjeu important en classificationnon-supervisée en permettant à une observation d'appartenir à plusieursclusters. Plusieurs méthodes ont été proposées pour faire face à cetteproblématique en utilisant plusieurs approches usuelles de classification. Cependant,malgré l'efficacité de ces méthodes à déterminer des groupes non-disjoints,elles échouent lorsque les données comportent des groupes de densités différentescar elles ignorent la densité locale de chaque groupe et ne considèrentque la distance Euclidienne entres les observations. Afin de détecter des groupesnon-disjoints de densités différentes, nous proposons deux méthodes de classificationintégrant la variation de densité des différentes classes dans le processusde classification. Des expériences réalisées sur des ensembles de données artificiellesmontrent que les méthodes proposées permettent d'obtenir de meilleuresperformances lorsque les données contiennent des groupes de densités différentes.	Hela Masmoudi, Chiheb-Eddine Ben N&#146;Cir, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001936		165	fr	fr	@gmail.com, @isg.rnu.tn, @isg.rnu.tn	identification de classe non- disjoindre ayants des densité différentes  le classification recouvrante correspondre à un enjeu important en classificationnon- superviser en permettre à un observation d' appartenir à plusieursclusters . plusieurs méthode avoir être proposer pour faire face à cetteproblématique en utiliser plusieurs approche usuel de classification . cependant , malgré le efficacité de ce méthode à déterminer un groupe non- disjoindre , elles échouer lorsque le donnée comporter un groupe de densité différentescar lui ignorer le densité local de chaque groupe et ne considèrentque le distance Euclidienne entrer le observation . Afin de détecter un groupesnon- disjoindre de densité différent , nous proposer deux méthode de classificationintégrant le variation de densité des différent classe dans le processusde classification . un expérience réaliser sur un ensemble de donnée artificiellesmontrent que le méthode proposer permettre d' obtenir de meilleuresperformances lorsque le donnée contenir un groupe de densité différent . 	Identification de classes non-disjointes ayants des densités différentes	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Identification de rôles communautaires dans des réseaux orientés appliquée à Twitter	La notion de structure de communautés est particulièrement utile pourétudier les réseaux complexes, car elle amène un niveau d'analyse intermédiaire,par opposition aux plus classiques niveaux local (voisinage des noeuds) et global(réseau entier). Le concept de rôle communautaire permet de décrire le positionnementd'un noeud en fonction de sa connectivité communautaire. Cependant,les approches existantes sont restreintes aux réseaux non-orientés, utilisentdes mesures topologiques ne considérant pas tous les aspects de la connectivitécommunautaire, et des méthodes d'identification des rôles non-généralisables àtous les réseaux. Nous proposons de résoudre ces problèmes en généralisant lesmesures existantes, et en utilisant une méthode non-supervisée pour déterminerles rôles. Nous illustrons l'intérêt de notre méthode en l'appliquant au réseaude Twitter. Nous montrons que nos modifications mettent en évidence les rôlesspécifiques d'utilisateurs particuliers du réseau, nommés capitalistes sociaux.	Nicolas Dugué, Vincent Labatut, Anthony Perez	http://editions-rnti.fr/render_pdf.php?p1&p=1001921		166	fr	fr		identification de rôle communautaire dans un réseau orienter appliquer à Twitter  le notion de structure de communauté être particulièrement utile pourétudier le réseau complexe , car elle amener un niveau d' analyse intermédiaire , par opposition aux plus classique niveau local ( voisinage des noeud ) et global ( réseau entier ) . le concept de rôle communautaire permettre de décrire le positionnementd'un noeud en fonction de son connectivité communautaire . cependant , le approche existant être restreindre aux réseau non- orienter , utilisentdes mesure topologique ne considérer pas tout le aspect de le connectivitécommunautaire , et des méthode d' identification des rôle non- généralisable àtous le réseau . Nous proposer de résoudre ce problème en généraliser lesmesures existant , et en utiliser un méthode non- superviser pour déterminerles rôle . Nous illustrer le intérêt de son méthode en le appliquer au réseaude Twitter . Nous montrer que son modification mettre en évidence le rôlesspécifiques d' utilisateur particulier du réseau , nommer capitaliste social . 	Identification de rôles communautaires dans des réseaux orientés appliquée à Twitter	2
Revue des Nouvelles Technologies de l'Information	EGC	2014	Incremental learning with latent factor models for attribute prediction in social-attribute networks	Dans ce travail, nous nous intéressons au problème de la prédiction d'attributs sur lesnoeuds dans un réseau social. La plupart des techniques sont hors ligne et ne sont pas adaptéesà des situations où les données arrivent massivement en flux comme dans le cas des médiassociaux. Dans ce travail, nous utilisons les modèles de variables latentes pour prédire les attributsinconnus des noeuds dans un réseau social et proposer une méthode pour mettre à jourincrémentalement le modèle avec des nouvelles données. Des expérimentations sur un jeu dedonnées issues des médias sociaux montrent que notre méthode est moins coûteuse en tempsde calcul et peut garantir des performances acceptables en comparaison avec les techniquesnon-incrémentales de l'état de l'art.	Duc Kinh Le Tran, Cécile Bothorel, Pascal Cheung Mon Chan	http://editions-rnti.fr/render_pdf.php?p1&p=1001916		167	en	fr	@telecom-bretagne.eu, @orange.com	Dans ce travail , nous clr intéresser au problème de le prédiction d' attribut sur lesnoeuds dans un réseau social . le plupart des technique être hors ligne et ne être pas adaptéesà un situation où le donnée arriver massivement en flux comme dans le cas des médiassociaux . Dans ce travail , nous utiliser le modèle de variable latent pour prédire le attributsinconnus des noeud dans un réseau social et proposer un méthode pour mettre à jourincrémentalement le modèle avec un nouveau donnée . un expérimentation sur un jeu dedonnées issue des média social montrer que son méthode être moins coûteux en tempsde calcul et pouvoir garantir un performance acceptable en comparaison avec le techniquesnon- incrémentales de le état de le art . 	Incremental learning with latent factor models for attribute prediction in social-attribute networks	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Intégration de plusieurs formes de représentations spatiales dans un modèle de simulation	In this paper, we focus on modeling expert knowledge for simulating complex landscapespatial dynamics. One modeling tool to do that is the Ocelet modeling language that usesinteraction graphs to describe spatial dynamics. Most present approaches impose an a priorichoice of spatial format between: (i) a vector format representing the shapes of the entities, or(ii) a gridding of space into regular elements (raster). In this paper we show how Ocelet wasextended to support the interaction semantics between these two spatial formats (vector andraster). As case study, we present a runoff model in a tropical insular environment.	Mathieu Castets, Pascal Degenne, Danny Lo Seen, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001988		168	fr	en	@teledetection.fr, @lirmm.fr	intégration de plusieurs forme de représentation spatial dans un modèle de simulation 	Intégration de plusieurs formes de représentations spatiales dans un modèle de simulation	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Intégration et visualisation de données liées thématiques sur un référentiel géographique	De nombreuses ressources publiées sur le Web des données sont décritespar une composante qui désigne d'une manière directe ou indirecte unelocalisation géographique. Comme toute autre propriété, cette information delocalisation peut être mise à profit pour permettre l'interconnexion des donnéesavec d'autres sources. Elle permet en outre leur représentation cartographique.Cependant, les informations de localisation utilisées dans les sources de donnéeslinked data peuvent parfois s'avérer imprécises ou hétérogènes d'une source àl'autre. Ceci rend donc leur exploitation pour réaliser une interconnexion difficile,voire impossible. Dans cet article, nous proposons de pallier ces difficultésen ancrant les données linked data thématiques aux objets d'un référentielgéographique. Nous mettons à profit le référentiel géographique afin de mettreen correspondance des données thématiques dotées d'indications de localisationhétérogènes. Nous exploitons enfin les relations de correspondance créées entredonnées thématiques et référentiel géographique dans une application de visualisationcartographique des données.	Abdelfettah Feliachi, Nathalie Abadie, Fayçal Hamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1001912		169	fr	fr	@ign.fr, @ign.fr, @cnam.fr	intégration et visualisation de donnée lier thématique sur un référentiel géographique  un nombreux ressource publier sur le Web des donnée être décritespar un composante qui désigner d' un manière direct ou indirect unelocalisation géographique . Comme tout autre propriété , ce information delocalisation pouvoir être mettre à profit pour permettre le interconnexion des donnéesavec d' autre source . Elle permettre en outre son représentation cartographique . cependant , le information de localisation utiliser dans le source de donnéeslinked dater pouvoir parfois clr avérer imprécis ou hétérogène d' un source àl'autre . ceci rendre donc son exploitation pour réaliser un interconnexion difficile , voire impossible . Dans ce article , nous proposer de pallier ce difficultésen ancrer le donnée linked dater thématique aux objet d' un référentielgéographique . Nous mettre à profit le référentiel géographique afin de mettreen correspondance des donnée thématique doter d' indication de localisationhétérogènes . Nous exploiter enfin le relation de correspondance créer entredonnées thématique et référentiel géographique dans un application de visualisationcartographique des donnée . 	Intégration et visualisation de données liées thématiques sur un référentiel géographique	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Investigation visuelle d'événements dans un grand flot de liens	Nous présentons une nouvelle méthode d'analyse exploratoirede grands flots de liens que nous appliquons à la détection d'événementssignificatifs dans plus de 2 millions d'interactions (pendant 4 mois) entreutilisateurs du réseau social en ligne Github. Nous combinons une méthodestatistique de détection automatique d'événements dans une série temporelle,Outskewer, avec un système de visualisation de graphes. Outskewer identifiedes instants de l'évolution du graphe d'interactions méritant d'être étudiés, etun analyste peut valider et interpréter ces événements par la visualisation demotifs anormaux dans les sous-graphes correspondants. Nous montrons par demultiples exemples que cette approche 1) permet de détecter des événementspertinents et de rejeter ceux qui ne le sont pas, 2) est adaptée à une démarcheexploratoire car elle ne nécessite pas de connaissance a priori sur les données.	Sébastien Heymann, Bénédicte Le Grand	http://editions-rnti.fr/render_pdf.php?p1&p=1001918		170	fr	fr	@lip6.fr, @univ-paris1.fr	investigation visuel d' événement dans un grand flot de liens  Nous présenter un nouveau méthode d' analyse exploratoirede grand flot de lien que nous appliquer à le détection d' événementssignificatifs dans plus de 2 million d' interaction ( pendant 4 moi ) entreutilisateurs du réseau social en ligne Github . Nous combiner un méthodestatistique de détection automatique d' événement dans un série temporel , Outskewer , avec un système de visualisation de graphe . Outskewer identifiedes instant de le évolution du graphe d' interaction méritant d' être étudier , etun analyste pouvoir valider et interpréter ce événement par le visualisation demotifs anormal dans le sous-graphes correspondant . Nous montrer par demultiples exemple que ce approche 1 ) permettre de détecter un événementspertinents et de rejeter celui qui ne le être pas , 2 ) être adapter à un démarcheexploratoire car elle ne nécessiter pas un connaissance avoir priori sur le donnée . 	Investigation visuelle d'événements dans un grand flot de liens	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	La subjectivité dans le discours médical : sur les traces de l'incertitude et des émotions	Les acteurs et usagers du domaine médical (médecins, infirmiers, patients,internes, pharmaciens, etc.) ne sont pas issus de la même catégorie socioprofessionnelleet ne présentent pas le même niveau de maîtrise du domaine.Leurs écrits en témoignent et véhiculent, de plus, la subjectivité qui leur estpropre. Nous nous intéressons à l'étude automatisée de la subjectivité dans lediscours médical dans des textes en langue française. Nous confrontons le discoursdes médecins (articles scientifiques, rapports cliniques) à celui des patients(messages de forums de santé) en analysant contrastivement les différencesd'emploi des descripteurs tels que les marqueurs d'incertitude et de polarité,les marques émotives non lexicales (smileys, ponctuations répétées, etc.)et lexicales, et les termes médicaux relatifs aux pathologies, traitements et procédures.Nous effectuons une annotation et catégorisation automatiques des documentsafin de mieux observer les spécificités que présentent les discours médicauxciblés.	Pierre Chauveau Thoumelin, Natalia Grabar	http://editions-rnti.fr/render_pdf.php?p1&p=1001958		171	fr	fr	@gmail.com, @univ-lille3.fr	le subjectivité dans le discours médical : sur le trace de le incertitude et des émotions  le acteur et usager du domaine médical ( médecin , infirmier , patient , interne , pharmacien , etc. ) ne être pas issir de le même catégorie socioprofessionnelleet ne présenter pas le même niveau de maîtrise du domaine . son écrit en témoigner et véhiculer , de plus , le subjectivité qui leur estpropre . Nous nous intéresser à le étude automatiser de le subjectivité dans lediscours médical dans un texte en langue français . Nous confronter le discoursdes médecin ( article scientifique , rapport clinique ) à celui des patient ( message de forum de santé ) en analyser contrastivement le différencesd'emploi des descripteur tel que le marqueur d' incertitude et de polarité , le marque émotif non lexical ( smiley , ponctuation répéter , etc. ) et lexical , et le terme médical relatif aux pathologie , traitement et procédure . Nous effectuer un annotation et catégorisation automatique des documentsafin de mieux observer le spécificité que présenter le discours médicauxciblés . 	La subjectivité dans le discours médical : sur les traces de l'incertitude et des émotions	2
Revue des Nouvelles Technologies de l'Information	EGC	2014	Les nouvelles théories de l'incertain	La notion d'incertitude a été longtemps un sujet de controverses. En particulier la prééminencede la théorie des probabilités dans les sciences tend à gommer les différences présentesdans les premières tentatives de formalisation, remontant au 17ème siècle, entre l'incertitudedue à la variabilité des phénomènes répétables et l'incertitude due au manque d'information(dite épistémique). L'école Bayésienne affirme que quelle que soit l'origine de l'incertitude,celle-ci peut être modélisée par une distribution de probabilité unique. Cette affirmation a étébeaucoup remise en cause dans les trente dernières années. En effet l'emploi systématiqued'une distribution unique en cas d'information partielle mène à des utilisations paradoxales dela théorie des probabilités.Dans de nombreux domaines, il est crucial de distinguer entre l'incertitude due à la variabilitéd'observations et l'incertitude due à l'ignorance partielle. Cette dernière peut être réduitepar l'obtention de nouvelles informations, mais pas la première, dont on ne se prémunit quepar des actions concrètes. Dans le cas des bases de données, il est souvent supposé qu'ellessont précises, et l'incertitude correspondante est souvent négligée. Quant elle est abordée onreste souvent dans une approche probabiliste orthodoxe.Néanmoins, les statisticiens ont développé des outils qui ne relèvent pas de la théorie deKolmogorov pour pallier le manque de données (intervalles de confiance, principe de maximumde vraisemblance...).De nouvelles théories de l'incertain ont émergé, qui offrent la possibilité de représenter lesincertitudes épistémiques et aléatoires de façon distincte, notamment l'incertitude épistémique,en remplaçant la distribution de probabilité unique par une famille de distributions possibles,cette famille étant d'autant plus grande que l'information est absente. Cette représentationcomplexe possède des cas particuliers plus simples à utiliser en pratique, comme les ensemblesaléatoires (théorie des fonctions de croyance), les distributions de possibilité (représentant desensembles flous de valeurs possibles) et les p-boxes, notamment.Le but de cet exposé est de susciter l'intérêt pour ces nouvelles théories de l'incertain,d'en donner les bases formelles, d'en discuter la philosophie sous-jacente, de faire le lien aveccertaines notions en statistique, et de les illustrer sur des exemples.	Didier Dubois	http://editions-rnti.fr/render_pdf.php?p1&p=1001906		172	fr	fr	@irit.fr	le nouveau théorie de le incertain  le notion d' incertitude avoir être longtemps un sujet de controverse . En particulier le prééminencede le théorie des probabilité dans le science tendre à gommer le différence présentesdans le premier tentative de formalisation , remonter au 17ème siècle , entre le incertitudedue à le variabilité des phénomène répétables et le incertitude devoir au manque d' information ( dire épistémique ) . le école Bayésienne affirmer que quelle que être le origine de le incertitude , celui _-ci pouvoir être modéliser par un distribution de probabilité unique . ce affirmation avoir étébeaucoup remettre en cause dans le trente dernier année . En effet le emploi systématiqued'une distribution unique en cas d' information partiel mener à un utilisation paradoxal dela théorie des probabilité . Dans un nombreux domaine , il être crucial de distinguer entre le incertitude devoir à le variabilitéd'observations et le incertitude devoir à le ignorance partiel . ce dernier pouvoir être réduitepar le obtention de nouveau information , mais pas le premier , dont on ne clr prémunir quepar un action concret . Dans le cas des base de donnée , il être souvent supposer qu' ellessont précis , et le incertitude correspondant être souvent négliger . Quant elle être aborder onreste souvent dans un approche probabiliste orthodoxe . néanmoins , le statisticien avoir développer un outil qui ne relever pas de le théorie deKolmogorov pour pallier le manque de donnée ( intervalle de confiance , principe de maximumde vraisemblance ... ) . un nouveau théorie de le incertain avoir émerger , qui offrir le possibilité de représenter lesincertitudes épistémiques et aléatoire de façon distinct , notamment le incertitude épistémique , en remplacer le distribution de probabilité unique par un famille de distribution possible , ce famille être d' autant plus grand que le information être absent . ce représentationcomplexe posséder un cas particulier plus simple à utiliser en pratique , comme le ensemblesaléatoires ( théorie des fonction de croyance ) , le distribution de possibilité ( représenter desensembles flou de valeur possible ) et le p-boxes , notamment . le but de ce exposé être de susciter le intérêt pour ce nouveau théorie de le incertain , d' en donner le base formel , d' en discuter le philosophie sous-jacent , de faire le lien aveccertaines notion en statistique , et de les illustrer sur un exemple . 	Les nouvelles théories de l'incertain	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	"LOCAL-GENERATOR : ""diviser pour régner"" pour l'extraction des traverses minimales d'un hypergraphe"	Du fait qu'elles apportent des solutions dans de nombreuses applications,les traverses minimales des hypergraphes ne cessent de susciter l'intérêt dela communauté scientifique et le développement d'algorithmes pour les calculer.Dans cet article, nous présentons une nouvelle approche pour l'optimisation del'extraction des traverses minimales basée sur les notions d'hypergraphe partielet de traverses minimales locales selon une stratégie diviser pour régner. Nousintroduisons aussi un nouvel algorithme, appelé LOCAL-GENERATOR pour lecalcul des traverses minimales. Les expérimentations effectuées sur divers jeuxde données ont montré l'intérêt de notre approche, notamment sur les hypergraphesayant un nombre de transversalité élevé et renfermant un nombre trèsimportant de traverses minimales.	M. Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001935		173	fr	fr	@univ-st-etienne.fr, @fst.rnu.tn	" LOCAL-GENERATOR : " " diviser pour régner " " pour le extraction des traverse minimal d' un hypergraphe "  Du faire qu' elles apporter un solution dans un nombreux application , le traverse minimal des hypergraphes ne cesser de susciter le intérêt dela communauté scientifique et le développement d' algorithme pour les calculer . Dans ce article , nous présenter un nouveau approche pour le optimisation del'extraction des traverse minimal baser sur le notion d' hypergraphe partielet de traverse minimal local selon un stratégie diviser pour régner . Nousintroduisons aussi un nouveau algorithme , appeler LOCAL-GENERATOR pour lecalcul un traverse minimal . le expérimentation effectuer sur divers jeuxde donner avoir montrer le intérêt de son approche , notamment sur le hypergraphesayant un nombre de transversalité élever et renfermer un nombre trèsimportant de traverse minimal . 	"LOCAL-GENERATOR : ""diviser pour régner"" pour l'extraction des traverses minimales d'un hypergraphe"	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	L'utilisation des entités nommées pour l'expansion sémantique des requêtes Web	Les entités nommées sont des éléments intéressants pour les applicationsfondées sur le Traitement du Langage Naturel. Dans le cas de la recherched'information, les entités nommées sont largement employées par les utilisateursdu web dans les requêtes de recherche, soit pour définir un concept debase, soit pour décrire un autre concept dans la requête. Du côté du modèlede recherche, les entités nommées sont des éléments riches en information quiaident à mieux cibler les documents pertinents. Dans cet article, nous étudionsl'avantage d'étendre les entités nommées dans la requête. L'idée est d'utiliserune technique d'expansion sémantique sur une ontologie générale (Yago) pourdésambiguïser les entités nommées et pour trouver leurs différentes appellationsque l'on intègre dans la requête en utilisant 3 approches : sac de mots, dépendanceséquentielle, et concept clé. Nous mesurons l'efficacité de ces expériencesen termes de précision et rappel, et nous étudions l'effet du rôle des entités nomméessur l'expansion. Nous concluons que l'expansion des entités nommées estune méthode simple qui améliore significativement la qualité de la recherchequand elle est comparée à un modèle de référence sans expansion. De plus, cetteméthode est assez compétitive par rapport à l'approche pseudo retour de pertinencesouvent utilisée pour l'expansion de la requête.	Bissan Audeh, Philippe Beaune, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1001910		174	fr	fr	@emse.fr	le utilisation des entité nommer pour le expansion sémantique des requête Web  le entité nommer être un élément intéressant pour le applicationsfondées sur le Traitement du Langage Naturel . Dans le cas de le recherched'information , le entité nommer être largement employer par le utilisateursdu web dans le requête de recherche , soit pour définir un concept debase , soit pour décrire un autre concept dans le requête . Du côté du modèlede recherche , le entité nommer être un élément riche en information quiaident à mieux cibler le document pertinent . Dans ce article , nous étudionsl'avantage d' étendre le entité nommer dans le requête . le idée être d' utiliserune technique d' expansion sémantique sur un ontologie général ( Yago ) pourdésambiguïser le entité nommer et pour trouver son différent appellationsque le on intégrer dans le requête en utiliser 3 approche : sac de mot , dépendanceséquentielle , et concept clé . Nous mesurer le efficacité de ce expériencesen terme de précision et rappel , et nous étudier le effet du rôle des entité nomméessur le expansion . Nous conclure que le expansion des entité nommer estune méthode simple qui améliorer significativement le qualité de le recherchequand elle être comparer à un modèle de référence sans expansion . De plus , cetteméthode être assez compétitif par rapport à le approche pseudo retour de pertinencesouvent utiliser pour le expansion de le requête . 	L'utilisation des entités nommées pour l'expansion sémantique des requêtes Web	2
Revue des Nouvelles Technologies de l'Information	EGC	2014	Méthodologie 3-way d'extraction d'un modèle articulatoire de la parole à partir des données d'un locuteur	Pour parler, le locuteur met en mouvement un ensemble complexed'articulateurs : la mâchoire qu'il ouvre plus ou moins, la langue à laquelle ilfait prendre de nombreuses formes et positions, les lèvres qui lui permettent delaisser l'air s'échapper plus ou moins brutalement, etc. Le modèle articulatoirele plus connu est celui de Maeda (1990), obtenu à partir d'Analyses en ComposantesPrincipales faites sur les tableaux de coordonnées des points des articulateursd'un locuteur en train de parler. Nous proposons ici une analyse 3-way dumême type de données, après leur transformation en tableaux de distances. Nousvalidons notre modèle par la prédiction des sons prononcés, qui s'avère presqueaussi bonne que celle du modèle acoustique, et même meilleure quand on prenden compte la co-articulation.	Martine Cadot, Yves Laprie	http://editions-rnti.fr/render_pdf.php?p1&p=1001985		175	fr	fr	@loria.fr, @loria.fr	méthodologie 3- way d' extraction d' un modèle articulatoire de le parole à partir un donnée d' un locuteur  Pour parler , le locuteur mettre en mouvement un ensemble complexed'articulateurs : le mâchoire qu' il ouvrer plus ou moins , le langue à laquelle ilfait prendre un nombreux forme et position , le lèvre qui lui permettre delaisser le air clr échapper plus ou moins brutalement , etc. le modèle articulatoirele plus connaître être celui de Maeda ( 1990 ) , obtenir à partir d' Analyses en ComposantesPrincipales faire sur le tableau de coordonnée des point des articulateursd'un locuteur en train de parler . Nous proposer ici un analyse 3- way dumême type de donnée , après son transformation en tableau de distance . Nousvalidons son modèle par le prédiction des son prononcer , qui clr avérer presqueaussi bon que celui du modèle acoustique , et même meilleur quand on prenden compter le co- articulation . 	Méthodologie 3-way d'extraction d'un modèle articulatoire de la parole à partir des données d'un locuteur	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Mining the Crowd	Harnessing a crowd of Web users for data collection has recently become a wide-spreadphenomenon. A key challenge is that the human knowledge forms an open world and it is thusdifficult to know what kind of information we should be looking for. Classic databases haveaddressed this problem by data mining techniques that identify interesting data patterns. Thesetechniques, however, are not suitable for the crowd. This is mainly due to properties of thehuman memory, such as the tendency to remember simple trends and summaries rather thanexact details. Following these observations, we develop here a novel model for crowd mining.We will consider in the talk the logical, algorithmic, and methodological foundations neededfor such a mining process, as well as the applications that can benefit from the knowledgemined from crowd.	Tova Milo	http://editions-rnti.fr/render_pdf.php?p1&p=1001908		176	en	en	@cs.tau.ac.il	mining crowd harness crowd web user datum collection recently become wide spreadphenomenon key challenge human knowledge form open world thusdifficult know kind information look for classic databasis haveaddressed problem datum mining technique identify interesting datum pattern thesetechnique however suitable crowd mainly due property thehuman memory tendency remember simple trend summary rather thanexact detail follow observation develop novel model crowd mining we consider talk logical algorithmic methodological foundation neededfor mining process well application benefit knowledgemine crowd	Mining the Crowd	2
Revue des Nouvelles Technologies de l'Information	EGC	2014	Modélisation de trajectoires cible/caméra : requêtes spatio-temporelles dans le cadre de la videosurveillance	Le nombre de caméras de vidéosurveillance installées dans le monde augmente chaquejour. En France, le système de la RATP déployé sur Paris comprend 9000 caméras fixes et19000 mobiles. Lors de faits particuliers (e.g., agressions, vols), les opérateurs de vidéo surveillancese basent sur les indications spatiales et temporelles de la victime et sur leur connaissancede la localisation des caméras pour sélectionner les contenus intéressants pour l'enquête.Deux grands problèmes peuvent alors survenir : (1) le temps de réponse est long (jusqu'à plusieursjours de traitement) et (2) un risque important de perte de résultats à cause d'une mauvaiseconnaissance du terrain (appel à des opérateurs extérieurs). Le but de notre recherche estde définir des outils d'assistance aux opérateurs qui puissent, à partir d'une trajectoire donnée,sélectionner de façon automatique les caméras pertinentes par rapport à la requête.	Dana Codreanu, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1001982		177	fr	fr	@irit.fr	modélisation de trajectoire cible  caméra : requête spatio- temporel dans le cadre de le videosurveillance  le nombre de caméra de vidéosurveillance installer dans le monde augmenter chaquejour . En France , le système de le RATP déployer sur Paris comprendre 9000 caméra fixe et19000 mobile . lors de fait particulier ( exempli gratia , agression , vol ) , le opérateur de vidéo surveillancese baser sur le indication spatial et temporel de le victime et sur son connaissancede le localisation des caméra pour sélectionner le contenu intéressant pour le enquête . Deux grand problème pouvoir alors survenir : ( 1 ) le temps de réponse être long ( jusqu' à plusieursjours de traitement ) et ( 2 ) un risque important de perte de résultat à cause d' un mauvaiseconnaissance du terrain ( appel à un opérateur extérieur ) . le but de son recherche estde définir un outil d' assistance aux opérateur qui pouvoir , à partir d' un trajectoire donner , sélectionner de façon automatique le caméra pertinent par rapport à le requête . 	Modélisation de trajectoires cible/caméra : requêtes spatio-temporelles dans le cadre de la videosurveillance	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Motifs récursifs : extraction ascendante hiérarchique d'ensembles d'items ou d'évènements pour le résumé de données transactionnelles ou séquentielles	Nous proposons une méthode originale pour extraire un résumé compact,représentatif et intelligible des motifs fréquents dans des données transactionnellesou séquentielles. Notre approche consiste à extraire un nouveau typede motifs que nous appelons motifs récursifs, i.e. des motifs de motifs, à l'aided'un algorithme hiérarchique agglomératif nommé RepaMiner. Nous généronsnon pas un simple ensemble de motifs mais une véritable structure dérivée dedendrogrammes, le RPgraph.	Julien Blanchard	http://editions-rnti.fr/render_pdf.php?p1&p=1001956		178	fr	fr	@univ-nantes.fr	motif récursif : extraction ascendant hiérarchique d' ensemble d' item ou d' évènements pour le résumé de donnée transactionnel ou séquentielles  Nous proposer un méthode original pour extraire un résumé compact , représentatif et intelligible des motif fréquent dans un donnée transactionnellesou séquentiel . son approche consister à extraire un nouveau typede motif que nous appeler motif récursif , i.e. des motif de motif , à le aided'un algorithme hiérarchique agglomératif nommer RepaMiner . Nous généronsnon pas un simple ensemble de motif mais un véritable structure dériver dedendrogrammes , le RPgraph . 	Motifs récursifs : extraction ascendante hiérarchique d'ensembles d'items ou d'évènements pour le résumé de données transactionnelles ou séquentielles	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Passage aux noyaux en classification recouvrante	La classification recouvrante correspond à un domaine d'étude très actifces dernières années et dont l'objectif est d'organiser un ensemble de donnéesen groupes d'individus similaires avec la particularité d'autoriser des chevauchementsentre les groupes. Parmi les approches étudiées nous nous intéressonsaux extensions recouvrantes des modèles de type moindres carrés et constatonsles difficultés théoriques et pratiques liées à leur adaptation aux noyaux. Nousformulons alors une nouvelle définition ensembliste pour caractériser un recouvrementde plusieurs classes, nous montrons que cette modélisation permet lerecours aux noyaux et nous proposons une solution algorithmique efficace pourrépondre au problème de la classification recouvrante à noyaux.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001931		179	fr	fr	@univ-orleans.fr	passage aux noyau en classification recouvrante  le classification recouvrante correspondre à un domaine d' étude très actifces dernier année et dont le objectif être d' organiser un ensemble de donnéesen groupe d' individu similaire avec le particularité d' autoriser un chevauchementsentre le groupe . Parmi le approche étudier nous nous intéressonsaux extension recouvrantes des modèle de type moindre carrer et constatonsles difficulté théorique et pratique lier à son adaptation aux noyau . Nousformulons alors un nouveau définition ensembliste pour caractériser un recouvrementde plusieurs classe , nous montrer que ce modélisation permettre lerecours aux noyau et nous proposer un solution algorithmique efficace pourrépondre au problème de le classification recouvrante à noyau . 	Passage aux noyaux en classification recouvrante	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Pondération de blocs de variables en bi-partitionnement topologique	Dans cet article, nous proposons une nouvelle approche permettantà la fois le bi-partitionnement topologique (bi-clustering) et la pondération deblocs variables. Le modèle que nous proposons FBR-BiTM (Feature Block Relevanceusing BiTM) permet de découvrir un espace topologique d'un ensembled'observations et de variables en associant un nouveau score de pondération àchaque sous ensemble de variables. L'estimation des coefficients de pondérationest réalisée dans le même processus d'apprentissage que le bi-partitionnement.Ces pondérations sont locales et associées à chaque prototype. Elles reflètentl'importance locale de chaque bloc de variables pour le bi-partitionnement. L'évaluationmontre que l'approche proposée, comparée	Amine Chaibi, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001943		180	fr	fr	@univ-paris13.fr	pondération de bloc de variable en bi-partitionnement topologique  Dans ce article , nous proposer un nouveau approche permettantà le foi le bi-partitionnement topologique ( bi-clustering ) et le pondération deblocs variable . le modèle que nous proposer FBR-BiTM ( Feature Block Relevanceusing BiTM ) permettre de découvrir un espace topologique d' un ensembled'observations et de variable en associer un nouveau score de pondération àchaque sous ensemble de variable . le estimation des coefficient de pondérationest réaliser dans le même processus d' apprentissage que le bi-partitionnement . ce pondération être local et associer à chaque prototype . Elles reflètentl'importance local de chaque bloc de variable pour le bi-partitionnement . le évaluationmontre que le approche proposer , comparée 	Pondération de blocs de variables en bi-partitionnement topologique	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Prédiction de valeurs manquantes dans les bases de données-- Une première approche fondée sur la notion de proportion analogique	Cet article présente une méthode originale de prédiction de valeursmanquantes dans les bases de données relationnelles, fondée sur la notion deproportion analogique. Nous montrons en particulier comment un algorithmeproposé dans le cadre de la classification automatique peut être adapté à cette fin.Deux cas sont considérés : celui d'une base de données transactionnelle (attributsbooléens), et celui où les valeurs manquantes peuvent être de type numérique.	William Correa Beltran, Hélène Jaudoin, Olivier Pivert	http://editions-rnti.fr/render_pdf.php?p1&p=1001961		181	fr	fr	@irisa.fr, @irisa.fr, @irisa.fr	prédiction de valeur manquant dans le base de donnée -- un premier approche fonder sur le notion de proportion analogique  ce article présenter un méthode original de prédiction de valeursmanquantes dans le base de donnée relationnel , fonder sur le notion deproportion analogique . Nous montrer en particulier comment un algorithmeproposé dans le cadre de le classification automatique pouvoir être adapter à ce fin . Deux cas être considérer : celui d' un base de donnée transactionnel ( attributsbooléens ) , et celui où le valeur manquant pouvoir être de type numérique . 	Prédiction de valeurs manquantes dans les bases de données-- Une première approche fondée sur la notion de proportion analogique	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Que ressentent les patients ?	Les forums de santé en ligne sont des espaces d'échanges où les patientspartagent leurs sentiments à propos de leurs maladies, traitements, etc.Sous couvert d'anonymat, ils expriment très librement leurs expériences personnelles.Ces forums sont donc une source d'informations très utile pour les professionnelsde santé afin de mieux identifier et comprendre les problèmes, lescomportements et les sentiments de leurs patients. Dans cet article, nous proposonsd'exploiter les messages des forums via des techniques de fouille de textespour extraire des traces d'émotions (e.g. joie, colère, surprise , etc.).	Soumia Melzi, Amine Abdaoui, Jérôme Azé, Sandra Bringay, Pascal Poncelet, Florence Galtier	http://editions-rnti.fr/render_pdf.php?p1&p=1001957		182	fr	fr		Que ressentir le patient ?  le forum de santé en ligne être un espace d' échange où le patientspartagent son sentiment à propos de son maladie , traitement , etc. Sous couvert d' anonymat , ils exprimer très librement son expérience personnel . ce forum être donc un source d' information très utile pour le professionnelsde santé afin de mieux identifier et comprendre le problème , lescomportements et le sentiment de son patient . Dans ce article , nous proposonsd'exploiter le message des forum via un technique de fouille de textespour extraire un trace d' émotion ( e.g. joie , colère , surprise , etc. ) . 	Que ressentent les patients ?	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Réconciliation des profils dans les réseaux sociaux	It is not uncommon that individuals create multiple profiles across several SNSs, eachcontaining partially overlapping sets of personal information. As a result, the creation of aglobal profile that gives an holistic view of the information of an individual requires methodsthat automatically match, or reconciliates, profiles across SNSs. In this paper, we focus on theproblem of identifying, or matching, the profiles of any individual across social networks.	Nacéra Bennacer, Coriane Nana Jipmo, Antonio Penta, Gianluca Quercini	http://editions-rnti.fr/render_pdf.php?p1&p=1001915		183	fr	en	@supelec.fr, @di.unito.it	réconciliation des profil dans le réseau sociaux 	Réconciliation des profils dans les réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Reconstruction et analyse sémantique de chronologies cybercriminelles	La reconstruction de chronologies d'évènements cybercriminels (oureconstruction d'évènements) est une étape primordiale dans une investigationnumérique. Cette phase permet aux enquêteurs d'avoir une vue des évènementssurvenus durant un incident. La reconstruction d'évènements requiert l'étuded'importants volumes de données en raison de l'omniprésence des nouvellestechnologies dans notre quotidien. De plus, les conclusions produites se doiventde respecter les critères fixés par la justice. Afin de répondre à ces challenges,nous proposons une nouvelle méthodologie basée sur une ontologie permettantd'assister les enquêteurs tout au long du processus d'enquête.	Yoan Chabot, Aurélie Bertaux, Tahar Kechadi, Christophe Nicolle	http://editions-rnti.fr/render_pdf.php?p1&p=1001969		184	fr	fr	@ucdconnect.ie	reconstruction et analyse sémantique de chronologie cybercriminelles  le reconstruction de chronologie d' évènements cybercriminels ( oureconstruction d' évènements ) être un étape primordial dans un investigationnumérique . ce phase permettre aux enquêteur d' avoir un vue des évènementssurvenus durant un incident . le reconstruction d' évènements requérir le étuded'importants volume de donnée en raison de le omniprésence des nouvellestechnologies dans son quotidien . De plus , le conclusion produire clr doiventde respecter le critère fixer par le justice . Afin de répondre à ce challenge , nous proposer un nouveau méthodologie baser sur un ontologie permettantd'assister le enquêteur tout au long du processus d' enquête . 	Reconstruction et analyse sémantique de chronologies cybercriminelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Règles d'association inter-langues au service de la recherche d'information multilingue	Dans cet article, nous proposons de montrer l'intérêt et l'utilité de déploiementdes règles d'association inter-langues (RAILs) dans le domaine de laRecherche d'Information Multilingue (RIM). Ces règles sont des connaissancesadditionnelles résultantes d'un processus de fouille de grands corpus parallèlesalignés au niveau de la phrase. En effet, leurs conclusions exprimées dans unelangue cible représentent des traductions potentielles de leurs prémisses, expriméesdans une langue source. Nous illus trons l'utilisation des RAILs dans lecontexte de la RIM à travers deux propositions, à savoir : (i) la traduction desrequêtes et (ii) la traduction des termes de l'index. L'évaluation expérimentale aété menée sur la collection de documents MUCHMORE. Les résultats ont montréune amélioration significative de la pertinence système.	Belhaj Rhouma Sourour, Asma Ben Achour, Malek Hajjem, Chiraz Latiri	http://editions-rnti.fr/render_pdf.php?p1&p=1001927		185	fr	fr	@gmail.com, @gmail.com, @gmail.com, @gnet.tn	règle d' association inter-langues au service de le recherche d' information multilingue  Dans ce article , nous proposer de montrer le intérêt et le utilité de déploiementdes règle d' association inter-langues ( RAILs ) dans le domaine de laRecherche d' information Multilingue ( RIM ) . ce règle être des connaissancesadditionnelles résultante d' un processus de fouille de grand corpus parallèlesalignés au niveau de le phrase . En effet , son conclusion exprimer dans unelangue cible représenter un traduction potentiel de son prémisse , expriméesdans un langue source . Nous illus trons le utilisation des rail dans lecontexte de le RIM à travers deux proposition , à savoir : ( i ) le traduction desrequêtes et ( ii ) le traduction des terme de le index . le évaluation expérimental aété mener sur le collection de document MUCHMORE . le résultat avoir montréune amélioration significatif de le pertinence système . 	Règles d'association inter-langues au service de la recherche d'information multilingue	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Representative training sets for classification and the variability of empirical distributions	We propose a novel approach for the estimation of the size of trainingsets that are needed for constructing valid models in machine learning and datamining. We aim to provide a good representation of the underlying populationwithout making any distributional assumptions.Our technique is based on the computation of the standard deviation of the 2-statistics of a series of samples. When successive statistics are relatively close,we assume that the samples produced represent adequately the true underlyingdistribution of the population, and the models learned from these samples willbehave almost as well as models learned on the entire population.We validate our results by experiments involving classifiers of various levels ofcomplexity and learning capabilities.	Saaid Baraty, Dan Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1001940		186	en	en	@cs.umb.edu, @cs.umb.edu	representative training set classification variability empirical distribution propose novel approach estimation size trainingset need construct valid model machine learn datamining aim provide good representation underlie populationwithout make distributional assumption our technique base computation standard deviation 2 statistic series sample successive statistic relatively close we assume sample produce represent adequately true underlyingdistribution population model learn sample willbehave almost well model learn entire population we validate result experiment involve classifier various level ofcomplexity learn capability	Representative training sets for classification and the variability of empirical distributions	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Requêtes skyline en présence d'exceptions	Dans cet article, nous nous intéressons à la recherche des points lesplus intéressants au sens de l'ordre de Pareto, i.e., à l'évaluation de requêtes« skyline » , dans des jeux de données présentant des anomalies. Il n'est pas rareque les données, de petites annonces par exemple, soient peuplées d'erreurs oud'exceptions qui peuvent perturber la recherche des meilleurs points car cellescisont susceptibles de dominer les autres points. L'approche présentée vise àcalculer les requêtes skyline malgré la présence de ces exceptions, sans pourautant les écarter définitivement, et à présenter graphiquement les résultats defaçon à identifier rapidement les points d'intérêt et les anomalies potentielles.	Hélène Jaudoin, Olivier Pivert, Daniel Rocacher	http://editions-rnti.fr/render_pdf.php?p1&p=1001944		187	fr	fr	@enssat.fr, @enssat.fr, @enssat.fr	requête skyline en présence d' exceptions  Dans ce article , nous clr intéresser à le recherche des point lesplus intéressant au sens de le ordre de Pareto , i.e. , à le évaluation de requête « skyline » , dans un jeu de donnée présenter un anomalie . Il n' être pas rareque le donnée , de petit annonce par exemple , être peupler d' erreur oud'exceptions qui pouvoir perturber le recherche des meilleur point car cellescisont susceptible de dominer le autre point . le approche présenter viser àcalculer le requête skyline malgré le présence de ce exception , sans pourautant les écarter définitivement , et à présenter graphiquement le résultat defaçon à identifier rapidement le point d' intérêt et le anomalie potentiel . 	Requêtes skyline en présence d'exceptions	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Sélection de prototypes en vue d'une catégorisation de textes avec les K plus proches voisins : étude comparative	La technique des K plus proches voisins (KNN) est une méthoded'apprentissage à base d'instances, elle a été appliquée dans la catégorisationde textes depuis de nombreuses années. En contraste avec ses performances declassification, il est reconnu que cet algorithme est lent pendant la classificationd'un nouveau document. Les Techniques de sélection de prototypes sont apparuescomme des méthodes très compétitives pour améliorer le KNN grâce à laréduction des données. L'étude contenue dans ce papier a pour objectif d'analyserl'impact de ces méthodes sur la performance de la classification de textesavec l'algorithme KNN.	Fatiha Barigou, Baya Naouel Barigou, Baghdad Atmani, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1001926		188	fr	fr	@gmail.com, @yahoo.fr	sélection de prototype en vue d' un catégorisation de texte avec le K plus proche voisin : étude comparative  le technique des K plus proche voisin ( KNN ) être un méthoded'apprentissage à base d' instance , elle avoir être appliquer dans le catégorisationde texte depuis un nombreux année . En contraste avec son performance declassification , il être reconnaître que ce algorithme être lent pendant le classificationd'un nouveau document . le technique de sélection de prototype être apparuescomme un méthode très compétitif pour améliorer le KNN grâce à laréduction des donnée . le étude contenir dans ce papier avoir pour objectif d' analyserl'impact de ce méthode sur le performance de le classification de textesavec le algorithme KNN . 	Sélection de prototypes en vue d'une catégorisation de textes avec les K plus proches voisins : étude comparative	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Sélection d'une méthode de classification multi-label pour un système interactif	L'objectif de cet article est d'évaluer la capacité de 12 algorithmesde classification multi-label à apprendre, en peu de temps, avec peu d'exemplesd'apprentissage. Les résultats expérimentaux montrent des différences importantesentre les méthodes analysées, pour les 3 mesures d'évaluation choisies:Log-Loss, Ranking-Loss et Temps d'apprentissage/prédiction, et les meilleursrésultats sont obtenus avec: multi-label k Nearest neighbours (ML-kNN), suivide Ensemble de Classifier Chains (ECC) et Ensemble de Binary Relevance (EBR).	Noureddine Yacine Nair Benrekia, Pascale Kuntz, Franck Meyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001941		189	fr	fr	@orange.com, @univ-nantes.fr	sélection d' un méthode de classification multi-label pour un système interactif  le objectif de ce article être d' évaluer le capacité de 12 algorithmesde classification multi-label à apprendre , en peu de temps , avec peu d' exemplesd'apprentissage . le résultat expérimental montrer un différence importantesentre le méthode analyser , pour le 3 mesure d' évaluation choisir : Log-Loss , Ranking-Loss et temps d' apprentissage  prédiction , et le meilleursrésultats être obtenir avec : multi-label k Nearest neighbours ( ML-kNN ) , suivide ensemble de Classifier Chains ( ECC ) et ensemble de Binary Relevance ( EBR ) . 	Sélection d'une méthode de classification multi-label pour un système interactif	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Sous échantillonnage et machine à noyaux élastiques pour la classification de données de mouvement capturé	Dans le domaine de la reconnaissance de gestes isolés, bon nombrede travaux se sont intéressés à la réduction de dimension sur l'axe spatial pourréduire à la fois la complexité algorithmique et la variabilité des réalisationsgestuelles. Il est assez étonnant de constater que peu de ces méthodes se sontexplicitement penchées sur la réduction de dimension sur l'axe temporel. Enmatière de complexité, la réduction de dimension sur cet axe est un enjeu majeurquant à l'utilisabilité de distances élastiques en complexité quadratique. Parailleurs, la prise en compte de la variabilité sur cet axe demeure une source avéréede gain de performance. Pour tenter d'apporter un éclairage en matière deréduction de dimension sur l'axe temporel, nous présentons dans cet article uneapproche basée sur un sous échantillonnage temporel associé à l'exploitationd'un apprentissage automatique à base de noyaux élastiques. Nous montronsexpérimentalement, sur deux jeux de données très référencés dans la communautéet très opposés en matière de qualité de capture de mouvement, qu'il estpossible de réduire sensiblement le nombre de postures sur les trajectoires temporellestout en conservant, grâce à des noyaux élastiques, des performances dereconnaissance au niveau de l'état de l'art du domaine. Le gain de complexitéobtenu rend une telle approche éligible pour des applications temps-réel.	Pierre-François Marteau, Sylvie Gibet, Clément Reverdy	http://editions-rnti.fr/render_pdf.php?p1&p=1001928		190	fr	fr		Sous échantillonnage et machine à noyau élastique pour le classification de donnée de mouvement capturé  Dans le domaine de le reconnaissance de geste isoler , bon nombrede travail clr être intéresser à le réduction de dimension sur le axe spatial pourréduire à le foi le complexité algorithmique et le variabilité des réalisationsgestuelles . Il être assez étonnant de constater que peu de ce méthode clr sontexplicitement pencher sur le réduction de dimension sur le axe temporel . Enmatière de complexité , le réduction de dimension sur ce axe être un enjeu majeurquant à le utilisabilité de distance élastique en complexité quadratique . Parailleurs , le prise en compte de le variabilité sur ce axe demeurer un source avéréede gain de performance . Pour tenter d' apporter un éclairage en matière deréduction de dimension sur le axe temporel , nous présenter dans ce article uneapproche baser sur un sous échantillonnage temporel associer à le exploitationd'un apprentissage automatique à base de noyau élastique . Nous montronsexpérimentalement , sur deux jeu de donnée très référencer dans le communautéet très opposer en matière de qualité de capture de mouvement , qu' il estpossible de réduire sensiblement le nombre de posture sur le trajectoire temporellestout en conserver , grâce à un noyau élastique , un performance dereconnaissance au niveau de le état de le art du domaine . le gain de complexitéobtenu rendre un tel approche éligible pour un application temps-réel . 	Sous échantillonnage et machine à noyaux élastiques pour la classification de données de mouvement capturé	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Stratégies argumentatives pour la classification collaborative multicritères des connaissances cruciales	Dans cet article, nous proposons une approche argumentative visant àautomatiser la résolution des conflits entre les décideurs qui ont des préférencescontradictoires lors d'une classification multicritères collaborative des connaissancescruciales. Notre étude expérimentale a prouvé que cette approche peutrésoudre jusqu'à 81% des conflits et améliorer la qualité d'approximation dedécideurs d'un taux de 0.62 pour un récepteur et de 0.15 pour un initiateur.	Sarra Bouzayane, Inès Saad	http://editions-rnti.fr/render_pdf.php?p1&p=1001981		191	fr	fr	@yahoo.fr, @france-bs.com	Stratégies argumentatif pour le classification collaboratif multicritères des connaissance cruciales  Dans ce article , nous proposer un approche argumentatif viser àautomatiser le résolution des conflit entre le décideur qui avoir un préférencescontradictoires lors d' un classification multicritères collaboratif des connaissancescruciales . son étude expérimental avoir prouver que ce approche peutrésoudre jusqu' à 81 \% des conflit et améliorer le qualité d' approximation dedécideurs d' un taux de 0.62 pour un récepteur et de 0.15 pour un initiateur . 	Stratégies argumentatives pour la classification collaborative multicritères des connaissances cruciales	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Symétries et Extraction de Motifs Ensemblistes	Les symétries sont des propriétés structurelles qu'on détecte dans ungrand nombre de bases de données. Dans cet article, nous étudions l'exploitationdes symétries pour élaguer l'espace de recherche dans les problèmes d'extractionde motifs ensemblistes. Notre approche est basée sur une intégrationdynamique des symétries dans les algorithmes de type Apriori permettant de réduirel'espace des motifs candidats. En effet, pour un motif donné, les symétriesnous permettent de déduire les motifs qui lui sont symétriques et vérifiant parconséquent les mêmes propriétés. Nous détaillons notre approche en utilisantl'exemple des motifs fréquents. Ensuite, nous la généralisons au cadre unificateurde Mannila et Toivonen pour l'extraction des motifs ensemblistes. Les expériencesmenées montrent la faisabilité et l'apport de notre approche d'élagagebasé sur les symétries.	Said Jabbour, Mehdi Khiari, Lakhdar Sais, Yakoub Salhi, Karim Tabia	http://editions-rnti.fr/render_pdf.php?p1&p=1001953		192	fr	fr	@univ-artois.fr, @zto-technology.com	symétrie et extraction de motif Ensemblistes  le symétrie être un propriété structurel qu' on détecter dans ungrand nombre de base de donnée . Dans ce article , nous étudier le exploitationdes symétrie pour élaguer le espace de recherche dans le problème d' extractionde motif ensembliste . son approche être baser sur un intégrationdynamique des symétrie dans le algorithme de type Apriori permettre de réduirel'espace des motif candidat . En effet , pour un motif donner , le symétriesnous permettre de déduire le motif qui lui être symétrique et vérifier parconséquent le même propriété . Nous détailler son approche en utilisantl'exemple des motif fréquent . ensuite , nous le généralisons au cadre unificateurde Mannila et Toivonen pour le extraction des motif ensembliste . le expériencesmenées montrer le faisabilité et le apport de son approche d' élagagebasé sur le symétrie . 	Symétries et Extraction de Motifs Ensemblistes	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	The Hitchhiker's Guide to Ontology	Artificial Intelligence has long had the dream of making computers smarter. For quite sometime, this vision has remained just that: a dream. With the development of large knowledgebases, though, we now have large amounts of semantic information at our hands. This changesthe game of AI. Computers have indeed become smarter. In this talk, we present the latestdevelopments in the field: The construction of general purpose knowledge bases (includingYAGO and DBpedia, as well as NELL and TextRunner), and their applications to tasks thatwere previously out of scope, The extraction of fine-grained information from natural languagetexts, semantic query answering, and the interpretation of newspaper texts at large scale.	Fabian Suchanek	http://editions-rnti.fr/render_pdf.php?p1&p=1001909		193	en	en	@suchanek.name	hitchhiker s guide ontology artificial intelligence long dream make computer smarter quite sometime vision remain that dream development large knowledgebasis though large amount semantic information hand changesthe game ai computer indeed become smarter talk present latestdevelopment field construction general purpose knowledge basis includingyago dbpedia well nell textrunner application task thatwere previously scope extraction fine grained information natural languagetext semantic query answering interpretation newspaper text large scale	The Hitchhiker's Guide to Ontology	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Un système de détection de thématiques populaires sur Twitter	With the ever-growing amount of messages exchanged via Twitter, there is an increasinginterest in filtering this information, which is delivered under the form of a stream of messages.In this paper, we present a system for detecting popular topics in Twitter. The system can beapplied to static corpora and can also handle the live Twitter stream.	Adrien Guille, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001990		194	fr	en	@pseudonyme	un système de détection de thématique populaire sur Twitter 	Un système de détection de thématiques populaires sur Twitter	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche algébrique au problème du consensus de partitions	En classification non-supervisée, le consensus de partitions a pour objectifde produire une partition unique, représentant le consensus, à partir d'unensemble de partitions où chacune est engendrée indépendamment des autres,voire avec des méthodologies différentes. En complément des techniques ayantleur qualité propre en terme de robustesse ou de passage à l'échelle, nous apportonsun point de vue original sur le consensus de partitions, c'est-à-dire, par lebiais de définitions algébriques qui permettent d'établir la nature des déductionspouvant être réalisées dans une approche systématique (p.ex. un système à basede connaissances). Nous fondons notre approche sur le treillis des partitions pourlequel nous montrons comment peuvent être adjoint des opérateurs dans le butde formuler une expression caractérisant le consensus à partir d'un ensemble departitions.	Frédéric Dumonceaux, Guillaume Raschia, Marc Gelgon	http://editions-rnti.fr/render_pdf.php?p1&p=1001937		195	fr	fr	@univ-nantes.fr	un approche algébrique au problème du consensus de partitions  En classification non- superviser , le consensus de partition avoir pour objectifde produire un partition unique , représenter le consensus , à partir d' unensemble de partition où chacun être engendrer indépendamment un autre , voire avec un méthodologie différent . En complément des technique ayantleur qualité propre en terme de robustesse ou de passage à le échelle , nous apportonsun point de vue original sur le consensus de partition , c' est-à-dire , par lebiais de définition algébrique qui permettre d' établir le nature des déductionspouvant être réaliser dans un approche systématique ( page ex. un système à basede connaissance ) . Nous fonder son approche sur le treillis des partition pourlequel nous montrer comment pouvoir être adjoint des opérateur dans le butde formuler un expression caractériser le consensus à partir d' un ensemble departitions . 	Une approche algébrique au problème du consensus de partitions	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche basée sur STATIS pour la fusion de cartes topologiques auto-organisées	Dans le cadre des cartes topologiques, nous proposons une nouvelleapproche d'ensemble clusters basée sur la méthode STATIS. Les méthodes d'ensembleclusters visent à améliorer la qualité de la partition d'un jeu de donnéesà travers la combinaison de plusieurs partitions.Les différentes partitions peuvent être obtenues en faisant varier les paramètresd'un algorithme (choix des centres initiaux, du voisinage initial et final des cellulesdans le cas des cartes topologiques auto-organisée SOM, etc). L'approcheprésentée dans cette communication repose sur la méthode d'analyse de donnéesmulti-tableaux STATIS pour déterminer une matrice compromis représentant aumieux la similarité entre les partitions issues des cartes topologiques. La fusiondes cartes topologiques est alors obtenue à travers une classification basée surcette matrice compromis. La méthode proposée est illustrée sur des donnéesréelles issues de l'UCI et sur des données simulées.	Mory Ouattara, Ndeye Niang, Rania Gasri, Fouad Badran, Corinne Mandin	http://editions-rnti.fr/render_pdf.php?p1&p=1001947		196	fr	fr	@cnam.fr, @cnam.fr, @cstb.fr, @cstb.fr	un approche baser sur STATIS pour le fusion de carte topologique auto- organisées  Dans le cadre des carte topologique , nous proposer un nouvelleapproche d' ensemble clusters baser sur le méthode STATIS . le méthode d' ensembleclusters viser à améliorer le qualité de le partition d' un jeu de donnéesà travers le combinaison de plusieurs partition . le différent partition pouvoir être obtenir en faire varier le paramètresd'un algorithme ( choix des centre initial , du voisinage initial et final des cellulesdans le cas des carte topologique auto- organiser SOM , etc ) . le approcheprésentée dans ce communication reposer sur le méthode d' analyse de donnéesmulti-tableaux STATIS pour déterminer un matrice compromettre représenter aumieux le similarité entre le partition issu des carte topologique . le fusiondes carte topologique être alors obtenir à travers un classification baser surcette matrice compromettre . le méthode proposer être illustrer sur un donnéesréelles issu de le UCI et sur un donnée simuler . 	Une approche basée sur STATIS pour la fusion de cartes topologiques auto-organisées	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche PPC pour la fouille de données séquentielles	Nous proposons dans cet article une nouvelle approche croisant destechniques de programmation par contraintes et de fouille pour l'extraction demotifs séquentiels. Le modèle que nous proposons offre un cadre générique etdéclaratif pour modéliser et résoudre des contraintes de nature hétérogène	Jean-Philippe Métivier, Samir Loudni, Thierry Charnois	http://editions-rnti.fr/render_pdf.php?p1&p=1001951		197	fr	fr		un approche PPC pour le fouille de donnée séquentielles  Nous proposer dans ce article un nouveau approche croiser destechniques de programmation par contrainte et de fouille pour le extraction demotifs séquentiel . le modèle que nous proposer offrir un cadre générique etdéclaratif pour modéliser et résoudre un contrainte de nature hétérogène 	Une approche PPC pour la fouille de données séquentielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approcheWeb sémantique et combinatoire pour un système de recommandation sensible au contexte appliqué à l'apprentissage mobile	Au vu de l'émergence rapide des nouvelles technologies mobiles et lacroissance des offres et besoins d'une société en mouvement, les travaux se multiplientpour identifier de nouvelles plateformes d'apprentissage pertinentes afind'améliorer et faciliter l'apprentissage à distance. La prochaine étape de l'apprentissageà distance est naturellement le port de l'e-learning (apprentissageélectronique) vers les nouveaux systèmes mobiles. On parle de m-learning (apprentissagemobile). Nos travaux portent sur le développement d'une nouvellearchitecture pour le m-learning dont l'objectif est d'adapter et recommander desparcours de formations selon les contraintes contextuelles de l'apprenant.	Fayrouz Soualah Alila, Christophe Nicolle, Florence Mendes	http://editions-rnti.fr/render_pdf.php?p1&p=1001979		198	fr	fr	@checksem.fr, @crossknowledge.com	un approcheWeb sémantique et combinatoire pour un système de recommandation sensible au contexte appliquer à le apprentissage mobile  Au voir de le émergence rapide des nouveau technologie mobile et lacroissance des offre et besoin d' un société en mouvement , le travail clr multiplientpour identifier un nouveau plateforme d' apprentissage pertinent afind'améliorer et faciliter le apprentissage à distance . le prochain étape de le apprentissageà distance être naturellement le port de le e-learning ( apprentissageélectronique ) vers le nouveau système mobile . On parler de m-learning ( apprentissagemobile ) . son travail porter sur le développement d' un nouvellearchitecture pour le m-learning dont le objectif être d' adapter et recommander desparcours de formation selon le contrainte contextuel de le apprenant . 	Une approcheWeb sémantique et combinatoire pour un système de recommandation sensible au contexte appliqué à l'apprentissage mobile	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une heuristique pour le paramétrage automatique de l'algorithme de clustering spectral	Trouver le nombre optimal de groupes dans le contexte d'un algorithmede clustering est un problème notoirement difficile. Dans cet article,nous en décrivons et évaluons une solution approchée dans le cas de l'algorithmespectral. Notre méthode présente l'avantage d'être déterministe, et peucoûteuse. Nous montrons qu'elle fonctionne de manière satisfaisante dans beaucoupde cas, même si quelques limites amènent des perspectives à ce travail.	Pierrick Bruneau, Olivier Parisot, Philippe Pinheiro	http://editions-rnti.fr/render_pdf.php?p1&p=1001933		199	fr	fr	@lippmann.lu	un heuristique pour le paramétrage automatique de le algorithme de clustering spectral  trouver le nombre optimal de groupe dans le contexte d' un algorithmede clustering être un problème notoirement difficile . Dans ce article , nous en décrivons et évaluer un solution approcher dans le cas de le algorithmespectral . son méthode présent le avantage d' être déterministe , et peucoûteuse . Nous montrer qu' elle fonctionner de manière satisfaisant dans beaucoupde cas , même si quelque limite amener un perspective à ce travail . 	Une heuristique pour le paramétrage automatique de l'algorithme de clustering spectral	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode hybride pour la prédiction du profil des auteurs	Dans cet article, nous nous intéressons à la détection du profil desauteurs (âge, genre) à travers leurs discussions. La méthode proposée s'appuiesur la classification automatique qui utilise certaines données extraites d'une manièrestatistique à partir de corpus source. Nous présentons une méthode hybridequi combine l'analyse de surface dans les textes avec une méthode d'apprentissageautomatique. A fin d'obtenir une meilleure gestion de ces données, nousnous sommes basés sur l'utilisation des arbres de décision. Notre méthode adonné des résultats intéressants pour la détection du genre.	Seifeddine Mechti, Maher Jaoua, Lamia Hadrich Belguith	http://editions-rnti.fr/render_pdf.php?p1&p=1001973		200	fr	fr	@gmail.com, @fsegs.rnu.t, @fsegs.rnu.t	un méthode hybride pour le prédiction du profil des auteurs  Dans ce article , nous clr intéresser à le détection du profil desauteurs ( âge , genre ) à travers son discussion . le méthode proposer clr appuiesur le classification automatique qui utiliser certain donnée extraire d' un manièrestatistique à partir de corpus source . Nous présenter un méthode hybridequi combiner le analyse de surface dans le texte avec un méthode d' apprentissageautomatique . A fin d' obtenir un meilleur gestion de ce donnée , nousnous somme baser sur le utilisation des arbre de décision . son méthode adonner des résultat intéressant pour le détection du genre . 	Une méthode hybride pour la prédiction du profil des auteurs	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode pour caractériser les communautés des réseaux dynamiques à attributs	De nombreux systèmes complexes sont étudiés via l'analyse de réseauxdits complexes ayant des propriétés topologiques typiques. Parmi cellesci,les structures de communautés sont particulièrement étudiées. De nombreusesméthodes permettent de les détecter, y compris dans des réseaux contenant desattributs nodaux, des liens orientés ou évoluant dans le temps. La détection prendla forme d'une partition de l'ensemble des noeuds, qu'il faut ensuite caractériserrelativement au système modélisé. Nous travaillons sur l'assistance à cettetâche de caractérisation. Nous proposons une représentation des réseaux sous laforme de séquences de descripteurs de noeuds, qui combinent les informationstemporelles, les mesures topologiques, et les valeurs des attributs nodaux. Lescommunautés sont caractérisées au moyen des motifs séquentiels émergents lesplus représentatifs issus de leurs noeuds. Ceci permet notamment la détectionde comportements inhabituels au sein d'une communauté. Nous décrivons uneétude empirique sur un réseau de collaboration scientifique.	Gu&#776;nce Keziban Orman, Vincent Labatut, Marc Plantevit, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001919		201	fr	fr	@insa-lyon.fr, @gsu.edu.tr, @gsu.edu.tr, @liris.cnrs.fr	un méthode pour caractériser le communauté des réseau dynamique à attributs  un nombreux système complexe être étudier via le analyse de réseauxdits complexe avoir un propriété topologique typique . Parmi cellesci , le structure de communauté être particulièrement étudier . De nombreusesméthodes permettre de les détecter , y comprendre dans un réseau contenir desattributs nodaux , un lien orienter ou évoluer dans le temps . le détection prendla forme d' un partition de le ensemble des noeud , qu' il faillir ensuite caractériserrelativement au système modéliser . Nous travailler sur le assistance à cettetâche de caractérisation . Nous proposer un représentation des réseau sous laforme de séquence de descripteur de noeud , qui combiner le informationstemporelles , le mesure topologique , et le valeur des attribut nodaux . Lescommunautés être caractériser au moyen des motif séquentiel émergent lesplus représentatif issir de son noeud . ceci permettre notamment le détectionde comportement inhabituel au sein d' un communauté . Nous décrire uneétude empirique sur un réseau de collaboration scientifique . 	Une méthode pour caractériser les communautés des réseaux dynamiques à attributs	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode pour la détection de thématiques populaires sur Twitter	L'explosion du volume de messages échangés via Twitter entraîne unphénomène de surcharge informationnelle pour ses utilisateurs. Il est donc crucialde doter ces derniers de moyens les aidant à filtrer l'information brute, laquelleest délivrée sous la forme d'un flux de messages. Dans cette optique, nousproposons une méthode basée sur la modélisation de l'anomalie dans la fréquencede création de liens dynamiques entre utilisateurs pour détecter les picsde popularité et extraire une liste ordonnée de thématiques populaires. Les expérimentationsmenées sur des données réelles montrent que la méthode proposéeest capable d'identifier et localiser efficacement les thématiques populaires.	Adrien Guille, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001917		202	fr	fr		un méthode pour le détection de thématique populaire sur Twitter  le explosion du volume de message échanger via Twitter entraîner unphénomène de surcharge informationnel pour son utilisateur . Il être donc crucialde doter ce dernier de moyen les aider à filtrer le information brut , laquelleest délivrer sous le forme d' un flux de message . Dans ce optique , nousproposons un méthode baser sur le modélisation de le anomalie dans le fréquencede création de lien dynamique entre utilisateur pour détecter le picsde popularité et extraire un liste ordonner de thématique populaire . le expérimentationsmenées sur un donnée réel montrer que le méthode proposéeest capable d' identifier et localiser efficacement le thématique populaire . 	Une méthode pour la détection de thématiques populaires sur Twitter	3
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une nouvelle approche pour la sélection de variables basée sur une métrique d'estimation de la qualité	La maximisation d'étiquetage (F-max) est une métrique non biaiséed'estimation de la qualité d'une classification non supervisée (clustering) qui favoriseles clusters ayant une valeur maximale de F-mesure d'étiquetage. Danscet article, nous montrons qu'une adaptation de cette métrique dans le cadrede la classification supervisée permet de réaliser une sélection de variables etde calculer pour chacune d'elles une fonction de contraste. La méthode est expérimentéesur différents types de données textuelles. Dans ce contexte, nousmontrons que cette technique améliore les performances des méthodes de classificationde façon très significative par rapport à l'état de l'art des techniquesde sélection de variables, notamment dans le cas de la classification de donnéestextuelles déséquilibrées, fortement multidimensionnelles et bruitées.	Jean-Charles Lamirel, Pascal Cuxac, Kafil Hajlaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001923		203	fr	fr	@loria.fr, @inist.fr	un nouveau approche pour le sélection de variable baser sur un métrique d' estimation de le qualité  le maximisation d' étiquetage ( F-max ) être un métrique non biaiséed'estimation de le qualité d' un classification non superviser ( clustering ) qui favoriseles clusters avoir un valeur maximal de F-mesure d' étiquetage . Danscet article , nous montrer qu' un adaptation de ce métrique dans le cadrede le classification superviser permettre de réaliser un sélection de variable etde calculer pour chacun d' lui un fonction de contraste . le méthode être expérimentéesur différent type de donnée textuel . Dans ce contexte , nousmontrons que ce technique améliorer le performance des méthode de classificationde façon très significatif par rapport à le état de le art des techniquesde sélection de variable , notamment dans le cas de le classification de donnéestextuelles déséquilibrer , fortement multidimensionnel et bruiter . 	Une nouvelle approche pour la sélection de variables basée sur une métrique d'estimation de la qualité	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Utilisation de relations ontologiques pour la comparaison d'images décrites par des annotations sémantiques	Face à la complexité des nouvelles générations d'images médicales, les processus de recherche d'images basés sur leurs contenus visuels peuvent s'avérer insuffisants. Cet article propose une nouvelle approche basée sur l'annotation des images via des termes sémantiques pouvant pallier ce problème. Elle repose sur la combinaison d'une distance hiérarchique permettant de comparer les images en considérant les corrélations entre les termes utilisés pour les décrire et d'une mesure de similarité permettant d'évaluer la proximité sémantique entre des termes ontologiques. Cette approche est validée dans le cadre de la recherche d'images tomodensitométriques.	Camille Kurtz, Daniel Rubin	http://editions-rnti.fr/render_pdf.php?p1&p=1001991		204	fr	fr	@parisdescartes.fr, @stanford.edu	utilisation de relation ontologique pour le comparaison d' image décrire par un annotation sémantiques  face à le complexité des nouveau génération d' image médical , le processus de recherche d' image baser sur son contenu visuel pouvoir clr avérer insuffisant . ce article proposer un nouveau approche baser sur le annotation des image via un terme sémantique pouvoir pallier ce problème . Elle reposer sur le combinaison d' un distance hiérarchique permettre de comparer le image en considérer le corrélation entre le terme utiliser pour les décrire et d' un mesure de similarité permettre d' évaluer le proximité sémantique entre un terme ontologique . ce approche être valider dans le cadre de le recherche d' image tomodensitométriques . 	Utilisation de relations ontologiques pour la comparaison d'images décrites par des annotations sémantiques	2
Revue des Nouvelles Technologies de l'Information	EGC	2014	Vectorisation paramétrée des données textuelles	Automatic processing of textual data enables users to analyze semi-automatically and on alarge scale the data. This analysis is based on two successive processes: (i) representation oftexts, (ii) gathering of textual data (clustering). The software described in this paper focuses onthe first step of the process by offering expert a parameterized representation of textual data.	Célia Da Costa Pereira, Mathieu Lafourcade, Patrick Lloret, Cédric Lopez, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001987		205	fr	en	@unice.fr, @lirmm.fr, @succeed-together.eu, @objetdirect.com, @cirad.fr	Vectorisation paramétrer des donnée textuelles 	Vectorisation paramétrée des données textuelles	1
Revue des Nouvelles Technologies de l'Information	EGC	2014	Vers une classification non supervisée adaptée pour obtenir des arbres de décision simplifiés	L'induction d'arbre de décision est une technique puissante et populairepour extraire de la connaissance. Néanmoins, les arbres de décision obtenusdepuis des données issues du monde réel peuvent être très complexes et donc difficilesà exploiter. Dans ce cadre, cet article présente une solution originale pouradapter le résultat d'une classification non supervisée quelconque afin d'obtenirdes arbres de décision simplifiés pour chaque cluster.	Olivier Parisot, Yoanne Didry, Pierrick Bruneau, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1001965		206	fr	fr	@lippmann.lu	Vers un classification non superviser adapter pour obtenir un arbre de décision simplifiés  le induction d' arbre de décision être un technique puissant et populairepour extraire de le connaissance . néanmoins , le arbre de décision obtenusdepuis des donnée issu du monde réel pouvoir être très complexe et donc difficilesà exploiter . Dans ce cadre , ce article présenter un solution original pouradapter le résultat d' un classification non superviser quelconque afin d' obtenirdes arbre de décision simplifier pour chaque cluster . 	Vers une classification non supervisée adaptée pour obtenir des arbres de décision simplifiés	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Vers une modularité pour données vectorielles	La modularité, introduite par Newman pour mesurer la qualité d'unepartition des sommets d'un graphe, ne prend pas en compte d'éventuelles valeursassociées à ces sommets. Dans cet article, nous introduisons une mesure de modularitécomplémentaire, basée sur l'inertie, et adaptée pour évaluer la qualitéd'une partition d'éléments représentés dans un espace vectoriel réel. Cette mesurese veut un pendant pour la classification non supervisée de la modularitéde Newman. Nous présentons également 2Mod-Louvain, une méthode utilisantce critère de modularité basée sur l'inertie conjointement à la modularité deNewman pour détecter des communautés dans des réseaux d'information. Lesexpérimentations que nous avons menées ont montré qu'en exploitant à la foisles données relationnelles et vectorielles, 2Mod-Louvain détectait plus efficacementles communautés que des méthodes utilisant un seul type de données etqu'elle était robuste face à des dégradations des données.	David Combe, Christine Largeron, Elod Egyed-Zsigmond, Mathias Géry	http://editions-rnti.fr/render_pdf.php?p1&p=1001914		207	fr	fr	@univ-st-etienne.fr, @insa-lyon.fr	Vers un modularité pour donnée vectorielles  le modularité , introduire par Newman pour mesurer le qualité d' unepartition des sommet d' un graphe , ne prendre pas en compte d' éventuel valeursassociées à ce sommet . Dans ce article , nous introduire un mesure de modularitécomplémentaire , baser sur le inertie , et adapter pour évaluer le qualitéd'une partition d' élément représenter dans un espace vectoriel réel . ce mesurese vouloir un pendant pour le classification non superviser de le modularitéde Newman . Nous présenter également 2Mod-Louvain , un méthode utilisantce critère de modularité baser sur le inertie conjointement à le modularité deNewman pour détecter un communauté dans un réseau d' information . Lesexpérimentations que nous avoir mener avoir montrer qu' en exploiter à le foisles donnée relationnel et vectoriel , 2Mod-Louvain détecter plus efficacementles communauté que un méthode utiliser un seul type de donnée etqu'elle être robuste face à un dégradation des donnée . 	Vers une modularité pour données vectorielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2014	Visualisation de données de prosopographie pour la reconstruction de carrières de personnages et de réseaux socio-professionnels	Dans cet article nous présentons deux approches de visualisation développéesdans le cadre d'un projet collaboratif sur l'accès et l'exploitation desdonnées prosopographiques de la Renaissance en France. L'objectif du projetest de modéliser et réaliser un portail sémantique assurant l'accès à différentesbases de données prosopographiques existantes afin de permettre une meilleureexploration et exploitation de ces données. Dans ce cadre, nous avons proposédeux interfaces de visualisation ProsoGraph et ProsoMap qui s'appuient respectivementsur la visualisation de graphes de réseaux sociaux et la visualisationde lieux géographiques et de trajectoires spatio-temporelles. Les deux interfacescommuniquent avec le portail via une couche sémantique et lui offrent des fonctionnalitésd'interrogation supplémentaires.	Nizar Messai, Thomas Devogele	http://editions-rnti.fr/render_pdf.php?p1&p=1001978		208	fr	fr	@univ-tours.fr	visualisation de donnée de prosopographie pour le reconstruction de carrière de personnage et de réseau socio- professionnels  Dans ce article nous présenter deux approche de visualisation développéesdans le cadre d' un projet collaboratif sur le accès et le exploitation desdonnées prosopographiques de le Renaissance en France . le objectif du projetest de modéliser et réaliser un portail sémantique assurer le accès à différentesbases de donnée prosopographiques existant afin de permettre un meilleureexploration et exploitation de ce donnée . Dans ce cadre , nous avoir proposédeux interface de visualisation ProsoGraph et ProsoMap qui clr appuyer respectivementsur le visualisation de graphe de réseau social et le visualisationde lieu géographique et de trajectoire spatio- temporel . le deux interfacescommuniquent avec le portail via un couche sémantique et lui offrir un fonctionnalitésd'interrogation supplémentaire . 	Visualisation de données de prosopographie pour la reconstruction de carrières de personnages et de réseaux socio-professionnels	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	20 ans de découverte de motifs : une étude bibliographique quantitative	Depuis deux décennies, la découverte de motifs a été l'un des champs de recherche les plus actifs de l'exploration de données. Cet article en établit une étude bibliographique quantitative en nous appuyant sur 1030 publications issues de 5 conférences internationales majeures : KDD, PKDD, PAKDD, ICDM et SDM. Nous avons d'abord mesuré depuis 2005 un sévère ralentissement de l'activité de recherche dédiée à la découverte de motifs. Puis, nous avons quantifié les principales contributions en terme de langages, de contraintes et de représentations condensées de sorte à comprendre ce ralentissement et à esquisser les directions actuelles.	Arnaud Giacometti, Dominique Haoyuan Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001830		248	fr	fr	@univ-tours.fr	20 an de découverte de motif : un étude bibliographique quantitative  Depuis deux décennie , le découverte de motif avoir être le un des champ de recherche le plus actif de le exploration de donnée . ce article en établir un étude bibliographique quantitatif en nous appuyer sur 1030 publication issir de 5 conférence international majeur : KDD , PKDD , PAKDD , ICDM et SDM . Nous avoir d' abord mesurer depuis 2005 un sévère ralentissement de le activité de recherche dédier à le découverte de motif . Puis , nous avoir quantifier le principal contribution en terme de langage , de contrainte et de représentation condenser de sorte à comprendre ce ralentissement et à esquisser le direction actuel . 	20 ans de découverte de motifs : une étude bibliographique quantitative	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	3D : de nouvelles perspectives en fouille exploratoire avec la stéréoscopie	Si la 3D est un sujet de débat dans la communauté, les expériences sur lesquelles s'appuient les discussions concernent le plus souvent des restitutions visuelles basées sur une projection classique en perspective linéaire. L'objectif de cette communication est de renouveler le cadre expérimental en étudiant l'impact de l'ajout de la disparité binoculaire. Nous nous focalisons ici sur une tâche importante en analyse de réseaux : l'identification de communautés. Et nous comparons la 3D monoscopique et la 3D stéréoscopique à la fois pour la performance de résolution de la tâche et pour le comportement exploratoire à travers l'analyse du mouvement du pointeur de la souris et de la dynamique des modifications de points de vue sur les graphes. Nos résultats expérimentaux mettent en évidence des performances significativement meilleures pour la 3D stéréoscopique et des différences comportementales dans l'exploration avec un centrage plus important sur des zones restreintes en stéréoscopie.	Nicolas Greffard, Fabien Picarougne, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1001831		249	fr	fr	@univ-nantes.fr	3D : un nouveau perspective en fouille exploratoire avec le stéréoscopie  Si le 3D être un sujet de débat dans le communauté , le expérience sur lesquelles clr appuyer le discussion concerner le plus souvent un restitution visuel baser sur un projection classique en perspective linéaire . le objectif de ce communication être de renouveler le cadre expérimental en étudier le impact de le ajout de le disparité binoculaire . Nous nous focaliser ici sur un tâche important en analyse de réseau : le identification de communauté . Et nous comparer le 3D monoscopique et le 3D stéréoscopique à le foi pour le performance de résolution de le tâche et pour le comportement exploratoire à travers le analyse du mouvement du pointeur de le souris et de le dynamique des modification de point de vue sur le graphe . son résultat expérimental mettre en évidence des performance significativement meilleur pour le 3D stéréoscopique et des différence comportemental dans le exploration avec un centrage plus important sur un zone restreindre en stéréoscopie . 	3D : de nouvelles perspectives en fouille exploratoire avec la stéréoscopie	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	A POS Tagger analysed in collaboration environments and literary texts	Part-of-speech (POS) tagging is often used in other modules of natural language processing and therefore the results of this process should be as precise as possible. Many different types of taggers have been developed to improve the accuracy of the results in the field of literature or newspapers. Nowadays when the internet is widespread, the environments for online collaboration as chats, forums, blogs, wikis have become important means of communication. The purpose of this research is to analyse the results of tagging the words obtained from the labelling of the words from the online collaboration environments and literary texts with the corresponding parts of speech. In the case of POS tagging, the ambiguities arise due to the fact that a word may have multiple morphological values depending on context.	Dumitru-Clementin Cercel, Stefan Trausan-Matu	http://editions-rnti.fr/render_pdf.php?p1&p=1001847		250	en	en	@gmail.com, @cs.pub.ro	pos tagger analyse collaboration environment literary text part of speech pos tag often used module natural language processing therefore result process precise possible many different type tagger develop improve accuracy result field literature newspaper nowadays internet widespread environment online collaboration chat forum blog wiki become important mean communication purpose research analyse result tag word obtain labelling word online collaboration environment literary text corresponding part speech case pos tag ambiguity arise due fact word may multiple morphological value depend context	A POS Tagger analysed in collaboration environments and literary texts	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Accélération de la méthode des K plus proches voisins pour la catégorisation de textes	Parmi la panoplie de classificateurs utilisés dans la catégorisation de textes, nous nous intéressons à l'algorithme des k-voisins les plus proches. Ces performances le situent parmi les meilleures méthodes de catégorisation de textes. Toutefois, il présente certaines limites: (i) coût mémoire car il faut stocker l'ensemble d'apprentissage en entier et (ii) coût élevé de calcul car il doit explorer l'ensemble d'apprentissage pour classer un nouveau document. Dans ce papier, nous proposons une nouvelle démarche pour réduire ce temps de classification sans dégrader les performances de classification.	Fatiha Barigou, Baghdad Atmani, Youcef Bouziane, Naouel Barigou	http://editions-rnti.fr/render_pdf.php?p1&p=1001841		251	fr	fr	@gamil.com, @gmail.com	accélération de le méthode des K plus proche voisin pour le catégorisation de textes  Parmi le panoplie de classificateur utiliser dans le catégorisation de texte , nous clr intéresser à le algorithme des k-voisins le plus proche . ce performance le situer parmi le meilleur méthode de catégorisation de texte . toutefois , il présenter certain limite : ( i ) coût mémoire car il faillir stocker le ensemble d' apprentissage en entier et ( ii ) coût élever de calcul car il devoir explorer le ensemble d' apprentissage pour classer un nouveau document . Dans ce papier , nous proposer un nouveau démarche pour réduire ce temps de classification sans dégrader le performance de classification . 	Accélération de la méthode des K plus proches voisins pour la catégorisation de textes	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse conceptuelle de données de simulation de systèmes complexes pour l'aide à la décision : Application à la conception d'une cabine d'avion	Dans cet article nous présentons une approche conceptuelle d'aide à la décision dans la conception de systèmes complexes. Cette approche s'appuie sur le formalisme de l'analyse de concepts formels par similarité (ACFS) pour la classification, la visualisation et l'exploration de données de simulation afin d'aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents. L'approche est illustrée sur un cas test de conception de cabine d'un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin d'identifier celles qui assurent un confort convenable pour les passagers la cabine. La classification des données de simulation avec leurs scores de confort en utilisant l'ACFS permet d'identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers. Les résultats obtenus ont été confirmés et validés par de nouvelles simulations.	Nizar Messai, Cassio Melo, Mohamed Hamdaoui, Dung Bui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001835		252	fr	fr	@univ-tours.fr, @ecp.fr	analyse conceptuel de donnée de simulation de système complexe pour le aide à le décision : application à le conception d' un cabine d' avion  Dans ce article nous présenter un approche conceptuel d' aide à le décision dans le conception de système complexe . ce approche clr appuyer sur le formalisme de le analyse de concept formel par similarité ( ACFS ) pour le classification , le visualisation et le exploration de donnée de simulation afin d' aider le concepteur de système complexe à identifier le choix de conception le plus pertinent . le approche être illustrer sur un cas test de conception de cabine d' un avion de ligne fournir par le partenaire industriel et qui consister à étudier le donnée de simulation de différent configuration du système de ventilation de le cabine afin d' identifier celui qui assurer un confort convenable pour le passager le cabine . le classification des donnée de simulation avec son score de confort en utiliser le ACFS permettre d' identifier pour chaque paramètre de conception simuler le plage de valeur possible qui assurer un confort convenable pour le passager . le résultat obtenir avoir être confirmer et valider par un nouveau simulation . 	Analyse conceptuelle de données de simulation de systèmes complexes pour l'aide à la décision : Application à la conception d'une cabine d'avion	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse de réseaux sociaux par l'analyse formelle de concepts	L'analyse formelle de concepts (AFC) est un formalisme de représentation et d'extraction de connaissance fondé sur les notions de concepts et de treillis de concepts (Galois).L'AFC a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel, les bases et entrepôts de données, l'extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine, la psychologie, la linguistique et la sociologie.Dans cette présentation, nous allons explorer le potentiel de l'AFC et de quelques extensions de cette théorie (ex. analyse triadique de concepts) dans l'analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex. détection de communautés et d'individus influents à partir d'un réseau d'amis) ou même de réseaux hétérogènes (ex. extraction de règles d'association d'un réseau bibliographique).	Rokia Missaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001814		253	fr	fr	@uqo.ca	analyse de réseau social par le analyse formel de concepts  le analyse formel de concept ( AFC ) être un formalisme de représentation et d' extraction de connaissance fonder sur le notion de concept et de treillis de concept ( Galois ) . le AFC avoir être exploiter avec succès dans plusieurs domaine en informatique tel le génie logiciel , le base et entrepôt de donnée , le extraction et le gestion de le connaissance et dans plusieurs application du monde réel comme le médecine , le psychologie , le linguistique et le sociologie . Dans ce présentation , nous aller explorer le potentiel de le AFC et de quelque extension de ce théorie ( ex. analyse triadique de concept ) dans le analyse de réseau social en vue de découvrir un connaissance à partir de réseau homogène simple ( ex. détection de communauté et d' individu influent à partir d' un réseau d' ami ) ou même de réseau hétérogène ( ex. extraction de règle d' association d' un réseau bibliographique ) . 	Analyse de réseaux sociaux par l'analyse formelle de concepts	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse des réclamations d'allocataires de la CAF : un cas d'étude en fouille de données	La gestion des réclamations est un élément fondamental dans la relation client. C'est le cas en particulier pour la Caisse Nationale des Allocations Familiales qui veut mettre en place une politique nationale pour faciliter cette gestion. Dans cet article, nous décrivons la démarche que nous avons adoptée afin de traiter automatiquement les réclamations provenant d'allocataires de la CAF du Rhône. Les données brutes mises à notre disposition nécessitent une série importante de prétraitements pour les rendre utilisables. Une fois ces données correctement nettoyées, des techniques issues de l'analyse des données et de l'apprentissage non supervisé nous permettent d'extraire à la fois une typologie des réclamations basée sur leur contenu textuel mais aussi une typologie des allocataires réclamants. Après avoir présenté ces deux typologies, nous les mettons en correspondance afin de voir comment les allocataires se distribuent selon les différents types de réclamation.	Sabine Loudcher, Julien Velcin, Vincent Forissier, Cyril Broilliard, Philippe Simonnot	http://editions-rnti.fr/render_pdf.php?p1&p=1001866		254	fr	fr	@univ-lyon2.fr, @cnedi69.cnafmail.fr, @cafrhone.cnafmail.fr	analyse des réclamation d' allocataire de le CAF : un cas d' étude en fouille de données  le gestion des réclamation être un élément fondamental dans le relation client . C' être le cas en particulier pour le caisse national des allocation familiale qui vouloir mettre en place un politique national pour faciliter ce gestion . Dans ce article , nous décrire le démarche que nous avoir adopter afin de traiter automatiquement le réclamation provenir d' allocataire de le CAF du Rhône . le donnée brut mettre à son disposition nécessiter un série important de prétraitements pour les rendre utilisable . un foi ce donnée correctement nettoyer , un technique issu de le analyse des donnée et de le apprentissage non superviser nous permettre d' extraire à le foi un typologie des réclamation baser sur son contenu textuel mais aussi un typologie des allocataire réclamants . Après avoir présenter ce deux typologie , nous les mettre en correspondance afin de voir comment le allocataire clr distribuer selon le différent type de réclamation . 	Analyse des réclamations d'allocataires de la CAF : un cas d'étude en fouille de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse Relationnelle de Concepts pour l'exploration de données relationnelles	L'Analyse Relationnelle de Concepts (ARC) est une extension de l'Analyse Formelle de Concepts (AFC), une méthode de classification non supervisée d'objets sous forme de treillis de concepts. L'ARC supporte en plus la gestion de relations entre objets de différents contextes ce qui permet d'établir des liens entre les concepts des différents treillis. Cette particularité lui permet d'être plus intuitive à utiliser pour extraire des connaissances à partir de données relationnelles et de donner des résultats plus riches. Malheureusement lorsque les jeux de données présentent de nombreuses relations, les résultats obtenus sont difficilement exploitables et des problèmes de passages à l'échelle se posent. Nous proposons dans cet article une adaptation possible de l'ARC pour explorer les relations de manière supervisée pour augmenter la pertinence des résultats obtenus et réduire le temps de calcul. Nous prenons pour exemple des données hydrobiologiques ayant trait à la qualité des milieux aquatiques.	Xavier Dolques, Florence Le Ber, Marianne Huchard, Clémentine Nebut	http://editions-rnti.fr/render_pdf.php?p1&p=1001829		255	fr	fr	@unistra.fr, @lirmm.fr	analyse Relationnelle de concept pour le exploration de donnée relationnelles  le analyse Relationnelle de Concepts ( ARC ) être un extension de le analyse Formelle de Concepts ( AFC ) , un méthode de classification non superviser d' objet sous forme de treillis de concept . le ARC supporter en plus le gestion de relation entre objet de différent contexte ce qui permettre d' établir un lien entre le concept des différent treillis . ce particularité lui permettre d' être plus intuitif à utiliser pour extraire un connaissance à partir de donnée relationnel et de donner un résultat plus riche . malheureusement lorsque le jeu de donnée présenter un nombreux relation , le résultat obtenir être difficilement exploitable et des problème de passage à le échelle clr poser . Nous proposer dans ce article un adaptation possible de le ARC pour explorer le relation de manière superviser pour augmenter le pertinence des résultat obtenir et réduire le temps de calcul . Nous prendre pour exemple des donnée hydrobiologiques avoir trait à le qualité des milieu aquatique . 	Analyse Relationnelle de Concepts pour l'exploration de données relationnelles	3
Revue des Nouvelles Technologies de l'Information	EGC	2013	Approche orientée objet sémantique et coopérative pour la classification des images de zones urbaines à très haute résolution	La classification orientée objet (COO) prend de plus en plus de dimension dans les travaux de télédétection grâce à sa capacité d'intégrer des connaissances de haut niveau telles que la taille, la forme et les informations de voisinage. Cependant, les approches existantes restent tributaires de l'étape de construction des objets à cause de l'absence d'interaction entre celle-ci et celle de leur identification. Dans cet article, nous proposons une approche sémantique, hiérarchique et collaborative entre les algorithmes de croissances de régions et une classification orientée objet supervisée, permettant une coopération entre l'extraction et l'identification des objets de l'image. Les expériences menées sur une image de très haute résolution de la région de Strasbourg ont confirmé l'intérêt de l'approche introduite.	Aymen Sellaouti, Atef Hamouda, Aline Deruyver, Cédric Wemmert	http://editions-rnti.fr/render_pdf.php?p1&p=1001827		256	fr	fr	@gmail.com, @lsiit.u-strasbg.fr, @yahoo.fr, @unistra.fr	approche orienter objet sémantique et coopératif pour le classification des image de zone urbain à très haut résolution  le classification orienter objet ( COO ) prendre de plus en plus de dimension dans le travail de télédétection grâce à son capacité d' intégrer un connaissance de haut niveau tel que le taille , le forme et le information de voisinage . cependant , le approche existant rester tributaire de le étape de construction des objet à cause de le absence d' interaction entre celui _-ci et celui de son identification . Dans ce article , nous proposer un approche sémantique , hiérarchique et collaboratif entre le algorithme de croissance de région et un classification orienter objet superviser , permettre un coopération entre le extraction et le identification des objet de le image . le expérience mener sur un image de très haut résolution de le région de Strasbourg avoir confirmer le intérêt de le approche introduire . 	Approche orientée objet sémantique et coopérative pour la classification des images de zones urbaines à très haute résolution	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Classification multi-étiquettes pour l'alignement multiple de séquences protéiques	Cet article présente une application de classification multi-étiquettes permettant de déterminer le programme à utiliser pour construire un alignement multiple d'un ensemble de séquences protéiques donné. Dans un premier temps, nous avons réussi à améliorer le système existant, Alexsys en ajoutant des attributs. Dans un second temps, nous déterminons pour un ensemble de séquences protéiques donné le ou les aligneurs capable de produire les alignements de meilleur score, à epsilon près. Les mesures de performances propres à la classification multi-étiquette nous permettent d'analyser l'influence de epsilon et de choisir une valeur assez petite pour distinguer les meilleurs aligneurs des autres.	Lina Fahed, Gabriel Frey, Julie Dawn Thompson, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1001864		257	fr	fr	@gmail.com, @unistra.fr, @unistra.fr, @igbmc.fr	classification multi-étiquettes pour le alignement multiple de séquence protéiques  ce article présenter un application de classification multi-étiquettes permettre de déterminer le programme à utiliser pour construire un alignement multiple d' un ensemble de séquence protéique donner . Dans un premier temps , nous avoir réussir à améliorer le système existant , Alexsys en ajouter un attribut . Dans un second temps , nous déterminer pour un ensemble de séquence protéique donner le ou le aligneurs capable de produire le alignement de meilleur score , à epsilon près . le mesure de performance propre à le classification multi-étiquette nous permettre d' analyser le influence de epsilon et de choisir un valeur assez petit pour distinguer le meilleur aligneurs des autre . 	Classification multi-étiquettes pour l'alignement multiple de séquences protéiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Classifications croisées de données de trajectoires contraintes par un réseau routier	Le clustering (ou classification non supervisée) de trajectoires a fait l'objet d'un nombre considérable de travaux de recherche. La majorité de ces travaux s'est intéressée au cas où les objets mobiles engendrant ces trajectoires se déplacent librement dans un espace euclidien et ne prennent pas en compte les contraintes liées à la structure sous-jacente du réseau qu'ils parcourent (ex. réseau routier). Dans le présent article, nous proposons au contraire la prise en compte explicite de ces contraintes. Nous représenterons les relations entre trajectoires et segments routiers par un graphe biparti et nous étudierons la classification de ses sommets. Nous illustrerons, sur un jeu de données synthétiques, l'utilité d'une telle étude pour comprendre la dynamique du mouvement dans le réseau routier et analyser le comportement des véhicules qui l'empruntent.	Mohamed K. El Mahrsi, Romain Guigourès, Fabrice Rossi, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001854		258	fr	fr	@telecom-paristech.fr, @univ-paris1.fr, @univ-paris1.fr, @univ-paris1.fr, @orange.com, @orange.com	classification croiser de donnée de trajectoire contraindre par un réseau routier  le clustering ( ou classification non superviser ) de trajectoire avoir faire le objet d' un nombre considérable de travail de recherche . le majorité de ce travail clr être intéresser au cas où le objet mobile engendrer ce trajectoire clr déplacer librement dans un espace euclidien et ne prendre pas en compte le contrainte lier à le structure sous-jacent du réseau qu' ils parcourir ( ex. réseau routier ) . Dans le présent article , nous proposer au contraire le prise en compte explicite de ce contrainte . Nous représenter le relation entre trajectoire et segment routier par un graphe biparti et nous étudier le classification de son sommet . Nous illustrer , sur un jeu de donnée synthétique , le utilité d' un tel étude pour comprendre le dynamique du mouvement dans le réseau routier et analyser le comportement des véhicule qui l' emprunter . 	Classifications croisées de données de trajectoires contraintes par un réseau routier	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Comprendre et interpréter les données : enjeux et implantations d'un système de codage dans des gisements de données historiques	L'accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd'hui deux grands types de difficultés aux historiens.Le premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante. C'est par exemple le cas pour l'intégration d'un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au Lamop, ou même dans le cadre d'un projet dont le seul lien est une problématique spatiale et temporelle (projet ANR-DFG, Euroscientia).Le deuxième tient en la nature des données introduites dans ces différents systèmes : elles sont souvent hétérogènes, ambiguës, floues. Pour que le chercheur puisse se les approprier, les données doivent faire l'objet d'un véritable travail, afin de comprendre comment elles ont été obtenues, structurées. L'historien doit donc les évaluer et les valider s'il souhaite les mettre en relation. Cette évaluation nécessitant, elle-même de pouvoir être commentée, partagée et critiquée par d'autres chercheurs.Dans les deux cas, il est nécessaire de développer des outils d'appropriation, qui permettent d'entrer dans le réel historique contenu dans les stocks de données. C'est là la fonction du projet Histobase, un système permettant d'entrer dans la structuration des gisements, d'en évaluer l'information, d'ajouter des couches d'interprétation (qualification de l'information historique) de les évaluer et de partager les données « obtenues ». Chacune des analyses individuelles et collectives fait l'objet d'une mémorisation. Il faut pour cela laisser une place importante aux historiens en tant qu'expert en prêtant une attention particulière aux processus métiers qu'ils mettent en oeuvre.	Stéphane Lamassé, Julien Alerini	http://editions-rnti.fr/render_pdf.php?p1&p=1001816		259	fr	fr	@univ-paris1.fr, @wanadoo.fr	comprendre et interpréter le donnée : enjeu et implantation d' un système de codage dans un gisement de donnée historiques  le accès croissant à un information pléthorique et le développement de gisement de donnée ambitieux poser aujourd' hui deux grand type de difficulté aux historien . le premier consister à mettre en relation des gisement qui avoir être développer de manière indépendant . C' être par exemple le cas pour le intégration d' un ensemble de base de donnée prosopographiques développer entre 1980 et 2010 au Lamop , ou même dans le cadre d' un projet dont le seul lien être un problématique spatial et temporel ( projet ANR-DFG , Euroscientia ) . le deuxième tenir en le nature des donnée introduire dans ce différent système : elles être souvent hétérogène , ambigu , flou . Pour que le chercheur pouvoir clr les approprier , le donnée devoir faire le objet d' un véritable travail , afin de comprendre comment elles avoir être obtenir , structurer . le historien devoir donc les évaluer et les valider clr il souhaiter les mettre en relation . ce évaluation nécessiter , lui-même de pouvoir être commenter , partager et critiquer par un autre chercheur . Dans le deux cas , il être nécessaire de développer un outil d' appropriation , qui permettre d' entrer dans le réel historique contenir dans le stock de donnée . C' être là le fonction du projet Histobase , un système permettre d' entrer dans le structuration des gisement , d' en évaluer le information , d' ajouter un couche d' interprétation ( qualification de le information historique ) de les évaluer et de partager le donnée « obtenir » . chacun des analyse individuel et collectif faire le objet d' un mémorisation . Il faillir pour cela laisser un place important aux historien en tant qu' expert en prêter un attention particulier aux processus métier qu' ils mettre en oeuvre . 	Comprendre et interpréter les données : enjeux et implantations d'un système de codage dans des gisements de données historiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Construction de descripteurs à partir du coclustering pour la classification supervisée de séries temporelles	Nous présentons un processus de construction de descripteurs pour la classification supervisée de séries temporelles. Ce processus est libre de tout paramétrage utilisateur et se décompose en trois étapes : (i) à partir des données originales, nous générons de multiples nouvelles représentations simples ; (ii) sur chacune de ces représentations, nous appliquons un algorithme de coclustering ; (iii) à partir des résultats de co-clustering, nous construisons de nouveaux descripteurs pour les séries temporelles. Nous obtenons une nouvelle base de données objets-attributs dont les objets (identifiant les séries temporelles) sont décrits par des attributs issus des diverses représentations générées. Nous utilisons un classifieur Bayésien sur cette nouvelle base de données. Nous montrons expérimentalement que ce processus offre de très bonnes performances prédictives comparées à l'état de l'art.	Dominique Gay, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001855		260	fr	fr	@orange.com	construction de descripteur à partir du coclustering pour le classification superviser de série temporelles  Nous présenter un processus de construction de descripteur pour le classification superviser de série temporel . ce processus être libre de tout paramétrage utilisateur et clr décomposer en trois étape : ( i ) à partir un donnée original , nous générer de multiple nouveau représentation simple ; ( ii ) sur chacun de ce représentation , nous appliquer un algorithme de coclustering ; ( iii ) à partir un résultat de co- clustering , nous construire de nouveau descripteur pour le série temporel . Nous obtenir un nouveau base de donnée objets-attributs dont le objet ( identifier le série temporel ) être décrire par un attribut issu des divers représentation générer . Nous utiliser un classifieur Bayésien sur ce nouveau base de donnée . Nous montrer expérimentalement que ce processus offrir de très bon performance prédictif comparer à le état de le art . 	Construction de descripteurs à partir du coclustering pour la classification supervisée de séries temporelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Découverte des soft-skypatterns avec une approche PPC	Les skypatterns sont des motifs traduisant des préférences de l'utilisateur selon une relation de dominance. Dans cet article, nous introduisons la notion de souplesse dans la problématique des skypatterns et nous montrons comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement. Nous proposons une méthode efficace d'extraction de skypatterns ainsi que de soft-skypatterns, méthode fondée sur la programmation par contraintes. La pertinence de notre approche est illustrée à travers une étude de cas en chémoinformatique pour la découverte de toxicophores.	Willy Ugarte, Patrice Boizumault, Samir Loudni, Bruno Crémilleux, Alban Lepailleur	http://editions-rnti.fr/render_pdf.php?p1&p=1001838		261	fr	fr	@unicaen.fr	découverte des soft-skypatterns avec un approche PPC  le skypatterns être un motif traduire un préférence de le utilisateur selon un relation de dominance . Dans ce article , nous introduire le notion de souplesse dans le problématique des skypatterns et nous montrer comment celui _-ci permettre de découvrir un motif intéressant qui être manquer autrement . Nous proposer un méthode efficace d' extraction de skypatterns ainsi que de soft-skypatterns , méthode fonder sur le programmation par contrainte . le pertinence de son approche être illustrer à travers un étude de cas en chémoinformatique pour le découverte de toxicophores . 	Découverte des soft-skypatterns avec une approche PPC	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Detecting Academic Plagiarism with Graphs	In this paper, we tackle the problem of detecting academic plagiarism, which is considered as a severe problem owing to the convenience of online publishing. Typical information retrieval methods, stopword-based methods and ngerprinting methods, are commonly used to detect plagiarism by using the sequence of words as they appear in the article. As such, they fail to detect plagiarism when an author reconstructs a source article by re-ordering and recombining phrases. Because graph structure ts for representing relationships between entities, we propose a novel plagiarism detection method, in which we use graphs to represent documents by modeling grammatical relationships between words. Experimental results show that our proposed method outperforms two n-gram methods and increases recall values by 10 to 20%.	Bin-Hui Chou, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001848		262	en	en	@i.kyushu-u.ac.jp, @inf.kyushu-u.ac.jp	detect academic plagiarism graph paper tackle problem detect academic plagiarism consider severe problem owing convenience online publish typical information retrieval method stopword base method ngerprint method commonly used detect plagiarism used sequence word appear article such fail detect plagiarism author reconstruct source article re order recombining phrase graph structure t represent relationship entity propose novel plagiarism detection method use graph represent document modele grammatical relationship word experimental result show proposed method outperform two n gram method increase recall value 10 20	Detecting Academic Plagiarism with Graphs	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Détection efficace des traverses minimales d'un hypergraphe par élimination de la redondance	L'extraction des traverses minimales d'un hypergraphe est une problématique réputée comme particulièrement difficile et qui a fait l'objet de plusieurs travaux dans la littérature. Dans cet article, nous établissons un lien entre les concepts de la fouille de données et ceux de la théorie des hypergraphes, proposant ainsi un cadre méthodologique pour le calcul des traverses minimales. Le nombre de ces traverses minimales étant, souvent, exponentiel même pour des hypergraphes simples, nous proposons d'en représenter l'ensemble de manière concise et exacte. Pour ce faire, nous introduisons la notion de traverses minimales irrédondantes, à partir desquelles nous pouvons retrouver l'ensemble global de toutes les traverses minimales, à l'aide de l'algorithme IMT-EXTRACTOR. Une étude expérimentale de ce nouvel algorithme a confirmé l'intérêt de l'approche introduite.	Mohamed Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001833		263	fr	fr	@fst.rnu.tn, @univ-st-etienne.fr	détection efficace des traverse minimal d' un hypergraphe par élimination de le redondance  le extraction des traverse minimal d' un hypergraphe être un problématique réputer comme particulièrement difficile et qui avoir faire le objet de plusieurs travail dans le littérature . Dans ce article , nous établir un lien entre le concept de le fouille de donnée et celui de le théorie des hypergraphes , proposer ainsi un cadre méthodologique pour le calcul des traverse minimal . le nombre de ce traverse minimal être , souvent , exponentiel même pour un hypergraphes simple , nous proposer d' en représenter le ensemble de manière concis et exact . Pour ce faire , nous introduire le notion de traverse minimal irrédondantes , à partir desquelles nous pouvoir retrouver le ensemble global de tout le traverse minimal , à le aide de le algorithme IMT-EXTRACTOR . un étude expérimental de ce nouveau algorithme avoir confirmer le intérêt de le approche introduire . 	Détection efficace des traverses minimales d'un hypergraphe par élimination de la redondance	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Détection précoce de tendances produits dans le cadre des activités commerciales de la grande distribution	"Dans ce papier, nous présentons une nouvelle approche qui permet la détection précoce de tendances ""produits"" dans le cadre des activités commerciales de la grande distribution. S'agissant d'un domaine où la concurrence est très vive entre les différentes enseignes avec des enjeux financiers colossaux, les stratégies commerciales ont pour principal objectif de fidéliser la clientèle pour limiter leur défection. C'est là qu'intervient la détection des changements de tendances produits, qui va permettre d'anticiper l'attrition de la clientèle. Déceler des tendances suffisamment tôt permettra aux décideurs de mettre en place des stratégies préventives efficaces à moindre coût. Notre objectif est donc d'analyser et de modéliser clairement les changements de tendances et leurs impacts potentiels globaux sur les achats des clients. Nous illustrerons notre approche sur des données réelles d'achats de clients d'une grande enseigne."	Gaël Bardury, Jean-Emile Symphor	http://editions-rnti.fr/render_pdf.php?p1&p=1001840		264	fr	fr	@gmail.com, @univ-ag.fr	détection précoce de tendance produire dans le cadre des activité commercial de le grand distribution  " Dans ce papier , nous présenter un nouveau approche qui permettre le détection précoce de tendance " " produit " " dans le cadre des activité commercial de le grand distribution . . clr agir d' un domaine où le concurrence être très vif entre le différent enseigne avec un enjeu financier colossal , le stratégie commercial avoir pour principal objectif de fidéliser le clientèle pour limiter son défection . . C' être là qu' intervenir le détection des changement de tendance produit , qui aller permettre d' anticiper le attrition de le clientèle . . Déceler des tendance suffisamment tôt permettre aux décideur de mettre en place des stratégie préventif efficace à moindre coût . . son objectif être donc d' analyser et de modéliser clairement le changement de tendance et son impact potentiel global sur le achat des client . . Nous illustrer son approche sur un donnée réel d' achat de client d' un grand enseigne . " 	Détection précoce de tendances produits dans le cadre des activités commerciales de la grande distribution	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Enrichissement d'ontologies grâce à l'annotation sémantique de pages web	Nous présentons une approche pour enrichir automatiquement une ontologie à partir d'un ensemble de pages web structurées. Cette approche s'appuie sur un noyau d'ontologie initial. Son originalité est d'exploiter conjointement la structure des documents et des annotations sémantiques produites à l'aide du noyau d'ontologie pour identifier de nouveaux concepts et des spécialisations de relations qui enrichissent l'ontologie. Nous avons implémenté et évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage.	Nathalie Aussenac-Gilles, Davide Buscaldi, Catherine Comparot, Mouna Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1001839		265	fr	fr	@irit.fr, @univ-paris13.fr	enrichissement d' ontologie grâce à le annotation sémantique de page web  Nous présenter un approche pour enrichir automatiquement un ontologie à partir d' un ensemble de page web structurer . ce approche clr appuyer sur un noyau d' ontologie initial . son originalité être d' exploiter conjointement le structure des document et des annotation sémantique produire à le aide du noyau d' ontologie pour identifier un nouveau concept et des spécialisation de relation qui enrichir le ontologie . Nous avoir implémenter et évaluer ce processus en réaliser un ontologie de plante à partir de fiche de jardinage . 	Enrichissement d'ontologies grâce à l'annotation sémantique de pages web	2
Revue des Nouvelles Technologies de l'Information	EGC	2013	Étude des corrélations spatio-temporelles des appels mobiles en France	Nous proposons dans cet article de présenter une application d'analyse d'une base de données de grande taille issue du secteur des télécommunications. Le problème consiste à segmenter un territoire et caractériser les zones ainsi définies grâce au comportement des habitants en terme de téléphonie mobile. Nous disposons pour cela d'un réseau d'appels inter-antennes construit pendant une période de cinq mois sur l'ensemble de la France. Nous proposons une analyse en deux phases. La première couple les antennes émettrices dont les appels sont similairement distribués sur les antennes réceptrices et vice versa. Une projection de ces groupes d'antennes sur une carte de France permet une visualisation des corrélations entre la géographie du territoire et le comportement de ses habitants en terme de téléphonie. La seconde phase découpe l'année en périodes entre lesquelles on observe un changement de distributions d'appels sortant des groupes d'antennes. On peut ainsi caractériser l'évolution temporelle du comportement des usagers de mobiles dans chacune des zones du pays.	Romain Guigourès, Marc Boullé, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1001865		266	fr	fr	@orange.com, @univ-paris1.fr	Étude des corrélation spatio- temporel des appel mobile en France  Nous proposer dans ce article de présenter un application d' analyse d' un base de donnée de grand taille issue du secteur des télécommunication . le problème consister à segmenter un territoire et caractériser le zone ainsi définir grâce au comportement des habitant en terme de téléphonie mobile . Nous disposer pour cela d' un réseau d' appel inter-antennes construire pendant un période de cinq moi sur le ensemble de le France . Nous proposer un analyse en deux phase . le premier coupler le antenne émetteur dont le appel être similairement distribuer sur le antenne récepteur et vice verser . un projection de ce groupe d' antenne sur un carte de France permettre un visualisation des corrélation entre le géographie du territoire et le comportement de son habitant en terme de téléphonie . le second phase découper le année en période entre lesquelles on observer un changement de distribution d' appel sortant des groupe d' antenne . On pouvoir ainsi caractériser le évolution temporel du comportement des usager de mobile dans chacun des zone du pays . 	Étude des corrélations spatio-temporelles des appels mobiles en France	3
Revue des Nouvelles Technologies de l'Information	EGC	2013	Étude des techniques d'oubli dans les moindres carrés récursifs pour l'apprentissage incrémental de systèmes d'inférence floue évolutifs : application à la reconnaissance de formes	Cet article étudie les possibilités d'utilisation d'oubli dans l'apprentissage incrémental en-ligne de classifieurs évolutifs basés sur des systèmes d'inférence floue. Pour cela, nous étudions différentes possibilités, existant dans la littérature dédiée au contrôle, pour introduire de l'oubli dans l'algorithme des moindres carrés récursifs. Nous présentons l'impact de ces différentes techniques dans le contexte de l'apprentissage incrémental de classifieurs évolutifs en environnement non stationnaire. Ces approches sont évaluées, pour l'optimisation des systèmes d'inférence floue, sur la problématique de la reconnaissance de gestes manuscrits sur surface tactile.	Manuel Bouillon, Eric Anquetil, Abdullah Almaksour	http://editions-rnti.fr/render_pdf.php?p1&p=1001818		267	fr	fr	@irisa.fr	Étude des technique d' oubli dans le moindre carrer récursif pour le apprentissage incrémental de système d' inférence flou évolutif : application à le reconnaissance de formes  ce article étudier le possibilité d' utilisation d' oubli dans le apprentissage incrémental en-ligne de classifieur évolutif baser sur un système d' inférence flou . Pour cela , nous étudier différent possibilité , existant dans le littérature dédier au contrôle , pour introduire de le oubli dans le algorithme des moindre carrer récursif . Nous présenter le impact de ce différent technique dans le contexte de le apprentissage incrémental de classifieur évolutif en environnement non stationnaire . ce approche être évaluer , pour le optimisation des système d' inférence flou , sur le problématique de le reconnaissance de geste manuscrit sur surface tactile . 	Étude des techniques d'oubli dans les moindres carrés récursifs pour l'apprentissage incrémental de systèmes d'inférence floue évolutifs : application à la reconnaissance de formes	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Évolution d'une ontologie dédiée à la représentation de relations n-aires	Nous nous intéressons dans cet article à la problématique d'évolution d'une ontologie permettant de représenter des relations n-aires. Nous présentons la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence structurelle. Nous illustrerons nos propos sur une ontologie dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives.	Rim Touhami, Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu	http://editions-rnti.fr/render_pdf.php?p1&p=1001862		268	fr	fr	@risk, @agroparistech.fr, @supagro.inra.fr, @agroparistech.fr	Évolution d' un ontologie dédier à le représentation de relation n-aires  Nous nous intéresser dans ce article à le problématique d' évolution d' un ontologie permettre de représenter un relation n-aires . Nous présenter le représentation formel des changement applicable à son ontologie permettre de modifier son structure tout en maintenir son cohérence structurel . Nous illustrer son propos sur un ontologie dédier à le représentation de relation n-aires entre un donnée expérimental quantitatif . 	Évolution d'une ontologie dédiée à la représentation de relations n-aires	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction de motifs condensés dans un unique graphe orienté acyclique attribué	Les graphes orientés acycliques attribués peuvent être utilisés dans beaucoup de domaines applicatif. Dans ce papier, nous étudions un nouveau domaine de motif pour permettre leur analyse : les chemins pondérés fréquents. Nous proposons en conséquence des contraintes primitives permettant d'évaluer leur pertinence (par exemple, les contraintes de fréquence et de compacité), et un algorithme extrayant ces solutions. Nous aboutissons à une représentation condensée dont l'efficacité et le passage à l'échelle sont étudiés empiriquement.	Jérémy Sanhes, Frédéric Flouvat, Claude Pasquier, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001837		269	fr	fr	@univ-nc.nc, @unice.fr	extraction de motif condenser dans un unique graphe orienter acyclique attribué  le graphe orienter acyclique attribuer pouvoir être utiliser dans beaucoup de domaine applicatif . Dans ce papier , nous étudier un nouveau domaine de motif pour permettre son analyse : le chemin pondérer fréquent . Nous proposer en conséquence des contrainte primitif permettre d' évaluer son pertinence ( par exemple , le contrainte de fréquence et de compacité ) , et un algorithme extraire ce solution . Nous aboutir à un représentation condenser dont le efficacité et le passage à le échelle être étudier empiriquement . 	Extraction de motifs condensés dans un unique graphe orienté acyclique attribué	2
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction de motifs fréquents dans des arbres attribués	L'extraction de motifs fréquents est une tâche importante en fouille de données. Initialement centrés sur la découverte d'ensembles d'items fréquents, les premiers travaux ont été étendus pour extraire des motifs structurels comme des séquences, des arbres ou des graphes. Dans cet article, nous proposons une nouvelle méthode de fouille de données qui consiste à extraire de nouveaux types de motifs à partir d'une collection d'arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des ensembles d'attributs. L'extraction de ces motifs (appelés sous-arbres attribués) combine une recherche d'ensembles d'items fréquents à une recherche de sous-arbres et nécessite d'explorer un immense espace de recherche. Nous présentons plusieurs nouveaux algorithmes d'extraction d'arbres attribués et montrons que leurs implémentations peuvent efficacement extraire des motifs fréquents à partir de grands jeux de données.	Claude Pasquier, Jérémy Sanhes, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001836		270	fr	fr	@univ-nc.nc, @unice.fr	extraction de motif fréquent dans un arbre attribués  le extraction de motif fréquent être un tâche important en fouille de donnée . initialement centrer sur le découverte d' ensemble d' item fréquent , le premier travail avoir être étendre pour extraire un motif structurel comme un séquence , un arbre ou des graphe . Dans ce article , nous proposer un nouveau méthode de fouille de donnée qui consister à extraire un nouveau type de motif à partir d' un collection d' arbre attribuer . le arbre attribuer être un arbre dans lesquels le noeud être associer à un ensemble d' attribut . le extraction de ce motif ( appeler sous-arbres attribuer ) combiner un recherche d' ensemble d' item fréquent à un recherche de sous-arbres et nécessiter d' explorer un immense espace de recherche . Nous présenter plusieurs nouveau algorithme d' extraction d' arbre attribuer et montrer que son implémentation pouvoir efficacement extraire un motif fréquent à partir un grand jeu de donnée . 	Extraction de motifs fréquents dans des arbres attribués	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction des nombres de Betti avec un modèle génératif	L'analyse exploratoire de données multidimensionnelles est un problème complexe. Nous proposons d'extraire certains invariants topologiques appelés nombre de Betti, pour synthétiser la topologie de la structure sous-jacente aux données. Nous définissons un modèle génératif basé sur le complexe simplicial de Delaunay dont nous estimons les paramètres par l'optimisation du critère d'information Bayésien (BIC). Ce Complexe Simplicial Génératif nous permet d'extraire les nombres de Betti de données jouets et d'images d'objets en rotation. Comparé à la technique géométrique des Witness Complex, le CSG apparait plus robuste aux données bruitées.	Maxime Maillot, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001826		271	fr	fr	@cea.fr, @cea.fr, @utc.fr	extraction des nombre de Betti avec un modèle génératif  le analyse exploratoire de donnée multidimensionnel être un problème complexe . Nous proposer d' extraire certain invariants topologique appeler nombre de Betti , pour synthétiser le topologie de le structure sous-jacent aux donnée . Nous définir un modèle génératif baser sur le complexe simplicial de Delaunay dont nous estimer le paramètre par le optimisation du critère d' information Bayésien ( BIC ) . ce complexe Simplicial Génératif nous permettre d' extraire le nombre de Betti de donnée jouet et d' image d' objet en rotation . comparer à le technique géométrique des Witness Complex , le CSG apparait plus robuste aux donnée bruiter . 	Extraction des nombres de Betti avec un modèle génératif	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction et filtrage de syntagmes nominaux pour la Recherche d'Information	Nous proposons dans cet article un Système de Recherche d'Information (SRI) qui se base sur des techniques d'indexation de textes en langue naturelle. Nous présentons une méthode d'indexation de documents qui repose sur une approche hybride pour la sélection de descripteurs textuels. Cette approche emploie des traitements du langage naturel pour l'extraction des syntagmes nominaux et sur un filtrage statistique basé sur l'information mutuelle pour sélectionner les syntagmes nominaux les plus informatifs pour le processus d'indexation. Nous effectuons des expérimentations en utilisant le corpus Le Monde 94 de la collection CLEF 2001 et sur le SRI Lemur pour évaluer l'approche proposée.	Chedi Bechikh Ali, Hatem Haddad	http://editions-rnti.fr/render_pdf.php?p1&p=1001842		272	fr	fr	@gmail.com, @gmail.com	extraction et filtrage de syntagme nominal pour le recherche d' Information  Nous proposer dans ce article un système de recherche d' Information ( SRI ) qui clr baser sur un technique d' indexation de texte en langue naturel . Nous présenter un méthode d' indexation de document qui reposer sur un approche hybride pour le sélection de descripteur textuel . ce approche employer des traitement du langage naturel pour le extraction des syntagme nominal et sur un filtrage statistique baser sur le information mutuel pour sélectionner le syntagme nominal le plus informatif pour le processus d' indexation . Nous effectuer un expérimentation en utiliser le corpus le Monde 94 de le collection clef 2001 et sur le SRI Lemur pour évaluer le approche proposer . 	Extraction et filtrage de syntagmes nominaux pour la Recherche d'Information	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction optimisée de Règles d'Association Positives et Négatives (RAPN)	La littérature s'est beaucoup intéressée à l'extraction de règles d'association positives et peu à l'extraction de règles négatives en raison essentiellement du coût de calculs et du nombre prohibitif de règles extraites qui sont pour la plupart redondantes et inintéressantes. Dans cet article, nous nous sommes intéressés aux algorithmes d'extraction de RAPN (Règles d'Association Positives et Négatives) reposant sur l'algorithme fondateur Apriori. Nous avons fait une étude de ceux-ci en mettant en évidence leurs avantages et leurs inconvénients. A l'issue de cette étude, nous avons proposé un nouvel algorithme qui améliore cette extraction au niveau du nombre et de la qualité des règles extraites et au niveau du parcours de recherche des règles. L'étude s'est terminée par une évaluation de cet algorithme sur plusieurs bases de données.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001832		273	fr	fr	@isima, @isima	extraction optimiser de règle d' association Positives et Négatives ( RAPN )  le littérature clr être beaucoup intéresser à le extraction de règle d' association positif et peu à le extraction de règle négatif en raison essentiellement du coût de calcul et du nombre prohibitif de règle extraire qui être pour le plupart redondant et inintéressant . Dans ce article , nous clr sommer intéresser aux algorithme d' extraction de RAPN ( règle d' association Positives et Négatives ) reposer sur le algorithme fondateur Apriori . Nous avoir faire un étude de celui _-ci en mettre en évidence son avantage et son inconvénient . A le issue de ce étude , nous avoir proposer un nouveau algorithme qui améliorer ce extraction au niveau du nombre et de le qualité des règle extraire et au niveau du parcours de recherche des règle . le étude clr être terminer par un évaluation de ce algorithme sur plusieurs base de donnée . 	Extraction optimisée de Règles d'Association Positives et Négatives (RAPN)	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Grille bivariée pour la détection de changement dans un flux étiqueté	Nous présentons une méthode en-ligne de détection de changement de concept dans un flux étiqueté. Notre méthode de détection est basée sur un critère supervisé bivarié qui permet d'identifier si les données de deux fenêtres proviennent ou non de la même distribution. Notre méthode a l'intérêt de n'avoir aucun a priori sur la distribution des données, ni sur le type de changement et est capable de détecter des changements de différentes natures (changement dans la moyenne, dans la variance...). Les expérimentations montrent que notre méthode est plus performante et robuste que les méthodes de l'état de l'art testées. De plus, à part la taille des fenêtres, elle ne requiert aucun paramètre utilisateur.	Christophe Salperwyck, Marc Boullé, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001859		274	fr	fr	@orange.com	grille bivariée pour le détection de changement dans un flux étiqueté  Nous présenter un méthode en-ligne de détection de changement de concept dans un flux étiqueter . son méthode de détection être baser sur un critère superviser bivarié qui permettre d' identifier si le donnée de deux fenêtre provenir ou non de le même distribution . son méthode avoir le intérêt de n' avoir aucun avoir priori sur le distribution des donnée , ni sur le type de changement et être capable de détecter un changement de différent nature ( changement dans le moyenne , dans le variance ... ) . le expérimentation montrer que son méthode être plus performant et robuste que le méthode de le état de le art tester . De plus , à part le taille des fenêtre , elle ne requérir aucun paramètre utilisateur . 	Grille bivariée pour la détection de changement dans un flux étiqueté	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Identification de compatibilités entre descripteurs de lieux et apprentissage automatique	Les travaux présentés dans cet article s'inscrivent dans le paradigme des recherches visant à acquérir des relations sémantiques à partir de folksonomies (ensemble de tags attribués à des ressources par des utilisateurs). Nous expérimentons plusieurs approches issues de l'état de l'art ainsi que l'apport de l'apprentissage automatique pour l'identification de relations entre tags. Nous obtenons dans le meilleur des cas un taux d'erreur de 23,7 % (relations non reconnues ou fausses), ce qui est encourageant au vu de la difficulté de la tâche (les annotateurs humains ont un taux de désaccord de 12%).	Estelle Delpech, Laurent Candillier, Léa Laporte, Samuel Phan	http://editions-rnti.fr/render_pdf.php?p1&p=1001850		275	fr	fr	@nomao.com, @ebuzzing.com, @irit.fr	identification de compatibilité entre descripteur de lieu et apprentissage automatique  le travail présenter dans ce article clr inscrire dans le paradigme des recherche viser à acquérir un relation sémantique à partir de folksonomies ( ensemble de tags attribuer à un ressource par un utilisateur ) . Nous expérimenter plusieurs approche issu de le état de le art ainsi que le apport de le apprentissage automatique pour le identification de relation entre tags . Nous obtenir dans le meilleur des cas un taux d' erreur de 23_,_7 \% ( relation non reconnaître ou faux ) , ce qui être encourageant au voir de le difficulté de le tâche ( le annotateur humain avoir un taux de désaccord de 12 \% ) . 	Identification de compatibilités entre descripteurs de lieux et apprentissage automatique	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Identification de complexes protéine-protéine par combinaison de classifieurs. Application à Escherichia Coli	Nous proposons une approche permettant de prédire des complexes impliquant trois protéines (appelés trimères) à partir de combinaison de classifieurs appris sur des complexes n'impliquant que deux protéines (dimères). La prédiction de ces trimères repose sur deux hypothèses biologiques : (i) deux protéines orthologues présentent des caractéristiques fonctionnelles similaires; (ii) deux protéines interagissant sous la forme d'un complexe sous-tendent une fonction biologique essentielle à l'espèce concernée. Ces deux hypothèses sont exploitées pour décrire chaque paire de protéines par l'ensemble des espèces pour lesquelles elles possèdent un orthologue. Un ensemble de mesures de qualité classiquement utilisées pour évaluer l'intérêt des règles d'association est utilisé pour évaluer la force du lien entre les deux protéines. L'organisme modèle Escherichia Coli a été utilisé pour évaluer notre approche.	Thomas Bourquard, Damien M. de Vienne, Jérôme Azé	http://editions-rnti.fr/render_pdf.php?p1&p=1001863		276	fr	fr	@tours.inra.fr, @crg.es, @lri.fr	identification de complexe protéine-protéine par combinaison de classifieur . application à Escherichia Coli  Nous proposer un approche permettre de prédire un complexe impliquer trois protéine ( appelé trimère ) à partir de combinaison de classifieur apprendre sur un complexe n' impliquer que deux protéine ( dimère ) . le prédiction de ce trimère reposer sur deux hypothèse biologique : ( i ) deux protéine orthologues présenter un caractéristique fonctionnel similaire ; ( ii ) deux protéine interagir sous le forme d' un complexe sous-tendent un fonction biologique essentiel à le espèce concerner . ce deux hypothèse être exploiter pour décrire chaque paire de protéine par le ensemble des espèce pour lesquelles elles posséder un orthologue . un ensemble de mesure de qualité classiquement utiliser pour évaluer le intérêt des règle d' association être utiliser pour évaluer le force du lien entre le deux protéine . le organisme modèle Escherichia Coli avoir être utiliser pour évaluer son approche . 	Identification de complexes protéine-protéine par combinaison de classifieurs. Application à Escherichia Coli	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Inférence de réseaux biologiques : un défi pour la fouille de données structurées	"La réponse cellulaire d'un organisme vivant à un signal donné, hormone, stress ou médicament, met en jeu des mécanismes complexes d'interaction et de régulation entre les gènes, les ARN messagers, les protéines et d'autres éléments tels que les micro-ARNs. On parle de réseau d'interaction pour décrire l'ensemble des interactions possibles entre protéines et de réseau de régulation génique pour représenter un ensemble de régulations entre gènes. Identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet d'envisager de mieux soigner par le biais du ciblage thérapeutique. Puisque les techniques expérimentales de mesure à grande échelle, récemment développées, fournissent des données d'observation de ces réseaux, ce problème d'identification de réseau, généralement appelé inférence de réseau en biologie des systèmes, s'inscrit dans le cadre général de la fouille de données et plus particulièrement de l'apprentissage artificiel. Voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes d'étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.En partant des deux problématiques distinctes que sont l'inférence de réseau d'interaction et l'inférence de réseau de régulation, je montrerai que ces deux tâches d'apprentissage posent, chacune de manière différente, la problématique de la prédiction de sorties structurées. L'inférence de réseau d'interaction entre protéines, vue comme un problème transductif de prédiction de liens, peut être résolue comme un problème d'apprentissage d'un noyau de sortie à partir d'un noyau d'entrée. L'inférence de réseau de régulation, impliquant la modélisation d'un système dynamique, peut être abordée par l'approximation parcimonieuse et structurée de fonctions à valeurs vectorielles. Je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de Hilbert, fondés sur des noyaux à valeur opérateur, qui fournissent d'excellents résultats en inférence de réseaux biologiques. Des expériences in silico sur des données artificielles, chez la levure du boulanger ou chez l'homme illustreront mes propos. En fin d'exposé, je tracerai quelques perspectives concernant les "" nouveaux "" défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées."	Florence D'Alche-Buc	http://editions-rnti.fr/render_pdf.php?p1&p=1001815		277	fr	fr	@ibisc.fr	inférence de réseau biologique : un défi pour le fouille de donnée structurées  " le réponse cellulaire d' un organisme vivre à un signal donner , hormone , stress ou médicament , mettre en jeu des mécanisme complexe d' interaction et de régulation entre le gène , le ARN messager , le protéine et un autre élément tel que le micro- ARNs . . On parler de réseau d' interaction pour décrire le ensemble des interaction possible entre protéine et de réseau de régulation génique pour représenter un ensemble de régulation entre gène . . Identifier ce interaction et ce régulation ouvrer le porte à un meilleur compréhension du vivre et permettre d' envisager de mieux soigner par le biais du ciblage thérapeutique . . Puisque le technique expérimental de mesure à grand échelle , récemment développer , fournir un donnée d' observation de ce réseau , ce problème d' identification de réseau , généralement appeler inférence de réseau en biologie des système , clr inscrire dans le cadre général de le fouille de donnée et plus particulièrement de le apprentissage artificiel . . voilà maintenant quelque année que ce problématique avoir être poser à son communauté et durant lesquelles le échange entre biologiste et informaticien avoir non seulement permettre aux biologiste d' étoffer son boîte à outil mais aussi aux informaticien de concevoir un nouveau méthode de fouille de donnée . . En partir un deux problématique distinct que être le inférence de réseau d' interaction et le inférence de réseau de régulation , je montrer que ce deux tâche d' apprentissage poser , chacun de manière différent , le problématique de le prédiction de sortie structurer . . le inférence de réseau d' interaction entre protéine , voir comme un problème transductif de prédiction de lien , pouvoir être résoudre comme un problème d' apprentissage d' un noyau de sortie à partir d' un noyau d' entrée . . le inférence de réseau de régulation , impliquer le modélisation d' un système dynamique , pouvoir être aborder par le approximation parcimonieux et structurer de fonction à valeur vectoriel . . Je présenter un ensemble de nouveau outil de régression à sortie dans un espace de Hilbert , fonder sur un noyau à valeur opérateur , qui fournir un excellent résultat en inférence de réseau biologique . . un expérience in silico sur un donnée artificiel , chez le levure du boulanger ou chez le homme illustrer son propos . . En fin d' exposé , je tracer quelque perspective concernant le " " nouveau " " défi dans le domaine de le bioinformatique et dans celui de le prédiction de sortie structurer . " 	Inférence de réseaux biologiques : un défi pour la fouille de données structurées	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Les capitalistes sociaux sur Twitter : détection via des mesures de similarité	Les réseaux sociaux tels que Twitter font partie du phénomène de Déluge des données, expression utilisée pour décrire l'apparition de données de plus en plus volumineuses et complexes. Pour représenter ces réseaux, des graphes orientés sont souvent utilisés. Dans cet article, nous nous focalisons sur deux aspects de l'analyse du réseau social de Twitter. En premier lieu, notre but est de trouver une méthode efficace et haut niveau pour stocker et manipuler le graphe du réseau social en utilisant des ressources informatiques raisonnables. Cet axe de recherche constitue un enjeu majeur puisqu'il est ainsi possible de traiter des graphes à échelle réelle sur des machines potentiellement accessibles par tous. Ensuite, nous étudions les capitalistes sociaux, un type particulier d'utilisateurs de Twitter observé par Ghosh et al. (2012). Nous proposons une méthode pour détecter et classifier efficacement ces utilisateurs.	Nicolas Dugué, Anthony Perez	http://editions-rnti.fr/render_pdf.php?p1&p=1001852		278	fr	fr		le capitaliste social sur Twitter : détection via un mesure de similarité  le réseau social tel que Twitter faire partie du phénomène de déluge des donnée , expression utiliser pour décrire le apparition de donnée de plus en plus volumineux et complexe . Pour représenter ce réseau , un graphe orienter être souvent utiliser . Dans ce article , nous clr focaliser sur deux aspect de le analyse du réseau social de Twitter . En premier lieu , son but être de trouver un méthode efficace et haut niveau pour stocker et manipuler le graphe du réseau social en utiliser un ressource informatique raisonnable . ce axe de recherche constituer un enjeu majeur puisqu' il être ainsi possible de traiter un graphe à échelle réel sur un machine potentiellement accessible par tout . ensuite , nous étudier le capitaliste social , un type particulier d' utilisateur de Twitter observer par Ghosh et al. ( 2012 ) . Nous proposer un méthode pour détecter et classifier efficacement ce utilisateur . 	Les capitalistes sociaux sur Twitter : détection via des mesures de similarité	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Modèle de Recherche d'Information Sociale Centré Utilisateur	L'émergence des réseaux sociaux a révolutionné leWeb en permettant notamment aux individus de prolonger leur connexion virtuelle en une relation plus réelle et de partager leurs connaissances. Ce nouveau contexte de diffusion de l'information sur le Web peut constituer un moyen efficace pour cerner les besoins en information des utilisateurs du Web, et permettre à la recherche d'information (RI) de mieux répondre à ces besoins en adaptant les modèles d'indexation et d'interrogation. L'exploitation des réseaux sociaux confronte la RI à plusieurs défis dont les plus importants concernent la représentation de l'information dans ce modèle social de RI et son évaluation, en l'absence de collections de test et de compétitions dédiées. Dans cet article, nous présentons un modèle de RI sociale dans lequel nous proposons de modéliser et d'exploiter le contexte social de l'utilisateur. Nous avons évalué notre modèle à l'aide d'une collection de test de RI sociale construite à partir des annotations du réseau social de bookmarking collaboratif Delicious.	Chahrazed Bouhini, Mathias Géry, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001846		279	fr	fr	@univ-st-etienne.fr	modèle de recherche d' information Sociale Centré Utilisateur  le émergence des réseau social avoir révolutionner leWeb en permettre notamment aux individu de prolonger son connexion virtuel en un relation plus réel et de partager son connaissance . ce nouveau contexte de diffusion de le information sur le Web pouvoir constituer un moyen efficace pour cerner le besoin en information des utilisateur du Web , et permettre à le recherche d' information ( RI ) de mieux répondre à ce besoin en adapter le modèle d' indexation et d' interrogation . le exploitation des réseau social confronter le RI à plusieurs défi dont le plus important concerner le représentation de le information dans ce modèle social de RI et son évaluation , en le absence de collection de test et de compétition dédier . Dans ce article , nous présenter un modèle de RI social dans lequel nous proposer de modéliser et d' exploiter le contexte social de le utilisateur . Nous avoir évaluer son modèle à le aide d' un collection de test de RI social construire à partir un annotation du réseau social de bookmarking collaboratif Delicious . 	Modèle de Recherche d'Information Sociale Centré Utilisateur	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Non-disjoint grouping of text documents based Word Sequence Kernel	This paper deals with two issues in text clustering which are the detection of non disjoint groups and the representation of textual data. In fact, a text document can discuss several themes and then, it must belong to several groups. The learning algorithm must be able to produce non disjoint clusters and assigns documents to several clusters. The second issue concerns the data representation. Textual data are often represented as a bag of features such as terms, phrases or concepts. This representation of text avoids correlation between terms and doesn't give importance to the order of words in the text. We propose a non supervised learning method able to detect overlapping groups in text document by considering text as a sequence of words and using the Word Sequence Kernel as similarity measure. The experiments show that the proposed method outperforms existing overlapping methods using the bag of word representation in terms of clustering accuracy and detect more relevant groups in textual documents.	Chiheb-Eddine Ben N'Cir, Afef Zenned, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001843		280	en	en	@isg.rnu.tn, @gmail.com, @isg.rnu.tn	non disjoint grouping text document base word sequence kernel paper deal two issue text cluster detection non disjoint group representation textual data fact text document discuss several theme then must belong several group learn algorithm must able produce non disjoint cluster assign document several cluster second issue concern data representation textual datum often represent bag feature term phrase concept representation text avoid correlation term doesn t give importance order word text propose non supervise learn method able detect overlapping group text document consider text sequence word used word sequence kernel similarity measure experiment show proposed method outperform exist overlapping method used bag word representation term cluster accuracy detect relevant group textual document	Non-disjoint grouping of text documents based Word Sequence Kernel	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Nouvelle approche de bi-partitionnement topologique	Dans ce papier, nous proposons une nouvelle approche topologique de bi-partitionnement (bi-clustering) appelée BiTM en utilisant les cartes autoorganisatrices. L'idée principale de l'approche est d'utiliser une seule carte pour le partitionnement simultané des lignes (observations) et des colonnes (variables). Contrairement aux approches utilisant les cartes topologiques, notre modèle ne nécessite pas de pré-traitement de la base de données. Ainsi, une nouvelle fonction de coût est proposée. De plus, BiTM fournit une visualisation topologique des blocs ou bi-clusters facilement interprétable. Les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique.	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001820		281	fr	fr	@univ-paris13.fr	nouveau approche de bi-partitionnement topologique  Dans ce papier , nous proposer un nouveau approche topologique de bi-partitionnement ( bi-clustering ) appeler BiTM en utiliser le carte autoorganisatrices . le idée principal de le approche être d' utiliser un seul carte pour le partitionnement simultané des ligne ( observation ) et des colonne ( variable ) . contrairement aux approche utiliser le carte topologique , son modèle ne nécessiter pas de pré-traitement de le base de donnée . ainsi , un nouveau fonction de coût être proposer . De plus , BiTM fournir un visualisation topologique des bloc ou bi-clusters facilement interprétable . le résultat obtenir être très encourageant et prometteur pour continuer dans ce optique . 	Nouvelle approche de bi-partitionnement topologique	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Paramétrage intelligent de l'alignement d'ontologies par l'intégrale de Choquet	Le nombre croissant d'ontologies rend le processus d'alignement une composante essentielle du Web sémantique. Plusieurs outils ont été conçus dans le but de produire des alignements. La qualité des alignements fournis par ces outils est étroitement liée à certains paramètres qui régissent leurs traitements. Dans ce papier, nous proposons une nouvelle approche permettant l'adaptation automatique des paramètres d'alignement d'ontologies par l'utilisation de l'intégrale de Choquet, comme un opérateur d'agrégation. Les expérimentations montrent une nette amélioration des résultats par rapport à un paramétrage statique et figé.	Marouen Kachroudi, Sami Zghal, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001857		282	fr	fr	@fst.rnu.tn, @fsjegj.rnu.tn	Paramétrage intelligent de le alignement d' ontologie par le intégral de Choquet  le nombre croissant d' ontologie rendre le processus d' alignement un composante essentiel du Web sémantique . plusieurs outil avoir être concevoir dans le but de produire un alignement . le qualité des alignement fournir par ce outil être étroitement lier à certain paramètre qui régir son traitement . Dans ce papier , nous proposer un nouveau approche permettre le adaptation automatique des paramètre d' alignement d' ontologie par le utilisation de le intégral de Choquet , comme un opérateur d' agrégation . le expérimentation montrer un net amélioration des résultat par rapport à un paramétrage statique et figer . 	Paramétrage intelligent de l'alignement d'ontologies par l'intégrale de Choquet	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Processus itératif d'extraction de classes en non supervisée	Nous proposons dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif. L'approche utilise une méthode d'extraction de classes basée sur la détection de limite de classe, chaque classe étant définie par son centre. Nous avons également défini des critères d'évaluation adaptés à la méthode proposée. Plusieurs expérimentations ont montré l'intérêt de l'approche dans divers problèmes.	Alexandre Blansché, Lydia Boudjeloud	http://editions-rnti.fr/render_pdf.php?p1&p=1001817		283	fr	fr	@univ-lorraine.fr	processus itératif d' extraction de classe en non supervisée  Nous proposer dans ce article un nouveau approche de classification non superviser où le classe être obtenir le un après le autre suivre un processus itératif . le approche utiliser un méthode d' extraction de classe baser sur le détection de limite de classe , chaque classe être définir par son centre . Nous avoir également définir un critère d' évaluation adapter à le méthode proposer . plusieurs expérimentation avoir montrer le intérêt de le approche dans divers problème . 	Processus itératif d'extraction de classes en non supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Ré-écriture de requêtes dans un système d'intégration sémantique	Nous décrivons la deuxième phase de réalisation d'un système d'intégration qui minimise l'intervention humaine habituellement nécessaire. Après la phase de construction semi-automatique du schéma (ontologie) global décrite dans de précédents articles, nous présentons ici le processus de ré-écriture de requêtes globales en des requêtes adressées aux sources.	Cheikh Niang, Béatrice Bouchou, Moussa Lo, Yacine Sam	http://editions-rnti.fr/render_pdf.php?p1&p=1001858		284	fr	fr	@univ-tours.fr, @ugb.edu.sn	Ré-écriture de requête dans un système d' intégration sémantique  Nous décrire le deuxième phase de réalisation d' un système d' intégration qui minimiser le intervention humain habituellement nécessaire . Après le phase de construction semi-automatique du schéma ( ontologie ) global décrire dans un précédent article , nous présenter ici le processus de ré-écriture de requête global en un requête adresser aux source . 	Ré-écriture de requêtes dans un système d'intégration sémantique	3
Revue des Nouvelles Technologies de l'Information	EGC	2013	Recherche de documents similaires sur le web par segmentations hiérarchiques et extraction de mots-clés	La recherche de documents similaires est un processus qui consiste à trouver les documents présentant des similitudes, comme la copie ou la reformulation, sur des bases documentaires ou sur internet. Elle est utilisée notamment pour protéger la propriété intellectuelle de productions issues de l'enseignement, de la recherche ou de l'industrie. Dans cet article, nous définissons une approche automatique pour permettant d'extraire des mots-clés d'un document en effectuant un bouclage sur une succession de découpage de plus en plus petit. Cette approche permet d'obtenir des mots-clés impossibles à obtenir par une approche globale notamment quand la thématique, le style ou le contenu d'un document varient dans le document. L'objectif est de permettre la détection des documents présentant des similitudes en utilisant uniquement des mots-clés.	Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1001860		285	fr	fr	@compilatio.net	recherche de document similaire sur le web par segmentation hiérarchique et extraction de mot _-clés  le recherche de document similaire être un processus qui consister à trouver le document présenter un similitude , comme le copie ou le reformulation , sur un base documentaire ou sur internet . Elle être utiliser notamment pour protéger le propriété intellectuel de production issu de le enseignement , de le recherche ou de le industrie . Dans ce article , nous définir un approche automatique pour permettre d' extraire un mot _-clé d' un document en effectuer un bouclage sur un succession de découpage de plus en plus petit . ce approche permettre d' obtenir un mot _-clé impossible à obtenir par un approche global notamment quand le thématique , le style ou le contenu d' un document varier dans le document . le objectif être de permettre le détection des document présenter un similitude en utiliser uniquement un mot _-clé . 	Recherche de documents similaires sur le web par segmentations hiérarchiques et extraction de mots-clés	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Réutiliser les connaissances d'expert pour assister l'analyse de l'activité sur simulateur pleine échelle de conduite de centrale nucléaire - Approche à base de M-Trace	Notre travail porte sur l'aide à l'observation de l'activité dans les simulateurs pleine échelle de centrale nucléaire pour assister les formateurs pendant les simulations. Notre approche consiste à représenter l'activité sous la forme de trace modélisée et à les transformer afin d'extraire et de visualiser des informations de haut niveau permettant aux formateurs de mieux retracer et analyser les simulations. Afin de valider notre approche, nous avons conçu le prototype D3KODE que nous avons évalué avec des experts formateurs d'EDF.	Olivier Champalle, Karim Sehaba	http://editions-rnti.fr/render_pdf.php?p1&p=1001828		286	fr	fr	@liris.fr	réutiliser le connaissance d' expert pour assister le analyse de le activité sur simulateur plein échelle de conduite de central nucléaire - approche à base de M-Trace  son travail porter sur le aide à le observation de le activité dans le simulateur plein échelle de centrale nucléaire pour assister le formateur pendant le simulation . son approche consister à représenter le activité sous le forme de trace modéliser et à les transformer afin d' extraire et de visualiser un information de haut niveau permettre aux formateur de mieux retracer et analyser le simulation . Afin de valider son approche , nous avoir concevoir le prototype D3KODE que nous avoir évaluer avec un expert formateur d' EDF . 	Réutiliser les connaissances d'expert pour assister l'analyse de l'activité sur simulateur pleine échelle de conduite de centrale nucléaire - Approche à base de M-Trace	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Sélection de variables non supervisée sous contraintes hiérarchiques	La sélection des variables a un rôle très important dans la fouille de données lorsqu'un grand nombre de variables est disponible. Ainsi, certaines variables peuvent être peu significatives, corrélées ou non pertinentes. Une méthode de sélection a pour objectif de mesurer la pertinence d'un ensemble utilisant principalement un critère d'évaluation. Nous présentons dans cet article un critère non supervisé permettant de mesurer la pertinence d'un sous-ensemble de variables. Ce dernier repose sur l'utilisation du score Laplacien auquel nous avons ajouté des contraintes hiérarchiques. Travailler dans le cadre non supervisé est un vrai challenge dans ce domaine dû à l'absence des étiquettes de classes. Les résultats obtenus sur plusieurs bases de tests sont très encourageants et prometteurs.	Nhat-Quang Doan, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001823		287	fr	fr	@univ-paris13.fr	sélection de variable non superviser sous contrainte hiérarchiques  le sélection des variable avoir un rôle très important dans le fouille de donnée lorsqu' un grand nombre de variable être disponible . ainsi , certain variable pouvoir être peu significatif , corréler ou non pertinent . un méthode de sélection avoir pour objectif de mesurer le pertinence d' un ensemble utiliser principalement un critère d' évaluation . Nous présenter dans ce article un critère non superviser permettre de mesurer le pertinence d' un sous-ensemble de variable . ce dernier reposer sur le utilisation du score Laplacien auquel nous avoir ajouter des contrainte hiérarchique . travailler dans le cadre non superviser être un vrai challenge dans ce domaine devoir à le absence des étiquette de classe . le résultat obtenir sur plusieurs base de test être très encourageant et prometteur . 	Sélection de variables non supervisée sous contraintes hiérarchiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	SNOW, un algorithme exploratoire pour le subspace clustering	Cet article propose un nouvel algorithme pour le problème de subspace clustering dénommé SNOW. Contrairement aux approches descendantes classiques, il ne repose pas sur l'hypothèse de localité et permet l'affectation d'une donnée à plusieurs clusters dans des sous-espaces différents. Les expérimentations préliminaires montrent que notre approche obtient de meilleurs résultats que l'algorithme COPAC sur une base de référence et a été appliquée sur une base de données réelles.	Sylvain Dormieu, Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1001868		288	fr	fr	@gmail.com, @lip6.fr	SNOW , un algorithme exploratoire pour le subspace clustering  ce article proposer un nouveau algorithme pour le problème de subspace clustering dénommer SNOW . contrairement aux approche descendant classique , il ne reposer pas sur le hypothèse de localité et permettre le affectation d' un donner à plusieurs clusters dans un sous-espaces différent . le expérimentation préliminaire montrer que son approche obtenir de meilleur résultat que le algorithme COPAC sur un base de référence et avoir être appliquer sur un base de donnée réel . 	SNOW, un algorithme exploratoire pour le subspace clustering	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Technique de factorisation multi-biais pour des recommandations dynamiques	La factorisation de matrices offre une grande qualité de prédiction pour les systèmes de recommandation. Mais sa nature statique empêche de tenir compte des nouvelles notes que les utilisateurs produisent en continu. Ainsi, la qualité des prédictions décroît entre deux factorisations lorsque de nombreuses notes ne sont pas prises en compte. La quantité de notes écartées est d'autant plus grande que la période entre deux factorisation est longue, ce qui accentue la baisse de qualité.Nos travaux visent à améliorer la qualité des recommandations. Nous proposons une factorisation de matrices utilisant des groupes de produits et intégrant en ligne les nouvelles notes des utilisateurs. Nous attribuons à chaque utilisateur un biais pour chaque groupe de produits similaires que nous mettons à jour. Ainsi, nous améliorons significativement les prédictions entre deux factorisations. Nos expérimentations sur des jeux de données réels montrent l'efficacité de notre approche.	Modou Gueye, Talel Abdesssalem, Hubert Naacke	http://editions-rnti.fr/render_pdf.php?p1&p=1001856		289	fr	fr	@telecom-paristech.fr, @ucad.sn, @lip6.fr	technique de factorisation multi-biais pour un recommandation dynamiques  le factorisation de matrice offrir un grand qualité de prédiction pour le système de recommandation . Mais son nature statique empêcher de tenir compte des nouveau note que le utilisateur produire en continu . ainsi , le qualité des prédiction décroître entre deux factorisation lorsque un nombreux note ne être pas prendre en compte . le quantité de note écarter être d' autant plus grand que le période entre deux factorisation être long , ce qui accentuer le baisse de qualité . son travail viser à améliorer le qualité des recommandation . Nous proposer un factorisation de matrice utiliser un groupe de produit et intégrer en ligne le nouveau note des utilisateur . Nous attribuer à chaque utilisateur un biais pour chaque groupe de produit similaire que nous mettre à jour . ainsi , nous améliorer significativement le prédiction entre deux factorisation . son expérimentation sur un jeu de donnée réel montrer le efficacité de son approche . 	Technique de factorisation multi-biais pour des recommandations dynamiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Text2Geo : des données textuelles aux informations géospatiales	Dans cet article, nous nous intéressons aux méthodes d'extraction d'informations spatiales dans des documents textuels. Nous présentons la méthode hybride Text2Geo qui combine une approche d'extraction d'informations, fondée sur des patrons avec une approche de classification supervisée permettant d'explorer le contexte associé. Nous discutons des résultats expérimentaux obtenus sur le jeu de données de l'étang de Thau.	Sabiha Tahrat, Eric Kergosien, Sandra Bringay, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001861		290	fr	fr	@lirmm.fr, @teledetection.fr, @univ-pau.fr	Text2Geo : un donnée textuel aux information géospatiales  Dans ce article , nous clr intéresser aux méthode d' extraction d' information spatial dans un document textuel . Nous présenter le méthode hybride Text2Geo qui combiner un approche d' extraction d' information , fonder sur un patron avec un approche de classification superviser permettre d' explorer le contexte associer . Nous discuter un résultat expérimental obtenir sur le jeu de donnée de le étang de Thau . 	Text2Geo : des données textuelles aux informations géospatiales	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	ToTeM: une méthode de détection de communautés adaptées aux réseaux d'information	Alors que les réseaux sociaux s'attachaient à représenter des entités et les relations qui existaient entre elles, les réseaux d'information intègrent également des attributs décrivant ces entités ; ce qui conduit à revisiter les méthodes d'analyse et de fouille de ces réseaux. Dans cet article, nous proposons une méthode de classification des sommets d'un graphe qui exploite d'une part leurs relations et d'autre part les attributs les caractérisant. Cette méthode reprend le principe de la méthode de Louvain en l'étendant de façon à permettre la manipulation d'attributs continus d'une manière symétrique à ce qui existe pour les relations.	David Combe, Christine Largeron, Elod Egyed-Zsigmond, Mathias Géry	http://editions-rnti.fr/render_pdf.php?p1&p=1001849		291	fr	fr	@univ-st-etienne.fr, @insa-lyon.fr	totem : un méthode de détection de communauté adapter aux réseau d' information  alors que le réseau social clr attacher à représenter un entité et le relation qui exister entre lui , le réseau d' information intégrer également un attribut décrire ce entité ; ce qui conduire à revisiter le méthode d' analyse et de fouille de ce réseau . Dans ce article , nous proposer un méthode de classification des sommet d' un graphe qui exploiter d' un part son relation et d' autre part le attribut les caractériser . ce méthode reprendre le principe de le méthode de Louvain en l' étendre de façon à permettre le manipulation d' attribut continu d' un manière symétrique à ce qui exister pour le relation . 	ToTeM: une méthode de détection de communautés adaptées aux réseaux d'information	4
Revue des Nouvelles Technologies de l'Information	EGC	2013	Towards a New Science of Big Data Analytics, based on the Geometry and the Topology of Complex, Hierarchic Systems	"My work is concerned with pattern recognition, knowledge discovery, computer learning and statistics. I address how geometry and topology can uncover and empower the semantics of data. In addition to the semantics of data that can be explored using Correspondence Analysis and related multivariate data analyses, hierarchy is a fundamental concept in this work. I address not only low dimensional projection for display purposes, but carry out search and pattern recognition, whenever useful, in very high dimensional spaces. High dimensional spaces present very different characteristics from low dimensions, I have shown that in a particular sense very high dimensional space becomes, as dimensionality increases, hierarchical. I have also shown how in hierarchy, and hence in an ultrametric topological mapping of information space, we track change or anomaly or rupture.In this presentation, the first theme discussed is that of linear time hierarchical clustering with application to sky survey data in astronomy, and to chemo-informatics. The second theme discussed is computational text analysis. It is interesting to note that J.P. Benzécri's original motivation was in language and linguistics. In my text analysis work, I have taken the dictum of McKee (Story : Substance, Structure, Style and the Principles of Screenwriting, Methuen, 1999) that ""text is the sensory surface of a work of art"" and show just how this insight can be rendered in computational terms. This leads to demarcating, tracking, statistical modelling, visualizing, and pattern recognition of narrative. In an application to collaborative writing, I developed an interactive framework for critiquing, and assessing fit and appropriateness of content, on the basis of semantics, leading to books that were published as e-books, having been written by school children in a few days of collaborative class work. In many aspects of this work, hierarchy expresses both continuity and change in the textual narrative or in the narrative of chronological events."	Fionn Murtagh	http://editions-rnti.fr/render_pdf.php?p1&p=1001813		292	en	en	@cs.rhul.ac.uk	towards new science big data analytic base geometry topology complex hierarchic system my work concerned pattern recognition knowledge discovery computer learn statistic address geometry topology uncover empower semantic datum addition semantic data explore used correspondence analysis relate multivariate data analysis hierarchy fundamental concept work address low dimensional projection display purpose carry search pattern recognition whenever useful high dimensional space high dimensional space present different characteristic low dimension show particular sense high dimensional space become dimensionality increase hierarchical also show hierarchy hence ultrametric topological mapping information space track change anomaly rupture in presentation first theme discuss linear time hierarchical cluster application sky survey datum astronomy chemo informatic second theme discuss computational text analysis interesting note j p benzécri s original motivation language linguistic text analysis work take dictum mckee story substance structure style principle screenwriting methuen 1999 text sensory surface work art show insight render computational term lead demarcating track statistical modelling visualizing pattern recognition narrative application collaborative writing develop interactive framework critiqu assess fit appropriateness content basis semantic leading book published e book write school child day collaborative class work many aspect work hierarchy express continuity change textual narrative narrative chronological event	Towards a New Science of Big Data Analytics, based on the Geometry and the Topology of Complex, Hierarchic Systems	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Un Critère d'évaluation pour la construction de variables à base d'itemsets pour l'apprentissage supervisé multi-tables	Dans le contexte de la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement liés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. Dans cet article, nous proposons un Framework basé sur des itemsets pour la construction de variables à partir des tables secondaires. L'informativité de ces nouvelles variables est évaluée dans le cadre de la classification supervisée au moyen d'un critère régularisé qui vise à éviter le surapprentissage. Pour ce faire, nous introduisons un espace de modèles basés sur des itemsets dans la table secondaire ainsi qu'une estimation de la densité conditionnelle des variables construites correspondantes. Une distribution a priori est définie sur cet espace de modèles, pour obtenir ainsi un critère sans paramètres permettant d'évaluer la pertinence des variables construites. Des expérimentations préliminaires montrent la pertinence de l'approche.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1001824		293	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com, @u-cergy.fr	un Critère d' évaluation pour le construction de variable à base d' itemsets pour le apprentissage superviser multi-tables  Dans le contexte de le fouille de donnée multi-tables , le donnée être représenter sous un format relationnel dans lequel le individu de le table cible être potentiellement lier à plusieurs enregistrement dans un table secondaire en relation un-à-plusieurs . Dans ce article , nous proposer un Framework baser sur un itemsets pour le construction de variable à partir un table secondaire . le informativité de ce nouveau variable être évaluer dans le cadre de le classification superviser au moyen d' un critère régulariser qui viser à éviter le surapprentissage . Pour ce faire , nous introduire un espace de modèle baser sur un itemsets dans le table secondaire ainsi qu' un estimation de le densité conditionnel des variable construites correspondant . un distribution avoir priori être définir sur ce espace de modèle , pour obtenir ainsi un critère sans paramètre permettre d' évaluer le pertinence des variable construites . un expérimentation préliminaire montrer le pertinence de le approche . 	Un Critère d'évaluation pour la construction de variables à base d'itemsets pour l'apprentissage supervisé multi-tables	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Un système hybride de recherche d'information intégrant le raisonnement à partir de cas et la composition d'ontologies	La croissance des informations disponibles sur le web nécessite des outils de recherche de plus en plus performants permettant de répondre efficacement aux besoins des utilisateurs. Dans ce contexte, l'utilisation des ontologies présente des atouts importants. Cependant, la construction manuelle d'ontologies est très coûteuse, ceci a poussé à proposer des approches permettant d'automatiser cette construction. Cet article présente un système de recherche d'information hybride basée sur le Raisonnement à Partir de Cas (RàPC) et la composition d'ontologies. Ce système vise à combiner la construction automatique d'ontologies modulaires et le RàPC, qui a pour but d'améliorer les résultats de recherche d'information (RI). Des expérimentations ont été menées et les résultats obtenus montrent une amélioration de la précision dans le cas d'une recherche d'information sur le Web.	Ghada Besbes, Hajer Baazaoui-Zghal, Henda Ben Ghezela	http://editions-rnti.fr/render_pdf.php?p1&p=1001845		294	fr	fr	@gmail.com, @riadi.rnu.tn, @cck.rnu.tn	un système hybride de recherche d' information intégrer le raisonnement à partir de cas et le composition d' ontologies  le croissance des information disponible sur le web nécessiter un outil de recherche de plus en plus performant permettre de répondre efficacement aux besoin des utilisateur . Dans ce contexte , le utilisation des ontologie présent des atout important . cependant , le construction manuel d' ontologie être très coûteux , ceci avoir pousser à proposer un approche permettre d' automatiser ce construction . ce article présenter un système de recherche d' information hybride baser sur le raisonnement à Partir de cas ( RàPC ) et le composition d' ontologie . ce système viser à combiner le construction automatique d' ontologie modulaire et le RàPC , qui avoir pour but d' améliorer le résultat de recherche d' information ( RI ) . un expérimentation avoir être mener et le résultat obtenir montrer un amélioration de le précision dans le cas d' un recherche d' information sur le Web . 	Un système hybride de recherche d'information intégrant le raisonnement à partir de cas et la composition d'ontologies	1
Revue des Nouvelles Technologies de l'Information	EGC	2013	Une approche en programmation par contraintes pour la classification non supervisée	Dans cet article, nous abordons le problème de classification non supervisée sous contraintes fondé sur la programmation par contraintes (PPC). Nous considérons comme critère d'optimisation la minimisation du diamètre maximal des clusters. Nous proposons un modèle pour cette tâche en PPC et nous montrons aussi l'importance des stratégies de recherche pour améliorer son efficacité. Notre modèle basé sur la distance entre les objets permet de traiter des données qualitatives et quantitatives. Des contraintes supplémentaires sur les clusters et les instances peuvent directement être ajoutées. Des expériences sur des ensembles de données classiques montrent l'intérêt de notre approche.	Thi-Bich-Hanh Dao, Khanh-Chuong Duong, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001822		295	fr	fr	@univ-orleans.fr	un approche en programmation par contrainte pour le classification non supervisée  Dans ce article , nous aborder le problème de classification non superviser sous contrainte fonder sur le programmation par contrainte ( PPC ) . Nous considérer comme critère d' optimisation le minimisation du diamètre maximal des clusters . Nous proposer un modèle pour ce tâche en PPC et nous montrer aussi le importance des stratégie de recherche pour améliorer son efficacité . son modèle baser sur le distance entre le objet permettre de traiter un donnée qualitatif et quantitatif . un contrainte supplémentaire sur le clusters et le instance pouvoir directement être ajouter . un expérience sur un ensemble de donnée classique montrer le intérêt de son approche . 	Une approche en programmation par contraintes pour la classification non supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Une nouvelle mesure pour l'évaluation des méthodes d'extraction de thématiques : la Vraisemblance Généralisée	Les méthodes dédiées à l'extraction automatique de thématiques sont issues de domaines variés : linguistique computationnelle, TAL, algèbre linéaire, statistique, etc. A ces méthodes spécifiques, peuvent s'ajouter des méthodes adaptées d'autres domaines, notamment de l'apprentissage automatique non supervisé. Les résultats produits par l'ensemble de ces méthodes prennent des formes hétérogènes : partitions de documents, distributions de probabilités sur les mots, matrices. Cela pose clairement un problème pour les comparer de manière uniforme. Dans cet article, nous proposons une nouvelle mesure de qualité, intitulée Vraisemblance Généralisée, pour permettre une évaluation et ainsi la comparaison de différentes méthodes d'extraction de thématiques. Les résultats, obtenus sur un corpus de documents Web autour des élections présidentielles françaises de 2012, ainsi que sur le corpus Associated Press, montrent la pertinence de la mesure proposée.	Mohamed Dermouche, Julien Velcin, Sabine Loudcher, Leila Khouas	http://editions-rnti.fr/render_pdf.php?p1&p=1001851		296	fr	fr	@univ-lyon2.fr, @amisw.com	un nouveau mesure pour le évaluation des méthode d' extraction de thématique : le vraisemblance Généralisée  le méthode dédier à le extraction automatique de thématique être issir de domaine varier : linguistique computationnelle , TAL , algèbre linéaire , statistique , etc. A ce méthode spécifique , pouvoir clr ajouter un méthode adapter d' autre domaine , notamment de le apprentissage automatique non superviser . le résultat produire par le ensemble de ce méthode prendre un forme hétérogène : partition de document , distribution de probabilité sur le mot , matrice . cela poser clairement un problème pour les comparer de manière uniforme . Dans ce article , nous proposer un nouveau mesure de qualité , intituler Vraisemblance généraliser , pour permettre un évaluation et ainsi le comparaison de différent méthode d' extraction de thématique . le résultat , obtenir sur un corpus de document Web autour un élection présidentiel français de 2012 , ainsi que sur le corpus Associated Press , montrer le pertinence de le mesure proposer . 	Une nouvelle mesure pour l'évaluation des méthodes d'extraction de thématiques : la Vraisemblance Généralisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Unsupervised Video Tag Correction System	We present a new system for video auto tagging which aims at correcting and completing the tags provided by users for videos uploaded on the Internet. Unlike most existing systems, we do not learn any tag classifiers or use the questionable textual information to compare our videos. We propose to compare directly the visual content of the videos described by different sets of features such as Bag-of-visual-Words or frequent patterns built from them. Then, we propagate tags between visually similar videos according to the frequency of these tags in a given video neighborhood. We also propose a controlled experimental set up to evaluate such a system. Experiments show that with suitable features, we are able to correct a reasonable amount of tags in Web videos.	Hoang-Tung Tran, Elisa Fromont, François Jacquenet, Baptiste Jeudy, Adrien Martins	http://editions-rnti.fr/render_pdf.php?p1&p=1001867		297	en	en	@univ-st-etienne.fr, @univ-st-etienne.fr	unsupervised video tag correction system present new system video auto tag aim correct complete tag provide user video upload internet unlike exist system learn tag classifier use questionable textual information compare video propose compare directly visual content video describe different set feature bag of visual word frequent pattern build them then propagate tag visually similar video accord frequency tag give video neighborhood also propose controlled experimental set evaluate system experiment show suitable feature able correct reasonable amount tag web video	Unsupervised Video Tag Correction System	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Validation d'une carte cognitive	Les cartes cognitives sont un modèle graphique représentant des influences entre des concepts. Malgré le fait qu'une carte cognitive soit relativement simple à construire, certaines influences peuvent se contredire l'une l'autre. Cet article propose différents critères pour valider une carte cognitive, c'est-àdire indiquer si la carte contient ou non des contradictions. Nous distinguons deux types de critères : les critères de vérification qui valident une carte cognitive en déterminant sa cohérence interne et les critères de test qui valident une carte à partir d'un ensemble de contraintes choisies par le concepteur.	Aymeric Le Dorze, Laurent Garcia, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001825		298	fr	fr	@univ-angers.fr	validation d' un carte cognitive  le carte cognitif être un modèle graphique représenter un influence entre un concept . Malgré le fait qu' un carte cognitif être relativement simple à construire , certain influence pouvoir clr contredire le un le autre . ce article proposer différents critère pour valider un carte cognitif , c' est-àdire indiquer si le carte contenir ou non des contradiction . Nous distinguer deux type de critère : le critère de vérification qui valider un carte cognitif en déterminant son cohérence interne et le critère de test qui valider un carte à partir d' un ensemble de contrainte choisir par le concepteur . 	Validation d'une carte cognitive	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers un cadre évolutif de classification non supervisée	La classification non supervisée (clustering) évolutive surpasse généralement par celle statique en produisant des groupes de données (clusters) qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme. Dans ce travail, nous présentons un cadre différent pour le clustering évolutif d'une manière incrémentale par un suivi précis des variables de proximité temporelles entre les objets suivis par un clustering statique ordinaire.	Mohamed Charouel, Minyar Sassi-Hidri, Mohamed Ali Zoghlami	http://editions-rnti.fr/render_pdf.php?p1&p=1001821		299	fr	fr	@enit.rnu.tn, @gmail.com	Vers un cadre évolutif de classification non supervisée  le classification non superviser ( clustering ) évolutif surpasser généralement par celui statique en produire un groupe de donnée ( clusters ) qui refléter le tendance à long terme tout en être robuste aux variation à court terme . Dans ce travail , nous présenter un cadre différent pour le clustering évolutif d' un manière incrémentale par un suivi précis des variable de proximité temporel entre le objet suivre par un clustering statique ordinaire . 	Vers un cadre évolutif de classification non supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une architecture multicouche d'ontologies dédiée à la résolution mixte de problèmes	Dans cet article, nous nous intéressons à la gestion d'expériences générées au sein des processus de résolution mixte (individuelle et/ou collective) de problèmes afin d'assister la capitalisation et le partage des connaissances dans les environnements collaboratifs. Dans ce contexte, nous proposons un cadre ontologique générique par rapport au domaine dédié à la modélisation formelle et consensuelle de ces expériences en adoptant une architecture multicouche basée sur quatre strates. La première strate est basée sur la spécialisation d'ontologies fondationnelles. La deuxième strate est basée sur la conception de trois patrons conceptuels ontologiques (PCO) noyaux (le PCO organisationnel, le PCO téléologique et le PCO argumentatif modélisant respectivement les acteurs, le problème et les solutions proposées). La troisième strate est basée sur la spécialisation des PCO noyaux dans un domaine particulier et la dernière strate est basée sur l'instanciation du modèle ontologique de domaine pour la représentation d'une situation du monde réel.	Nesrine Ben Yahia, Narjès Bellamine Ben Saoud, Henda Hajjami Ben Ghezala	http://editions-rnti.fr/render_pdf.php?p1&p=1001844		300	fr	fr	@ensi.rnu.tn, @ensi.rnu.tn, @ensi.rnu.tn	Vers un architecture multicouche d' ontologie dédier à le résolution mixte de problèmes  Dans ce article , nous clr intéresser à le gestion d' expérience générer au sein des processus de résolution mixte ( individuel et collectif ) de problème afin d' assister le capitalisation et le partage des connaissance dans le environnement collaboratif . Dans ce contexte , nous proposer un cadre ontologique générique par rapport au domaine dédier à le modélisation formel et consensuel de ce expérience en adopter un architecture multicouche baser sur quatre strate . le premier strate être baser sur le spécialisation d' ontologie fondationnelles . le deuxième strate être baser sur le conception de trois patron conceptuel ontologique ( PCO ) noyau ( le PCO organisationnel , le PCO téléologique et le PCO argumentatif modéliser respectivement le acteur , le problème et le solution proposer ) . le troisième strate être baser sur le spécialisation des PCO noyau dans un domaine particulier et le dernier strate être baser sur le instanciation du modèle ontologique de domaine pour le représentation d' un situation du monde réel . 	Vers une architecture multicouche d'ontologies dédiée à la résolution mixte de problèmes	2
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une Automatisation de la Construction de Variables pour la Classification Supervisée	Dans cet article, nous proposons un cadre visant à automatiser la construction de variables pour l'apprentissage supervisé, en particulier dans le cadre multi-tables. La connaissance du domaine est spécifiée d'une part en structurant les données en variables, tables et liens entre tables, d'autre part en choisissant des règles de construction de variables. L'espace de construction de variables ainsi défini est potentiellement infini, ce qui pose des problèmes d'exploration combinatoire et de sur-apprentissage. Nous introduisons une distribution de probabilité a priori sur l'espace des variables constructibles, ainsi qu'un algorithme performant de tirage d'échantillons dans cette distribution. Des expérimentations intensives montrent que l'approche est robuste et performante.	Marc Boullé, Dhafer Lahbib	http://editions-rnti.fr/render_pdf.php?p1&p=1001819		301	fr	fr	@orange.com	Vers un automatisation de le construction de Variables pour le classification Supervisée  Dans ce article , nous proposer un cadre viser à automatiser le construction de variable pour le apprentissage superviser , en particulier dans le cadre multi-tables . le connaissance du domaine être spécifier d' un part en structurer le donnée en variable , table et lien entre table , d' autre part en choisir un règle de construction de variable . le espace de construction de variable ainsi définir être potentiellement infini , ce qui poser un problème d' exploration combinatoire et de sur-apprentissage . Nous introduire un distribution de probabilité avoir priori sur le espace des variable constructible , ainsi qu' un algorithme performant de tirage d' échantillon dans ce distribution . un expérimentation intensif montrer que le approche être robuste et performant . 	Vers une Automatisation de la Construction de Variables pour la Classification Supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une mesure de similarité pour les séquences complexes	Le calcul de similarité entre les séquences est d'une extrême importance dans de nombreuses approches d'explorations de données. Il existe une multitude de mesures de similarités de séquences dans la littérature. Or, la plupart de ces mesures sont conçues pour des séquences simples, dites séquences d'items. Dans ce travail, nous étudions d'un point de vue purement combinatoire le problème de similarité entre des séquences complexes (i.e., des séquences d'ensembles ou itemsets). Nous présentons de nouveaux résultats afin de compter efficacement toutes les sous-séquences communes à deux séquences. Ces résultats théoriques sont la base d'une mesure de similarité calculée efficacement grâce à une approche de programmation dynamique.	Elias Egho, Chedy Raïssi, Toon Calders, Thomas Bourquard, Nicolas Jay, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001853		302	fr	fr	@loria.fr, @ulb.ac.be	Vers un mesure de similarité pour le séquence complexes  le calcul de similarité entre le séquence être d' un extrême importance dans un nombreux approche d' exploration de donnée . Il exister un multitude de mesure de similarité de séquence dans le littérature . Or , le plupart de ce mesure être concevoir pour un séquence simple , dire séquence d' item . Dans ce travail , nous étudier d' un point de vue purement combinatoire le problème de similarité entre un séquence complexe ( i.e. , un séquence d' ensemble ou itemsets ) . Nous présenter un nouveau résultat afin de compter efficacement tout le sous-séquences commun à deux séquence . ce résultat théorique être le base d' un mesure de similarité calculer efficacement grâce à un approche de programmation dynamique . 	Vers une mesure de similarité pour les séquences complexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2013	Visualisation radiale : approche parallèle entre CPU et GPU	Dans cet article, nous proposons une parallélisation sur CPU et GPU d'une méthode de visualisation radiale à base de points d'intérêt. Nous montrons que cette approche peut visualiser avec des temps très courts des millions de données sur des dizaines de dimensions, et nous étudions l'efficacité de la parallélisation dans différentes configurations.	Tianyang Liu, Fatma Bouali, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001834		303	fr	fr	@univ-tours.fr, @univ-lille2.fr	visualisation radiale : approche parallèle entre CPU et GPU  Dans ce article , nous proposer un parallélisation sur CPU et GPU d' un méthode de visualisation radiale à base de point d' intérêt . Nous montrer que ce approche pouvoir visualiser avec un temps très court des million de donnée sur un dizaine de dimension , et nous étudier le efficacité de le parallélisation dans différent configuration . 	Visualisation radiale : approche parallèle entre CPU et GPU	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Antipattern Detection inWeb Ontologies: an Experiment using SPARQL Queries	Ontology antipatterns are structures that reflect ontology modelling problems because they lead to inconsistencies, bad reasoning performance or bad formalisation of domain knowledge. We propose four methods for the detection of antipatterns using SPARQL queries. We conduct some experiments to detect antipattern in a corpus of OWL ontologies.	Catherine Roussey, Oscar Corcho, Ond&#711;rej ?váb-Zamazal, François Scharffe, Stephan Bernard	http://editions-rnti.fr/render_pdf.php?p1&p=1001177	http://editions-rnti.fr/render_pdf.php?p=1001177	323	en	en		antipattern detection inweb ontology experiment used sparql query ontology antipattern structure reflect ontology model problem lead inconsistency bad reasoning performance bad formalisation domain knowledge propose four method detection antipattern used sparql query conduct experiment detect antipattern corpus owl ontology	Antipattern Detection inWeb Ontologies: an Experiment using SPARQL Queries	2
Revue des Nouvelles Technologies de l'Information	EGC	2012	Apprentissage d'ensemble d'opérateurs de projection orthogonale pour la détection de nouveauté	Dans ce papier, nous proposons une approche de détection de nouveautéfondée sur les opérateurs de projection orthogonale et l'idée de doublebootstrap (bi- bootstrap). Notre approche appelée Random Subspace NoveltyDetection Filter (RS-NDF), combine une technique de rééchantillonnage etl'idée d'apprentissage d'ensemble. RS-NDF est un ensemble de filtres NDF(Novelty Detection Filter), induits à partir d'échantillons bootstrap des donnéesd'apprentissage, en utilisant une sélection aléatoire des variables pour l'apprentissagedes filtres. RS-NDF utilise donc un double bootstrap, c'est à dire unrééchantillonnage avec remise sur les observations et un rééchantillonnage sansremise sur les variables. La prédiction est faite par l'agrégation des prédictionsde l'ensemble des filtres. RS-NDF présente généralement une importante améliorationdes performances par rapport au modèle de base NDF unique. Grâce àson algorithme d'apprentissage en ligne, l'approche RS-NDF est également enmesure de suivre les changements dans les données au fil du temps. Plusieursmétriques de performance montrent que l'approche proposée est plus efficace,robuste et offre de meilleures performances pour la détection de nouveauté comparéeaux autres techniques existantes.	Fatma Hamdi, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001156	http://editions-rnti.fr/render_pdf.php?p=1001156	324	fr	fr	@univ-paris13.fr	apprentissage d' ensemble d' opérateur de projection orthogonal pour le détection de nouveauté  Dans ce papier , nous proposer un approche de détection de nouveautéfondée sur le opérateur de projection orthogonal et le idée de doublebootstrap ( bi-bootstrap ) . son approche appeler Random Subspace NoveltyDetection Filter ( RS-NDF ) , combiner un technique de rééchantillonnage etl'idée d' apprentissage d' ensemble . RS-NDF être un ensemble de filtre NDF ( Novelty Detection Filter ) , induire à partir d' échantillon bootstrap des donnéesd'apprentissage , en utiliser un sélection aléatoire des variable pour le apprentissagedes filtre . RS-NDF utiliser donc un double bootstrap , c' être à dire unrééchantillonnage avec remise sur le observation et un rééchantillonnage sansremise sur le variable . le prédiction être faire par le agrégation des prédictionsde le ensemble des filtre . RS-NDF présenter généralement un important améliorationdes performance par rapport au modèle de base NDF unique . grâce àson algorithme d' apprentissage en ligne , le approche RS-NDF être également enmesure de suivre le changement dans le donnée au fil du temps . Plusieursmétriques de performance montrer que le approche proposer être plus efficace , robuste et offrir un meilleur performance pour le détection de nouveauté comparéeaux autre technique existant . 	Apprentissage d'ensemble d'opérateurs de projection orthogonale pour la détection de nouveauté	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Apprentissage par analyse linéaire discriminante des paramètres de fusion pour la recherche d'information multimédia texte-image	Avec le développement du numérique, des quantités très importantesde documents composés de texte et d'images sont échangés, ce qui nécessite ledéveloppement demodèles permettant d'exploiter efficacement ces informationsmultimédias. Dans le contexte de la recherche d'information, unmodèle possibleconsiste à représenter séparément les informations textuelles et visuelles et àcombiner linéairement les scores issus de chaque représentation. Cette approchenécessite le paramétrage de poids afin d'équilibrer la contribution de chaquemodalité. Le but de cet article est de présenter une nouvelle méthode permettantd'apprendre ces poids, basée sur l'analyse linéaire discriminante de Fisher(ALD). Des expérimentations réalisées sur la collection ImageCLEF montrentque l'apprentissage des poids grâce à l'ALD est pertinent et que la combinaisondes scores correspondante améliore significativement les résultats par rapport àl'utilisation d'une seule modalité.	Christophe Moulin, Christine Largeron, Cécile Barat, Mathias Géry, Christophe Ducottet	http://editions-rnti.fr/render_pdf.php?p1&p=1001201	http://editions-rnti.fr/render_pdf.php?p=1001201	325	fr	fr		apprentissage par analyse linéaire discriminant des paramètre de fusion pour le recherche d' information multimédia texte-image  Avec le développement du numérique , un quantité très importantesde document composer de texte et d' image être échanger , ce qui nécessiter ledéveloppement demodèles permettre d' exploiter efficacement ce informationsmultimédias . Dans le contexte de le recherche d' information , unmodèle possibleconsiste à représenter séparément le information textuel et visuel et àcombiner linéairement le score issu de chaque représentation . ce approchenécessite le paramétrage de poids afin d' équilibrer le contribution de chaquemodalité . le but de ce article être de présenter un nouveau méthode permettantd'apprendre ce poids , baser sur le analyse linéaire discriminant de Fisher ( ALD ) . un expérimentation réaliser sur le collection ImageCLEF montrentque le apprentissage des poids grâce à le ALD être pertinent et que le combinaisondes score correspondant améliorer significativement le résultat par rapport àl'utilisation d' un seul modalité . 	Apprentissage par analyse linéaire discriminante des paramètres de fusion pour la recherche d'information multimédia texte-image	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Biological event extraction using SVM and composite kernel function	With an overwhelming of experimental and computational results inmolecular biology, there is an increasing interest to provide tools that will automaticallyextract structured biological information recorded in freely availabletext. Extraction of named entities such as protein, gene or disease names andof simple relations of these entities, such as statements of protein-protein interactionshas gained certain success, and now the new focus research has beenmoving to higher level of information extraction such as co-reference resolutionand event extraction. It is precisely the last of these tasks which will be focusedin this paper. The biological event template allows detailed representations ofcomplex natural language statements, which is specified by a trigger and argumentslabeled by semantic roles.In this paper, we have developed a biological event extraction approach whichuses Support Vector Machines (SVM) and a suitable composite kernel functionto identify triggers and to assign the corresponding arguments. Also, we makeuse of a number of features based on both syntactic and contextual informationwhich where automatically learned from the training data.We implemented our event extraction system using the state-of-the-art of NLPtools. We achieved competitive results compared to the BioNLP'09 Shared taskbenchmark.	Maha Amami, Aymen Elkhlifi, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001150	http://editions-rnti.fr/render_pdf.php?p=1001150	326	en	en	@isg.rnu.tn, @paris4.sorbonne.fr, @ihec.rnu.tn	biological event extraction used svm composite kernel function overwhelming experimental computational result inmolecular biology increase interest provide tool automaticallyextract structured biological information record freely availabletext extraction name entity protein gene disease name andof simple relation entity statement protein protein interactionsha gain certain success new focus research beenmove higher level information extraction co reference resolutionand event extraction precisely last task focusedin paper biological event template allow detailed representation ofcomplex natural language statement specify trigger argumentslabele semantic role in paper develop biological event extraction approach whichuse support vector machine svm suitable composite kernel functionto identify trigger assign corresponding argument also makeuse number feature base syntactic contextual informationwhich automatically learn training datum we implemented event extraction system used state of the art nlptool achieve competitive result compare bionlp 09 shared taskbenchmark	Biological event extraction using SVM and composite kernel function	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Caractérisation et extraction de biclusters de valeurs similaires avec l'analyse de concepts triadiques	Le biclustering de données numériques est devenu depuis le début desannées 2000 une tâche importante d'analyse de données, particulièrement pourl'étude de données biologiques d'expression de gènes. Un bicluster représenteune association forte entre un ensemble d'objets et un ensemble d'attributs dansune table de données numériques. Les biclusters de valeurs similaires peuventêtre vus comme des sous-tables maximales de valeurs proches. Seules quelquesméthodes se sont penchées sur une extraction complète (i.e. non heuristique),exacte et non redondante de tels motifs, qui reste toujours un problème difficile,tandis qu'aucun cadre théorique fort ne permet leur caractérisation. Dans le présentarticle, nous introduisons des liens importants avec l'analyse formelle deconcepts. Plus particulièrement, nous montrons de manière originale que l'analysede concepts triadiques (TCA) propose un cadre mathématique intéressant etpuissant pour le biclustering de données numériques. De cette manière, les algorithmesexistants de la TCA, qui s'appliquent habituellement à des données binaires,peuvent être utilisés (directement ou après quelques modifications) aprèsun prétraitement des données pour l'extraction désirée.	Mehdi Kaytoue-Uberall, Sergei O. Kuznetsov, Amedeo Napoli, Juraj Macko, Wagner Meira Jr	http://editions-rnti.fr/render_pdf.php?p1&p=1001166	http://editions-rnti.fr/render_pdf.php?p=1001166	327	fr	fr	@dcc.ufmg.br	caractérisation et extraction de biclusters de valeur similaire avec le analyse de concept triadiques  le biclustering de donnée numérique être devenir depuis le début desannées 2000 un tâche important d' analyse de donnée , particulièrement pourl'étude de donnée biologique d' expression de gène . un bicluster représenteune association fort entre un ensemble d' objet et un ensemble d' attribut dansune table de donnée numérique . le biclusters de valeur similaire peuventêtre voir comme un sous-tables maximal de valeur proche . seul quelquesméthodes clr être pencher sur un extraction complet ( id est non heuristique ) , exact et non redondant de tel motif , qui rester toujours un problème difficile , tandis qu' aucun cadre théorique fort ne permettre son caractérisation . Dans le présentarticle , nous introduire un lien important avec le analyse formel deconcepts . plus particulièrement , nous montrer de manière original que le analysede concept triadiques ( TCA ) proposer un cadre mathématique intéressant etpuissant pour le biclustering de donnée numérique . De ce manière , le algorithmesexistants de le TCA , qui clr appliquer habituellement à un donnée binaire , pouvoir être utiliser ( directement ou après quelque modification ) aprèsun prétraitement un donnée pour le extraction désirer . 	Caractérisation et extraction de biclusters de valeurs similaires avec l'analyse de concepts triadiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification Conceptuelle avec Généralisation par Intervalles	Nous nous intéressons aux méthodes de classification hiérarchique oupyramidale, où chaque classe formée correspond à un concept, i.e. une paire (extension,intension), considérant des données décrites par des variables quantitativesà valeurs réelles ou intervalles, ordinales et/ou prenant la forme de distributionde probabilités/fréquences sur un ensemble de catégories. Les concepts sontobtenus par une correspondance de Galois avec généralisation par intervalles, cequi permet de traiter les données de différents types dans un cadre commun. Unemesure de la généralité d'un concept est alors calculée sous une forme communepour les différents types de variables. Un exemple illustre la méthode proposée.	Géraldine Polaillon, Paula Brito	http://editions-rnti.fr/render_pdf.php?p1&p=1001141	http://editions-rnti.fr/render_pdf.php?p=1001141	328	fr	fr	@fep.up.pt, @supelec.fr	classification conceptuel avec généralisation par Intervalles  Nous nous intéresser aux méthode de classification hiérarchique oupyramidale , où chaque classe former correspondre à un concept , i.e. un paire ( extension , intension ) , considérant un donnée décrire par un variable quantitativesà valeur réel ou intervalle , ordinal et prendre le forme de distributionde probabilité  fréquence sur un ensemble de catégorie . le concept sontobtenus par un correspondance de Galois avec généralisation par intervalle , cequi permettre de traiter le donnée de différent type dans un cadre commun . Unemesure de le généralité d' un concept être alors calculer sous un forme communepour le différent type de variable . un exemple illustrer le méthode proposer . 	Classification Conceptuelle avec Généralisation par Intervalles	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification de données EEG par algorithme évolutionnaire pour l'étude d'états de vigilance	"L'objectif de ce travail est de prédire l'état de vigilance d'un individuà partir de l'étude de son activité cérébrale (signaux d'électro-encéphalographieEEG). La variable à prédire est binaire (état de vigilance ""normal"" ou ""relaxé"").Des EEG de 44 participants dans les deux états (88 enregistrements), ont étérecueillis via un casque à 58 électrodes. Après une étape de prétraitement et devalidation des données, un critère nommé ""critère des pentes"" a été choisi. Desméthodes de classification supervisée usuelles (k plus proches voisins, arbresbinaires de décision (CART), forêts aléatoires, PLS et sparse PLS discriminante)ont été appliquées afin de fournir des prédictions de l'état des participants. Lecritère utilisé a ensuite été raffiné grâce à un algorithme génétique, ce qui apermis de construire un modèle fiable (taux de bon classement moyen par CARTégal à 86.68 ± 1.87%) et de sélectionner une électrode parmi les 58 initiales."	Laurent Vezard, Pierrick Legrand, Marie Chavent, Frédérique Faïta-Aïnseba, Julien Clauzel	http://editions-rnti.fr/render_pdf.php?p1&p=1001198	http://editions-rnti.fr/render_pdf.php?p=1001198	329	fr	fr	@inria.fr, @u-bordeaux2.fr	classification de donnée EEG par algorithme évolutionnaire pour le étude d' état de vigilance  " le objectif de ce travail être de prédire le état de vigilance d' un individuà partir de le étude de son activité cérébral ( signal d' électro- encéphalographieEEG ) . . le variable à prédire être binaire ( état de vigilance " " normal " " ou " " relaxer " " ) . . Des EEG de 44 participant dans le deux état ( 88 enregistrement ) , avoir étérecueillis via un casque à 58 électrode . . Après un étape de prétraitement et devalidation des donnée , un critère nommer " " critère des pente " " avoir être choisir . . Desméthodes de classification superviser usuel ( k plus proche voisin , arbresbinaires de décision ( CART ) , forêt aléatoire , PLS et sparse PLS discriminant ) avoir être appliquer afin de fournir un prédiction de le état des participant . . Lecritère utiliser avoir ensuite être raffiner grâce à un algorithme génétique , ce qui apermis de construire un modèle fiable ( taux de bon classement moyen par CARTégal à 86.68 ± 1 .87 \% ) et de sélectionner un électrode parmi le 58 initial . " 	Classification de données EEG par algorithme évolutionnaire pour l'étude d'états de vigilance	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification des données catégorielles via la maximisation spectrale de la modularité	Ce papier présente un algorithme spectrale pour maximiser le critèrede la modularité étendu à la classification des données catégorielles. Il met enevidence la connexion formelle entre la maximisation de la modularité et la classificationspectrale, il présente en particulier le problème de maximisation de lamodularité sous forme d'un problème algèbrique de maximisation de la trace.Nous développons ensuite un algorithme efficace pour trouver la partition optimalemaximisant le critère de modularité. Les résultats expérimentaux montrentl'efficacité de notre approche	Lazhar Labiod, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001194	http://editions-rnti.fr/render_pdf.php?p=1001194	330	fr	fr	@parisdescartes.fr, @univ-paris13.fr	classification des donnée catégoriel via le maximisation spectral de le modularité  ce papier présenter un algorithme spectral pour maximiser le critèrede le modularité étendre à le classification des donnée catégoriel . Il mettre enevidence le connexion formel entre le maximisation de le modularité et le classificationspectrale , il présenter en particulier le problème de maximisation de lamodularité sous forme d' un problème algèbrique de maximisation de le trace . Nous développer ensuite un algorithme efficace pour trouver le partition optimalemaximisant le critère de modularité . le résultat expérimental montrentl'efficacité de son approche 	Classification des données catégorielles via la maximisation spectrale de la modularité	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification probabiliste non supervisée et visualisation des données séquentielles	Nous proposons dans ce papier un nouvel algorithme de classificationnon supervisée à base de modèle de mélange topologique pour des donnéesnon i.i.d (non independently and identically distributed). Ce nouveau paradigmeprobabiliste, plonge les cartes topologiques probabilistes dans une formulationsous forme de chaînes de Markov cachées. Dans cette formulation, la générationd'une observation à un instant donné du temps est conditionnée par les étatsvoisins au même instant du temps. Ainsi, une grande proximité impliquera unegrande probabilité pour la contribution à la génération. L'approche proposée estévaluée en utilisant des données séquentielles réelles issues des bases de donnéesde l'Institut Nationale de l'Audiovisuel (INA). Les résultats obtenus sonttrès encourageants et prometteurs.	Rakia Jaziri, Mustapha Lebbah, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001187	http://editions-rnti.fr/render_pdf.php?p=1001187	331	fr	fr	@univ-paris13.fr, @ina.fr	classification probabiliste non superviser et visualisation des donnée séquentielles  Nous proposer dans ce papier un nouveau algorithme de classificationnon superviser à base de modèle de mélange topologique pour un donnéesnon i . i . d ( non independently and identically distributed ) . ce nouveau paradigmeprobabiliste , plonger le carte topologique probabiliste dans un formulationsous forme de chaîne de Markov cacher . Dans ce formulation , le générationd'une observation à un instant donner du temps être conditionner par le étatsvoisins au même instant du temps . ainsi , un grand proximité impliquer unegrande probabilité pour le contribution à le génération . le approche proposer estévaluée en utiliser un donnée séquentiel réel issue des base de donnéesde le institut national de le Audiovisuel ( INA ) . le résultat obtenir sonttrès encourageant et prometteur . 	Classification probabiliste non supervisée et visualisation des données séquentielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification topologique probabiliste pour des données catégorielles	Cet article présente une carte auto-organisatrice probabiliste pour l'analyseet la classification topologique des données catégorielles. En considérant unmodèle de mélanges parcimonieux nous introduisons une nouvelle carte autoorganisatrice(SOM) probabiliste. L'estimation des paramètres de notre modèleest réalisée à l'aide de l'algorithme EM classique. Contrairement à SOM, l'algorithmed'apprentissage proposé optimise une fonction objective. Ces performancesont été évaluées sur des données réelles et les résultats obtenus sontencourageants et prometteurs à la fois pour la classification et pour la modélisation.	Nicoleta Rogovschi, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1001188	http://editions-rnti.fr/render_pdf.php?p=1001188	332	fr	fr	@parisdescartes.fr	classification topologique probabiliste pour un donnée catégorielles  ce article présenter un carte auto- organisatrice probabiliste pour le analyseet le classification topologique des donnée catégoriel . En considérer unmodèle de mélange parcimonieux nous introduire un nouveau carte autoorganisatrice ( SOM ) probabiliste . le estimation des paramètre de son modèleest réaliser à le aide de le algorithme EM classique . contrairement à SOM , le algorithmed'apprentissage proposer optimiser un fonction objectif . ce performancesont être évaluer sur un donnée réel et le résultat obtenir sontencourageants et prometteur à le foi pour le classification et pour le modélisation . 	Classification topologique probabiliste pour des données catégorielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering de séquences d'activités pour l'étude de procédures neurochirurgicales	L'utilisation de modèles de procédure chirurgicale (Surgical ProcessModel, SPM) a récemment émergé dans le domaine de la conception d'outilsd'intervention chirurgicale assistée par ordinateur. Ces modèles, qui sont utiliséspour analyser et évaluer les interventions, représentent des procédures chirurgicales(Surgical Process, SP) qui sont formalisées comme des structures symboliquesdécrivant une chirurgie à un niveau de granularité donné. Un enjeu importantréside dans la définition de métriques permettant la comparaison et l'évaluationde ces procédures. Ainsi, les relations entre ces métriques et des donnéespré-opératoires permettent de classer les chirurgies pour mettre en lumière desinformations sur la procédure elle-même, mais également sur le comportementdu chirurgien. Dans ce papier, nous étudions la classification automatique d'unensemble de procédures chirurgicales en utilisant l'algorithme Dynamic TimeWarping (DTW) pour calculer une mesure de similarité entre procédures chirurgicales.L'utilisation de DTW permet de se concentrer sur les différents typesd'activité effectués pendant la procédure, ainsi que sur leur séquencement touten réduisant les différences temporelles. Des expériences ont été menées sur 24procédures chirurgicales d'hernie discale lombaire dans le but de discriminer leniveau d'expertise des chirurgiens à partir d'une classification connue. A l'aided'un algorithme de clustering hiérarchique utilisant DTW nous avons retrouvédeux groupes de chirurgiens présentant des niveaux d'expertise différents (junioret senior).	Germain Forestier, Florent Lalys, Laurent Riffaud, Brivael Trelhu, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1001153	http://editions-rnti.fr/render_pdf.php?p=1001153	333	fr	fr		Clustering de séquence d' activité pour le étude de procédure neurochirurgicales  le utilisation de modèle de procédure chirurgical ( Surgical ProcessModel , SPM ) avoir récemment émerger dans le domaine de le conception d' outilsd'intervention chirurgical assister par ordinateur . ce modèle , qui être utiliséspour analyser et évaluer le intervention , représenter un procédure chirurgical ( Surgical Process , SP ) qui être formaliser comme un structure symboliquesdécrivant un chirurgie à un niveau de granularité donner . un enjeu importantréside dans le définition de métrique permettre le comparaison et le évaluationde ce procédure . ainsi , le relation entre ce métrique et des donnéespré-opératoires permettre de classer le chirurgie pour mettre en lumière desinformations sur le procédure lui-même , mais également sur le comportementdu chirurgien . Dans ce papier , nous étudier le classification automatique d' unensemble de procédure chirurgical en utiliser le algorithme Dynamic TimeWarping ( DTW ) pour calculer un mesure de similarité entre procédure chirurgical . le utilisation de DTW permettre de clr concentrer sur le différent typesd'activité effectuer pendant le procédure , ainsi que sur son séquencement touten réduire le différence temporel . un expérience avoir être mener sur 24procédures chirurgical d' hernie discal lombaire dans le but de discriminer leniveau d' expertise des chirurgien à partir d' un classification connaître . A le aided'un algorithme de clustering hiérarchique utiliser DTW nous avoir retrouvédeux groupe de chirurgien présenter un niveau d' expertise différent ( junioret senior ) . 	Clustering de séquences d'activités pour l'étude de procédures neurochirurgicales	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering hiérarchique non paramétrique de données fonctionnelles	Dans cet article, il est question de clustering de courbes. Nous proposonsune méthode non paramétrique qui segmente les courbes en clusters etdiscrétise en intervalles les variables continues décrivant les points de la courbe.Le produit cartésien de ces partitions forme une grille de données qui est inféréeen utilisant une approche Bayésienne de sélection de modèle ne faisant aucunehypothèse concernant les courbes. Enfin, une technique de post-traitement, visantà réduire le nombre de clusters dans le but d'améliorer l'interprétabilitédes clusters, est proposée. Elle consiste à fusionner successivement et de façonoptimale les clusters, ce qui revient à réaliser une classification hiérarchique ascendantedont la mesure de dissimilarité correspond à la variation du critère.De manière intéressante, cette mesure est en fait une somme pondérée de divergencesde Kullback-Leibler entre les distributions des clusters avant et aprèsfusions. L'intérêt de l'approche dans le cadre de l'analyse exploratoire de donnéesfonctionnelles est illustré par un jeu de données artificiel et réel.	Marc Boullé, Romain Guigourès, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1001137	http://editions-rnti.fr/render_pdf.php?p=1001137	334	fr	fr	@orange.com, @univ-paris1.fr	Clustering hiérarchique non paramétrique de donnée fonctionnelles  Dans ce article , il être question de clustering de courbe . Nous proposonsune méthode non paramétrique qui segmenter le courbe en clusters etdiscrétise en intervalle le variable continu décrire le point de le courbe . le produit cartésien de ce partition former un grille de donnée qui être inféréeen utiliser un approche Bayésienne de sélection de modèle ne faire aucunehypothèse concerner le courbe . enfin , un technique de post-traitement , visantà réduire le nombre de clusters dans le but d' améliorer le interprétabilitédes clusters , être proposer . Elle consister à fusionner successivement et de façonoptimale le clusters , ce qui revenir à réaliser un classification hiérarchique ascendantedont le mesure de dissimilarité correspondre à le variation du critère . De manière intéressant , ce mesure être en fait un somme pondérer de divergencesde Kullback-Leibler entre le distribution des clusters avant et aprèsfusions . le intérêt de le approche dans le cadre de le analyse exploratoire de donnéesfonctionnelles être illustrer par un jeu de donnée artificiel et réel . 	Clustering hiérarchique non paramétrique de données fonctionnelles	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering multi-niveaux de graphes : hiérarchique et topologique		Nhat-Quang Doan, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001205	http://editions-rnti.fr/render_pdf.php?p=1001205	335	fr		@univ-paris13.fr	Clustering multi-niveaux de graphes : hiérarchique et topologique 	Clustering multi-niveaux de graphes : hiérarchique et topologique	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Combinaison de classificateurs simples pour une sélection rapide de caractéristiques	La sélection de caractéristiques est une technique permettant de choisirles caractéristiques les plus pertinentes, celles adaptées à la résolution d'unproblème particulier. Les méthodes classiques présentent certains inconvénients.Par exemple, elles peuvent être trop complexes, elles peuvent faire dépendreles caractéristiques sélectionnées du classificateur utilisé, elles risquent de sélectionnerdes caractéristiques redondantes. Dans le but de limiter ces inconvénients,nous proposons dans cet article une nouvelle méthode rapide de sélectionde caractéristiques basée sur la construction et la sélection de classificateurssimples associés à chacune des caractéristiques. Une optimisation par unalgorithme génétique est proposée afin de trouver la meilleure combinaison desclassificateurs. Différentes méthodes de combinaison sont considérées et adaptéesà notre problème. Cette méthode a été appliquée sur différents ensemblesde caractéristiques de tailles variées et construite à partir de la base de chiffresmanuscrits MNIST. Les résultats obtenus montrent la robustesse de l'approcheainsi que l'efficacité de la méthode. En moyenne, le nombre de caractéristiquessélectionnées a diminué de 69,9% tout en conservant le taux de reconnaissance.	Hassan Chouaib, Florence Cloppet, Salvatore-Antoine Tabbone, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001196	http://editions-rnti.fr/render_pdf.php?p=1001196	336	fr	fr	@parisdescartes.fr, @loria.fr	combinaison de classificateur simple pour un sélection rapide de caractéristiques  le sélection de caractéristique être un technique permettre de choisirles caractéristique le plus pertinent , celui adapter à le résolution d' unproblème particulier . le méthode classique présenter certain inconvénient . Par exemple , elles pouvoir être trop complexe , elles pouvoir faire dépendreles caractéristique sélectionner du classificateur utiliser , elles risquer de sélectionnerdes caractéristique redondant . Dans le but de limiter ce inconvénient , nous proposer dans ce article un nouveau méthode rapide de sélectionde caractéristique baser sur le construction et le sélection de classificateurssimples associer à chacun des caractéristique . un optimisation par unalgorithme génétique être proposer afin de trouver le meilleur combinaison desclassificateurs . différent méthode de combinaison être considérer et adaptéesà son problème . ce méthode avoir être appliquer sur différent ensemblesde caractéristique de taille varier et construire à partir de le base de chiffresmanuscrits MNIST . le résultat obtenir montrer le robustesse de le approcheainsi que le efficacité de le méthode . En moyenne , le nombre de caractéristiquessélectionnées avoir diminuer de 69 , 9 \% tout en conserver le taux de reconnaissance . 	Combinaison de classificateurs simples pour une sélection rapide de caractéristiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Combinaison de classification supervisée et non-supervisée par la théorie des fonctions de croyance	Nous proposons dans cet article une nouvelle approche de classificationfondée sur la théorie des fonctions de croyance. Cette méthode repose surla fusion entre la classification supervisée et la classification non supervisée. Eneffet, nous sommes face à un problème de manque de données d'apprentissagepour des applications dont les résultats de classification supervisée et non superviséesont très variables selon les classificateurs employés. Les résultats ainsiobtenus sont par conséquent considérés comme incertains.Notre approche se propose de combiner les résultats des deux types de classificationen exploitant leur complémentarité via la théorie des fonctions de croyance.Celle-ci permet de tenir compte de l'aspect d'incertitude et d'imprécision. Aprèsavoir dresser les différentes étapes de notre nouveau schéma de classification,nous détaillons la fusion de classificateurs. Cette nouvelle approche est appliquéesur des données génériques, issues d'une vingtaine de bases de données.Les résultats obtenus ont montré l'efficacité de l'approche proposée.	Fatma Karem, Mounir Dhibi, Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001143	http://editions-rnti.fr/render_pdf.php?p=1001143	337	fr	fr	@yahoo.fr, @ensta-bretagne.fr, @univ-rennes1.fr	combinaison de classification superviser et non- superviser par le théorie des fonction de croyance  Nous proposer dans ce article un nouveau approche de classificationfondée sur le théorie des fonction de croyance . ce méthode reposer surla fusion entre le classification superviser et le classification non superviser . Eneffet , nous sommer face à un problème de manque de donnée d' apprentissagepour des application dont le résultat de classification superviser et non superviséesont très variable selon le classificateur employer . le résultat ainsiobtenus être par conséquent considérer comme incertain . son approche clr proposer de combiner le résultat des deux type de classificationen exploiter son complémentarité via le théorie des fonction de croyance . Celle _-ci permettre de tenir compte de le aspect d' incertitude et d' imprécision . Aprèsavoir dresser le différent étape de son nouveau schéma de classification , nous détailler le fusion de classificateur . ce nouveau approche être appliquéesur des donnée générique , issir d' un vingtaine de base de donnée . le résultat obtenir avoir montrer le efficacité de le approche proposer . 	Combinaison de classification supervisée et non-supervisée par la théorie des fonctions de croyance	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Community Detection in Social Networks with Attribute and Relationship Data		The Anh Dang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001204	http://editions-rnti.fr/render_pdf.php?p=1001204	338	en		@univ-paris13.fr	Community Detection in Social Networks with Attribute and Relationship Data 	Community Detection in Social Networks with Attribute and Relationship Data	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Découverte de règles d'association pour l'aide à la prévision des accidents maritimes	Les systèmes de surveillance maritime permettent la récupération et lafusion des informations sur les navires (position, vitesse, etc.) à des fins de suividu trafic maritime sur un dispositif d'affichage. Aujourd'hui, l'identification desrisques à partir de ces systèmes est difficilement automatisable compte-tenu del'expertise à formaliser, du nombre important de navires et de la multiplicité desrisques (collision, échouement, etc). De plus, le remplacement périodique desopérateurs de surveillance complique la reconnaissance d'événements anormauxqui sont éparses et parcellaires dans le temps et l'espace. Dans l'objectif de faireévoluer ces systèmes de surveillance maritime, nous proposons dans cet article,une approche originale fondée sur le data mining pour l'extraction de motifsfréquents. Cette approche se focalise sur des règles de prévision et de ciblagepour l'identification automatique des situations induisant ou constituant le cadredes accidents maritimes.	Bilal Idiri, Aldo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001192	http://editions-rnti.fr/render_pdf.php?p=1001192	339	fr	fr	@mines-paristech.fr	découverte de règle d' association pour le aide à le prévision des accident maritimes  le système de surveillance maritime permettre le récupération et lafusion des information sur le navire ( position , vitesse , etc. ) à un fin de suividu trafic maritime sur un dispositif d' affichage . Aujourd' hui , le identification desrisques à partir de ce système être difficilement automatisable compte-tenu del'expertise à formaliser , du nombre important de navire et de le multiplicité desrisques ( collision , échouement , etc ) . De plus , le remplacement périodique desopérateurs de surveillance compliquer le reconnaissance d' événement anormauxqui être épars et parcellaire dans le temps et le espace . Dans le objectif de faireévoluer ce système de surveillance maritime , nous proposer dans ce article , un approche original fonder sur le data mining pour le extraction de motifsfréquents . ce approche clr focaliser sur un règle de prévision et de ciblagepour le identification automatique des situation induire ou constituer le cadredes accident maritime . 	Découverte de règles d'association pour l'aide à la prévision des accidents maritimes	6
Revue des Nouvelles Technologies de l'Information	EGC	2012	Détection de groupes outliers en classification non supervisée	"Nous proposons dans ce papier une nouvelle méthode de détection degroupes outliers. Notre mesure nommée GOF (Group Outlier Factor) est estiméepar l'apprentissage non-supervisé. Nous l'avons intégré dans l'apprentissage descartes topologiques. Notre approche est basée sur la densité relative de chaquegroupe de données, et fournit simultanément un partitionnement des donnéeset un indicateur quantitatif (GOF) sur ""la particularité"" de chaque cluster ougroupe. Les résultats obtenus sont très encourageants et prometteurs pour continuerdans cette optique."	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001184	http://editions-rnti.fr/render_pdf.php?p=1001184	340	fr	fr	@univ-paris13.fr	détection de groupe outliers en classification non supervisée  " Nous proposer dans ce papier un nouveau méthode de détection degroupes outliers . . son mesure nommer GOF ( Group Outlier Factor ) être estiméepar le apprentissage non- superviser . . Nous l' avoir intégrer dans le apprentissage descartes topologique . . son approche être baser sur le densité relatif de chaquegroupe de donnée , et fournir simultanément un partitionnement des donnéeset un indicateur quantitatif ( GOF ) sur " " le particularité " " de chaque cluster ougroupe . . le résultat obtenir être très encourageant et prometteur pour continuerdans ce optique . " 	Détection de groupes outliers en classification non supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Détection non supervisée d'une sous-population par méthode d'ensemble et changement de représentation itératif	L'apprentissage non supervisé a classiquement pour objectif la détectionde sous-populations homogènes (classes) considérées de manière équivalentesans information a priori sur celles-ci. Le problème étudié dans cet articleest quelque peu distinct. On se focalise ici uniquement sur une sous-populationd'intérêt que l'on cherche à identifier avec un rappel et une précision optimales.Nous proposons, pour cela, une méthode s'appuyant sur les principes suivants :(1) travailler dans l'espace de représentation fourni par des experts faibles pourcette tâche, (2) confronter ces experts pour détecter des seuils de sélection pluspertinents, et (3) les combiner itérativement afin de converger vers l'expert idéal.Cette méthode est éprouvée et comparée sur des données synthétiques.	Christine Martin, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1001195	http://editions-rnti.fr/render_pdf.php?p=1001195	341	fr	fr	@agroparistech.fr	détection non superviser d' un sous-population par méthode d' ensemble et changement de représentation itératif  le apprentissage non superviser avoir classiquement pour objectif le détectionde sous-populations homogène ( classe ) considérer de manière équivalentesans information avoir priori sur celui _-ci . le problème étudier dans ce articleest quelque peu distinct . On clr focaliser ici uniquement sur un sous-populationd'intérêt que le on chercher à identifier avec un rappel et un précision optimal . Nous proposer , pour cela , un méthode clr appuyer sur le principe suivant : ( 1 ) travailler dans le espace de représentation fournir par un expert faible pourcette tâche , ( 2 ) confronter ce expert pour détecter un seuil de sélection pluspertinents , et ( 3 ) les combiner itérativement afin de converger vers le expert idéal . ce méthode être éprouver et comparer sur un donnée synthétique . 	Détection non supervisée d'une sous-population par méthode d'ensemble et changement de représentation itératif	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Development of a distributed recommender system using the Hadoop Framework	Producing high quality recommendations has become a challenge inthe recent years. Indeed, the growth in the quantity of data involved in the recommendationprocess pose some scalability and effectiveness problems. Theseissues have encouraged the research of new technologies. Instead of developinga new recommender system we improve an already existing method. A distributedframework was considered based on the known quality and simplicity ofthe MapReduce project. The Hadoop Open Source project played a fundamentalrole in this research. It undoubtedly encouraged and facilitated the constructionof our application, supplying all tools needed. Our main goal in this research wasto prove that building a distributed recommender system was not only possible,but simple and productive.	Raja Chiky, Renata Ghisloti, Zakia Kazi Aoul	http://editions-rnti.fr/render_pdf.php?p1&p=1001179	http://editions-rnti.fr/render_pdf.php?p=1001179	342	en	en	@isep.fr	development distribute recommender system used hadoop framework produce high quality recommendation become challenge inthe recent year indeed growth quantity data involved recommendationprocess pose scalability effectiveness problem theseissue encourage research new technology instead developinga new recommender system improve already exist method distributedframework consider base know quality simplicity ofthe mapreduce project hadoop open source project play fundamentalrole research undoubtedly encourage facilitate constructionof application supply tool need main goal research wasto prove build distribute recommender system possible but simple productive	Development of a distributed recommender system using the Hadoop Framework	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Evaluating Bayesian Networks by Sampling with Simplified Assumptions	The most common fitness evaluation for Bayesian networks in the presence of data is the Cooper-Herskovitz criterion. This technique involves massive amounts of data and, therefore, expansive computations. We propose a cheaper alternative evaluation method using simplified ssumptions which produces evaluations that are strongly correlated with the Cooper-Herskovitz criterion.	Saaid Baraty, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1001135	http://editions-rnti.fr/render_pdf.php?p=1001135	343	en	en	@cs.umb.edu	evaluate bayesian network sampling simplified assumption common fitness evaluation bayesian network presence datum cooper herskovitz criterion technique involve massive amount datum and therefore expansive computation propose cheaper alternative evaluation method used simplified ssumption produce evaluation strongly correlated cooper herskovitz criterion	Evaluating Bayesian Networks by Sampling with Simplified Assumptions	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Evaluation rapide du diamètre d'un graphe	"Lors de l'analyse de graphes, il est important de connaître leurs propriétésafin de pouvoir par exemple identifier leur structure et les comparer.Une des caractérisations importante de ces graphes repose sur le fait de déterminers'il s'agit ou non d'un ""petit monde"". Pour ce faire, la valeur du diamètredu graphe est essentielle. Or la mesure du diamètre est pour un très grandgraphe, une opération extrêmement longue. Nous proposons un algorithme endeux phases qui permet d'obtenir rapidement une estimation du diamètre d'ungraphe avec une proportion d'erreur faible. En réduisant cet algorithme à uneseule phase et en acceptant une marge d'erreur plus élevée, nous obtenons uneestimation très rapide du diamètre. Nous testons cet algorithme sur deux grandsgraphes de terrain (plus d'un million de noeuds) et comparons ses performancesavec celles d'un algorithme de référence BFS (Breadth-First Search). Les résultatsobtenus sont décrits et commentés."	Christian Belbeze, Max Chevalier, Chantal Soulé-Dupuy	http://editions-rnti.fr/render_pdf.php?p1&p=1001183	http://editions-rnti.fr/render_pdf.php?p=1001183	344	fr	fr	@belbeze.com, @irit.fr, @irit.fr	Evaluation rapide du diamètre d' un graphe  " Lors de le analyse de graphe , il être important de connaître son propriétésafin de pouvoir par exemple identifier son structure et les comparer . . un des caractérisation important de ce graphe reposer sur le fait de déterminers'il clr agir ou non d' un " " petit monde " " . . Pour ce faire , le valeur du diamètredu graphe être essentiel . . Or le mesure du diamètre être pour un très grandgraphe , un opération extrêmement long . . Nous proposer un algorithme endeux phase qui permettre d' obtenir rapidement un estimation du diamètre d' ungraphe avec un proportion d' erreur faible . . En réduire ce algorithme à uneseule phase et en accepter un marge d' erreur plus élever , nous obtenir uneestimation très rapide du diamètre . . Nous tester ce algorithme sur deux grandsgraphes de terrain ( plus d' un million de noeud ) et comparer son performancesavec celui d' un algorithme de référence BFS ( Breadth-First Search ) . . le résultatsobtenus être décrire et commenter . " 	Evaluation rapide du diamètre d'un graphe	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Exploitation de l'asymétrie entre termes pour l'extraction automatique de taxonomies à partir de textes	Nous présentons dans cet article une nouvelle approche pour la générationautomatique de structures lexicales (ou taxonomies) à partir de textes.Cette tâche est fondée sur l'hypothèse forte selon laquelle l'accumulation defaits statistiques simples sur les usages en corpus permet d'approximer des informationsde niveau sémantique sur le lexique. Nous utilisons la prétopologiecomme cadre de travail afin de formaliser et de combiner plusieurs hypothèsessur les usages terminologiques et enfin de structurer le lexique sous la formed'une taxonomie. Nous considérons également le problème de l'évaluation destaxonomies résultantes et proposons un nouvel indice afin de les comparer et depositionner notre approche par rapport à la littérature.	Davide Buscaldi, Guillaume Cleuziou, Gaël Dias, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1001200	http://editions-rnti.fr/render_pdf.php?p=1001200	345	fr	fr	@univ-orleans.fr, @unicaen.fr, @cesi.fr	exploitation de le asymétrie entre terme pour le extraction automatique de taxonomie à partir de textes  Nous présenter dans ce article un nouveau approche pour le générationautomatique de structure lexical ( ou taxonomie ) à partir de texte . ce tâche être fonder sur le hypothèse fort selon laquelle le accumulation defaits statistique simple sur le usage en corpus permettre d' approximer des informationsde niveau sémantique sur le lexique . Nous utiliser le prétopologiecomme cadre de travail afin de formaliser et de combiner plusieurs hypothèsessur le usage terminologique et enfin de structurer le lexique sous le formed'une taxonomie . Nous considérer également le problème de le évaluation destaxonomies résultante et proposer un nouveau indice afin de les comparer et depositionner son approche par rapport à le littérature . 	Exploitation de l'asymétrie entre termes pour l'extraction automatique de taxonomies à partir de textes	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de co-variations entre des propriétés de sommets et leur position topologique dans un graphe attribué	L'analyse de grands réseaux est très étudiée en fouille de données.Toutefois, les approches existantes proposent une analyse soit à un niveau macroscopique(étude des propriétés globales comme la distribution des degrés),soit à un niveau microscopique (extraction de sous-graphes fréquents ou denses).Nous proposons une nouvelle méthode qui effectue une analyse intermédiairepermettant de découvrir des motifs regroupant des propriétés microscopiques etmacroscopiques du réseau. Ces motifs capturent des co-variations entre des propriétésnumériques relatives aux sommets. Par exemple, un motif mésoscopiquedans un réseau de co-auteurs peut être plus le nombre de publications à EGC estimportant, plus la centralité des sommets correspondants dans le réseau l'estégalement. Notre contribution est multiple. D'abord, ce travail est le premierà exploiter conjointement des propriétés locales et des propriétés topologiques.De plus, nous produisons de nouvelles avancées dans le domaine de l'extractionde co-variations en revisitant les motifs émergents dans ce contexte. Enfin, nousrapportons une analyse d'un réseau bibliographique réel issu de DBLP.	Adriana Prado, Marc Plantevit, Celine Robardet, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001171	http://editions-rnti.fr/render_pdf.php?p=1001171	346	fr	fr		extraction de co- variation entre un propriété de sommet et son position topologique dans un graphe attribué  le analyse de grand réseau être très étudier en fouille de donnée . toutefois , le approche existant proposer un analyse être à un niveau macroscopique ( étude des propriété global comme le distribution des degré ) , soit à un niveau microscopique ( extraction de sous-graphes fréquent ou dense ) . Nous proposer un nouveau méthode qui effectuer un analyse intermédiairepermettant de découvrir un motif regrouper un propriété microscopique etmacroscopiques du réseau . ce motif capturer des co- variation entre un propriétésnumériques relatif aux sommet . Par exemple , un motif mésoscopiquedans un réseau de co- auteur pouvoir être plus le nombre de publication à EGC estimportant , plus le centralité des sommet correspondant dans le réseau le estégalement . son contribution être multiple . D' abord , ce travail être le premierà exploiter conjointement un propriété local et des propriété topologique . De plus , nous produire de nouveau avancer dans le domaine de le extractionde co- variation en revisiter le motif émergent dans ce contexte . enfin , nousrapportons un analyse d' un réseau bibliographique réel issir de DBLP . 	Extraction de co-variations entre des propriétés de sommets et leur position topologique dans un graphe attribué	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de Dépendances Fonctionnelles Approximatives	La découverte de dépendances fonctionnelles (DF) à partir d'une relationexistante est une technique importante pour l'analyse de Bases de Données.L'ensemble des DF exactes ou approximatives extraites par les algorithmes existantsest valide tant que la relation n'est pas modifiée. Ceci est insuffisant pourdes situations réelles où les relations sont constamment mises à jour.Nous proposons une approche incrémentale qui maintiens à jour l'ensemble desDF valides, exactes ou approximatives selon une erreur donnée, quand des tuplessont insérés et supprimés. Les résultats expérimentaux indiquent que lors de l'extractionde DF à partir d'une relation continuellement modifiée, les algorithmesexistants sont sensiblement dépassés par notre stratégie incrémentale.	Noel Novelli, Ekaterina Simonenko	http://editions-rnti.fr/render_pdf.php?p1&p=1001503	http://editions-rnti.fr/render_pdf.php?p=1001503	347	fr	fr	@lri.fr, @lif.univ-mrs.fr	extraction de Dépendances Fonctionnelles Approximatives  le découverte de dépendance fonctionnel ( DF ) à partir d' un relationexistante être un technique important pour le analyse de base de Données . le ensemble des DF exact ou approximatif extraire par le algorithme existantsest valider tant que le relation n' être pas modifier . ceci être insuffisant pourdes situation réel où le relation être constamment mettre à jour . Nous proposer un approche incrémentale qui maintenir à jour le ensemble desDF valide , exact ou approximatif selon un erreur donner , quand des tuplessont insérer et supprimer . le résultat expérimental indiquer que lors de le extractionde DF à partir d' un relation continuellement modifier , le algorithmesexistants être sensiblement dépasser par son stratégie incrémentale . 	Extraction de Dépendances Fonctionnelles Approximatives	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de Liens Fréquents dans les Réseaux Sociaux	"Cet article présente FLMin, une nouvelle méthode d'extraction de motifsfréquents dans les réseaux sociaux. Contrairement aux méthodes traditionnellesqui s'intéressent uniquement aux régularités structurelles, l'originalité denotre approche réside dans sa capacité à exploiter la structure et les attributs desnoeuds pour extraire des régularités, que nous appelons ""liens fréquents"", dansles liens entre des noeuds partageant des caractéristiques communes."	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001174	http://editions-rnti.fr/render_pdf.php?p=1001174	348	fr	fr	@univ-ag.fr	extraction de lien Fréquents dans le réseau Sociaux  " ce article présent FLMin , un nouveau méthode d' extraction de motifsfréquents dans le réseau social . . contrairement aux méthode traditionnellesqui clr intéresser uniquement aux régularité structurel , le originalité denotre approcher résider dans son capacité à exploiter le structure et le attribut desnoeuds pour extraire un régularité , que nous appeler " " lien fréquent " " , dansles lien entre un noeud partager un caractéristique commun . " 	Extraction de Liens Fréquents dans les Réseaux Sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de séquences fréquentes avec intervalles d'incertitude	"Lors de l'extraction des séquences, la granularité temporelle est plusou moins importante selon les besoins des utilisateurs et les contraintes du domained'application. Nous proposons un algorithme d'extraction de séquencesfréquentes par intervalles à partir de séquences à estampilles temporelles discrètes.Nous intégrons une relaxation des contraintes temporelles en introduisantla définition de ""séquences temporelles par intervalles"" (STI). Ces intervalles reflètentune incertitude sur les occurrences précises des évènements. Nous formalisonsce nouveau concept en exhibant certaines de ses propriétés et nous menonsquelques expériences afin de comparer (qualitativement) nos résultats avec uneautre proposition assez proche de la nôtre"	Asma Ben Zakour, Sofian Maabout, Mohamed Mosbah, Marc Sistiaga	http://editions-rnti.fr/render_pdf.php?p1&p=1001160	http://editions-rnti.fr/render_pdf.php?p=1001160	349	fr	fr	@labri.fr, @labri.fr, @2moro.fr, @2moro.fr	extraction de séquence fréquent avec intervalle d' incertitude  " Lors de le extraction des séquence , le granularité temporel être plusou moins important selon le besoin des utilisateur et le contrainte du domained'application . . Nous proposer un algorithme d' extraction de séquencesfréquentes par intervalle à partir de séquence à estampille temporel discret . . Nous intégrer un relaxation des contrainte temporel en introduisantla définition de " " séquence temporel par intervalle " " ( STI ) . . ce intervalle reflètentune incertitude sur le occurrence précis des évènements . . Nous formalisonsce nouveau concept en exhiber certains de son propriété et lui menonsquelques expérience afin de comparer ( qualitativement ) son résultat avec uneautre proposition assez proche de le nôtre " 	Extraction de séquences fréquentes avec intervalles d'incertitude	2
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de sous-parties ciblées d'une ontologie généraliste pour enrichir une ontologie particulière	Différentes ressources ontologiques généralistes de très grande tailleont été développées de façon collective et sont aujourd'hui disponibles sur leweb. Ainsi l'ontologie YAGO est une énorme base de connaissances décrivantplus de 2 millions d'entités. Afin de tirer parti de ce gigantesque travail collectif,nous montrons comment en extraire des sous-parties thématiquement focaliséespour enrichir une autre ontologie, dite cible, de taille plus limitée mais de domainecentré sur une application particulière 1.	Fayçal Hamdi, Brigitte Safar, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001176	http://editions-rnti.fr/render_pdf.php?p=1001176	350	fr	fr	@lri.fr	extraction de sous-parties cibler d' un ontologie généraliste pour enrichir un ontologie particulière  différent ressource ontologique généraliste de très grand tailleont être développer de façon collectif et être aujourd' hui disponible sur leweb . ainsi le ontologie YAGO être un énorme base de connaissance décrivantplus de 2 million d' entité . Afin de tirer parti de ce gigantesque travail collectif , nous montrer comment en extraire un sous-parties thématiquement focaliséespour enrichir un autre ontologie , dire cible , de taille plus limiter mais de domainecentré sur un application particulier 1 . 	Extraction de sous-parties ciblées d'une ontologie généraliste pour enrichir une ontologie particulière	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction d'opinions appliquée à des critères	Les technologies de l'information et le succès des services associés(e.g., blogs, forums,...) ont ouvert la voie à un mode d'expression massive d'opinionssur les sujets les plus variés. Récemment, de nouvelles techniques de détectionautomatique d'opinions (opinion mining) ont fait leur apparition et viades analyses statistiques des avis exprimés, tendent à dégager une tendance globaledes opinions exprimées par les internautes. Néanmoins une analyse plusfine de celle-ci montre que les arguments avancés par les internautes relèvent decritères de jugement distincts. Ici, un film sera décrié pour un scénario décousu,là il sera encensé pour une bande son époustouflante. Dans cet article, nous proposons,après avoir caractérisé automatiquement des critères dans un document,d'en extraire l'opinion relative. A partir d'un ensemble restreint de mots clésd'opinions, notre approche construit automatiquement une base d'apprentissagede documents issus du web et en déduit un lexique de mots ou d'expressionsd'opinions spécifiques au domaine d'application. Des expériences menées surdes jeux de données réelles illustrent l'efficacité de l'approche.	Benjamin Duthil, François Trousset, Gérard Dray, Pascal Poncelet, Jacky Montmain	http://editions-rnti.fr/render_pdf.php?p1&p=1001197	http://editions-rnti.fr/render_pdf.php?p=1001197	351	fr	fr	@mines-ales.fr, @lirmm.fr	extraction d' opinion appliquer à un critères  le technologie de le information et le succès des service associer ( exempli gratia , blog , forum , ... ) avoir ouvrir le voie à un mode d' expression massif d' opinionssur le sujet le plus varier . récemment , un nouveau technique de détectionautomatique d' opinion ( opinion mining ) avoir faire son apparition et viades analyse statistique des avis exprimer , tendre à dégager un tendance globaledes opinion exprimer par le internaute . néanmoins un analyse plusfine de celui _-ci montrer que le argument avancer par le internaute relever decritères de jugement distinct . ici , un film être décrier pour un scénario découdre , là il être encenser pour un bande son époustouflant . Dans ce article , nous proposer , après avoir caractériser automatiquement un critère dans un document , d' en extraire le opinion relatif . A partir d' un ensemble restreindre de mot clésd'opinions , son approche construire automatiquement un base d' apprentissagede document issir du web et en déduit un lexique de mot ou d' expressionsd'opinions spécifique au domaine d' application . un expérience mener surdes jeu de donnée réel illustrer le efficacité de le approche . 	Extraction d'opinions appliquée à des critères	3
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction et gestion d'informations pour la construction d'une base vidéo d'apprentissage	"Indexer une vidéo consiste à rattacher un ou plusieurs concepts à dessegments de cette vidéo, un concept étant défini comme une représentation intellectuelled'une idée abstraite. L'indexation automatique se base sur l'extractionautomatique de caractéristiques fournies par un système de traitement d'images.Cependant, il est nécessaire de définir les index ou concepts. Pour cela il fautdéfinir le lien qui existe entre ces caractéristiques et ces concepts. Ce qui sépareles caractéristiques extraites sur lesquelles se base l'indexation automatique etles concepts est appelé fossé sémantique qui est le manque de concordance entreles informations que les machines peuvent extraire depuis les documents numériqueset les interprétations que les humaines en font. La définition d'un conceptpeut être faite automatiquement si l'on dispose d'une base d'apprentissage liéeau concept. Dans ce cas, il est possible ""d'apprendre"" le concept de manièrestatistique. Mais la construction de cette base d'apprentissage nécessite de faireintervenir un utilisateur ou un expert applicatif. En fait, il s'agit de s'appuyer surses connaissances pour extraire des segments vidéo représentatifs du conceptque l'on souhaite définir. On peut lui demander d'indexer manuellement la based'apprentissage, mais cette opération est longue et fastidieuse. Dans cet article,nous proposons une méthode qui permet d'extraire l'expertise pour que l'implicationde l'expert soit la plus simple et la plus limitée possible."	Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1001190	http://editions-rnti.fr/render_pdf.php?p=1001190	352	fr	fr	@litii.com	extraction et gestion d' information pour le construction d' un base vidéo d' apprentissage  " indexer un vidéo consister à rattacher un ou plusieurs concept à dessegments de ce vidéo , un concept être définir comme un représentation intellectuelled'une idée abstraire . . le indexation automatique clr baser sur le extractionautomatique de caractéristique fournir par un système de traitement d' image . . cependant , il être nécessaire de définir le index ou concept . . Pour cela il fautdéfinir le lien qui exister entre ce caractéristique et ce concept . . Ce qui sépareles caractéristique extraire sur lesquelles clr baser le indexation automatique etles concept être appeler fossé sémantique qui être le manque de concordance entreles information que le machine pouvoir extraire depuis le document numériqueset le interprétation que le humain en faire . . le définition d' un conceptpeut être faire automatiquement si le on disposer d' un base d' apprentissage liéeau concept . . Dans ce cas , il être possible " " d' apprendre " " le concept de manièrestatistique . . Mais le construction de ce base d' apprentissage nécessiter de faireintervenir un utilisateur ou un expert applicatif . . En fait , il clr agir de clr appuyer surses connaissance pour extraire un segment vidéo représentatif du conceptque le on souhaiter définir . . On pouvoir lui demander d' indexer manuellement le based'apprentissage , mais ce opération être long et fastidieux . . Dans ce article , nous proposer un méthode qui permettre d' extraire le expertise pour que le implicationde le expert être le plus simple et le plus limiter possible . " 	Extraction et gestion d'informations pour la construction d'une base vidéo d'apprentissage	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction incrémentale de séquences fréquentes dans un flux d'itemsets		Thomas Guyet, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1001206	http://editions-rnti.fr/render_pdf.php?p=1001206	353	fr		@agrocampus-ouest.fr, @inria.fr	Extraction incrémentale de séquences fréquentes dans un flux d'itemsets 	Extraction incrémentale de séquences fréquentes dans un flux d'itemsets	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Human Detection by a Small Autonomous Mobile Robot	Nous proposons une méthode utilisant les histogrammes de gradientorienté (HOG) et les séparateurs à vaste marge (SVM) pour la détection de personnesà partir d'images prises depuis un petit robot mobile autonome. Les travauxantérieurs réalisés dans le domaine de la détection d'êtres humains à partird'images ne peuvent pas être employés pour ce type d'application car ils supposentque les images sont prises à partir d'une position élevée (au moins lahauteur d'un petit enfant) alors que la taille de notre robot n'est que de 15cm.Nous employons à la fois les HOG et les SVM car cette combinaison de méthodesest reconnue comme étant celle ayant le plus de succès pour la détectionde personnes. Pour traiter une grande variété de formes humaines, principalementen raison de la distance existant entre les personnes et le robot, nous avonsdéveloppé une nouvelleméthode de prédiction à deux étapes utilisant deux typesde classificateurs SVM qui reposent sur une estimation de la distance. L'estimationest basée sur une proportion de pixels de couleur de peau dans l'image, cequi nous permet de clairement séparer notre problème de la détection de corpsentier et de celle de corps partiel. Les essais réalisés dans un bureau ont montrédes résultats prometteurs de notre méthode avec une valeur de F de 0,93.	Kouhei Takemoto, Shigeru Takano, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001172	http://editions-rnti.fr/render_pdf.php?p=1001172	354	en	fr	@sls.kyushu-u.ac.jp, @inf.kyushu-u.ac.jp, @inf.kyushu-u.ac.jp	Nous proposer un méthode utiliser le histogramme de gradientorienté ( HOG ) et le séparateur à vaste marge ( SVM ) pour le détection de personnesà partir d' image prendre depuis un petit robot mobile autonome . le travauxantérieurs réaliser dans le domaine de le détection d' être humain à partird'images ne pouvoir pas être employer pour ce type d' application car ils supposentque le image être prendre à partir d' un position élever ( au moins lahauteur d' un petit enfant ) alors que le taille de son robot n' être que de 15cm . Nous employer à le foi le HOG et le SVM car ce combinaison de méthodesest reconnaître comme être celui avoir le plus de succès pour le détectionde personne . Pour traiter un grand variété de forme humain , principalementen raison de le distance existant entre le personne et le robot , nous avonsdéveloppé un nouvelleméthode de prédiction à deux étape utiliser deux typesde classificateur SVM qui reposer sur un estimation de le distance . le estimationest baser sur un proportion de pixel de couleur de peau dans le image , cequi nous permettre de clairement séparer son problème de le détection de corpsentier et de celui de corps partiel . le essai réaliser dans un bureau avoir montrédes résultat prometteur de son méthode avec un valeur de F de 0_,_93 . 	Human Detection by a Small Autonomous Mobile Robot	2
Revue des Nouvelles Technologies de l'Information	EGC	2012	Identification et caractérisation de différents types de boycott par des méthodes d'Analyse de Données		Henri Ralambondrainy, Marinette Amirault-Thébault	http://editions-rnti.fr/render_pdf.php?p1&p=1001207	http://editions-rnti.fr/render_pdf.php?p=1001207	355	fr		@univ-reunion.fr, @iae.univ-poitiers.fr	Identification et caractérisation de différents types de boycott par des méthodes d'Analyse de Données 	Identification et caractérisation de différents types de boycott par des méthodes d'Analyse de Données	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	K-moyennes contraintes par un classifieur Application à la personnalisation de scores de campagnes	Lorsqu'on désire contacter un client pour lui proposer un produit oncalcule au préalable la probabilité qu'il achètera ce produit. Cette probabilitéest calculée à l'aide d'un modèle prédictif pour un ensemble de clients. Le servicemarketing contacte ensuite ceux ayant la plus forte probabilité d'acheter leproduit. En parallèle, et avant le contact commercial, il peut être intéressant deréaliser une typologie des clients qui seront contactés. L'idée étant de proposerdes campagnes différenciées par groupe de clients. Cet article montre commentil est possible de contraindre la typologie, réalisée à l'aide des k-moyennes, àrespecter la proximité des clients vis-à-vis de leur score d'appétence.	Vincent Lemaire, Nicolas Creff, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1001155	http://editions-rnti.fr/render_pdf.php?p=1001155	356	fr	fr		K-moyennes contraindre par un classifieur Application à le personnalisation de score de campagnes  Lorsqu' on désirer contacter un client pour lui proposer un produit oncalcule au préalable le probabilité qu' il acheter ce produit . ce probabilitéest calculer à le aide d' un modèle prédictif pour un ensemble de client . le servicemarketing contacter ensuite celui avoir le plus fort probabilité d' acheter leproduit . En parallèle , et avant le contact commercial , il pouvoir être intéressant deréaliser un typologie des client qui être contacter . le idée être de proposerdes campagne différencier par groupe de client . ce article montrer commentil être possible de contraindre le typologie , réaliser à le aide des k-moyennes , àrespecter le proximité des client vis-à-vis de son score d' appétence . 	K-moyennes contraintes par un classifieur Application à la personnalisation de scores de campagnes	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	L'extraction de règles de dépendance bien définies entre ensembles de variables multivaluées	Cet article étudie la faisabilité et l'intérêt de l'extraction de règles dedépendance entre ensembles de variables multivaluées en comparaison du problèmebien connu de l'extraction des règles d'association fréquentes. Une règlede dépendance correspond à une dépendance fonctionnelle approximative caractériséeprincipalement par l'entropie conditionnelle associée. L'article montrecomment établir une analogie formelle entre les deux familles de règles et commentadapter à l'aide de cette analogie l'algorithme « Eclat » afin d'extraire d'unjeu de données les règles de dépendance dites bien définies. Une étude expérimentaleconclut sur les forces et inconvénients des règles de dépendance biendéfinies vis-à-vis des règles d'association fréquentes	Frédéric Pennerath	http://editions-rnti.fr/render_pdf.php?p1&p=1001168	http://editions-rnti.fr/render_pdf.php?p=1001168	357	fr	fr		le extraction de règle de dépendance bien définir entre ensemble de variable multivaluées  ce article étudier le faisabilité et le intérêt de le extraction de règle dedépendance entre ensemble de variable multivaluées en comparaison du problèmebien connaître de le extraction des règle d' association fréquent . un règlede dépendance correspondre à un dépendance fonctionnel approximatif caractériséeprincipalement par le entropie conditionnel associer . le article montrecomment établir un analogie formel entre le deux famille de règle et commentadapter à le aide de ce analogie le algorithme « Eclat » afin d' extraire d' unjeu de donnée le règle de dépendance dire bien définir . un étude expérimentaleconclut sur le force et inconvénient des règle de dépendance biendéfinies vis-à-vis un règle d' association fréquentes 	L'extraction de règles de dépendance bien définies entre ensembles de variables multivaluées	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Méta-règles pour la génération de règles négatives	La littérature s'est beaucoup intéressée à l'extraction de règles classiques(ou positives) et peu à l'extraction des règles négatives en raison essentiellementd'une part, du coût de calculs et d'autre part, du nombre prohibitif derègles redondantes et inintéressantes extraites. La démarche que nous avons retenueest de dégager les règles négatives lors de l'extraction des règles positives,et pour cela, nous recherchons les règles négatives que l'on peut inférer ou pas àpartir de la pertinence d'une règle positive. Ces différentes inférences vont êtreformalisées par un ensemble de méta-règles.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001162	http://editions-rnti.fr/render_pdf.php?p=1001162	358	fr	fr	@isima, @isima	Méta-règles pour le génération de règle négatives  le littérature clr être beaucoup intéresser à le extraction de règle classique ( ou positif ) et peu à le extraction des règle négatif en raison essentiellementd'une part , du coût de calcul et d' autre part , du nombre prohibitif derègles redondant et inintéressant extraire . le démarche que nous avoir retenueest de dégager le règle négatif lors de le extraction des règle positif , et pour cela , nous rechercher le règle négatif que le on pouvoir inférer ou pas àpartir de le pertinence d' un règle positif . ce différent inférence aller êtreformalisées par un ensemble de méta-règles . 	Méta-règles pour la génération de règles négatives	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Mining Genetic Interactions in Genome-Wide Association Study	Advanced biotechnologies have rendered feasible high-throughput data collecting in human and other model organisms. The availability of such data holds promise for dissecting complex biological processes. Making sense of the flood of biological data poses great statistical and computational challenges. I will discuss the problem of mining gene-gene interactions in high-throughput genetic data. Finding genetic interactions is an important biological problem since many common diseases are caused by joint effects of genes. Previously, it was considered intractable to find genetic interactions in the whole-genome scale due to the enormous search space. The problem was commonly addressed using heuristics which do not guarantee the optimality of the solution. I will show that by utilizing the upper bound of the test statistic and effectively indexing the data, we can dramatically prune the search space and reduce computational burden. Moreover, our algorithms guarantee to find the optimal solution. In addition to handling specific statistical tests, our algorithms can be applied to a wide range of study types by utilizing convexity, a common property of many commonly used statistics.	Wei Wang    	http://editions-rnti.fr/render_pdf.php?p1&p=1001128	http://editions-rnti.fr/render_pdf.php?p=1001128	359	en	en	@cs.unc.edu	mining genetic interaction genome wide association study advanced biotechnology render feasible high throughput datum collect human model organism availability data hold promise dissect complex biological process make sense flood biological data pose great statistical computational challenge discuss problem mining gene gene interaction high throughput genetic data finding genetic interaction important biological problem since many common disease cause joint effect gene previously consider intractable find genetic interaction whole genome scale due enormous search space problem commonly address used heuristic guarantee optimality solution show utilize upper bound test statistic effectively indexing datum dramatically prune search space reduce computational burden moreover algorithms guarantee find optimal solution addition handle specific statistical test algorithm applied wide range study type utilize convexity common property many commonly used statistic	Mining Genetic Interactions in Genome-Wide Association Study	
Revue des Nouvelles Technologies de l'Information	EGC	2012	Modèle de supervision d'interactions non-intrusif basé sur les ontologies	L'automatisation et la supervision des systèmes pervasifs est à l'heureactuelle principalement basée sur l'utilisation massive de capteurs distribuésdans l'environnement. Dans cet article, nous proposons un modèle de supervisiond'interactions basé sur l'analyse sémantique des logs domotiques (commandesémises par l'utilisateur), visant à limiter l'utilisation de ces capteurs :le principe est d'utiliser des outils d'inférences avancés, afin de déduire les informationshabituellement captées. Pour cela, une ontologie, automatiquementdérivée d'un processus dirigé par les modèles, définit les interactions utilisateursystème.L'utilisation d'un système de règles permet ensuite d'inférer des informationssur la localisation et l'intention de l'utilisateur, dans le but de réaliserdu monitoring et de proposer des services domotiques adaptés.	Willy Allègre, Thomas Burger, Pascal Berruet, Jean-Yves Antoine	http://editions-rnti.fr/render_pdf.php?p1&p=1001148	http://editions-rnti.fr/render_pdf.php?p=1001148	360	fr	fr	@univ-ubs.fr	modèle de supervision d' interaction non- intrusif baser sur le ontologies  le automatisation et le supervision des système pervasifs être à le heureactuelle principalement baser sur le utilisation massif de capteur distribuésdans le environnement . Dans ce article , nous proposer un modèle de supervisiond'interactions baser sur le analyse sémantique des logs domotiques ( commandesémises par le utilisateur ) , viser à limiter le utilisation de ce capteur : le principe être d' utiliser un outil d' inférence avancer , afin de déduire le informationshabituellement capter . Pour cela , un ontologie , automatiquementdérivée d' un processus diriger par le modèle , définir le interaction utilisateursystème . le utilisation d' un système de règle permettre ensuite d' inférer un informationssur le localisation et le intention de le utilisateur , dans le but de réaliserdu monitoring et de proposer un service domotiques adapter . 	Modèle de supervision d'interactions non-intrusif basé sur les ontologies	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	PLS path modeling and regularized generalized canonical correlation analysis for multi-block data analysis	Regularized generalized canonical correlation analysis (RGCCA) is a generalization of regularizedcanonical correlation analysis to three or more sets of variables. It constitutes a generalframework for many multi-block data analysis methods. It combines the power of multi-blockdata analysis methods (maximization of well identified criteria) and the flexibility of PLS pathmodeling (the researcher decides which blocks are connected and which are not). Searchingfor a fixed point of the stationary equations related to RGCCA, a new monotone convergentalgorithm, very similar to the PLS algorithm proposed by Herman Wold, is obtained. Finally,a practical example is discussed.	Michel Tenenhaus	http://editions-rnti.fr/render_pdf.php?p1&p=1001133	http://editions-rnti.fr/render_pdf.php?p=1001133	361	en	en	@hec.fr	pls path modeling regularize generalized canonical correlation analysis multus block data analysis regularize generalized canonical correlation analysis rgcca generalization regularizedcanonical correlation analysis three set variable constitute generalframework many multus block data analysis method combine power multus blockdata analysis method maximization well identified criterium flexibility pls pathmodele the researcher decide block connect not searchingfor fixed point stationary equation related rgcca new monotone convergentalgorithm similar pls algorithm propose herman wold obtain finally a practical example discuss	PLS path modeling and regularized generalized canonical correlation analysis for multi-block data analysis	
Revue des Nouvelles Technologies de l'Information	EGC	2012	Prétraitement Supervisé des Variables Numériques pour la Fouille de Données Multi-Tables	Le prétraitement des variables numériques dans le contexte de lafouille de données multi-tables diffère de celui des données classiques individuvariable.La difficulté vient principalement des relations un-à-plusieurs où lesindividus de la table cible sont potentiellement associés à plusieurs enregistrementsdans des tables secondaires. Dans cet article, nous décrivons une méthodede discrétisation des variables numériques situées dans des tables secondaires.Nous proposons un critère qui évalue les discrétisations candidates pour ce typede variables. Nous décrivons un algorithme d'optimisation simple qui permetd'obtenir la meilleure discrétisation en intervalles de fréquence égale pour lecritère proposé. L'idée est de projeter dans la table cible l'information contenuedans chaque variable secondaire à l'aide d'un vecteur d'attributs (un attributpar intervalle de discrétisation). Chaque attribut représente le nombre de valeursde la variable secondaire appartenant à l'intervalle correspondant. Ces attributsd'effectifs sont conjointement partitionnés à l'aide de modèles en grille de donnéesafin d'obtenir une meilleure séparation des valeurs de la classe. Des expérimentationssur des jeux de données réelles et artificielles révèlent que l'approchede discrétisation permet de découvrir des variables secondaires pertinentes.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1001191	http://editions-rnti.fr/render_pdf.php?p=1001191	362	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com, @u-cergy.fr	Prétraitement Supervisé des Variables Numériques pour le fouille de Données Multi-Tables  le prétraitement des variable numériques dans le contexte de lafouille de donnée multi-tables différer de celui des donnée classique individuvariable . le difficulté venir principalement un relation un-à-plusieurs où lesindividus de le table cible être potentiellement associer à plusieurs enregistrementsdans des table secondaire . Dans ce article , nous décrire un méthodede discrétisation des variable numériques situer dans un table secondaire . Nous proposer un critère qui évaluer le discrétisations candidat pour ce typede variable . Nous décrire un algorithme d' optimisation simple qui permetd'obtenir le meilleur discrétisation en intervalle de fréquence égal pour lecritère proposer . le idée être de projeter dans le table cible le information contenuedans chaque variable secondaire à le aide d' un vecteur d' attribut ( un attributpar intervalle de discrétisation ) . chaque attribut représenter le nombre de valeursde le variable secondaire appartenir à le intervalle correspondant . ce attributsd'effectifs être conjointement partitionnés à le aide de modèle en grille de donnéesafin d' obtenir un meilleur séparation des valeur de le classe . un expérimentationssur des jeu de donnée réel et artificiel révéler que le approchede discrétisation permettre de découvrir un variable secondaire pertinent . 	Prétraitement Supervisé des Variables Numériques pour la Fouille de Données Multi-Tables	3
Revue des Nouvelles Technologies de l'Information	EGC	2012	Raisonner sur une ontologie cartographique pour concevoir des légendes de cartes	Concevoir une carte géographique, plus particulièrement sa légende,exige des compétences spécifiques. L'objectif de ce papier est de présenter unebase de connaissances destinée à aider tout utilisateur à concevoir une ou plusieurslégendes adaptées à son besoin et conformes aux règles de cartographie.La base de connaissances est formée d'une ontologie de la cartographie nomméeOntoCarto, d'un corpus de règles : OntoCartoRules et d'un moteur de raisonnement: Corese. Dans ce papier, chaque demande de conception de légende estvue comme une instanciation particulière de l'ontologie, associée à une sélectionde règles pertinentes dans le corpus de règles, sur laquelle Corese va raisonnerpour construire des légendes adaptées à la configuration spécifique traitée. Laconception de la légende s'appuie sur la définition de deux hiérarchies d'objetsgéographiques et cartographiques. Les principes de fonctionnement de Coresesont présentés. Un prototype a été implémenté et des extraits des résultats sontmontrés.	Catherine Dominguès, Olivier Corby, Fayrouz Soualah-Alila	http://editions-rnti.fr/render_pdf.php?p1&p=1001178	http://editions-rnti.fr/render_pdf.php?p=1001178	363	fr	fr	@ign.fr, @inria.fr, @yahoo.fr	raisonner sur un ontologie cartographique pour concevoir un légende de cartes  concevoir un carte géographique , plus particulièrement son légende , exiger un compétence spécifique . le objectif de ce papier être de présenter unebase de connaissance destiner à aider tout utilisateur à concevoir un ou plusieurslégendes adapter à son besoin et conforme aux règle de cartographie . le base de connaissance être former d' un ontologie de le cartographie nomméeOntoCarto , d' un corpus de règle : OntoCartoRules et d' un moteur de raisonnement : Corese . Dans ce papier , chaque demande de conception de légende estvue comme un instanciation particulier de le ontologie , associer à un sélectionde règle pertinent dans le corpus de règle , sur laquelle Corese aller raisonnerpour construire un légende adapter à le configuration spécifique traiter . Laconception de le légende clr appuyer sur le définition de deux hiérarchie d' objetsgéographiques et cartographiques . le principe de fonctionnement de Coresesont présenter . un prototype avoir être implémenter et des extrait des résultat sontmontrés . 	Raisonner sur une ontologie cartographique pour concevoir des légendes de cartes	4
Revue des Nouvelles Technologies de l'Information	EGC	2012	Recherche d'Information Agrégée dans des documents XML basée sur les Réseaux Bayésiens	Dans cet article, nous nous intéressons à la recherche agrégée dansdes documents XML. Pour cela, nous proposons un modèle basé sur les réseauxbayésiens. Les relations de dépendances entre requête-termes d'indexation ettermes d'indexation-éléments sont quantifiées par des mesures de probabilité.Dans ce modèle, la requête de l'utilisateur déclenche un processus de propagationpour trouver des éléments. Ainsi, au lieu de récupérer une liste des élémentsqui sont susceptibles de répondre à la requête, notre objectif est d'agréger dansun agrégat des éléments pertinents, non-redondants et complémentaires. Nousavons évalué notre approche dans le cadre de la compagne d'évaluation INEX2009 et avons présenté quelques résultats expérimentaux mettant en évidencel'impact de l'agrégation de tels éléments.	Najeh Naffakhi, Mohand Boughanem, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001202	http://editions-rnti.fr/render_pdf.php?p=1001202	364	fr	fr	@irit.fr, @irit.fr, @isg.rnu.tn, @ihec.rnu.tn	recherche d' information Agrégée dans un document XML baser sur le réseau Bayésiens  Dans ce article , nous clr intéresser à le recherche agréger dansdes document XML . Pour cela , nous proposer un modèle baser sur le réseauxbayésiens . le relation de dépendance entre requête-termes d' indexation ettermes d' indexation-éléments être quantifier par un mesure de probabilité . Dans ce modèle , le requête de le utilisateur déclencher un processus de propagationpour trouver un élément . ainsi , au lieu de récupérer un liste des élémentsqui être susceptible de répondre à le requête , son objectif être d' agréger dansun agrégat des élément pertinent , non- redondant et complémentaire . Nousavons évaluer son approche dans le cadre de le compagne d' évaluation INEX2009 et avoir présenter quelque résultat expérimental mettre en évidencel'impact de le agrégation de tel élément . 	Recherche d'Information Agrégée dans des documents XML basée sur les Réseaux Bayésiens	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Relational Learning from Spatial Data: Retrospect and Prospect	Learning from spatial data is characterized by two main features. First, spatial objects have a locational property which implicitly defines several spatial relationships (topological, directional, distancebased) between objects. Second, attributes of spatially related units tend to be statistically correlated. These two features argue against the assumption of the independent generation of data samples (i.i.d. assumption) underlying classic machine learning algorithms, and motivate the application of relational learning algorithms, whose inferences are based on both instance properties and relations between data. This relational learning approach to spatial domains has already been investigated in the last decade, and important accomplishments in this direction have already been performed. In this talk, we retrospectively survey major achievements on relational learning from spatial data and we report open problems which still challenges researchers and prospectively suggest important topics for incorporation into a research agenda.	Donato Malerba	http://editions-rnti.fr/render_pdf.php?p1&p=1001131	http://editions-rnti.fr/render_pdf.php?p=1001131	365	en	en	@di.uniba.it	relational learn spatial data retrospect prospect learn spatial datum characterize two main feature first spatial object locational property implicitly define several spatial relationship topological directional distancebased object second attribute spatially related unit tend statistically correlated two feature argue assumption independent generation data sample i i d assumption underlie classic machine learn algorithms motivate application relational learning algorithms whose inference base instance property relation data relational learn approach spatial domain already investigate last decade important accomplishment direction already perform talk retrospectively survey major achievement relational learn spatial data report open problem still challenge researcher prospectively suggest important topic incorporation research agenda	Relational Learning from Spatial Data: Retrospect and Prospect	
Revue des Nouvelles Technologies de l'Information	EGC	2012	Réorganisation hiérarchique de visualisations dans OLAP	"Dans cet article nous proposons un nouvel algorithme pour la réorganisationhiérarchique des cubes OLAP (On-Line Analytical Processing) ayantpour objectif d'améliorer leur visualisation. Cet algorithme se caractérise par lefait qu'il peut traiter des dimensions organisées hiérarchiquement et optimiserconjointement les dimensions du cube, contrairement aux autres approches. Ilutilise un algorithme génétique qui réorganise des arbres n-aires quelconques. Ila été intégré dans une interface OLAP puis testé en comparaison avec d'autresapproches de réorganisation, et fournit des résultats très positifs. A ce titre,nous avons également généralisé l'algorithme heuristique classique BEA (""bondenergy algorithm"") au cas de hiérarchies OLAP. Enfin, notre approche a été évaluéepar des utilisateurs et les résultats soulignent l'intérêt de la réorganisationdans des exemples de tâches à résoudre pour OLAP."	Sébastien Lafon, Fatma Bouali, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001181	http://editions-rnti.fr/render_pdf.php?p=1001181	366	fr	fr	@univ-tours.fr, @univ-lille2.fr, @ceries-lab.com	réorganisation hiérarchique de visualisation dans OLAP  " Dans ce article nous proposer un nouveau algorithme pour le réorganisationhiérarchique des cube OLAP ( On-Line Analytical Processing ) ayantpour objectif d' améliorer son visualisation . . ce algorithme clr caractériser par lefait qu' il pouvoir traiter un dimension organiser hiérarchiquement et optimiserconjointement le dimension du cube , contrairement aux autre approche . . Ilutilise un algorithme génétique qui réorganiser un arbre n-aires quelconque . . Ila être intégrer dans un interface OLAP puis tester en comparaison avec un autresapproches de réorganisation , et fournir un résultat très positif . . A ce titre , nous avoir également généraliser le algorithme heuristique classique BEA ( " " bondenergy algorithm " " ) au cas de hiérarchie OLAP . . enfin , son approche avoir être évaluéepar des utilisateur et le résultat souligner le intérêt de le réorganisationdans des exemple de tâche à résoudre pour OLAP . " 	Réorganisation hiérarchique de visualisations dans OLAP	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Représentations de services web : impact sur la découverte et la recommandation		Mustapha Aznag, Mohamed Quafafou, Nicolas Durand, Zahi Jarir	http://editions-rnti.fr/render_pdf.php?p1&p=1001203	http://editions-rnti.fr/render_pdf.php?p=1001203	367	fr		@univmed.fr, @ucam.ac.ma	Représentations de services web : impact sur la découverte et la recommandation 	Représentations de services web : impact sur la découverte et la recommandation	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	RICSH : Recherche d'information contextuelle par segmentation thématique de documents	Le but principal des systèmes de recherche d'informations (SRI) classiquesest de retrouver dans un corpus de documents l'information considéréecomme pertinente pour une requête utilisateur. Cette pertinence est souvent liéeà la fréquence d'apparition des termes dans le texte par rapport au corpus sanstenir compte du contexte de la recherche. Partant de ce constat, nous proposonsdans cet article une approche pour la recherche d'information contextuelle parsegmentation thématique de documents (RICSH). Cette approche s'appuie surla méthode de pondération tf-idf que nous avons adaptée dans notre cas pourindexer le corpus. Cette adaptation se situe au niveau de l'importance du termeet de son pouvoir de discrimination par rapport aux fragments de textes et nonau corpus. Ces fragments sont obtenus grâce à un processus d'identification desunités thématiques les plus pertinentes pour chaque document.	Fadila Bentayeb, Omar Boussaid, Rachid Aknouche	http://editions-rnti.fr/render_pdf.php?p1&p=1001199	http://editions-rnti.fr/render_pdf.php?p=1001199	368	fr	fr	@univ-lyon2.fr	RICSH : recherche d' information contextuel par segmentation thématique de documents  le but principal des système de recherche d' information ( SRI ) classiquesest de retrouver dans un corpus de document le information considéréecomme pertinent pour un requête utilisateur . ce pertinence être souvent liéeà le fréquence d' apparition des terme dans le texte par rapport au corpus sanstenir compter du contexte de le recherche . partir de ce constat , nous proposonsdans ce article un approche pour le recherche d' information contextuel parsegmentation thématique de document ( RICSH ) . ce approche clr appuyer surla méthode de pondération tf-idf que nous avoir adapter dans son cas pourindexer le corpus . ce adaptation clr situer au niveau de le importance du termeet de son pouvoir de discrimination par rapport aux fragment de texte et nonau corpus . ce fragment être obtenir grâce à un processus d' identification desunités thématique le plus pertinent pour chaque document . 	RICSH : Recherche d'information contextuelle par segmentation thématique de documents	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Sélection Bayésienne de Modèles avec Prior Dépendant des Données	Cet article analyse la consistance asymptotique des modèles en grilleappliqués à l'estimation de densité jointe de deux variables catégorielles. Lesmodèles en grille considèrent un partitionnement des valeurs de chacune des variables,le produit Cartésien des partitions formant une grille dont les cellulespermettent de résumer la table de contingence des deux variables. Le meilleurmodèle de co-partitionnement est recherché au moyen d'une approche MAP(maximum a posteriori), présentant la particularité peu orthodoxe d'exploiterune famille de modèles et une distribution a priori de ces modèles qui dépendentdes données. Ces modèles sont par nature des modèles de l'échantillon d'apprentissage,et non de la distribution sous-jacente. Nous démontrons la consistancede l'approche, qui se comporte comme un estimateur universel de densité jointeconvergeant asymptotiquement vers la vraie distribution jointe.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001136	http://editions-rnti.fr/render_pdf.php?p=1001136	369	fr	fr	@orange.com	sélection Bayésienne de Modèles avec Prior Dépendant des Données  ce article analyser le consistance asymptotique des modèle en grilleappliqués à le estimation de densité joindre de deux variable catégoriel . Lesmodèles en grille considérer un partitionnement des valeur de chacun des variable , le produit Cartésien des partition former un grille dont le cellulespermettent de résumer le table de contingence des deux variable . le meilleurmodèle de co- partitionnement être rechercher au moyen d' un approche MAP ( maximum avoir posteriori ) , présenter le particularité peu orthodoxe d' exploiterune famille de modèle et un distribution avoir priori de ce modèle qui dépendentdes donner . ce modèle être par nature des modèle de le échantillon d' apprentissage , et non de le distribution sous-jacent . Nous démontrer le consistancede le approche , qui clr comporter comme un estimateur universel de densité jointeconvergeant asymptotiquement vers le vrai distribution joindre . 	Sélection Bayésienne de Modèles avec Prior Dépendant des Données	5
Revue des Nouvelles Technologies de l'Information	EGC	2012	Solving Problems with Visual Analytics: Challenges and Applications	Never before in history data is generated and collected at such high volumes as it is today. As the volumes of data available to business people, scientists, and the public increase,their effective use becomes more challenging. Keeping up to date with the flood of data,using standard tools for data analysis and exploration, is fraught with difficulty. The field ofvisual analytics seeks to provide people with better and more effective ways to understandand analyze large datasets, while also enabling them to act upon their findings immediately. Visual analytics integrates the analytic capabilities of the computer and the abilities of the human analyst, allowing novel discoveries and empowering individuals to take control of the analytical process. Visual analytics enables unexpected and hidden insights, which may lead to beneficial and profitable innovation. The talk presents the challenges of visual analytics and exemplifies them with application examples, illustrating the exiting potential of current visual analysis techniques.	Daniel Keim	http://editions-rnti.fr/render_pdf.php?p1&p=1001130	http://editions-rnti.fr/render_pdf.php?p=1001130	370	en	en	@uni-konstanz.de	solve problem visual analytic challenge application never history data generate collected high volume today volume datum available business person scientist public increase their effective use become challenging keep date flood datum used standard tool data analysis exploration fraught difficulty field ofvisual analytic seek provide person better effective way understandand analyze large dataset also enable act upon finding immediately visual analytic integrate analytic capability computer ability human analyst allow novel discovery empower individual take control analytical process visual analytic enable unexpected hidden insight may lead beneficial profitable innovation talk present challenge visual analytic exemplify application example illustrate exit potential current visual analysis technique	Solving Problems with Visual Analytics: Challenges and Applications	11
Revue des Nouvelles Technologies de l'Information	EGC	2012	Structuration des décisions de jurisprudence basée sur une ontologie juridique en langue arabe	L'informatique juridique, est un domaine en évolution constante. Lecontexte général de notre travail est l'élaboration d'un système de recherchede jurisprudence tunisienne en langue arabe. L'objectif opérationnel de ce systèmeest de fournir une aide aux juristes pour résoudre une situation juridiquedonnée en mettant à leur disposition une collection de situations similaires cequi améliorera leur raisonnement futur. Une ontologie du domaine juridiqueconstruite à partir des documents des décisions juridiques est nécessaire dansnotre contexte.Cette ontologie a pour but : (i) la structuration des décisions, (ii)la formulation des requêtes d'interrogation de la base des décisions, et (iii) larecherche des décisions. Dans cet article, nous présentons l'architecture de notresystème de recherche de jurisprudence. Nous nous focalisons sur l'ontologie dudomaine de jurisprudence que nous avons élaborée, aisni que sur le module destructuration des décisions.	Karima Dhouib, Sylvie Desprès, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001175	http://editions-rnti.fr/render_pdf.php?p=1001175	371	fr	fr	@isets.rnu.tn, @univ-paris13.fr, @fsegs.rnu.t	structuration des décision de jurisprudence baser sur un ontologie juridique en langue arabe  le informatique juridique , être un domaine en évolution constant . Lecontexte général de son travail être le élaboration d' un système de recherchede jurisprudence tunisien en langue arabe . le objectif opérationnel de ce systèmeest de fournir un aide aux juriste pour résoudre un situation juridiquedonnée en mettre à son disposition un collection de situation similaire cequi améliorer son raisonnement futur . un ontologie du domaine juridiqueconstruite à partir un document des décision juridique être nécessaire dansnotre contexte . ce ontologie avoir pour but : ( i ) le structuration des décision , ( ii ) le formulation des requête d' interrogation de le base des décision , et ( iii ) larecherche des décision . Dans ce article , nous présenter le architecture de notresystème de recherche de jurisprudence . Nous nous focaliser sur le ontologie dudomaine de jurisprudence que nous avoir élaborer , aisni que sur le module destructuration des décision . 	Structuration des décisions de jurisprudence basée sur une ontologie juridique en langue arabe	4
Revue des Nouvelles Technologies de l'Information	EGC	2012	SweetDeki : le wiki sémantique couteau suisse du réseau social ISICIL	Le projet ANR ISICIL 1 mixe les nouvelles applications virales duweb avec des représentations formelles et des processus d'entreprise pour les intégrerdans les pratiques de veille en entreprise. Les outils développés s'appuientsur les interfaces avancées des applications du web 2.0 (blog, wiki, social bookmarking,extensions de navigateurs) pour les interactions et sur les technologiesdu web sémantique pour l'interopérabilité et le traitement de l'information. Leprésent article décrit plus précisément le wiki sémantique développé dans lecadre de ce projet et son intégration au coeur du framework ISICIL	Michel Buffa, Guillaume Husson, Nicolas Delaforge	http://editions-rnti.fr/render_pdf.php?p1&p=1001151	http://editions-rnti.fr/render_pdf.php?p=1001151	372	fr	fr	@unice.fr, @unice.fr, @inria.fr	SweetDeki : le wiki sémantique couteau suisse du réseau social ISICIL  le projet ANR ISICIL 1 mixer le nouveau application viral duweb avec un représentation formel et des processus d' entreprise pour le intégrerdans le pratique de veille en entreprise . le outil développer clr appuientsur le interface avancer des application du web 2.0 ( blog , wiki , social bookmarking , extension de navigateur ) pour le interaction et sur le technologiesdu web sémantique pour le interopérabilité et le traitement de le information . Leprésent article décrire plus précisément le wiki sémantique développer dans lecadre de ce projet et son intégration au coeur du framework ISICIL 	SweetDeki : le wiki sémantique couteau suisse du réseau social ISICIL	3
Revue des Nouvelles Technologies de l'Information	EGC	2012	TMD-MINER : Une nouvelle approche pour la détection des diffuseurs dans un système communautaire	Plusieurs méthodes ont été développées ces dernières années pour détecter,dans un réseau social, les membres qualifiés, selon les auteurs, d'influenceurs,de médiateurs, d'ambassadeurs ou encore d'experts. Dans cet article, nousproposons un nouveau cadre méthodologique permettant d'identifier des diffuseursdans le contexte où seule l'information sur l'appartenance des membres duréseau à des communautés est disponible. Ce cadre, basé sur une représentationdu réseau sous forme d'hypergraphe, nous a permis de formaliser la notion dediffuseur et d'introduire l'algorithme TMD-MINER, dédié à la détection des diffuseurset basé sur les itemsets essentiels.	Mohamed Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001193	http://editions-rnti.fr/render_pdf.php?p=1001193	373	fr	fr	@univ-st-etienne.fr, @fst.rnu.tn	TMD-MINER : un nouveau approche pour le détection des diffuseur dans un système communautaire  plusieurs méthode avoir être développer ce dernier année pour détecter , dans un réseau social , le membre qualifier , selon le auteur , d' influenceurs , de médiateur , d' ambassadeur ou encore d' expert . Dans ce article , nousproposons un nouveau cadre méthodologique permettre d' identifier un diffuseursdans le contexte où seul le information sur le appartenance des membre duréseau à un communauté être disponible . ce cadre , baser sur un représentationdu réseau sous forme d' hypergraphe , nous avoir permettre de formaliser le notion dediffuseur et d' introduire le algorithme TMD-MINER , dédier à le détection des diffuseurset baser sur le itemsets essentiel . 	TMD-MINER : Une nouvelle approche pour la détection des diffuseurs dans un système communautaire	3
Revue des Nouvelles Technologies de l'Information	EGC	2012	Topological Decomposition and Heuristics for High Speed Clustering of Complex Networks	With the exponential growth in the size of data and networks, developmentof new and fast techniques to analyze and explore these networks isbecoming a necessity. Moreover the emergence of scale free and small worldproperties in real world networks has stimulated lots of activity in the field ofnetwork analysis and data mining. Clustering remains a fundamental techniqueto explore and organize these networks. A challenging problem is to find a clusteringalgorithm that works well in terms of clustering quality and is efficient interms of time complexity.In this paper, we propose a fast clustering algorithm which combines someheuristics with a Topological Decomposition to obtain a clustering. The algorithmwhich we call Topological Decomposition and Heuristics for Clustering(TDHC) is highly efficient in terms of asymptotic time complexity as comparedto other existing algorithms in the literature. We also introduce a number ofHeuristics to complement the clustering algorithm which increases the speed ofthe clustering process maintaining the high quality of clustering. We show theeffectiveness of the proposed clustering method on different real world data setsand compare its results with well known clustering algorithms.	Faraz Zaidi, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001147	http://editions-rnti.fr/render_pdf.php?p=1001147	374	en	en	@pafkiet.edu.pk, @labri.fr	topological decomposition heuristic high speed cluster complex network exponential growth size data network developmentof new fast technique analyze explore network isbecome necessity moreover emergence scale free small worldproperty real world network stimulate lot activity field ofnetwork analysis datum mining cluster remain fundamental techniqueto explore organize network challenge problem find clusteringalgorithm work well term cluster quality efficient interms time complexity in paper propose fast cluster algorithm combine someheuristic topological decomposition obtain cluster algorithmwhich call topological decomposition heuristic cluster tdhc highly efficient term asymptotic time complexity comparedto exist algorithms literature also introduce number ofheuristics complement cluster algorithm increase speed ofthe cluster process maintain high quality cluster show theeffectiveness propose cluster method different real world data setsand compare result well know cluster algorithms	Topological Decomposition and Heuristics for High Speed Clustering of Complex Networks	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Transformation de l'espace de description pour l'apprentissage par transfert	"Dans ce papier, nous proposons une étude sur l'utilisation de l'apprentissagetopologique pondéré et les méthodes de factorisation matricielle pourtransformer l'espace de représentation d'un jeu de données ""sparse"" afin d'augmenterla qualité de l'apprentissage, et de l'adapter au cas de l'apprentissagepar transfert. La factorisation matricielle nous permet de trouver des variableslatentes et l'apprentissage topologique pondéré est utilisé pour détecter les pluspertinentes parmi celles-ci. La représentation de nouvelles données est basée surleurs projections sur le modèle topologique pondéré.Pour l'apprentissage par transfert, nous proposons une nouvelle méthode où lareprésentation des données est faite de la même manière que dans la premièrephase, mais en utilisant un modèle topologique élagué.Les expérimentations sont présentées dans le cadre d'un Challenge Internationaloù nous avons obtenu des résultats prometteurs (5ieme rang de la compétitioninternationale).1 Introduction"	Nistor Grozavu, Younès Bennani, Lazhar Labiod	http://editions-rnti.fr/render_pdf.php?p1&p=1001185	http://editions-rnti.fr/render_pdf.php?p=1001185	375	fr	fr	@univ-paris13.fr, @parisdescartes.fr	transformation de le espace de description pour le apprentissage par transfert  " Dans ce papier , nous proposer un étude sur le utilisation de le apprentissagetopologique pondérer et le méthode de factorisation matriciel pourtransformer le espace de représentation d' un jeu de donnée " " sparse " " afin d' augmenterla qualité de le apprentissage , et de l' adapter au cas de le apprentissagepar transfert . . le factorisation matriciel nous permettre de trouver un variableslatentes et le apprentissage topologique pondérer être utiliser pour détecter le pluspertinentes parmi celui _-ci . . le représentation de nouveau donnée être baser surleurs projection sur le modèle topologique pondérer . . Pour le apprentissage par transfert , nous proposer un nouveau méthode où lareprésentation un donnée être faire de le même manière que dans le premièrephase , mais en utiliser un modèle topologique élaguer . . le expérimentation être présenter dans le cadre d' un challenge Internationaloù nous avoir obtenir un résultat prometteur ( 5ieme rang de le compétitioninternationale ) . . 1 Introduction " 	Transformation de l'espace de description pour l'apprentissage par transfert	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Un algorithme de classification automatique pour des données relationnelles multi-vues	classification automatique (De Carvalho et al., 2012) capable de partitionnerdes objets en prenant en compte de manière simultanée plusieurs matricesde dissimilarité qui les décrivent. Ces matrices peuvent avoir été généréesen utilisant différents ensembles de variables et de fonctions de dissimilarité.Cette méthode, basée sur l'algorithme de nuées dynamiques est conçu pour fournirune partition et un prototype pour chaque classe tout en découvrant une pondérationpertinante pour chaque matrice de dissimilarité en optimisant un critèred'adéquation entre les classes et leurs représentants. Ces pondérations changentà chaque itération de l'algorithme et sont différentes pour chacune des classes.Nous présentons aussi plusieurs outils d'aide à l'interprétation des groupes et dela partition fournie par cette nouvelle méthode. Deux exemples illustrent l'interêtde la méthode. Le premier utilise des données concernant des chiffres manuscrits(0 à 9) numérisés en images binaires provenant de l'UCI. Le second utilise unensemble de rapports dont nous connaissons une classification experte donnée àpriori.	Francisco de Assis Tenório de Carvalho, Filipe M. de Melo, Yves Lechevallier, Thierry Despeyroux	http://editions-rnti.fr/render_pdf.php?p1&p=1001142	http://editions-rnti.fr/render_pdf.php?p=1001142	376	fr	fr	@cin.ufpe.br, @inria.fr	un algorithme de classification automatique pour un donnée relationnel multi-vues  classification automatique ( De Carvalho et al. , 2012 ) capable de partitionnerdes objet en prendre en compte de manière simultané plusieurs matricesde dissimilarité qui les décrire . ce matrice pouvoir avoir être généréesen utiliser différents ensemble de variable et de fonction de dissimilarité . ce méthode , baser sur le algorithme de nuée dynamique être concevoir pour fournirune partition et un prototype pour chaque classe tout en découvrir un pondérationpertinante pour chaque matrice de dissimilarité en optimiser un critèred'adéquation entre le classe et son représentant . ce pondération changentà chaque itération de le algorithme et être différent pour chacun des classe . Nous présenter aussi plusieurs outil d' aide à le interprétation des groupe et dela partition fournir par ce nouveau méthode . Deux exemple illustrer le interêtde le méthode . le premier utiliser un donnée concernant un chiffre manuscrit ( 0 à 9 ) numériser en image binaire provenir de le UCI . le second utiliser unensemble de rapport dont nous connaître un classification expert donner àpriori . 	Un algorithme de classification automatique pour des données relationnelles multi-vues	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Un assistant utilisateur pour le choix et le paramétrage des méthodes de fouille visuelle de données	Nous nous intéressons dans cet article au problème de l'automatisation du processus de choix et de paramétrage des visualisations en fouille visuelle de données. Pour résoudre ce problème, nous avons développé un assistant utilisateur qui effectue deux étapes : à partir des objectifs annoncés par l'utilisateur et des caractéristiques de ses données, le système commence par proposer à l'utilisateur différents appariements entre la base de données à visualiser et les visualisations qu'il gère. Ces appariements sont générés par une heuristique utilisant une base de connaissances sur les visualisations et la perception visuelle. Ensuite, afin d'affiner les différents paramétrages suggérés par le système, nous utilisons un algorithme génétique interactif qui permet aux utilisateurs d'évaluer et d'ajuster visuellement ces paramétrages. Nous présentons une évaluation utilisateur qui montre l'intérêt de notre système pour deux tâches.	Abdelheq Et-tahir Guettala, Fatma Bouali, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001180	http://editions-rnti.fr/render_pdf.php?p=1001180	377	fr	fr	@univ-tours.fr, @ceries-lab.com, @univ-lille2.fr	un assistant utilisateur pour le choix et le paramétrage des méthode de fouille visuel de données  Nous nous intéresser dans ce article au problème de le automatisation du processus de choix et de paramétrage des visualisation en fouille visuel de donnée . Pour résoudre ce problème , nous avoir développer un assistant utilisateur qui effectuer deux étape : à partir un objectif annoncer par le utilisateur et des caractéristique de son donnée , le système commencer par proposer à le utilisateur différent appariement entre le base de donnée à visualiser et le visualisation qu' il gérer . ce appariement être générer par un heuristique utiliser un base de connaissance sur le visualisation et le perception visuel . ensuite , afin d' affiner le différent paramétrages suggérer par le système , nous utiliser un algorithme génétique interactif qui permettre aux utilisateur d' évaluer et d' ajuster visuellement ce paramétrages . Nous présenter un évaluation utilisateur qui montrer le intérêt de son système pour deux tâche . 	Un assistant utilisateur pour le choix et le paramétrage des méthodes de fouille visuelle de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Un environnement efficace pour la classification d'images à grande échelle	La plupart des processus de classification d'images comportent troisprincipales étapes : l'extraction de descripteurs de bas niveaux, la création d'unvocabulaire visuel par quantification et l'apprentissage à l'aide d'un algorithmede classification (eg.SVM). De nombreux problèmes se posent pour le passageà l'échelle comme avec l'ensemble de données ImageNet contenant 14 millionsd'images et 21,841 classes. La complexité concerne le temps d'exécution dechaque tâche et les besoins en mémoire et disque (eg. le stockage des SIFTs nécessite11To). Nous présentons une version parallèle de LibSVM pour traiter degrands ensembles de données dans un temps raisonnable. De plus, il y a beaucoupde perte d'information lors de la phase de quantification et les mots visuelsobtenus ne sont pas assez discriminants pour de grands ensembles d'images.Nous proposons d'utiliser plusieurs descripteurs simultanément pour améliorerla précision de la classification sur de grands ensembles d'images. Nous présentonsnos premiers résultats sur les 10 plus grandes classes (24,817 images)d'ImageNet.	Thanh-Nghi Doan, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001189	http://editions-rnti.fr/render_pdf.php?p=1001189	378	fr	fr	@irisa.fr	un environnement efficace pour le classification d' image à grand échelle  le plupart des processus de classification d' image comporter troisprincipales étape : le extraction de descripteur de bas niveau , le création d' unvocabulaire visuel par quantification et le apprentissage à le aide d' un algorithmede classification ( eg . SVM ) . un nombreux problème clr poser pour le passageà le échelle comme avec le ensemble de donnée ImageNet contenir 14 millionsd'images et 21_,_841 classe . le complexité concerner le temps d' exécution dechaque tâche et le besoin en mémoire et disque ( eg . le stockage des SIFTs nécessite11To ) . Nous présenter un version parallèle de LibSVM pour traiter degrands ensemble de donnée dans un temps raisonnable . De plus , il y avoir beaucoupde perte d' information lors de le phase de quantification et le mot visuelsobtenus ne être pas assez discriminant pour un grand ensemble d' image . Nous proposer d' utiliser plusieurs descripteur simultanément pour améliorerla précision de le classification sur un grand ensemble d' image . Nous présentonsnos premier résultat sur le 10 plus grand classe ( 24 , 817 image ) d' ImageNet . 	Un environnement efficace pour la classification d'images à grande échelle	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Une approche multidimensionnelle basée sur les comportements individuels pour la prédiction de la diffusion de l'information sur Twitter	Aujourd'hui, les réseaux sociaux en ligne sont devenus des outils trèspuissants de propagation de l'information. Ils favorisent la diffusion rapide àgrande échelle de contenu et les conséquences d'une information inexacte voirefausse peuvent alors prendre une ampleur considérable. Par conséquent il devientindispensable de proposer des moyens d'analyser le phénomène de diffusionde l'information dans ces réseaux. De nombreuses études récentes ont traitéde la modélisation du processus de diffusion de l'information, essentiellementd'un point de vue topologique et dans une perspective théorique, mais les facteursimpliqués sont encore méconnus. Nous proposons ici une solution pratiquedont l'objectif est de prédire la dynamique temporelle de la diffusion au sein deTwitter, basée sur des techniques d'apprentissage automatique. Notre approcherepose sur l'inférence de probabilités de diffusion tirées d'une analyse multidimensionnelledes comportements individuels. Les expérimentations menéesmontrent l'intérêt de la modélisation proposée.	Adrien Guille, Hakim Hacid, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001173	http://editions-rnti.fr/render_pdf.php?p=1001173	379	fr	fr	@univ-lyon2.fr, @alcatel-lucent.com	un approche multidimensionnel baser sur le comportement individuel pour le prédiction de le diffusion de le information sur Twitter  Aujourd' hui , le réseau social en ligne être devenir un outil trèspuissants de propagation de le information . Ils favoriser le diffusion rapide àgrande échelle de contenu et le conséquence d' un information inexact voirefausse pouvoir alors prendre un ampleur considérable . Par conséquent il devientindispensable de proposer un moyen d' analyser le phénomène de diffusionde le information dans ce réseau . un nombreux étude récent avoir traitéde le modélisation du processus de diffusion de le information , essentiellementd'un point de vue topologique et dans un perspective théorique , mais le facteursimpliqués être encore méconnaître . Nous proposer ici un solution pratiquedont le objectif être de prédire le dynamique temporel de le diffusion au sein deTwitter , baser sur un technique d' apprentissage automatique . son approcherepose sur le inférence de probabilité de diffusion tirer d' un analyse multidimensionnelledes comportement individuel . le expérimentation menéesmontrent le intérêt de le modélisation proposer . 	Une approche multidimensionnelle basée sur les comportements individuels pour la prédiction de la diffusion de l'information sur Twitter	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Une distance hiérarchique basée sur la sémantique pour la comparaison d'histogrammes nominaux	La plupart des distances entre histogrammes sont définies pour comparerdes histogrammes ordonnés (dont les entités représentées sont totalementordonnées) ou des histogrammes nominaux (dont les entités représentées nepeuvent pas être comparées). Cependant, il n'existe aucune distance qui permettede comparer des histogrammes nominaux dans lesquels il est possible dequantifier des valeurs de proximité sémantique entre les entités considérées. Cetarticle propose une nouvelle distance permettant de pallier ce problème. Dans unpremier temps, une hiérarchie d'histogrammes, obtenue par le biais d'une fusionprogressive des entités considérées (prenant en compte leurs proximités sémantiques),est construite. Pour chaque étage de cette hiérarchie, une distance standardde comparaison d'histogrammes nominaux est calculée. Finalement, pourobtenir la distance proposée, ces différentes distances sont fusionnées en prenanten compte la cohérence sémantique associée aux niveaux de chaque étage de lahiérarchie. Cette distance a été validée dans le cadre de la classification de donnéesgéographiques. Les résultats obtenus sont encourageants et montrent ainsil'intérêt et l'utilité de cette dernière pour des processus de fouille de données.	Camille Kurtz	http://editions-rnti.fr/render_pdf.php?p1&p=1001144	http://editions-rnti.fr/render_pdf.php?p=1001144	380	fr	fr	@unistra.fr	un distance hiérarchique baser sur le sémantique pour le comparaison d' histogramme nominaux  le plupart des distance entre histogramme être définir pour comparerdes histogramme ordonner ( dont le entité représenter être totalementordonnées ) ou des histogramme nominal ( dont le entité représenter nepeuvent pas être comparer ) . cependant , il n' exister aucun distance qui permettede comparer un histogramme nominal dans lesquels il être possible dequantifier des valeur de proximité sémantique entre le entité considérer . Cetarticle proposer un nouveau distance permettre de pallier ce problème . Dans unpremier temps , un hiérarchie d' histogramme , obtenir par le biais d' un fusionprogressive des entité considérer ( prendre en compte son proximité sémantique ) , être construire . Pour chaque étage de ce hiérarchie , un distance standardde comparaison d' histogramme nominal être calculer . finalement , pourobtenir le distance proposer , ce différent distance être fusionner en prenanten compter le cohérence sémantique associer aux niveau de chaque étage de lahiérarchie . ce distance avoir être valider dans le cadre de le classification de donnéesgéographiques . le résultat obtenir être encourageant et montrer ainsil'intérêt et le utilité de ce dernier pour un processus de fouille de donnée . 	Une distance hiérarchique basée sur la sémantique pour la comparaison d'histogrammes nominaux	2
Revue des Nouvelles Technologies de l'Information	EGC	2012	User Evaluation: Why?	Research in information visualisation has changed significantly in the past two decades.Once it was sufficient to simply design and implement an impressive visualisation system.Today editors and reviewers expect papers to present not only a novel system, but empiricalevidence of its worth. Why has this change come about, and what impact has it had on thoseworking in this area? This talk will discuss how a field dominated by algorithms and toolsbecame infected by human participants, and why this is a positive development in a maturingresearch discipline.	Helen Purchase	http://editions-rnti.fr/render_pdf.php?p1&p=1001132	http://editions-rnti.fr/render_pdf.php?p=1001132	381	en	en	@dcs.gla.ac.uk	user evaluation why research information visualisation change significantly past two decade once sufficient simply design implement impressive visualisation system today editor reviewer expect paper present novel system empiricalevidence worth change come about impact thosework area talk discuss field dominate algorithms toolsbecame infected human participant positive development maturingresearch discipline	User Evaluation: Why?	16
Revue des Nouvelles Technologies de l'Information	EGC	2012	Utilisation d'invariants pour une médiation inter-domaines de modèles utilisateurs : ressources invariantes et invariants sémantiques	Les services de personnalisation du Web 2.0 reposent sur l'exploitationde modèles utilisateurs. Schématiquement, plus la quantité d'informationssur les utilisateurs est grande, meilleures sont la modélisation et la qualité du service.En pratique, nombre de services rencontrent un problème de manque d'informationssur les utilisateurs. Dans cet article, nous y répondons par médiationinter-domaines de modèles utilisateurs, c'est-à-dire la complétion de modèles enexploitant des données d'un autre domaine. La médiation que nous proposonsrepose sur un transfert d'informations inter-domaines. Ce transfert consiste enl'utilisation de couples invariants ou très corrélés pouvant être des couples deressources ou de descripteurs sémantiques, identifiés après enrichissement sémantiquedes modèles. Nous montrons que le transfert sous forme de couple deressources permet une complétion de qualité et que l'exploitation de descripteurssémantiques augmente la couverture à qualité égale. Enrichir sémantiquementest donc bénéfique pour le transfert inter-domaines.	Emilien Perrin, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001170	http://editions-rnti.fr/render_pdf.php?p=1001170	382	fr	fr	@loria.fr	utilisation d' invariants pour un médiation inter-domaines de modèle utilisateur : ressource invariant et invariant sémantiques  le service de personnalisation du Web 2.0 reposer sur le exploitationde modèle utilisateur . schématiquement , plus le quantité d' informationssur le utilisateur être grand , meilleur être le modélisation et le qualité du service . En pratique , nombre de service rencontrer un problème de manque d' informationssur le utilisateur . Dans ce article , nous y répondre par médiationinter-domaines de modèle utilisateur , c' est-à-dire le complétion de modèle enexploitant des donnée d' un autre domaine . le médiation que nous proposonsrepose sur un transfert d' information inter-domaines . ce transfert consister enl'utilisation de couple invariant ou très corréler pouvoir être un couple deressources ou de descripteur sémantique , identifier après enrichissement sémantiquedes modèle . Nous montrer que le transfert sous forme de couple deressources permettre un complétion de qualité et que le exploitation de descripteurssémantiques augmenter le couverture à qualité égal . enrichir sémantiquementest donc bénéfique pour le transfert inter-domaines . 	Utilisation d'invariants pour une médiation inter-domaines de modèles utilisateurs : ressources invariantes et invariants sémantiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Validation et optimisation d'une décomposition hiérarchique de graphes	De nombreux algorithmes de fragmentation de graphes fonctionnentpar agrégations ou divisions successives de sous-graphes menant à une décompositionhiérarchique du réseau étudié. Une question importante dans ce domaineest de savoir si cette hiérarchie reflète la structure du réseau ou si elle n'estqu'un artifice lié au déroulement de la procédure. Nous proposons un moyen devalider et, au besoin, d'optimiser la décomposition multi-échelle produite parce type de méthode. On applique notre approche sur l'algorithme proposé parBlondel et al. (2008) basé sur la maximisation de la modularité. Dans ce cadre,une généralisation de cette mesure de qualité au cas multi-niveaux est introduite.Nous testons notre méthode sur des graphes aléatoires ainsi que sur des exemplesréels issus de divers domaines.	Francois Queyroi	http://editions-rnti.fr/render_pdf.php?p1&p=1001182	http://editions-rnti.fr/render_pdf.php?p=1001182	383	fr	fr	@labri.fr	validation et optimisation d' un décomposition hiérarchique de graphes  un nombreux algorithme de fragmentation de graphe fonctionnentpar agrégation ou division successif de sous-graphes mener à un décompositionhiérarchique du réseau étudier . un question important dans ce domaineest de savoir si ce hiérarchie refléter le structure du réseau ou si elle n' estqu'un artifice lier au déroulement de le procédure . Nous proposer un moyen devalider et , au besoin , d' optimiser le décomposition multi-échelle produire parce type de méthode . On appliquer son approche sur le algorithme proposer parBlondel et al. ( 2008 ) baser sur le maximisation de le modularité . Dans ce cadre , un généralisation de ce mesure de qualité au cas multi-niveaux être introduire . Nous tester son méthode sur un graphe aléatoire ainsi que sur un exemplesréels issir de divers domaine . 	Validation et optimisation d'une décomposition hiérarchique de graphes	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers la construction d'un observatoire des pratiques agricoles : gestion et propagation de l'imprécision des données agronomiques	L'un des objectifs d'Observox est de traiter et gérer l'imprécisiondes données agronomiques tant spatialement (parcelles agricoles) et quantitativement(quantités de produits disséminées) et de toujours associer une évaluationde la qualité aux données. Aussi, nous avons choisi le cadre théorique desensembles flous. A partir d'un modèle conceptuel gérant l'imperfection, nousconstruisons une base de données gérant des entités spatiotemporelles imprécisesappelées « entités agronomiques floues ». Cependant, ce choix de représentationrend possible le chevauchement des composantes spatiales entre entités.Dans ce cas, nous propageons l'imprécision du spatial vers le quantitatif àl'aide d'un opérateur de caractère additif qui prend en compte à la fois l'informationspatiale et quantitative, et qui fournit une information quantitative localeet floue. Le système ainsi construit nous permet d'obtenir une représentationfloue des quantités de produits phytosanitaires disséminés à chaque endroit duterritoire étudié.	Asma Zoghlami, Karima Zayrit, Cyril de Runz, Eric Desjardin, Herman Akdag	http://editions-rnti.fr/render_pdf.php?p1&p=1001158	http://editions-rnti.fr/render_pdf.php?p=1001158	384	fr	fr	@univ-reims.fr, @ai.univ-paris8.fr	Vers le construction d' un observatoire des pratique agricole : gestion et propagation de le imprécision des donnée agronomiques  le un des objectif d' Observox être de traiter et gérer le imprécisiondes donner agronomique tant spatialement ( parcelle agricole ) et quantitativement ( quantité de produit disséminer ) et de toujours associer un évaluationde le qualité aux donnée . aussi , nous avoir choisir le cadre théorique desensembles flou . A partir d' un modèle conceptuel gérer le imperfection , nousconstruisons un base de donnée gérer un entité spatiotemporelles imprécisesappelées « entité agronomique flou » . cependant , ce choix de représentationrend possible le chevauchement des composante spatial entre entité . Dans ce cas , nous propager le imprécision du spatial vers le quantitatif àl'aide d' un opérateur de caractère additif qui prendre en compte à le foi le informationspatiale et quantitatif , et qui fournir un information quantitatif localeet flou . le système ainsi construire nous permettre d' obtenir un représentationfloue des quantité de produit phytosanitaires disséminer à chaque endroit duterritoire étudier . 	Vers la construction d'un observatoire des pratiques agricoles : gestion et propagation de l'imprécision des données agronomiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers une approche efficace d'extraction de motifs spatio-séquentiels	Ces dernières années, l'augmentation de la quantité d'informationsspatio-temporelles stockées dans les bases de données a fait naître de nouveauxbesoins, notamment en matière de gestion des risques naturels, sanitaires ou anthropiques(p. ex. compréhension de la dynamique d'une épidémie de Dengue).Dans cet article, nous définissons un cadre théorique pour l'extraction de motifsspatio-séquentiels, séquences de motifs spatiaux représentant l'évolution dansle temps d'une localisation et de son voisinage. Nous proposons un algorithmed'extraction efficace qui effectue un parcours en profondeur en s'appuyant surdes projections successives de la base de données. Nous introduisons égalementune mesure d'intérêt adaptée aux aspects spatio-temporels de ces motifs. Les expérimentationsréalisées sur des jeux de données réels soulignent la pertinencede l'approche proposée par rapport aux méthodes de la littérature.	Hugo Alatrista Salas, Sandra Bringay, Frédéric Flouvat, Nazha Selmaoui-Folcher, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001159	http://editions-rnti.fr/render_pdf.php?p=1001159	385	fr	fr	@teledetection.fr, @lirmm.fr, @univ-nc.nc	Vers un approche efficace d' extraction de motif spatio- séquentiels  ce dernier année , le augmentation de le quantité d' informationsspatio- temporel stocker dans le base de donnée avoir faire naître de nouveauxbesoins , notamment en matière de gestion des risque naturel , sanitaire ou anthropique ( page ex. compréhension de le dynamique d' un épidémie de Dengue ) . Dans ce article , nous définir un cadre théorique pour le extraction de motifsspatio- séquentiel , séquence de motif spatial représenter le évolution dansle temps d' un localisation et de son voisinage . Nous proposer un algorithmed'extraction efficace qui effectuer un parcours en profondeur en clr appuyer surdes projection successif de le base de donnée . Nous introduire égalementune mesure d' intérêt adapter aux aspect spatio- temporel de ce motif . le expérimentationsréalisées sur un jeu de donnée réel souligner le pertinencede le approche proposer par rapport aux méthode de le littérature . 	Vers une approche efficace d'extraction de motifs spatio-séquentiels	1
Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers une méthode automatique de construction de hiérarchies contextuelles	Dans de nombreux domaines (e.g., fouille de données, entrepôts dedonnées), l'existence de hiérarchies sur certains attributs peut être extrêmementutile dans le processus analytique. Toutefois, cette connaissance n'est pas toujoursdisponible ou adaptée. Il est alors nécessaire de disposer d'un processusde découverte automatique pour palier ce problème. Dans cet article, nous combinonset adaptons des techniques issues de la théorie de l'information et duclustering pour proposer une technique orientée données de construction automatiquede taxonomies. Les deux principaux avantages d'une telle approchesont son caractère totalement non-supervisé et l'absence de paramètre utilisateurà spécifier. Afin de valider notre approche, nous l'avons appliquée sur desdonnées réelles et avons conduit plusieurs types d'expérimentation. D'abord,les hiérarchies obtenues ont été expertisées pour en examiner le pouvoir informatif.Ensuite, nous avons évalué l'apport de ces taxonomies comme support àdes tâches de fouille de données nécessitant une définition hiérarchique des valeursd'attributs : l'extraction de séquences fréquentes multidimensionnelles etmulti-niveaux ainsi que la construction de résumés de tables relationnelles. Lesrésultats obtenus permettent de conclure quant à l'intérêt de notre approche	Dino Ienco, Yoann Pitarch, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001186	http://editions-rnti.fr/render_pdf.php?p=1001186	386	fr	fr	@teledetection.fr, @cs.aau.dk, @lirmm.fr	Vers un méthode automatique de construction de hiérarchie contextuelles  Dans un nombreux domaine ( e.g. , fouille de donnée , entrepôt dedonnées ) , le existence de hiérarchie sur certain attribut pouvoir être extrêmementutile dans le processus analytique . toutefois , ce connaissance n' être pas toujoursdisponible ou adapter . Il être alors nécessaire de disposer d' un processusde découverte automatique pour palier ce problème . Dans ce article , nous combinonset adapter un technique issu de le théorie de le information et duclustering pour proposer un technique orienter donner de construction automatiquede taxonomie . le deux principal avantage d' un tel approchesont son caractère totalement non- superviser et le absence de paramètre utilisateurà spécifier . Afin de valider son approche , nous l' avoir appliquer sur desdonnées réel et avoir conduire plusieurs type d' expérimentation . D' abord , le hiérarchie obtenir avoir être expertiser pour en examiner le pouvoir informatif . ensuite , nous avoir évaluer le apport de ce taxonomie comme support àdes tâche de fouille de donnée nécessiter un définition hiérarchique des valeursd'attributs : le extraction de séquence fréquent multidimensionnel etmulti-niveaux ainsi que le construction de résumé de table relationnel . Lesrésultats obtenir permettre de conclure quant à le intérêt de son approche 	Vers une méthode automatique de construction de hiérarchies contextuelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2012	Webmarks : Le marquage d'intérêt sur le Web de données	Depuis son apparition au sein du W3C, la définition de la ressourceWeb n'a cessé d'évoluer au delà du simple document. Lieu, service, conceptd'ontologie, représentation d'un objet réel ou non, la ressource web est complexeet il nous a semblé que les outils à disposition des internautes pour sa manipulation,comme les bookmarks par exemple, n'exploitaient pas pleinementces nouvelles dimensions. Dans cet article, nous présenterons le modèle Webmarksqui permet de préciser l'objet du marquage, la ressource, mais égalementl'intérêt de l'auteur de la marque. L'implémentation de ce modèle au sein duprojet ISICIL sera également présentée et nous discuterons de son apport encomparaison des technologies existantes	Nicolas Delaforge, Fabien Gandon	http://editions-rnti.fr/render_pdf.php?p1&p=1001152	http://editions-rnti.fr/render_pdf.php?p=1001152	387	fr	fr	@inria.fr	Webmarks : le marquage d' intérêt sur le Web de données  Depuis son apparition au sein du W3C , le définition de le ressourceWeb n' avoir cesser d' évoluer au delà du simple document . lieu , service , conceptd'ontologie , représentation d' un objet réel ou non , le ressource web être complexeet il nous avoir sembler que le outil à disposition des internaute pour son manipulation , comme le bookmarks par exemple , n' exploiter pas pleinementces nouveau dimension . Dans ce article , nous présenter le modèle Webmarksqui permettre de préciser le objet du marquage , le ressource , mais égalementl'intérêt de le auteur de le marque . le implémentation de ce modèle au sein duprojet ISICIL être également présenter et nous discuter de son apport encomparaison un technologie existantes 	Webmarks : Le marquage d'intérêt sur le Web de données	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	@KRex : une méthode de construction des connaissances pour la maîtrise des activités à risques - application au domaine de la sécurité nucléaire	Dans les industries à risque, comme le nucléaire, les connaissances liées au savoir et à l'expérience participent à la maîtrise des activités. Elles sont explicites, formalisables dans des documents, ou tacites, expression du savoir faire moins souvent prise en compte. AREVA développe la méthode @KRex pour valoriser le retour d'expérience existant, créer une dynamique d'extraction et de capitalisation des connaissances, faciliter leur partage et leur enrichissement. Cette communication décrit le protocole expérimental de construction des connaissances explicites et tacites du métier sécurité nucléaire.	Julien Giudici, Hervé Janiaut, Rémy Gautier	http://editions-rnti.fr/render_pdf.php?p1&p=1001010	http://editions-rnti.fr/render_pdf.php?p=1001010	450	fr	fr	@krex, @ensam.eu, @ensam.eu, @areva.com, @krex	@KRex : un méthode de construction des connaissance pour le maîtrise des activité à risque - application au domaine de le sécurité nucléaire  Dans le industrie à risque , comme le nucléaire , le connaissance lier au savoir et à le expérience participer à le maîtrise des activité . Elles être explicite , formalisables dans un document , ou tacite , expression du savoir faire moins souvent prendre en compte . AREVA développer le méthode@KRex pour valoriser le retour d' expérience existant , créer un dynamique d' extraction et de capitalisation des connaissance , faciliter son partage et son enrichissement . ce communication décrire le protocole expérimental de construction des connaissance explicite et tacite du métier sécurité nucléaire . 	@KRex : une méthode de construction des connaissances pour la maîtrise des activités à risques - application au domaine de la sécurité nucléaire	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	A la recherche des tweets porteurs d'informations journalistiques		Benjamin Rosoor, Laurent Sebag, Sandra Bringay, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000961	http://editions-rnti.fr/render_pdf.php?p=1000961	451	fr		@webreport.fr, @lirmm.fr	A la recherche des tweets porteurs d'informations journalistiques 	A la recherche des tweets porteurs d'informations journalistiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Acquisition de structures lexico-sémantiques à partir de textes : un nouveau cadre de travail fondé sur une structuration prétopologique	Les structures lexico-sémantiques jouent un rôle essentiel dans les processus de fouille de textes. En codant les relations sémantiques entre concepts du discours elles apportent une connaissance stratégiques pour enrichir les capacités de raisonnement. Le développement de telles structures étant fortement limité du fait des efforts nécessaires à leur construction, nous proposons un nouveau formalisme d'acquisition automatique d'ontologies terminologiques à partir de textes. Nous utilisons pour cela une formalisation prétopologique de l'espace des termes sur laquelle s'appuie un modèle générique de structuration. Nous présentons une étude empirique préliminaire rendant compte du potentiel de ce modèle en terme d'extraction de connaissances.	Guillaume Cleuziou, Gaël Dias, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1000936	http://editions-rnti.fr/render_pdf.php?p=1000936	452	fr	fr	@univ-orleans.fr, @di.ubi.pt	acquisition de structure lexico- sémantique à partir de texte : un nouveau cadre de travail fonder sur un structuration prétopologique  le structure lexico- sémantique jouer un rôle essentiel dans le processus de fouille de texte . En coder le relation sémantique entre concept du discours elles apporter un connaissance stratégique pour enrichir le capacité de raisonnement . le développement de tel structure être fortement limiter du fait des effort nécessaire à son construction , nous proposer un nouveau formalisme d' acquisition automatique d' ontologie terminologique à partir de texte . Nous utiliser pour cela un formalisation prétopologique de le espace des terme sur laquelle clr appuyer un modèle générique de structuration . Nous présenter un étude empirique préliminaire rendre compte du potentiel de ce modèle en terme d' extraction de connaissance . 	Acquisition de structures lexico-sémantiques à partir de textes : un nouveau cadre de travail fondé sur une structuration prétopologique	4
Revue des Nouvelles Technologies de l'Information	EGC	2011	Adaptation de l'algorithme CART pour la tarification des risques en assurance non-vie	Les développements récents en tarification de l'assurance non-vie se concentrent majoritairement sur la maîtrise et l'amélioration des Modèles Linéaires Généralisés. Performants, ces modèles imposent cependant à la fois des contraintes sur la structure du risque modélisé et sur les interactions entre variables explicatives du risque. Ces restrictions peuvent conduire, dans certaines sous-populations d'assurés, à une estimation biaisée de la prime d'assurance. Les arbres de régression permettent de s'affranchir de ces contraintes et, de plus, augmentent la lisibilité des résultats de la tarification. Nous présentons une modification de l'algorithme CART pour prendre en compte les spécificités des données d'assurance non-vie. Nous comparons alors notre proposition aux modèles linéaires généralisés sur un portefeuille réel de véhicules. Notre proposition réduit les mesures d'erreur entre le risque mesuré et le risque modélisé, et permet ainsi une meilleure tarification.	Antoine Paglia, Martial Phélippé-Guinvarc'h, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1001028	http://editions-rnti.fr/render_pdf.php?p=1001028	453	fr	fr	@gmail.com, @sfr.fr, @groupama.com, @telecom-bretagne.eu	adaptation de le algorithme CART pour le tarification des risque en assurance non- vie  le développement récent en tarification de le assurance non- vie clr concentrer majoritairement sur le maîtrise et le amélioration des Modèles Linéaires Généralisés . performant , ce modèle imposer cependant à le foi des contrainte sur le structure du risque modéliser et sur le interaction entre variable explicatives du risque . ce restriction pouvoir conduire , dans certain sous-populations d' assuré , à un estimation biaiser de le prime d' assurance . le arbre de régression permettre de clr affranchir de ce contrainte et , de plus , augmenter le lisibilité des résultat de le tarification . Nous présenter un modification de le algorithme CART pour prendre en compte le spécificité des donnée d' assurance non- vie . Nous comparer alors son proposition aux modèle linéaire généraliser sur un portefeuille réel de véhicule . son proposition réduire le mesure d' erreur entre le risque mesurer et le risque modéliser , et permettre ainsi un meilleur tarification . 	Adaptation de l'algorithme CART pour la tarification des risques en assurance non-vie	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Agrégation robuste de données massives à la volée : application aux compteurs électriques communicants	Dans les années à venir, plusieurs millions de compteurs électriques communicants seront déployés sur l'ensemble du territoire français. Afin d'assurer la fiabilité d'un réseau de cette envergure nous proposons une topologie de communication multi-chemins qui repose sur la duplication des données transmises. Toute exploitation des données collectées doit alors tenir compte de la présence d'éléments dupliqués. Dans cet article, nous proposons une nouvelle méthode permettant de calculer en ligne des consommations électriques agrégées (agrégation spatiale). L'idée est d'adapter l'algorithme probabiliste Summation sketch de Considine et al. au contexte des compteurs communicants. Cette approche a l'avantage d'être insensible à la duplication et permet de profiter de la structure massivement distribuée du réseau de communication des futurs compteurs électriques. L'expérimentation de cette méthode sur des données réelles montre qu'elle donne une bonne précision sur l'estimation des consommations agrégées. Cette approche est aussi complétée par une méthode basée sur la théorie des sondages : On obtient une meilleure réactivité de l'estimateur avec rapidement et donc sur des données significativement partielles une erreur inférieure à 2.5%	Yousra Chabchoub, Benoît Grossin	http://editions-rnti.fr/render_pdf.php?p1&p=1000943	http://editions-rnti.fr/render_pdf.php?p=1000943	454	fr	fr	@edf.fr, @isep.fr	agrégation robuste de donnée massif à le volé : application aux compteur électrique communicants  Dans le année à venir , plusieurs million de compteur électrique communicant être déployer sur le ensemble du territoire français . Afin d' assurer le fiabilité d' un réseau de ce envergure nous proposer un topologie de communication multi-chemins qui reposer sur le duplication des donnée transmettre . tout exploitation des donnée collecter devoir alors tenir compte de le présence d' élément dupliquer . Dans ce article , nous proposer un nouveau méthode permettre de calculer en ligne des consommation électrique agréger ( agrégation spatial ) . le idée être d' adapter le algorithme probabiliste Summation sketch de Considine et al. au contexte des compteur communicant . ce approche avoir le avantage d' être insensible à le duplication et permettre de profiter de le structure massivement distribuer du réseau de communication des futur compteur électrique . le expérimentation de ce méthode sur un donnée réel montrer qu' elle donner un bon précision sur le estimation des consommation agréger . ce approche être aussi compléter par un méthode baser sur le théorie des sondage : On obtenir un meilleur réactivité de le estimateur avec rapidement et donc sur un donnée significativement partiel un erreur inférieur à 2 .5 \% 	Agrégation robuste de données massives à la volée : application aux compteurs électriques communicants	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Aide à l'Analyse Visuelle de Réseaux Sociaux pour la Détection de Comportements Suspects	Cet article traite de l'analyse visuelle de réseaux sociaux pour la détection de comportements suspects à partir de données de communications fournies à des enquêteurs suivant deux procédures : l'interception légale et la rétention de données. Nous proposons les contributions suivantes : (i) un modèle de données et un ensemble d'opérateurs pour interroger ces données dans le but d'extraire des comportements suspects et (ii) une représentation visuelle conviviale pour une navigation simplifiée dans les données de communication accompagnée avec une implémentation.	Amyn Bennamane, Hakim Hacid, Arnaud Ansiaux, Alain Cagnati	http://editions-rnti.fr/render_pdf.php?p1&p=1000948	http://editions-rnti.fr/render_pdf.php?p=1000948	455	fr	fr	@alcatel-lucent.com, @interieur.gouv.fr	aide à le analyse visuel de réseau Sociaux pour le détection de comportement Suspects  ce article traire de le analyse visuel de réseau social pour le détection de comportement suspect à partir de donnée de communication fournir à un enquêteur suivant deux procédure : le interception légal et le rétention de donnée . Nous proposer le contribution suivant : ( i ) un modèle de donnée et un ensemble d' opérateur pour interroger ce donnée dans le but d' extraire un comportement suspect et ( ii ) un représentation visuel convivial pour un navigation simplifier dans le donnée de communication accompagner avec un implémentation . 	Aide à l'Analyse Visuelle de Réseaux Sociaux pour la Détection de Comportements Suspects	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Algorithmes de recherche exhaustif et guidé pour la recommandation d'un expert dans un réseau professionnel		Maria Malek	http://editions-rnti.fr/render_pdf.php?p1&p=1000970	http://editions-rnti.fr/render_pdf.php?p=1000970	456	fr		@eisti.fr	Algorithmes de recherche exhaustif et guidé pour la recommandation d'un expert dans un réseau professionnel 	Algorithmes de recherche exhaustif et guidé pour la recommandation d'un expert dans un réseau professionnel	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse comparative de méthodologies et d'outils de construction automatique d'ontologies à partir de ressources textuelles	Plusieurs méthodologies et outils de construction automatique des ontologies à partir de ressources textuelles ont été proposés ces dernières années. Dans cet article nous analysons quatre approches en les comparant à une approche de référence - Methontology. Dans leur sélection nous avons privilégié celles qui couvrent l'ensemble des étapes du processus de construction d'ontologies. Puis nous analysons et comparons la portée, les limites et les performances des implémentations logicielles associées aux approches analysées. Ces outils ont été testés sur un corpus de ressources textuelles, et nous avons comparé leurs résultats à ceux obtenus manuellement.	Toader Gherasim, Mounira Harzallah, Giuseppe Berio, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000989	http://editions-rnti.fr/render_pdf.php?p=1000989	457	fr	fr	@univ-nantes.fr, @univ-nantes.fr, @univ-ubs.fr	analyse comparatif de méthodologie et d' outil de construction automatique d' ontologie à partir de ressource textuelles  plusieurs méthodologie et outil de construction automatique des ontologie à partir de ressource textuel avoir être proposer ce dernier année . Dans ce article nous analyser quatre approche en les comparer à un approche de référence - Methontology . Dans son sélection nous avoir privilégier celui qui couvrir le ensemble des étape du processus de construction d' ontologie . Puis nous analyser et comparer le portée , le limite et le performance des implémentation logiciel associer aux approche analyser . ce outil avoir être tester sur un corpus de ressource textuel , et nous avoir comparer son résultat à celui obtenir manuellement . 	Analyse comparative de méthodologies et d'outils de construction automatique d'ontologies à partir de ressources textuelles	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse du comportement limite d'indices probabilistes pour une sélection discriminante	Nous étudions ici le comportement de deux types d'indices probabilistes discriminants en présence de données dont le volume va en croissant. A cet égard, un modèle spécifique de croissance de la taille des données et de liaison entre variables est mis en oeuvre et celui-ci va permettre de déterminer le comportement limite des différents indices quel que soit le niveau de liaison entre la prémisse et la conclusion de la règle donnée. La clarté des résultats obtenus nous conduit à en chercher l'explication formelle. L'expérimentation a été effectuée avec la base de données UCI Wages.	Sylvie Guillaume, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001036	http://editions-rnti.fr/render_pdf.php?p=1001036	458	fr	fr	@isima, @irisa.fr	analyse du comportement limiter d' indice probabiliste pour un sélection discriminante  Nous étudier ici le comportement de deux type d' indice probabiliste discriminant en présence de donnée dont le volume aller en croissant . A ce égard , un modèle spécifique de croissance de le taille des donnée et de liaison entre variable être mettre en oeuvre et celui _-ci aller permettre de déterminer le comportement limiter un différent indice quel que être le niveau de liaison entre le prémisse et le conclusion de le règle donner . le clarté des résultat obtenir nous conduire à en chercher le explication formel . le expérimentation avoir être effectuer avec le base de donnée UCI Wages . 	Analyse du comportement limite d'indices probabilistes pour une sélection discriminante	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse factorielle des correspondances hiérarchique pour la fouille d'images	Nous proposons un outil graphique interactif qui permet de visualiser et d'extraire des connaissances à partir des résultats de l'Analyse Factorielle des Correspondances (AFC) sur les images. L'AFC est une technique descriptive développée pour analyser des tableaux de contingence. L'AFC est originellement utilisée dans l'Analyse des Données Textuelles (ADT) où le corpus est représenté par un tableau de contingence croisant des documents et des mots. Dans la fouille d'images, nous définissons d'abord les « mots visuels » dans les images (analogues aux mots textuels). Ces mots visuels sont construits à partir des descripteurs locaux SIFT (Scale Invariant Feature Transform) dans l'image. Ensuite, nous appliquons l'AFC sur le tableau de contingence obtenu. Notre outil (appelé HCAViz) analyse ce tableau de contingence de façon récursive et aide l'utilisateur à interpréter et interagir avec les résultats de l'AFC. D'abord, les résultats de la première AFC sur les images sont visualisés. L'utilisateur sélectionne ensuite un groupe d'images et fait une deuxième AFC sur le nouveau tableau de contingence. Ce processus peut continuer jusqu'à ce qu'un thème « pur » se dévoile. Ceci permet de découvrir une arborescence des thèmes dans une collection d'images. Une application sur la base Caltech-4 illustre l'intérêt de HCAViz dans la fouille d'images.	Nguyen-Khang Pham, Annie Morin, François Poulet, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1000941	http://editions-rnti.fr/render_pdf.php?p=1000941	459	fr	fr	@cit.ctu.edu.vn, @irisa.fr, @inria.fr	analyse factoriel des correspondance hiérarchique pour le fouille d' images  Nous proposer un outil graphique interactif qui permettre de visualiser et d' extraire un connaissance à partir un résultat de le analyse factorielle des correspondance ( AFC ) sur le image . le AFC être un technique descriptif développer pour analyser un tableau de contingence . le AFC être originellement utiliser dans le Analyse des Données Textuelles ( ADT ) où le corpus être représenter par un tableau de contingence croiser un document et des mot . Dans le fouille d' image , nous définir d' abord le « mot visuel » dans le image ( analogue aux mot textuel ) . ce mot visuel être construire à partir un descripteur local SIFT ( Scale Invariant Feature Transform ) dans le image . ensuite , nous appliquer le AFC sur le tableau de contingence obtenir . son outil ( appeler HCAViz ) analyser ce tableau de contingence de façon récursif et aider le utilisateur à interpréter et interagir avec le résultat de le AFC . D' abord , le résultat de le premier AFC sur le image être visualiser . le utilisateur sélectionner ensuite un groupe d' image et faire un deuxième AFC sur le nouveau tableau de contingence . ce processus pouvoir continuer jusqu' à ce qu' un thème « pur » clr dévoiler . ceci permettre de découvrir un arborescence des thème dans un collection d' image . un application sur le base Caltech- 4 illustrer le intérêt de HCAViz dans le fouille d' image . 	Analyse factorielle des correspondances hiérarchique pour la fouille d'images	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse spatiotemporelle des vecteurs de mouvement : application au comptage des personnes	Cet article présente une nouvelle approche qui permet de compter le nombre d'individus franchissant une ligne de comptage. L'approche proposée accumule dans le temps les vecteurs de mouvement pour chaque point de la ligne de comptage formant une carte spatiotemporelle. Une procédure de détection en ligne des blobs est ensuite utilisée afin de déterminer les régions de la carte spatiotemporelle qui correspondent à des personnes franchissant cette ligne. Le nombre d'individus associé à chaque blob est estimé grâce à un modèle de régression linéaire appliqué aux caractéristiques du blob. L'approche proposée est validée sur la base de plusieurs ensembles de données enregistrées à l'aide d'une caméra verticale ou d'une caméra oblique.	Yassine Benabbas, Tarek Yahiaoui, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000942	http://editions-rnti.fr/render_pdf.php?p=1000942	460	fr	fr	@lifl.fr	analyse spatiotemporelle des vecteur de mouvement : application au comptage des personnes  ce article présenter un nouveau approche qui permettre de compter le nombre d' individu franchir un ligne de comptage . le approche proposer accumuler dans le temps le vecteur de mouvement pour chaque point de le ligne de comptage former un carte spatiotemporelle . un procédure de détection en ligne des blobs être ensuite utiliser afin de déterminer le région de le carte spatiotemporelle qui correspondre à un personne franchir ce ligne . le nombre d' individu associer à chaque blob être estimer grâce à un modèle de régression linéaire appliquer aux caractéristique du blob . le approche proposer être valider sur le base de plusieurs ensemble de donnée enregistrer à le aide d' un caméra vertical ou d' un caméra oblique . 	Analyse spatiotemporelle des vecteurs de mouvement : application au comptage des personnes	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Annotation d'entités nommées par extraction de règles de transduction	La reconnaissance d'entités nommées est une problématique majoritairement traitée par des modèles spécifiés à l'aide de règles ou par apprentissage numérique. Les premiers ont le désavantage d'être coûteux à développer pour obtenir une couverture satisfaisante, les seconds sont souvent difficiles à interpréter par des experts (linguistes). Dans cet article, nous présentons une approche, dont l'objectif est d'extraire des règles symboliques discriminantes qu'un humain puisse consulter. A partir d'un corpus de référence, nous extrayons des règles de transduction, dont seules les plus informatives sont retenues. Elles sont ensuite appliquées pour effectuer une annotation : à cet effet, un algorithme recherche parmi les annotations possibles celles de meilleure qualité en termes de couverture et de probabilité. Nous présentons les résultats expérimentaux et discutons de l'intérêt et des perspectives de notre approche.	Arnaud Soulet, Damien Nouvel	http://editions-rnti.fr/render_pdf.php?p1&p=1000937	http://editions-rnti.fr/render_pdf.php?p=1000937	461	fr	fr	@univ-tours.fr, @univ-tours.fr	annotation d' entité nommer par extraction de règle de transduction  le reconnaissance d' entité nommer être un problématique majoritairement traiter par un modèle spécifier à le aide de règle ou par apprentissage numérique . le premier avoir le désavantage d' être coûteux à développer pour obtenir un couverture satisfaisant , le second être souvent difficile à interpréter par un expert ( linguiste ) . Dans ce article , nous présenter un approche , dont le objectif être d' extraire un règle symbolique discriminant qu' un humain pouvoir consulter . A partir d' un corpus de référence , nous extraire un règle de transduction , dont seul le plus informatif être retenir . Elles être ensuite appliquer pour effectuer un annotation : à ce effet , un algorithme rechercher parmi le annotation possible celui de meilleur qualité en terme de couverture et de probabilité . Nous présenter le résultat expérimental et discuter de le intérêt et des perspective de son approche . 	Annotation d'entités nommées par extraction de règles de transduction	6
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apport de la catégorisation iconique pour la gestion coopérative des connaissances1		Xiaoyue Ma, Jean-Pierre Cahier, L'Hédi Zaher	http://editions-rnti.fr/render_pdf.php?p1&p=1000978	http://editions-rnti.fr/render_pdf.php?p=1000978	462	fr		@utt.fr, @gmail.com	Apport de la catégorisation iconique pour la gestion coopérative des connaissances1 	Apport de la catégorisation iconique pour la gestion coopérative des connaissances1	
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apport des données thématiques dans les systèmes de recommandation : hybridation et démarrage à froid	"Des travaux récents (Pilaszy et al., 2009) suggèrent que les métadonnées sont quasiment inutiles pour les systèmes de recommandation, y compris en situation de cold-start : les données de logs de notation sont beaucoup plus informatives. Nous étudions, sur une base de référence de logs d'usages pour la recommandation automatique de DVD (Netflix), les performances de systèmes de recommandation basés sur des sources de données collaboratives, thématiques et hybrides en situation de démarrage à froid (cold-start). Nous exhibons des cas expérimentaux où les métadonnées apportent plus que les données de logs d'usage (collaboratives) pour la performance prédictive. Pour gérer le cold-start d'un système de recommandation, nous montrons que des approches ""en cascade"", thématiques puis hybrides, puis collaboratives, seraient plus appropriées."	Frank Meyer, Éric Gaussier, Fabrice Clérot, Julien Schluth	http://editions-rnti.fr/render_pdf.php?p1&p=1000947	http://editions-rnti.fr/render_pdf.php?p=1000947	463	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com, @imag.fr, @gmail.com	apport des donnée thématique dans le système de recommandation : hybridation et démarrage à froid  " un travail récent ( Pilaszy et al. , 2009 ) suggérer que le métadonnées être quasiment inutile pour le système de recommandation , y comprendre en situation de cold-start : le donnée de logs de notation être beaucoup plus informatif . . Nous étudier , sur un base de référence de logs d' usage pour le recommandation automatique de DVD ( Netflix ) , le performance de système de recommandation baser sur un source de donnée collaboratif , thématique et hybride en situation de démarrage à froid ( cold-start ) . . Nous exhiber un cas expérimental où le métadonnées apporter plus que le donnée de logs d' usage ( collaboratif ) pour le performance prédictif . . Pour gérer le cold-start d' un système de recommandation , nous montrer que un approche " " en cascade " " , thématique puis hybride , puis collaboratif , être plus approprier . " 	Apport des données thématiques dans les systèmes de recommandation : hybridation et démarrage à froid	5
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apprendre les contraintes topologiques dans les cartes auto-organisatrices	La Carte Auto-Organisatrice (SOM : Self-Organizing Map) est une méthode populaire pour l'analyse de la structure d'un ensemble de données. Cependant, certaines contraintes topologiques de la SOM sont fixées avant l'apprentissage et peuvent ne pas être pertinentes pour la représentation de la structure des données. Dans cet article nous nous proposons d'améliorer les performances des SOM avec un nouvel algorithme qui apprend les contraintes topologiques de la carte à partir des données. Des expériences sur des bases de données artificielles et réelles montrent que l'algorithme proposé produit de meilleurs résultats que SOM classique. Ce n'est pas le cas avec une relaxation triviale des contraintes topologiques, qui résulte en une forte augmentation de l'erreur topologique de la carte.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000939	http://editions-rnti.fr/render_pdf.php?p=1000939	464	fr	fr	@univ-paris13.fr	apprendre le contrainte topologique dans le carte auto- organisatrices  le Carte Auto- Organisatrice ( SOM : Self-Organizing Map ) être un méthode populaire pour le analyse de le structure d' un ensemble de donnée . cependant , certain contrainte topologique de le SOM être fixer avant le apprentissage et pouvoir ne pas être pertinent pour le représentation de le structure des donnée . Dans ce article nous nous proposer d' améliorer le performance des SOM avec un nouveau algorithme qui apprendre le contrainte topologique de le carte à partir un donnée . un expérience sur un base de donnée artificiel et réel montrer que le algorithme proposer produire de meilleur résultat que SOM classique . Ce n' être pas le cas avec un relaxation trivial des contrainte topologique , qui résulter en un fort augmentation de le erreur topologique de le carte . 	Apprendre les contraintes topologiques dans les cartes auto-organisatrices	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apprentissage génératif de la structure de réseaux logiques de Markov à partir d'un graphe des prédicats	Les Réseaux Logiques de Markov (MLNs) combinent l'apport statistique des Réseaux de Markov à la logique du premier ordre. Dans cette approche, chaque clause logique se voit affectée d'un poids, l'instanciation des clauses permettant alors de produire un Réseau deMarkov. L'apprentissage d'un MLN consiste à apprendre d'une part sa structure (la liste de clauses logiques) et d'autre part les poids de celles-ci. Nous proposons ici une méthode d'apprentissage génératif de Réseau Logique de Markov. Cette méthode repose sur l'utilisation d'un graphe des prédicats, produit à partir d'un ensemble de prédicats et d'une base d'apprentissage. Une méthode heuristique de variabilisation est mise en oeuvre afin de produire le jeu de clauses candidates. Les résultats présentés montrent l'intérêt de notre approche au regard de l'état de l'art.	Quang-Thang Dinh, Matthieu Exbrayat, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000993	http://editions-rnti.fr/render_pdf.php?p=1000993	465	fr	fr	@univ-orleans.fr	apprentissage génératif de le structure de réseau logique de Markov à partir d' un graphe des prédicats  le réseau logique de Markov ( MLNs ) combiner le apport statistique des réseau de Markov à le logique du premier ordre . Dans ce approche , chaque clause logique clr voir affecter d' un poids , le instanciation des clause permettre alors de produire un réseau deMarkov . le apprentissage d' un MLN consister à apprendre d' un part son structure ( le liste de clause logique ) et d' autre part le poids de celui _-ci . Nous proposer ici un méthode d' apprentissage génératif de Réseau Logique de Markov . ce méthode reposer sur le utilisation d' un graphe des prédicat , produire à partir d' un ensemble de prédicat et d' un base d' apprentissage . un méthode heuristique de variabilisation être mettre en oeuvre afin de produire le jeu de clause candidat . le résultat présenter montrer le intérêt de son approche au regard de le état de le art . 	Apprentissage génératif de la structure de réseaux logiques de Markov à partir d'un graphe des prédicats	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Cartes cognitives : une exploitation à base d'échelle, vue et profil	Une carte cognitive est un réseau d'influences entre différents concepts. Le modèle des cartes cognitives permet à un utilisateur de calculer l'influence entre deux concepts. Les cartes cognitives contenant un grand nombre de concepts et d'influences sont difficiles à comprendre. Cet article introduit la notion de carte cognitive ontologique qui associe une ontologie à une carte cognitive classique pour en organiser les concepts. Afin de faciliter la compréhension d'une carte, l'utilisateur peut obtenir une vue de cette carte la simplifiant selon une échelle qu'il aura choisie. Un profil peut être créé pour construire des vues correspondant aux objectifs d'un type d'utilisateur. Si une carte est manipulée par différents utilisateurs, leurs profils combinés permettent de construire une vue partagée.	Lionel Chauvin, David Genest, Aymeric Le Dorze, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001008	http://editions-rnti.fr/render_pdf.php?p=1001008	466	fr	fr	@univ-angers.fr	carte cognitif : un exploitation à base d' échelle , vue et profil  un carte cognitif être un réseau d' influence entre différents concept . le modèle des carte cognitif permettre à un utilisateur de calculer le influence entre deux concept . le carte cognitif contenir un grand nombre de concept et d' influence être difficile à comprendre . ce article introduire le notion de carte cognitif ontologique qui associer un ontologie à un carte cognitif classique pour en organiser le concept . Afin de faciliter le compréhension d' un carte , le utilisateur pouvoir obtenir un vue de ce carte le simplifier selon un échelle qu' il avoir choisir . un profil pouvoir être créer pour construire un vue correspondre aux objectif d' un type d' utilisateur . Si un carte être manipuler par différents utilisateur , son profil combiner permettre de construire un vue partager . 	Cartes cognitives : une exploitation à base d'échelle, vue et profil	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Catégorisation des mesures d'intérêt pour l'extraction des connaissances	"La recherche de règles d'association intéressantes est un domaine de recherche important et actif en fouille de données. Les algorithmes de la famille Apriori reposent sur deux mesures pour extraire les règles, le support et la confiance. Bien que ces deux mesures possèdent des vertus algorithmiques accélératrices, elles génèrent un nombre prohibitif de règles dont la plupart sont redondantes et sans intérêt. Il est donc nécessaire de disposer d'autres mesures filtrant les règles inintéressantes. Des travaux ont été réalisés pour dégager les ""bonnes"" propriétés des mesures d'extraction des règles et ces propriétés ont été évaluées sur 61 mesures. L'objectif de cet article est de dégager des catégories de mesures afin de répondre à une préoccupation des utilisateurs : le choix d'une ou plusieurs mesures lors d'un processus d'extraction des connaissances dans le but d'éliminer les règles valides non pertinentes extraites par le couple (support, confiance). L'évaluation des propriétés sur les 61 mesures a permis de dégager 9 classes de mesures, classes obtenues grâce à deux techniques : une méthode de la classification ascendante hiérarchique et une version de la méthode de classification non-hiérarchique des k-moyennes."	Sylvie Guillaume, Dhouha Grissa, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001014	http://editions-rnti.fr/render_pdf.php?p=1001014	467	fr	fr	@isima, @isima, @isima	catégorisation des mesure d' intérêt pour le extraction des connaissances  " le recherche de règle d' association intéressant être un domaine de recherche important et actif en fouille de donnée . . le algorithme de le famille Apriori reposer sur deux mesure pour extraire le règle , le support et le confiance . . bien que ce deux mesure posséder un vertu algorithmique accélérateur , elles générer un nombre prohibitif de règle dont le plupart être redondant et sans intérêt . . Il être donc nécessaire de disposer un autre mesure filtrer le règle inintéressant . . un travail avoir être réaliser pour dégager le " " bon " " propriété des mesure d' extraction des règle et ce propriété avoir être évaluer sur 61 mesure . . le objectif de ce article être de dégager un catégorie de mesure afin de répondre à un préoccupation des utilisateur : le choix d' un ou plusieurs mesure lors d' un processus d' extraction des connaissance dans le but d' éliminer le règle valide non pertinent extraire par le couple ( support , confiance ) . . le évaluation des propriété sur le 61 mesure avoir permettre de dégager 9 classe de mesure , classe obtenir grâce à deux technique : un méthode de le classification ascendant hiérarchique et un version de le méthode de classification non- hiérarchique des k-moyennes . " 	Catégorisation des mesures d'intérêt pour l'extraction des connaissances	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Classificateurs aléatoires Topologiques à base de graphes de voisinage	En apprentissage supervisé, les Méthodes Ensemble (ME) ont montré leurs qualités. L'une des méthodes de référence dans ce domaine est les Forêts Aléatoires (FA). Cette dernière repose sur des partitionnements de l'espace de représentation selon des frontières parallèles aux axes ou obliques. Les conséquences de cette façon de partitionner l'espace de représentation peuvent affecter la qualité de chaque prédicteur. Il nous a semblé que cette approche pouvait être améliorée si on se libérait de cette contrainte de manière à mieux coller à la structure topologique de l'ensemble d'apprentissage. Dans cet article, nous proposons une nouvelle ME basée sur des graphes de voisinage dont les performances, sur nos premières expérimentations, sont aussi bonnes que celles des FA.	Fabien Rico, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000933	http://editions-rnti.fr/render_pdf.php?p=1000933	468	fr	fr	@univ-lyon1.fr, @univ-lyon2.fr	classificateur aléatoire Topologiques à base de graphe de voisinage  En apprentissage superviser , le méthode ensemble ( ME ) avoir montrer son qualité . le un des méthode de référence dans ce domaine être le forêt Aléatoires ( FA ) . ce dernier reposer sur un partitionnements de le espace de représentation selon un frontière parallèle aux axe ou oblique . le conséquence de ce façon de partitionner le espace de représentation pouvoir affecter le qualité de chaque prédicteur . Il nous avoir sembler que ce approche pouvoir être améliorer si on clr libérer de ce contrainte de manière à mieux coller à le structure topologique de le ensemble d' apprentissage . Dans ce article , nous proposer un nouveau ME baser sur un graphe de voisinage dont le performance , sur son premier expérimentation , être aussi bon que celui des FA . 	Classificateurs aléatoires Topologiques à base de graphes de voisinage	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Classification des aéronefs par estimation de la pose	Dans le présent travail, nous proposons un outil d'aide à la reconnaissance de cibles radar basé sur la signature de forme et de la pose de la cible. La tâche principale dans le cadre de cet article consiste à établir la fonction de recherche d'images ISAR par l'exemple en exploitant l'information de pose estimée depuis les images ISAR. L'objectif est d'introduire l'information de pose dans l'indexation des images, notamment dans la phase de sélection des images candidates. Nous proposons une nouvelle méthode d'estimation de la pose basée sur l'axe le plus symétrique de la cible. La méthode proposée est ensuite comparée avec d'autres techniques connues telles que la transformée de Hough et la transformée en ondelette. Enfin, la tâche de classification est réalisée en utilisant les k-plus proches voisins incluant l'information de la pose.	Mohamed Nabil Saidi, Abdelmalek Toumi, Ali Khenchaf, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1001012	http://editions-rnti.fr/render_pdf.php?p=1001012	469	fr	fr	@ensieta.fr, @fsr.ac.ma	classification des aéronef par estimation de le pose  Dans le présent travail , nous proposer un outil d' aide à le reconnaissance de cible radar baser sur le signature de forme et de le pose de le cible . le tâche principal dans le cadre de ce article consister à établir le fonction de recherche d' image ISAR par le exemple en exploiter le information de pose estimer depuis le image ISAR . le objectif être d' introduire le information de pose dans le indexation des image , notamment dans le phase de sélection des image candidat . Nous proposer un nouveau méthode d' estimation de le pose baser sur le axe le plus symétrique de le cible . le méthode proposer être ensuite comparer avec un autre technique connaître tel que le transformer de Hough et le transformer en ondelette . enfin , le tâche de classification être réaliser en utiliser le k-plus proche voisin inclure le information de le pose . 	Classification des aéronefs par estimation de la pose	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Closed-set-based Discovery of Representative Association Rules Revisited	"The output of an association rule miner is often huge in practice. This is why several concise lossless representations have been proposed, such as the ""essential"" or ""representative"" rules. We revisit the algorithm given by Kryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS 2189, 350-359) for mining representative rules. We show that its output is sometimes incomplete, due to an oversight in its mathematical validation, and we propose an alternative complete generator that works within only slightly larger running times."	José L Balcazar , Cristina Tîrnauca	http://editions-rnti.fr/render_pdf.php?p1&p=1001032	http://editions-rnti.fr/render_pdf.php?p=1001032	470	en	en	@unican.es	close set base discovery representative association rule revisit the output association rule miner often huge practice several concise lossless representation propose essential representative rule revisit algorithm give kryszkiewicz int symp intelligent data analysis 2001 springer verlag lnc 2189 350 359 mining representative rule show output sometimes incomplete due oversight mathematical validation propose alternative complete generator work within slightly larger run time	Closed-set-based Discovery of Representative Association Rules Revisited	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Comparaison entre deux indices pour l'évaluation probabiliste discriminante des règles d'association	"L'élaboration d'une échelle de probabilité discriminante pour la comparaison mutuelle entre plusieurs attributs observés sur un échantillon d'objets de ""grosse"" taille, nécessite une normalisation préalable. L'objet de cet article est l'analyse comparée entre deux approches. La première dérive de l' ""Analyse de la Vraisemblance des Liens Relationnels Normalisée"". La seconde est fondée sur la notion de ""Valeur Test"" sur un échantillon virtuel de taille 100, synthétisant l'échantillon initial."	Israël-César Lerman, Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1001034	http://editions-rnti.fr/render_pdf.php?p=1001034	471	fr	fr	@irisa.fr, @isima	comparaison entre deux indice pour le évaluation probabiliste discriminant des règle d' association  " le élaboration d' un échelle de probabilité discriminant pour le comparaison mutuel entre plusieurs attribut observer sur un échantillon d' objet de " " gros " " taille , nécessiter un normalisation préalable . . le objet de ce article être le analyse comparer entre deux approche . . le premier dérive de le " " analyse de le vraisemblance des lien Relationnels normaliser " " . . le second être fonder sur le notion de " " Valeur Test " " sur un échantillon virtuel de taille 100 , synthétiser le échantillon initial . " 	Comparaison entre deux indices pour l'évaluation probabiliste discriminante des règles d'association	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Complex Information Processing	It is commonplace nowadays to claim that information is everywhere and that, as a result, finding the right information (mathematically : according to a set of criteria optimizing a specific goal) is very difficult. Defence applications have to cope with similar problems : communication networks, surveillance and information systems transmit and generate significant amounts of complex information which cannot be processed with low level algorithms. The challenge is to build high-level processing units (which demand a lot of computing power) so as process video streams and communication packets with little possibility of a false alarm as automatically as possible. Methods for processing, aligning, merging low-level and high-level information (from syntactic to semantic information) extracted from still images, videos, speech, text and the Internet are being considered. The framework includes theoretical approaches, algorithms as well as evaluation methods. Topics of interest are data fusion, learning techniques, data mining, HCI, even Artificial Intelligence. Defence applications are numerous, from scene understanding to weak signal detection.	Jacques Blanc-Talon	http://editions-rnti.fr/render_pdf.php?p1&p=1000922	http://editions-rnti.fr/render_pdf.php?p=1000922	472	en	en	@dga.defense.gouv.fr	complex information processing commonplace nowadays claim information everywhere that result find right information mathematically accord set criterium optimize specific goal difficult defence application cope similar problem communication network surveillance information system transmit generate significant amount complex information cannot process low level algorithms challenge build high level processing unit which demand lot compute power process video stream communication packet little possibility false alarm automatically possible method process aligning merge low level high level information from syntactic semantic information extract still image video speech text internet consider framework include theoretical approach algorithm well evaluation method topic interest datum fusion learn technique datum mining hci even artificial intelligence defence application numerous scene understanding weak signal detection	Complex Information Processing	733
Revue des Nouvelles Technologies de l'Information	EGC	2011	Conception et implémentation d'une nouvelle technique cellulaire de discrétisation : intégration dans TANAGRA		Mohamed Benamina, Baghdad Atmani	http://editions-rnti.fr/render_pdf.php?p1&p=1000976	http://editions-rnti.fr/render_pdf.php?p=1000976	473	fr		@gmail.com, @gmail.com	Conception et implémentation d'une nouvelle technique cellulaire de discrétisation : intégration dans TANAGRA 	Conception et implémentation d'une nouvelle technique cellulaire de discrétisation : intégration dans TANAGRA	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Construction d'une Ontologie d'aide au renforcement de la sécurité des systèmes de transport automatisés.		Lassaâd Mejri, Ahmed Maalel, Habib Hadj Mabrouk, Henda Ben Ghezela Hadjami	http://editions-rnti.fr/render_pdf.php?p1&p=1000967	http://editions-rnti.fr/render_pdf.php?p=1000967	474	fr		@yahoo.fr, @gmail.com, @gnet.tn, @inrets.fr	Construction d'une Ontologie d'aide au renforcement de la sécurité des systèmes de transport automatisés. 	Construction d'une Ontologie d'aide au renforcement de la sécurité des systèmes de transport automatisés.	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Construction ontologique à partir de séquences d'expression de champignons		Houda Fyad, Karim Bouamrane, Baghdad Atmani, Claire Toffano-Nioche	http://editions-rnti.fr/render_pdf.php?p1&p=1000968	http://editions-rnti.fr/render_pdf.php?p=1000968	475	fr		@gmail.com, @u-psud.fr	Construction ontologique à partir de séquences d'expression de champignons 	Construction ontologique à partir de séquences d'expression de champignons	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Data stream summarization by on-line histograms clustering		Antonio Balzanella, Lidia Rivoli, Rosanna Verde	http://editions-rnti.fr/render_pdf.php?p1&p=1000977	http://editions-rnti.fr/render_pdf.php?p=1000977	476	en		@gmail.com, @unina.it	Data stream summarization by on-line histograms clustering 	Data stream summarization by on-line histograms clustering	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Découverte de motifs d'évolution significatifs dans les séries temporelles d'images satellites	Les séries temporelles d'images satellites (ou Satellite Image Time Series - SITS) sont d'importantes sources d'informations sur l'évolution du territoire. Étudier ces images permet de comprendre les changements sur des zones précises mais aussi de découvrir des schémas d'évolution à grande échelle. Toutefois, découvrir ces phénomènes impose de répondre à plusieurs défis qui sont liés aux caractéristiques des SITS et à leurs contraintes. Premièrement, chaque pixel d'une image satellite est décrit par plusieurs valeurs (les niveaux radiométriques sur différentes longueurs d'ondes). Deuxièmement, ces motifs d'évolution portent sur des périodes très longues et ne sont pas forcément synchrones selon les régions. Troisièmement, les régions qui ne sont pas concernées par des évolutions significatives sont majoritaires et leur domination rend difficile l'extraction des motifs d'évolution. Dans cet article, nous proposons une méthode qui répond à ces difficultés et nous la validons sur une série d'images satellites acquises sur une période de 20 ans.	François Petitjean, Florent Masseglia, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1001037	http://editions-rnti.fr/render_pdf.php?p=1001037	477	fr	fr	@unistra.fr, @inria.fr	découverte de motif d' évolution significatif dans le série temporel d' image satellites  le série temporel d' image satellite ( ou Satellite image Time Series - SITS ) être d' important source d' information sur le évolution du territoire . Étudier ce image permettre de comprendre le changement sur un zone précis mais aussi de découvrir un schéma d' évolution à grand échelle . toutefois , découvrir ce phénomène imposer de répondre à plusieurs défi qui être lier aux caractéristique des SITS et à son contrainte . premièrement , chaque pixel d' un image satellite être décrire par plusieurs valeur ( le niveau radiométriques sur différent longueur d' onde ) . deuxièmement , ce motif d' évolution porter sur un période très long et ne être pas forcément synchrone selon le région . troisièmement , le région qui ne être pas concerner par un évolution significatif être majoritaire et son domination rendre difficile le extraction des motif d' évolution . Dans ce article , nous proposer un méthode qui répondre à ce difficulté et nous le validons sur un série d' image satellite acquérir sur un période de 20 an . 	Découverte de motifs d'évolution significatifs dans les séries temporelles d'images satellites	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Des graphes de documents aux réseaux sociaux		Michel Plantié, Michel Crampes	http://editions-rnti.fr/render_pdf.php?p1&p=1000972	http://editions-rnti.fr/render_pdf.php?p=1000972	478	fr		@mines-ales.fr	Des graphes de documents aux réseaux sociaux 	Des graphes de documents aux réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection de changements de distribution dans un flux de données : une approche supervisée	L'analyse de flux de données traite des données massives grâce à des algorithmes en ligne qui évitent le stockage exhaustif des données. La détection de changements dans la distribution d'un flux est une question importante dont les applications potentielles sont nombreuses. Dans cet article, la détection de changement est transposée en un problème d'apprentissage supervisé. Nous avons choisi d'utiliser la méthode de discrétisation supervisée MODL car celle-ci présente des propriétés intéressantes. Notre approche est comparée favorablement à une méthode de l'état-de-l'art sur des flux de données artificiels.	Marc Boullé, Alexis Bondu	http://editions-rnti.fr/render_pdf.php?p1&p=1000944	http://editions-rnti.fr/render_pdf.php?p=1000944	479	fr	fr	@edf.fr, @orange-ftgroup.com	détection de changement de distribution dans un flux de donnée : un approche supervisée  le analyse de flux de donnée traire un donnée massif grâce à un algorithme en ligne qui éviter le stockage exhaustif des donnée . le détection de changement dans le distribution d' un flux être un question important dont le application potentiel être nombreux . Dans ce article , le détection de changement être transposer en un problème d' apprentissage superviser . Nous avoir choisir d' utiliser le méthode de discrétisation superviser MODL car celui _-ci présent des propriété intéressant . son approche être comparer favorablement à un méthode de le état-de -l'art sur un flux de donnée artificiel . 	Détection de changements de distribution dans un flux de données : une approche supervisée	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection de redondances dans les tableaux guidée par une ontologie	Nous nous intéressons dans cet article à la réconciliation d'annotations floues associées à des tableaux de données par une méthode d'annotation sémantique, qui est guidée par une ontologie de domaine. Etant donnés deux tableaux, la méthode consiste à détecter leurs instances de relation redondantes. Elle s'appuie sur les connaissances déclarées dans l'ontologie, ainsi que sur des scores de similarité entre les annotations floues représentées par des sous-ensembles flous numériques ou par des sous-ensembles flous symboliques	Rania Khefifi, Patrice Buche, Juliette Dibie-Barthélemy, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001016	http://editions-rnti.fr/render_pdf.php?p=1001016	480	fr	fr	@lri.fr, @supagro.inra.fr, @risk, @agroparistech.fr	détection de redondance dans le tableau guider par un ontologie  Nous nous intéresser dans ce article à le réconciliation d' annotation flou associer à un tableau de donnée par un méthode d' annotation sémantique , qui être guider par un ontologie de domaine . Etant donner deux tableau , le méthode consister à détecter son instance de relation redondant . Elle clr appuyer sur le connaissance déclarer dans le ontologie , ainsi que sur un score de similarité entre le annotation flou représenter par un sous-ensemble flou numérique ou par un sous-ensemble flou symboliques 	Détection de redondances dans les tableaux guidée par une ontologie	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection des profils à long terme et à court terme dans les réseaux sociaux	La conception des profils et contextes utilisateurs se situe au coeur de l'étude et de la mise en oeuvre des mécanismes de personnalisation ou d'adaptation de contenus (recherche d'information, systèmes de recommandation, etc.). Plusieurs modèles et dimensions de profils et contextes sont décrits dans la littérature. Dans la vie réelle tout comme dans les systèmes d'information, le comportement de l'utilisateur est très souvent influencé par son environnement social. Cependant, la dimension sociale des profils et contextes utilisateurs reste très peu étudiée et évaluée. Dans cet article, nous présentons une méthode de visualisation des profils utilisateurs permettant d'évaluer la pertinence du réseau social de l'utilisateur dans l'évolution de son profil. L'expérimentation de la méthode à partir de Facebook permet d'identifier d'une part, les centres d'intérêts à court-terme et à long-terme des profils utilisateurs, et d'autre part, l'influence réelle à court-terme et à long-terme du réseau social de chaque utilisateur. Ces résultats démontrent l'intérêt de modéliser et d'intégrer une dimension sociale dans les profils et contextes utilisateurs, afin de tenter d'améliorer les mécanismes de personnalisation ou d'adaptation de contenus.	Dieudonné Tchuente, Marie-Françoise Canut, Nadine Baptiste-Jessel	http://editions-rnti.fr/render_pdf.php?p1&p=1000987	http://editions-rnti.fr/render_pdf.php?p=1000987	481	fr	fr	@irit.fr, @iut-blagnac.fr, @irit.fr	détection des profil à long terme et à court terme dans le réseau sociaux  le conception des profil et contexte utilisateur clr situer au coeur de le étude et de le mise en oeuvre des mécanisme de personnalisation ou d' adaptation de contenu ( recherche d' information , système de recommandation , etc. ) . plusieurs modèle et dimension de profil et contexte être décrire dans le littérature . Dans le vie réel tout comme dans le système d' information , le comportement de le utilisateur être très souvent influencer par son environnement social . cependant , le dimension social des profil et contexte utilisateur rester très peu étudier et évaluer . Dans ce article , nous présenter un méthode de visualisation des profil utilisateur permettre d' évaluer le pertinence du réseau social de le utilisateur dans le évolution de son profil . le expérimentation de le méthode à partir de Facebook permettre d' identifier d' un part , le centre d' intérêt à court-terme et à long-terme des profil utilisateur , et d' autre part , le influence réel à court-terme et à long-terme du réseau social de chaque utilisateur . ce résultat démontrer le intérêt de modéliser et d' intégrer un dimension social dans le profil et contexte utilisateur , afin de tenter d' améliorer le mécanisme de personnalisation ou d' adaptation de contenu . 	Détection des profils à long terme et à court terme dans les réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Early Classification on Temporal Sequences	Early classification of temporal sequences has applications in, for example, health informatics, intrusion detection, anomaly detection, and scientific and engineering sequence data monitoring. In early classification, instead of optimizing accuracy, our goal is to produce classification as early as possible provided that the accuracy meets some expectation. In this talk, I will advocate early classification as an exciting and challenging research problem, which has not been systematically studied in the literature. I will discuss several interesting formulations of the problem, which provide complimentary features possibly desirable in different application scenarios. I will also review some of our recent progress on this aspect.	Jian Pei	http://editions-rnti.fr/render_pdf.php?p1&p=1000918	http://editions-rnti.fr/render_pdf.php?p=1000918	482	en	en	@cs.sfu.ca	early classification temporal sequence early classification temporal sequence application in example health informatic intrusion detection anomaly detection scientific engineering sequence data monitoring early classification instead optimize accuracy goal produce classification early possible provide accuracy meet expectation talk advocate early classification exciting challenge research problem systematically studied literature discuss several interesting formulation problem provide complimentary feature possibly desirable different application scenario also review recent progress aspect	Early Classification on Temporal Sequences	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Entropic-Genetic Clustering	This paper addresses the clustering problem given the similarity matrix of a dataset. We define two distinct criteria with the aim of simultaneously minimizing the cut size and obtaining balanced clusters. The first criterion minimizes the similarity between objects belonging to different clusters and is an objective generally met in clustering. The second criterion is formulated with the aid of generalized entropy. The trade-off between these two objectives is explored using a multi-objective genetic algorithm with enhanced operators	Mihaela Breaban, Henri Luchian, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1000931	http://editions-rnti.fr/render_pdf.php?p=1000931	483	en	en	@infoiasi.ro, @cs.umb.edu	entropic genetic cluster paper address cluster problem give similarity matrix dataset define two distinct criterium aim simultaneously minimize cut size obtain balanced cluster first criterion minimize similarity object belong different cluster objective generally meet cluster second criterion formulate aid generalized entropy trade off two objective explore used multus objective genetic algorithm enhance operator	Entropic-Genetic Clustering	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Equilibrer l'analyse des motifs fréquents	Cet article propose une méthode originale d'évaluation de la qualité des motifs en anticipant la manière qui sera utilisée pour les analyser. Nous commençons par introduire le modèle de l'analyse aléatoire d'un ensemble de motifs selon une mesure d'intérêt. Avec ce modèle, nous constatons que l'étude des motifs fréquents avec le support conduit à une analyse déséquilibrée du jeu de données. Afin que chaque transaction reçoive la même attention, nous définissons le support équilibré qui corrige le support classique en pondérant les transactions. Nous proposons alors un algorithme qui calcule ces poids et nous validons expérimentalement son efficacité.	Arnaud Giacometti, Arnaud Soulet, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000927	http://editions-rnti.fr/render_pdf.php?p=1000927	484	fr	fr	@univ-tours.fr	Equilibrer le analyse des motif fréquents  ce article proposer un méthode original d' évaluation de le qualité des motif en anticiper le manière qui être utiliser pour les analyser . Nous commencer par introduire le modèle de le analyse aléatoire d' un ensemble de motif selon un mesure d' intérêt . Avec ce modèle , nous constater que le étude des motif fréquent avec le support conduire à un analyse déséquilibrer du jeu de donnée . Afin que chaque transaction recevoir le même attention , nous définir le support équilibrer qui corriger le support classique en pondérer le transaction . Nous proposer alors un algorithme qui calculer ce poids et nous valider expérimentalement son efficacité . 	Equilibrer l'analyse des motifs fréquents	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Equivalence topologique entre mesures de proximité	Le choix d'une mesure de proximité entre objets a un impact direct sur les résultats de toute opération de classification, de comparaison, d'évaluation ou de structuration d'un ensemble d'objets. Pour un problème donné, l'utilisateur est amené à choisir une parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence choisie, comme celle basée sur les préordonnances, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche pour comparer les mesures de proximité. Celle-ci est basée sur l'équivalence topologique. A cet effet, nous introduisons un nouveau concept baptisé équivalence topologique. Ce dernier fait appel à la structure de voisinage local. Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure. Nous établissons ensuite des liens formels avec l'équivalence en préordonnance. Les deux approches sont comparées sur le plan théorique et sur le plan empirique. Nous illustrons le principe de cette comparaison sur un exemple simple pour une quinzaine de mesures de proximités de la littérature.	Djamel Abdelkader Zighed, Rafik Abdesselam, Ahmed Bounekkar	http://editions-rnti.fr/render_pdf.php?p1&p=1000928	http://editions-rnti.fr/render_pdf.php?p=1000928	485	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr, @univ-lyon1.fr	Equivalence topologique entre mesure de proximité  le choix d' un mesure de proximité entre objet avoir un impact direct sur le résultat de tout opération de classification , de comparaison , d' évaluation ou de structuration d' un ensemble d' objet . Pour un problème donner , le utilisateur être amener à choisir un parmi le nombreux mesure de proximité existant . Or , selon le notion d' équivalence choisir , comme celui baser sur le préordonnances , certains être plus ou moins équivalent . Dans ce article , nous proposer un nouveau approche pour comparer le mesure de proximité . Celle _-ci être baser sur le équivalence topologique . A ce effet , nous introduire un nouveau concept baptiser équivalence topologique . ce dernier faire appel à le structure de voisinage local . Nous proposer alors de définir le équivalence topologique entre deux mesure de proximité à travers le structure topologique induire par chaque mesure . Nous établir ensuite un lien formel avec le équivalence en préordonnance . le deux approche être comparer sur le plan théorique et sur le plan empirique . Nous illustrer le principe de ce comparaison sur un exemple simple pour un quinzaine de mesure de proximité de le littérature . 	Equivalence topologique entre mesures de proximité	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Estimation de la densité d'arcs dans les graphes de grande taille: une alternative à la détection de clusters	"La recherche de structures dans les graphes est un sujet étudié depuis longtemps, qui a bénéficié d'un regain d'intérêt avec la mise à disposition de graphes de grande taille sur le web, tels les réseaux sociaux. De nombreuses méthodes de recherche de clusters ""naturels"" dans les graphes ont été proposées, fondées notamment sur la modularité de Newman. On introduit dans cet article une nouvelle façon de résumer la structure des graphes de grande taille, en utilisant des estimateurs de densité des arcs exploitant des modèles en grille, basés sur un co-partitionnent des noeuds source et cible des arcs. Les structures identifiées par cette méthode vont au delà de la ""classique"" détection de clusters dans les graphes, et permettent d'estimer asymptotiquement la densité des arcs. Les expérimentations confirment le potentiel de l'approche, qui permet d'identifier des structures fortement informatives dans les graphes, sans faire l'hypothèse d'une décomposition en clusters denses."	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000986	http://editions-rnti.fr/render_pdf.php?p=1000986	486	fr	fr	@orange-ftgroup.com	estimation de le densité d' arc dans le graphe de grand taille : un alternative à le détection de clusters  " le recherche de structure dans le graphe être un sujet étudier depuis longtemps , qui avoir bénéficier d' un regain d' intérêt avec le mise à disposition de graphe de grand taille sur le web , tel le réseau social . . un nombreux méthode de recherche de clusters " " naturel " " dans le graphe avoir être proposer , fonder notamment sur le modularité de Newman . . On introduire dans ce article un nouveau façon de résumer le structure des graphe de grand taille , en utiliser un estimateur de densité des arc exploiter un modèle en grille , baser sur un co- partitionnent des noeud source et cible des arc . . le structure identifier par ce méthode aller au delà de le " " classique " " détection de clusters dans le graphe , et permettre d' estimer asymptotiquement le densité des arc . . le expérimentation confirmer le potentiel de le approche , qui permettre d' identifier un structure fortement informatif dans le graphe , sans faire le hypothèse d' un décomposition en clusters dense . " 	Estimation de la densité d'arcs dans les graphes de grande taille: une alternative à la détection de clusters	4
Revue des Nouvelles Technologies de l'Information	EGC	2011	Être ou ne pas être usager d'internet telle est la question ?		Abdoulaye Sarr, Philippe Lenca, Annabelle Boutet, Jocelyne Tremenbert	http://editions-rnti.fr/render_pdf.php?p1&p=1000963	http://editions-rnti.fr/render_pdf.php?p=1000963	487	fr		@yahoo.fr, @telecom-bretagne.eu, @rsouin, @telecom-bretagne.eu, @telecom-bretagne.eu, @rsouin	Être ou ne pas être usager d'internet telle est la question ? 	Être ou ne pas être usager d'internet telle est la question ?	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Evaluation des outils d'extraction terminologique Quezao et Acabit	L'article décrit l'évaluation de deux outils d'extraction terminologique Acabit et Quezao. Si acabit est plus connu car librement disponible, Quezao est issu des travaux d'Orange Labs sur la recherche d'informations. Après une comparaison sur les approches théoriques des deux systèmes, une évaluation concrète va porter sur un corpus d'actualité (2424Actu) pour l'aspect qualitatif et sur un corpus de presse pour l'aspect quantitatif	Edmond Lassalle, Prem Kumar Casimir, Emilie Guimier De Neef	http://editions-rnti.fr/render_pdf.php?p1&p=1000938	http://editions-rnti.fr/render_pdf.php?p=1000938	488	fr	fr	@orange-ftgroup.com	Evaluation des outil d' extraction terminologique Quezao et Acabit  le article décrire le évaluation de deux outil d' extraction terminologique Acabit et Quezao . Si acabit être plus connaître car librement disponible , Quezao être issir un travail d' Orange Labs sur le recherche d' information . Après un comparaison sur le approche théorique des deux système , un évaluation concret aller porter sur un corpus d' actualité ( 2424Actu ) pour le aspect qualitatif et sur un corpus de presse pour le aspect quantitatif 	Evaluation des outils d'extraction terminologique Quezao et Acabit	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction de motifs séquentiels contextuels	Les motifs séquentiels traditionnels ne tiennent généralement pas compte des informations contextuelles fréquemment associées aux données séquentielles. Dans le cas des séquences d'achats de clients dans un magasin, l'extraction classique de motifs se focalise sur les achats des clients sans considérer leur catégorie socio-professionnelle, leur sexe, leur âge. Or, en considérant le fait qu'un motif séquentiel est spécifique à un contexte donné, un expert pourra adapter sa stratégie au type du client et prendre les décisions adéquates. Dans cet article, nous proposons d'extraire des motifs de la forme «l'achat des produits A et B suivi de l'achat du produit C est spécifique aux jeunes clients». En mettant en valeur les propriétés formelles de tels contextes, nous développons un algorithme efficace d'extraction de motifs séquentiels contextuels. Les expérimentations effectuées sur un jeu de données réelles montrent les apports et l'efficacité de l'approche proposée.	Julien Rabatel, Sandra Bringay	http://editions-rnti.fr/render_pdf.php?p1&p=1000924	http://editions-rnti.fr/render_pdf.php?p=1000924	489	fr	fr	@lirmm.fr	extraction de motif séquentiel contextuels  le motif séquentiel traditionnel ne tenir généralement pas compte des information contextuel fréquemment associer aux donnée séquentiel . Dans le cas des séquence d' achat de client dans un magasin , le extraction classique de motif clr focaliser sur le achat des client sans considérer son catégorie socio- professionnel , son sexe , son âge . Or , en considérer le fait qu' un motif séquentiel être spécifique à un contexte donner , un expert pouvoir adapter son stratégie au type du client et prendre le décision adéquat . Dans ce article , nous proposer d' extraire un motif de le forme « le achat des produit A et B suivre de le achat du produit C être spécifique aux jeune client » . En mettre en valeur le propriété formel de tel contexte , nous développer un algorithme efficace d' extraction de motif séquentiel contextuel . le expérimentation effectuer sur un jeu de donnée réel montrer le apport et le efficacité de le approche proposer . 	Extraction de motifs séquentiels contextuels	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction de motifs temporels à partir de séquences d'événements avec intervalles temporels	La fouille de base de données séquentielles a pour objet l'extraction de motifs séquentiels représentatifs. La plupart des méthodes concernent des motifs composés d'événements liés par des relations temporelles basées sur la précédence des instants. Pourtant, dans de nombreuses situations réelles une information quantitative sur la durée des événements ou le délai inter-événements est nécessaire pour discriminer les phénomènes. Nous proposons deux algorithmes, QTIAPriori et QTIPrefixSpan, pour extraire des motifs temporels composés d'événements associés à des intervalles décrivant leur position dans le temps et leur durée. Chacun d'eux ajoute aux algorithmes GSP et PrefixSpan une étape de catégorisation d'intervalles multi-dimensionnels pour extraire les intervalles temporelles représentatifs. Les expérimentations sur des données simulées montrent la capacité des algorithmes à extraire des motifs précis en présence de bruit et montrent l'amélioration des performances en temps de calcul.	Rene Quiniou, Thomas Guyet	http://editions-rnti.fr/render_pdf.php?p1&p=1000925	http://editions-rnti.fr/render_pdf.php?p=1000925	490	fr	fr	@agrocampus-ouest.fr, @irisa.fr	extraction de motif temporel à partir de séquence d' événement avec intervalle temporels  le fouille de base de donnée séquentiel avoir pour objet le extraction de motif séquentiel représentatif . le plupart des méthode concerner un motif composer d' événement lier par un relation temporel baser sur le précédence des instant . pourtant , dans un nombreux situation réel un information quantitatif sur le durée des événement ou le délai inter-événements être nécessaire pour discriminer le phénomène . Nous proposer deux algorithme , QTIAPriori et QTIPrefixSpan , pour extraire un motif temporel composer d' événement associer à un intervalle décrire son position dans le temps et son durée . chacun d' lui ajouter aux algorithme GSP et PrefixSpan un étape de catégorisation d' intervalle multi-dimensionnels pour extraire le intervalle temporel représentatif . le expérimentation sur un donnée simuler montrer le capacité des algorithme à extraire un motif précis en présence de bruit et montrer le amélioration des performance en temps de calcul . 	Extraction de motifs temporels à partir de séquences d'événements avec intervalles temporels	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction et Analyse de réseaux sociaux issus de Bases de Données Relationnelles	Dans un contexte d'entreprise, beaucoup d'informations importantes restent stockées dans des bases de données relationnelles, constituant une source riche pour construire des réseaux sociaux. Le réseau, ainsi extrait, a souvent une taille importante ce qui rend son analyse et sa visualisation difficiles. Dans ce travail, nous proposons une étape d'extraction suivie d'une étape d'agrégation des réseaux sociaux à partir des bases de données relationnelles. L'étape d'extraction ou de construction transforme une base de données relationnelle en base de données graphe, puis le réseau social est extrait. L'étape d'agrégation, qui est basée sur l'algorithme k-SNAP, produit un graphe résumé.	Rania Soussi, Amine Louati, Marie-Aude Aufaure, Hajer Baazaoui Zghal, Yves Lechevallier, Henda Ben Ghezela Hadjami	http://editions-rnti.fr/render_pdf.php?p1&p=1000988	http://editions-rnti.fr/render_pdf.php?p=1000988	491	fr	fr	@ecp.fr, @riadi.rnu.tn, @inria.fr	extraction et analyse de réseau social issir de base de Données Relationnelles  Dans un contexte d' entreprise , beaucoup d' information important rester stocker dans un base de donnée relationnel , constituer un source riche pour construire un réseau social . le réseau , ainsi extraire , avoir souvent un taille important ce qui rendre son analyse et son visualisation difficile . Dans ce travail , nous proposer un étape d' extraction suivre d' un étape d' agrégation des réseau social à partir un base de donnée relationnel . le étape d' extraction ou de construction transformer un base de donnée relationnel en base de donnée graphe , puis le réseau social être extraire . le étape d' agrégation , qui être baser sur le algorithme k-SNAP , produire un graphe résumer . 	Extraction et Analyse de réseaux sociaux issus de Bases de Données Relationnelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction sous contraintes d'ensembles de cliques homogènes	Nous proposons une méthode de fouille de données sur des graphes ayant un ensemble d'étiquettes associé à chaque sommet. Une application est, par exemple, d'analyser un réseau social de chercheurs co-auteurs lorsque des étiquettes précisent les conférences dans lesquelles ils publient.Nous définissons l'extraction sous contraintes d'ensembles de cliques tel que chaque sommet des cliques impliquées partage suffisamment d'étiquettes. Nous proposons une méthode pour calculer tous les Ensembles Maximaux de Cliques dits Homogènes qui satisfont une conjonction de contraintes fixée par l'analyste et concernant le nombre de cliques séparées, la taille des cliques ainsi que le nombre d'étiquettes partagées. Les expérimentations montrent que l'approche fonctionne sur de grands graphes construits à partir de données réelles et permet la mise en évidence de structures intéressantes	Pierre-Nicolas Mougel, Marc Plantevit, Christophe Rigotti, Olivier Gandrillon, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000999	http://editions-rnti.fr/render_pdf.php?p=1000999	492	fr	fr	@liris.cnrs.fr, @univ-lyon1.fr	extraction sous contrainte d' ensemble de clique homogènes  Nous proposer un méthode de fouille de donnée sur un graphe avoir un ensemble d' étiquette associer à chaque sommet . un application être , par exemple , d' analyser un réseau social de chercheur co- auteur lorsque un étiquette préciser le conférence dans lesquelles ils publier . Nous définir le extraction sous contrainte d' ensemble de clique tel que chaque sommet des clique impliquer partager suffisamment d' étiquette . Nous proposer un méthode pour calculer tout le ensemble Maximaux de Cliques dire Homogènes qui satisfaire un conjonction de contrainte fixer par le analyste et concerner le nombre de clique séparer , le taille des clique ainsi que le nombre d' étiquette partager . le expérimentation montrer que le approche fonctionner sur un grand graphe construire à partir de donnée réel et permettre le mise en évidence de structure intéressantes 	Extraction sous contraintes d'ensembles de cliques homogènes	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Heuristique pour l'extraction de motifs ensemblistes bruités	La recherche de motifs ensemblistes dans des matrices de données booléennes est une problématique importante dans un processus d'extraction de connaissances. Elle consiste à rechercher tous les rectangles de 1 dans une matrice de données à valeurs dans {0,1} dans lesquelles l'ordre des lignes et colonnes n'est pas important. Plusieurs algorithmes ont été développés pour répondre à ce problème, mais s'adaptent difficilement à des données réelles susceptibles de contenir du bruit. Un des effets du bruit est de pulvériser un motif pertinent en un ensemble de sous-motifs recouvrants et peu pertinents, entraînant une explosion du nombre de motifs résultats. Dans le cadre de ce travail, nous proposons une nouvelle approche heuristique basée sur les algorithmes de graphes pour la recherche de motifs ensemblistes dans des contextes binaires bruités. Pour évaluer notre approche, différents tests ont été réalisés sur des données synthétiques et des données réelles issues d'applications bioinformatiques.	Céline Rouveirol, Lucas Létocart, Karima Mouhoubi	http://editions-rnti.fr/render_pdf.php?p1&p=1001002	http://editions-rnti.fr/render_pdf.php?p=1001002	493	fr	fr	@univ-paris13.fr	heuristique pour le extraction de motif ensembliste bruités  le recherche de motif ensembliste dans un matrice de donnée booléen être un problématique important dans un processus d' extraction de connaissance . Elle consister à rechercher tout le rectangle de 1 dans un matrice de donnée à valeur dans { 0 , 1 } dans lesquelles le ordre des ligne et colonne n' être pas important . plusieurs algorithme avoir être développer pour répondre à ce problème , mais clr adapter difficilement à un donnée réel susceptible de contenir du bruit . un des effet du bruit être de pulvériser un motif pertinent en un ensemble de sous-motifs recouvrants et peu pertinent , entraîner un explosion du nombre de motif résultat . Dans le cadre de ce travail , nous proposer un nouveau approche heuristique baser sur le algorithme de graphe pour le recherche de motif ensembliste dans un contexte binaire bruiter . Pour évaluer son approche , différent test avoir être réaliser sur un donnée synthétique et un donnée réel issir d' application bioinformatique . 	Heuristique pour l'extraction de motifs ensemblistes bruités	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Import automatique et interactif de données dans les systèmes de visualisations	La première étape du processus de visualisation d'information consiste à transformer les données d'un format brut vers une structure de données utilisable par les différents composants de visualisation. Dans les applications réelles, cette première étape représente une barrière empêchant l'accès des utilisateurs novices à une riche variété de techniques de visualisation. Par exemple, il peut être techniquement impossible pour un utilisateur lambda de transformer des données arborescentes en un modèle de graphe pouvant utiliser une représentation à base de TreeMap. Une autre barrière est aussi la multitude de transformations possible des données brutes. Il faut pouvoir explorer cet ensemble de combinaisons. Basé sur nos retours d'expériences avec des utilisateurs finaux, dans cet article, nous considérons que le format brut est sous forme tabulaire. Ce format est le plus couramment utilisé et est facilement accessible par nos utilisateurs. Nous proposons une méthode novatrice permettant de générer automatiquement des graphes valués à partir de n'importe quelle table. En analysant le contenu de chaque dimension nous identifions les interconnexions entre celles-ci. Puis nous caractérisons les entités, les attributs et les relations possibles au sein des tables. Finalement, nous intégrons l'utilisateur dans le processus de transformation en lui proposant un ensemble de transformations valides.	David Auber, Frédéric Gilbert	http://editions-rnti.fr/render_pdf.php?p1&p=1001006	http://editions-rnti.fr/render_pdf.php?p=1001006	494	fr	fr	@labri.fr, @labri.fr	import automatique et interactif de donnée dans le système de visualisations  le premier étape du processus de visualisation d' information consister à transformer le donnée d' un format brut vers un structure de donnée utilisable par le différent composant de visualisation . Dans le application réel , ce premier étape représenter un barrière empêcher le accès des utilisateur novice à un riche variété de technique de visualisation . Par exemple , il pouvoir être techniquement impossible pour un utilisateur lambda de transformer un donnée arborescent en un modèle de graphe pouvoir utiliser un représentation à base de TreeMap . un autre barrière être aussi le multitude de transformation possible des donnée brut . Il faillir pouvoir explorer ce ensemble de combinaison . baser sur son retour d' expérience avec un utilisateur final , dans ce article , nous considérer que le format brut être sous forme tabulaire . ce format être le plus couramment utiliser et être facilement accessible par son utilisateur . Nous proposer un méthode novateur permettre de générer automatiquement un graphe valués à partir de n' importer quel table . En analyser le contenu de chaque dimension nous identifier le interconnexion entre celui _-ci . Puis nous caractériser le entité , le attribut et le relation possible au sein des table . finalement , nous intégrer le utilisateur dans le processus de transformation en lui proposer un ensemble de transformation valide . 	Import automatique et interactif de données dans les systèmes de visualisations	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Intégration de données haptiques brutes dans des systèmes experts de diagnostic des connaissances	Cet article a pour cadre un environnement informatique pour l'apprentissage humain (EIAH) dédié à la chirurgie orthopédique, et plus précisément sur le diagnostic des connaissances des apprenants. Pour ce faire, un réseau bayésien infère à partir d'exercices que les étudiants réalisent sur un simulateur avec bras articulé. Ce réseau résulte d'une approche centrée expert du domaine, comme très souvent dans les EIAH. Pourtant, dans un domaine comme la chirurgie où les connaissances sont tacites, le geste de l'apprenant semble intéressant à considérer. Le but de nos travaux est donc d'adopter une démarche plus centrée sur les données en incorporant au réseau bayésien les données haptiques continues issues du simulateur. Divers problèmes se posent néanmoins, d'une part sur le besoin d'étudier la nature des données pour conserver la généricité du système, et d'autre part pour trouver des méthodes de validation pertinentes concernant leur traitement	Sébastien Lallé, Vanda Luengo	http://editions-rnti.fr/render_pdf.php?p1&p=1001026	http://editions-rnti.fr/render_pdf.php?p=1001026	495	fr	fr	@imag.fr	intégration de donnée haptiques brut dans un système expert de diagnostic des connaissances  ce article avoir pour cadre un environnement informatique pour le apprentissage humain ( EIAH ) dédier à le chirurgie orthopédique , et plus précisément sur le diagnostic des connaissance des apprenant . Pour ce faire , un réseau bayésien inférer à partir d' exercice que le étudiant réaliser sur un simulateur avec bras articuler . ce réseau résulter d' un approche centrer expert du domaine , comme très souvent dans le EIAH . pourtant , dans un domaine comme le chirurgie où le connaissance être tacite , le geste de le apprenant sembler intéressant à considérer . le but de son travail être donc d' adopter un démarche plus centrer sur le donnée en incorporer au réseau bayésien le donnée haptiques continu issue du simulateur . divers problème clr poser néanmoins , d' un part sur le besoin d' étudier le nature des donnée pour conserver le généricité du système , et d' autre part pour trouver un méthode de validation pertinent concerner son traitement 	Intégration de données haptiques brutes dans des systèmes experts de diagnostic des connaissances	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Interprétation graphique de la courbe ROC		François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1000969	http://editions-rnti.fr/render_pdf.php?p=1000969	496	fr		@unicaen.fr	Interprétation graphique de la courbe ROC 	Interprétation graphique de la courbe ROC	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Interprétation spectrale de la classification relationnelle	Ce papier présente une vue spectrale sur l'approche de l'analyse relationnelle pour la classification des données catégorielles. Il établit d'abord le lien théorique entre l'approche de l'analyse relationnelle et le problème de classification spectrale. En particulier, le problème de classification relationnelle est présenté comme un problème de maximisation de trace, ce problème est donc transformé par la relaxation spectrale en un problème d'optimisation sous contraintes qui peut être résolu par des multiplicateurs de Lagrange, la solution est donnée par un problème de valeurs propres.	Lazhar Labiod, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000997	http://editions-rnti.fr/render_pdf.php?p=1000997	497	fr	fr	@univ-paris13.fr	interprétation spectral de le classification relationnelle  ce papier présenter un vue spectral sur le approche de le analyse relationnel pour le classification des donnée catégoriel . Il établir d' abord le lien théorique entre le approche de le analyse relationnel et le problème de classification spectral . En particulier , le problème de classification relationnel être présenter comme un problème de maximisation de trace , ce problème être donc transformer par le relaxation spectral en un problème d' optimisation sous contrainte qui pouvoir être résoudre par un multiplicateur de Lagrange , le solution être donner par un problème de valeur propre . 	Interprétation spectrale de la classification relationnelle	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Introduction de l'ingénierie ontologique dans la méthodologie de développement des progiciels de gestion des collectivités territoriales		Wilfried Despagne, Thomas Burger	http://editions-rnti.fr/render_pdf.php?p1&p=1000975	http://editions-rnti.fr/render_pdf.php?p=1000975	498	fr		@univ-ubs.fr, @univ-ubs.fr, @mgdis.fr	Introduction de l'ingénierie ontologique dans la méthodologie de développement des progiciels de gestion des collectivités territoriales 	Introduction de l'ingénierie ontologique dans la méthodologie de développement des progiciels de gestion des collectivités territoriales	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	ISICIL : Intégration Sémantique d'Informations à travers des Communautés d'Intelligence en Ligne		Pavel Arapov, Sébastien Comos, Olivier Corby, Nicolas Delaforge, Guillaume Erétéo, Catherine Faron-Zucker, Michel Buffa, Fabien Gandon, Guillaume Husson, Freddy Limpens	http://editions-rnti.fr/render_pdf.php?p1&p=1000960	http://editions-rnti.fr/render_pdf.php?p=1000960	499	fr		@inria.fr	ISICIL : Intégration Sémantique d'Informations à travers des Communautés d'Intelligence en Ligne 	ISICIL : Intégration Sémantique d'Informations à travers des Communautés d'Intelligence en Ligne	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Les moteurs de wikis sémantiques : un état de l'art	Cet article est un état de l'art sur les moteurs de wiki sémantique, en particulier sur leur utilisation des technologies du Web sémantique. Les principales notions liées aux wikis sémantiques sont d'abord présentées. Ensuite, plusieurs projets actifs de moteurs de wiki sont comparés selon différents points de vue. Finalement, des recommandations sont données pour le choix d'un moteur de wiki. En conclusion, les auteurs s'interrogent sur les perspectives des wikis sémantiques telles que la faible interopérabilité de certains moteurs.	Thomas Meilender, Nicolas Jay, Jean Lieber, Fabien Palomares	http://editions-rnti.fr/render_pdf.php?p1&p=1001020	http://editions-rnti.fr/render_pdf.php?p=1001020	500	fr	fr	@a2zi.fr, @loria.fr	le moteur de wiki sémantique : un état de le art  ce article être un état de le art sur le moteur de wiki sémantique , en particulier sur son utilisation des technologie du web sémantique . le principal notion lier aux wiki sémantique être d' abord présenter . ensuite , plusieurs projet actif de moteur de wiki être comparer selon différent point de vue . finalement , un recommandation être donner pour le choix d' un moteur de wiki . En conclusion , le auteur clr interroger sur le perspective des wiki sémantique tel que le faible interopérabilité de certain moteur . 	Les moteurs de wikis sémantiques : un état de l'art	6
Revue des Nouvelles Technologies de l'Information	EGC	2011	M3A : Une plateforme d'ingénierie de maintenance assistée par apprentissage automatique		Abdelkader Benameur, Baghdad Atmani	http://editions-rnti.fr/render_pdf.php?p1&p=1000954	http://editions-rnti.fr/render_pdf.php?p=1000954	501	fr		@yahoo.fr, @univ-oran.dz	M3A : Une plateforme d'ingénierie de maintenance assistée par apprentissage automatique 	M3A : Une plateforme d'ingénierie de maintenance assistée par apprentissage automatique	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mesure de concordance pour les bases de données évidentielles	Dans cet article, nous proposons une mesure de concordance d'une source avec les autres sources. Cette mesure pourra servir à réduire l'importance de ses fonctions de masse avant de les combiner afin de trouver un compromis et donc réduire le conflit. Cette mesure sera illustrée par des données réelles.	Mouna Chebbah, Arnaud Martin, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1000945	http://editions-rnti.fr/render_pdf.php?p=1000945	502	fr	fr	@gnet.tn, @ihec.rnu.tn, @univ-rennes1.fr	mesure de concordance pour le base de donnée évidentielles  Dans ce article , nous proposer un mesure de concordance d' un source avec le autre source . ce mesure pouvoir servir à réduire le importance de son fonction de masse avant de les combiner afin de trouver un compromis et donc réduire le conflit . ce mesure être illustrer par un donnée réel . 	Mesure de concordance pour les bases de données évidentielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mesures d'hétérogénéité sémantique des systèmes P2P non-structurés	L'autonomie des participants dans les systèmes P2P pour le partage de données peut conduire à une situation d'hétérogénéité sémantique dans le cas où les participants utilisent leurs propres ontologies pour représenter leurs données. Dans cet article nous commençons par définir des mesures de disparité entre participants en considérant leurs contextes sémantiques. En considérant la topologie du système et les disparités entre participants, nous proposons des mesures d'hétérogénéité sémantique d'un système P2P non-structuré.	Thomas Cerqueus, Sylvie Cazalens, Philippe Lamarre	http://editions-rnti.fr/render_pdf.php?p1&p=1000984	http://editions-rnti.fr/render_pdf.php?p=1000984	503	fr	fr	@univ-nantes.fr	mesure d' hétérogénéité sémantique des système P2P non- structurés  le autonomie des participant dans le système P2P pour le partage de donnée pouvoir conduire à un situation d' hétérogénéité sémantique dans le cas où le participant utiliser son propre ontologie pour représenter son donnée . Dans ce article nous commencer par définir un mesure de disparité entre participant en considérer son contexte sémantique . En considérer le topologie du système et le disparité entre participant , nous proposer un mesure d' hétérogénéité sémantique d' un système P2P non- structurer . 	Mesures d'hétérogénéité sémantique des systèmes P2P non-structurés	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mixer les moyens pour extraire les gloses	Nous proposons d'extraire des connaissances lexicales en exploitant les « gloses » de mot, ces descriptions spontanées de sens, repérables par des marqueurs lexicaux et des configurations morpho-syntaxiques spécifiques. Ainsi dans l'extrait suivant, le mot testing est suivi d'une glose en c'est-à dire : « 10 % de ces embauches vont porter sur un métier qui monte : le «testing», c'est-à-dire la maîtrise des méthodologies rigoureuses de test des logiciels». Cette approche ouvre des perspectives pour l'acquisition lexicale et terminologique, fondamentale pour de nombreuses tâches. Dans cet article, nous comparons deux façons d'extraire les unités en relation de glose : patrons et statistiques d'associations d'unités sur le web, en les évaluant sur des données réelles.	Augusta Mela, Mathieu Roche, Mohamed el Amine Bekhtaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000935	http://editions-rnti.fr/render_pdf.php?p=1000935	504	fr	fr	@univ-montp3.fr, @lirmm.fr, @gmail.com	mixer le moyen pour extraire le gloses  Nous proposer d' extraire un connaissance lexical en exploiter le « glose » de mot , ce description spontané de sens , repérable par un marqueur lexical et des configuration morpho- syntaxique spécifique . ainsi dans l' extraire suivre , le mot testing être suivre d' un glose en c' est-à dire : « 10 \% de ce embauche aller porter sur un métier qui monter : le « testing » , c' est-à-dire le maîtrise des méthodologie rigoureux de test des logiciel » . ce approche ouvrer un perspective pour le acquisition lexical et terminologique , fondamental pour un nombreux tâche . Dans ce article , nous comparer deux façon d' extraire le unité en relation de glose : patron et statistique d' association d' unité sur le web , en les évaluer sur un donnée réel . 	Mixer les moyens pour extraire les gloses	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mobility, Data Mining and Privacy: Mining Human Movement Patterns from Trajectory Data	The technologies of mobile communications and ubiquitous computing pervade our society, and wireless networks sense the movement of people and vehicles, generating large volumes of mobility data, such as mobile phone call records and GPS tracks. This is a scenario of great opportunities and risks : on one side, mining this data can produce useful knowledge, supporting sustainable mobility and intelligent transportation systems ; on the other side, individual privacy is at risk, as the mobility data contain sensitive personal information. A new multidisciplinary research area is emerging at this crossroads of mobility, data mining, and privacy. The talk assesses this research frontier from a data mining perspective, and illustrates the results of a European-wide research project called GeoPKDD, Geographic Privacy-Aware Knowledge Discovery and Delivery. GeoPKDD has created an integrated platform named MATLAS for complex analysis of mobility data, which combines spatio-temporal querying capabilities with data mining, visual analytics and semantic technologies, thus providing a full support for the Mobility Knowledge Discovery process. In this talk, we focus on the key data mining models : trajectory patterns and trajectory clustering, and illustrate the analytical power of our system in unvealing the complexity of urban mobility in a large metropolitan area by means of a large scale experiment, based on a massive real life GPS dataset, obtained from 17,000 vehicles with on-board GPS receivers, tracked during one week of ordinary mobile activity in the urban area of the city of Milan, Italy.	Fosca Giannotti	http://editions-rnti.fr/render_pdf.php?p1&p=1000920	http://editions-rnti.fr/render_pdf.php?p=1000920	505	en	en	@isti.cnr.it	mobility datum mining privacy mining human movement pattern trajectory datum technology mobile communication ubiquitous compute pervade society wireless network sense movement person vehicle generate large volume mobility data mobile phone call record gps track scenario great opportunity risk one side mining data produce useful knowledge support sustainable mobility intelligent transportation system side individual privacy risk mobility data contain sensitive personal information new multidisciplinary research area emerge crossroad mobility datum mining privacy talk assess research frontier data mining perspective illustrate result european wide research project call geopkdd geographic privacy aware knowledge discovery delivery geopkdd create integrate platform name matla complex analysis mobility data combine spatio temporal query capability datum mining visual analytic semantic technology thus provide full support mobility knowledge discovery process talk focus key datum mining model trajectory pattern trajectory cluster illustrate analytical power system unveal complexity urban mobility large metropolitan area mean large scale experiment base massive real life gps dataset obtain 17 000 vehicle on board gps receiver track one week ordinary mobile activity urban area city milan italy	Mobility, Data Mining and Privacy: Mining Human Movement Patterns from Trajectory Data	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modèle pour une analyse du phénomène de linéarité de catégories sémantiques dans les énoncés en français		Bernard Decobert	http://editions-rnti.fr/render_pdf.php?p1&p=1000982	http://editions-rnti.fr/render_pdf.php?p=1000982	506	fr		@wanadoo.fr	Modèle pour une analyse du phénomène de linéarité de catégories sémantiques dans les énoncés en français 	Modèle pour une analyse du phénomène de linéarité de catégories sémantiques dans les énoncés en français	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation de la dynamique de phénomènes spatio-temporels par des séquences de motifs	Dans ce papier, nous proposons un nouveau cadre théorique permettant de modéliser la dynamique de phénomènes spatio-temporels. Nous définissons le concept de séquences spatio-temporelles de motifs afin de capturer les interactions entre des ensembles de propriétés et un phénomène à observer. Un algorithme incrémental est proposé pour extraire des séquences spatiotemporelles de motifs sous contraintes, et une nouvelle structure de données est mise en place afin d'améliorer ses performances. Un prototype a été développé et testé sur des données réelles.	Loïc Mabit, Nazha Selmaoui-Folcher, Frédéric Flouvat	http://editions-rnti.fr/render_pdf.php?p1&p=1001001	http://editions-rnti.fr/render_pdf.php?p=1001001	507	fr	fr	@univ-nc.nc	modélisation de le dynamique de phénomène spatio- temporel par un séquence de motifs  Dans ce papier , nous proposer un nouveau cadre théorique permettre de modéliser le dynamique de phénomène spatio- temporel . Nous définir le concept de séquence spatio- temporel de motif afin de capturer le interaction entre un ensemble de propriété et un phénomène à observer . un algorithme incrémental être proposer pour extraire un séquence spatiotemporelles de motif sous contrainte , et un nouveau structure de donnée être mettre en place afin d' améliorer son performance . un prototype avoir être développer et tester sur un donnée réel . 	Modélisation de la dynamique de phénomènes spatio-temporels par des séquences de motifs	4
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation de la propagation de l'information sur le Web : de l'extraction des données à la simulation	Nous proposons un modèle de la propagation de l'information dans un réseau, en détaillant toutes les étapes de sa réalisation et de son utilisation dans un cadre de simulation. A partir de données réelles extraites du Web, nous identifions parmi les sources des catégories de comportements de publication distincts. Nous proposons ensuite une extension d'un modèle de diffusion de l'information existant, afin d'augmenter son pouvoir d'expression, en particulier pour reproduire ces comportements de publication, puis nous le validons sur un exemple de simulation.	François Nel, Marie-Jeanne Lesot, Philippe Capet, Thomas Delavallade	http://editions-rnti.fr/render_pdf.php?p1&p=1001023	http://editions-rnti.fr/render_pdf.php?p=1001023	508	fr	fr	@lip6.fr, @thalesgroup.com	modélisation de le propagation de le information sur le Web : de le extraction des donnée à le simulation  Nous proposer un modèle de le propagation de le information dans un réseau , en détailler tout le étape de son réalisation et de son utilisation dans un cadre de simulation . A partir de donnée réel extraire du Web , nous identifier parmi le source des catégorie de comportement de publication distinct . Nous proposer ensuite un extension d' un modèle de diffusion de le information existant , afin d' augmenter son pouvoir d' expression , en particulier pour reproduire ce comportement de publication , puis nous le validons sur un exemple de simulation . 	Modélisation de la propagation de l'information sur le Web : de l'extraction des données à la simulation	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation d'une ressource termino-ontologique de domaine pour l'annotation sémantique de tableaux	Nous proposons dans cet article une modélisation d'une ressource termino-ontologique (RTO) de domaine, guidée par la tâche d'annotation sémantique de tableaux. L'annotation d'un tableau consiste à annoter ses cellules, pour pouvoir ensuite identifier les concepts représentés par ses colonnes et enfin identifier la ou les relations n-aires qu'il représente. La RTO proposée permet d'une part de modéliser dans sa composante lexicale les termes utilisés pour l'annotation des cellules en intégrant la gestion des synonymes et du multilingue, et, d'autre part, de modéliser dans sa composante conceptuelle les concepts symboliques, les concepts numériques et les relations n-aires, qui sont propres au domaine étudié.	Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu, Abir Saïd	http://editions-rnti.fr/render_pdf.php?p1&p=1001021	http://editions-rnti.fr/render_pdf.php?p=1001021	509	fr	fr	@supagro.inra.fr, @risk, @agroparistech.fr	modélisation d' un ressource termino- ontologique de domaine pour le annotation sémantique de tableaux  Nous proposer dans ce article un modélisation d' un ressource termino- ontologique ( RTO ) de domaine , guider par le tâche d' annotation sémantique de tableau . le annotation d' un tableau consister à annoter son cellule , pour pouvoir ensuite identifier le concept représenter par son colonne et enfin identifier le ou le relation n-aires qu' il représenter . le RTO proposer permettre d' un part de modéliser dans son composante lexical le terme utiliser pour le annotation des cellule en intégrer le gestion des synonyme et du multilingue , et , d' autre part , de modéliser dans son composante conceptuel le concept symbolique , le concept numérique et le relation n-aires , qui être propre au domaine étudier . 	Modélisation d'une ressource termino-ontologique de domaine pour l'annotation sémantique de tableaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Moteur de questions réponses à partir de données du web sémantique		Michel Plu	http://editions-rnti.fr/render_pdf.php?p1&p=1000958	http://editions-rnti.fr/render_pdf.php?p=1000958	510	fr		@orange-ftgroup.com	Moteur de questions réponses à partir de données du web sémantique 	Moteur de questions réponses à partir de données du web sémantique	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Moteur de questions-réponses d'une base de connaissances	Cet article présente comment la gestion et l'exploitation de connaissances issues du site web Wikipedia ont permis de développer une telle fonction qui a été intégrée depuis février 2010 dans un moteur de recherche internet français pour le grand public. Aujourd'hui cette fonction est capable de répondre à des questions formulées en langage naturelle sur environs 170 000 lieux ou personnes. La formalisation des données extraites de wikipedia en connaissances au format OWL ou RDFS a permis de déduire de nouvelles informations manquantes, de typer les entités nommées trouvées et de traiter de nouvelles formes de questions qui étaient non traitées.	Michel Plu, Johannes Heinecke	http://editions-rnti.fr/render_pdf.php?p1&p=1001024	http://editions-rnti.fr/render_pdf.php?p=1001024	511	fr	fr	@orange-ftgroup.com	moteur de questions-réponses d' un base de connaissances  ce article présent comment le gestion et le exploitation de connaissance issu du site web Wikipedia avoir permettre de développer un tel fonction qui avoir être intégrer depuis février 2010 dans un moteur de recherche internet français pour le grand public . Aujourd' hui ce fonction être capable de répondre à un question formuler en langage naturel sur environs 170_000 lieu ou personne . le formalisation des donnée extraire de wikipedia en connaissance au format OWL ou RDFS avoir permettre de déduire un nouveau information manquant , de typer le entité nommer trouver et de traiter un nouveau forme de question qui étayer non traiter . 	Moteur de questions-réponses d'une base de connaissances	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Motifs Séquentiels delta-Libres	Bien que largement étudiée, l'extraction de motifs séquentiels reste une tâche très difficile et pose aussi le défi du grand nombre de motifs produits. Dans cet article, nous proposons une nouvelle approche extrayant les motifs séquentiels les plus généraux à fréquence similaire. Nous montrons en quoi l'extension de cette notion, déjà connue pour les motifs ensemblistes, est un problème particulièrement difficile pour les séquences. Les motifs delta-libres ainsi produits sont en nombre réduit et facilitent les usages d'un processus de fouille et nous montrons leur apport comme descripteurs dans un contexte de classification de séquences.	Marc Plantevit, Chedy Raïssi, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1000926	http://editions-rnti.fr/render_pdf.php?p=1000926	512	fr	fr	@liris.cnrs.fr, @loria.fr, @unicaen.fr	motif Séquentiels delta-Libres  bien que largement étudier , le extraction de motif séquentiel rester un tâche très difficile et poser aussi le défi du grand nombre de motif produit . Dans ce article , nous proposer un nouveau approche extraire le motif séquentiel le plus général à fréquence similaire . Nous montrer en quoi le extension de ce notion , déjà connaître pour le motif ensembliste , être un problème particulièrement difficile pour le séquence . le motif delta-libres ainsi produire être en nombre réduire et faciliter le usage d' un processus de fouille et nous montrer son apport comme descripteur dans un contexte de classification de séquence . 	Motifs Séquentiels delta-Libres	
Revue des Nouvelles Technologies de l'Information	EGC	2011	MuMIe: Une Approche Automatique pour l'Interopérabilité des Métadonnées	Avec l'explosion du multimedia, l'utilisation des métadonnées est devenue cruciale pour assurer une bonne gestion des contenus. Cependant, il est nécessaire d assurer un accès uniforme aux métadonnées. Plusieurs techniques ont ainsi été développées afin de réaliser cette interopérabilité. La plupart d'entre elles sont spécifiques à un seul langage de description. Les systèmes de matching existants présentent certaines limites, en particulier dans le traitement des informations structurelles. Nous présentons dans cet article un nouveau système d'intégration qui supporte des schémas provenant de langages descriptifs différents. De plus, la méthode de matching proposée a recours à plusieurs types d'information de façon à augmenter la précision de matching	Samir Amir, Ioan Marius Bilasco, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000985	http://editions-rnti.fr/render_pdf.php?p=1000985	513	fr	fr	@lifl.fr	MuMIe : un approche automatique pour le interopérabilité des Métadonnées  Avec le explosion du multimedia , le utilisation des métadonnées être devenir crucial pour assurer un bon gestion des contenu . cependant , il être nécessaire d assurer un accès uniforme aux métadonnées . plusieurs technique avoir ainsi être développer afin de réaliser ce interopérabilité . le plupart d' entre elles être spécifique à un seul langage de description . le système de matching existant présenter certain limite , en particulier dans le traitement des information structurel . Nous présenter dans ce article un nouveau système d' intégration qui supporter un schéma provenir de langage descriptif différent . De plus , le méthode de matching proposer avoir recours à plusieurs type d' information de façon à augmenter le précision de matching 	MuMIe: Une Approche Automatique pour l'Interopérabilité des Métadonnées	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Nomao : la recherche géolocalisée personnalisée		Laurent Candillier	http://editions-rnti.fr/render_pdf.php?p1&p=1000955	http://editions-rnti.fr/render_pdf.php?p=1000955	514	fr		@nomao.com	Nomao : la recherche géolocalisée personnalisée 	Nomao : la recherche géolocalisée personnalisée	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Nouvelle approche de fouille de graphes AC-réduits fréquents	La fouille de graphes est devenue une piste de recherche intéressante et un défi réel en matière de fouille de données. Parmi les différentes familles de motifs de graphes, les graphes fréquents permettent une caractérisation intéressante des groupes de graphes, ainsi qu'une discrimination des différents graphes lors de la classification ou de la segmentation. A cause de la NP-complétude du test d'isomorphisme de sous-graphes et de l'immensité de l'espace de recherche, les algorithmes de fouille de graphes sont exponentiels en temps d'exécution et/ou occupation mémoire. Dans cet article, nous étudions un nouvel opérateur de projection polynomial nommé AC-projection basé sur une propriété clé du domaine de la programmation par contraintes, à savoir l'arc consistance. Cet opérateur est censé remplacer l'utilisation de l'isomorphisme de sous-graphes en établissant un biais sur la projection. Cette étude est suivie d'une évaluation expérimentale du pouvoir discriminant des patterns AC-réduits découverts.	Brahim Douar, Michel Liquiere, Cherif Chiraz Latiri, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001004	http://editions-rnti.fr/render_pdf.php?p=1001004	515	fr	fr	@lirmm.fr, @gnet.tn, @fst.rnu.tn	nouveau approche de fouille de graphe AC-réduits fréquents  le fouille de graphe être devenir un piste de recherche intéressant et un défi réel en matière de fouille de donnée . Parmi le différent famille de motif de graphe , le graphe fréquent permettre un caractérisation intéressant des groupe de graphe , ainsi qu' un discrimination des différent graphe lors de le classification ou de le segmentation . A cause de le NP-complétude du test d' isomorphisme de sous-graphes et de le immensité de le espace de recherche , le algorithme de fouille de graphe être exponentiel en temps d' exécution et occupation mémoire . Dans ce article , nous étudier un nouveau opérateur de projection polynomial nommer AC-projection baser sur un propriété clé du domaine de le programmation par contrainte , à savoir le arc consistance . ce opérateur être censé remplacer le utilisation de le isomorphisme de sous-graphes en établir un biais sur le projection . ce étude être suivre d' un évaluation expérimental du pouvoir discriminer un pattern AC-réduits découvert . 	Nouvelle approche de fouille de graphes AC-réduits fréquents	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Optimisation de l'extraction de l'alignement des ontologies avec la contrainte de différence	Dans ce papier, nous proposons une approche basée sur la programmation par contraintes pour aborder efficacement le problème de l'alignement des ontologies, et plus particulièrement l'extraction des correspondances à partir des mesures de similarités. La complexité de ce problème est accentuée dans les applications à caractère dynamique où l'aspect performance est capital. Plus précisément, nous exploitons la contrainte globale de différence développée dans le domaine de la programmation par contraintes pour extraire un alignement total et injectif. Nous montrons que cette approche est efficace et se prête à une mise en oeuvre à la fois interactive et automatique.	Moussa Benaissa, Yahia Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000991	http://editions-rnti.fr/render_pdf.php?p=1000991	516	fr	fr	@yahoo.fr, @yahoo.fr	optimisation de le extraction de le alignement des ontologie avec le contrainte de différence  Dans ce papier , nous proposer un approche baser sur le programmation par contrainte pour aborder efficacement le problème de le alignement des ontologie , et plus particulièrement le extraction des correspondance à partir un mesure de similarité . le complexité de ce problème être accentuer dans le application à caractère dynamique où le aspect performance être capital . plus précisément , nous exploiter le contrainte global de différence développer dans le domaine de le programmation par contrainte pour extraire un alignement total et injectif . Nous montrer que ce approche être efficace et clr prêter à un mise en oeuvre à le foi interactif et automatique . 	Optimisation de l'extraction de l'alignement des ontologies avec la contrainte de différence	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Optimisation directe des poids de modèles dans un prédicteur Bayésien naïf moyenné	Le classifieur Bayésien naïf est un outil de classification efficace en pratique pour de nombreux problèmes réels, en dépit de l'hypothèse restrictive d'indépendance des variables conditionnellement à la classe. Récemment, de nouvelles méthodes permettant d'améliorer la performance de ce classifieur ont vu le jour, sur la base à la fois de sélection de variables et de moyennage de modèles. Dans cet article, nous proposons une extension de la sélection de variables pour le classifieur Bayésien naïf, en considérant un modèle de pondération des variables utilisées et des algorithmes d'optimisation directe de ces poids. Les expérimentations confirment la pertinence de notre approche, en permettant une diminution significative du nombre de variables utilisées, sans perte de performance prédictive.	Marc Boullé, Romain Guigourès	http://editions-rnti.fr/render_pdf.php?p1&p=1000932	http://editions-rnti.fr/render_pdf.php?p=1000932	517	fr	fr	@orange-ftgroup.com	optimisation direct des poids de modèle dans un prédicteur Bayésien naïf moyenné  le classifieur Bayésien naïf être un outil de classification efficace en pratique pour un nombreux problème réel , en dépit de le hypothèse restrictif d' indépendance des variable conditionnellement à le classe . récemment , un nouveau méthode permettre d' améliorer le performance de ce classifieur avoir voir le jour , sur le base à le foi de sélection de variable et de moyennage de modèle . Dans ce article , nous proposer un extension de le sélection de variable pour le classifieur Bayésien naïf , en considérer un modèle de pondération des variable utiliser et des algorithme d' optimisation direct de ce poids . le expérimentation confirmer le pertinence de son approche , en permettre un diminution significatif du nombre de variable utiliser , sans perte de performance prédictif . 	Optimisation directe des poids de modèles dans un prédicteur Bayésien naïf moyenné	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Parameter-free association rule mining with yacaree		José L Balcazar 	http://editions-rnti.fr/render_pdf.php?p1&p=1000952	http://editions-rnti.fr/render_pdf.php?p=1000952	518	en		@unican.es	Parameter-free association rule mining with yacaree 	Parameter-free association rule mining with yacaree	7
Revue des Nouvelles Technologies de l'Information	EGC	2011	Point of View Based Clustering of Socio-Semantic Networks		Juan David Cruz, Cécile Bothorel, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000973	http://editions-rnti.fr/render_pdf.php?p=1000973	519	en		@telecom-bretagne.eu, @irisa.fr	Point of View Based Clustering of Socio-Semantic Networks 	Point of View Based Clustering of Socio-Semantic Networks	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Pondération et classification simultanée de données binaires et continues	Dans cet article, nous proposons une nouvelle approche de classification topologique et de pondération des variables mixtes (qualitatives et quantitatives codées en binaire) durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage est combiné à un mécanisme de pondération des différentes variables sous forme de poids d'influence sur la pertinence des variables. L'apprentissage des pondérations et des prototypes est réalisé d'une manière simultanée en favorisant une classification optimisée des données. L'approche proposée a été validée sur des données qualitatives codées en binaire et plusieurs bases de données mixtes.	Nicoleta Rogovschi, Mustapha Lebbah, Nistor Grozavu	http://editions-rnti.fr/render_pdf.php?p1&p=1000930	http://editions-rnti.fr/render_pdf.php?p=1000930	520	fr	fr	@parisdescartes.fr, @univ-paris13.fr	pondération et classification simultané de donnée binaire et continues  Dans ce article , nous proposer un nouveau approche de classification topologique et de pondération des variable mixte ( qualitatif et quantitatif coder en binaire ) durant un processus d' apprentissage non superviser . ce approche être baser sur le modèle des carte auto- organisatrice . le apprentissage être combiner à un mécanisme de pondération des différent variable sous forme de poids d' influence sur le pertinence des variable . le apprentissage des pondération et des prototype être réaliser d' un manière simultané en favoriser un classification optimiser des donnée . le approche proposer avoir être valider sur un donnée qualitatif coder en binaire et plusieurs base de donnée mixte . 	Pondération et classification simultanée de données binaires et continues	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	PPMI : étude formelle d'une variante à valeurs positives de la PMI		Mohamed Nadif, François Role	http://editions-rnti.fr/render_pdf.php?p1&p=1000965	http://editions-rnti.fr/render_pdf.php?p=1000965	521	fr		@univ-paris5.fr, @parisdescartes.fr	PPMI : étude formelle d'une variante à valeurs positives de la PMI 	PPMI : étude formelle d'une variante à valeurs positives de la PMI	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Prévision de trajectoires de cyclones à l'aide de forêts aléatoires avec arbres de régression	Nous présentons une étude pour la prédiction des trajectoires de cyclones dans l'océan Atlantique Nord à partir de données issues d'images satellites. On y extrait des mesures de vitesses de vent, de vorticité, d'humidité (base JRA-25) et des mesures de latitude, de longitude et de vitesse de vent instantanée des cyclones toutes les 6 heures (base IBTrACS). Les modèles de référence à ce jour ne tiennent pas compte des corrélations entre les données et les prévisions ce qui limite leur intérêt pour certains utilisateurs. Nous proposons ainsi de prédire le déplacement en latitude et le déplacement en longitude au même instant à un horizon de 120 h toutes les 6 h à l'aide de forêts aléatoires avec arbres de régression. Sur le long terme, à partir de 18 h, la méthode proposée donne de meilleurs résultats que les méthodes existantes.	Sterenn Liberge, Sileye O. Ba, Philippe Lenca, Ronan Fablet	http://editions-rnti.fr/render_pdf.php?p1&p=1001031	http://editions-rnti.fr/render_pdf.php?p=1001031	522	fr	fr	@telecom-bretagne.eu	prévision de trajectoire de cyclone à le aide de forêt aléatoire avec arbre de régression  Nous présenter un étude pour le prédiction des trajectoire de cyclone dans le océan Atlantique Nord à partir de donnée issu d' image satellite . On y extraire un mesure de vitesse de vent , de vorticité , d' humidité ( base JRA-25 ) et un mesure de latitude , de longitude et de vitesse de vent instantané des cyclone tout le 6 heure ( base IBTrACS ) . le modèle de référence à ce jour ne tenir pas compter un corrélation entre le donnée et le prévision ce qui limiter son intérêt pour certain utilisateur . Nous proposer ainsi de prédire le déplacement en latitude et le déplacement en longitude au même instant à un horizon de 120 heure tout le 6 heure à le aide de forêt aléatoire avec arbre de régression . Sur le long terme , à partir de 18 heure , le méthode proposer donne de meilleur résultat que le méthode existant . 	Prévision de trajectoires de cyclones à l'aide de forêts aléatoires avec arbres de régression	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Prise en compte du réseau de sources pour la fusion d'informations		Thomas Bärecke, Marie-Jeanne Lesot, Herman Akdag, Bernadette Bouchon-Meunier	http://editions-rnti.fr/render_pdf.php?p1&p=1000980	http://editions-rnti.fr/render_pdf.php?p=1000980	523	fr		@lip6.fr	Prise en compte du réseau de sources pour la fusion d'informations 	Prise en compte du réseau de sources pour la fusion d'informations	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Propositionaliser des attributs numériques sans les discrétiser, ni les agréger	La fouille de données relationnelles considère des données contenues dans au moins deux tables reliées par une association un-à-plusieurs, par exemple des clients et leurs achats, ou des molécules et leurs atomes. Une façon de fouiller ces données consiste à transformer les données en une seule table attribut-valeur. Cette transformation est appelée propositionalisation. Les approches existantes gèrent principalement les attributs catégoriels. Une première solution est donc de discrétiser les attributs numériques pour les transformer en attributs catégoriels. Les approches alternatives, qui gèrent les attributs numériques, consistent à les agréger. Nous proposons une approche duale de la discrétisation, qui inverse l'ordre de traitement du nombre d'objets et du seuil, et dont la discrétisation généralise les quartiles. Nous pouvons ainsi construire des attributs que les approches existantes de propositionalisation ne peuvent pas construire, et qui ne peuvent pas non plus être obtenus par les systèmes complets de fouille de données.	Agnès Braud, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1000998	http://editions-rnti.fr/render_pdf.php?p=1000998	524	fr	fr	@unistra.fr	Propositionaliser un attribut numérique sans le discrétiser , ni les agréger  le fouille de donnée relationnel considérer un donnée contenir dans au moins deux table relier par un association un-à-plusieurs , par exemple des client et son achat , ou un molécule et son atome . un façon de fouiller ce donnée consister à transformer le donnée en un seul table attribut-valeur . ce transformation être appeler propositionalisation . le approche existant gérer principalement le attribut catégoriel . un premier solution être donc de discrétiser le attribut numérique pour les transformer en attribut catégoriel . le approche alternatif , qui gérer le attribut numérique , consister à les agréger . Nous proposer un approche dual de le discrétisation , qui inverser le ordre de traitement du nombre d' objet et du seuil , et dont le discrétisation généraliser le quartile . Nous pouvoir ainsi construire un attribut que le approche existant de propositionalisation ne pouvoir pas construire , et qui ne pouvoir pas non plus être obtenir par le système complet de fouille de donnée . 	Propositionaliser des attributs numériques sans les discrétiser, ni les agréger	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Reasoning about the learning process	Data Mining is faced with new challenges. In emerging applications (like financial data, traffic TCP/IP, sensor networks, etc) data continuously flow eventually at high speed. The processes generating data evolve over time, and the concepts we are learning change. In this talk we present a one-pass classification algorithm able to detect and react to changes. We present a framework that identify contexts using drift detection, characterize contexts using meta-learning, and select the most appropriate base model for the incoming data using unlabeled examples. Evolving data requires that learning algorithms must be able to monitor the learning process and the ability of predictive self-diagnosis. A significant and useful characteristic is diagnostics - not only after failure has occurred, but also predictive (before failure). These aspects require monitoring the evolution of the learning process, taking into account the available resources, and the ability of reasoning and learning about it.	João Gama	http://editions-rnti.fr/render_pdf.php?p1&p=1000921	http://editions-rnti.fr/render_pdf.php?p=1000921	525	en	en	@fep.up.pt	reasoning learn process datum mining face new challenge emerge application like financial data traffic tcp ip sensor network etc data continuously flow eventually high speed process generate datum evolve time concept learn change talk present one pass classification algorithm able detect react change present framework identify context used drift detection characterize context used meta learn select appropriate base model incoming data used unlabeled example evolve datum require learn algorithm must able monitor learn process ability predictive self diagnosis significant useful characteristic diagnostic failure occur also predictive before failure aspect require monitoring evolution learn process take account available resource ability reasoning learn it	Reasoning about the learning process	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Reconnaissance d'Actions par Modélisation du Mouvement	Cet article propose une approche utilisant les modèles de direction et de magnitude de mouvement pour détecter les actions qui sont effectuées par des êtres humains dans des séquences vidéo. Des mélanges Gaussiens et de lois de von Mises sont estimés à partir des orientations et des magnitudes des vecteurs du flux optique calculés pour chaque bloc de la scène. Les paramètres de ces modèles sont estimés grâce à un algorithme d'apprentissage en ligne. Les actions sont reconnues grâce à une mesure qui se base sur la distance de Bhattacharyya et qui permet de comparer le modèle d'une séquence donnée avec les modèles créés à partir de séquences d'apprentissage. L'approche proposée est évaluée sur deux ensembles de vidéos contenant des actions variées exécutées aussi bien dans des environnements intérieur qu'extérieur.	Yassine Benabbas, Adel Lablack, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000940	http://editions-rnti.fr/render_pdf.php?p=1000940	526	fr	fr	@lifl.fr	reconnaissance d' action par modélisation du Mouvement  ce article proposer un approche utiliser le modèle de direction et de magnitude de mouvement pour détecter le action qui être effectuer par un être humain dans un séquence vidéo . un mélange Gaussiens et de loi de von Mises être estimer à partir un orientation et des magnitude des vecteur du flux optique calculer pour chaque bloc de le scène . le paramètre de ce modèle être estimer grâce à un algorithme d' apprentissage en ligne . le action être reconnaître grâce à un mesure qui clr baser sur le distance de Bhattacharyya et qui permettre de comparer le modèle d' un séquence donner avec le modèle créer à partir de séquence d' apprentissage . le approche proposer être évaluer sur deux ensemble de vidéo contenir un action varier exécuter aussi bien dans un environnement intérieur qu' extérieur . 	Reconnaissance d'Actions par Modélisation du Mouvement	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Résumés et interrogations de logs de requêtes OLAP	Une façon d'assister l'analyse d'entrepôt de données repose sur l'exploitation et la fouille de fichiers logs de requêtes OLAP. Mais, à notre connaissance, il n'existe pas de méthode permettant d'obtenir une représentation d'un tel log qui soit à la fois concise et exploitable. Dans ce papier, nous proposons une méthode pour résumer et interroger des logs de requêtes OLAP. L'idée de base est qu'une requête résume une autre requête et qu'un log, qui est une séquence de requêtes, résume un autre log. Notre cadre formel est composé d'une algèbre simple destinée à résumer des requêtes OLAP, et d'une mesure évaluant la qualité du résumé obtenu. Nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité, et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement. Des tests sur des logs de requêtes MDX ont montré l'intérêt de notre approche.	Julien Aligon, Elsa Negre, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000951	http://editions-rnti.fr/render_pdf.php?p=1000951	527	fr	fr	@univ-tours.fr	résumé et interrogation de logs de requête OLAP  un façon d' assister le analyse d' entrepôt de donnée reposer sur le exploitation et le fouille de fichier logs de requête OLAP . Mais , à son connaissance , il n' exister pas un méthode permettre d' obtenir un représentation d' un tel logarithme qui être à le foi concis et exploitable . Dans ce papier , nous proposer un méthode pour résumer et interroger un logs de requête OLAP . le idée de base être qu' un requête résumer un autre requête et qu' un logarithme , qui être un séquence de requête , résumer un autre logarithme . son cadre formel être composer d' un algèbre simple destiner à résumer un requête OLAP , et d' un mesure évaluer le qualité du résumé obtenir . Nous proposer également plusieurs stratégie pour calculer automatiquement un résumé de logs de bon qualité , et nous montrer comment un propriété simple sur le résumé pouvoir être utiliser pour interroger un logarithme efficacement . un test sur un logs de requête MDX avoir montrer le intérêt de son approche . 	Résumés et interrogations de logs de requêtes OLAP	7
Revue des Nouvelles Technologies de l'Information	EGC	2011	Sélection des variables informatives pour l'apprentissage supervisé multi-tables	Dans la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. La plupart des approches existantes opèrent en transformant la représentation multi-tables, notamment par mise à plat. Par conséquent, on perd la représentation initiale naturellement compacte mais également on risque d'introduire des biais statistiques. Notre approche a pour objectif d'évaluer l'informativité des variables explicatives originelles par rapport à la variable cible dans le contexte des relations un-à-plusieurs. Elle consiste à résumer l'information contenue dans chaque variable par un tuple d'attributs représentant les effectifs des modalités de celle-ci. Des modèles en grilles multivariées sont alors employés pour qualifier l'information apportée conjointement par les nouveaux attributs, ce qui revient à une estimation de densité conditionnelle de la variable cible connaissant la variable explicative en relation un-à-plusieurs. Les premières expérimentations sur des bases de données artificielles et réelles montrent qu'on arrive à identifier les variables explicatives potentiellement pertinentes sur tout le domaine relationnel.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000995	http://editions-rnti.fr/render_pdf.php?p=1000995	528	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com, @u-cergy.fr	sélection des variable informatif pour le apprentissage superviser multi-tables  Dans le fouille de donnée multi-tables , le donnée être représenter sous un format relationnel dans lequel le individu de le table cible être potentiellement associer à plusieurs enregistrement dans un table secondaire en relation un-à-plusieurs . le plupart des approche existant opérer en transformer le représentation multi-tables , notamment par mise à plat . Par conséquent , on perdre le représentation initial naturellement compact mais également on risquer d' introduire un biais statistique . son approche avoir pour objectif d' évaluer le informativité des variable explicatives originel par rapport à le variable cible dans le contexte des relation un-à-plusieurs . Elle consister à résumer le information contenir dans chaque variable par un tuple d' attribut représenter le effectif des modalité de celui _-ci . un modèle en grille multivariées être alors employer pour qualifier le information apporter conjointement par le nouveau attribut , ce qui revenir à un estimation de densité conditionnel de le variable cible connaître le variable explicatif en relation un-à-plusieurs . le premier expérimentation sur un base de donnée artificiel et réel montrer qu' on arriver à identifier le variable explicatif potentiellement pertinent sur tout le domaine relationnel . 	Sélection des variables informatives pour l'apprentissage supervisé multi-tables	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Service de recherche Web3.0 de contenus audiovisuels		François Paulus, Jérôme Royan	http://editions-rnti.fr/render_pdf.php?p1&p=1000957	http://editions-rnti.fr/render_pdf.php?p=1000957	529	fr		@semsoft-corp.com, @orange-ftgroup.com	Service de recherche Web3.0 de contenus audiovisuels 	Service de recherche Web3.0 de contenus audiovisuels	
Revue des Nouvelles Technologies de l'Information	EGC	2011	Structuration automatique des flux télévisuels par apprentissage non supervisé des répétitions		Rakia Jaziri, Mustapha Lebbah, Younès Bennani, Jean-Hugues Chenot	http://editions-rnti.fr/render_pdf.php?p1&p=1000974	http://editions-rnti.fr/render_pdf.php?p=1000974	530	fr		@univ-paris13.fr, @ina.fr	Structuration automatique des flux télévisuels par apprentissage non supervisé des répétitions 	Structuration automatique des flux télévisuels par apprentissage non supervisé des répétitions	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Système de recherche de musique adaptable à la perception de chaque utilisateur	Dans le cadre de nos travaux sur le portage linguistique des systèmes de gestion de contenu traitant des énoncés spontanés en langue naturelle, nous présentons ici une évaluation du portage d'IMRS (système de recherche de morceau de musique en langue naturelle) Kumamoto (2007) du japonais vers le français. Cette évaluation peut se faire au niveau des représentations internes en les comparant, ou au niveau de la tâche. Ici, nous nous intéressons à une évaluation liée à la tâche en proposant un service Web qui permet de mesurer la performance globale de la nouvelle version obtenue. Nous avons par la suite cherché à améliorer et ajouter de nouvelles fonctionnalités en proposant un service de recherche de musique adaptable à la perception de chaque utilisateur. En effet, un même morceau de musique peut être jugé calme pour un premier auditeur, très calme pour un deuxième, et assez calme pour un troisième, etc. On se demande l'impression finale que porte ce dernier morceau de musique. C'est naturel que les utilisateurs évaluent différemment un même morceau de musique car ils ont des perceptions différentes. Devant cette situation, nous proposons un service de recherche de musique basé des méthodes simples et automatisées et qui sont adaptables à la perception de chaque utilisateur.	Najeh Hajlaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000962	http://editions-rnti.fr/render_pdf.php?p=1000962	531	fr	fr	@imag.fr	système de recherche de musique adaptable à le perception de chaque utilisateur  Dans le cadre de son travail sur le portage linguistique des système de gestion de contenu traiter un énoncé spontané en langue naturel , nous présenter ici un évaluation du portage d' IMRS ( système de recherche de morceau de musique en langue naturel ) Kumamoto ( 2007 ) du japonais vers le français . ce évaluation pouvoir clr faire au niveau des représentation interne en les comparer , ou au niveau de le tâche . ici , nous clr intéresser à un évaluation lier à le tâche en proposer un service Web qui permettre de mesurer le performance global de le nouveau version obtenir . Nous avoir par le suite chercher à améliorer et ajouter un nouveau fonctionnalité en proposer un service de recherche de musique adaptable à le perception de chaque utilisateur . En effet , un même morceau de musique pouvoir être juger calme pour un premier auditeur , très calme pour un deuxième , et assez calme pour un troisième , etc. On clr demander le impression final que porter ce dernier morceau de musique . C' être naturel que le utilisateur évaluer différemment un même morceau de musique car ils avoir un perception différent . Devant ce situation , nous proposer un service de recherche de musique baser des méthode simple et automatiser et qui être adaptable à le perception de chaque utilisateur . 	Système de recherche de musique adaptable à la perception de chaque utilisateur	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Système pour la catégorisation automatique des offres d'emploi en une typologie de fonctions	Depuis les deux dernières décennies, l'augmentation du nombre de sites d'emploi sur Internet a accentué la nécessité de proposer des outils d'aide à la décision adaptés aux besoins des recruteurs. Cet article présente un système pour la catégorisation des textes d'offres d'emploi destinées à être diffusées sur Internet. Après un pré-traitement adapté des offres, les termes descripteurs sont choisis en fonction de leur pouvoir discriminant vis-à-vis des différentes classes ce qui permet de réduire leur nombre de manière significative. Les offres sont ensuite représentées par leurs coordonnées dans l'espace factoriel obtenu par analyse des correspondances et la classification réalisée dans un cadre supervisé à l'aide de SVM.	Julie Séguéla	http://editions-rnti.fr/render_pdf.php?p1&p=1001009	http://editions-rnti.fr/render_pdf.php?p=1001009	532	fr	fr	@multiposting.fr	système pour le catégorisation automatique des offre d' emploi en un typologie de fonctions  Depuis le deux dernier décennie , le augmentation du nombre de site d' emploi sur Internet avoir accentuer le nécessité de proposer un outil d' aide à le décision adapter aux besoin des recruteur . ce article présenter un système pour le catégorisation des texte d' offre d' emploi destiner à être diffuser sur Internet . Après un pré-traitement adapter des offre , le terme descripteur être choisir en fonction de son pouvoir discriminer vis-à-vis un différent classe ce qui permettre de réduire son nombre de manière significatif . le offre être ensuite représenter par son coordonnée dans le espace factoriel obtenir par analyse des correspondance et le classification réaliser dans un cadre superviser à le aide de SVM . 	Système pour la catégorisation automatique des offres d'emploi en une typologie de fonctions	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Towards a DistributedWeb Search Engine	In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (230 millions at the end of 2009) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability ; in spite of network latency and scattered data. In this talk we present the main challenges behind the design of a distributed Web retrieval system and our research in all the components of a search engine : crawling, indexing, and query processing.	Ricardo Baeza-Yates	http://editions-rnti.fr/render_pdf.php?p1&p=1000919	http://editions-rnti.fr/render_pdf.php?p=1000919	533	en	en	@barcelonamedia.org	towards distributedweb search engine ocean web data web search engine primary way access content data order petabyte current search engine large centralized system base replicate cluster web data however always evolve number web site continue grow rapidly 230 million end 2009 currently 20 billion index page hand internet user one billion hundred million query issue day near future centralized system likely become less effective data query load thus suggest need fully distribute search engine engine need maintain high quality answer fast response time high query throughput high availability scalability spite network latency scattered data talk present main challenge behind design distribute web retrieval system research component search engine crawl indexing query processing	Towards a DistributedWeb Search Engine	
Revue des Nouvelles Technologies de l'Information	EGC	2011	Treillis des concepts SKYLINES : Analyse multidimensionnelle des SKYLINES fondée sur les ensembles en accord	Le concept de SKYLINE a été introduit pour mettre en évidence les objets « les meilleurs » selon différents critères. Une généralisation multidimensionnelle du SKYLINE a été proposée à travers le SKYCUBE qui réunit tous les SKYLINES possibles selon toutes les combinaisons de critères et permet d'analyser les liens entre objets SKYLINES. Comme le data cube, le SKYCUBE s'avère extrêmement volumineux si bien que des approches de réduction sont incontournables. Dans cet article, nous définissons une approche de matérialisation partielle du SKYCUBE. L'idée sous-jacente est d'éliminer de la représentation les Skycuboïdes facilement re-calculables. Pour atteindre cet objectif de réduction, nous caractérisons un cadre formel : le treillis des concepts ACCORDS. Cette structure combine la notion d'ensemble en accord et le treillis des concepts. À partir de cette structure, nous dérivons le treillis des concepts SKYLINES qui en est une instance contrainte. Le point fort de notre approche est d'être orientée attribut ce qui permet de borner le nombre de noeuds du treillis et d'obtenir une navigation efficace à travers les Skycuboïdes.	Sébastien Nedjar, Fabien Pesci, Lotfi Lakhal, Rosine Cicchetti	http://editions-rnti.fr/render_pdf.php?p1&p=1000949	http://editions-rnti.fr/render_pdf.php?p=1000949	534	fr	fr		treillis des concept SKYLINES : analyse multidimensionnel des SKYLINES fonder sur le ensemble en accord  le concept de SKYLINE avoir être introduire pour mettre en évidence le objet « le meilleur » selon différent critère . un généralisation multidimensionnel du SKYLINE avoir être proposer à travers le SKYCUBE qui réunir tout le SKYLINES possible selon tout le combinaison de critère et permettre d' analyser le lien entre objet SKYLINES . Comme le dater cube , le SKYCUBE clr avérer extrêmement volumineux si bien que un approche de réduction être incontournable . Dans ce article , nous définir un approche de matérialisation partiel du SKYCUBE . le idée sous-jacent être d' éliminer de le représentation le Skycuboïdes facilement re-calculables . Pour atteindre ce objectif de réduction , nous caractériser un cadre formel : le treillis des concept ACCORDS . ce structure combiner le notion d' ensemble en accord et le treillis des concept . À partir de ce structure , nous dériver le treillis des concept SKYLINES qui en être un instance contraindre . le point fort de son approche être d' être orienter attribut ce qui permettre de borner le nombre de noeud du treillis et d' obtenir un navigation efficace à travers le Skycuboïdes . 	Treillis des concepts SKYLINES : Analyse multidimensionnelle des SKYLINES fondée sur les ensembles en accord	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un critère Bayésien pour évaluer la robustesse des règles de classification	L'utilisation de règles de classification dans les modèles prédictifs a été très étudiée ces dernières années. La forme simple et interprétable des règles en font des motifs très populaires. Les classifieurs combinant des règles de classification intéressantes (selon une mesure d'intérêt) offrent de bonnes performances de prédictions. Cependant, les performances de ces classifieurs dépendent de la mesure d'intérêt (e.g., confiance, taux d'accroissement, ... ) et du seuillage (non-trivial) de cette mesure pour déterminer les règles pertinentes. De plus, il est facile de montrer que les règles extraites ne sont pas individuellement robustes. Dans cet article, nous proposons un nouveau critère pour évaluer la robustesse des règles de classification dans les données Booléennes. Notre critère est issu d'une approche Bayésienne : nous proposons une expression analytique de la probabilité d'une règle connaissant les données. Ainsi, les règles les plus probables sont robustes. Le critère Bayésien nous permet alors d'identifier (sans paramètre) les règles robustes parmi un ensemble de règles données.	Marc Boullé, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001013	http://editions-rnti.fr/render_pdf.php?p=1001013	535	fr	fr	@orange-ftgroup.com	un critère Bayésien pour évaluer le robustesse des règle de classification  le utilisation de règle de classification dans le modèle prédictif avoir être très étudier ce dernier année . le forme simple et interprétable des règle en faire un motif très populaire . le classifieur combinant des règle de classification intéressant ( selon un mesure d' intérêt ) offrir de bon performance de prédiction . cependant , le performance de ce classifieur dépendre de le mesure d' intérêt ( exempli gratia , confiance , taux d' accroissement , ... ) et du seuillage ( non- trivial ) de ce mesure pour déterminer le règle pertinent . De plus , il être facile de montrer que le règle extraire ne être pas individuellement robuste . Dans ce article , nous proposer un nouveau critère pour évaluer le robustesse des règle de classification dans le donnée Booléennes . son critère être issir d' un approche Bayésienne : nous proposer un expression analytique de le probabilité d' un règle connaître le donnée . ainsi , le règle le plus probable être robuste . le critère Bayésien nous permettre alors d' identifier ( sans paramètre ) le règle robuste parmi un ensemble de règle donner . 	Un critère Bayésien pour évaluer la robustesse des règles de classification	1
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un cycle de vie complet pour l'enrichissement sémantique des folksonomies	Les tags fournis par les utilisateurs des plateformes de tagging social ne sont pas explicitement liés sémantiquement, et ceci limite considérablement les possibilités d'exploitation de ces données. Nous présentons dans cet article notre approche pour l'enrichissement sémantiques des folksonomies qui intègre une combinaison de traitements automatiques ainsi que la capture des contributions de structuration des utilisateurs via une interface ergonomique. De plus, notre modèle supporte les points de vue qui divergent tout en permettant de les combiner en respectant leur cohérence locale. Cette approche s'adresse aux communautés de connaissances collaborant en ligne, et en intégrant leurs usages, nous sommes en mesure de proposer un cycle de vie complet pour le processus de structuration sémantique des folksonomies. La navigation dans les données de tagging est ainsi améliorée, et les folksonomies peuvent alors être directement intégrées dans la construction de thesauri.	Freddy Limpens, Fabien Gandon, Michel Buffa	http://editions-rnti.fr/render_pdf.php?p1&p=1000990	http://editions-rnti.fr/render_pdf.php?p=1000990	536	fr	fr	@inria.fr, @unice.fr	un cycle de vie complet pour le enrichissement sémantique des folksonomies  le tags fournir par le utilisateur des plateforme de tagging social ne être pas explicitement lier sémantiquement , et ceci limiter considérablement le possibilité d' exploitation de ce donnée . Nous présenter dans ce article son approche pour le enrichissement sémantique des folksonomies qui intégrer un combinaison de traitement automatique ainsi que le capture des contribution de structuration des utilisateur via un interface ergonomique . De plus , son modèle supporter le point de vue qui divergent tout en permettre de les combiner en respecter son cohérence local . ce approche clr adresser aux communauté de connaissance collaborer en ligne , et en intégrer son usage , nous sommer en mesure de proposer un cycle de vie complet pour le processus de structuration sémantique des folksonomies . le navigation dans le donnée de tagging être ainsi améliorer , et le folksonomies pouvoir alors être directement intégrer dans le construction de thesauri . 	Un cycle de vie complet pour l'enrichissement sémantique des folksonomies	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un outil de géolocalisation et de résumé automatique pour faciliter l'accès à l'information dans des corpus d'actualité		Emilie Guimier De Neef, Aurélien Bossard, Frédéric Gavignet, Olivier Collin	http://editions-rnti.fr/render_pdf.php?p1&p=1000956	http://editions-rnti.fr/render_pdf.php?p=1000956	537	fr		@orange-ftgroup.com	Un outil de géolocalisation et de résumé automatique pour faciliter l'accès à l'information dans des corpus d'actualité 	Un outil de géolocalisation et de résumé automatique pour faciliter l'accès à l'information dans des corpus d'actualité	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un outil de navigation dans un espace sémantique		Yann Vigile Hoareau, Murat Ahat, David Medernach, Marc Bui	http://editions-rnti.fr/render_pdf.php?p1&p=1000959	http://editions-rnti.fr/render_pdf.php?p=1000959	538	fr		@gmail.com, @etu.epeh.sorbonne.fr, @gmail.com, @ephe.sorbonne.fr	Un outil de navigation dans un espace sémantique 	Un outil de navigation dans un espace sémantique	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un système cellulaire neuro-symbolique pour l'extraction et la gestion des connaissances	Le CNSS - Cellular Neuro-Symbolic System - est un système hybride ralliant conjointement le neuro-symbolique et le cellulaire. CNSS permet, à partir d'une base de cas pratique, de faire coopérer un réseau de neurones, un graphe d'induction et un automate cellulaire pour la construction d'un modèle de prédiction. En détectant et en éliminant les individus non applicables et les variables non pertinentes, le réseau de neurones optimise la base d'apprentissage. Le résultat ainsi obtenu est affiné par un processus d'apprentissage symbolique à base de graphe d'induction. Ce raffinement se fait par une modélisation booléenne qui va assister l'apprentissage symbolique à optimiser le graphe d'induction et va assurer, par la suite, la représentation et la génération des règles de classification sous forme conjonctives avant d'entamer la phase de déduction par un moteur d'inférence cellulaire. CNSS a été testé sur plusieurs applications en utilisant des problèmes académiques et réels. Les résultats montrent que le système CNSS a des performances supérieures et de nombreux avantages.	Baghdad Atmani, Mohamed Benamina, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1000934	http://editions-rnti.fr/render_pdf.php?p=1000934	539	fr	fr	@gmail.com, @gmail.com, @yahoo.fr	un système cellulaire neuro- symbolique pour le extraction et le gestion des connaissances  le CNSS - Cellular Neuro- Symbolic System - être un système hybride rallier conjointement le neuro- symbolique et le cellulaire . CNSS permettre , à partir d' un base de cas pratique , de faire coopérer un réseau de neurone , un graphe d' induction et un automate cellulaire pour le construction d' un modèle de prédiction . En détecter et en éliminer le individu non applicable et le variable non pertinent , le réseau de neurone optimiser le base d' apprentissage . le résultat ainsi obtenir être affiner par un processus d' apprentissage symbolique à base de graphe d' induction . ce raffinement clr faire par un modélisation booléen qui aller assister le apprentissage symbolique à optimiser le graphe d' induction et aller assurer , par le suite , le représentation et le génération des règle de classification sous forme conjonctif avant d' entamer le phase de déduction par un moteur d' inférence cellulaire . CNSS avoir être tester sur plusieurs application en utiliser un problème académique et réel . le résultat montrer que le système CNSS avoir un performance supérieur et un nombreux avantage . 	Un système cellulaire neuro-symbolique pour l'extraction et la gestion des connaissances	
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une Approche à Base de Règles Floues pour les Requêtes à Préférences Contextuelles		Olivier Pivert, Amine Mokhtari, Allel HadjAli	http://editions-rnti.fr/render_pdf.php?p1&p=1000964	http://editions-rnti.fr/render_pdf.php?p=1000964	540	fr		@enssat.fr	Une Approche à Base de Règles Floues pour les Requêtes à Préférences Contextuelles 	Une Approche à Base de Règles Floues pour les Requêtes à Préférences Contextuelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une mesure de distance dans l'espace des alignements entre parties potentiellement homologues de deux ontologies légères	Nous proposons dans cet article une méthode qui calcule la distance entre ontologies dans un but d'aide à la décision sur la pertinence ou non de leur fusion. Cette méthode calcule la distance entre parties homologues de deux ontologies par rapport à leurs niveaux de détail et leurs structures taxonomiques, et ce en exploitant les correspondances produites par un alignement préalablement effectué entre ces ontologies, et en adaptant la méthode de la distance d'édition entre arbres ordonnés. Nous limitons notre étude ici aux ontologies légères, c'est à dire des taxonomies représentées en langages OWL, le langage d'ontologies pour le Web. Notre méthode a été implémentée et testée sur des ontologies réelles, et les résultats obtenus semblent prometteurs.	Ammar Mechouche, Nathalie Abadie, Sébastien Mustière	http://editions-rnti.fr/render_pdf.php?p1&p=1001018	http://editions-rnti.fr/render_pdf.php?p=1001018	541	fr	fr		un mesure de distance dans le espace des alignement entre partie potentiellement homologue de deux ontologie légères  Nous proposer dans ce article un méthode qui calculer le distance entre ontologie dans un but d' aide à le décision sur le pertinence ou non de son fusion . ce méthode calculer le distance entre partie homologue de deux ontologie par rapport à son niveau de détail et son structure taxonomique , et ce en exploiter le correspondance produire par un alignement préalablement effectuer entre ce ontologie , et en adapter le méthode de le distance d' édition entre arbre ordonner . Nous limiter son étude ici aux ontologie léger , c' être à dire un taxonomie représenter en langage OWL , le langage d' ontologie pour le Web . son méthode avoir être implémenter et tester sur un ontologie réel , et le résultat obtenir sembler prometteur . 	Une mesure de distance dans l'espace des alignements entre parties potentiellement homologues de deux ontologies légères	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une méthodologie de recommandations produits fondée sur l'actionnabilité et l'intérêt économique des clients	Dans un contexte économique difficile, la fidélisation des clients figure au premier rang des préoccupations des entreprises. En effet, selon le Gartner, fidéliser des clients existants coûterait beaucoup moins cher que prospecter de nouveaux clients. Pour y parvenir, les entreprises optimisent la marge et le cycle de vie des clients en développant une relation personnalisée aboutissant à de meilleures recommandations. Dans cet article, nous proposons une méthodologie pour les systèmes de recommandations fondée sur l'analyse des chiffres d'affaires des clients sur des familles de produits. Plus précisément, la méthodologie consiste à extraire des comportements de référence sous la forme de règles d'association et à en évaluer l'intérêt économique et l'actionnabilité. Les recommandations sont réalisées en ciblant les contre-exemples les plus actionnables sur les règles les plus rentables. Notre méthodologie est appliquée sur 12 000 clients et 100 000 produits de VMMatériaux afin d'orienter les commerciaux sur les possibilités d'accroissement de la valeur client.	Julien Blanchard, Thomas Piton, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000946	http://editions-rnti.fr/render_pdf.php?p=1000946	542	fr	fr	@univ-nantes.fr, @vm-materiaux.fr	un méthodologie de recommandation produire fonder sur le actionnabilité et le intérêt économique des clients  Dans un contexte économique difficile , le fidélisation des client figurer au premier rang des préoccupation des entreprise . En effet , selon le Gartner , fidéliser un client existant coûter beaucoup moins cher que prospecter un nouveau client . Pour y parvenir , le entreprise optimiser le marge et le cycle de vie des client en développer un relation personnaliser aboutir à un meilleur recommandation . Dans ce article , nous proposer un méthodologie pour le système de recommandation fonder sur le analyse des chiffre d' affaire des client sur un famille de produit . plus précisément , le méthodologie consister à extraire un comportement de référence sous le forme de règle d' association et à en évaluer le intérêt économique et le actionnabilité . le recommandation être réaliser en cibler le contre-exemple le plus actionnable sur le règle le plus rentable . son méthodologie être appliquer sur 12_000 client et 100_000 produit de VMMatériaux afin d' orienter le commercial sur le possibilité d' accroissement de le valeur client . 	Une méthodologie de recommandations produits fondée sur l'actionnabilité et l'intérêt économique des clients	2
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une nouvelle approche pour l'extraction non supervisée de critères	Récemment de nouvelles techniques regroupées sous le vocable de détection automatique d'opinions (opinion mining) ont fait leur apparition et proposent une évaluation globale d'un document. Ainsi, elles ne permettent pas de mettre en avant le fait que les personnes expriment une opinion très positive du scénario d'un film alors qu'elles trouvent que les acteurs sont médiocres. Dans cet article, nous proposons de caractériser automatiquement les segments de textes relevant d'un critère donné sur un corpus de critiques.	Benjamin Duthil, François Trousset, Mathieu Roche, Michel Plantié, Gérard Dray, Jacky Montmain	http://editions-rnti.fr/render_pdf.php?p1&p=1000981	http://editions-rnti.fr/render_pdf.php?p=1000981	543	fr	fr	@mines-ales.fr, @lirmm.fr	un nouveau approche pour le extraction non superviser de critères  récemment de nouveau technique regrouper sous le vocable de détection automatique d' opinion ( opinion mining ) avoir faire son apparition et proposer un évaluation global d' un document . ainsi , elles ne permettre pas de mettre en avant le fait que le personne exprimer un opinion très positif du scénario d' un film alors qu' elles trouver que le acteur être médiocre . Dans ce article , nous proposer de caractériser automatiquement le segment de texte relever d' un critère donner sur un corpus de critique . 	Une nouvelle approche pour l'extraction non supervisée de critères	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une nouvelle approche visuelle pour la classification hiérarchique et topologique	"Nous proposons dans cet article une nouvelle méthode de classification hiérarchique et topologique. Notre approche consiste à construire de manière auto-organisée une partition de données représentées par un ensemble ""forêt"" d'arbres répartis sur une grille 2D. Chaque cellule de la grille est modélisée par un arbre dont les noeuds représentent les données. La partition globale obtenue est visualisée à l'aide d'une carte de TreeMap dans laquelle chaque TreeMap représente un arbre de données. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Des résultats numériques et visuels seront présentés et discutés."	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001038	http://editions-rnti.fr/render_pdf.php?p=1001038	544	fr	fr	@univ-paris13.fr	un nouveau approche visuel pour le classification hiérarchique et topologique  " Nous proposer dans ce article un nouveau méthode de classification hiérarchique et topologique . . son approche consister à construire de manière auto- organiser un partition de donnée représenter par un ensemble " " forêt " " d' arbre répartir sur un grille 2D. chaque cellule de le grille être modéliser par un arbre dont le noeud représenter le donnée . . le partition global obtenir être visualiser à le aide d' un carte de TreeMap dans laquelle chaque TreeMap représenter un arbre de donnée . . Nous évaluer le capacité et le performance de son approche sur un donnée aux difficulté variable . . un résultat numérique et visuel être présenter et discuter . " 	Une nouvelle approche visuelle pour la classification hiérarchique et topologique	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Utilisation d'une ontologie du domaine pour la découverte du contenu de bases de données géographiques	L'essor récent des technologies associées à la géomatique a permis la production rapide de nombreuses données géographiques. Or, pour tirer profit de ces données, il convient de pouvoir évaluer leur pertinence et leur complexité vis à vis de l'application à laquelle on les destine. Dans cet article, nous présentons une application permettant à un utilisateur de découvrir le contenu de bases de données géographiques, à savoir, quels types d'entités géographiques sont représentés au sein de chaque base et comment. Pour accéder à ces informations l'utilisateur interroge le système via une ontologie globale du domaine qui décrit les types d'entités topographiques du monde réel. Des ontologies locales ou d'application sont utilisées pour formaliser les spécifications de chaque base de données décrite. Elles sont annotées à l'aide de concepts issus de l'ontologie globale. Ce système est implémenté sous la forme d'une interface Web et inclut un affichage cartographique d'échantillons de données	Ammar Mechouche, Nathalie Abadie, Emeric Prouteau, Sébastien Mustière	http://editions-rnti.fr/render_pdf.php?p1&p=1000983	http://editions-rnti.fr/render_pdf.php?p=1000983	545	fr	fr		utilisation d' un ontologie du domaine pour le découverte du contenu de base de donnée géographiques  le essor récent des technologie associer à le géomatique avoir permettre le production rapide de nombreux donnée géographique . Or , pour tirer profit de ce donnée , il convenir de pouvoir évaluer son pertinence et son complexité vivre à vis de le application à laquelle on les destiner . Dans ce article , nous présenter un application permettre à un utilisateur de découvrir le contenu de base de donnée géographique , à savoir , quel type d' entité géographique être représenter au sein de chaque base et comment . Pour accéder à ce information le utilisateur interroger le système via un ontologie global du domaine qui décrire le type d' entité topographique du monde réel . un ontologie local ou d' application être utiliser pour formaliser le spécification de chaque base de donnée décrire . Elles être annoter à le aide de concept issu de le ontologie global . ce système être implémenter sous le forme d' un interface Web et inclure un affichage cartographique d' échantillon de données 	Utilisation d'une ontologie du domaine pour la découverte du contenu de bases de données géographiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Utilisation de la Machine Cellulaire pour la Détection des Courriels Indésirables		Fatiha Barigou, Baghdad Atmani, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1000979	http://editions-rnti.fr/render_pdf.php?p=1000979	546	fr		@gmail.com, @univ-oran.dz, @yahoo.fr	Utilisation de la Machine Cellulaire pour la Détection des Courriels Indésirables 	Utilisation de la Machine Cellulaire pour la Détection des Courriels Indésirables	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Utiliser des résultats d'alignement pour enrichir une ontologie	En établissant des relations entre des concepts issus de deux ontologies distinctes, les outils d'alignement peuvent être utilisés pour enrichir une des deux ontologies avec les concepts de l'autre. A partir d'une expérience menée dans le cadre du projet ANR GeOnto 1 dans le domaine de la topographie, cet article identifie des traitements complémentaires à l'alignement pour l'enrichissement et montre leur mise en oeuvre dans TaxoMap Framework.	Fayçal Hamdi, Brigitte Safar, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000992	http://editions-rnti.fr/render_pdf.php?p=1000992	547	fr	fr	@lri.fr	utiliser un résultat d' alignement pour enrichir un ontologie  En établir un relation entre un concept issir de deux ontologie distinct , le outil d' alignement pouvoir être utiliser pour enrichir un des deux ontologie avec le concept de le autre . A partir d' un expérience mener dans le cadre du projet ANR GeOnto 1 dans le domaine de le topographie , ce article identifier un traitement complémentaire à le alignement pour le enrichissement et montrer son mise en oeuvre dans TaxoMap Framework . 	Utiliser des résultats d'alignement pour enrichir une ontologie	3
Revue des Nouvelles Technologies de l'Information	EGC	2011	Vers la fusion d'informations hétérogènes et partielles pour l'aide au codage diagnostique		Laurent Lecornu, Clara Le Guillou, Frédéric Le Saux, Matthieu Hubert, Julien Montagner, John Puentes, Jean-Michel Cauvin	http://editions-rnti.fr/render_pdf.php?p1&p=1000971	http://editions-rnti.fr/render_pdf.php?p=1000971	548	fr		@telecom-bretagne.eu, @chu-brest.fr	Vers la fusion d'informations hétérogènes et partielles pour l'aide au codage diagnostique 	Vers la fusion d'informations hétérogènes et partielles pour l'aide au codage diagnostique	0
Revue des Nouvelles Technologies de l'Information	EGC	2011	Visualisation de l'intra et inter structure des groupes en classification non supervisée	La croissance exponentielle des données engendre des volumétries de bases de données très importantes. Une solution couramment envisagée est l'utilisation d'une description condensée des propriétés et de la structure des données. De ce fait, il devient crucial de disposer d'outils de visualisation capables de représenter la structure des données, non pas à partir des données elles mêmes, mais à partir de ces descriptions condensées. Nous proposons une méthode de description des données à partir de prototypes enrichis puis segmentés à l'aide d'un algorithme adapté de classification non supervisée. Nous introduisons ensuite un procédé de visualisation capable de mettre en valeur la structure intra et inter-groupes des données.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001005	http://editions-rnti.fr/render_pdf.php?p=1001005	549	fr	fr	@univ-paris13.fr	visualisation de le intra et inter structure des groupe en classification non supervisée  le croissance exponentiel des donnée engendrer un volumétrie de base de donnée très important . un solution couramment envisager être le utilisation d' un description condenser des propriété et de le structure des donnée . De ce fait , il devenir crucial de disposer d' outil de visualisation capable de représenter le structure des donnée , non pas à partir un donnée elles même , mais à partir de ce description condenser . Nous proposer un méthode de description des donnée à partir de prototype enrichir puis segmenter à le aide d' un algorithme adapter de classification non superviser . Nous introduire ensuite un procédé de visualisation capable de mettre en valeur le structure intra et inter-groupes des donnée . 	Visualisation de l'intra et inter structure des groupes en classification non supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	"AbsTop-K &#945;: un algorithme d'extraction de paires abstraites hautement corrélées pour mieux recommander dans la ""longue traine"	"De nombreux systèmes de recommandation se focalisent sur les articles(que nous appellerons ""items"") les plus ""populaires"" et ignorent souventla ""longue traîne"" des produits qui le sont moins. Nous proposons l'algorithmeAbsTop-k&#945; qui améliore les recommandations en se basant sur la combinaison(pondérée par &#945;) de paires hautement corrélées entre des abstractions d'items etentre des paires d'items concrets classiquement recherchées."	Minh Thu Tran Nguyen, François Sempé, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001439	http://editions-rnti.fr/render_pdf.php?p=1001439	621	fr	fr		" AbsTop-K α : un algorithme d' extraction de paire abstraire hautement corréler pour mieux recommander dans le " " long traine "  " un nombreux système de recommandation clr focaliser sur le article ( que nous appeler " " item " " ) le plus " " populaire " " et ignorer souventla " " long traîne " " des produit qui le être moins . . Nous proposer le algorithmeAbsTop-kα qui améliorer le recommandation en clr baser sur le combinaison ( pondérer par α ) de paire hautement corréler entre un abstraction d' item etentre des paire d' item concret classiquement rechercher . " 	"AbsTop-K &#945;: un algorithme d'extraction de paires abstraites hautement corrélées pour mieux recommander dans la ""longue traine"	
Revue des Nouvelles Technologies de l'Information	EGC	2010	Action Rules and Meta-actions		Zbigniew W. Ras	http://editions-rnti.fr/render_pdf.php?p1&p=1001261	http://editions-rnti.fr/render_pdf.php?p=1001261	622	en		@uncc.edu	Action Rules and Meta-actions 	Action Rules and Meta-actions	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Affichage de publicités sur des portails web	Nous nous intéressons au problème de l'affichage de publicités surle web. De plus en plus d'annonceurs souhaitent maintenant payer uniquementlorsque quelqu'un clique sur leurs publicités. Dans ce modèle, l'opérateur duportail a intérêt à identifier les publicités les plus cliquées, selon ses catégoriesde visiteurs. Comme les probabilités de clic sont inconnues a priori, il s'agit d'undilemme exploration/exploitation. Ce problème a souvent été traité en ne tenantpas compte de contraintes provenant du monde réel : les campagnes de publicitésont une durée de vie et possèdent un nombre de clics à assurer et ne pas dépasser.Pour cela, nous introduisons une approche hybride (MAB+LP) entre la programmationlinéaire et les bandits. Nos algorithmes sont testés sur des modèles créésavec un important acteur du web commercial. Ces expériences montrent que cesapproches atteignent une performance très proche de l'optimum et mettent enévidence des aspects clés du problème.	Victor Gabillon, Jérémie Mary, Philippe Preux	http://editions-rnti.fr/render_pdf.php?p1&p=1001268	http://editions-rnti.fr/render_pdf.php?p=1001268	623	fr	fr	@inria.fr	affichage de publicité sur un portail web  Nous nous intéresser au problème de le affichage de publicité surle web . De plus en plus d' annonceur souhaiter maintenant payer uniquementlorsque quelque un clique sur son publicité . Dans ce modèle , le opérateur duportail avoir intérêt à identifier le publicité le plus cliquer , selon son catégoriesde visiteur . Comme le probabilité de clic être inconnu avoir priori , il clr agir d' undilemme exploration  exploitation . ce problème avoir souvent être traiter en ne tenantpas compte de contrainte provenir du monde réel : le campagne de publicitésont un durée de vie et posséder un nombre de clic à assurer et ne pas dépasser . Pour cela , nous introduire un approche hybride ( MAB + LP ) entre le programmationlinéaire et le bandit . son algorithme être tester sur un modèle créésavec un important acteur du web commercial . ce expérience montrer que cesapproches atteindre un performance très proche de le optimum et mettre enévidence des aspect clé du problème . 	Affichage de publicités sur des portails web	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Agrégation de systèmes de fermetures: cas des hiérarchies, topologies et géométries convexes		Florent Domenach	http://editions-rnti.fr/render_pdf.php?p1&p=1001469	http://editions-rnti.fr/render_pdf.php?p=1001469	624	fr		@unic.ac.cy	Agrégation de systèmes de fermetures: cas des hiérarchies, topologies et géométries convexes 	Agrégation de systèmes de fermetures: cas des hiérarchies, topologies et géométries convexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Aide à la décision pour la maintenance ferroviaire préventive	La maintenance de trains est un problème particulièrement délicat liéà de nombreux enjeux à la fois financiers, sécuritaires et énergétiques. Nous nousintéressons à la mise en place d'une maintenance préventive basée sur la détectionet la correction de tout comportement anormal susceptible de provoquer unproblème majeur dans un futur proche. Nous proposons ainsi un outil d'aide à ladécision afin de (i) dégager des connaissances utiles sur l'historique des trains,et (ii) détecter et étudier les anomalies comportementales, dans le but de prendredes décisions optimales en termes de maintenance ferroviaire	Julien Rabatel, Sandra Bringay, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001319	http://editions-rnti.fr/render_pdf.php?p=1001319	625	fr	fr	@lirmm.fr	aide à le décision pour le maintenance ferroviaire préventive  le maintenance de train être un problème particulièrement délicat liéà de nombreux enjeu à le foi financier , sécuritaire et énergétique . Nous nousintéressons à le mise en place d' un maintenance préventif baser sur le détectionet le correction de tout comportement anormal susceptible de provoquer unproblème majeur dans un futur proche . Nous proposer ainsi un outil d' aide à ladécision afin de ( i ) dégager un connaissance utile sur le historique des train , et ( ii ) détecter et étudier le anomalie comportemental , dans le but de prendredes décision optimal en terme de maintenance ferroviaire 	Aide à la décision pour la maintenance ferroviaire préventive	8
Revue des Nouvelles Technologies de l'Information	EGC	2010	Allier CSPs et motifs locaux pour la découverte de motifs sous contraintes n-aires	Dans cet article, nous étudions la relation entre la découverte de motifssous contraintes et les CSPs (Constraint Satisfaction Problems) afin de définirdes contraintes de plus haut niveau qui sont précieuses pour mener à bien destâches de fouille de données. Pour cela, nous proposons une approche de modélisationet d'extraction de motifs sous contraintes n-aires exploitant les motifslocaux. L'utilisateur définit un ensemble de contraintes n-aires et un solveur deCSP génère l'ensemble des solutions. Notre approche profite des progrès récentssur l'extraction de motifs locaux et permet de modéliser de manière concise etélégante tout ensemble de contraintes combinant plusieurs motifs locaux, permettantainsi la découverte de motifs répondant mieux aux buts finaux de l'utilisateur.Les expériences menées montrent la faisabilité de notre approche.	Mehdi Khiari, Patrice Boizumault, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1001292	http://editions-rnti.fr/render_pdf.php?p=1001292	626	fr	fr	@unicaen.fr	Allier CSPs et motif local pour le découverte de motif sous contrainte n-aires  Dans ce article , nous étudier le relation entre le découverte de motifssous contrainte et le CSPs ( Constraint Satisfaction Problems ) afin de définirdes contrainte de plus haut niveau qui être précieux pour mener à bien destâches de fouille de donnée . Pour cela , nous proposer un approche de modélisationet d' extraction de motif sous contrainte n-aires exploiter le motifslocaux . le utilisateur définir un ensemble de contrainte n-aires et un solveur deCSP générer le ensemble des solution . son approche profiter un progrès récentssur le extraction de motif local et permettre de modéliser de manière concis etélégante tout ensemble de contrainte combiner plusieurs motif local , permettantainsi le découverte de motif répondre mieux aux but final de le utilisateur . le expérience mener montrer le faisabilité de son approche . 	Allier CSPs et motifs locaux pour la découverte de motifs sous contraintes n-aires	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse de documents pédagogiques en vue de leur annotation	L'utilisation des documents pédagogiques, disponibles sur le web,devient de plus en plus large tant pour l'enseignant qui a besoin de préparerson support de cours que pour l'étudiant qui désire, par exemple, s'autoformer.La description d'un document pédagogique, en l'alimentant par desmétadonnées, s'avère une solution qui confère une valeur ajoutée au documentafin d'expliciter des informations placées dans ce document. Dans cetteoptique, nous proposons une méthode d'annotation de documentspédagogiques selon différents points de vue, qui est basée sur l'analysesémantique des éléments discursifs du texte	Boutheina Smine, Rim Faiz, Jean-Pierre Desclés	http://editions-rnti.fr/render_pdf.php?p1&p=1001502	http://editions-rnti.fr/render_pdf.php?p=1001502	627	fr	fr	@yahoo.fr, @ihec.rnu.tn, @paris4.sorbonne.fr	analyse de document pédagogique en vue de son annotation  le utilisation des document pédagogique , disponible sur le web , devenir de plus en plus large tant pour le enseignant qui avoir besoin de préparerson support de cour que pour le étudiant qui désirer , par exemple , clr autoformer . le description d' un document pédagogique , en l' alimenter par desmétadonnées , clr avérer un solution qui conférer un valeur ajouter au documentafin d' expliciter un information placer dans ce document . Dans cetteoptique , nous proposer un méthode d' annotation de documentspédagogiques selon différent point de vue , qui être baser sur le analysesémantique des élément discursif du texte 	Analyse de documents pédagogiques en vue de leur annotation	5
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse de séquences d'événements avec TraMineR		Nicolas S. Müller, Matthias Studer, Alexis Gabadinho, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1001397	http://editions-rnti.fr/render_pdf.php?p=1001397	628	fr		@unige.ch	Analyse de séquences d'événements avec TraMineR 	Analyse de séquences d'événements avec TraMineR	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse en ligne d'objets complexes avec l'analyse factorielle	Les entrepôts de données et l'analyse en ligne OLAP (On-line AnalysisProcessing) présentent des solutions reconnues et efficaces pour le processusd'aide à la décision. Notamment l'analyse en ligne, grâce aux opérateurs OLAP,permet de naviguer et de visualiser des données représentées dans un cube multidimensionnel.Mais lorsque les données ou les objets à analyser sont complexes,il est nécessaire de redéfinir et d'enrichir ces opérateurs OLAP. Dans cet article,nous proposons de combiner l'analyse OLAP et la fouille de données (data mining)afin de créer un nouvel opérateur de visualisation d'objets complexes. Cetopérateur utilise l'analyse factorielle des correspondances.	Loïc Mabit, Sabine Loudcher, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001321	http://editions-rnti.fr/render_pdf.php?p=1001321	629	fr	fr	@gmail.com, @univ-lyon2.fr	analyse en ligne d' objet complexe avec le analyse factorielle  le entrepôt de donnée et le analyse en ligne OLAP ( On-line AnalysisProcessing ) présenter un solution reconnaître et efficace pour le processusd'aide à le décision . notamment le analyse en ligne , grâce aux opérateur OLAP , permettre de naviguer et de visualiser un donnée représenter dans un cube multidimensionnel . Mais lorsque le donnée ou le objet à analyser être complexe , il être nécessaire de redéfinir et d' enrichir ce opérateur OLAP . Dans ce article , nous proposer de combiner le analyse OLAP et le fouille de donnée ( data mining ) afin de créer un nouveau opérateur de visualisation d' objet complexe . Cetopérateur utiliser le analyse factoriel des correspondance . 	Analyse en ligne d'objets complexes avec l'analyse factorielle	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse globale du flux optique pour la détection d'évènements dans une scène de foule	Les systèmes de vidéo-surveillance sont de plus en plus autonomesdans la détection des événements anormaux. Cet article présente une méthode dedétection des flux majeurs et des évènements qui surviennent dans une scène defoule. Ces détections sont effectuées en utilisant un modèle directionnel construità partir d'un mélange de lois de von Mises appliqué à l'orientation des vecteursde mouvement. Les flux majeurs sont alors calculés en récupérant les orientationsles plus importantes des mélanges. Divers évènements se produisant dansune foule sont aussi détectés en utilisant en plus du modèle d'orientation, unmodèle probabiliste de magnitude des vecteurs de mouvement. Les résultats del'expérimentation sur un échantillon de vidéos d'événements sont présentés.	Yassine Benabbas, Nacim Ihaddadene, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001315	http://editions-rnti.fr/render_pdf.php?p=1001315	630	fr	fr	@lifl.fr	analyse global du flux optique pour le détection d' évènements dans un scène de foule  le système de vidéo- surveillance être de plus en plus autonomesdans le détection des événement anormal . ce article présenter un méthode dedétection des flux majeur et des évènements qui survenir dans un scène defoule . ce détection être effectuer en utiliser un modèle directionnel construità partir d' un mélange de loi de von Mises appliquer à le orientation des vecteursde mouvement . le flux majeur être alors calculer en récupérer le orientationsles plus important des mélange . divers évènements clr produire dansune foule être aussi détecter en utiliser en plus du modèle d' orientation , unmodèle probabiliste de magnitude des vecteur de mouvement . le résultat del'expérimentation sur un échantillon de vidéo d' événement être présenter . 	Analyse globale du flux optique pour la détection d'évènements dans une scène de foule	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse incrémentale des usages pour le routage des requêtes dans les systèmes pairs à pairs		Taoufik Yeferny, Khedija Arour	http://editions-rnti.fr/render_pdf.php?p1&p=1001427	http://editions-rnti.fr/render_pdf.php?p=1001427	631	fr		@gmail.com, @issatm.rnu.tn	Analyse incrémentale des usages pour le routage des requêtes dans les systèmes pairs à pairs 	Analyse incrémentale des usages pour le routage des requêtes dans les systèmes pairs à pairs	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Applying Markov Logic to Document Annotation and Citation Deduplication	Structured learning approaches are able to take into account the relationalstructure of data, thus promising an enhancement over non-relationalapproaches. In this paper we explore two document-related tasks in relationaldomains setting, the annotation of semi-structured documents and the citationdeduplication. For both tasks, we report results of comparing relational learningapproach namely Markov logic, to non-relational one namely Support VectorMachines (SVM). We discover that increased complexity due to the relationalsetting is difficult to manage in large scale cases, where non-relational modelsmight perform better. Moreover, our experiments show that in Markov logic,the contribution of its probabilistic component decreases in large scale domains,and it tends to act like First-order logic (FOL).	Jean Baptiste Faddoul, Boris Chidlovskii	http://editions-rnti.fr/render_pdf.php?p1&p=1001329	http://editions-rnti.fr/render_pdf.php?p=1001329	632	en	en		apply markov logic document annotation citation deduplication structure learn approach able take account relationalstructure datum thus promising enhancement non relationalapproach paper explore two document related task relationaldomain set annotation semi structured document citationdeduplication task report result compare relational learningapproach namely markov logic non relational one namely support vectormachine svm discover increase complexity due relationalset difficult manage large scale case non relational modelsmight perform better moreover experiment show markov logic the contribution probabilistic component decrease large scale domain and tend act like first order logic fol	Applying Markov Logic to Document Annotation and Citation Deduplication	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apport de la technique de fouille de données spatiales dans la prédiction des risques engendrés par les changements climatiques		Hana Alouaoui, Sami Yassine Turki, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001451	http://editions-rnti.fr/render_pdf.php?p=1001451	633	fr		@isteub.rnu.tn, @insat.rnu.tn	Apport de la technique de fouille de données spatiales dans la prédiction des risques engendrés par les changements climatiques 	Apport de la technique de fouille de données spatiales dans la prédiction des risques engendrés par les changements climatiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage de patrons lexico-syntaxiques à partir de textes	Ce papier présente une approche d'apprentissage de patrons lexicosyntaxiquesà partir de textes annotés. Les patrons lexico-syntaxiques sont utiliséspour identifier des relations lexicales dans les corpus textuels. Leur constructionmanuelle est une tâche fastidieuse et des solutions permettant l'apprentissagesont souhaitables. Nous proposons une approche d'apprentissage qui reposesur l'utilisation des chemins de dépendance pour représenter les patrons et l'implémentationd'un algorithme de classification. L'approche a été appliquée dansle domaine biomédical pour identifier des patrons lexico-syntaxiques exprimantdes relations fonctionnelles.	Valentina Dragos, Marie-Christine Jaulent	http://editions-rnti.fr/render_pdf.php?p1&p=1001371	http://editions-rnti.fr/render_pdf.php?p=1001371	634	fr	fr	@upmc.fr, @upmc.fr	apprentissage de patron lexico- syntaxique à partir de textes  ce papier présenter un approche d' apprentissage de patron lexicosyntaxiquesà partir de texte annoter . le patron lexico- syntaxique être utiliséspour identifier un relation lexical dans le corpus textuel . son constructionmanuelle être un tâche fastidieux et un solution permettre le apprentissagesont souhaitable . Nous proposer un approche d' apprentissage qui reposesur le utilisation des chemin de dépendance pour représenter le patron et le implémentationd'un algorithme de classification . le approche avoir être appliquer dansle domaine biomédical pour identifier un patron lexico- syntaxique exprimantdes relation fonctionnel . 	Apprentissage de patrons lexico-syntaxiques à partir de textes	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage de spécifications de CSP		Matthieu Lopez, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001465	http://editions-rnti.fr/render_pdf.php?p=1001465	635	fr		@univ-orleans.fr	Apprentissage de spécifications de CSP 	Apprentissage de spécifications de CSP	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage supervisé adaptatif de Concepts Formels à partir des données nominales		Mondher Maddouri, Nida Meddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001372	http://editions-rnti.fr/render_pdf.php?p=1001372	636	fr		@gmail.com, @fst.rnu.tn	Apprentissage supervisé adaptatif de Concepts Formels à partir des données nominales 	Apprentissage supervisé adaptatif de Concepts Formels à partir des données nominales	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche biomimétique coopérative pour la visualisation de grands graphes multidimensionels	Face à la quantité sans cesse grandissante de données stockées, les algorithmes de fouille etde visualisation de données doivent pouvoir être capable de traiter de grandes quantités de données.Une des solutions est d'effectuer un prétraitement des données permettant la réductionde la dimension des données sans perte significative d'informations. L'idée est donc de réduirel'ensemble de descripteurs avant de faire appel à la méthode de visualisation sous forme d'ungraphe.	Lydia Boudjeloud, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001412	http://editions-rnti.fr/render_pdf.php?p=1001412	637	fr	fr	@univ-metz.fr, @univ-paris13.fr	approche biomimétique coopérative pour le visualisation de grand graphe multidimensionels  face à le quantité sans cesse grandissant de donnée stocker , le algorithme de fouille etde visualisation de donnée devoir pouvoir être capable de traiter un grand quantité de donnée . un des solution être d' effectuer un prétraitement des donnée permettre le réductionde le dimension des donnée sans perte significatif d' information . le idée être donc de réduirel'ensemble de descripteur avant de faire appel à le méthode de visualisation sous forme d' ungraphe . 	Approche biomimétique coopérative pour la visualisation de grands graphes multidimensionels	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche complexe de l'analyse de documents anciens	Cet article présente une méthode complexe pour la caractérisation etl'indexation d'images graphiques de documents anciens. A partir d'un bref étatde l'art, une méthode pour décrire ces images en tenant compte de leur complexitéest proposée. Trois étapes principales de ce traitement sont détailléesdont une méthode novatrice d'analyse, de segmentation et de description destraits. Les résultats sont issus de travaux en cours et sont encourageants	Mickaël Coustaty, Giap NGuyen, Vincent Courboulay, Jean-Marc Ogier	http://editions-rnti.fr/render_pdf.php?p1&p=1001368	http://editions-rnti.fr/render_pdf.php?p=1001368	638	fr	fr	@univ-lr.fr	approche complexe de le analyse de document anciens  ce article présenter un méthode complexe pour le caractérisation etl'indexation d' image graphique de document ancien . A partir d' un bref étatde le art , un méthode pour décrire ce image en tenir compte de son complexitéest proposer . Trois étape principal de ce traitement être détailléesdont un méthode novateur d' analyse , de segmentation et de description destraits . le résultat être issir de travail en cour et être encourageants 	Approche complexe de l'analyse de documents anciens	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche Sémantique pour la Préservation de la Vie Privée dans les Médias Sociaux		Hakim Hacid, Johann Stan, Johann Daigremont	http://editions-rnti.fr/render_pdf.php?p1&p=1001446	http://editions-rnti.fr/render_pdf.php?p=1001446	639	fr		@alcatel-lucent.com	Approche Sémantique pour la Préservation de la Vie Privée dans les Médias Sociaux 	Approche Sémantique pour la Préservation de la Vie Privée dans les Médias Sociaux	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Bien cube, les données textuelles peuvent s'agréger !	La masse des données aujourd'hui disponibles engendre des besoinscroissants de méthodes décisionnelles adaptées aux données traitées. Ainsi, récemmentde nouvelles approches fondées sur des cubes de textes sont apparuespour pouvoir analyser et extraire de la connaissance à partir de documents. L'originalitéde ces cubes est d'étendre les approches traditionnelles des entrepôts etdes technologies OLAP à des contenus textuels. Dans cet article, nous nous intéressonsà deux nouvelles fonctions d'agrégation. La première propose une nouvellemesure de TF-IDF adaptative permettant de tenir compte des hiérarchiesassociées aux dimensions. La seconde est une agrégation dynamique permettantde faire émerger des groupements correspondant à une situation réelle. Lesexpériences menées sur des données issues du serveur HAL d'une universitéconfirment l'intérêt de nos propositions.	Sandra Bringay, Anne Laurent, Pascal Poncelet, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001367	http://editions-rnti.fr/render_pdf.php?p=1001367	640	fr	fr	@lirmm.fr, @cemagref.fr	bien cube , le donnée textuel pouvoir clr agréger !  le masse des donnée aujourd' hui disponible engendrer un besoinscroissants de méthode décisionnel adapter aux donnée traiter . ainsi , récemmentde nouveau approche fonder sur un cube de texte être apparuespour pouvoir analyser et extraire de le connaissance à partir de document . le originalitéde ce cube être d' étendre le approche traditionnel des entrepôt etdes technologie OLAP à un contenu textuel . Dans ce article , nous clr intéressonsà deux nouveau fonction d' agrégation . le premier proposer un nouvellemesure de TF-IDF adaptatif permettre de tenir compte des hiérarchiesassociées aux dimension . le second être un agrégation dynamique permettantde faire émerger un groupement correspondre à un situation réel . Lesexpériences mener sur un donnée issu du serveur HAL d' un universitéconfirment le intérêt de son proposition . 	Bien cube, les données textuelles peuvent s'agréger !	4
Revue des Nouvelles Technologies de l'Information	EGC	2010	Caractériser la terminologie des usagers de santé dans le domaine du cancer du sein	"Internet est devenu une source importante d'informations médicalespour les patients et leurs proches : recherche d'informations sur leurs maladieset les dernières recherches cliniques, ainsi que pour y constituer des communautés""numériques"" de dialogue et de partage. Cependant, accès à Internet nesignifie pas nécessairement accès à l'information. Le manque de familiarité avecle langage médical constitue un problème majeur pour les usagers de santé dansl'accès à l'information et son interprétation. Ce papier s'inscrit dans la problématiqued'étude et de caractérisation de la terminologie des usagers de santépour pouvoir proposer des services adaptés à leur langage et à leur niveau deconnaissances. Le travail réalisé est une ontologie dans le domaine du cancerdu sein orientée vers les usagers de santé. Cette ontologie est construite à partird'un ensemble de corpus de textes représentant deux catégories : les médiateurset les usagers de santé. Les éléments de cette ontologie ont été analysés en utilisantdes méthodes quantitatives et qualitatives sur plusieurs niveaux : termes,concepts et relations."	Radja Messai, Michel Simonet, Nathalie Bricon-Souf, Mireille Mousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001326	http://editions-rnti.fr/render_pdf.php?p=1001326	641	fr	fr	@imag.fr, @univ-lille2.fr, @univ-lille2.fr, @chu-grenoble.fr	caractériser le terminologie des usager de santé dans le domaine du cancer du sein  " Internet être devenir un source important d' information médicalespour le patient et son proche : recherche d' information sur son maladieset le dernier recherche clinique , ainsi que pour y constituer un communauté " " numérique " " de dialogue et de partage . . cependant , accès à Internet nesignifie pas nécessairement accès à le information . . le manque de familiarité avecle langage médical constituer un problème majeur pour le usager de santé dansl'accès à le information et son interprétation . . ce papier clr inscrire dans le problématiqued'étude et de caractérisation de le terminologie des usager de santépour pouvoir proposer un service adapter à son langage et à son niveau deconnaissances . . le travail réaliser être un ontologie dans le domaine du cancerdu sein orienter vers le usager de santé . . ce ontologie être construire à partird'un ensemble de corpus de texte représenter deux catégorie : le médiateurset le usager de santé . . le élément de ce ontologie avoir être analyser en utilisantdes méthode quantitatif et qualitatif sur plusieurs niveau : terme , concept et relation . " 	Caractériser la terminologie des usagers de santé dans le domaine du cancer du sein	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	CARTOCEL : Un outil de cartographie des connaissances guidée par la machine cellulaire CASI	Nous présentons, dans ce papier, l'outil CARTOCEL (CARTOgraphiesCELlulaires) permettant une visualisation automatique et dynamique desdomaines de connaissances. Le fonctionnement de CARTOCEL est basé surune approche originale de modélisation booléenne de la cartographie des domainesde connaissances métiers/stratégiques inspirée du principe de la machinecellulaire CASI (Cellular Automata for Symbolic Induction). Le but,après une modélisation booléenne de la cartographie des domaines de connaissances,est double : d'une part affiner la cartographie par une fouille de donnéeorchestrée par CASI, et d'autre part réduire la complexité de stockage, ainsique le temps de calcul	Menaouer Brahami, Baghdad Atmani, Mostéfa Mokaddem	http://editions-rnti.fr/render_pdf.php?p1&p=1001375	http://editions-rnti.fr/render_pdf.php?p=1001375	642	fr	fr	@gmail.com, @gmail.com, @gmail.com, @univ-oran.dz	CARTOCEL : un outil de cartographie des connaissance guider par le machine cellulaire CASI  Nous présenter , dans ce papier , le outil CARTOCEL ( CARTOgraphiesCELlulaires ) permettre un visualisation automatique et dynamique desdomaines de connaissance . le fonctionnement de CARTOCEL être baser surune approche original de modélisation booléen de le cartographie des domainesde connaissance métier  stratégique inspirer du principe de le machinecellulaire CASI ( Cellular Automata for Symbolic induction ) . le but , après un modélisation booléen de le cartographie des domaine de connaissance , être double : d' un part affiner le cartographie par un fouille de donnéeorchestrée par CASI , et d' autre part réduire le complexité de stockage , ainsique le temps de calcul 	CARTOCEL : Un outil de cartographie des connaissances guidée par la machine cellulaire CASI	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	ChorML : Résumés visuels de bases des données géographiques		Ibtissem Cherni, Karla Lopez, Robert Laurini, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001443	http://editions-rnti.fr/render_pdf.php?p=1001443	643	fr		@insa-lyon.fr, @insat.rnu.tn	ChorML : Résumés visuels de bases des données géographiques 	ChorML : Résumés visuels de bases des données géographiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classer, discriminer et visualiser des séquences d'événements	Cet article 1 présente un ensemble d'outils destiné à analyser des séquencesd'événements en sciences sociales et à visualiser les résultats obtenus.Nous commençons par formaliser la notion de séquence d'événements avant dedéfinir une mesure de dissimilarité entre ces séquences afin de construire destypologies et de tester les liens entre ces séquences et d'autres variables d'intérêts.Initialement définie par Moen (2000), cette mesure se base sur la notion dedistance d'édition entre séquences et permet d'identifier les différences d'ordonnancementet de temporalité des événements. Nous proposons une extension decelle-ci afin de pouvoir prendre en compte la simultanéité des événements ainsiqu'une méthode de normalisation qui garantit le respect de l'inégalité triangulaire.Dans un deuxième temps, nous présentons un ensemble d'outils destinésà interpréter les résultats. Nous proposons ainsi deux méthodes de visualisationd'un ensemble de séquences et nous introduisons la notion de sous-séquencediscriminante qui permet d'identifier les différences d'ordonnancement des événementsles plus significatives entre groupes. L'ensemble des outils présentés estdisponible au sein de la librairie R TraMineR.	Matthias Studer, Nicolas S. Müller, Gilbert Ritschard, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1001264	http://editions-rnti.fr/render_pdf.php?p=1001264	644	fr	fr	@unige.ch	classer , discriminer et visualiser un séquence d' événements  ce article 1 présenter un ensemble d' outil destiner à analyser un séquencesd'événements en science social et à visualiser le résultat obtenir . Nous commencer par formaliser le notion de séquence d' événement avant dedéfinir un mesure de dissimilarité entre ce séquence afin de construire destypologies et de tester le lien entre ce séquence et d' autre variable d' intérêt . initialement définir par Moen ( 2000 ) , ce mesure clr baser sur le notion dedistance d' édition entre séquence et permettre d' identifier le différence d' ordonnancementet de temporalité des événement . Nous proposer un extension decelle _-ci afin de pouvoir prendre en compte le simultanéité des événement ainsiqu'une méthode de normalisation qui garantir le respect de le inégalité triangulaire . Dans un deuxième temps , nous présenter un ensemble d' outil destinésà interpréter le résultat . Nous proposer ainsi deux méthode de visualisationd'un ensemble de séquence et nous introduire le notion de sous-séquencediscriminante qui permettre d' identifier le différence d' ordonnancement des événementsles plus significatif entre groupe . le ensemble des outil présenter estdisponible au sein de le librairie R TraMineR . 	Classer, discriminer et visualiser des séquences d'événements	12
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification de documents : calcul d'une distance structurelle	La classification des documents numériques garantit un accès rapideet ciblé à l'information. Si nous considérons qu'un document est représenté parsa ou ses structures, définir des classes de documents revient à définir desclasses de structures. Une classe structurelle représente donc des structures« proches ». Ainsi, associer la structure d'un document à sa classe structurellerevient à calculer une distance dite « structurelle ». Elle tiendra compte à lafois de l'organisation des éléments (position des noeuds, chemin), du coûtd'adaptation des représentants des classes ainsi que de la représentativité dessous-graphes. Sur un corpus de documents représentant des notices de livresissus de la bibliothèque de l'université, nous discuterons de la construction decette distance, de l'intérêt de chacun des trois paramètres utilisés	Karim Djemal, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau	http://editions-rnti.fr/render_pdf.php?p1&p=1001369	http://editions-rnti.fr/render_pdf.php?p=1001369	645	fr	fr	@irit.fr, @univ-tlse1.fr	classification de document : calcul d' un distance structurelle  le classification des document numérique garantir un accès rapideet cibler à le information . Si nous considérer qu' un document être représenter parsa ou son structure , définir un classe de document revenir à définir desclasses de structure . un classe structurel représenter donc un structure « proche » . ainsi , associer le structure d' un document à son classe structurellerevient à calculer un distance dire « structurel » . Elle tenir compter à lafois de le organisation des élément ( position des noeud , chemin ) , du coûtd'adaptation des représentant des classe ainsi que de le représentativité dessous-graphes . Sur un corpus de document représenter un notice de livresissus de le bibliothèque de le université , nous discuter de le construction decette distance , de le intérêt de chacun des trois paramètre utilisés 	Classification de documents : calcul d'une distance structurelle	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification et Selection de caracteristique basees sur les concepts semantiques pour la recherche d'information multimedia	Le besoin récent de nombreuses applications multimédia basées sur le contenu a engendré une demande croissante de technologies dans le domaine de la recherche d'information multimédia. Basée sur l'état de l'art des techniques existantes, nous proposons dans cet article une approche de recherche d'information multimédia qui prend en compte les informations de scène et exploite un modèle de sélection de caractéristiques. Les principaux avantages de notre modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de sélection de caractéristiques; (iii) un index multidimensionnel. Notre framework propose un bon compromis entre précision et rapidité de la recherche	Thierry Urruty, Ismail Elsayad, Adel Lablack, Yue Feng, Jose M. Joemon	http://editions-rnti.fr/render_pdf.php?p1&p=1001271	http://editions-rnti.fr/render_pdf.php?p=1001271	646	fr	fr	@lill.lr	classification et Selection de caracteristique basees sur le concept semantiques pour le recherche d' information multimedia  le besoin récent de nombreux application multimédia baser sur le contenu avoir engendrer un demande croissant de technologie dans le domaine de le recherche d' information multimédia . baser sur le état de le art des technique existant , nous proposer dans ce article un approche de recherche d' information multimédia qui prendre en compte le information de scène et exploiter un modèle de sélection de caractéristique . le principal avantage de son modèle de recherche par rapport aux modèle existant être : ( i ) un méthode de classification baser sur un catégorie de concept sémantique ; ( ii ) un modèle de recherche par rapport aux modèle existant être : ( i ) un méthode de classification baser sur un catégorie de concept sémantique ; ( ii ) un modèle de sélection de caractéristique ; ( iii ) un index multidimensionnel . son framework proposer un bon compromis entre précision et rapidité de le recherche 	Classification et Selection de caracteristique basees sur les concepts semantiques pour la recherche d'information multimedia	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification supervisée pour de grands nombres de classes à prédire : une approche par co-partitionnement des variables explicatives et à expliquer	Dans la phase de préparation des données du data mining, les méthodesde discrétisation et de groupement de valeurs supervisé possèdent denombreuses applications : interprétation, estimation de densité conditionnelle,sélection de type filtre des variables, recodage des variables en amont des classifieurs.Ces méthodes supposent habituellement un faible nombre de valeur àexpliquer (classes), typiquement moins d'une dizaine, et trouvent leur limitequand leur nombre augmente. Dans cet article, nous introduisons une extensiondes méthodes de discrétisation et groupement de valeurs, consistant à partitionnerd'une part la variable explicative, d'autre part la variable à expliquer.Le meilleur co-partitionnement est recherché au moyen d'une approche Bayesiennede la sélection de modèle. Nous présentons ensuite comment utiliser cetteméthode de prétraitement en préparation pour le classifieur Bayesien naïf. Desexpérimentations intensives démontrent l'apport de la méthode dans le cas decentaines de classes.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001352	http://editions-rnti.fr/render_pdf.php?p=1001352	647	fr	fr	@orange-ftgroup.com	classification superviser pour un grand nombre de classe à prédire : un approche par co- partitionnement des variable explicatif et à expliquer  Dans le phase de préparation des donnée du data mining , le méthodesde discrétisation et de groupement de valeur superviser posséder denombreuses application : interprétation , estimation de densité conditionnel , sélection de type filtre des variable , recodage des variable en amont des classifieur . ce méthode supposer habituellement un faible nombre de valeur àexpliquer ( classe ) , typiquement moins d' un dizaine , et trouver son limitequand son nombre augmenter . Dans ce article , nous introduire un extensiondes méthode de discrétisation et groupement de valeur , consister à partitionnerd'une part le variable explicatif , d' autre part le variable à expliquer . le meilleur co- partitionnement être rechercher au moyen d' un approche Bayesiennede le sélection de modèle . Nous présenter ensuite comment utiliser cetteméthode de prétraitement en préparation pour le classifieur Bayesien naïf . Desexpérimentations intensif démontrer le apport de le méthode dans le cas decentaines de classe . 	Classification supervisée pour de grands nombres de classes à prédire : une approche par co-partitionnement des variables explicatives et à expliquer	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	CND-Cube : Nouvelle représentation concise sans perte d'information d'un cube de données	"Le calcul des cubes de données est excessivement coûteux aussi bienen temps d'exécution qu'en mémoire et son stockage sur disque peut s'avérerprohibitif. Plusieurs efforts ont été consacrés à ce problème à travers les cubesfermés, où les cellules préservant la sémantique d'agrégation sont réduites à unecellule, sans perte d'information. Dans cet article, nous introduisons le conceptdu cube de données non-dérivable fermé, nommé CND-Cube, qui généralisela notion des modèles non-dérivables fermés fréquents bidimensionnels à uncontexte multidimensionnel. Nous proposons un nouvel algorithme pour extrairele CND-Cube à partir des bases de données multidimensionnelles en se basantsur trois contraintes anti-monotones, à savoir ""être fréquent"", ""être non dérivable""et ""être un générateur minimal"". Les expériences montrent que notreproposition fournit la représentation la plus concise d'un cube de données et elleest ainsi la plus efficace pour réduire l'espace de stockage"	Hanen Brahmi, Tarek Hamrouni, Riadh Ben Messaoud, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001303	http://editions-rnti.fr/render_pdf.php?p=1001303	648	fr	fr	@fst.rnu.tn, @fsegn.rnu.tn	CND-Cube : nouveau représentation concis sans perte d' information d' un cube de données  " le calcul des cube de donnée être excessivement coûteux aussi bienen temps d' exécution qu' en mémoire et son stockage sur disque pouvoir clr avérerprohibitif . . plusieurs effort avoir être consacrer à ce problème à travers le cubesfermés , où le cellule préserver le sémantique d' agrégation être réduire à unecellule , sans perte d' information . . Dans ce article , nous introduire le conceptdu cube de donnée non- dérivable fermer , nommer CND-Cube , qui généralisela notion des modèle non- dérivable fermer fréquent bidimensionnels à uncontexte multidimensionnel . . Nous proposer un nouveau algorithme pour extrairele CND-Cube à partir un base de donnée multidimensionnel en clr basantsur trois contrainte anti- monotone , à savoir " " être fréquent " " , " " être non dérivable " " et " " être un générateur minimal " " . . le expérience montrer que notreproposition fournir le représentation le plus concis d' un cube de donnée et elleest ainsi le plus efficace pour réduire le espace de stockage " 	CND-Cube : Nouvelle représentation concise sans perte d'information d'un cube de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Codage et classification non supervisée d'un corpus maya : extraire des contextes pour situer l'inconnu par rapport au connu	L'écriture logosyllabique des anciens Mayas comprend plus de 500signes et est en bonne partie déchiffrée, avec des degrés de certitude divers.Nous avons appliqué au codex de Dresde, l'un des trois seuls manuscrits quinous soient parvenus, codé sous LATEXavec le systèmemayaTEX, notre méthodede représentation graduée, par apprentissage non supervisé hybride entre clusteringet analyse factorielle oblique, sous la métrique de Hellinger, afin d'obtenirune image nuancée des thèmes traités : les individus statistiques sont les 212segments de folio du codex, et leurs attributs sont les 1687 bigrammes de signesextraits. Pour comparaison, nous avons introduit dans cette approche endogèneun élément exogène, la décomposition en éléments des signes composites, pourpréciser plus finement les contenus. La rétro-visualisation dans le texte originaldes résultats et expressions dégagées éclaire la signification de certains glyphespeu compris, en les situant dans des contextes clairement interprétables.	Mohamed Hallab, Bruno Delprat, Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1001366	http://editions-rnti.fr/render_pdf.php?p=1001366	649	fr	fr	@yahoo.fr, @club-internet.fr, @univ-fcomte.fr	codage et classification non superviser d' un corpus maya : extraire un contexte pour situer le inconnu par rapport au connu  le écriture logosyllabique des ancien Mayas comprendre plus de 500signes et être en bon partie déchiffrer , avec un degré de certitude divers . Nous avoir appliquer au codex de Dresde , le un des trois seul manuscrit quinous être parvenir , coder sous LATEXavec le systèmemayaTEX , son méthodede représentation graduer , par apprentissage non superviser hybride entre clusteringet analyse factoriel oblique , sous le métrique de Hellinger , afin d' obtenirune image nuancer des thème traiter : le individu statistique être le 212segments de folio du codex , et son attribut être le 1687 bigrammes de signesextraits . Pour comparaison , nous avoir introduire dans ce approche endogèneun élément exogène , le décomposition en élément des signe composite , pourpréciser plus finement le contenu . le rétro- visualisation dans le texte originaldes résultat et expression dégager éclaire le signification de certain glyphespeu comprendre , en les situer dans un contexte clairement interprétable . 	Codage et classification non supervisée d'un corpus maya : extraire des contextes pour situer l'inconnu par rapport au connu	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Combiner approche logique et numérique pour la réconciliation de données et l'alignement d'ontologies		Marie-Christine Rousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001262	http://editions-rnti.fr/render_pdf.php?p=1001262	650	fr		@imag.fr	Combiner approche logique et numérique pour la réconciliation de données et l'alignement d'ontologies 	Combiner approche logique et numérique pour la réconciliation de données et l'alignement d'ontologies	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	CombinerWeb 2.0 et Web Sémantique pour réduire les disparités d'expertise au sein de blogs d'entreprise	Avec l'avènement d'applications sociales en entreprise (blogs, wikis,etc.), il est fréquent que des individus aux niveaux d'expertise relativement distantsse réunissent au sein de communautés en ligne. Ces disparités d'expertisese traduisent entre autres par des comportements différents dans la manière detagguer les contenus créés, notamment en ce qui concerne les termes utilisés,rendant ainsi complexe la découverte d'informations pourtant publiées. Dans cetarticle, nous mettons en avant la possibilité offerte par les technologies du WebSémantique, combinées avec les paradigmes du Web Social, de résoudre cetteproblématique. Nous proposons ainsi une chaine de traitement combinant ontologies,wikis sémantiques et indexation de contenus permettant la production degraphes sémantiques interconnectés et facilitant de cette manière la découvertede contenus créés au sein de tels systèmes	Alexandre Passant, Philippe Laublet	http://editions-rnti.fr/render_pdf.php?p1&p=1001270	http://editions-rnti.fr/render_pdf.php?p=1001270	651	fr	fr	@deri.org, @paris-sorbonne.fr	CombinerWeb 2.0 et Web Sémantique pour réduire le disparité d' expertise au sein de blog d' entreprise  Avec le avènement d' application social en entreprise ( blog , wiki , etc. ) , il être fréquent que un individu aux niveau d' expertise relativement distantsse réunir au sein de communauté en ligne . ce disparité d' expertisese traduire entre autre par un comportement différent dans le manière detagguer le contenu créer , notamment en ce qui concerner le terme utiliser , rendre ainsi complexe le découverte d' information pourtant publier . Dans cetarticle , nous mettre en avant le possibilité offrir par le technologie du WebSémantique , combiner avec le paradigme du Web Social , de résoudre cetteproblématique . Nous proposer ainsi un chaine de traitement combiner ontologie , wiki sémantique et indexation de contenu permettre le production degraphes sémantique interconnecter et faciliter de ce manière le découvertede contenir créer au sein de tel systèmes 	CombinerWeb 2.0 et Web Sémantique pour réduire les disparités d'expertise au sein de blogs d'entreprise	
Revue des Nouvelles Technologies de l'Information	EGC	2010	Comparaison de critères de pureté pour l'intégration de connaissances en clustering semi-supervisé	L'utilisation de connaissances pour améliorer les processus de fouillede données a mobilisé un important effort de recherche ces dernières années. Ilest cependant souvent difficile de formaliser ce type de connaissances, commecelles-ci sont souvent dépendantes du domaine. Dans cet article, nous nous intéressonsà l'intégration de connaissances sous la forme d'objets étiquetés dansles algorithmes de clustering. Plusieurs critères permettant d'évaluer la puretédes clusters sont présentés et leur comportement est comparé sur des jeux dedonnées artificiels. Les avantages et les inconvénients de chaque critère sontanalysés pour aider l'utilisateur à faire un choix.	Germain Forestier, Cédric Wemmert, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1001278	http://editions-rnti.fr/render_pdf.php?p=1001278	652	fr	fr	@unistra.fr	comparaison de critère de pureté pour le intégration de connaissance en clustering semi-supervisé  le utilisation de connaissance pour améliorer le processus de fouillede donner avoir mobiliser un important effort de recherche ce dernier année . Ilest cependant souvent difficile de formaliser ce type de connaissance , commecelles _-ci être souvent dépendant du domaine . Dans ce article , nous clr intéressonsà le intégration de connaissance sous le forme d' objet étiqueter dansles algorithme de clustering . plusieurs critère permettre d' évaluer le puretédes clusters être présenter et son comportement être comparer sur un jeu dedonnées artificiel . le avantage et le inconvénient de chaque critère sontanalysés pour aider le utilisateur à faire un choix . 	Comparaison de critères de pureté pour l'intégration de connaissances en clustering semi-supervisé	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Comparaisons structurelles de grandes bases de données par apprentissage non-supervisé	Dans le domaine de la fouille de données, mesurer les similitudesentre différents sous-ensembles est une question importante qui a été peu étudiéejusqu'à présent. Dans cet article, nous proposons une nouvelle méthodebasée sur l'apprentissage non-supervisé. Les différents sous-ensembles à comparersont caractérisés au moyen d'un modèle à base de prototypes. Ensuite, lesdifférences entre les modèles sont détectées en utilisant une mesure de similarité	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001276	http://editions-rnti.fr/render_pdf.php?p=1001276	653	fr	fr		comparaison structurel de grand base de donnée par apprentissage non- supervisé  Dans le domaine de le fouille de donnée , mesurer le similitudesentre différent sous-ensemble être un question important qui avoir être peu étudiéejusqu'à présent . Dans ce article , nous proposer un nouveau méthodebasée sur le apprentissage non- superviser . le différent sous-ensemble à comparersont caractériser au moyen d' un modèle à base de prototype . ensuite , lesdifférences entre le modèle être détecter en utiliser un mesure de similarité 	Comparaisons structurelles de grandes bases de données par apprentissage non-supervisé	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Composition de ServicesWeb Basée sur les Réseau Sociaux	Nous proposons dans cet article une première approche qui consisteà exploiter les réseaux sociaux afin de faciliter la composition de services parles utilisateurs finaux. Nous introduisons un Framework, nommé Social Composer(SoCo), qui implémente cette approche. SoCo fournit à l'utilisateur desrecommandations dynamiques de services basées entre autre sur le réseau socialde l'utilisateur qui est construit implicitement à partir des interactions entre lesutilisateurs, les services, les différentes compositions opérées par les membresdu réseau social, ainsi que le réseau social global.	Abderrahmane Maaradji, Hakim Hacid, Johann Daigremont, Noël Crespi	http://editions-rnti.fr/render_pdf.php?p1&p=1001290	http://editions-rnti.fr/render_pdf.php?p=1001290	654	fr	fr	@alcatel-lucent.com, @it-sudparis.eu	composition de ServicesWeb baser sur le réseau Sociaux  Nous proposer dans ce article un premier approche qui consisteà exploiter le réseau social afin de faciliter le composition de service parler utilisateur final . Nous introduire un Framework , nommer Social Composer ( SoCo ) , qui implémenter ce approche . SoCo fournir à le utilisateur desrecommandations dynamique de service baser entre autre sur le réseau socialde le utilisateur qui être construire implicitement à partir un interaction entre lesutilisateurs , le service , le différent composition opérer par le membresdu réseau social , ainsi que le réseau social global . 	Composition de ServicesWeb Basée sur les Réseau Sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Construction de noyaux pour l'apprentissage supervisé à partir d'arbres aléatoires	Nous montrons qu'un ensemble d'arbres de décision avec une composantealéatoire permet de construire un noyau efficace destiné à l'apprentissagesupervisé. Nous étudions théoriquement les propriétés d'un tel noyau et montronsque sous des conditions très souvent rencontrées en pratique, il existe uneséparabilité linéaire entre exemples de classes distinctes dans l'espace induit parcelui-ci. Parallèlement, nous observons également que le classique vote à la majoritéd'un ensemble d'arbres est un hyperplan (sans garantie d'optimalité) dansl'espace induit par le noyau. Enfin, comme le montrent nos expérimentations,l'utilisation conjointe d'un ensemble d'arbres et d'un séparateur à vaste marge(SVM) aboutit à des résultats extrêmement encourageants.	Vincent Pisetta, Pierre-Emmanuel Jouve, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001351	http://editions-rnti.fr/render_pdf.php?p=1001351	655	fr	fr	@rithme.eu, @fenics.com, @univ-lyon2.fr	construction de noyau pour le apprentissage superviser à partir d' arbre aléatoires  Nous montrer qu' un ensemble d' arbre de décision avec un composantealéatoire permettre de construire un noyau efficace destiner à le apprentissagesupervisé . Nous étudier théoriquement le propriété d' un tel noyau et montronsque sous un condition très souvent rencontrer en pratique , il exister uneséparabilité linéaire entre exemple de classe distinct dans le espace induire parcelui _-ci . parallèlement , nous observer également que le classique vote à le majoritéd'un ensemble d' arbre être un hyperplan ( sans garantie d' optimalité ) dansl'espace induire par le noyau . enfin , comme le montrer son expérimentation , le utilisation conjoindre d' un ensemble d' arbre et d' un séparateur à vaste marge ( SVM ) aboutir à un résultat extrêmement encourageant . 	Construction de noyaux pour l'apprentissage supervisé à partir d'arbres aléatoires	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Cubes Fermés / Quotients Émergents	"Le concept de Cube Émergent a été introduit afin de comparer deuxdata cubes. Dans cet article, nous introduisons deux nouvelles représentationsréduites du Cube Émergent sans perte des mesures : le Cube Fermé Émergent etle Cube Quotient Émergent. La première représentation est basée sur le conceptde fermeture cubique. C'est la plus petite représentation possible du cube dedonnées émergent. À partir du Cube Fermé Émergent et donc en stockant le minimumd'informations, il est possible de répondre efficacement aux requêtes quipeuvent être exécutées sur le Cube Émergent lui-même. La seconde représentations'appuie sur la structure du Cube Quotient qui a été proposé pour résumer uncube de données. Le Cube Quotient est revisité afin de le doter d'une sémantiquebasée sur la fermeture cubique et donc adapté au contexte du Cube Émergent. LeCube Quotient Émergent résultant est moins réduit que le Cube Fermé Émergentmais il préserve la propriété de "" spécialisation/généralisation "" du data cube quipermet la navigation au sein du Cube Émergent. Nous établissons également lelien entre les deux représentations introduites et celle basée sur les bordures classiquesen fouille de données. Des expérimentations effectuées sur divers jeux dedonnées visent à comparer la taille des différentes représentations."	Sébastien Nedjar, Alain Casali, Rosine Cicchetti, Lotfi Lakhal	http://editions-rnti.fr/render_pdf.php?p1&p=1001306	http://editions-rnti.fr/render_pdf.php?p=1001306	656	fr	fr	@lif.univ-mrs.fr	cube Fermés  quotient Émergents  " le concept de Cube Émergent avoir être introduire afin de comparer deuxdata cube . . Dans ce article , nous introduire deux nouveau représentationsréduites du Cube Émergent sans perte des mesure : le Cube Fermé Émergent etle Cube Quotient Émergent . . le premier représentation être baser sur le conceptde fermeture cubique . . C' être le plus petit représentation possible du cube dedonnées émerger . . À partir du cube Fermé Émergent et donc en stocker le minimumd'informations , il être possible de répondre efficacement aux requête quipeuvent être exécuter sur le Cube Émergent lui-même . . le second représentations'appuie sur le structure du Cube Quotient qui avoir être proposer pour résumer uncube de donnée . . le Cube Quotient être revisiter afin de le doter d' un sémantiquebasée sur le fermeture cubique et donc adapter au contexte du cube Émergent . . LeCube Quotient Émergent résulter être moins réduire que le Cube Fermé Émergentmais il préserver le propriété de " " spécialisation  généralisation " " du dater cube quipermet le navigation au sein du cube Émergent . . Nous établir également lelien entre le deux représentation introduire et celui baser sur le bordure classiquesen fouiller de donnée . . un expérimentation effectuer sur divers jeu dedonnées viser à comparer le taille des différent représentation . " 	Cubes Fermés / Quotients Émergents	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	DaFOE: une plateforme pour construire des ontologies à partir de textes et de thésaurus		Jean Charlet, Sylvie Szulman, Nathalie Aussenac-Gilles, Adeline Nazarenko, Nathalie Hernandez, Nadia Nadah, Éric Sardet, Jean Delahousse, Valery Teguiak, Audrey Baneyx	http://editions-rnti.fr/render_pdf.php?p1&p=1001386	http://editions-rnti.fr/render_pdf.php?p=1001386	657	fr		@upmc.fr	DaFOE: une plateforme pour construire des ontologies à partir de textes et de thésaurus 	DaFOE: une plateforme pour construire des ontologies à partir de textes et de thésaurus	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Découverte des dépendances fonctionnelles conditionnelles fréquentes	Les Dépendances Fonctionnelles Conditionnelles (DFC) ont été introduitesen 2007 pour le nettoyage des données. Elles peuvent être considéréescomme une unification de Dépendances Fonctionnelles (DF) classiques et deRègles d'Association (RA) puisqu'elles permettent de spécifier des dépendancesmixant des attributs et des couples de la forme attribut/valeur.Dans cet article, nous traitons le problème de la découverte des DFC, i.e. déterminerune couverture de l'ensemble des DFC satisfaites par une relation r. Nousmontrons comment une technique connue pour la découverte des DF (exacteset approximatives) peut être étendue aux DFC. Cette technique a été implémentéeet des expériences ont été menées pour montrer la faisabilité et le passage àl'échelle de notre proposition.	Thierno Diallo, Noel Novelli	http://editions-rnti.fr/render_pdf.php?p1&p=1001311	http://editions-rnti.fr/render_pdf.php?p=1001311	658	fr	fr	@insa-lyon.fr, @lif.univ-mrs.fr	découverte des dépendance fonctionnel conditionnel fréquentes  le dépendance Fonctionnelles Conditionnelles ( DFC ) avoir être introduitesen 2007 pour le nettoyage des donnée . Elles pouvoir être considéréescomme un unification de dépendance Fonctionnelles ( DF ) classique et deRègles d' association ( ra ) puisqu' elles permettre de spécifier un dépendancesmixant des attribut et des couple de le forme attribut  valeur . Dans ce article , nous traiter le problème de le découverte des DFC , i.e. déterminerune couverture de le ensemble des DFC satisfaire par un relation r . Nousmontrons comment un technique connaître pour le découverte des DF ( exacteset approximatif ) pouvoir être étendre aux DFC . ce technique avoir être implémentéeet des expérience avoir être mener pour montrer le faisabilité et le passage àl'échelle de son proposition . 	Découverte des dépendances fonctionnelles conditionnelles fréquentes	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Découverte d'itemsets fréquents fermés sur architectures multicoeurs	Dans ce papier nous proposons PLCM, un algorithme parallèle dedécouverte d'itemsets fréquents fermés basé sur l'algorithme LCM, reconnucomme l'algorithme séquentiel le plus efficace pour cette tâche. Nous présentonsaussi une interface de parallélisme à la fois simple et puissante basée sur lanotion de Tuple Space, qui permet d'avoir une bonne répartition dynamique dutravail.Grâce à une étude expérimentale détaillée, nous montrons que PLCM est le seulalgorithme qui soit suffisamment générique pour calculer efficacement des itemsetsfréquents fermés à la fois sur des bases creuses et sur des bases denses,améliorant ainsi l'état de l'art.	Benjamin Négrevergne, Alexandre Termier, Jean-François Méhaut, Takeaki Uno	http://editions-rnti.fr/render_pdf.php?p1&p=1001339	http://editions-rnti.fr/render_pdf.php?p=1001339	659	fr	fr	@imag.fr, @nii.jp	découverte d' itemsets fréquent fermer sur architecture multicoeurs  Dans ce papier nous proposer PLCM , un algorithme parallèle dedécouverte d' itemsets fréquent fermer baser sur le algorithme LCM , reconnucomme le algorithme séquentiel le plus efficace pour ce tâche . Nous présentonsaussi un interface de parallélisme à le foi simple et puissant baser sur lanotion de Tuple Space , qui permettre d' avoir un bon répartition dynamique dutravail . grâce à un étude expérimental détailler , nous montrer que PLCM être le seulalgorithme qui être suffisamment générique pour calculer efficacement un itemsetsfréquents fermer à le foi sur un base creux et sur un base dense , améliorer ainsi le état de le art . 	Découverte d'itemsets fréquents fermés sur architectures multicoeurs	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Density estimation on data streams : an application to Change Detection	In recent years, the amount of data to process has increased in manyapplication areas such as network monitoring, web click and sensor data analysis. Data stream mining answers to the challenge of massive data processing, this paradigm allows for treating pieces of data on the fly and overcoming data storage. The detection of changes in a data stream distribution is an important issue. This article proposes a new schema of change detection :i) the summarization of the input data stream by a set of micro-clusters;ii) the estimate of the data stream distribution exploiting micro-clusters;iii) the estimate of the divergence between the current estimated distribution and a reference distribution;iv) diagnostic step through the contribution of each predictive variable to the overall divergence between both distributions.Our schema of change detection is applied and evaluated on artificial data streams.	Marie-Luce Picard, Benoît Grossin, Alexis Bondu	http://editions-rnti.fr/render_pdf.php?p1&p=1001297	http://editions-rnti.fr/render_pdf.php?p=1001297	660	en	en	@edf.fr	density estimation data stream application change detection recent year amount datum process increase manyapplication area network monitor web click sensor data analysis data stream mining answer challenge massive data processing paradigm allow treat piece data fly overcome datum storage detection change data stream distribution important issue article propose new schema change detection i summarization input datum stream set micro cluster ii estimate data stream distribution exploit micro cluster iii estimate divergence current estimated distribution reference distribution iv diagnostic step contribution predictive variable overall divergence distribution our schema change detection apply evaluated artificial data stream	Density estimation on data streams : an application to Change Detection	8
Revue des Nouvelles Technologies de l'Information	EGC	2010	Detecting Anomalies in Data Streams using Statecharts	The environment around us is progressively equipped withvarious sensors, producing data continuously. The applications usingthese data face many challenges, such as data stream integration over anattribute (such as time) and knowledge extraction from raw data. In thispaper we propose one approach to face those two challenges. First, datastreams integration is performed using statecharts which represents aresume of data produced by the corresponding data producer. Second,we detect anomalous events over temporal relations among statecharts.We describe our approach in a demonstration scenario, that is using avisual tool called Patternator	Vasile-Marian Scuturici, Dan-Mircea Suciu, Romain Vuillemot, Aris Ouksel, Lionel Brunie	http://editions-rnti.fr/render_pdf.php?p1&p=1001394	http://editions-rnti.fr/render_pdf.php?p=1001394	661	en	en		detect anomaly datum stream used statechart environment around us progressively equip withvarious sensor produce datum continuously application usingthese datum face many challenge data stream integration anattribute such time knowledge extraction raw data thispaper propose one approach face two challenge first datastreams integration perform used statechart represent aresume data produced corresponding data producer second we detect anomalous event temporal relation among statechart we describe approach demonstration scenario used avisual tool call patternator	Detecting Anomalies in Data Streams using Statecharts	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Détection des mouvements anormaux dans des vidéos		Md. Haidar Sharif, Husam Alustwani, Ioan Marius Bilasco, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001448	http://editions-rnti.fr/render_pdf.php?p=1001448	662	fr		@lifl.fr	Détection des mouvements anormaux dans des vidéos 	Détection des mouvements anormaux dans des vidéos	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Développement de méthodes de classification basées sur l'Analyse de Concepts Formels sous la plateforme WEKA		Besma Khalfi, Rahma Cherif, Nida Meddouri, Mondher Maddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001373	http://editions-rnti.fr/render_pdf.php?p=1001373	663	fr		@hotmail.com, @gmail.com, @fst.rnu.tn	Développement de méthodes de classification basées sur l'Analyse de Concepts Formels sous la plateforme WEKA 	Développement de méthodes de classification basées sur l'Analyse de Concepts Formels sous la plateforme WEKA	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Differentes variantes GMM-SMOs pour l'identification du locuteur	Dans cet article, nous présentons différentes variantes GMM-SMOs pour l'identification du locuteur en mode indépendant du texte. Pour mettre en oeuvre les différents systèmes, nous avons opté une représentation multi-gaussienne de l'espace des caractéristiques basées sur l'algorithme Expectation Maximisation (EM). Ces nouvelles représentations constituent les vecteurs d'entrés pour entraîner les supports vecteurs machines (SVMs) par l'algorithme de type Optimisation par Minimisation Séquentielle (SMO).	Siwar Zribi Boujelbene, Dorra Ben Ayed Mezghanni, Noureddine Ellouze	http://editions-rnti.fr/render_pdf.php?p1&p=1001421	http://editions-rnti.fr/render_pdf.php?p=1001421	664	fr	fr	@yahoo.fr, @isi.rnu.tn, @enit.rnu.tn	Differentes variante GMM-SMOs pour le identification du locuteur  Dans ce article , nous présenter différent variante GMM-SMOs pour le identification du locuteur en mode indépendant du texte . Pour mettre en oeuvre le différent système , nous avoir opter un représentation multi-gaussienne de le espace des caractéristique baser sur le algorithme Expectation Maximisation ( EM ) . ce nouveau représentation constituer le vecteur d' entrer pour entraîner le support vecteurs machine ( SVMs ) par le algorithme de type optimisation par minimisation Séquentielle ( SMO ) . 	Differentes variantes GMM-SMOs pour l'identification du locuteur	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Etude comparative des langages de requêtes sémantiques pour l'extraction des liens complexes dans une base de connaissances		Thabet Slimani, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1001445	http://editions-rnti.fr/render_pdf.php?p=1001445	665	fr		@issatm.rnu.tn, @ihec.rnu.tn	Etude comparative des langages de requêtes sémantiques pour l'extraction des liens complexes dans une base de connaissances 	Etude comparative des langages de requêtes sémantiques pour l'extraction des liens complexes dans une base de connaissances	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Etude de stabilité de méthodes de sélection de motifs à partir des séquences protéiques		Rabie Saidi, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001454	http://editions-rnti.fr/render_pdf.php?p=1001454	666	fr		@isima, @fst.rnu.tn	Etude de stabilité de méthodes de sélection de motifs à partir des séquences protéiques 	Etude de stabilité de méthodes de sélection de motifs à partir des séquences protéiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Expansion de requêtes SQL par une ontologie de domaine	Cet article traite un problème dans le domaine de la gestion des basesde données classiques. Il s'agit d'exploiter une ontologie de domaine pour aiderl'utilisateur d'une base de données relationnelle dans sa recherche et de luipermettre une interrogation transparente de la base de données. Pour cela, nousproposons une approche d'expansion automatique de requêtes SQL lorsquecelles-ci n'ont pas de réponses. Notre approche est décrite par un algorithmedéfini de manière générique afin d'être utilisé pour une base de données quelconque.	Ines Fayech, Habib Ounalli	http://editions-rnti.fr/render_pdf.php?p1&p=1001345	http://editions-rnti.fr/render_pdf.php?p=1001345	667	fr	fr	@yahoo.fr, @fst.rnu.tn	Expansion de requête SQL par un ontologie de domaine  ce article traire un problème dans le domaine de le gestion des basesde donner classique . Il clr agir d' exploiter un ontologie de domaine pour aiderl'utilisateur d' un base de donnée relationnel dans son recherche et de luipermettre un interrogation transparent de le base de donnée . Pour cela , nousproposons un approche d' expansion automatique de requête SQL lorsquecelles _-ci n' avoir pas un réponse . son approche être décrire par un algorithmedéfini de manière générique afin d' être utiliser pour un base de donnée quelconque . 	Expansion de requêtes SQL par une ontologie de domaine	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Explication de décisions de réconciliation : approche fondée sur les réseaux de Petri colorés	L'objectif des systèmes d'intégration de données est de faciliter l'exploitationet l'interprétation d'informations hétérogènes provenant de différentessources. Lorsque l'on doit intégrer de grands volumes de données, le recours àun expert n'est pas envisageable mais l'exploitation de processus d'intégrationautomatiques peut introduire des approximations ou des erreurs. Nous nous focalisonssur les résultats fournis par les méthodes de réconciliation de données.Ces dernières comparent les données entre elles et détectent celles qui réfèrent àla même entité du monde réel. Pour renforcer la confiance des utilisateurs dansles résultats retournés par ces méthodes, nous proposons dans cet article une approched'explication graphique fondée sur les réseaux de Petri colorés qui estparticulièrement adaptée aux approches de réconciliation globales, numériqueset guidées par une ontologie.	Souhir Gahbiche, Nathalie Pernelle, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001313	http://editions-rnti.fr/render_pdf.php?p=1001313	668	fr	fr	@limsi.fr, @lri.fr, @lri.fr	explication de décision de réconciliation : approcher fonder sur le réseau de Petri colorés  le objectif des système d' intégration de donnée être de faciliter le exploitationet le interprétation d' information hétérogène provenir de différentessources . Lorsque le on devoir intégrer un grand volume de donnée , le recours àun expert n' être pas envisageable mais le exploitation de processus d' intégrationautomatiques pouvoir introduire un approximation ou des erreur . Nous nous focalisonssur le résultat fournir par le méthode de réconciliation de donnée . ce dernier comparer le donnée entre lui et détecter celui qui référer àla même entité du monde réel . Pour renforcer le confiance des utilisateur dansles résultat retourner par ce méthode , nous proposer dans ce article un approched'explication graphique fonder sur le réseau de Petri colorer qui estparticulièrement adapter aux approche de réconciliation global , numériqueset guider par un ontologie . 	Explication de décisions de réconciliation : approche fondée sur les réseaux de Petri colorés	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Exploration de dépendances fonctionnelles et de règles d'association avec OLAP		Pierre Allard, Sébastien Ferré	http://editions-rnti.fr/render_pdf.php?p1&p=1001408	http://editions-rnti.fr/render_pdf.php?p=1001408	669	fr		@irisa.fr, @irisa.fr	Exploration de dépendances fonctionnelles et de règles d'association avec OLAP 	Exploration de dépendances fonctionnelles et de règles d'association avec OLAP	
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de la région d'intérêt d'une personne sur un obstacle		Adel Lablack, Thierry Urruty, Yassine Benabbas, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001430	http://editions-rnti.fr/render_pdf.php?p=1001430	670	fr		@lifl.fr	Extraction de la région d'intérêt d'une personne sur un obstacle 	Extraction de la région d'intérêt d'une personne sur un obstacle	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de motifs graduels clos	"La découverte automatique de règles et motifs graduels (""plus l'âged'une personne est élevé, plus son salaire est élevé"") trouve de très nombreusesapplications sur des bases de données réelles (e.g. biologie, flots de données decapteurs). Si des algorithmes de plus en plus efficaces sont proposés dans desarticles récents, il n'en reste pas moins que ces méthodes génèrent un nombrede motifs tellement important que les experts peinent à les exploiter. Dans cetarticle, nous proposons donc une représentation condensée des motifs graduelsen introduisant les concepts théoriques associés aux opérateurs de fermeture surde tels motifs."	Sarra Ayouni, Sadok Ben Yahia, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001293	http://editions-rnti.fr/render_pdf.php?p=1001293	671	fr	fr	@fst.rnu.tn, @lirmm.fr	extraction de motif graduel clos  " le découverte automatique de règle et motif graduel ( " " plus le âged'une personne être élever , plus son salaire être élever " " ) trouver de très nombreusesapplications sur un base de donnée réel ( e.g. biologie , flot de donnée decapteurs ) . . Si un algorithme de plus en plus efficace être proposer dans desarticles récent , il n' en rester pas moins que ce méthode générer un nombrede motif tellement important que le expert peiner à les exploiter . . Dans cetarticle , nous proposer donc un représentation condenser des motif graduelsen introduire le concept théorique associer aux opérateur de fermeture surde tel motif . " 	Extraction de motifs graduels clos	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de règles d'association séquentielle à l'aide de modèles semi-paramétriques à risques proportionnels	La recherche de liens entre objets fréquents a été popularisée par lesméthodes d'extraction de règles d'association. Dans le cas de séquences d'événements,les méthodes de fouille permettent d'extraire des sous-séquences quipeuvent ensuite être exprimées sous la forme de règles d'association séquentielleentre événements. Cette utilisation de la fouille de séquences pour la recherchede liens entre des événements pose deux problèmes. Premièrement, lecritère principal utilisé pour sélectionner les sous-séquences d'événements estla fréquence, or les occurrences de certains événements peuvent être fortementliées entre elles même lorsqu'elles sont peu fréquentes. Deuxièmement, les mesuresactuelles utilisées pour caractériser les règles d'association ne tiennent pascompte du caractère temporel des données, comme l'importance du timing desévénements ou le problème des données censurées. Dans cet article, nous proposonsune méthode pour rechercher des liens significatifs entre des événementsà l'aide de modèles de durée. Les règles d'association sont construites à partirdes motifs séquentiels observés dans un ensemble de séquences. L'influence surle risque que l'événement « conclusion » se produise après le ou les événements« prémisse » est estimée à l'aide d'un modèle semi-paramétrique à risques proportionnels.Outre la présentation de la méthode, l'article propose une comparaisonavec d'autres mesures d'association	Nicolas S. Müller, Matthias Studer, Gilbert Ritschard, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1001263	http://editions-rnti.fr/render_pdf.php?p=1001263	672	fr	fr	@unige.ch	extraction de règle d' association séquentiel à le aide de modèle semi-paramétriques à risque proportionnels  le recherche de lien entre objet fréquent avoir être populariser par lesméthodes d' extraction de règle d' association . Dans le cas de séquence d' événement , le méthode de fouille permettre d' extraire un sous-séquences quipeuvent ensuite être exprimer sous le forme de règle d' association séquentielleentre événement . ce utilisation de le fouille de séquence pour le recherchede lien entre un événement poser deux problème . premièrement , lecritère principal utiliser pour sélectionner le sous-séquences d' événement estla fréquence , or le occurrence de certain événement pouvoir être fortementliées entre lui même lorsqu' elles être peu fréquent . deuxièmement , le mesuresactuelles utiliser pour caractériser le règle d' association ne tenir pascompte du caractère temporel des donnée , comme le importance du timing desévénements ou le problème des donnée censurer . Dans ce article , nous proposonsune méthode pour rechercher un lien significatif entre un événementsà le aide de modèle de durée . le règle d' association être construire à partirdes motif séquentiel observer dans un ensemble de séquence . le influence surle risquer que le événement « conclusion » clr produire après le ou le événement « prémisse » être estimer à le aide d' un modèle semi-paramétrique à risque proportionnel . Outre le présentation de le méthode , le article proposer un comparaisonavec d' autre mesure d' association 	Extraction de règles d'association séquentielle à l'aide de modèles semi-paramétriques à risques proportionnels	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction des séquences fermées fréquentes à partir de corpus parallèles : Application à la traduction automatique	Dans cet article, nous abordons la problématique d'extraction de séquencesfréquentes à partir de corpus de textes parallèles en prenant en comptel'ordre d'apparition des mots dans une phrase. Notre finalité est d'exploiter cesséquences dans la traduction automatique (TA). Nous introduisons ainsi la notionde règles associatives inter-langues (RAIL) et nous définissons notre modèlede traduction à base de ces associations. Nous décrivons également les différentesexpérimentations conduites sur le corpus EUROPARL afin de construire àpartir des RAIL une table de traduction bilingue qui est intégrée par la suite dansun processus complet de TA.	Cherif Chiraz Latiri, Cyrine Nasri, Kamel Smaïli, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001266	http://editions-rnti.fr/render_pdf.php?p=1001266	673	fr	fr	@gnet.tn, @gmail.com, @fst.rnu.tn, @loria.fr	extraction des séquence fermer fréquent à partir de corpus parallèle : application à le traduction automatique  Dans ce article , nous aborder le problématique d' extraction de séquencesfréquentes à partir de corpus de texte parallèle en prendre en comptel'ordre d' apparition des mot dans un phrase . son finalité être d' exploiter cesséquences dans le traduction automatique ( TA ) . Nous introduire ainsi le notionde règle associatif inter-langues ( rail ) et nous définir son modèlede traduction à base de ce association . Nous décrire également le différentesexpérimentations conduite sur le corpus EUROPARL afin de construire àpartir des rail un table de traduction bilingue qui être intégrer par le suite dansun processus complet de TA . 	Extraction des séquences fermées fréquentes à partir de corpus parallèles : Application à la traduction automatique	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction d'itemsets distinctifs dans les flux de données	L'extraction d'itemsets distinctifs est un sujet de recherche récent quiconnait plusieurs algorithmes pour les données statiques (Knobbe et Ho, 2006;Heikinheimo et al., 2007). Ces solutions ne sont toutefois pas conçues pour lecas des flux de données, pour lesquels les temps de réponse doivent être aussifaibles que possible. Nous considérons le problème de l'extraction d'itemsetsdistinctifs dans les flux, qui peut avoir de nombreuses applications dans la sélectionde variables, la classification ou encore la recherche d'information. Nousproposons l'heuristique IDkF (Itemsets Distinctifs dans les Flux) et des résultatsd'expérimentations en comparaison d'une technique de la littérature.	Chongsheng Zhang, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1001291	http://editions-rnti.fr/render_pdf.php?p=1001291	674	fr	fr	@inria.fr	extraction d' itemsets distinctif dans le flux de données  le extraction d' itemsets distinctif être un sujet de recherche récent quiconnait plusieurs algorithme pour le donnée statique ( Knobbe et Ho , 2006 ; Heikinheimo et al. , 2007 ) . ce solution ne être toutefois pas concevoir pour lecas un flux de donnée , pour lesquels le temps de réponse devoir être aussifaibles que possible . Nous considérer le problème de le extraction d' itemsetsdistinctifs dans le flux , qui pouvoir avoir un nombreux application dans le sélectionde variable , le classification ou encore le recherche d' information . Nousproposons le heuristique IDkF ( Itemsets Distinctifs dans le flux ) et des résultatsd'expérimentations en comparaison d' un technique de le littérature . 	Extraction d'itemsets distinctifs dans les flux de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Fouille visuelle de données en 3D et réalité virtuelle : état de l'art	La fouille visuelle de données (ou Visual Data Mining, VDM) a pourobjectif de faciliter l'interprétation des résultats issus d'une fouille de données,grâce à l'usage de représentations graphiques. Au cours de la dernière décennie,un grand nombre de techniques de visualisation d'information ont été mises aupoint, permettant la visualisation de données multidimensionnelles dans des environnementsvirtuels. Lors des travaux antérieurs, les chercheurs ont proposédes taxonomies pour classer les techniques de VDM (Chi (2000), Herman et al.(2000)). Toutefois, ces taxonomies ne prennent en compte que partiellement lestechniques récentes relatives à l'utilisation de la 3D et de la réalité virtuelle. Lebut de cet article est de faire un état de l'art récent et spécifique à ces techniques.Celles-ci sont détaillées, classées et comparées selon différents critères : les applications,l'encodage graphique, les techniques d'interaction, les avantages etles inconvénients de chaque approche. Ces techniques sont présentées dans destableaux accompagnées d'illustrations graphiques	Zohra Ben Said, Fabrice Guillet, Paul Richard	http://editions-rnti.fr/render_pdf.php?p1&p=1001282	http://editions-rnti.fr/render_pdf.php?p=1001282	675	fr	fr	@univ-nantes.fr, @univ-angers.fr	fouille visuel de donnée en 3D et réalité virtuel : état de le art  le fouille visuel de donnée ( ou Visual Data Mining , VDM ) avoir pourobjectif de faciliter le interprétation des résultat issir d' un fouille de donnée , grâce à le usage de représentation graphique . Au cour de le dernier décennie , un grand nombre de technique de visualisation d' information avoir être mettre aupoint , permettre le visualisation de donnée multidimensionnel dans un environnementsvirtuels . lors un travail antérieur , le chercheur avoir proposédes taxonomie pour classer le technique de VDM ( Chi ( 2000 ) , Herman et al. ( 2000 ) ) . toutefois , ce taxonomie ne prendre en compte que partiellement lestechniques récent relatif à le utilisation de le 3D et de le réalité virtuel . Lebut de ce article être de faire un état de le art récent et spécifique à ce technique . Celles _-ci être détailler , classer et comparer selon différents critère : le application , le encodage graphique , le technique d' interaction , le avantage etles inconvénient de chaque approche . ce technique être présenter dans destableaux accompagner d' illustration graphiques 	Fouille visuelle de données en 3D et réalité virtuelle : état de l'art	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Gestion sémantique des droits d'accès au contenu: l'ontologie AMO	Dans cet article nous proposons une approche de la gestion des droitsd'accès pour les systèmes de gestion de contenu qui reposent sur les modèles ettechniques du web sémantique. Nous présentons l'ontologie AMO qui consiste(1) en un ensemble de classes et propriétés permettant d'annoter les ressourcesdont il s'agit de contrôler l'accès et (2) en une base de règles d'inférence modélisantla stratégie de gestion des droits à mettre en oeuvre. Appliquées sur la based'annotations des ressources, ces règles permettent de gérer les ressources selonune stratégie donnée. Cette modélisation garantit ainsi l'adaptabilité de l'ontologieà différentes stratégies de gestion des droits d'accès. Nous illustrons l'utilisationde l'ontologie AMO sur les documents du projet ANR ISICIL produitspar le wiki sémantique SweetWiki. Nous montrons comment les documents sontannotés avec AMO, quelles règles sont mises en oeuvre et quelles requêtes permettentle contrôle de l'accès aux documents.	Michel Buffa, Catherine Faron-Zucker, Anna Kolomoyskaya	http://editions-rnti.fr/render_pdf.php?p1&p=1001340	http://editions-rnti.fr/render_pdf.php?p=1001340	676	fr	fr	@unice.fr	gestion sémantique des droit d' accès au contenu : le ontologie AMO  Dans ce article nous proposer un approche de le gestion des droitsd'accès pour le système de gestion de contenu qui reposer sur le modèle ettechniques du web sémantique . Nous présenter le ontologie AMO qui consister ( 1 ) en un ensemble de classe et propriété permettre d' annoter le ressourcesdont il clr agir de contrôler le accès et ( 2 ) en un base de règle d' inférence modélisantla stratégie de gestion des droit à mettre en oeuvre . appliquer sur le based'annotations des ressource , ce règle permettre de gérer le ressource selonune stratégie donner . ce modélisation garantir ainsi le adaptabilité de le ontologieà différent stratégie de gestion des droit d' accès . Nous illustrer le utilisationde le ontologie AMO sur le document du projet ANR ISICIL produitspar le wiki sémantique SweetWiki . Nous montrer comment le document sontannotés avec AMO , quel règle être mettre en oeuvre et quel requête permettentle contrôle de le accès aux document . 	Gestion sémantique des droits d'accès au contenu: l'ontologie AMO	12
Revue des Nouvelles Technologies de l'Information	EGC	2010	Identifying the Presence of Communities in Complex Networks Through Topological Decomposition and Component Densities	The exponential growth of data in various fields such as Social Networksand Internet has stimulated lots of activity in the field of network analysisand data mining. Identifying Communities remains a fundamental technique toexplore and organize these networks. Few metrics are widely used to discoverthe presence of communities in a network. We argue that these metrics do nottruly reflect the presence of communities by presenting counter examples. Thisis because these metrics concentrate on local cohesiveness among nodes wherethe goal is to judge whether two nodes belong to the same community or viseversa. Thus loosing the overall perspective of the presence of communities in theentire network. In this paper, we propose a new metric to identify the presenceof communities in real world networks. This metric is based on the topologicaldecomposition of networks taking into account two important ingredients of realworld networks, the degree distribution and the density of nodes. We show theeffectiveness of the proposed metric by testing it on various real world data sets	Faraz Zaidi, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001286	http://editions-rnti.fr/render_pdf.php?p=1001286	677	en	en	@labri.fr	identify presence community complex network topological decomposition component density exponential growth datum various field social networksand internet stimulate lot activity field network analysisand data mining identify community remain fundamental technique toexplore organize network metric widely used discoverthe presence community network argue metric nottruly reflect presence community present counter example thisis metric concentrate local cohesiveness among node wherethe goal judge whether two node belong community viseversa thus loose overall perspective presence community theentire network paper propose new metric identify presenceof community real world network metric base topologicaldecomposition network take account two important ingredient realworld network degree distribution density node show theeffectiveness propose metric testing various real world data set	Identifying the Presence of Communities in Complex Networks Through Topological Decomposition and Component Densities	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	IncFDs: un nouvel algorithme d'inférence incrémentale des dépendances fonctionnelles	L'inférence des dépendances fonctionnelles est l'une des problématiquesles plus étudiées en bases de données. Elle a fait l'objet de plusieurstravaux qui ont proposé des algorithmes afin d'inférer, efficacement, les dépendancesfonctionnelles pour les utiliser dans différents domaines : administrationde bases de données, ré-ingénierie, optimisation des requêtes,etc. Toutefois,pour les application réelles, les bases de données sont évolutives et les relationssont fréquemment augmentées ou diminuées de tuples. Par conséquent, afin des'adapter à ce cadre dynamique, une solution consiste à appliquer l'un des algorithmes,disponibles dans la littérature, pour inférer les dépendances fonctionnelles,après chaque mise à jour. Cette solution étant coûteuse, nous proposons,dans cet article, d'inférer les dépendances fonctionnelles d'une manière incrémentale.À cet effet, nous introduisons un nouvel algorithme, appelé INCFDS, etnous évaluons ses performances par rapport à l'approche classique d'inférencedes dépendances fonctionnelles à partir d'une relation dynamique.	Ghada Gasmi	http://editions-rnti.fr/render_pdf.php?p1&p=1001310	http://editions-rnti.fr/render_pdf.php?p=1001310	678	fr	fr	@gmail.com	IncFDs : un nouveau algorithme d' inférence incrémentale des dépendance fonctionnelles  le inférence des dépendance fonctionnel être le un des problématiquesles plus étudier en base de donnée . Elle avoir faire le objet de plusieurstravaux qui avoir proposer un algorithme afin d' inférer , efficacement , le dépendancesfonctionnelles pour les utiliser dans différent domaine : administrationde base de donnée , ré-ingénierie , optimisation des requête , etc. toutefois , pour le application réel , le base de donnée être évolutif et le relationssont fréquemment augmenter ou diminuer de tuples . Par conséquent , afin des'adapter à ce cadre dynamique , un solution consister à appliquer le un des algorithme , disponible dans le littérature , pour inférer le dépendance fonctionnel , après chaque mise à jour . ce solution être coûteux , nous proposer , dans ce article , d' inférer le dépendance fonctionnel d' un manière incrémentale . À ce effet , nous introduire un nouveau algorithme , appeler INCFDS , etnous évaluer son performance par rapport à le approche classique d' inférencedes dépendance fonctionnel à partir d' un relation dynamique . 	IncFDs: un nouvel algorithme d'inférence incrémentale des dépendances fonctionnelles	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Indexation et recherche d'images à très grande échelle avec une AFC incrémentale et parallèle sur GPU	Nous présentons un nouvel algorithme incrémental et parallèled'analyse factorielle des correspondances (AFC) pour la recherche d'images àgrande échelle en utilisant le processeur de la carte graphique (GPU). L'AFCest adaptée à la recherche d'images par le contenu en utilisant des descripteurslocaux des images (SIFT). L'AFC permet de réduire le nombre de dimensionset de découvrir des thèmes qui permettent de diminuer le nombre d'images àparcourir et donc le temps de réponse d'une requête. Pour traiter de trèsgrandes bases d'images, nous présentons une version incrémentale et parallèled'AFC, puis nous utilisons ses indicateurs pour construire des fichiers inverséspour retrouver les images contenant les mêmes thèmes que l'image requête.Cette étape est elle aussi parallélisée sur GPU pour obtenir des réponsesrapides. Les résultats numériques sur la base de données d'images Nistér-Stewénius plongée dans 1 million d'images de FlickR montrent que notrealgorithme incrémental et parallèle est très significativement plus rapide que saversion standard	Nguyen-Khang Pham, François Poulet, Annie Morin, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1001281	http://editions-rnti.fr/render_pdf.php?p=1001281	679	fr	fr	@irisa.fr	indexation et recherche d' image à très grand échelle avec un AFC incrémentale et parallèle sur GPU  Nous présenter un nouveau algorithme incrémental et parallèled'analyse factoriel des correspondance ( AFC ) pour le recherche d' image àgrande échelle en utiliser le processeur de le carte graphique ( GPU ) . le AFCest adapter à le recherche d' image par le contenu en utiliser un descripteurslocaux des image ( SIFT ) . le AFC permettre de réduire le nombre de dimensionset de découvrir un thème qui permettre de diminuer le nombre d' image àparcourir et donc le temps de réponse d' un requête . Pour traiter de trèsgrandes base d' image , nous présenter un version incrémentale et parallèled'AFC , puis nous utiliser son indicateur pour construire un fichier inverséspour retrouver le image contenir le même thème que le image requêter . ce étape être elle aussi parallélisée sur GPU pour obtenir un réponsesrapides . le résultat numérique sur le base de donnée d' image Nistér-Stewénius plongée dans 1 million d' image de FlickR montrer que notrealgorithme incrémental et parallèle être très significativement plus rapide que saversion standard 	Indexation et recherche d'images à très grande échelle avec une AFC incrémentale et parallèle sur GPU	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Indice de complexité pour le tri et la comparaison de séquences catégorielles	Cet article1 propose un nouvel indice de la complexité de séquencescatégorielles. Bien que conçu pour des séquences représentant des trajectoiresbiographiques telles que celles rencontrées dans les sciences sociales, il s'appliqueà tous types de listes ordonnées d'états. L'indice prend en compte deuxaspects distincts, soit la complexité induite par l'ordonnancement des états successifsqui est mesurée par le nombre de transitions (changements d'état) et lacomplexité liée à la distribution des états dont rend compte l'entropie	Alexis Gabadinho, Gilbert Ritschard, Matthias Studer, Nicolas S. Müller	http://editions-rnti.fr/render_pdf.php?p1&p=1001267	http://editions-rnti.fr/render_pdf.php?p=1001267	680	fr	fr	@unige.ch	indice de complexité pour le tri et le comparaison de séquence catégorielles  ce article1 proposer un nouveau indice de le complexité de séquencescatégorielles . bien que concevoir pour un séquence représenter un trajectoiresbiographiques tel que celui rencontrer dans le science social , il clr appliqueà tout type de liste ordonner d' état . le indice prendre en compte deuxaspects distinct , soit le complexité induire par le ordonnancement des état successifsqui être mesurer par le nombre de transition ( changement d' état ) et lacomplexité lier à le distribution des état dont rendre compter le entropie 	Indice de complexité pour le tri et la comparaison de séquences catégorielles	7
Revue des Nouvelles Technologies de l'Information	EGC	2010	Inférence Bayesienne du Maximum d'Entropie pour le Diagnostic du Cancer		Fadi Dornaika, Fadi Chakik	http://editions-rnti.fr/render_pdf.php?p1&p=1001423	http://editions-rnti.fr/render_pdf.php?p=1001423	681	fr		@ehu.es, @ul.edu.lb	Inférence Bayesienne du Maximum d'Entropie pour le Diagnostic du Cancer 	Inférence Bayesienne du Maximum d'Entropie pour le Diagnostic du Cancer	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Intégration de Connaissances a Priori dans le Principe du Maximum d'Entropie	Cet article montre que si l'on dispose d'une connaissance a priori surle problème en main, l'intégration de cette dernière dans le processus d'apprentissaged'une machine intelligente pour des tâches de classification peut améliorerla performance de cette machine. Nous étudions l'effet de l'intégration de laconnaissance a priori de convexité sur le processus d'apprentissage du principedu Maximum d'Entropie (MaxEnt) en utilisant des exemples virtuels. Nous testonsles idées proposées sur un problème benchmark bien connu dans la littératuredes machines d'apprentissage, le problème de formes d'ondes de Breiman.Nous avons abouti à un taux d'erreur de généralisation de 15.57% qui est trèsproche du taux d'erreur théorique estimé par Breiman (14%).	Fadi Chakik, Fadi Dornaika	http://editions-rnti.fr/render_pdf.php?p1&p=1001356	http://editions-rnti.fr/render_pdf.php?p=1001356	682	fr	fr	@ehu.es, @ul.edu.lb	intégration de Connaissances avoir Priori dans le Principe du Maximum d' Entropie  ce article montrer que si le on disposer d' un connaissance avoir priori surle problème en main , le intégration de ce dernier dans le processus d' apprentissaged'une machine intelligent pour un tâche de classification pouvoir améliorerla performance de ce machine . Nous étudier le effet de le intégration de laconnaissance avoir priori de convexité sur le processus d' apprentissage du principedu Maximum d' Entropie ( MaxEnt ) en utiliser un exemple virtuel . Nous testonsles idée proposer sur un problème benchmark bien connaître dans le littératuredes machine d' apprentissage , le problème de forme d' onde de Breiman . Nous avoir aboutir à un taux d' erreur de généralisation de 15 .57 \% qui être trèsproche du taux d' erreur théorique estimer par Breiman ( 14 \% ) . 	Intégration de Connaissances a Priori dans le Principe du Maximum d'Entropie	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Intégration interactive de contraintes pour la réduction de dimensions et la visualisation	"Il existe aujourd'hui de nombreuses méthodes de réduction de dimensions,que ce soit dans un cadre supervisé ou non supervisé. L'un des intérêts deces méthodes est de pouvoir visualiser les données, avec pour objectif que lesobjets qui apparaissent ""visuellement"" proches soient similaires, dans un sensqui correspond aux connaissances d'un expert du domaine ou qui soit conformeaux informations de supervision. Nous nous plaçons ici dans un contexte semisuperviséoù des connaissances sont ajoutées de façon interactive : ces informationsseront apportées sous forme de contraintes exprimant les écarts entrela représentation observée et les connaissances d'un expert. Nous pourrons parexemple spécifier que deux objets proches dans l'espace d'observation sont enfait peu similaires, ou inversement. La méthode utilisée ici dérive de l'analyseen composantes principales (ACP), à laquelle nous proposons d'intégrer deuxtypes de contraintes. Nous présentons une méthode de résolution qui a été implémentéedans un logiciel offrant une représentation 3D des données et grâceauquel l'utilisateur peut ajouter des contraintes de manière interactive, puis visualiserles modifications induites par ces contraintes. Deux types d'expérimentationsont présentés, reposant respectivement sur un jeu de données synthétiqueet sur des jeux standards : ces tests montrent qu'une représentation de bonnequalité peut être obtenue avec un nombre limité de contraintes ajoutées."	Guillaume Cleuziou, Frédéric Moal, Lionel Martin, Matthieu Exbrayat	http://editions-rnti.fr/render_pdf.php?p1&p=1001320	http://editions-rnti.fr/render_pdf.php?p=1001320	683	fr	fr	@univ-orleans.fr	intégration interactif de contrainte pour le réduction de dimension et le visualisation  " Il exister aujourd' hui de nombreux méthode de réduction de dimension , que ce être dans un cadre superviser ou non superviser . . le un des intérêt deces méthode être de pouvoir visualiser le donnée , avec pour objectif que lesobjets qui apparaître " " visuellement " " proche être similaire , dans un sensqui correspondre aux connaissance d' un expert du domaine ou qui être conformeaux information de supervision . . Nous nous placer ici dans un contexte semisuperviséoù des connaissance être ajouter de façon interactif : ce informationsseront apporter sous forme de contrainte exprimer le écart entrela représentation observer et le connaissance d' un expert . . Nous pouvoir parexemple spécifier que deux objet proche dans le espace d' observation être enfait peu similaire , ou inversement . . le méthode utiliser ici dérive de le analyseen composante principal ( ACP ) , à laquelle nous proposer d' intégrer deuxtypes de contrainte . . Nous présenter un méthode de résolution qui avoir être implémentéedans un logiciel offrir un représentation 3D des donnée et grâceauquel le utilisateur pouvoir ajouter un contrainte de manière interactif , puis visualiserles modification induire par ce contrainte . . Deux type d' expérimentationsont présenter , reposer respectivement sur un jeu de donnée synthétiqueet sur un jeu standard : ce test montrer qu' un représentation de bonnequalité pouvoir être obtenir avec un nombre limiter de contrainte ajouter . " 	Intégration interactive de contraintes pour la réduction de dimensions et la visualisation	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Interrogation des résumés de flux de données	Les systèmes de gestion de flux de données (SGFD) ont été conçusafin de traiter une masse importante de données produites en ligne de façoncontinue. Etant donné que les ressources matérielles ne permettent pas de conservertoute cette volumétrie, seule la partie récente du flux est mémorisée dans lamémoire du SGFD. Ainsi, les requêtes évaluées par ces systèmes ne peuvent porterque sur les données les plus récentes du flux. Par conséquent, les SGFD actuelsne peuvent pas traiter des requêtes qui portent sur des périodes très longues.Nous proposons dans cet article, une approche permettant d'évaluer des requêtesqui portent sur une période plus longue que la mémoire du SGFD. Ces fenêtresfont appels à des données récentes et des données historisées. Nous présentonsle niveau logique de cette approche ainsi que son implantation sous le SGFD Esper.Une technique d'échantillonnage associée à une technique de fenêtre pointde repère est appliquée pour conserver une représentation compacte des donnéesdu flux.	Nesrine Gabsi, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001300	http://editions-rnti.fr/render_pdf.php?p=1001300	684	fr	fr	@telecom-paristech.fr, @orange-ftgroup.com	interrogation des résumé de flux de données  le système de gestion de flux de donnée ( SGFD ) avoir être conçusafin de traiter un masse important de donnée produire en ligne de façoncontinue . Etant donner que le ressource matériel ne permettre pas de conservertoute ce volumétrie , seul le partie récent du flux être mémoriser dans lamémoire du SGFD . ainsi , le requête évaluer par ce système ne pouvoir porterque sur le donnée le plus récent du flux . Par conséquent , le SGFD actuelsne pouvoir pas traiter un requête qui porter sur un période très long . Nous proposer dans ce article , un approche permettre d' évaluer un requêtesqui porter sur un période plus long que le mémoire du SGFD . ce fenêtresfont appel à un donnée récent et des donnée historiser . Nous présentonsle niveau logique de ce approche ainsi que son implantation sous le SGFD Esper . un technique d' échantillonnage associer à un technique de fenêtre pointde repère être appliquer pour conserver un représentation compact des donnéesdu flux . 	Interrogation des résumés de flux de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	K-WORDS LAB : un outil d'analyse des mots clés permettant d'explorer les dynamiques d'un domaine scientifique.		Audrey Baneyx, Philippe Breucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001376	http://editions-rnti.fr/render_pdf.php?p=1001376	685	fr		@ifris.org, @ifris.org	K-WORDS LAB : un outil d'analyse des mots clés permettant d'explorer les dynamiques d'un domaine scientifique. 	K-WORDS LAB : un outil d'analyse des mots clés permettant d'explorer les dynamiques d'un domaine scientifique.	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	KGRAM: une machine abstraite de graphes de connaissance	Cet article présente la machine abstraite de graphes de connaissanceKGRAM qui unifie les notions d'homomorphisme de graphe et de calcul de requêtestelles que celles du langage SPARQL sur des données RDF. KGRAMimplémente un ensemble extensible d'expressions qui définissent une famille delangages abstraits d'interrogation de graphes, GRAAL. Nous décrivons la sémantiquedynamique de GRAAL en Sémantique Naturelle et nous présentons lamachine abstraite KGRAM conçue comme l'interprète de GRAAL, qui implémenteles règles de sémantique naturelle du langage.	Olivier Corby, Catherine Faron-Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001328	http://editions-rnti.fr/render_pdf.php?p=1001328	686	fr	fr	@inria.fr, @unice.fr	KGRAM : un machine abstraire de graphe de connaissance  ce article présenter le machine abstraire de graphe de connaissanceKGRAM qui unifier le notion d' homomorphisme de graphe et de calcul de requêtestelles que celui du langage SPARQL sur un donnée RDF . KGRAMimplémente un ensemble extensible d' expression qui définir un famille delangages abstrait d' interrogation de graphe , GRAAL . Nous décrire le sémantiquedynamique de GRAAL en Sémantique Naturelle et nous présenter lamachine abstraire KGRAM concevoir comme le interprète de GRAAL , qui implémenteles règle de sémantique naturel du langage . 	KGRAM: une machine abstraite de graphes de connaissance	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	Le conflit dans la théorie des fonctions de croyance	Le conflit apparaît naturellement lorsque plusieurs sources d'informationsimparfaites sont en jeu. La théorie des fonctions de croyance offre unformalisme adapté à la fusion d'informations dans lequel la considération duconflit est centrale. Ce travail propose de revenir sur les différentes définitionsdu conflit dans cette théorie, tentant de les synthétiser et de montrer commentsupprimer ce conflit, ou bien comment en tenir compte lors de la combinaisondes informations.	Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001411	http://editions-rnti.fr/render_pdf.php?p=1001411	687	fr	fr	@ensieta.fr	le conflit dans le théorie des fonction de croyance  le conflit apparaître naturellement lorsque plusieurs source d' informationsimparfaites être en jeu . le théorie des fonction de croyance offrir unformalisme adapter à le fusion d' information dans lequel le considération duconflit être central . ce travail proposer de revenir sur le différent définitionsdu conflit dans ce théorie , tenter de les synthétiser et de montrer commentsupprimer ce conflit , ou bien comment en tenir compte lors de le combinaisondes information . 	Le conflit dans la théorie des fonctions de croyance	4
Revue des Nouvelles Technologies de l'Information	EGC	2010	Modèle de Langue à base de Concepts pour la Recherche d'Information	La majorité des modèles de langue appliqués à la recherched'information repose sur l'hypothèse d'indépendance des mots.Plus précisément, ces modèles sont estimés à partir des mots simplesapparaissant dans les documents sans considérer les éventuelles relationssémantiques et conceptuelles. Pour pallier ce problème, deux grandesapproches ont été explorées : la première intègre des dépendances d'ordresurfacique entre les mots, et la seconde repose sur l'utilisation des ressourcessémantiques pour capturer les dépendances entre les mots. Le modèle delangue que nous présentons dans cet article s'inscrit dans la seconde approche.Nous proposons d'intégrer les dépendances entre les mots en représentant lesdocuments et les requêtes par les concepts.	Lynda Said L'Hadj, Mohand Boughanem	http://editions-rnti.fr/render_pdf.php?p1&p=1001501	http://editions-rnti.fr/render_pdf.php?p=1001501	688	fr	fr	@esi.dz, @irit.fr	modèle de Langue à base de concept pour le recherche d' Information  le majorité des modèle de langue appliquer à le recherched'information reposer sur le hypothèse d' indépendance des mot . plus précisément , ce modèle être estimer à partir un mot simplesapparaissant dans le document sans considérer le éventuel relationssémantiques et conceptuel . Pour pallier ce problème , deux grandesapproches avoir être explorer : le premier intégrer un dépendance d' ordresurfacique entre le mot , et le second reposer sur le utilisation des ressourcessémantiques pour capturer le dépendance entre le mot . le modèle delangue que nous présenter dans ce article clr inscrire dans le second approche . Nous proposer d' intégrer le dépendance entre le mot en représenter lesdocuments et le requête par le concept . 	Modèle de Langue à base de Concepts pour la Recherche d'Information	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Modélisation et interrogation de données XML multidimensionnelles	XML étant devenu omniprésent et ses techniques de stockage et d'interrogationde plus en plus efficaces, le nombre de cas d'utilisations de ces technologiesaugmente tous les jours. Un sujet prometteur est l'intégration d'XML etdes entrepôts de données, dans laquelle une base de données XML native stockeles données multidimensionnelles et exécute des requêtes OLAP écrites à l'aidedu langage d'interrogation XML XQuery. Ce papier explore les questions quipeuvent survenir lors de l'implémentation d'un tel entrepôt de données XML.	Boris Verhaegen, Esteban Zimányi, Serge Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001327	http://editions-rnti.fr/render_pdf.php?p=1001327	689	fr	fr	@ulb.ac.be	modélisation et interrogation de donnée XML multidimensionnelles  xml être devenir omniprésent et son technique de stockage et d' interrogationde plus en plus efficace , le nombre de cas d' utilisation de ce technologiesaugmente tout le jour . un sujet prometteur être le intégration d' XML etdes entrepôt de donnée , dans laquelle un base de donnée XML natif stockeles donnée multidimensionnel et exécuter un requête OLAP écrire à le aidedu langage d' interrogation XML XQuery . ce papier explorer le question quipeuvent survenir lors de le implémentation d' un tel entrepôt de donnée XML . 	Modélisation et interrogation de données XML multidimensionnelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Mysins : Make Your Semantic INformation System		Anthony Ventresque, Thomas Cerqueus, Louis-Alexandre Celton, Gaëtan Hervouet, Damien Levin, Philippe Lamarre, Sylvie Cazalens	http://editions-rnti.fr/render_pdf.php?p1&p=1001382	http://editions-rnti.fr/render_pdf.php?p=1001382	690	en		@univ-nantes.fr, @univ-nantes.fr, @univ-nantes.fr	Mysins : Make Your Semantic INformation System 	Mysins : Make Your Semantic INformation System	5
Revue des Nouvelles Technologies de l'Information	EGC	2010	Objective Novelty of Association Rules: Measuring the Confidence Boost1	"On sait bien que la confiance des régles d'association n'est pas vraimentsatisfaisant comme mésure d'interêt. Nous proposons, au lieu de la substituerpar des autres mésures (soit, en l'employant de façon conjointe a desautres mésures), évaluer la nouveauté de chaque régle par comparaison de saconfiance par rapport á des régles plus fortes qu'on trouve au même ensemblede données. C'est á dire, on considère un seuil ""relative"" de confiance au lieu duseuil absolute habituel. Cette idée se précise avec la magnitude du ""confidenceboost"", mésurant l'increment rélative de confiance prés des régles plus fortes.Nous prouvons que nôtre proposte peut remplacer la ""confidence width"" et leblockage de régles employés a des publications précedentes."	José L Balcazar 	http://editions-rnti.fr/render_pdf.php?p1&p=1001308	http://editions-rnti.fr/render_pdf.php?p=1001308	691	en	fr	@unican.es	" On savoir bien que le confiance des régles d' association n' être pas vraimentsatisfaisant comme mésure d' interêt . . Nous proposer , au lieu de le substituerpar des autre mésures ( soit , en l' employer de façon conjoindre avoir desautres mésures ) , évaluer le nouveauté de chaque régle par comparaison de saconfiance par rapport á des régles plus fort qu' on trouver au même ensemblede donnée . . C' être á dire , on considérer un seuil " " relatif " " de confiance au lieu duseuil absolute habituel . . ce idée clr préciser avec le magnitude du " " confidenceboost " " , mésurant le increment rélative de confiance prés des régles plus fort . . Nous prouver que nôtre proposte pouvoir remplacer le " " confidence width " " et leblockage de régles employé avoir un publication précedentes . " 	Objective Novelty of Association Rules: Measuring the Confidence Boost1	4
Revue des Nouvelles Technologies de l'Information	EGC	2010	OSOM : un algorithme de construction de cartes topologiques recouvrantes	Les modèles de classification recouvrante ont montré leur capacité àgénérer une organisation plus fidèle aux données tout en conservant la simplificationattendue par une structuration en classes strictes. Par ailleurs les modèlesneuronaux non-supervisés sont plébiscités lorsqu'il s'agit de visualiser la structurede classes.Nous proposons dans cette étude d'étendre les cartes auto-organisatrices traditionnellesaux cartes auto-organisatrices recouvrantes. Nous montrons que cettenouvelle structure apporte des solutions à certaines problématiques spécifiquesen classification recouvrante (nombre de classes, complexité, cohérence des recouvrements).L'algorithme OSOM s'inspire de la version recouvrante des nuées dynamiqueset de l'approche de Kohonen pour générer de telles cartes recouvrantes. Nousdiscutons du modèle proposé d'un point de vue théorique (fonction d'énergieassociée, complexité, ...). Enfin nous présentons un cadre d'évaluation généraleque nous utilisons pour valider les résultats obtenus sur des données réelles.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001272	http://editions-rnti.fr/render_pdf.php?p=1001272	692	fr	fr	@univ-orleans.fr	OSOM : un algorithme de construction de carte topologique recouvrantes  le modèle de classification recouvrante avoir montrer son capacité àgénérer un organisation plus fidèle aux donnée tout en conserver le simplificationattendue par un structuration en classe strict . Par ailleurs le modèlesneuronaux non- superviser être plébisciter lorsqu' il clr agir de visualiser le structurede classe . Nous proposer dans ce étude d' étendre le carte auto- organisatrice traditionnellesaux carte auto- organisatrice recouvrantes . Nous montrer que cettenouvelle structure apporter un solution à certain problématique spécifiquesen classification recouvrante ( nombre de classe , complexité , cohérence des recouvrement ) . le algorithme OSOM clr inspirer de le version recouvrante des nuée dynamiqueset de le approche de Kohonen pour générer de tel carte recouvrantes . Nousdiscutons du modèle proposer d' un point de vue théorique ( fonction d' énergieassociée , complexité , ... ) . enfin nous présenter un cadre d' évaluation généraleque nous utiliser pour valider le résultat obtenir sur un donnée réel . 	OSOM : un algorithme de construction de cartes topologiques recouvrantes	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Pattern Mining: The Past, Present, and Future	Pattern mining is one of the fundamental techniques in data mining. As one increases thecomplexity of the pattern types, from subsets, to subsequences, subtrees, and subgraphs, onediscovers potentially more informative patterns. In this talk I will offer a tour of the past andthe present research landscape in this area, and I'll conclude with some thoughts on directionsfor the future	Mohammed Zaki	http://editions-rnti.fr/render_pdf.php?p1&p=1001258	http://editions-rnti.fr/render_pdf.php?p=1001258	693	en	en	@cs.rpi.edu	pattern mining past present future pattern mine one fundamental technique datum mining one increase thecomplexity pattern type subset subsequence subtree subgraph onediscover potentially informative pattern talk offer tour past andthe present research landscape area i ll conclude thought directionsfor future	Pattern Mining: The Past, Present, and Future	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	PCAR : Nouvelle Approche de Génération de Règles d'Association Cycliques	Les règles d'association cycliques vise la découverte de nouvelles relationsentre des produits qui varient d'une façon régulièrement cyclique dans letemps. Dans ce cadre, nous introduisons, un nouvel algorithme nommé PCARcaractérisé par sa performance et son aspect incrémental. L'étude empirique quenous avons menée montre la robustesse et l'efficacité de notre algorithme proposévs. ceux de la littérature	Mohamed Salah Gouider, Eya Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1001415	http://editions-rnti.fr/render_pdf.php?p=1001415	694	fr	fr	@gmail.com, @isg.rnu.tn	PCAR : nouveau approche de génération de règle d' association Cycliques  le règle d' association cyclique viser le découverte de nouveau relationsentre des produit qui varier d' un façon régulièrement cyclique dans letemps . Dans ce cadre , nous introduire , un nouveau algorithme nommer PCARcaractérisé par son performance et son aspect incrémental . le étude empirique quenous avoir mener montre le robustesse et le efficacité de son algorithme proposévs . celui de le littérature 	PCAR : Nouvelle Approche de Génération de Règles d'Association Cycliques	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	PGP-mc : extraction parallèle efficace de motifs graduels	"Initialement utilisés pour les systèmes de commande, les règles et motifsgraduels (de la forme ""plus une personne est âgée, plus son salaire est élevé"")trouvent de très nombreuses applications, par exemple dans les domainesde la biologie, des données en flots (e.g. issues de réseaux de capteurs), etc. Trèsrécemment, des algorithmes ont été proposés pour extraire automatiquementde tels motifs. Cependant, même si certains d'entre eux ont permis des gainsde performance importants, les algorithmes restent coûteux et ne permettentpas de traiter efficacement les bases de données réelles souvent très volumineuses(en nombre de lignes et/ou nombre d'attributs). Nous proposons doncdans cet article une méthode originale de recherche de ces motifs utilisant lemulti-threading pour exploiter au mieux les multiples coeurs présents dans laplupart des ordinateurs et serveurs actuels. L'efficacité de cette approche est validéepar une étude expérimentale."	Anne Laurent, Benjamin Négrevergne, Nicolas Sicard, Alexandre Termier	http://editions-rnti.fr/render_pdf.php?p1&p=1001336	http://editions-rnti.fr/render_pdf.php?p=1001336	695	fr	fr	@lirmm.fr, @efrei.fr, @imag.fr, @imag.fr	PGP-mc : extraction parallèle efficace de motif graduels  " initialement utiliser pour le système de commande , le règle et motifsgraduels ( de le forme " " plus un personne être âgé , plus son salaire être élever " " ) trouver de très nombreux application , par exemple dans le domainesde le biologie , un donnée en flot ( e.g. issue de réseau de capteur ) , etc. Trèsrécemment , un algorithme avoir être proposer pour extraire automatiquementde tel motif . . cependant , même si certains d' entre lui avoir permettre des gainsde performance important , le algorithme rester coûteux et ne permettentpas de traiter efficacement le base de donnée réel souvent très volumineux ( en nombre de ligne et nombre d' attribut ) . . Nous proposer doncdans ce article un méthode original de recherche de ce motif utiliser lemulti-threading pour exploiter au mieux le multiple coeur présent dans laplupart un ordinateur et serveur actuel . . le efficacité de ce approche être validéepar un étude expérimental . " 	PGP-mc : extraction parallèle efficace de motifs graduels	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Prédiction de séries temporelles et applications à l'analyse de séquences vidéo		Rémi Auguste, Ahmed El Ghini, Ioan Marius Bilasco, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001468	http://editions-rnti.fr/render_pdf.php?p=1001468	696	fr		@lifl.fr	Prédiction de séries temporelles et applications à l'analyse de séquences vidéo 	Prédiction de séries temporelles et applications à l'analyse de séquences vidéo	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	PretopoLib: la librairie JAVA de la Prétopologie	PretopoLib est une librairie JAVA implémentant les concepts de laprétopologie. Son intérêt réside dans la représentation de structures de donnéespermettant la manipulation des données par des opérations ensemblistes.Celle-ci offre un cadre de développement d'algorithmes efficaces pour la fouillede données, l'apprentissage topologique et la modélisation des systèmes complexes.	Sofiane Ben Amor, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1001405	http://editions-rnti.fr/render_pdf.php?p=1001405	697	fr	fr	@ephe.sorbonne.fr	PretopoLib : le librairie JAVA de le Prétopologie  PretopoLib être un librairie JAVA implémenter le concept de laprétopologie . son intérêt résider dans le représentation de structure de donnéespermettant le manipulation des donnée par un opération ensembliste . Celle _-ci offrir un cadre de développement d' algorithme efficace pour le fouillede donner , le apprentissage topologique et le modélisation des système complexe . 	PretopoLib: la librairie JAVA de la Prétopologie	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Proposition d'opérateurs OLAP pour un modèle multidimensionnel à base d'objets complexes		Doulkifli Boukraâ, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001407	http://editions-rnti.fr/render_pdf.php?p=1001407	698	fr		@esi.dz, @univ-lyon2.fr	Proposition d'opérateurs OLAP pour un modèle multidimensionnel à base d'objets complexes 	Proposition d'opérateurs OLAP pour un modèle multidimensionnel à base d'objets complexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Proposition d'une méthode de classification associative adaptative	La classification associative est une méthode de prédiction à base derègles issue de la fouille de règles d'association. Cette méthode est particulièrementintéressante car elle recherche de façon exhaustive les règles d'associationpertinentes qu'elle filtre pour ne garder que les règles d'association de classe(celles admettant pour conséquent une modalité de classe), qui sont utiliséescomme classifieur. Les connaissances produites sont ainsi directement interprétables.Des études antérieures montrent les inconvénients de cette approche,qu'il s'agisse de la génération massive de règles non utilisées ou de la mauvaiseprédiction de la classe minoritaire lorsque les classes sont déséquilibrées.Nous proposons une approche originale du type boosting de règles d'associationde classes qui utilise comme classifieur faible une base de règles significativesconstruites par un algorithme de génération d'itemsets fréquents qui se limiteà l'extraction des seules règles de classe significatives et qui prend en comptele déséquilibre des données. Des comparaisons avec d'autres méthodes de classificationassociative montrent que notre approche améliore la précision et lerappel.	Emna Bahri, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001346	http://editions-rnti.fr/render_pdf.php?p=1001346	699	fr	fr	@univ-lyon2.fr	proposition d' un méthode de classification associatif adaptative  le classification associatif être un méthode de prédiction à base derègles issue de le fouille de règle d' association . ce méthode être particulièrementintéressante car elle rechercher de façon exhaustif le règle d' associationpertinentes qu' elle filtrer pour ne garder que le règle d' association de classe ( celui admettre pour conséquent un modalité de classe ) , qui être utiliséescomme classifieur . le connaissance produire être ainsi directement interprétable . un étude antérieur montrer le inconvénient de ce approche , qu' il clr agir de le génération massif de règle non utiliser ou de le mauvaiseprédiction de le classe minoritaire lorsque le classe être déséquilibrer . Nous proposer un approche original du type boosting de règle d' associationde classe qui utiliser comme classifieur faible un base de règle significativesconstruites par un algorithme de génération d' itemsets fréquent qui clr limiteà le extraction des seul règle de classe significatif et qui prendre en comptele déséquilibre des donnée . un comparaison avec un autre méthode de classificationassociative montrer que son approche améliorer le précision et lerappel . 	Proposition d'une méthode de classification associative adaptative	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Protein Graph Repository	Protein Graph Repository (PGR) est i, outil bioinformatique sur le web permettant d'obtenir une nouvelle representation de protéines sous la forme de graphes d'acides aminés, une représentation plus simple et plus facile à étudier par les moyens informatiques et statistiques dédiés aux graphes. La génération des graphes est faite à partir d'un parseur appliqué sur des fichiers des protéines PDB extraits de la base Protein Data Bank et en precisant les parametres et la methode a utiliser. Les graphes generes sont ensuite enregistres dans un entrepot doté de moyens de recherche, de filtrage et de telechargement. PGR peut etre provisoirement consulte à l'adresse http://www.enode-edition.com/pgr/, il est spécialement dédié aux recherches intéressées à l'étude de données protéiques sous la forme de graphes et permettra donc de fournir des échantillons pour des travaux expérimentaux.	Wajdi Dhifli, Rabie Saidi	http://editions-rnti.fr/render_pdf.php?p1&p=1001403	http://editions-rnti.fr/render_pdf.php?p=1001403	700	en	fr	@gmail.com, @isima	Protein Graph Repository ( PGR ) être i , outil bioinformatique sur le web permettre d' obtenir un nouveau representation de protéine sous le forme de graphe d' acide aminer , un représentation plus simple et plus facile à étudier par le moyen informatique et statistique dédier aux graphe . le génération des graphe être faire à partir d' un parseur appliquer sur un fichier des protéine PDB extrait de le base Protein Data Bank et en precisant le parametres et le methode avoir utiliser . le graphe generes être ensuite enregistrer dans un entrepot doter de moyen de recherche , de filtrage et de telechargement . PGR pouvoir etre provisoirement consulter à le adresse http_:_ , il être spécialement dédier aux recherche intéresser à le étude de donnée protéique sous le forme de graphe et permettre donc de fournir un échantillon pour un travail expérimental . 	Protein Graph Repository	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Recent Advances in Partitioning Clustering Algorithms for Interval-Valued Data		Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001260	http://editions-rnti.fr/render_pdf.php?p=1001260	701	en		@cin.ufpe.br	Recent Advances in Partitioning Clustering Algorithms for Interval-Valued Data 	Recent Advances in Partitioning Clustering Algorithms for Interval-Valued Data	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Recherche sémantique sur le Web basée sur l'ontologie Modulaire et le raisonnement à base de cas	Dans ce papier, nous présentons une approche de recherche sémantiquebasée sur les ontologies modulaires et le raisonnement à base de cas(RaPC). Un cas représente l'ensemble des requêtes similaires associées à leursrésultats pertinents. Les ontologies modulaires sont utilisées pour représenteret indexer les cas qui sont construits sur la base des requêtes antérieures et lesrésultats pertinents sélectionnés par les utilisateurs. La similarité à based'ontologies est utilisée pour retrouver les cas similaires à la requête utilisateuret pour fournir à celui-ci des propositions de reformulation de requêtes correspondantsà son besoin. La principale contribution de ce travail réside dans l'utilisationd'un mécanisme de RaPC et une représentation ontologique à deuxfins: l'amélioration de la recherche sémantique et l'enrichissement d'ontologiesà partir de cas. L'expérimentation de l'approche proposée montre que la précisionet le rappel des résultats se sont nettement améliorés.	Nesrine Ben Mustapha, Hajer Baazaoui Zghal, Marie-Aude Aufaure, Henda Ben Ghézala	http://editions-rnti.fr/render_pdf.php?p1&p=1001457	http://editions-rnti.fr/render_pdf.php?p=1001457	702	fr	fr	@riadi.rnu.tn, @ecp.fr	recherche sémantique sur le Web baser sur le ontologie Modulaire et le raisonnement à base de cas  Dans ce papier , nous présenter un approche de recherche sémantiquebasée sur le ontologie modulaire et le raisonnement à base de cas ( RaPC ) . un cas représenter le ensemble des requête similaire associer à leursrésultats pertinent . le ontologie modulaire être utiliser pour représenteret indexer le cas qui être construire sur le base des requête antérieur et lesrésultats pertinent sélectionner par le utilisateur . le similarité à based'ontologies être utiliser pour retrouver le cas similaire à le requête utilisateuret pour fournir à celui _-ci des proposition de reformulation de requête correspondantsà son besoin . le principal contribution de ce travail résider dans le utilisationd'un mécanisme de RaPC et un représentation ontologique à deuxfins : le amélioration de le recherche sémantique et le enrichissement d' ontologiesà partir de cas . le expérimentation de le approche proposer montre que le précisionet le rappel des résultat clr être nettement améliorer . 	Recherche sémantique sur le Web basée sur l'ontologie Modulaire et le raisonnement à base de cas	
Revue des Nouvelles Technologies de l'Information	EGC	2010	Reconnaissance de concepts basée sur l'apprentissage		Wahiba Ben Abdessalem Karaa, Bilel Bouchamia	http://editions-rnti.fr/render_pdf.php?p1&p=1001410	http://editions-rnti.fr/render_pdf.php?p=1001410	703	fr		@isg.rnu.tn, @hotmail.com	Reconnaissance de concepts basée sur l'apprentissage 	Reconnaissance de concepts basée sur l'apprentissage	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Réduction bi-directionnelle d'images - Vers une méthode d'extraction de caractéristiques multi-niveaux	Inspiré des performances du cerveau humain à identifier les élémentspar la vue, le problème de la réduction de la dimension dans le domaine de laperception visuelle consiste à extraire une quantité réduite des caractéristiquesd'un ensemble d'images afin de les identifier.Ce papier présente une approche innovante bi-directionnelle d'extraction de caractéristiquesd'images fondée sur l'utilisation partielle d'une méthode spatiotemporelle.Les expériences numériques appliquées sur 70000 images représentantdes chiffres écrits à la main ainsi que sur 698 images illustrant un visagesous différentes postures démontrent l'efficacité de notre approche à fortementréduire la dimension tout en conservant les relations intelligibles entre les objetsdes données, permettant même d'obtenir une meilleure classification à partir desversions réduites des images qu'à partir des versions originales	Marc Joliveau	http://editions-rnti.fr/render_pdf.php?p1&p=1001279	http://editions-rnti.fr/render_pdf.php?p=1001279	704	fr	fr	@cirrelt.ca	réduction bi-directionnelle d' image - Vers un méthode d' extraction de caractéristique multi-niveaux  inspirer des performance du cerveau humain à identifier le élémentspar le vue , le problème de le réduction de le dimension dans le domaine de laperception visuel consister à extraire un quantité réduire des caractéristiquesd'un ensemble d' image afin de les identifier . ce papier présenter un approche innovant bi-directionnelle d' extraction de caractéristiquesd'images fonder sur le utilisation partiel d' un méthode spatiotemporelle . le expérience numérique appliquer sur 70000 image représentantdes chiffre écrit à le main ainsi que sur 698 image illustrer un visagesous différent posture démontrer le efficacité de son approche à fortementréduire le dimension tout en conserver le relation intelligible entre le objetsdes donner , permettre même d' obtenir un meilleur classification à partir desversions réduire des image qu' à partir un version originales 	Réduction bi-directionnelle d'images - Vers une méthode d'extraction de caractéristiques multi-niveaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	REGLO : une nouvelle stratégie pour résumer un flux de séries temporelles	Les flux de séries temporelles sont aujourd'hui produits dans de nombreuxdomaines comme la finance (Zhu et Shasha (2002)), la surveillance deréseaux (Borgne et al. (2007); Airoldi et Faloutsos (2004)), la gestion de l'historiquedes usages fréquents (Giannella et al. (2003); Teng et al. (2003)), etc.Résumer de tels flux est devenu un domaine important qui permet de surveilleret d'enregistrer des informations fiables sur les séries observées. À ce jour, lamajorité des algorithmes de ce domaine s'est concentrée sur des résumés séparéset indépendants (Giannella et al. (2003); Zhu et Shasha (2002); Chen et al.(2002)), en accordant à chaque série le même espace en mémoire. Toutefois, lagestion de cet espace mémoire est un sujet important pour les flux de donnéeset une stratégie accordant la même quantité de mémoire à chaque série n'est pasforcément appropriée. Dans cet article, nous considérons que les séries doiventêtre en compétition vis à vis de l'espace mémoire, selon leur besoin de précision.Ainsi, nous proposons : (1) une stratégie de gestion de l'espace mémoireoptimisée et (2) une nouvelle méthode de résumé des séries temporelles par approximation.Dans ce but, nous observons à la fois l'erreur globale et les erreurslocales. La répartition de la mémoire suit les étapes suivantes : (1) recherchede la séquence la mieux représentée et (2) recherche de la partie à compresseren minimisant l'erreur. Nos expérimentations sur des données réelles montrentl'efficacité et la pertinence de notre approche.	Florent Masseglia, Alice Marascu, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001294	http://editions-rnti.fr/render_pdf.php?p=1001294	705	fr	fr	@inria.fr	REGLO : un nouveau stratégie pour résumer un flux de série temporelles  le flux de série temporel être aujourd' hui produire dans de nombreuxdomaines comme le finance ( Zhu et Shasha ( 2002 ) ) , le surveillance deréseaux ( borgne et al. ( 2007 ) ; Airoldi et Faloutsos ( 2004 ) ) , le gestion de le historiquedes usage fréquent ( Giannella et al. ( 2003 ) ; Teng et al. ( 2003 ) ) , etc. Résumer de tel flux être devenir un domaine important qui permettre de surveilleret d' enregistrer un information fiable sur le série observer . À ce jour , lamajorité des algorithme de ce domaine clr être concentrer sur un résumé séparéset indépendant ( Giannella et al. ( 2003 ) ; Zhu et Shasha ( 2002 ) ; Chen et al. ( 2002 ) ) , en accorder à chaque série le même espace en mémoire . toutefois , lagestion de ce espace mémoire être un sujet important pour le flux de donnéeset un stratégie accorder le même quantité de mémoire à chaque série n' être pasforcément approprier . Dans ce article , nous considérer que le série doiventêtre en compétition vis à vis de le espace mémoire , selon son besoin de précision . ainsi , nous proposer : ( 1 ) un stratégie de gestion de le espace mémoireoptimisée et ( 2 ) un nouveau méthode de résumé des série temporel par approximation . Dans ce but , nous observer à le foi le erreur global et le erreurslocales . le répartition de le mémoire suivre le étape suivant : ( 1 ) recherchede le séquence le mieux représenter et ( 2 ) recherche de le partie à compresseren minimiser le erreur . son expérimentation sur un donnée réel montrentl'efficacité et le pertinence de son approche . 	REGLO : une nouvelle stratégie pour résumer un flux de séries temporelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Regrouper les données textuelles et nommer les groupes à l'aide de classes recouvrantes	Organiser les données textuelles et en tirer du sens est un défi majeuraujourd'hui. Ainsi, lorsque l'on souhaite analyser un débat en ligne ou unforum de discussion, on voudrait pouvoir rapidement voir quels sont les principauxthèmes abordés et la manière dont la discussion se structure autour d'eux.Pour cela, et parce que un même texte peut être associé à plusieurs thèmes, nousproposons une méthode originale pour regrouper les données textuelles en autorisantles chevauchements et pour nommer chaque groupe de manière lisible.La contribution principale de cet article est une méthode globale qui permet deréaliser toute la chaîne, partant des données textuelles brutes jusqu'à la caractérisationdes groupes à un niveau sémantique qui dépasse le simple ensemble demots.	Marian-Andrei Rizoiu, Julien Velcin, Jean-Hugues Chauchat	http://editions-rnti.fr/render_pdf.php?p1&p=1001361	http://editions-rnti.fr/render_pdf.php?p=1001361	706	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr, @univ-lyon2.fr	regrouper le donnée textuel et nommer le groupe à le aide de classe recouvrantes  organiser le donnée textuel et en tirer du sens être un défi majeuraujourd'hui . ainsi , lorsque le on souhaiter analyser un débat en ligne ou unforum de discussion , on vouloir pouvoir rapidement voir quels être le principauxthèmes aborder et le manière dont le discussion clr structure autour d' lui . Pour cela , et parce que un même texte pouvoir être associer à plusieurs thème , nousproposons un méthode original pour regrouper le donnée textuel en autorisantles chevauchement et pour nommer chaque groupe de manière lisible . le contribution principal de ce article être un méthode global qui permettre deréaliser tout le chaîne , partir un donnée textuel brut jusqu' à le caractérisationdes groupe à un niveau sémantique qui dépasser le simple ensemble demots . 	Regrouper les données textuelles et nommer les groupes à l'aide de classes recouvrantes	6
Revue des Nouvelles Technologies de l'Information	EGC	2010	Requêtes skyline avec prise en compte des préférences utilisateurs pour des données volumineuses	"Appréhender, parcourir des données ou des connaissances reste unetâche difficile en particulier lorsque les utilisateurs sont confrontés à de gros volumesde données. De nombreux travaux se sont intéressés à extraire des points""skylines"" comme outil de restitution. La prise en compte des préférences a retenul'attention des travaux les plus récents mais les solutions existantes restenttrès consommatrices en terme de stockage d'informations additionnelles afind'obtenir des délais raisonnables de réponse aux requêtes. Notre proposition,EC2Sky (Efficient computation of compromises), se focalise sur deux points :(1) comment répondre efficacement à des requêtes de type skyline en présencede préférences utilisateurs malgré de gros volumes de données (aussi bien enterme de dimensions que de préférences) ; (2) comment restituer les connaissancesles plus pertinentes en soulignant les compromis associés aux préférencesspécifiées."	Tassadit Bouadi, Sandra Bringay, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001325	http://editions-rnti.fr/render_pdf.php?p=1001325	707	fr	fr	@lirmm.fr, @cemagref.fr	requête skyline avec prise en compte des préférence utilisateur pour un donnée volumineuses  " Appréhender , parcourir un donnée ou des connaissance rester unetâche difficile en particulier lorsque le utilisateur être confronter à un gros volumesde donner . . un nombreux travail clr être intéresser à extraire un point " " skylines " " comme outil de restitution . . le prise en compte des préférence avoir retenul'attention des travail le plus récent mais le solution existant restenttrès consommateur en terme de stockage d' information additionnel afind'obtenir des délai raisonnable de réponse aux requête . . son proposition , EC2Sky ( Efficient computation of compromettre ) , clr focaliser sur deux point : ( 1 ) comment répondre efficacement à un requête de type skyline en présencede préférence utilisateur malgré un gros volume de donnée ( aussi bien enterme de dimension que de préférence ) ; ; ( 2 ) comment restituer le connaissancesles plus pertinent en souligner le compromis associer aux préférencesspécifiées . " 	Requêtes skyline avec prise en compte des préférences utilisateurs pour des données volumineuses	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Résumé généraliste de flux de données	Lorsque le volume des données est trop important pour qu'elles soient stockéesdans une base de données, ou lorsque leur fréquence de production est élevée, les Systèmesde Gestion de Flux de Données (SGFD) permettent de capturer des flux d'enregistrementsstructurés et de les interroger à la volée par des requêtes permanentes (exécutées de façoncontinue). Mais les SGFD ne conservent pas l'historique des flux qui est perdu à jamais.Cette communication propose une définition formelle de ce que devrait être un résumé généralistede flux de données. La notion de résumé généraliste est liée à la capacité de répondreà des requêtes variées et de réaliser des tâches variées de fouille de données, en utilisant lerésumé à la place du flux d'origine. Une revue de plusieurs approches de résumés est ensuiteréalisée dans le cadre de cette définition.	Christine Potier, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001301	http://editions-rnti.fr/render_pdf.php?p=1001301	708	fr	fr	@telecom-paristech.fr	résumé généraliste de flux de données  Lorsque le volume des donnée être trop important pour qu' elles être stockéesdans un base de donnée , ou lorsque son fréquence de production être élever , le Systèmesde Gestion de Flux de Données ( SGFD ) permettre de capturer un flux d' enregistrementsstructurés et de les interroger à le volé par un requête permanent ( exécuter de façoncontinue ) . Mais le SGFD ne conserver pas le historique des flux qui être perdre à jamais . ce communication proposer un définition formel de ce que devoir être un résumé généralistede flux de donnée . le notion de résumé généraliste être lier à le capacité de répondreà des requête varier et de réaliser un tâche varier de fouille de donnée , en utiliser lerésumé à le place du flux d' origine . un revue de plusieurs approche de résumé être ensuiteréalisée dans le cadre de ce définition . 	Résumé généraliste de flux de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	RSS Merger		Fekade Getahun, Richard Chbeir	http://editions-rnti.fr/render_pdf.php?p1&p=1001396	http://editions-rnti.fr/render_pdf.php?p=1001396	709	en			RSS Merger 	RSS Merger	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	SALINES : un automate au service de l'extraction de motifs séquentiels multidimensionnels	Les entrepôts de données occupent aujourd'hui une place centrale dans le processus décisionnel.Outre leur consultation, une des finalités des entrepôts est de servir de socle aux techniquesde fouilles de données. Malheureusement, les approches existantes exploitent peu les particularitésdes entrepôts (multidimensionnalité, hiérarchies et données historiques). Parmi ces méthodes, l'extractionde motifs séquentiels multidimensionnels a récemment été étudiée. Nous montrons dans cetarticle que ces dernières ne tirent pas pleinement profit des hiérarchies et ne découvrent par conséquentqu'une partie seulement des motifs qualitativement intéressants. Nous proposons alors uneméthode d'extraction de motifs séquentiels multidimensionnels basée sur un automate et extrayantde nouveaux motifs. Les différentes expérimentations menées sur des jeux de données synthétiquesattestent des bonnes performances de notre proposition.	Yoann Pitarch, Lionel Vinceslas, Anne Laurent, Pascal Poncelet, Jean-Emile Symphor	http://editions-rnti.fr/render_pdf.php?p1&p=1001265	http://editions-rnti.fr/render_pdf.php?p=1001265	710	fr	fr	@lirmm.fr, @univ-ag.fr	saline : un automate au service de le extraction de motif séquentiel multidimensionnels  le entrepôt de donnée occuper aujourd' hui un place central dans le processus décisionnel . Outre son consultation , un des finalité des entrepôt être de servir de socle aux techniquesde fouille de donnée . malheureusement , le approche existant exploiter peu le particularitésdes entrepôt ( multidimensionnalité , hiérarchie et donnée historique ) . Parmi ce méthode , le extractionde motif séquentiel multidimensionnel avoir récemment être étudier . Nous montrer dans cetarticle que ce dernier ne tirer pas pleinement profit des hiérarchie et ne découvrir par conséquentqu'une partie seulement des motif qualitativement intéressant . Nous proposer alors uneméthode d' extraction de motif séquentiel multidimensionnel baser sur un automate et extrayantde nouveau motif . le différent expérimentation mener sur un jeu de donnée synthétiquesattestent des bon performance de son proposition . 	SALINES : un automate au service de l'extraction de motifs séquentiels multidimensionnels	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Sélection par entropie de descripteurs textuels pour la catégorisation de documents		Christophe Moulin, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001406	http://editions-rnti.fr/render_pdf.php?p=1001406	711	fr		@univ-st-etienne.fr	Sélection par entropie de descripteurs textuels pour la catégorisation de documents 	Sélection par entropie de descripteurs textuels pour la catégorisation de documents	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Self-Clustering for Identification of Customer Purchase Behaviours	La segmentation d'une base client peut avoir différents objectifs etplusieurs segmentation peuvent être utiles pour décrire les clients ou pour s'adapteravec les stratégies commerciales d'une entreprise. Dans ce papier, nous présentonsun schéma expérimental visant à proposer un ensemble de segmentationsalternatives. Ces segmentations sont produites sur des données réelles par latransformation des données initiales, la génération et la sélection de différentessegmentations.	Guillem Lefait, Gilles Goncalves, M. Tahar Kechadi	http://editions-rnti.fr/render_pdf.php?p1&p=1001273	http://editions-rnti.fr/render_pdf.php?p=1001273	712	en	fr	@ucd.ie, @ucd.ie, @univ-artois.fr	le segmentation d' un base client pouvoir avoir différent objectif etplusieurs segmentation pouvoir être utile pour décrire le client ou pour clr adapteravec le stratégie commercial d' un entreprise . Dans ce papier , nous présentonsun schéma expérimental viser à proposer un ensemble de segmentationsalternatives . ce segmentation être produire sur un donnée réel par latransformation des donnée initial , le génération et le sélection de différentessegmentations . 	Self-Clustering for Identification of Customer Purchase Behaviours	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	SequencesViewer : comment rendre accessible des motifs séquentiels de gènes trop nombreux	Les techniques d'extraction de connaissances appliquées aux gros volumesde données, issus de l'analyse de puces ADN, permettent de découvrirdes connaissances jusqu'alors inconnues. Or, ces techniques produisent de trèsnombreux résultats, difficilement exploitables par les experts. Nous proposonsun outil dédié à l'accompagnement de ces experts dans l'appropriation et l'exploitationde ces résultats. Cet outil est basé sur trois techniques de visualisation(nuages, systèmes solaire et treemap) qui permettent aux biologistes d'appréhenderde grandes quantités de motifs séquentiels (séquences ordonnées de gènes).	Arnaud Sallaberry, Nicolas Pecheur, Sandra Bringay, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001322	http://editions-rnti.fr/render_pdf.php?p=1001322	713	fr	fr	@labri.fr, @lirmm.fr, @lirmm.fr, @cemagref.fr	SequencesViewer : comment rendre accessible des motif séquentiel de gène trop nombreux  le technique d' extraction de connaissance appliquer aux gros volumesde donner , issir de le analyse de puce ADN , permettre de découvrirdes connaissance jusqu' alors inconnu . Or , ce technique produire de trèsnombreux résultat , difficilement exploitable par le expert . Nous proposonsun outil dédier à le accompagnement de ce expert dans le appropriation et le exploitationde ce résultat . ce outil être baser sur trois technique de visualisation ( nuage , système solaire et treemap ) qui permettre aux biologiste d' appréhenderde grand quantité de motif séquentiel ( séquence ordonner de gène ) . 	SequencesViewer : comment rendre accessible des motifs séquentiels de gènes trop nombreux	5
Revue des Nouvelles Technologies de l'Information	EGC	2010	SIAM: Système d'Indexation des Articles Médicaux		Jihen Majdoubi, Mohamed Tmar, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001447	http://editions-rnti.fr/render_pdf.php?p=1001447	714	fr		@yahoo.fr, @isims.rnu.tn, @fsegs.rnu.t	SIAM: Système d'Indexation des Articles Médicaux 	SIAM: Système d'Indexation des Articles Médicaux	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Simplification de données de vol pour un stockage optimal et une visualisation accélérée	Le projet RECORDS (collaboration entre industriels et université) apour objectif de développer une infrastructure de service sécurisée pour assurerle suivi et l'analyse des conditions d'utilisation d'aéronefs. Chaque aéronefest muni de capteurs. Au cours de chaque mission (vol) les données mesuréessont enregistrées localement. Ces dernières sont par la suite transférées dansune base de données centralisée à des fins d'analyse. Le problème rencontré estla grande quantité de données ainsi enregistrées, ce qui en rend l'exploitationdifficile. Dans cet article, nous proposons des techniques de compression et desimplification de données avec un taux de perte contrôlé. Nos expérimentationsmontrent des gains drastiques en volumétrie avec de très faibles pertes d'informations.Ceci représente une première étape avant d'appliquer des techniquesd'extraction de connaissances.	Ibrahim Chahid, Loic Martin, Sofian Maabout, Mohamed Mosbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001324	http://editions-rnti.fr/render_pdf.php?p=1001324	715	fr	fr	@2moro.fr, @labri.fr	simplification de donnée de vol pour un stockage optimal et un visualisation accélérée  le projet RECORDS ( collaboration entre industriel et université ) apour objectif de développer un infrastructure de service sécuriser pour assurerle suivre et le analyse des condition d' utilisation d' aéronef . chaque aéronefest munir de capteur . Au cour de chaque mission ( vol ) le donnée mesuréessont enregistrer localement . ce dernier être par le suite transférer dansune base de donnée centraliser à un fin d' analyse . le problème rencontrer estla grand quantité de donnée ainsi enregistrer , ce qui en rendre le exploitationdifficile . Dans ce article , nous proposer un technique de compression et desimplification de donnée avec un taux de perte contrôler . son expérimentationsmontrent des gain drastique en volumétrie avec un très faible perte d' information . ceci représenter un premier étape avant d' appliquer un techniquesd'extraction de connaissance . 	Simplification de données de vol pour un stockage optimal et une visualisation accélérée	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	SimTole	La plateforme SimTOLE est dédiee a l'evaluation d'algorithmes d'alignement d'ontologies heterogenes et reparties a travers un reseau pair a pair (P2P). Cette plateforme permet de simuler un réseau P2P dans lequel chaque pair dispose de sa propre ontologie ainsi que des outils permettant l'alignement entre l'ontologie locale et une ontologie stockée sur un pair distant. Le developpement de cette plateforme s'inscrit dans le cadre de travaux de recherche étudiant l'impact de la topologie du réseau P2P dans le processus d'inférence de correspondances sémantiques. Durant cette démonstration, la plateforme simTole est présentée puis testée pour illustrer des scénarii montrant comment affiner le processus d'alignement d'ontologies dans un réseau P2P.	Nicolas Lumineau, Lionel Médini	http://editions-rnti.fr/render_pdf.php?p1&p=1001390	http://editions-rnti.fr/render_pdf.php?p=1001390	716	en	fr	@liris.cnrs.fr	le plateforme SimTOLE être dédiee avoir le evaluation d' algorithme d' alignement d' ontologie heterogenes et repartie avoir travers un reseau pair avoir pair ( P2P ) . ce plateforme permettre de simuler un réseau P2P dans lequel chaque pair disposer de son propre ontologie ainsi que un outil permettre le alignement entre le ontologie local et un ontologie stocker sur un pair distant . le developpement de ce plateforme clr inscrire dans le cadre de travail de recherche étudier le impact de le topologie du réseau P2P dans le processus d' inférence de correspondance sémantique . Durant ce démonstration , le plateforme simTole être présenter puis tester pour illustrer un scénarii montrer comment affiner le processus d' alignement d' ontologie dans un réseau P2P . 	SimTole	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	SoTree : Auto-organisation topologique et hiérarchique des données	"Nous proposons dans cet article d'introduire une nouvelle approche pour la classification non supervisée hiérarchique. Notre méthode nommée So-Tree consiste à construire, d'une manière autonome et simultanée, une partition topologique et hiérarchique des données. Chaque ""cluster"" de la partition est associé à une cellule d'une grille 2D et est modélisé par un arbre, dont chaque noeud représente une donnée. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Les résultats préliminaires obtenus sont encourageants et prometteurs pour continuer dans cette direction."	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001358	http://editions-rnti.fr/render_pdf.php?p=1001358	717	fr	fr	@univ-paris13.fr	SoTree : Auto- organisation topologique et hiérarchique des données  " Nous proposer dans ce article d' introduire un nouveau approche pour le classification non superviser hiérarchique . . son méthode nommer So- Tree consister à construire , d' un manière autonome et simultané , un partition topologique et hiérarchique des donnée . . chaque " " cluster " " de le partition être associer à un cellule d' un grille 2D et être modéliser par un arbre , dont chaque noeud représenter un donner . . Nous évaluer le capacité et le performance de son approche sur un donnée aux difficulté variable . . le résultat préliminaire obtenir être encourageant et prometteur pour continuer dans ce direction . " 	SoTree : Auto-organisation topologique et hiérarchique des données	
Revue des Nouvelles Technologies de l'Information	EGC	2010	Sous-échantillonnage topographique par apprentissage semi-supervisé	Plusieurs aspects pourraient influencer les systèmes d'apprentissage existants.Un de ces aspects est lié au déséquilibre des classes dans lequel le nombre d'observationsappartenant à une classe, dépasse fortement celui des observations dans les autresclasses. Dans ce type de cas assez fréquent, le système d'apprentissage a des difficultésau cours de la phase d'entraînement liées au déséquilibre inter-classe. Nous proposonsune méthode de sous-échantillonnage adaptatif pour traiter ce type de bases déséquilibrées.Le processus procède par le sous-échantillonnage des données majoritaires, guidépar les données minoritaires tout au long de la phase d'un apprentissage semi-supervisée.Nous utilisons comme modèle d'apprentissage les cartes auto-organisatrices. L'approcheproposée a été validée sur plusieurs bases de données en utilisant les arbres de décisioncomme classificateur avec une validation croisée. Les résultats expérimentaux ont montrédes performances très prometteuses.	Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001277	http://editions-rnti.fr/render_pdf.php?p=1001277	718	fr	fr	@univ-paris13.fr	Sous-échantillonnage topographique par apprentissage semi-supervisé  plusieurs aspect pouvoir influencer le système d' apprentissage existant . un de ce aspect être lier au déséquilibre des classe dans lequel le nombre d' observationsappartenant à un classe , dépasser fortement celui des observation dans le autresclasses . Dans ce type de cas assez fréquent , le système d' apprentissage avoir un difficultésau cour de le phase d' entraînement lier au déséquilibre inter-classe . Nous proposonsune méthode de sous-échantillonnage adaptatif pour traiter ce type de base déséquilibrer . le processus procéder par le sous-échantillonnage des donnée majoritaire , guidépar le donnée minoritaire tout au long de le phase d' un apprentissage semi-supervisée . Nous utiliser comme modèle d' apprentissage le carte auto- organisatrice . le approcheproposée avoir être valider sur plusieurs base de donnée en utiliser le arbre de décisioncomme classificateur avec un validation croiser . le résultat expérimental avoir montrédes performance très prometteur . 	Sous-échantillonnage topographique par apprentissage semi-supervisé	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Suivi d'Automobiles par Classification Hiérarchique Ascendante		Abdelmalek Toumi, Christophe Osswald, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001429	http://editions-rnti.fr/render_pdf.php?p=1001429	719	fr		@ensieta.fr	Suivi d'Automobiles par Classification Hiérarchique Ascendante 	Suivi d'Automobiles par Classification Hiérarchique Ascendante	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Système d'extraction des connaissances à partir des données temporelles basé sur les Réseaux Bayésiens Dynamiques	Un grand nombre d'informations qui ont une structure complexeproviennent de diverses sources. Ces informations contiennent des connaissancestrès utiles pour l'aide à la décision. L'Extraction des Connaissances àpartir des Données (ECD), permet d'acquérir des informations pertinentes pourles systèmes interactifs d'aide à la décision (SIAD). Mais, dans plusieurs domaines,les données évoluent d'une manière dynamique et finissent par dépendrede plusieurs dimensions. Les Réseaux Bayésiens dynamiques (RBD)sont des modèles représentant des connaissances incertaines sur des phénomènescomplexes de processus dynamiques. Notre objectif revient à fixer lesmeilleures modèles de connaissances extraites par les RBD et à les utiliserpour la prise de décision dynamique. Ainsi, Nous proposons dans cet articleune démarche pour la mise en place d'un processus d'extraction des connaissancesà partir des données multidimensionnelles et temporelles.	Ghada Trabelsi, Mounir Ben Ayed, Adel M. Alimi	http://editions-rnti.fr/render_pdf.php?p1&p=1001299	http://editions-rnti.fr/render_pdf.php?p=1001299	720	fr	fr	@yahoo.fr, @ieee.org, @ieee.org	système d' extraction des connaissance à partir un donnée temporel baser sur le réseau Bayésiens Dynamiques  un grand nombre d' information qui avoir un structure complexeproviennent de divers source . ce information contenir des connaissancestrès utile pour le aide à le décision . le extraction des connaissance àpartir des Données ( ECD ) , permettre d' acquérir un information pertinent pourles système interactif d' aide à le décision ( SIAD ) . Mais , dans plusieurs domaine , le donnée évoluer d' un manière dynamique et finir par dépendrede plusieurs dimension . le réseau Bayésiens dynamique ( RBD ) être un modèle représenter un connaissance incertain sur un phénomènescomplexes de processus dynamique . son objectif revenir à fixer lesmeilleures modèle de connaissance extraire par le RBD et à le utiliserpour le prise de décision dynamique . ainsi , Nous proposer dans ce articleune démarche pour le mise en place d' un processus d' extraction des connaissancesà partir un donnée multidimensionnel et temporel . 	Système d'extraction des connaissances à partir des données temporelles basé sur les Réseaux Bayésiens Dynamiques	5
Revue des Nouvelles Technologies de l'Information	EGC	2010	Tulip: a Scalable Graph Visualization Framework	The Graph Visualization Framework Tulip now enjoys 10 years ofuser experience, and has matured its architecture and development cycle. Originallydesigned to interactively navigate large graphs, the framework integratesstate-of-the-art software engineering concepts and good practices. It offers alarge panel of graphical representations (traditional graph drawing as well asalternate representations). Tulip is most useful in a data mining and knowledgediscovery context, allowing users to easily add their own data analysis and computingroutines through its plug-in architecture.	David Auber, Patrick Mary, Morgan Mathiaut, Jonathan Dubois, Antoine Lambert, Dan Archambault, Romain Bourqui, Bruno Pinaud, Maylis Delest, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001374	http://editions-rnti.fr/render_pdf.php?p=1001374	721	en	en	@labri.fr, @inria.fr	tulip scalable graph visualization framework graph visualization framework tulip enjoy 10 year ofuser experience mature architecture development cycle originallydesign interactively navigate large graph framework integratesstate of the art software engineering concept good practice offer alarge panel graphical representation traditional graph drawing well asalternate representation tulip useful datum mining knowledgediscovery context allow user easily add data analysis computingroutine plug in architecture	Tulip: a Scalable Graph Visualization Framework	8
Revue des Nouvelles Technologies de l'Information	EGC	2010	Un modèle d'extraction de masses de croyance à partir de probabilités a posteriori pour une amélioration des performances en classification supervisée	L'objectif de cet article est de montrer que l'utilisation de la règle dedécision du maximum de masse de croyance en lieu et place de celle du maximumde probabilité a posteriori peut permettre de réduire le taux d'erreur en classificationsupervisée. Nous proposons une technique efficace pour extraire, à partird'un vecteur de probabilités a posteriori, un vecteur de masses de croyance surlequel baser la décision par le maximum de masse de croyance. L'applicationde notre méthode dans le domaine de la classification automatique en stades desommeil montre une amélioration des performances pouvant atteindre 80% deréduction du taux d'erreur de classification.	Teh Amouh, Monique Noirhomme-Fraiture, Benoît Macq	http://editions-rnti.fr/render_pdf.php?p1&p=1001349	http://editions-rnti.fr/render_pdf.php?p=1001349	722	fr	fr	@fundp.ac.be, @uclouvain.be	un modèle d' extraction de masse de croyance à partir de probabilité avoir posteriori pour un amélioration des performance en classification supervisée  le objectif de ce article être de montrer que le utilisation de le règle dedécision du maximum de masse de croyance en lieu et place de celui du maximumde probabilité avoir posteriori pouvoir permettre de réduire le taux d' erreur en classificationsupervisée . Nous proposer un technique efficace pour extraire , à partird'un vecteur de probabilité avoir posteriori , un vecteur de masse de croyance surlequel baser le décision par le maximum de masse de croyance . le applicationde son méthode dans le domaine de le classification automatique en stade desommeil montre un amélioration des performance pouvoir atteindre 80 \% deréduction du taux d' erreur de classification . 	Un modèle d'extraction de masses de croyance à partir de probabilités a posteriori pour une amélioration des performances en classification supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Un système d'aide à l'extraction de relations sémantiques pour la construction d'ontologies à partir de textes	Cet article présente une méthode d'extraction de relations sémantiquespour la construction d'ontologies à partir de corpus de textes. Notre objectif estde proposer une méthode générique, qui soit indépendante du domaine et de lalangue. Elle repose sur une analyse distributionnelle des unités sémantiques ducorpus pour faire émerger des relations sémantiques candidates. Cette méthodene fait aucune hypothèse sur les types de relations recherchées ni sur leur formelinguistique. Il s'agit de regrouper les associations de termes dans des classesqui représentent des relations sémantiques candidates. L'hypothèse sous-jacenteest que les occurrences de ces associations réunies sur la base des éléments decontexte qu'elles partagent ont des chances de relever d'une même relation sémantiqueet que les relations candidates ainsi proposées peuvent aider le travailde conceptualisation de l'ontologue	Rim Bentebibel, Adeline Nazarenko, Sylvie Szulman	http://editions-rnti.fr/render_pdf.php?p1&p=1001343	http://editions-rnti.fr/render_pdf.php?p=1001343	723	fr	fr	@univ-paris13.fr	un système d' aide à le extraction de relation sémantique pour le construction d' ontologie à partir de textes  ce article présenter un méthode d' extraction de relation sémantiquespour le construction d' ontologie à partir de corpus de texte . son objectif estde proposer un méthode générique , qui être indépendant du domaine et de lalangue . Elle reposer sur un analyse distributionnelle des unité sémantique ducorpus pour faire émerger un relation sémantique candidat . ce méthodene faire aucun hypothèse sur le type de relation rechercher ni sur son formelinguistique . Il clr agir de regrouper le association de terme dans un classesqui représenter un relation sémantique candidat . le hypothèse sous-jacenteest que le occurrence de ce association réunir sur le base des élément decontexte qu' elles partager avoir un chance de relever d' un même relation sémantiqueet que le relation candidat ainsi proposer pouvoir aider le travailde conceptualisation de le ontologue 	Un système d'aide à l'extraction de relations sémantiques pour la construction d'ontologies à partir de textes	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une approche fondée sur la corrélation entre prédicats pour le traitement des réponses pléthoriques	L'interrogation de bases de données, dont les dimensions ne cessentde croître, se heurte fréquemment au problème de la gestion des réponses pléthoriques.Une des approches envisageables pour réduire l'ensemble des résultatsretournés et le rendre exploitable est de contraindre la requête initiale parl'ajout de nouvelles conditions. L'approche présentée dans cet article s'appuiesur l'identification de liens de corrélation entre prédicats associés aux attributsde la relation concernée. La requête initiale peut ainsi être intensifiée automatiquementou par validation de l'utilisateur à travers l'ajout de prédicats prochessémantiquement de ceux spécifiés.	Patrick Bosc, Allel HadjAli, Olivier Pivert, Grégory Smits	http://editions-rnti.fr/render_pdf.php?p1&p=1001305	http://editions-rnti.fr/render_pdf.php?p=1001305	724	fr	fr	@enssat.fr, @univ-rennes1.fr	un approche fonder sur le corrélation entre prédicat pour le traitement des réponse pléthoriques  le interrogation de base de donnée , dont le dimension ne cessentde croître , clr heurter fréquemment au problème de le gestion des réponse pléthorique . un des approche envisageable pour réduire le ensemble des résultatsretournés et le rendre exploitable être de contraindre le requête initial parl'ajout de nouveau condition . le approche présenter dans ce article clr appuiesur le identification de lien de corrélation entre prédicat associer aux attributsde le relation concerner . le requête initial pouvoir ainsi être intensifier automatiquementou par validation de le utilisateur à travers le ajout de prédicat prochessémantiquement de celui spécifier . 	Une approche fondée sur la corrélation entre prédicats pour le traitement des réponses pléthoriques	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une approche probabiliste pour l'identification de structures de communautés	Dans cet article, nous valorisons et défendons l'idée que les modèles génératifs sont une approche prometteuse pour l'identification de structure de communautés (ISC). Nous proposons un nouveau modèle probabiliste pour l'idenditification de structures de communautés qui utilise le lissage afin de pallier le petit nombre de liens entre les noeuds. Notre modèle étant très sensible aux paramètres de lissage, nous proposons également une méthode basée sur la modularité pour leur estimation. Les résultats expérimentaux obtenus sur 3 jeux de données montrent que notre modèle SPCE est largement meilleur que le modèle PHITS	Nacim Fateh Chikhi, Bernard Rothenburger, Nathalie Aussenac-Gilles	http://editions-rnti.fr/render_pdf.php?p1&p=1001289	http://editions-rnti.fr/render_pdf.php?p=1001289	725	fr	fr	@irit.fr	un approche probabiliste pour le identification de structure de communautés  Dans ce article , nous valoriser et défendre le idée que le modèle génératif être un approche prometteur pour le identification de structure de communauté ( ISC ) . Nous proposer un nouveau modèle probabiliste pour le idenditification de structure de communauté qui utiliser le lissage afin de pallier le petit nombre de lien entre le noeud . son modèle être très sensible aux paramètre de lissage , nous proposer également un méthode baser sur le modularité pour son estimation . le résultat expérimental obtenir sur 3 jeu de donnée montrer que son modèle SPCE être largement meilleur que le modèle PHITS 	Une approche probabiliste pour l'identification de structures de communautés	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une méthode d'aide au management de connaissances pour améliorer le processus de suivi et d'évaluation de la prise en charge précoce des enfants IMC : application de l'ASHMS		Mohamed Turki, Inès Saad, Gilles Kassel, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001431	http://editions-rnti.fr/render_pdf.php?p=1001431	726	fr		@isetsf.rnu.tn, @fsegs.rnu.t, @supco-amiens.fr, @u-picardie.fr	Une méthode d'aide au management de connaissances pour améliorer le processus de suivi et d'évaluation de la prise en charge précoce des enfants IMC : application de l'ASHMS 	Une méthode d'aide au management de connaissances pour améliorer le processus de suivi et d'évaluation de la prise en charge précoce des enfants IMC : application de l'ASHMS	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une nouvelle approche de découverte des correspondances complexes entre ontologies	Les correspondances complexes ont été étudiées à plusieurs reprisesdans le domaine d'alignement de schémas de bases de données. Par contre,dans le domaine d'alignement des ontologies, elles ont été peu étudiées. Nousproposons, dans ce papier, une nouvelle approche de découverte de correspondancescomplexes entre deux ontologies. L'approche proposée est extensionnelle,terminologique et implicative. Dans cette approche, nous utilisons le modèledes règles d'association afin de découvrir des correspondances de typex &#8658; y1 &#8743; ... &#8743; yn entre deux ontologies.	Fatma Kaâbi, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001414	http://editions-rnti.fr/render_pdf.php?p=1001414	727	fr	fr	@yahoo.fr, @fsegs.rnu.t	un nouveau approche de découverte des correspondance complexe entre ontologies  le correspondance complexe avoir être étudier à plusieurs reprisesdans le domaine d' alignement de schéma de base de donnée . Par contre , dans le domaine d' alignement des ontologie , elles avoir être peu étudier . Nousproposons , dans ce papier , un nouveau approche de découverte de correspondancescomplexes entre deux ontologie . le approche proposer être extensionnelle , terminologique et implicative . Dans ce approche , nous utiliser le modèledes règle d' association afin de découvrir un correspondance de typex ⇒ y1 ∧ ... ∧ yn entre deux ontologie . 	Une nouvelle approche de découverte des correspondances complexes entre ontologies	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une nouvelle stratégie d'Apprentissage Bayésienne	Dans cet article, une nouvelle stratégie d'apprentissage actif est proposée. Cette stratégie est fondée sur une méthode de discrétisation Bayésienne semi-supervisée. Des expériences comparatives sont menées sur des données unidimensionnelles, l'objectif étant d'estimer la position d'un échelon à partir de données bruitées.	Alexis Bondu, Vincent Lemaire, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001463	http://editions-rnti.fr/render_pdf.php?p=1001463	728	fr	fr	@edf.fr, @orange-ftgroup.com	un nouveau stratégie d' Apprentissage Bayésienne  Dans ce article , un nouveau stratégie d' apprentissage actif être proposer . ce stratégie être fonder sur un méthode de discrétisation Bayésienne semi-supervisée . un expérience comparatif être mener sur un donnée unidimensionnel , le objectif être d' estimer le position d' un échelon à partir de donnée bruiter . 	Une nouvelle stratégie d'Apprentissage Bayésienne	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une Ontologie pour l'Acquisition et l'Exploitation des Connaissances en Conception Inventive	L'acquisition des connaissances en vue de résoudre des problèmesconcernant l'évolution des artefacts, comme elle se doit d'être pratiquée enconception inventive, a des caractéristiques spécifiques. Elle nécessite lasélection de certaines des connaissances qui peuvent induire des évolutions,elle amène à reformuler le problème initial afin de construire un modèleabstrait de l'artefact concerné. La méthode de conception inventive induite parla théorie de la Résolution des Problèmes Inventifs (aussi connue sousl'acronyme TRIZ) n'a pas encore fait l'objet d'une véritable formalisation.Nous proposons ici une ontologie des notions principales des concepts liés àl'acquisition des connaissances dans ce cadre. Cette ontologie, outre laclarification des notions en jeu, est utilisée comme support d'un environnementinformatique d'aide à la mise en oeuvre d'une méthode pour acquérir lesconnaissances et formuler les problèmes.	François Rousselot, Cecilia Zanni, Denis Cavallucci	http://editions-rnti.fr/render_pdf.php?p1&p=1001470	http://editions-rnti.fr/render_pdf.php?p=1001470	729	fr	fr	@insa-strasbourg.fr	un ontologie pour le acquisition et le Exploitation des connaissance en conception Inventive  le acquisition des connaissance en vue de résoudre un problèmesconcernant le évolution des artefact , comme elle clr devoir d' être pratiquer enconception inventif , avoir un caractéristique spécifique . Elle nécessiter lasélection de certains des connaissance qui pouvoir induire un évolution , elle amener à reformuler le problème initial afin de construire un modèleabstrait de le artefact concerner . le méthode de conception inventif induire parler théorie de le résolution des problème inventif ( aussi connaître sousl'acronyme TRIZ ) n' avoir pas encore faire le objet d' un véritable formalisation . Nous proposer ici un ontologie des notion principal des concept lier àl'acquisition des connaissance dans ce cadre . ce ontologie , outre laclarification des notion en jeu , être utiliser comme support d' un environnementinformatique d' aide à le mise en oeuvre d' un méthode pour acquérir lesconnaissances et formuler le problème . 	Une Ontologie pour l'Acquisition et l'Exploitation des Connaissances en Conception Inventive	2
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une structure basée sur les hiérarchies pour synthétiser les itemsets fréquents extraits dans des fenêtres temporelles	Le paradigme des flots de données rend impossible la conservation de l'intégralitéde l'historique d'un flot qu'il faut alors résumer. L'extraction d'itemsets fréquentssur des fenêtres temporelles semble tout à fait adaptée mais l'amoncellement des résultatsindépendants rend impossible l'exploitation de ces résultats. Nous proposons une structurebasée sur les hiérarchies des données afin d'unifiant ces résultats. De plus, puisque laplupart des données d'un flot présentent un caractère multidimensionnel, nous intégronsla prise en compte d'itemsets multidimensionnels. Enfin, nous pallions une faiblesse majeuredes Tilted TimeWindows (TTW) en prenant en compte la distribution des données.	Yoann Pitarch, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001467	http://editions-rnti.fr/render_pdf.php?p=1001467	730	fr	fr	@lirmm.fr	un structure baser sur le hiérarchie pour synthétiser le itemsets fréquent extrait dans un fenêtre temporelles  le paradigme des flot de donnée rendre impossible le conservation de le intégralitéde le historique d' un flot qu' il faillir alors résumer . le extraction d' itemsets fréquentssur des fenêtre temporel sembler tout à fait adapter mais le amoncellement des résultatsindépendants rendre impossible le exploitation de ce résultat . Nous proposer un structurebasée sur le hiérarchie des donnée afin d' unifier ce résultat . De plus , puisque laplupart un donnée d' un flot présenter un caractère multidimensionnel , nous intégronsla prendre en compte d' itemsets multidimensionnel . enfin , nous pallier un faiblesse majeuredes Tilted TimeWindows ( TTW ) en prendre en compte le distribution des donnée . 	Une structure basée sur les hiérarchies pour synthétiser les itemsets fréquents extraits dans des fenêtres temporelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Utilisation de graphes sémantiques pour l'extraction et la traduction des idées essentielles d'un texte		Romain André-Lovichi, Kamel Smaïli, David Langlois	http://editions-rnti.fr/render_pdf.php?p1&p=1001435	http://editions-rnti.fr/render_pdf.php?p=1001435	731	fr		@loria.fr, @loria.fr, @loria.fr	Utilisation de graphes sémantiques pour l'extraction et la traduction des idées essentielles d'un texte 	Utilisation de graphes sémantiques pour l'extraction et la traduction des idées essentielles d'un texte	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Vers une extraction et une visualisation des co-localisations adaptées aux experts	Une des tâches classiques en fouille de données spatiales est l'extractionde co-localisations intéressantes dans des données géo-référencées. L'objectifest de trouver des sous-ensembles de caractéristiques booléennes apparaissantfréquemment dans des objets spatiaux voisins. Toutefois, les relations découvertespeuvent ne pas être pertinentes pour les experts, et leur interprétation sousforme textuelle peut être difficile. Nous proposons, dans ce contexte, une nouvelleapproche pour intégrer la connaissance des experts dans la découverte desco-localisations, ainsi qu'une nouvelle représentation visuelle de ces motifs. Unprototype a été développé et intégré dans un SIG. Des expérimentations on étémenées sur des données géologiques réelles, et les résultats validés par un expertdu domaine.	Frédéric Flouvat, Nazha Selmaoui-Folcher, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001335	http://editions-rnti.fr/render_pdf.php?p=1001335	732	fr	fr	@univ-nc.nc	Vers un extraction et un visualisation des co- localisation adapter aux experts  un des tâche classique en fouille de donnée spatial être le extractionde co- localisation intéressant dans un donnée géo- référencer . le objectifest de trouver un sous-ensemble de caractéristique booléen apparaissantfréquemment dans un objet spatial voisin . toutefois , le relation découvertespeuvent ne pas être pertinent pour le expert , et son interprétation sousforme textuel pouvoir être difficile . Nous proposer , dans ce contexte , un nouvelleapproche pour intégrer le connaissance des expert dans le découverte desco- localisation , ainsi qu' un nouveau représentation visuel de ce motif . Unprototype avoir être développer et intégrer dans un SIG . un expérimentation on étémenées sur un donnée géologique réel , et le résultat valider par un expertdu domaine . 	Vers une extraction et une visualisation des co-localisations adaptées aux experts	1
Revue des Nouvelles Technologies de l'Information	EGC	2010	Visual Sentence-Phrase-Based Document Representation for Effective and Efficient Content-Based Image Retrieval	Having effective and efficient methods to get access to desired imagesis essential nowadays with the huge amount of digital images. This paperpresents an analogy between content-based image retrieval and text retrieval.We make this analogy from pixels to letters, patches to words, sets of patchesto phrases, and groups of sets of patches to sentences. To achieve a more accuratedocument matching, more informative features including phrases and sentencesare needed to improve these scenarios. The proposed approach is basedfirst on constructing different visual words using local patch extraction and description.After that, we study different association rules between frequent visualwords in the context of local regions in the image to construct visual phrases,which will be grouped to different sentences.	Ismail Elsayad, Jean Martinet, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001284	http://editions-rnti.fr/render_pdf.php?p=1001284	733	en	en	@lifl.fr	visual sentence phrase base document representation effective efficient content base image retrieval effective efficient method get access desire imagesis essential nowadays huge amount digital image paperpresent analogy content base image retrieval text retrieval we make analogy pixel letter patch word set patchesto phrase group set patch sentence achieve accuratedocument match informative feature include phrase sentencesare need improve scenario proposed approach basedfirst construct different visual word used local patch extraction description after that study different association rule frequent visualword context local region image construct visual phrase which group different sentence	Visual Sentence-Phrase-Based Document Representation for Effective and Efficient Content-Based Image Retrieval	0
Revue des Nouvelles Technologies de l'Information	EGC	2010	Visualisation de mesures agrégées pour l'estimation de la qualité des articlesWikipedia	Wikipedia, devenue l'une des bases de connaissances les plus populaires,pose le problème de la fiabilité de l'information qu'elle dissémine. Nousproposons WikipediaViz, un ensemble de visualisations basé sur un mecanismede collecte et d'agrégation de données d'édition Wikipedia pour aider le lecteurà appréhender la maturité d'un article. Nous listons cinq métriques importantes,déterminées lors de sessions de conception participative avec des experts Wikipediapour juger de la qualité, que nous présentons au lecteur sous forme devisualisations compactes et expressives, dépeignant le profil d'évolution d'un article.Nos études utilisateur ont montré queWikipediaViz réduisait significativementle temps requis pour évaluer la qualité en maintenant une bonne précision	Fanny Chevalier, Stéphane Huot, Jean-Daniel Fekete	http://editions-rnti.fr/render_pdf.php?p1&p=1001316	http://editions-rnti.fr/render_pdf.php?p=1001316	734	fr	fr	@inria.fr, @lri.fr, @inria.fr	visualisation de mesure agréger pour le estimation de le qualité des articlesWikipedia  Wikipedia , devenir le un des base de connaissance le plus populaire , poser le problème de le fiabilité de le information qu' elle disséminer . Nousproposons WikipediaViz , un ensemble de visualisation baser sur un mecanismede collecte et d' agrégation de donnée d' édition Wikipedia pour aider le lecteurà appréhender le maturité d' un article . Nous lister cinq métrique important , déterminer lors de session de conception participatif avec un expert Wikipediapour juger de le qualité , que nous présenter au lecteur sous forme devisualisations compact et expressif , dépeigner le profil d' évolution d' un article . son étude utilisateur avoir montrer queWikipediaViz réduire significativementle temps requérir pour évaluer le qualité en maintenir un bon précision 	Visualisation de mesures agrégées pour l'estimation de la qualité des articlesWikipedia	3
Revue des Nouvelles Technologies de l'Information	EGC	2010	WCUM pour l'analyse d'un site web	Dans ce papier, nous proposons une approche WCUM (Web Contentand Usage based Approach) permettant de relier l'analyse du contenu d'un siteWeb à l'analyse de l'usage afin de mieux comprendre les comportements de navigationsur le site. L'apport de ce travail réside d'une part dans la propositiond'une approche reliant l'analyse du contenu à l'analyse de l'usage et d'autre partdans l'extension de l'application des méthodes de block clustering, appliquéesgénéralement en bioinformatique, au contexte Web mining afin de profiter deleur pouvoir classificatoire dans la découverte de biclasses homogènes à partird'une partition des instances et une partition des attributs recherchées simultanément.	Malika Charrad, Yves Lechevallier, Mohamed Ben Ahmed, Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1001413	http://editions-rnti.fr/render_pdf.php?p=1001413	735	fr	fr	@riadi.rnu.tn, @inria.fr, @cnam.fr	WCUM pour le analyse d' un site web  Dans ce papier , nous proposer un approche WCUM ( Web Contentand Usage based Approach ) permettre de relier le analyse du contenu d' un siteWeb à le analyse de le usage afin de mieux comprendre le comportement de navigationsur le site . le apport de ce travail résider d' un part dans le propositiond'une approcher relier le analyse du contenu à le analyse de le usage et d' autre partdans le extension de le application des méthode de block clustering , appliquéesgénéralement en bioinformatique , au contexte Web mining afin de profiter deleur pouvoir classificatoire dans le découverte de biclasses homogène à partird'une partition des instance et un partition des attribut rechercher simultanément . 	WCUM pour l'analyse d'un site web	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	A Contextualization Service for a Personalized Access Model	Personalization paradigm aims at providing users with the most rel-evant content and services according to many factors such as interest center orlocation at the querying time. All this knowledge and requirements are orga-nized into user profiles and contexts. A user profile encompasses metadata de-scribing the user whereas a context groups information about the environmentof interaction between the user and the system. An interesting problem is there-fore to identify which part of the profile is significant in a given context. Thispaper proposes a contextualization service which allows defining relationshipsbetween user preferences and contexts. Further, we propose an approach forthe automatic discovery of these mappings by analyzing user behavior extractedfrom log files.	Sofiane Abbar, Mokrane Bouzeghoub, Dimitre Kostadinov, Stéphane Lopes	http://editions-rnti.fr/render_pdf.php?p1&p=1000771	http://editions-rnti.fr/render_pdf.php?p=1000771	806	en	en	@prism.uvsq.fr, @alcatel-lucent.fr	contextualization service personalized access model personalization paradigm aim provide user rel evant content service accord many factor interest center orlocation query time knowledge requirement orga nized user profile context user profile encompass metadata de scribing user whereas context group information environmentof interaction user system interesting problem there fore identify part profile significant give context thispaper propose contextualization service allow define relationshipsbetween user preference context further propose approach forthe automatic discovery mapping analyze user behavior extractedfrom log file	A Contextualization Service for a Personalized Access Model	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	Accompagner au début du 21ème siècle les organisations dans la mise en place d'une gestion des connaissances : retour d'expérience	Cet article présente succinctement le retour d'expérience d'Ardansdans l'implantation de systèmes de gestion de connaissances dans des organisationstrès variées au début de ce 21ème siècle.	Alain Berger, Jean-Pierre Cotton, Pierre Mariot	http://editions-rnti.fr/render_pdf.php?p1&p=1000811	http://editions-rnti.fr/render_pdf.php?p=1000811	807	fr	fr	@ardans.fr	accompagner au début du 21ème siècle le organisation dans le mise en place d' un gestion des connaissance : retour d' expérience  ce article présenter succinctement le retour d' expérience d' Ardansdans le implantation de système de gestion de connaissance dans un organisationstrès varier au début de ce 21ème siècle . 	Accompagner au début du 21ème siècle les organisations dans la mise en place d'une gestion des connaissances : retour d'expérience	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Acquisition de la théorie ontologique d'un système d'extraction d'information	La conception de systèmes d'Extraction d'Information (EI) destinésà extraire les réseaux d'interactions géniques décrits dans la littérature scientifiqueest un enjeu important. De tels systèmes nécessitent des représentationssophistiquées, s'appuyant sur des ontologies, afin de définir différentes relationsbiologiques, ainsi que les dépendances récursives qu'elles présentent entre elles.Cependant, l'acquisition de ces dépendances n'est pas possible avec les techniquesd'apprentissage automatique actuellement employées en EI, car ces dernièresne gèrent pas la récursivité. Afin de palier ces limitations, nous présentonsune application à l'EI de la Programmation Logique Inductive, en mode multipredicats.Nos expérimentations, effectuées sur un corpus bactérien, conduisentà un rappel global de 67.7% pour une précision de 75.5%.	Alain-Pierre Manine	http://editions-rnti.fr/render_pdf.php?p1&p=1000788	http://editions-rnti.fr/render_pdf.php?p=1000788	808	fr	fr	@univ-paris13.fr	acquisition de le théorie ontologique d' un système d' extraction d' information  le conception de système d' extraction d' Information ( EI ) destinésà extraire le réseau d' interaction géniques décrire dans le littérature scientifiqueest un enjeu important . De tel système nécessiter un représentationssophistiquées , clr appuyer sur un ontologie , afin de définir différent relationsbiologiques , ainsi que le dépendance récursif qu' elles présenter entre lui . cependant , le acquisition de ce dépendance n' être pas possible avec le techniquesd'apprentissage automatique actuellement employer en EI , car ce dernièresne gérer pas le récursivité . Afin de palier ce limitation , nous présentonsune application à le EI de le programmation Logique Inductive , en mode multipredicats . son expérimentation , effectuer sur un corpus bactérien , conduisentà un rappel global de 67 .7 \% pour un précision de 75 .5 \% . 	Acquisition de la théorie ontologique d'un système d'extraction d'information	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Acquisition, annotation et exploration interactive d'images stéréoscopiques en réalité virtuelle : application en dermatologie	Nous présentons dans cet article le système Skin3D qui implémentetous les composants matériels et logiciels nécessaires pour extraire desinformations dans des images 3D de peau. Il s'agit à la fois du matérield'éclairage et d'acquisition à base d'appareils photographiquesstéréoscopiques, d'une méthode de calibration de caméras utilisant lesalgorithmes génétiques, de matériel de réalité virtuelle pour restituer lesimages en stéréoscopie et interagir avec elles, et enfin d'un ensemble defonctionnalités interactives pour annoter les images, partager ces annotations etconstruire un hypermédia 3D. Nous présentons une étude comparativeconcernant la calibration et une application réelle de Skin3D sur des images devisages.	Mohammed Haouach, Karim Benzeroual, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000774	http://editions-rnti.fr/render_pdf.php?p=1000774	809	fr	fr	@univ-tours.fr, @ceries-lab.com	acquisition , annotation et exploration interactif d' image stéréoscopique en réalité virtuel : application en dermatologie  Nous présenter dans ce article le système Skin 3D qui implémentetous le composant matériel et logiciel nécessaire pour extraire desinformations dans un image 3D de peau . Il clr agir à le foi du matérield'éclairage et d' acquisition à base d' appareil photographiquesstéréoscopiques , d' un méthode de calibration de caméra utiliser lesalgorithmes génétique , de matériel de réalité virtuel pour restituer lesimages en stéréoscopie et interagir avec lui , et enfin d' un ensemble defonctionnalités interactif pour annoter le image , partager ce annotation etconstruire un hypermédia 3D. Nous présenter un étude comparativeconcernant le calibration et un application réel de Skin 3D sur un image devisages . 	Acquisition, annotation et exploration interactive d'images stéréoscopiques en réalité virtuelle : application en dermatologie	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Aggregative and Neighboring Approximations to Query Semi-Structured Documents	Structures heterogeneity in Web resources is a constant concern inelement retrieval (i.e. tag retrieval in semi-structured documents). In this paperwe present the SHIRI 1 querying approach which allows to reach more or lessstructured document parts without an a priori knowledge on their structuring.	Yassine Mrabet, Nathalie Pernelle, Nacéra Bennacer, Mouhamadou Thiam	http://editions-rnti.fr/render_pdf.php?p1&p=1000808	http://editions-rnti.fr/render_pdf.php?p=1000808	810	en	en	@lri.fr, @supelec.fr	aggregative neighboring approximation query semi structured document structure heterogeneity web resource constant concern inelement retrieval i e tag retrieval semi structured document paperwe present shiri 1 query approach allow reach lessstructured document part without priori knowledge structuring	Aggregative and Neighboring Approximations to Query Semi-Structured Documents	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	An approach for handling risk and uncertainty in multiarmed bandit problems	An approach is presented to deal with risk in multiarmed bandit prob-lems. Specifically, the well known exploration-exploitation dilemma is solvedfrom the point of view of maximizing an utility function which measures thedecision maker's attitude towards risk and uncertain outcomes. A link withthe preference theory is thus established. Simulations results are provided forin order to support the main ideas and to compare the approach with existingmethods, with emphasis on the short term (small sample size) behavior of theproposed method.	Fabrice Clérot, Stefano Perabò	http://editions-rnti.fr/render_pdf.php?p1&p=1000744	http://editions-rnti.fr/render_pdf.php?p=1000744	811	en	en	@orange.fr, @orange-ftgroup.com	approach handle risk uncertainty multiarm bandit problem approach present deal risk multiarmed bandit prob lem specifically well know exploration exploitation dilemma solvedfrom point view maximize utility function measure thedecision maker s attitude towards risk uncertain outcome link withthe preference theory thus establish simulation result provide forin order support main idea compare approach existingmethod emphasis short term small sample size behavior thepropose method	An approach for handling risk and uncertainty in multiarmed bandit problems	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse de dissimilarités par arbre d'induction	Dans cet article1, nous considérons des objets pour lesquels nous dis-posons d'une matrice des dissimilarités et nous nous intéressons à leurs liensavec des attributs. Nous nous centrons sur l'analyse de séquences d'états pourlesquelles les dissimilarités sont données par la distance d'édition. Toutefois, lesméthodes développées peuvent être étendues à tout type d'objets et de mesurede dissimilarités. Nous présentons dans un premier temps une généralisation del'analyse de variance (ANOVA) pour évaluer le lien entre des objets non mesu-rables (p. ex. des séquences) avec une variable catégorielle. La clef de l'approcheest d'exprimer la variabilité en termes des seules dissimilarités ce qui nous per-met d'identifier les facteurs qui réduisent le plus la variabilité. Nous présentonsun test statistique général qui peut en être déduit et introduisons une méthodeoriginale de visualisation des résultats pour les séquences d'états. Nous présen-tons ensuite une généralisation de cette analyse au cas de facteurs multiples et endiscutons les apports et les limites, notamment en terme d'interprétation. Fina-lement, nous introduisons une nouvelle méthode de type arbre d'induction quiutilise le test précédent comme critère d'éclatement. La portée des méthodesprésentées est illustrée à l'aide d'une analyse des facteurs discriminant le plusles trajectoires occupationnelles .	Gilbert Ritschard, Matthias Studer, Nicolas S. Müller, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1000733	http://editions-rnti.fr/render_pdf.php?p=1000733	812	fr	fr	@unige.ch	analyse de dissimilarités par arbre d' induction  Dans ce article1 , nous considérer un objet pour lesquels nous dis-posons d' un matrice des dissimilarités et nous nous intéresser à son liensavec des attribut . Nous nous centrer sur le analyse de séquence d' état pourlesquelles le dissimilarités être donner par le distance d' édition . toutefois , lesméthodes développer pouvoir être étendre à tout type d' objet et de mesurede dissimilarités . Nous présenter dans un premier temps un généralisation del'analyse de variance ( ANOVA ) pour évaluer le lien entre un objet non mesu-rables ( page ex. des séquence ) avec un variable catégoriel . le clef de le approcheest d' exprimer le variabilité en terme des seul dissimilarités ce qui nous per-met d' identifier le facteur qui réduire le plus le variabilité . Nous présentonsun test statistique général qui pouvoir en être déduire et introduire un méthodeoriginale de visualisation des résultat pour le séquence d' état . Nous présen-tons ensuite un généralisation de ce analyse au cas de facteur multiple et endiscutons le apport et le limite , notamment en terme d' interprétation . Fina-lement , nous introduire un nouveau méthode de type arbre d' induction quiutilise le test précédent comme critère d' éclatement . le portée des méthodesprésentées être illustrer à le aide d' un analyse des facteur discriminer le plusles trajectoire occupationnelles . 	Analyse de dissimilarités par arbre d'induction	22
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse de données pour la construction de modèles de procédures neurochirurgicales	Dans cet article, nous appliquons une méthode d'analyse sur desdescriptions de procédures de neurochirurgie dans le but d'en améliorer lacompréhension. La base de données XML utilisée dans cette étude estconstituée de la description de 157 chirurgies de tumeurs. Trois cent vingtdeux variables ont été identifiées et décomposées en variables prédictives(connues avant l'opération) et variables à prédire (décrivant des gesteschirurgicaux). Une analyse factorielle des correspondances (AFC) a étéréalisée sur les variables prédictives, ainsi qu'un arbre de décision basé sur undendrogramme préalablement établi. Six classes principales de variablesprédictives ont ainsi été identifiées. Puis, pour chacune de ces classes, uneanalyse AFC a été réalisée sur les variables à prédire, ainsi qu'un arbre dedécision. Bien que le nombre de cas et le choix des variables constituent unelimite à cette étude, nous avons réussi à prédire certaines caractéristiques liéesaux procédures en partant de données prédictives.	Brivael Trelhu, Florent Lalys, Laurent Riffaud, Xavier Morandi, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1000789	http://editions-rnti.fr/render_pdf.php?p=1000789	813	fr	fr	@irisa.fr, @irisa.fr, @irisa.fr, @chu-rennes.fr, @chu-rennes.fr	analyse de donnée pour le construction de modèle de procédure neurochirurgicales  Dans ce article , nous appliquer un méthode d' analyse sur desdescriptions de procédure de neurochirurgie dans le but d' en améliorer lacompréhension . le base de donnée XML utiliser dans ce étude estconstituée de le description de 157 chirurgie de tumeur . Trois cent vingtdeux variable avoir être identifier et décomposer en variable prédictif ( connaître avant le opération ) et variable à prédire ( décrire des gesteschirurgicaux ) . un analyse factoriel des correspondance ( AFC ) avoir étéréalisée sur le variable prédictif , ainsi qu' un arbre de décision baser sur undendrogramme préalablement établir . Six classe principal de variablesprédictives avoir ainsi être identifier . Puis , pour chacun de ce classe , uneanalyse AFC avoir être réaliser sur le variable à prédire , ainsi qu' un arbre dedécision . bien que le nombre de cas et le choix des variable constituer unelimite à ce étude , nous avoir réussir à prédire certain caractéristique liéesaux procédure en partir de donnée prédictif . 	Analyse de données pour la construction de modèles de procédures neurochirurgicales	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse et application de modèles de régression pour optimiser le retour sur investissement d'opérations commerciales	Les activités de négoce de matériaux sont un marché extrêmementcompétitif. Pour les acteurs de ce marché, les méthodes de fouille de donnéespeuvent s'avérer intéressantes en permettant de dégager des gains de rentabilitéimportants. Dans cet article, nous présenterons le retour d'expérience du projetde fouille de données mené chez VM Matériaux pour améliorer le retour surinvestissement d'opérations commerciales. La synergie des informaticiens, dumarketing et des experts métier a permis d'améliorer l'extraction des connais-sances à partir des données de manière à aboutir à la connaissance actionnable laplus pertinente possible et ainsi aider les experts métier à prendre des décisions.	Thomas Piton, Julien Blanchard, Henri Briand, Laurent Tessier, Gaëtan Blain	http://editions-rnti.fr/render_pdf.php?p1&p=1000735	http://editions-rnti.fr/render_pdf.php?p=1000735	814	fr	fr	@vm-materiaux.fr, @univ-nantes.fr, @kxen.com	analyse et application de modèle de régression pour optimiser le retour sur investissement d' opération commerciales  le activité de négoce de matériau être un marché extrêmementcompétitif . Pour le acteur de ce marché , le méthode de fouille de donnéespeuvent clr avérer intéressant en permettre de dégager un gain de rentabilitéimportants . Dans ce article , nous présenter le retour d' expérience du projetde fouille de donnée mener chez VM Matériaux pour améliorer le retour surinvestissement d' opération commercial . le synergie des informaticien , dumarketing et des expert métier avoir permettre d' améliorer le extraction des connais-sances à partir un donnée de manière à aboutir à le connaissance actionnable laplus pertinent possible et ainsi aider le expert métier à prendre un décision . 	Analyse et application de modèles de régression pour optimiser le retour sur investissement d'opérations commerciales	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse multigraduelle OLAP	Les systèmes décisionnels reposent sur des bases de données multidimensionnellesqui offrent un cadre adéquat aux analyses OLAP. L'articleprésente un nouvel opérateur OLAP nommé « BLEND » rendant possible desanalyses multigraduelles. Il s'agit de transformer la structuration multidimensionnellelors des interrogations pour analyser les mesures selon des niveauxde granularité différents recombinées comme un même paramètre. Nous menonsune étude des combinaisons valides de l'opération dans le contexte deshiérarchies strictes. Enfin, une première série d'expérimentations implantel'opération dans le contexte R-OLAP en montrant le faible coût de l'opération.	Gilles Hubert, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000768	http://editions-rnti.fr/render_pdf.php?p=1000768	815	fr	fr	@irit.fr	analyse multigraduelle OLAP  le système décisionnel reposer sur un base de donnée multidimensionnellesqui offrir un cadre adéquat aux analyse OLAP . le articleprésente un nouveau opérateur OLAP nommer « BLEND » rendre possible desanalyses multigraduelles . Il clr agir de transformer le structuration multidimensionnellelors des interrogation pour analyser le mesure selon un niveauxde granularité différent recombiner comme un même paramètre . Nous menonsune étude des combinaison valide de le opération dans le contexte deshiérarchies strict . enfin , un premier série d' expérimentation implantel'opération dans le contexte R-OLAP en montrer le faible coût de le opération . 	Analyse multigraduelle OLAP	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse sémantique spatio-temporelle pour les ontologies OWL-DL	L'analyse sémantique est un nouveau paradigmed'interrogation du Web Sémantique qui a pour objectif d'identifier lesassociations sémantiques reliant des individus décrits dans desontologies OWL-DL. Pour déduire davantage d'associationssémantiques et augmenter la précision de l'analyse, l'informationspatio-temporelle attachée aux ressources doit être prise en compte. Aces fins - et pour combler l'absence actuelle de raisonneurs spatiotemporeldéfini pour les ontologies RDF(S) et OWL-, nous proposonsle système de représentation et d'interrogation d'ontologies spatiotemporellesONTOAST, compatible avec le langage OWL-DL. Nousprésentons les principes de base de l'algorithme de découverted'associations sémantiques entre individus intégré dans ONTOAST.Cet algorithme utilise deux contextes, l'un spatial et l'autre temporelqui permettent d'affiner la recherche. Nous décrivons enfin l'approchemise en oeuvre pour la déduction de connexions spatiales entreindividus.	Alina Dia Miron, Jérôme Gensel, Marlène Villanova-Oliver	http://editions-rnti.fr/render_pdf.php?p1&p=1000784	http://editions-rnti.fr/render_pdf.php?p=1000784	816	fr	fr	@imag.fr	analyse sémantique spatio- temporel pour le ontologie OWL-DL  le analyse sémantique être un nouveau paradigmed'interrogation du Web Sémantique qui avoir pour objectif d' identifier lesassociations sémantique relier un individu décrire dans desontologies OWL-DL . Pour déduire davantage d' associationssémantiques et augmenter le précision de le analyse , le informationspatio- temporel attacher aux ressource devoir être prendre en compte . ace fin - et pour combler le absence actuel de raisonneur spatiotemporeldéfini pour le ontologie RDF ( S ) et OWL- , lui proposonsle système de représentation et d' interrogation d' ontologie spatiotemporellesONTOAST , compatible avec le langage OWL-DL . Nousprésentons le principe de base de le algorithme de découverted'associations sémantique entre individu intégrer dans ONTOAST . ce algorithme utiliser deux contexte , le un spatial et le autre temporelqui permettre d' affiner le recherche . Nous décrire enfin le approchemise en oeuvre pour le déduction de connexion spatial entreindividus . 	Analyse sémantique spatio-temporelle pour les ontologies OWL-DL	
Revue des Nouvelles Technologies de l'Information	EGC	2009	Assessing the uncertainty in knn Data Fusion		Tomàs Aluja-Banet, Josep Daunis-i-Estadella, Enric Ripoll	http://editions-rnti.fr/render_pdf.php?p1&p=1000794	http://editions-rnti.fr/render_pdf.php?p=1000794	817	en		@upc.edu, @udg.edu, @idescat.net	Assessing the uncertainty in knn Data Fusion 	Assessing the uncertainty in knn Data Fusion	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Binary Sequences and Association Graphs for Fast Detection of Sequential Patterns	We develop an efficient algorithm for detecting frequent patterns thatoccur in sequence databases under certain constraints. By combining the useof bit vector representations of sequence databases with association graphs weachieve superior time and low memory usage based on a considerable reductionof the number of candidate patterns.	Dan A. Simovici, Selim Mimaroglu	http://editions-rnti.fr/render_pdf.php?p1&p=1000781	http://editions-rnti.fr/render_pdf.php?p=1000781	818	en	en	@gmail.com, @cs.umb.edu	binary sequence association graph fast detection sequential pattern develop efficient algorithm detect frequent pattern thatoccur sequence databasis certain constraint combine useof bit vector representation sequence databasis association graph weachieve superior time low memory usage base considerable reductionof number candidate pattern	Binary Sequences and Association Graphs for Fast Detection of Sequential Patterns	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Caractérisation automatique des classes découvertes en classification non supervisée	Dans cet article, nous proposons une nouvelle approche de classifi- cation et de pondération des variables durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage de ces cartes topologiques est combiné à un mécanisme d'esti- mation de pertinences des différentes variables sous forme de poids d'influence sur la qualité de la classification. Nous proposons deux types de pondérations adaptatives : une pondération des observations et une pondération des distances entre observations. L'apprentissage simultané des pondérations et des prototypes utilisés pour la partition des observations permet d'obtenir une classification op- timisée des données. Un test statistique est ensuite utilisé sur ces pondérations pour élaguer les variables non pertinentes. Ce processus de sélection de variables permet enfin, grâce à la localité des pondérations, d'exhiber un sous ensemble de variables propre à chaque groupe (cluster) offrant ainsi sa caractérisation. L'approche proposée a été validé sur plusieurs bases de données et les résultats expérimentaux ont montré des performances très prometteuses.	Nistor Grozavu, Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000737	http://editions-rnti.fr/render_pdf.php?p=1000737	819	fr	fr	@univ-paris13.fr	caractérisation automatique des classe découvrir en classification non supervisée  Dans ce article , nous proposer un nouveau approche de classifi-cation et de pondération des variable durant un processus d' apprentissage non superviser . ce approche être baser sur le modèle des carte auto- organisatrice . le apprentissage de ce carte topologique être combiner à un mécanisme d' esti-mation de pertinence des différent variable sous forme de poids d' influence sur le qualité de le classification . Nous proposer deux type de pondération adaptatif : un pondération des observation et un pondération des distance entre observation . le apprentissage simultané des pondération et des prototype utiliser pour le partition des observation permettre d' obtenir un classification op-timisée des donnée . un test statistique être ensuite utiliser sur ce pondération pour élaguer le variable non pertinent . ce processus de sélection de variable permettre enfin , grâce à le localité des pondération , d' exhiber un sous ensemble de variable propre à chaque groupe ( cluster ) offrir ainsi son caractérisation . le approche proposer avoir être valider sur plusieurs base de donnée et le résultat expérimental avoir montrer un performance très prometteur . 	Caractérisation automatique des classes découvertes en classification non supervisée	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Ciblage des règles d'association intéressantes guidé par les connaissances du décideur	L'usage du modèle des règles d'association en fouille de données estlimité par la quantité prohibitive de règles qu'il fournit et nécessite la mise enplace d'une phase de post-traitement efficace afin de cibler les règles les plusutiles. Cet article propose une nouvelle approche intégrant explicitement lesconnaissances du décideur afin de filtrer et cibler les règles intéressantes.	Claudia Marinica, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000805	http://editions-rnti.fr/render_pdf.php?p=1000805	820	fr	fr	@univ-nantes.fr	ciblage des règle d' association intéressant guider par le connaissance du décideur  le usage du modèle des règle d' association en fouille de donnée estlimité par le quantité prohibitif de règle qu' il fournir et nécessiter le mise enplace d' un phase de post-traitement efficace afin de cibler le règle le plusutiles . ce article proposer un nouveau approche intégrer explicitement lesconnaissances du décideur afin de filtrer et cibler le règle intéressant . 	Ciblage des règles d'association intéressantes guidé par les connaissances du décideur	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	CISNA un système hybride LD+Règles pour gérer des connaissances		Alexis Bultey, François Rousselot, Cecilia Zanni, Denis Cavallucci	http://editions-rnti.fr/render_pdf.php?p1&p=1000821	http://editions-rnti.fr/render_pdf.php?p=1000821	821	fr		@insa-strasbourg.fr	CISNA un système hybride LD+Règles pour gérer des connaissances 	CISNA un système hybride LD+Règles pour gérer des connaissances	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Classification des Images de Télédétection avec ENVI FX	D'importants volumes d'images satellites et aériennes de tout type(panchromatiques, multispectrales, hyperspectrales) sont généréesquotidiennement, et leur classification par des méthodes semi-automatiquesdevient nécessaire. Le logiciel ENVI Feature eXtractionTM (ENVI FXTM) sebase sur une approche « objet » -par opposition à une approche pixelsclassique- et sur des algorithmes innovants, pour la segmentation et laclassification des images de télédétection avec un haut niveau de précision.	Franck Le Gall, Damien Barache, Ahmed Belaidi	http://editions-rnti.fr/render_pdf.php?p1&p=1000813	http://editions-rnti.fr/render_pdf.php?p=1000813	822	fr	fr	@ittvis.com, @ittvis.com, @ittvis.com	classification des image de télédétection avec ENVI FX  un important volume d' image satellite et aérien de tout type ( panchromatiques , multispectrales , hyperspectrales ) être généréesquotidiennement , et son classification par un méthode semi-automatiquesdevient nécessaire . le logiciel ENVI Feature eXtractionTM ( ENVI FXTM ) sebase sur un approche « objet » -par opposition à un approche pixelsclassique-et sur un algorithme innovant , pour le segmentation et laclassification des image de télédétection avec un haut niveau de précision . 	Classification des Images de Télédétection avec ENVI FX	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Collaborative Outlier Mining for Intrusion Detection	Intrusion detection is an important topic dealing with security of in-formation systems. Most successful Intrusion Detection Systems (IDS) rely onsignature detection and need to update their signature as fast as new attacks areemerging. On the other hand, anomaly detection may be utilized for this purpose,but it suffers from a high number of false alarms. Actually, any behaviour whichis significantly different from the usual ones will be considered as dangerousby an anomaly based IDS. Therefore, isolating true intrusions in a set of alarmsis a very challenging task for anomaly based intrusion detection. In this paper,we consider to add a new feature to such isolated behaviours before they can beconsidered as malicious. This feature is based on their possible repetition fromone information system to another. We propose a new outlier mining principleand validate it through a set of experiments.	Goverdhan Singh, Florent Masseglia, Céline Fiot, Alice Marascu, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000776	http://editions-rnti.fr/render_pdf.php?p=1000776	823	en	en	@inria.fr, @lirmm.fr	collaborative outlier mining intrusion detection intrusion detection important topic deal security in formation system successful intrusion detection system id rely onsignature detection need update signature fast new attack areemerge hand anomaly detection may utilize purpose but suffer high number false alarm actually behaviour whichi significantly different usual one consider dangerousby anomaly base id therefore isolate true intrusion set alarmsis challenge task anomaly base intrusion detection paper we consider add new feature isolate behaviour beconsider malicious feature base possible repetition fromone information system another propose new outlier mining principleand validate set experiment	Collaborative Outlier Mining for Intrusion Detection	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Comment valider automatiquement des relations syntaxiques induites	Nous présentons dans cet article des approches visant à valider desrelations syntaxiques induites de type Verbe-Objet. Ainsi, nous proposons d'u-tiliser dans un premier temps une approche s'appuyant sur des vecteurs séman-tiques déterminés à l'aide d'un thésaurus. La seconde approche emploie unevalidation Web. Nous effectuons des requêtes sur un moteur de recherche asso-ciées à des mesures statistiques afin de déterminer la pertinence d'une relationsyntaxique. Nous proposons enfin de combiner ces deux méthodes. La qualitéde nos approches de validation de relations syntaxiques a été évaluée en utilisantdes courbes ROC.	Nicolas Béchet, Mathieu Roche, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000749	http://editions-rnti.fr/render_pdf.php?p=1000749	824	fr	fr	@lirmm.fr	Comment valider automatiquement un relation syntaxique induites  Nous présenter dans ce article des approche viser à valider desrelations syntaxique induire de type Verbe-Objet . ainsi , nous proposer d' u-tiliser dans un premier temps un approche clr appuyer sur un vecteur séman-tiques déterminer à le aide d' un thésaurus . le second approche employer unevalidation Web . Nous effectuer un requête sur un moteur de recherche asso- ciées à un mesure statistique afin de déterminer le pertinence d' un relationsyntaxique . Nous proposer enfin de combiner ce deux méthode . le qualitéde son approche de validation de relation syntaxique avoir être évaluer en utilisantdes courbe ROC . 	Comment valider automatiquement des relations syntaxiques induites	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	Comparaison de distances et noyaux classiques par degré d'équivalence des ordres induits	Le choix d'une mesure pour comparer les données est au coeur destâches de recherche d'information et d'apprentissage automatique. Nous considéronsici ce problème dans le cas où seul l'ordre induit par la mesure importe,et non les valeurs numériques qu'elle fournit : cette situation est caractéristiquedes moteurs de recherche de documents par exemple. Nous étudions dans cecadre les mesures de comparaison classiques pour données numériques, tellesque les distances et les noyaux les plus courants. Nous identifions les mesureséquivalentes, qui induisent toujours le même ordre ; pour les mesures non équivalentes,nous quantifions leur désaccord par des degrés d'équivalence basés surle coefficient de Kendall généralisé. Nous étudions les équivalences et quasiéquivalencesà la fois sur les plans théorique et expérimental.	Maria Rifqi, Marcin Detyniecki, Marie-Jeanne Lesot	http://editions-rnti.fr/render_pdf.php?p1&p=1000738	http://editions-rnti.fr/render_pdf.php?p=1000738	825	fr	fr	@lip6.fr	comparaison de distance et noyau classique par degré d' équivalence des ordre induits  le choix d' un mesure pour comparer le donnée être au coeur destâches de recherche d' information et d' apprentissage automatique . Nous considéronsici ce problème dans le cas où seul le ordre induire par le mesure importer , et non le valeur numérique qu' elle fournir : ce situation être caractéristiquedes moteur de recherche de document par exemple . Nous étudier dans cecadre le mesure de comparaison classique pour donnée numérique , tellesque le distance et le noyau le plus courant . Nous identifier le mesureséquivalentes , qui induire toujours le même ordre ; pour le mesure non équivalent , nous quantifier son désaccord par un degré d' équivalence baser surle coefficient de Kendall généraliser . Nous étudier le équivalence et quasiéquivalencesà le foi sur le plan théorique et expérimental . 	Comparaison de distances et noyaux classiques par degré d'équivalence des ordres induits	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Constraint Programming for Data Mining		Luc De Raedt	http://editions-rnti.fr/render_pdf.php?p1&p=1000731	http://editions-rnti.fr/render_pdf.php?p=1000731	826	en		@cs.kuleuven.be	Constraint Programming for Data Mining 	Constraint Programming for Data Mining	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Construction de descripteurs pour classer à partir d'exemples bruités	En classification supervisée, la présence de bruit sur les valeurs desdescripteurs peut avoir des effets désastreux sur la performance des classifieurset donc sur la pertinence des décisions prises au moyen de ces modèles. Traiterce problème lorsque le bruit affecte un attribut classe a été très étudié. Il estplus rare de s'intéresser au bruit sur les autres attributs. C'est notre contextede travail et nous proposons la construction de nouveaux descripteurs robusteslorsque ceux des exemples originaux sont bruités. Les résultats expérimentauxmontrent la valeur ajoutée de cette construction par la comparaison des qualitésobtenues (e.g., précision) lorsque l'on utilise les méthodes de classification àpartir de différentes collections de descripteurs.	Nazha Selmaoui-Folcher, Jean-François Boulicaut, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1000742	http://editions-rnti.fr/render_pdf.php?p=1000742	827	fr	fr	@univ-nc.nc, @insa-lyon.fr	construction de descripteur pour classer à partir d' exemple bruités  En classification superviser , le présence de bruit sur le valeur desdescripteurs pouvoir avoir un effet désastreux sur le performance des classifieurset donc sur le pertinence des décision prendre au moyen de ce modèle . Traiterce problème lorsque le bruit affecter un attribut classe avoir être très étudier . Il estplus rare de clr intéresser au bruit sur le autre attribut . C' être son contextede travail et nous proposer le construction de nouveau descripteur robusteslorsque celui des exemple original être bruiter . le résultat expérimentauxmontrent le valeur ajouter de ce construction par le comparaison des qualitésobtenues ( e.g. , précision ) lorsque le on utiliser le méthode de classification àpartir de différent collection de descripteur . 	Construction de descripteurs pour classer à partir d'exemples bruités	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Contrôle des observations pour la gestion des systèmes de flux de données.	Les systèmes d'analyse de flux de données prennent de plus en plusd'importance dans un contexte où les données circulant sur les réseaux sont deplus en plus volumineuses et où la volonté de réagir au plus vite, en temps réel,devient un besoin nécessaire. Afin de permettre des analyses aussi rapides etefficaces que possible, il convient de pouvoir contrôler les flots de données et defocaliser les traitements sur les données pertinentes. Le protocole présenté dansce papier donne au module de traitement des capacités d'action et de contrôle surles observations remontantes en fonction de l'état de l'analyse. La diminutiondes flux résultant de telles focalisations permet des traitements beaucoup plusefficaces, plus pertinents et moins consommateurs de ressources. Les premiersrésultats montrent un réel gain de performances sur nos applications (facteur100).	Pierre Le Maigat, Christophe Dousson	http://editions-rnti.fr/render_pdf.php?p1&p=1000798	http://editions-rnti.fr/render_pdf.php?p=1000798	828	fr	fr	@orange-ftgroup.com	contrôle des observation pour le gestion des système de flux de donnée .  le système d' analyse de flux de donnée prendre de plus en plusd'importance dans un contexte où le donnée circuler sur le réseau être deplus en plus volumineux et où le volonté de réagir au plus vite , en temps réel , devenir un besoin nécessaire . Afin de permettre un analyse aussi rapide etefficaces que possible , il convenir de pouvoir contrôler le flot de donnée et defocaliser le traitement sur le donnée pertinent . le protocole présenter dansce papier donner au module de traitement des capacité d' action et de contrôle surles observation remontant en fonction de le état de le analyse . le diminutiondes flux résulter de tel focalisation permettre un traitement beaucoup plusefficaces , plus pertinent et moins consommateur de ressource . le premiersrésultats montrer un réel gain de performance sur son application ( facteur100 ) . 	Contrôle des observations pour la gestion des systèmes de flux de données.	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Correspondances de Galois pour la manipulation de contextes flous multi-valués	L'analyse formelle de concepts est une méthode fondée sur la correspondancede Galois et qui permet de construire des hiérarchies de conceptsformels à partir de tableaux de données binaires. Cependant de nombreux problèmesréels abordés en fouille de données comportent des données plus complexes.Afin de traiter de tels problèmes, nous proposons une conversion de donnéesfloues multi-valuées en attributs histogrammes et une correspondance deGalois adaptée à ce format. Notre propos est illustré avec un jeu de donnéessimples. Enfin, nous évaluons brièvement les résultats et les apports de cettecorrespondance de Galois par rapport à l'approche classique	Aurélie Bertaux, Florence Le Ber, Agnès Braud	http://editions-rnti.fr/render_pdf.php?p1&p=1000763	http://editions-rnti.fr/render_pdf.php?p=1000763	829	fr	fr	@engees.u-strasbg.fr, @urs.u-strasbg.fr	correspondance de Galois pour le manipulation de contexte flou multi-valués  le analyse formel de concept être un méthode fonder sur le correspondancede Galois et qui permettre de construire un hiérarchie de conceptsformels à partir de tableau de donnée binaire . cependant de nombreux problèmesréels aborder en fouille de donnée comporter un donnée plus complexe . Afin de traiter de tel problème , nous proposer un conversion de donnéesfloues multi-valuées en attribut histogramme et un correspondance deGalois adapter à ce format . son propos être illustrer avec un jeu de donnéessimples . enfin , nous évaluer brièvement le résultat et le apport de cettecorrespondance de Galois par rapport à le approche classique 	Correspondances de Galois pour la manipulation de contextes flous multi-valués	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	DBFrequentQueries : Extraction de requêtes fréquentes		Lucie Copin, Nicolas Pecheur, Anne Laurent, Yudi Augusta, Budi Sentana, Dominique Laurent, Tao-Yuan Jen	http://editions-rnti.fr/render_pdf.php?p1&p=1000822	http://editions-rnti.fr/render_pdf.php?p=1000822	830	fr		@gmail.com, @stikom-bali.ac.id, @lirmm.fr, @u-cergy.fr	DBFrequentQueries : Extraction de requêtes fréquentes 	DBFrequentQueries : Extraction de requêtes fréquentes	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	De l'utilisation de l'Analyse de Données Symboliques dans les Systèmes multi-agents	L'exploitation en temps réel de connaissances complexes est un défidans de nombreux domaines, tels que le web sémantique, la simulation ou lessystèmes multi-agents (SMA). Dans le paradigme multi-agents, des travaux ré-cents montrent que les communications multi-parties (CMP) offrent des oppor-tunités intéressantes en termes de réalisme des communications, diffusion desconnaissances et sémantique des actes de langage. Cependant, ces travaux seheurtent à la difficulté de mise en oeuvre des CMP, pour lesquelles les supportsde communications classiques sont insuffisants. Dans cet article, nous propo-sons d'utiliser le formalisme de l'Analyse de Données Symboliques (ADS) pourmodéliser les informations et les besoins des agents. Nous appuyons le routagedes messages sur cette modélisation dans le cadre d'un environnement de com-munication pour les systèmes multi-agents. Afin d'illustrer notre propos, nousutiliserons l'exemple de la gestion des communications dans un poste d'appelsd'urgence. Nous présentons ensuite notre retour d'expérience, et discutons lesperspectives ouvertes par la fertilisation croisée de l'ADS et des SMA.	Flavien Balbo, Julien Saunier, Edwin Diday, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1000746	http://editions-rnti.fr/render_pdf.php?p=1000746	831	fr	fr	@lamsade.dauphine.fr, @ceremade.dauphine.fr	De le utilisation de le analyse de Données Symboliques dans le système multi-agents  le exploitation en temps réel de connaissance complexe être un défidans de nombreux domaine , tel que le web sémantique , le simulation ou lessystèmes multi-agents ( SMA ) . Dans le paradigme multi-agents , un travail ré-cents montrer que le communication multi-parties ( CMP ) offrir un oppor-tunités intéressant en terme de réalisme des communication , diffusion desconnaissances et sémantique des acte de langage . cependant , ce travail seheurtent à le difficulté de mise en oeuvre des CMP , pour lesquelles le supportsde communication classique être insuffisant . Dans ce article , nous propo- son d' utiliser le formalisme de le analyse de Données Symboliques ( ADS ) pourmodéliser le information et le besoin des agent . Nous appuyer le routagedes message sur ce modélisation dans le cadre d' un environnement de com-munication pour le système multi-agents . Afin d' illustrer son propos , nousutiliserons le exemple de le gestion des communication dans un poste d' appelsd'urgence . Nous présenter ensuite son retour d' expérience , et discuter lesperspectives ouvrir par le fertilisation croiser de le ADS et des SMA . 	De l'utilisation de l'Analyse de Données Symboliques dans les Systèmes multi-agents	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Définition d'une stratégie de résolution de problèmes pour un robot humanoïde	"Nous avons développé un système dont le but est d'obtenir le logicielde commande d'un robot capable de simuler le comportement d'un humainplacé en situation de résolution de problèmes. Nous avons résolu ce problèmedans un environnement psychologique particulier où les comportements humainspeuvent être interprétés comme des 'observables' de leurs stratégies derésolution de problèmes. Notre solution contient de plus celle d'un autre problème,celui de construire une boucle complète commençant avec le comportementd'un groupe d'humains, son analyse et son interprétation en termesd'observables humaines, la définition des stratégies utilisées par les humains (ycompris celles qui sont inefficaces), l'interprétation des observables humainesen terme de mouvements du robot, la définition de ce qu'est une ""stratégie derobot "" en terme de stratégies humaines. La boucle est bouclée avec un langagede programmation capable de programmer ces stratégies robotiques, qui deviennentainsi à leur tour des observables, tout comme l'ont été les stratégieshumaines du début de la boucle. Nous expliquons comment nous avons été capablesdéfinir de façon objective ce que nous appelons une stratégie de robot.Notre solution assemble deux facteurs différents. L'un permet d'éviter lescomportements 'inhumains' et se fonde sur la moyenne des comportementsdes humains que nous avons observés. L'autre fournit une sorte 'd'humanité'au robot en lui permettant de dévier de cette moyenne par n fois l'écart typeobservé chez les humains qu'il doit simuler. Il devient alors possible de programmerdes comportements complètements humains."	Mary Felkin, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000806	http://editions-rnti.fr/render_pdf.php?p=1000806	832	fr	fr	@lri.fr, @lri.fr	définition d' un stratégie de résolution de problème pour un robot humanoïde  " Nous avoir développer un système dont le but être d' obtenir le logicielde commander d' un robot capable de simuler le comportement d' un humainplacé en situation de résolution de problème . . Nous avoir résoudre ce problèmedans un environnement psychologique particulier où le comportement humainspeuvent être interpréter comme un ' observable ' de son stratégie derésolution de problème . . son solution contenir de plus celui d' un autre problème , celui de construire un boucle complet commencer avec le comportementd'un groupe d' humain , son analyse et son interprétation en termesd'observables humain , le définition des stratégie utiliser par le humain ( ycompris celui qui être inefficace ) , le interprétation des observable humainesen terme de mouvement du robot , le définition de ce qu' être un " " stratégie derobot " " en terme de stratégie humain . . le boucle être boucler avec un langagede programmation capable de programmer ce stratégie robotique , qui deviennentainsi à son tour des observable , tout comme l' avoir être le stratégieshumaines du début de le boucle . . Nous expliquer comment nous avoir être capablesdéfinir de façon objectif ce que nous appeler un stratégie de robot . . son solution assembler deux facteur différent . . le un permettre d' éviter lescomportements ' inhumain ' et clr fonder sur le moyenne des comportementsdes humain que nous avoir observer . . le autre fournir un sorte ' d' humanité'au robot en lui permettre de dévier de ce moyenne par n foi le écart typeobservé chez le humain qu' il devoir simuler . Il devenir alors possible de programmerdes comportement complètement humain . " 	Définition d'une stratégie de résolution de problèmes pour un robot humanoïde	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	DEMON : Découverte de motifs séquentiels pour les puces adn	Prometteuses en terme de prévention, de dépistage, de diagnostic etd'actions thérapeutiques, les puces à ADN mesurent l'intensité des expressionsde plusieurs milliers de gènes. Dans cet article, nous proposons une nouvelleapproche appelée DEMON, pour extraire des motifs séquentiels à partir de don-nées issues des puces ADN et qui utilise des connaissances du domaine.	Paola Salle, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000803	http://editions-rnti.fr/render_pdf.php?p=1000803	833	fr	fr	@lirmm.fr	DEMON : découverte de motif séquentiel pour le puce adn  prometteur en terme de prévention , de dépistage , de diagnostic etd'actions thérapeutique , le puce à ADN mesurer le intensité des expressionsde plusieurs millier de gène . Dans ce article , nous proposer un nouvelleapproche appeler DEMON , pour extraire un motif séquentiel à partir de don _-né issir des puce ADN et qui utiliser un connaissance du domaine . 	DEMON : Découverte de motifs séquentiels pour les puces adn	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	DEMON-Visualisation : un outil pour la visualisation des motifs séquentiels extraits à partir de données biologiques		Wei Xing, Paola Salle, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000818	http://editions-rnti.fr/render_pdf.php?p=1000818	834	fr		@lirmm.fr	DEMON-Visualisation : un outil pour la visualisation des motifs séquentiels extraits à partir de données biologiques 	DEMON-Visualisation : un outil pour la visualisation des motifs séquentiels extraits à partir de données biologiques	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	DesEsper: un logiciel de pré-traitement de flux appliqué à la surveillance des centrales hydrauliques		Frédéric Flouvat, Sébastien Gassmann, Jean-Marc Petit, Alain Ribière	http://editions-rnti.fr/render_pdf.php?p1&p=1000819	http://editions-rnti.fr/render_pdf.php?p=1000819	835	fr		@univ-nc.nc, @insa-lyon.fr, @edf.fr	DesEsper: un logiciel de pré-traitement de flux appliqué à la surveillance des centrales hydrauliques 	DesEsper: un logiciel de pré-traitement de flux appliqué à la surveillance des centrales hydrauliques	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection de séquences atypiques basée sur un modèle de Markov d'ordre variable	Récemment, le nombre et le volume des bases de données séquentiellesbiologiques ont augmenté de manière considérable. Dans ce contexte, l'identificationdes anomalies est essentielle. La plupart des approches pour lesextraire se fondent sur une base d'apprentissage ne contenant pas d'outlier. Or,dans de très nombreuses applications, les experts ne disposent pas d'une tellebase. De plus, les méthodes existantes demeurent exigeantes en mémoire, cequi les rend souvent impossibles à utiliser. Nous présentons dans cet article unenouvelle approche, basée sur un modèle de Markov d'ordre variable et sur unemesure de similarité entre objets séquentiels. Nous ajoutons aux méthodes existantesun critère d'élagage pour contrôler la taille de l'espace de rechercheet sa qualité, ainsi qu'une inégalité de concentration précise pour la mesure desimilarité, conduisant à une meilleure détection des outliers. Nous démontronsexpérimentalement la validité de notre approche.	Cécile Low-Kam, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000766	http://editions-rnti.fr/render_pdf.php?p=1000766	836	fr	fr	@math.univ-montp2.fr, @lirmm.fr	détection de séquence atypique baser sur un modèle de Markov d' ordre variable  récemment , le nombre et le volume des base de donnée séquentiellesbiologiques avoir augmenter de manière considérable . Dans ce contexte , le identificationdes anomalie être essentiel . le plupart des approche pour lesextraire clr fonder sur un base d' apprentissage ne contenir pas d' outlier . Or , dans de très nombreux application , le expert ne disposer pas d' un tellebase . De plus , le méthode existant demeurer exigeant en mémoire , cequi les rendre souvent impossible à utiliser . Nous présenter dans ce article unenouvelle approche , baser sur un modèle de Markov d' ordre variable et sur unemesure de similarité entre objet séquentiel . Nous ajouter aux méthode existantesun critère d' élagage pour contrôler le taille de le espace de rechercheet son qualité , ainsi qu' un inégalité de concentration précis pour le mesure desimilarité , conduire à un meilleur détection des outliers . Nous démontronsexpérimentalement le validité de son approche . 	Détection de séquences atypiques basée sur un modèle de Markov d'ordre variable	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection d'intrusions dans un environnement collaboratif sécurisé	Pour pallier le problème des attaques sur les réseaux de nouvelles ap-proches de détection d'anomalies ou d'abus ont été proposées ces dernières an-nées et utilisent des signatures d'attaques pour comparer une nouvelle requêteet ainsi déterminer s'il s'agit d'une attaque ou pas. Cependant ces systèmes sontmis à défaut quand la requête n'existe pas dans la base de signature. Généra-lement, ce problème est résolu via une expertise humaine afin de mettre à jourla base de signatures. Toutefois, il arrive fréquemment qu'une attaque ait déjàété détectée dans une autre organisation et il serait utile de pouvoir bénéficier decette connaissance pour enrichir la base de signatures mais cette information estdifficile à obtenir car les organisations ne souhaitent pas forcément indiquer lesattaques qui ont eu lieu sur le site. Dans cet article nous proposons une nouvelleapproche de détection d'intrusion dans un environnement collaboratif sécurisé.Notre approche permet de considérer toute signature décrite sous la forme d'ex-pressions régulières et de garantir qu'aucune information n'est divulguée sur lecontenu des différents sites.	Nischal Verma, François Trousset, Pascal Poncelet, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000775	http://editions-rnti.fr/render_pdf.php?p=1000775	837	fr	fr	@gmail.com, @ema.fr, @lirmm.fr, @inria.fr	détection d' intrusion dans un environnement collaboratif sécurisé  Pour pallier le problème des attaque sur le réseau de nouveau ap-proches de détection d' anomalie ou d' abus avoir être proposer ce dernier an _-né et utiliser un signature d' attaque pour comparer un nouveau requêteet ainsi déterminer s' il clr agir d' un attaque ou pas . cependant ce système sontmis à défaut quand le requête n' exister pas dans le base de signature . Généra-lement , ce problème être résoudre via un expertise humain afin de mettre à jourla base de signature . toutefois , il arriver fréquemment qu' un attaque avoir déjàété détecter dans un autre organisation et il être utile de pouvoir bénéficier decette connaissance pour enrichir le base de signature mais ce information estdifficile à obtenir car le organisation ne souhaiter pas forcément indiquer lesattaques qui avoir avoir lieu sur le site . Dans ce article nous proposer un nouvelleapproche de détection d' intrusion dans un environnement collaboratif sécuriser . son approche permettre de considérer tout signature décrire sous le forme d' ex- pression régulier et de garantir qu' aucun information n' être divulguer sur lecontenu des différent site . 	Détection d'intrusions dans un environnement collaboratif sécurisé	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection d'objets atypiques dans un flot de données : une approche multi-résolution		Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000801	http://editions-rnti.fr/render_pdf.php?p=1000801	838	fr		@inria.fr	Détection d'objets atypiques dans un flot de données : une approche multi-résolution 	Détection d'objets atypiques dans un flot de données : une approche multi-résolution	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détermination du nombre des classes dans l'algorithme CROKI2 de classification croisée	Un des problèmes majeurs de la classification non supervisée est ladétermination ou la validation du nombre de classes dans la population. Ce problèmes'étend aux méthodes de bipartitionnement ou block clustering. Dans cepapier, nous nous intéressons à l'algorithme CROKI2 de classification croiséedes tableaux de contingence proposé par Govaert (1983). Notre objectif est dedéterminer le nombre de classes optimal sur les lignes et les colonnes à traversun ensemble de techniques de validation de classes proposés dans la littératurepour les méthodes classiques de classification.	Malika Charrad, Yves Lechevallier, Gilbert Saporta, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000797	http://editions-rnti.fr/render_pdf.php?p=1000797	839	fr	fr	@riadi.rnu.tn, @riadi.rnu.tn, @inria.fr, @cnam.fr	détermination du nombre des classe dans le algorithme CROKI2 de classification croisée  un des problème majeur de le classification non superviser être ladétermination ou le validation du nombre de classe dans le population . ce problèmes'étend aux méthode de bipartitionnement ou block clustering . Dans cepapier , nous clr intéresser à le algorithme CROKI2 de classification croiséedes tableau de contingence proposer par Govaert ( 1983 ) . son objectif être dedéterminer le nombre de classe optimal sur le ligne et le colonne à traversun ensemble de technique de validation de classe proposer dans le littératurepour le méthode classique de classification . 	Détermination du nombre des classes dans l'algorithme CROKI2 de classification croisée	5
Revue des Nouvelles Technologies de l'Information	EGC	2009	Diagnostic multi-sources adaptatif Application à la détection d'intrusion dans des serveursWeb	Le but d'un système adaptatif de diagnostic est de surveiller et diagnostiquerun système tout en s'adaptant à son évolution. Ceci passe par l'adaptationdes diagnostiqueurs qui précisent ou enrichissent leur propre modèle poursuivre au mieux le système au fil du temps. Pour détecter les besoins d'adaptation,nous proposons un cadre de diagnostic multi-sources s'inspirant de lafusion d'information. Des connaissances fournies par le concepteur sur des relationsattendues entre les diagnostiqueurs mono-source forment un méta-modèledu diagnostic. La compatibilité des résultats du diagnostic avec le méta-modèleest vérifiée en ligne. Lorsqu'une de ces relations n'est pas vérifiée, les diagnostiqueursconcernés sont modifiés.Nous appliquons cette approche à la conception d'un système adaptatif de détectiond'intrusion à partir d'un flux de connexions à un serveur Web. Les évaluationsdu système mettent en évidence sa capacité à améliorer la détection desintrusions connues et à découvrir de nouveaux types d'attaque.	Thomas Guyet, Wei Wang    , Rene Quiniou, Marie-Odile Cordier	http://editions-rnti.fr/render_pdf.php?p1&p=1000777	http://editions-rnti.fr/render_pdf.php?p=1000777	840	fr	fr	@irisa.fr, @gmail.fr	diagnostic multi-sources adaptatif application à le détection d' intrusion dans un serveursWeb  le but d' un système adaptatif de diagnostic être de surveiller et diagnostiquerun système tout en clr adapter à son évolution . ceci passer par le adaptationdes diagnostiqueurs qui préciser ou enrichir son propre modèle poursuivre au mieux le système au fil du temps . Pour détecter le besoin d' adaptation , nous proposer un cadre de diagnostic multi-sources clr inspirer de lafusion d' information . un connaissance fournir par le concepteur sur un relationsattendues entre le diagnostiqueurs mono- source former un méta-modèledu diagnostic . le compatibilité des résultat du diagnostic avec le méta-modèleest vérifier en ligne . Lorsqu' un de ce relation n' être pas vérifier , le diagnostiqueursconcernés être modifier . Nous appliquer ce approche à le conception d' un système adaptatif de détectiond'intrusion à partir d' un flux de connexion à un serveur Web . le évaluationsdu système mettre en évidence son capacité à améliorer le détection desintrusions connaître et à découvrir un nouveau type d' attaque . 	Diagnostic multi-sources adaptatif Application à la détection d'intrusion dans des serveursWeb	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	Empreintes conceptuelles et spatiales pour la caractérisation des réseaux sociaux	Cet article propose une méthode reposant sur l'utilisation del'Analyse Formelle de Concepts et des treillis de Galois pour l'analyse desystèmes complexes. Des statistiques reposant sur ces treillis permettent decalculer la distribution conceptuelle des objets classifiés par le treillis.L'expérimentation sur des échantillons de trois réseaux sociaux en ligneillustre l'utilisation de ces statistiques pour la caractérisation globale et pour lefiltrage automatique de ces systèmes.	Bénédicte Le Grand, Marie-Aude Aufaure, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1000779	http://editions-rnti.fr/render_pdf.php?p=1000779	841	fr	fr	@lip6.fr, @ecp.fr	empreinte conceptuel et spatial pour le caractérisation des réseau sociaux  ce article proposer un méthode reposer sur le utilisation del'Analyse Formelle de Concepts et des treillis de Galois pour le analyse desystèmes complexe . un statistique reposer sur ce treillis permettre decalculer le distribution conceptuel des objet classifier par le treillis . le expérimentation sur un échantillon de trois réseau social en ligneillustre le utilisation de ce statistique pour le caractérisation global et pour lefiltrage automatique de ce système . 	Empreintes conceptuelles et spatiales pour la caractérisation des réseaux sociaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Exploration de données de traçabilité issues de la RFID par apprentissage non-supervisé	La RFID (Radio Frequency IDentification) est une technologie avancée d'enregistrementde données spatio-temporelles de traçabilité. L'objectif de ce travail est de transformer cesdonnées spatio-temporelles en connaissances exploitables par les utilisateurs par l'intermé-diaire d'une méthode de classification automatique des données. Les systèmes RFID peuventêtre utilisés pour étudier les sociétés animales, qui sont des systèmes dynamiques complexescaractérisés par beaucoup d'interactions entre les individus (Fresneau et al., 1989). Le cadreapplicatif choisi pour ce travail est l'étude de la structure d'un groupe d'individus en interactionsociale et en particulier la division du travail au sein d'une colonie de fourmis1.La RFID générant d'importants volumes de données, il est nécessaire de développer desméthodes appropriées afin d'en comprendre le sens. Nous proposons pour cela un algorithmede classification topographique non-supervisée pour l'exploration de ce type de données, ca-pable de détecter les groupes d'individus exprimant le même comportement. L'algorithmeDS2L-SOM (Density-based Simultaneous Two-Level - SOM, Cabanes et Bennani (2008)) estcapable de détecter non seulement les groupes définis par une zone vide de donnée, grâce àune estimation de la pertinence des connexions entre référents, mais aussi les groupes défi-nis seulement par une diminution de densité, grâce à une estimation de la densité autour desréférents pendant l'apprentissage.	Guénaël Cabanes, Younès Bennani, Dominique Fresneau	http://editions-rnti.fr/render_pdf.php?p1&p=1000799	http://editions-rnti.fr/render_pdf.php?p=1000799	842	fr	fr	@univ-paris13.fr, @univ-paris13.fr	exploration de donnée de traçabilité issu de le RFID par apprentissage non- supervisé  le RFID ( radio Frequency identification ) être un technologie avancer d' enregistrementde donner spatio- temporel de traçabilité . le objectif de ce travail être de transformer cesdonnées spatio- temporel en connaissance exploitable par le utilisateur par le intermé-diaire d' un méthode de classification automatique des donnée . le système RFID peuventêtre utiliser pour étudier le société animal , qui être un système dynamique complexescaractérisés par beaucoup d' interaction entre le individu ( Fresneau et al. , 1989 ) . le cadreapplicatif choisir pour ce travail être le étude de le structure d' un groupe d' individu en interactionsociale et en particulier le division du travail au sein d' un colonie de fourmis1 . le RFID générer un important volume de donnée , il être nécessaire de développer desméthodes approprier afin d' en comprendre le sens . Nous proposer pour cela un algorithmede classification topographique non- superviser pour le exploration de ce type de donnée , ca-pable de détecter le groupe d' individu exprimer le même comportement . le algorithme DS2L-SOM ( Density-based Simultaneous Two- Level - SOM , Cabanes et Bennani ( 2008 ) ) estcapable de détecter non seulement le groupe définir par un zone vide de donner , grâce àune estimation de le pertinence des connexion entre référent , mais aussi le groupe défi-nis seulement par un diminution de densité , grâce à un estimation de le densité autour desréférents pendant le apprentissage . 	Exploration de données de traçabilité issues de la RFID par apprentissage non-supervisé	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Exploration des corrélations dans un classifieur Application au placement d'offres commerciales	Cet article présente une nouvelle méthode permettant d'explorer lesprobabilités délivrées par un modèle prédictif de classification. L'augmentationde la probabilité d'occurrence de l'une des classes du problème étudié est analyséeen fonction des variables explicatives prises isolément. La méthode proposéeest posée et illustrée dans un cadre général, puis explicitement dédiée au classifieurBayesien naïf. Son illustration sur les données du challenge PAKDD 2007montre que ce type d'exploration permet de créer des indicateurs performantsd'aide à la vente.	Vincent Lemaire, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1000739	http://editions-rnti.fr/render_pdf.php?p=1000739	843	fr	fr	@orange-ftgroup.com, @gfi.fr	exploration des corrélation dans un classifieur Application au placement d' offre commerciales  ce article présenter un nouveau méthode permettre d' explorer lesprobabilités délivrer par un modèle prédictif de classification . le augmentationde le probabilité d' occurrence de le un des classe du problème étudier être analyséeen fonction des variable explicatives prendre isolément . le méthode proposéeest poser et illustrer dans un cadre général , puis explicitement dédier au classifieurBayesien naïf . son illustration sur le donnée du challenge PAKDD 2007montre que ce type d' exploration permettre de créer un indicateur performantsd'aide à le vente . 	Exploration des corrélations dans un classifieur Application au placement d'offres commerciales	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Explorer3D : classification et visualisation de données		Matthieu Exbrayat, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1000817	http://editions-rnti.fr/render_pdf.php?p=1000817	844	fr		@univ-orleans.fr	Explorer3D : classification et visualisation de données 	Explorer3D : classification et visualisation de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction de motifs fermés dans des relations n-aires bruitées	L'extraction de motifs fermés dans des relations binaires a été trèsétudiée. Cependant, de nombreuses relations intéressantes sont n-aires avec n >2 et bruitées (nécessité d'une tolérance aux exceptions). Récemment, ces deuxproblèmes ont été traités indépendamment. Nous introduisons notre propositionpour combiner de telles fonctionnalités au sein d'un même algorithme.	Loïc Cerf, Jérémy Besson, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000748	http://editions-rnti.fr/render_pdf.php?p=1000748	845	fr	fr	@liris.cnrs.fr	extraction de motif fermer dans un relation n-aires bruitées  le extraction de motif fermer dans un relation binaire avoir être trèsétudiée . cependant , un nombreux relation intéressant être n-aires avec n suivre 2 et bruiter ( nécessiter d' un tolérance aux exception ) . récemment , ce deuxproblèmes avoir être traiter indépendamment . Nous introduire son propositionpour combiner de tel fonctionnalité au sein d' un même algorithme . 	Extraction de motifs fermés dans des relations n-aires bruitées	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction de Règles de Corrélation Décisionnelles	Dans cet article, nous introduisons deux nouveaux concepts : les règlesde corrélation décisionnelles et les vecteurs de contingence. Le premier résulted'un couplage entre les règles de corrélation et les règles de décision. Il permetde mettre en évidence des liens pertinents entre certains ensembles de motifsd'une relation binaire et les valeurs d'un attribut cible (appartenant à cette mêmerelation) en se basant à la fois sur la mesure du Khi-carré et sur le support desmotifs extraits. De par la nature du problème, les algorithmes par niveaux fontque l'extraction des résultats a lieu avec des temps de réponse élevés et uneoccupation mémoire importante. Afin de palier à ces deux inconvénients, nousproposons un algorithme basé sur l'ordre lectique et les vecteurs de contingence.	Alain Casali, Christian Ernst	http://editions-rnti.fr/render_pdf.php?p1&p=1000761	http://editions-rnti.fr/render_pdf.php?p=1000761	846	fr	fr	@lif.univ-mrs.fr, @emse.fr	extraction de règle de corrélation Décisionnelles  Dans ce article , nous introduire deux nouveau concept : le règlesde corrélation décisionnel et le vecteur de contingence . le premier résulted'un couplage entre le règle de corrélation et le règle de décision . Il permetde mettre en évidence des lien pertinent entre certain ensemble de motifsd'une relation binaire et le valeur d' un attribut cible ( appartenir à ce mêmerelation ) en clr baser à le foi sur le mesure du Khi-carré et sur le support desmotifs extrait . De par le nature du problème , le algorithme par niveau fontque le extraction des résultat avoir lieu avec un temps de réponse élever et uneoccupation mémoire important . Afin de palier à ce deux inconvénient , nousproposons un algorithme baser sur le ordre lectique et le vecteur de contingence . 	Extraction de Règles de Corrélation Décisionnelles	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction efficace de règles graduelles	"Les règles graduelles suscitent depuis quelques années un intérêt croissant.De telles règles, de la forme ""Plus (moins) A1 et ... plus (moins) An alorsplus (moins) B1 et ... plus (moins) Bn"" trouvent application dans de nombreuxdomaines tels que la bioinformatique, les contrôleurs flous, les relevés de capteursou encore les flots de données. Ces bases, souvent composées d'un grandnombre d'attributs, restent un verrou pour l'extraction automatique de connaissances,car elles rendent inefficaces les techniques de fouille habituelles (règlesd'association, clustering...). Dans cet article, nous proposons un algorithme efficaced'extraction d'itemset graduels basé sur l'utilisation des treillis. Nous définissonsformellement les notions de gradualité, ainsi que les algorithmes associés.Des expérimentations menées sur jeux de données synthétiques et réelsmontrent l'intérêt de notre méthode"	Lisa Di-Jorio, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000764	http://editions-rnti.fr/render_pdf.php?p=1000764	847	fr	fr	@lirmm.fr	extraction efficace de règle graduelles  " le règle graduel susciter depuis quelque année un intérêt croissant . . un tel règle , de le forme " " plus ( moins ) A1 et ... ... plus ( moins ) an alorsplus ( moins ) B1 et ... ... plus ( moins ) Bn " " trouver application dans de nombreuxdomaines tel que le bioinformatique , le contrôleur flou , le relever de capteursou encore le flot de donnée . . ce base , souvent composer d' un grandnombre d' attribut , rester un verrou pour le extraction automatique de connaissance , car elles rendre inefficace le technique de fouille habituel ( règlesd'association , clustering ... ) . . Dans ce article , nous proposer un algorithme efficaced'extraction d' itemset graduel baser sur le utilisation des treillis . . Nous définissonsformellement le notion de gradualité , ainsi que le algorithme associer . . un expérimentation mener sur jeu de donnée synthétique et réelsmontrent le intérêt de son méthode " 	Extraction efficace de règles graduelles	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	FCP-Growth, une adaptation de FP-Growth pour générer des règles d'association de classe		Emna Bahri, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000804	http://editions-rnti.fr/render_pdf.php?p=1000804	848	fr		@univ-lyon2.fr	FCP-Growth, une adaptation de FP-Growth pour générer des règles d'association de classe 	FCP-Growth, une adaptation de FP-Growth pour générer des règles d'association de classe	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Fouille de données dans les bases relationnelles pour l'acquisition d'ontologies riches en hiérarchies de classes	De par leur caractère structuré, les bases de données relationnellessont des sources précieuses pour la construction automatisée d'ontologies. Ce-pendant, une limite persistante des approches existantes est la production d'onto-logies de structure calquée sur celles des schémas relationnels sources. Dans cetarticle, nous décrivons la méthode RTAXON dont la particularité est d'identifierdes motifs de catégorisation dans les données afin de produire des ontologiesplus structurées, riches en hiérarchies. La méthode formalisée combine analyseclassique du schéma relationnel et fouille des données pour l'identification destructures hiérarchiques.	Farid Cerbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000786	http://editions-rnti.fr/render_pdf.php?p=1000786	849	fr	fr	@dassault-aviation.fr	fouille de donnée dans le base relationnel pour le acquisition d' ontologie riche en hiérarchie de classes  De par son caractère structurer , le base de donnée relationnellessont des source précieux pour le construction automatiser d' ontologie . Ce-pendant , un limite persistant des approche existant être le production d' onto- logies de structure calquer sur celui des schéma relationnel source . Dans cetarticle , nous décrire le méthode RTAXON dont le particularité être d' identifierdes motif de catégorisation dans le donnée afin de produire un ontologiesplus structurer , riche en hiérarchie . le méthode formaliser combiner analyseclassique du schéma relationnel et fouiller un donnée pour le identification destructurer hiérarchique . 	Fouille de données dans les bases relationnelles pour l'acquisition d'ontologies riches en hiérarchies de classes	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Fusion Symbolique pour la Recommandation de Programmes Télévisées	Nous proposons une approche générique pour la fusion d'informa-tions qui repose sur l'utilisation du modèle des Graphes Conceptuels et l'opé-ration de jointure maximale. Nous validons notre approche par le biais d'ex-périmentations. Ces expérimentations soulignent l'importance des heuristiquesmises en place.	Claire Laudy, Jean-Gabriel Ganascia	http://editions-rnti.fr/render_pdf.php?p1&p=1000796	http://editions-rnti.fr/render_pdf.php?p=1000796	850	fr	fr	@thalesgroup.com, @lip6.fr	fusion symbolique pour le recommandation de Programmes Télévisées  Nous proposer un approche générique pour le fusion d' informa-tions qui reposer sur le utilisation du modèle des graphe Conceptuels et le opé-ration de jointure maximal . Nous valider son approche par le biais d' ex- périmentations . ce expérimentation souligner le importance des heuristiquesmises en place . 	Fusion Symbolique pour la Recommandation de Programmes Télévisées	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Générer des règles de classification par Dopage de Concepts Formels	La classification supervisée est une tâche de fouille de données (DataMining), qui consiste à construire un classifieur à partir d'un ensemble d'exemplesétiquetés par des classes (phase d'apprentissage) et ensuite prédire les classesdes nouveaux exemples avec ce classifieur (phase de classification). En classi-fication supervisée, plusieurs approches ont été proposées dont l'approche ba-sée sur l'Analyse de Concepts Formels. L'apprentissage de Concepts Formelsest basé généralement sur la structure mathématique du treillis de Galois (outreillis de concepts). Cependant, la complexité exponentielle de génération d'untreillis de Galois a limité les champs d'application de ces systèmes. Dans cetarticle, nous présentons plusieurs méthodes de classification supervisée baséessur l'Analyse de Concepts Formels. Nous présentons aussi le boosting (dopage)de classifieurs, une technique de classification innovante. Enfin, nous proposonsle boosting de concepts formels, une nouvelle méthode adaptative qui construitseulement une partie du treillis englobant les meilleurs concepts. Ces conceptssont utilisés comme étant des règles de classification. Les résultats expérimen-taux réalisés ont prouvé l'intérêt de la méthode proposée par rapport à cellesexistantes.	Mondher Maddouri, Nida Meddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000760	http://editions-rnti.fr/render_pdf.php?p=1000760	851	fr	fr	@gmail.com, @fst.rnu.tn	générer un règle de classification par Dopage de Concepts Formels  le classification superviser être un tâche de fouille de donnée ( DataMining ) , qui consister à construire un classifieur à partir d' un ensemble d' exemplesétiquetés par un classe ( phase d' apprentissage ) et ensuite prédire le classesdes nouveau exemple avec ce classifieur ( phase de classification ) . En classi-fication superviser , plusieurs approche avoir être proposer dont le approche ba-sée sur le analyse de concept Formels . le apprentissage de Concepts Formelsest baser généralement sur le structure mathématique du treillis de Galois ( outreillis de concept ) . cependant , le complexité exponentiel de génération d' untreillis de Galois avoir limiter le champ d' application de ce système . Dans cetarticle , nous présenter plusieurs méthode de classification superviser baséessur le analyse de concept Formels . Nous présenter aussi le boosting ( dopage ) de classifieur , un technique de classification innovant . enfin , nous proposonsle boosting de concept formel , un nouveau méthode adaptatif qui construitseulement un partie du treillis englober le meilleur concept . ce conceptssont utiliser comme être un règle de classification . le résultat expérimen-taux réaliser avoir prouver le intérêt de le méthode proposer par rapport à cellesexistantes . 	Générer des règles de classification par Dopage de Concepts Formels	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Graphes des liens et anti liens statistiquement valides entre les mots d'un corpus textuel : test de randomisation TourneBool sur le corpus Reuters	"La définition du voisinage est un élément central en fouille de données, et de nombreuses définitions ont été avancées. Nous en proposons ici une version statistique issue de notre test de randomisation TourneBool, qui permet, à partir d'un tableau de relations binaires objets décrits/descripteurs, d'établir quelles relations entre descripteurs sont dues au hasard, et lesquelles ne le sont pas, sans faire d'hypothèse sur les lois de répartitions sous-jacentes, c'est à dire en tenant compte de lois de tous types sans avoir besoin de les spécifier. Ce test est basé sur la génération et l'exploitation d'un ensemble de matrices randomisées ayant les mêmes sommes marginales en lignes et colonnes que la matrice d'origine. Après une première application encourageante à un corpus textuel réduit, nous avons opéré le passage à l'échelle adéquat pour traiter des corpus textuels de taille réelle, comme celui des dépêches Reuters. Nous caractérisons le graphe des mots de ce corpus au moyen d'indicateurs classiques comme le coefficient de clustering, la distribution des degrés et de la taille des communautés, etc. Une autre caractéristique de TourneBool est qu'il permet aussi de dégager les ""anti liens"" entre mots, à savoir les mots qui s'évitent plus qu'attendu du fait du hasard. Le graphe des liens et celui des anti-liens seront caractérisés de la même façon."	Martine Cadot, Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1000783	http://editions-rnti.fr/render_pdf.php?p=1000783	852	fr	fr	@univ, @loria.fr, @loria.fr	graphe des lien et anti lien statistiquement valide entre le mot d' un corpus textuel : test de randomisation TourneBool sur le corpus Reuters  " le définition du voisinage être un élément central en fouille de donnée , et de nombreux définition avoir être avancer . . Nous en proposer ici un version statistique issue de son test de randomisation TourneBool , qui permettre , à partir d' un tableau de relation binaire objet décrire  descripteur , d' établir quel relation entre descripteur être devoir au hasard , et lesquelles ne le être pas , sans faire d' hypothèse sur le loi de répartition sous-jacent , c' être à dire en tenir compte de loi de tout type sans avoir besoin de les spécifier . . ce test être baser sur le génération et le exploitation d' un ensemble de matrice randomiser avoir le même somme marginal en ligne et colonne que le matrice d' origine . . Après un premier application encourageant à un corpus textuel réduire , nous avoir opérer le passage à le échelle adéquat pour traiter un corpus textuel de taille réel , comme celui des dépêche Reuters . . Nous caractériser le graphe des mot de ce corpus au moyen d' indicateur classique comme le coefficient de clustering , le distribution des degré et de le taille des communauté , etc. un autre caractéristique de TourneBool être qu' il permettre aussi de dégager le " " anti lien " " entre mot , à savoir le mot qui clr éviter plus qu' attendre du fait du hasard . . le graphe des lien et celui des anti- lien être caractériser de le même façon . " 	Graphes des liens et anti liens statistiquement valides entre les mots d'un corpus textuel : test de randomisation TourneBool sur le corpus Reuters	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Handling Texts ? A Challenge for Data Mining	The amount of data in free form by far surpasses the structured records in databases in theirnumber. However, standard learning algorithms require observations in the form of vectorsgiven a fixed set of attributes. For texts, there is no such fixed set of attributes. The bag ofwords representation yields vectors with as many components as there are words in a language.Hence, the classification of documents represented as bag of word vectors demands efficientlearning algorithms. The TCat model for the support vector machine (Joachims 2002) offers asound performance estimation for text classification.The huge mass of documents, in principle, offers answers to many questions and is oneof the most important sources of knowledge. However, information retrieval and text classi-fication deliver merely the document, in which the answer can be found by a human reader ?not the answer itself. Hence, information extraction has become an important topic: if we canextract information from text, we can apply standard machine learning to the extracted facts(Craven et al. 1998). First, information extraction has to recognize Named Entities (see, e.g.,Roessler, Morik 2005). Second, relations between these become the nucleus of events. Ex-tracting events from a complex web site with long documents allows to automatically discoverregularities which are otherwise hidden in the mass of sentences (see, e.g., Jungermann, Morik2008).	Katharina Morik	http://editions-rnti.fr/render_pdf.php?p1&p=1000732	http://editions-rnti.fr/render_pdf.php?p=1000732	853	en	en	@uni-dortmund.de	handle text challenge data mining amount datum free form far surpass structured record databasis theirnumber however standard learn algorithms require observation form vectorsgiven fix set attribute text fix set attribute bag ofword representation yield vector many component word language hence classification document represent bag word vector demand efficientlearn algorithms tcat model support vector machine joachim 2002 offer asound performance estimation text classification the huge mass document principle offer answer many question oneof important source knowledge however information retrieval text classi fication deliver merely document answer find human reader not answer itself hence information extraction become important topic canextract information text apply standard machine learn extract fact craven et al 1998 first information extraction recognize name entity see e g roessler morik 2005 second relation become nucleus event ex tract event complex web site long document allow automatically discoverregularity otherwise hidden mass sentence see e g jungermann morik2008	Handling Texts ? A Challenge for Data Mining	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	La carte GHSOM comme alternative à la SOM pour l'analyse exploratoire de données	L'objecif de cet article est de faire de la carte auto-organisatrice hiérarchique(GHSOM) un outil utilisable dans le cadre d'une démarche d'analyseexploratoire de données. La visualisation globale est un outil indispensable pourrendre les résultats d'une segmentation intelligibles pour un utilisateur. Nousproposons donc différents outils de visualisation pour la GHSOM équivalents àceux de la SOM.	Françoise Fessant, Fabrice Clérot, Pascal Gouzien	http://editions-rnti.fr/render_pdf.php?p1&p=1000734	http://editions-rnti.fr/render_pdf.php?p=1000734	854	fr	fr	@orange-ftgroup.com	le carte GHSOM comme alternatif à le SOM pour le analyse exploratoire de données  le objecif de ce article être de faire de le carte auto- organisatrice hiérarchique ( GHSOM ) un outil utilisable dans le cadre d' un démarche d' analyseexploratoire de donnée . le visualisation global être un outil indispensable pourrendre le résultat d' un segmentation intelligible pour un utilisateur . Nousproposons donc différent outil de visualisation pour le GHSOM équivalent àceux de le SOM . 	La carte GHSOM comme alternative à la SOM pour l'analyse exploratoire de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	La « créativité calculatoire » et les heuristiques créatives en synthèse de prédicats multiples	Nous présentons une approche à ce que nous appelons la « créativitécalculatoire », c'est-à-dire les procédés par lesquels une machine peut fairemontre d'une certaine créativité. Dans cet article, nous montronsessentiellement que la synthèse de prédicats multiples en programmationlogique inductive (ILP) et la synthèse de programmes à partir de spécificationsformelles (SPSF), deux domaines de l'informatique qui s'attaquent à desproblèmes où la notion de créativité est centrale, ont été amenés à ajouter àleur formalisme de base (l'ILP pour l'un, les tableaux de Beth pour l'autre)toute une série d'heuristiques. Cet article présente une collectiond'heuristiques qui sont destinées à fournir au programme une forme decréativité calculatoire. Dans cette présentation, l'accent est plutôt mis sur lesheuristiques de l'ILP mais lorsque cela était possible sans de trop longsdéveloppements, nous avons aussi présenté quelques heuristiques de la SPSF.L'outil indispensable de la créativité calculatoire est ce que nous appelons un'générateur d'atouts' dont une spécification (forcément informelle commenous le verrons) est fournie comme première conclusion aux exemples décritsdans le corps de l'article.	Marta Franová, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000747	http://editions-rnti.fr/render_pdf.php?p=1000747	855	fr	fr	@lri.fr, @lri.fr	le « créativité calculatoire » et le heuristique créatif en synthèse de prédicat multiples  Nous présenter un approche à ce que nous appeler le « créativitécalculatoire » , c' est-à-dire le procédé par lesquels un machine pouvoir fairemontre d' un certain créativité . Dans ce article , nous montronsessentiellement que le synthèse de prédicat multiple en programmationlogique inductif ( ILP ) et le synthèse de programme à partir de spécificationsformelles ( SPSF ) , deux domaine de le informatique qui clr attaquer à desproblèmes où le notion de créativité être central , avoir être amener à ajouter àleur formalisme de base ( le ILP pour le un , le tableau de Beth pour le autre ) tout un série d' heuristique . ce article présenter un collectiond'heuristiques qui être destiner à fournir au programme un forme decréativité calculatoire . Dans ce présentation , le accent être plutôt mettre sur lesheuristiques de le ILP mais lorsque cela être possible sans de trop longsdéveloppements , nous avoir aussi présenter quelque heuristique de le SPSF . le outil indispensable de le créativité calculatoire être ce que nous appeler un'générateur d' atouts'dont un spécification ( forcément informel commenous le voir ) être fournir comme premier conclusion aux exemple décritsdans le corps de le article . 	La « créativité calculatoire » et les heuristiques créatives en synthèse de prédicats multiples	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Le logiciel SYR pour l'Analyse de Données Symboliques		Filipe Afonso, Edwin Diday, Wassim Khaskhoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1000823	http://editions-rnti.fr/render_pdf.php?p=1000823	856	fr		@syrokko.com	Le logiciel SYR pour l'Analyse de Données Symboliques 	Le logiciel SYR pour l'Analyse de Données Symboliques	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Logiciel « DtmVic » Data and Text Mining: Visualisation, Inférence, Classification		Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1000815	http://editions-rnti.fr/render_pdf.php?p=1000815	857	en		@lebart.org	Logiciel « DtmVic » Data and Text Mining: Visualisation, Inférence, Classification 	Logiciel « DtmVic » Data and Text Mining: Visualisation, Inférence, Classification	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	L'Analyse Formelle de Concepts pour l'Extraction de Connaissances dans les Données d'Expression de Gènes	"L'analyse formelle de concepts (AFC, Ganter etWille (1999)) est uneméthode pertinente d'extraction de connaissances à partir de données complexesd'expression de gènes (Blachon et al. (2007), Motameny et al. (2008)). Dans cepapier, nous proposons d'extraire des groupes de gènes partageant un compor-tement similaire montrant des changements ""significatifs"" à travers divers envi-ronnements biologiques, servant d'hypothèses à la fonction des gènes."	Mehdi Kaytoue-Uberall, Sébastien Duplessis, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000793	http://editions-rnti.fr/render_pdf.php?p=1000793	858	fr	fr	@upc.edu, @udg.edu, @idescat.net	le analyse formel de concept pour le extraction de connaissance dans le donnée d' Expression de Gènes  " le analyse formel de concept ( AFC , Ganter etWille ( 1999 ) ) être uneméthode pertinent d' extraction de connaissance à partir de donnée complexesd'expression de gène ( Blachon et al. ( 2007 ) , Motameny et al. ( 2008 ) ) . . Dans cepapier , nous proposer d' extraire un groupe de gène partager un compor-tement similaire montrer un changement " " significatif " " à travers divers envi-ronnements biologique , servir d' hypothèse à le fonction des gène . " 	L'Analyse Formelle de Concepts pour l'Extraction de Connaissances dans les Données d'Expression de Gènes	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Management des connaissances dans le domaine du patrimoine culturel		Stefan du Château, Danielle Boulanger, Eunika Mercier-Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000792	http://editions-rnti.fr/render_pdf.php?p=1000792	859	fr			Management des connaissances dans le domaine du patrimoine culturel 	Management des connaissances dans le domaine du patrimoine culturel	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Méthode de regroupement par graphe de voisinage	Ce travail s'inscrit dans la problématique de l'apprentissage non su-pervisé. Dans ce cadre se retrouvent les méthodes de classification automatiquenon paramétriques qui reposent sur l'hypothèse que plus des individus sontproches dans l'espace de représentation, plus ils ont de chances de faire par-tie de la même classe. Cet article propose une nouvelle méthode de ce type quiconsidère la proximité à travers la structure fournie par un graphe de voisinage.	Fabrice Muhlenbach	http://editions-rnti.fr/render_pdf.php?p1&p=1000782	http://editions-rnti.fr/render_pdf.php?p=1000782	860	fr	fr	@univ-st-etienne.fr	méthode de regroupement par graphe de voisinage  ce travail clr inscrire dans le problématique de le apprentissage non su-pervisé . Dans ce cadre clr retrouver le méthode de classification automatiquenon paramétrique qui reposer sur le hypothèse que plus des individu sontproches dans le espace de représentation , plus ils avoir de chance de faire par-tie de le même classe . ce article proposer un nouveau méthode de ce type quiconsidère le proximité à travers le structure fournir par un graphe de voisinage . 	Méthode de regroupement par graphe de voisinage	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Modèle de préférences contextuelles pour les analyses OLAP	Cet article présente un environnement pour la personnalisation desanalyses OLAP afin de réduire la charge de navigation de l'utilisateur. Nousproposons un modèle de préférences contextuelles qui permet de restituer lesdonnées en fonction des préférences de l'utilisateur et de son contexted'analyse.	Houssem Jerbi, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000769	http://editions-rnti.fr/render_pdf.php?p=1000769	861	fr	fr	@irit.fr	modèle de préférence contextuel pour le analyse OLAP  ce article présenter un environnement pour le personnalisation desanalyses OLAP afin de réduire le charge de navigation de le utilisateur . Nousproposons un modèle de préférence contextuel qui permettre de restituer lesdonnées en fonction des préférence de le utilisateur et de son contexted'analyse . 	Modèle de préférences contextuelles pour les analyses OLAP	4
Revue des Nouvelles Technologies de l'Information	EGC	2009	Modélisation des connaissances dans le cadre de bibliothèques numériques spécialisées	Nous présentons une application innovante de la modélisation desconnaissances au domaine des bibliothèques numériques spécialisées. Nous utilisonsla spécification experte de la TEI (Text Encoding Initiative) pour modéliserla connaissance apportée par les chercheurs qui travaillent sur des archivesmanuscrites. Nous montrons les limites de la TEI dans le cas d'une approchediachronique du document, cette dernière impliquant la construction simultanéede structures de données concurrentes. Nous décrivons un modèle qui présentele problème et permet d'envisager des solutions. Enfin, nous justifions les structuresarborescentes sur lesquelles se base ce modèle.	Sylvie Calabretto, Pierre-Edouard Portier	http://editions-rnti.fr/render_pdf.php?p1&p=1000785	http://editions-rnti.fr/render_pdf.php?p=1000785	862	fr	fr	@insa-lyon.fr	modélisation des connaissance dans le cadre de bibliothèque numérique spécialisées  Nous présenter un application innovant de le modélisation desconnaissances au domaine des bibliothèque numérique spécialiser . Nous utilisonsla spécification expert de le TEI ( Text Encoding Initiative ) pour modéliserla connaissance apporter par le chercheur qui travailler sur un archivesmanuscrites . Nous montrer le limite de le TEI dans le cas d' un approchediachronique du document , ce dernier impliquer le construction simultanéede structure de donnée concurrent . Nous décrire un modèle qui présentele problème et permettre d' envisager un solution . enfin , nous justifier le structuresarborescentes sur lesquelles clr baser ce modèle . 	Modélisation des connaissances dans le cadre de bibliothèques numériques spécialisées	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Okmed et Wokm	Cet article traite de la problématique de la classification recouvrante(overlapping clustering) et propose deux variantes de l'approche OKM : OKMEDet WOKM. OKMED généralise k-médoïdes au cas recouvrant, il permet d'organiserun ensemble d'individus en classes non-disjointes, à partir d'une matricede distances. La méthode WOKM (Weighted-OKM) étend OKM par une pondérationlocale des classes ; cette variante autorise chaque individu à appartenir àplusieurs classes sur la base de critères différents. Des expérimentations sont réaliséessur une application cible : la classification de textes. Nous montrons alorsque OKMED présente un comportement similaire à OKM pour la métrique euclidienne,et offre la possibilité d'utiliser des métriques plus adaptées et d'obtenirde meilleures performances. Enfin, les résultats obtenus avec WOKM montrentun apport significatif de la pondération locale des classes	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1000736	http://editions-rnti.fr/render_pdf.php?p=1000736	863	en	fr	@univ-orleans.fr	ce article traire de le problématique de le classification recouvrante ( overlapping clustering ) et proposer deux variante de le approche OKM : OKMEDet WOKM . OKMED généraliser k-médoïdes au cas recouvrer , il permettre d' organiserun ensemble d' individu en classe non- disjoindre , à partir d' un matricede distance . le méthode WOKM ( Weighted-OKM ) étendre OKM par un pondérationlocale des classe ; ce variante autoriser chaque individu à appartenir àplusieurs classe sur le base de critère différent . un expérimentation être réaliséessur un application cible : le classification de texte . Nous montrer alorsque OKMED présenter un comportement similaire à OKM pour le métrique euclidien , et offrir le possibilité d' utiliser un métrique plus adapter et d' obtenirde meilleur performance . enfin , le résultat obtenir avec WOKM montrentun apport significatif de le pondération local des classes 	Okmed et Wokm	4
Revue des Nouvelles Technologies de l'Information	EGC	2009	Online and Adaptive Anomaly Detection: Detecting Intrusions in Unlabelled Audit Data Streams		Wei Wang    , Thomas Guyet, Rene Quiniou, Marie-Odile Cordier, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000802	http://editions-rnti.fr/render_pdf.php?p=1000802	864	en		@irisa.fr, @gmail.com, @inria.fr	Online and Adaptive Anomaly Detection: Detecting Intrusions in Unlabelled Audit Data Streams 	Online and Adaptive Anomaly Detection: Detecting Intrusions in Unlabelled Audit Data Streams	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Partitionnement d'ontologies pour le passage à l'échelle des techniques d'alignement	L'alignement d'ontologies est une tâche importante dans les systèmesd'intégration puisqu'elle autorise la prise en compte conjointe de ressourcesdécrites par des ontologies différentes, en identifiant des appariements entreconcepts. Avec l'apparition de très grandes ontologies dans des domaines commela médecine ou l'agronomie, les techniques d'alignement, qui mettent souventen oeuvre des calculs complexes, se trouvent face à un défi : passer à l'échelle.Pour relever ce défi, nous proposons dans cet article deux méthodes de partition-nement, conçues pour prendre en compte, le plus tôt possible, l'objectif d'ali-gnement. Ces méthodes permettent de décomposer les deux ontologies à aligneren deux ensembles de blocs de taille limitée et tels que les éléments susceptiblesd'être appariés se retrouvent concentrés dans un ensemble minimal de blocs quiseront effectivement comparés. Les résultats des tests effectuées avec nos deuxméthodes sur différents couples d'ontologies montrent leur efficacité.	Fayçal Hamdi, Brigitte Safar, Haïfa Zargayouna, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000787	http://editions-rnti.fr/render_pdf.php?p=1000787	865	fr	fr	@lri.fr, @univ-paris13.fr	Partitionnement d' ontologie pour le passage à le échelle des technique d' alignement  le alignement d' ontologie être un tâche important dans le systèmesd'intégration puisqu' elle autoriser le prise en compte conjoindre de ressourcesdécrites par un ontologie différent , en identifier un appariement entreconcepts . Avec le apparition de très grand ontologie dans un domaine commela médecine ou le agronomie , le technique d' alignement , qui mettre souventen oeuvre des calcul complexe , clr trouver face à un défi : passer à le échelle . Pour relever ce défi , nous proposer dans ce article deux méthode de partition-nement , concevoir pour prendre en compte , le plus tôt possible , le objectif d' ali-gnement . ce méthode permettre de décomposer le deux ontologie à aligneren deux ensemble de bloc de taille limiter et tel que le élément susceptiblesd'être apparier clr retrouver concentrer dans un ensemble minimal de bloc quiseront effectivement comparer . le résultat des test effectuer avec son deuxméthodes sur différent couple d' ontologie montrer son efficacité . 	Partitionnement d'ontologies pour le passage à l'échelle des techniques d'alignement	8
Revue des Nouvelles Technologies de l'Information	EGC	2009	Privacy and Data Mining: New Developments and Challenges	There is little doubt that data mining technologies create new challenges in the area of dataprivacy. In this talk, we will review some of the new developments in Privacy-preserving DataMining. In particular, we will discuss techniques in which data mining results can reveal per-sonal data, and how this can be prevented. We will look at the practically interesting situationswhere data to be mined is distributed among several parties. We will mention new applica-tions in which mining spatio-temporal data can lead to identification of personal information.We will argue that methods that effectively protect personal data, while at the same time pre-serve the quality of the data from the data analysis perspective, are some of the principal newchallenges before the field.	Stan Matwin	http://editions-rnti.fr/render_pdf.php?p1&p=1000730	http://editions-rnti.fr/render_pdf.php?p=1000730	866	en	en	@live.com	privacy datum mining new development challenge little doubt data mining technology create new challenge area dataprivacy talk review new development privacy preserve datamining particular discuss technique datum mining result reveal per sonal data prevent look practically interesting situationswhere data mine distribute among several party mention new applica tion mining spatio temporal datum lead identification personal information we argue method effectively protect personal data time pre serve quality datum data analysis perspective principal newchallenge field	Privacy and Data Mining: New Developments and Challenges	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Probabilistic Multi-classifier by SVMs from voting rule to voting features		Anh Phuc Trinh, David Buffoni, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000790	http://editions-rnti.fr/render_pdf.php?p=1000790	867	en		@lip6.fr	Probabilistic Multi-classifier by SVMs from voting rule to voting features 	Probabilistic Multi-classifier by SVMs from voting rule to voting features	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	RDBToOnto : un logiciel dédié à l'apprentissage d'ontologies à partir de bases de données relationnelles	RDBToOnto1 est un logiciel extensible qui permet d'élaborer des on-tologies précises à partir de bases de données relationnelles. Le processus sup-porté est largement automatisé, de l'extraction des données à la génération dumodèle de l'ontologie et son instanciation. Pour affiner le résultat, le processuspeut être orienté par des contraintes locales définies interactivement. C'est aussiun cadre facilitant la mise en oeuvre de nouvelles méthodes d'apprentissage.	Farid Cerbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000820	http://editions-rnti.fr/render_pdf.php?p=1000820	868	fr	fr	@dassault-aviation.fr	RDBToOnto : un logiciel dédier à le apprentissage d' ontologie à partir de base de donnée relationnelles  RDBToOnto1 être un logiciel extensible qui permettre d' élaborer un on-tologies précis à partir de base de donnée relationnel . le processus sup-porté être largement automatiser , de le extraction des donnée à le génération dumodèle de le ontologie et son instanciation . Pour affiner le résultat , le processuspeut être orienter par un contrainte local définir interactivement . C' être aussiun cadre faciliter le mise en oeuvre de nouveau méthode d' apprentissage . 	RDBToOnto : un logiciel dédié à l'apprentissage d'ontologies à partir de bases de données relationnelles	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Regroupement des Définitions de Sigles Biomédicaux	L'application présentée permet de regrouper les définitions de siglesissues des sciences du vivant par des mesures de proximité lexicale (approcheautomatique) et une intervention de l'expert (approche manuelle).	Ousmane Djanga, Hanine Hamzioui, Mickaël Hatchi, Isabelle Mougenot, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000816	http://editions-rnti.fr/render_pdf.php?p=1000816	869	fr	fr		regroupement des définition de Sigles Biomédicaux  le application présenter permettre de regrouper le définition de siglesissues des science du vivre par un mesure de proximité lexical ( approcheautomatique ) et un intervention de le expert ( approche manuel ) . 	Regroupement des Définitions de Sigles Biomédicaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Résumé hybride de flux de données par échantillonnage et classification automatique	"Face à la grande volumétrie des données générées par les systèmes informatiques,l'hypothèse de les stocker en totalité avant leur interrogation n'estplus possible. Une solution consiste à conserver un résumé de l'historique duflux pour répondre à des requêtes et pour effectuer de la fouille de données.Plusieurs techniques de résumé de flux de données ont été développées, tellesque l'échantillonnage, le clustering, etc. Selon le champ de requête, ces résuméspeuvent être classés en deux catégories: résumés spécialisés et résumés généralistes.Dans ce papier, nous nous intéressons aux résumés généralistes. Notreobjectif est de créer un résumé de bonne qualité, sur toute la période temporelle,qui nous permet de traiter une large panoplie de requêtes. Nous utilisons deuxalgorithmes : CluStream et StreamSamp. L'idée consiste à les combiner afin detirer profit des avantages de chaque algorithme. Pour tester cette approche, nousutilisons un Benchmark de données réelles ""KDD_99"". Les résultats obtenussont comparés à ceux obtenus séparément par les deux algorithmes."	Nesrine Gabsi, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000767	http://editions-rnti.fr/render_pdf.php?p=1000767	870	fr	fr	@telecom-paristech.fr, @orange-ftgroup.com	résumer hybride de flux de donnée par échantillonnage et classification automatique  " face à le grand volumétrie des donnée générer par le système informatique , le hypothèse de les stocker en totalité avant son interrogation n' estplus possible . . un solution consister à conserver un résumé de le historique duflux pour répondre à un requête et pour effectuer de le fouille de donnée . . plusieurs technique de résumé de flux de donnée avoir être développer , tellesque le échantillonnage , le clustering , etc. Selon le champ de requête , ce résuméspeuvent être classer en deux catégorie : résumé spécialiser et résumé généraliste . . Dans ce papier , nous clr intéresser aux résumé généraliste . . Notreobjectif être de créer un résumé de bon qualité , sur tout le période temporel , qui nous permettre de traiter un large panoplie de requête . . Nous utiliser deuxalgorithmes : : CluStream et StreamSamp . . le idée consister à les combiner afin detirer profit des avantage de chaque algorithme . . Pour tester ce approche , nousutilisons un Benchmark de donnée réel " " KDD _ 99 " " . . le résultat obtenussont comparer à celui obtenir séparément par le deux algorithme . " 	Résumé hybride de flux de données par échantillonnage et classification automatique	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	SoftJaccard : une mesure de similarité entre ensembles de chaînes de caractères pour l'unification d'entités nommées	Parmi lesmesures de similarité classiques utilisables sur des ensemblesfigure l'indice de Jaccard. Dans le cadre de cet article, nous en proposons uneextension pour comparer des ensembles de chaînes de caractères. Cette mesurehybride permet de combiner une distance entre chaînes de caractères, telle que ladistance de Levenstein, et l'indice de Jaccard. Elle est particulièrement adaptéepourmettre en correspondance des champs composés de plusieurs chaînes de caractères,comme par exemple, lorsqu'on se propose d'unifier des noms d'entitésnommées.	Christine Largeron, Bernard Kaddour, Maria Fernandez	http://editions-rnti.fr/render_pdf.php?p1&p=1000795	http://editions-rnti.fr/render_pdf.php?p=1000795	871	fr	fr	@univ-st-etienne.fr	SoftJaccard : un mesure de similarité entre ensemble de chaîne de caractère pour le unification d' entité nommées  Parmi lesmesures de similarité classique utilisable sur un ensemblesfigure le indice de Jaccard . Dans le cadre de ce article , nous en proposer uneextension pour comparer un ensemble de chaîne de caractère . ce mesurehybride permettre de combiner un distance entre chaîne de caractère , tel que ladistance de Levenstein , et le indice de Jaccard . Elle être particulièrement adaptéepourmettre en correspondance des champ composer de plusieurs chaîne de caractère , comme par exemple , lorsqu' on clr proposer d' unifier un nom d' entitésnommées . 	SoftJaccard : une mesure de similarité entre ensembles de chaînes de caractères pour l'unification d'entités nommées	4
Revue des Nouvelles Technologies de l'Information	EGC	2009	SPAMS, une nouvelle approche incrémentale pour l'extraction de motifs séquentiels fréquents dans les Data streams	L'extraction de motifs séquentiels fréquents dans les datastreams est un enjeu important traité par la communauté des chercheursen fouille de données. Plus encore que pour les bases de données, denombreuses contraintes supplémentaires sont à considérer de par la na-ture intrinsèque des streams. Dans cet article, nous proposons un nouvelalgorithme en une passe : SPAMS, basé sur la construction incrémentale,avec une granularité très fine par transaction, d'un automate appelé SPA,permettant l'extraction des motifs séquentiels dans les streams. L'infor-mation du stream est apprise à la volée, au fur et à mesure de l'insertionde nouvelles transactions, sans pré-traitement a priori. Les résultats ex-périmentaux obtenus montrent la pertinence de la structure utilisée ainsique l'efficience de notre algorithme appliqué à différents jeux de données.	Lionel Vinceslas, Jean-Emile Symphor, Alban Mancheron, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000765	http://editions-rnti.fr/render_pdf.php?p=1000765	872	fr	fr	@univ-ag.fr, @mancheron.infos.st, @ema.fr	spam , un nouveau approche incrémentale pour le extraction de motif séquentiel fréquent dans le Data streams  le extraction de motif séquentiel fréquent dans le datastreams être un enjeu important traiter par le communauté des chercheursen fouille de donnée . plus encore que pour le base de donnée , denombreuses contrainte supplémentaire être à considérer de par le na-ture intrinsèque des stream . Dans ce article , nous proposer un nouvelalgorithme en un passe : spam , baser sur le construction incrémentale , avec un granularité très fin par transaction , d' un automate appeler SPA , permettre le extraction des motif séquentiel dans le stream . le infor-mation du stream être apprendre à le volé , au fur et à mesure de le insertionde nouveau transaction , sans pré-traitement avoir priori . le résultat ex- périmentaux obtenir montrer le pertinence de le structure utiliser ainsique le efficience de son algorithme appliquer à différents jeu de donnée . 	SPAMS, une nouvelle approche incrémentale pour l'extraction de motifs séquentiels fréquents dans les Data streams	5
Revue des Nouvelles Technologies de l'Information	EGC	2009	SVM incrémental et parallèle sur GPU	Nous présentons un nouvel algorithme incrémental et parallèle deSéparateur à Vaste Marge (SVM ou Support Vector Machine) pour laclassification de très grands ensembles de données en utilisant le processeur dela carte graphique (GPUs, Graphics Processing Units). Les SVMs et lesméthodes de noyaux permettent de construire des modèles avec une bonneprécision mais ils nécessitent habituellement la résolution d'un programmequadratique ce qui requiert une grande quantité de mémoire et un long tempsd'exécution pour les ensembles de données de taille importante. Nousprésentons une extension de l'algorithme de Least Squares SVM (LS-SVM)proposé par Suykens et Vandewalle pour obtenir un algorithme incrémental etparallèle. Le nouvel algorithme est exécuté sur le processeur graphique pourobtenir une bonne performance à faible coût. Les résultats numériques sur lesensembles de données de l'UCI et Delve montrent que notre algorithmeincrémental et parallèle est environ 70 fois plus rapide sur GPU que sur CPUet significativement plus rapide (plus de 1000 fois) que les algorithmesstandards tels que LibSVM, SVM-perf et CB-SVM.	François Poulet, Thanh-Nghi Do, Van-Hoa Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000743	http://editions-rnti.fr/render_pdf.php?p=1000743	873	fr	fr	@irisa.fr, @telecom-bretagne.eu, @irisa.fr	SVM incrémental et parallèle sur GPU  Nous présenter un nouveau algorithme incrémental et parallèle deSéparateur à vaste Marge ( SVM ou Support Vector Machine ) pour laclassification de très grand ensemble de donnée en utiliser le processeur dela carte graphique ( GPUs , Graphics Processing Units ) . le SVMs et lesméthodes de noyau permettre de construire un modèle avec un bonneprécision mais ils nécessiter habituellement le résolution d' un programmequadratique ce qui requérir un grand quantité de mémoire et un long tempsd'exécution pour le ensemble de donnée de taille important . Nousprésentons un extension de le algorithme de Least Squares SVM ( LS-SVM ) proposer par Suykens et Vandewalle pour obtenir un algorithme incrémental etparallèle . le nouveau algorithme être exécuter sur le processeur graphique pourobtenir un bon performance à faible coût . le résultat numérique sur lesensembles de donnée de le UCI et Delve montrer que son algorithmeincrémental et parallèle être environ 70 foi plus rapide sur GPU que sur CPUet significativement plus rapide ( plus de 1000 foi ) que le algorithmesstandards tel que LibSVM , SVM-perf et CB-SVM . 	SVM incrémental et parallèle sur GPU	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	TAAABLE : système de recherche et de création, par adaptation, de recettes de cuisine	TAAABLE is a textual case-based reasoning system that, according to requested/forbiddeningredients, dish types and/or dish origins, retrieves cooking recipes. If no recipe satisifies theconstraints, TAAABLE adapts existing recipes by replacing some ingredients by other ones.	Amélie Cordier, Jean Lieber, Emmanuel Nauer, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1000812	http://editions-rnti.fr/render_pdf.php?p=1000812	874	fr	en	@liris.cnrs.fr, @loria.fr	TAAABLE : système de recherche et de création , par adaptation , de recette de cuisine 	TAAABLE : système de recherche et de création, par adaptation, de recettes de cuisine	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	TraMineR: une librairie R pour l'analyse de données séquentielles		Alexis Gabadinho, Nicolas S. Müller, Gilbert Ritschard, Matthias Studer	http://editions-rnti.fr/render_pdf.php?p1&p=1000814	http://editions-rnti.fr/render_pdf.php?p=1000814	875	fr		@unige.ch	TraMineR: une librairie R pour l'analyse de données séquentielles 	TraMineR: une librairie R pour l'analyse de données séquentielles	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un algorithme stable de décomposition pour l'analyse des réseaux sociaux dynamiques	Les réseaux dynamiques soulèvent de nouveaux problèmes d'analyses.Un outils efficace d'analyse doit non seulement permettre de décomposerces réseaux en groupes d'éléments similaires mais il doit aussi permettre la détectionde changements dans le réseau. Nous présentons dans cet article une nouvelleapproche pour l'analyse de tels réseaux. Cette technique est basée sur unalgorithme de décomposition de graphe en groupes chevauchants (ou chevauchement).La complexité de notre algorithme est O(|E| · deg2max +|V | · log(|V |))).La faible sensibilité de cet algorithme aux changements structuraux du réseaupermet d'en détecter les modifications majeures au cours du temps.	Romain Bourqui, Paolo Simonetto, Fabien Jourdan	http://editions-rnti.fr/render_pdf.php?p1&p=1000778	http://editions-rnti.fr/render_pdf.php?p=1000778	876	fr	fr	@labri.fr, @toulouse.inra.fr	un algorithme stable de décomposition pour le analyse des réseau social dynamiques  le réseau dynamique soulever un nouveau problème d' analyse . un outil efficace d' analyse devoir non seulement permettre de décomposerces réseau en groupe d' élément similaire mais il devoir aussi permettre le détectionde changement dans le réseau . Nous présenter dans ce article un nouvelleapproche pour le analyse de tel réseau . ce technique être baser sur unalgorithme de décomposition de graphe en groupe chevauchant ( ou chevauchement ) . le complexité de son algorithme être O ( |E| · deg2max plus | V | · log ( | V | ) ) ) . le faible sensibilité de ce algorithme aux changement structural du réseaupermet d' en détecter le modification majeur au cour du temps . 	Un algorithme stable de décomposition pour l'analyse des réseaux sociaux dynamiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un critère d'évaluation Bayésienne pour la construction d'arbres de décision	Nous présentons dans cet article un nouvel algorithme automatiquepour l'apprentissage d'arbres de décision. Nous abordons le problème selon uneapproche Bayésienne en proposant, sans aucun paramètre, une expression ana-lytique de la probabilité d'un arbre connaissant les données. Nous transformonsle problème de construction de l'arbre en un problème d'optimisation : nousrecherchons dans l'espace des arbres de décision, l'arbre optimum au sens ducritère Bayésien ainsi défini, c'est à dire l'arbre maximum a posteriori (MAP).L'optimisation est effectuée en exploitant une heuristique de pré-élagage. Desexpérimentations comparatives sur trente bases de l'UCI montrent que notreméthode obtient des performances prédictives proches de celles de l'état de l'arttout en étant beaucoup moins complexes.	Nicolas Voisine, Marc Boullé, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1000740	http://editions-rnti.fr/render_pdf.php?p=1000740	877	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com, @gfi.fr	un critère d' évaluation Bayésienne pour le construction d' arbre de décision  Nous présenter dans ce article un nouveau algorithme automatiquepour le apprentissage d' arbre de décision . Nous aborder le problème selon uneapproche Bayésienne en proposer , sans aucun paramètre , un expression ana-lytique de le probabilité d' un arbre connaître le donnée . Nous transformonsle problème de construction de le arbre en un problème d' optimisation : nousrecherchons dans le espace des arbre de décision , le arbre optimum au sens ducritère Bayésien ainsi définir , c' être à dire le arbre maximum avoir posteriori ( MAP ) . le optimisation être effectuer en exploiter un heuristique de pré-élagage . Desexpérimentations comparatif sur trente base de le UCI montrer que notreméthode obtenir un performance prédictif proche de celui de le état de le arttout en être beaucoup moins complexe . 	Un critère d'évaluation Bayésienne pour la construction d'arbres de décision	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un nouvel algorithme de forêts aléatoires d'arbres obliques particulièrement adapté à la classification de données en grandes dimensions	L'algorithme des forêts aléatoires proposé par Breiman permet d'ob-tenir de bons résultats en fouille de données comparativement à de nombreusesapproches. Cependant, en n'utilisant qu'un seul attribut parmi un sous-ensembled'attributs tiré aléatoirement pour séparer les individus à chaque niveau de l'arbre,cet algorithme perd de l'information. Ceci est particulièrement pénalisant avecles ensembles de données en grandes dimensions où il peut exister de nom-breuses dépendances entre attributs. Nous présentons un nouvel algorithme deforêts aléatoires d'arbres obliques obtenus par des séparateurs à vaste marge(SVM). La comparaison des performances de notre algorithme avec celles del'algorithme de forêts aléatoires des arbres de décision C4.5 et de l'algorithmeSVM montre un avantage significatif de notre proposition.	Thanh-Nghi Do, Stéphane Lallich, Nguyen-Khang Pham, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1000741	http://editions-rnti.fr/render_pdf.php?p=1000741	878	fr	fr	@telecom-bretagne.eu, @univ-lyon2.fr, @irisa.fr	un nouveau algorithme de forêt aléatoire d' arbre oblique particulièrement adapter à le classification de donnée en grand dimensions  le algorithme des forêt aléatoire proposer par Breiman permettre d' ob-tenir de bon résultat en fouille de donnée comparativement à de nombreusesapproches . cependant , en n' utiliser qu' un seul attribut parmi un sous-ensembled'attributs tirer aléatoirement pour séparer le individu à chaque niveau de le arbre , ce algorithme perdre de le information . ceci être particulièrement pénaliser avecles ensemble de donnée en grand dimension où il pouvoir exister de nom-breuses dépendance entre attribut . Nous présenter un nouveau algorithme deforêts aléatoire d' arbre oblique obtenir par un séparateur à vaste marge ( SVM ) . le comparaison des performance de son algorithme avec celui del'algorithme de forêt aléatoire des arbre de décision C4 . 5 et de le algorithmeSVM montrer un avantage significatif de son proposition . 	Un nouvel algorithme de forêts aléatoires d'arbres obliques particulièrement adapté à la classification de données en grandes dimensions	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un prototype cross-lingue multi-métiers : vers la Gestion Sémantique de Contenu d'Entreprise au service du Collaboratif Opérationnel	Le domaine « Qualité, Hygiène, Sécurité et Environnement »(QHSE) représente à l'heure actuelle un vecteur de progrès majeur pourl'industrie européenne. Le prototype « Semantic Quality Environment » (SQE)introduit dans cet article vise à démontrer la validité d'une architecturesémantique cross-lingue vouée à la collaboration multi-métiers et multilingue,dans le cadre d'un système banalisé de gestion de contenu d'entreprise dédié àl'industrie navale européenne.	Christophe Thovex, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000809	http://editions-rnti.fr/render_pdf.php?p=1000809	879	fr	fr	@orange.fr, @univ-nantes.fr	un prototype cross-lingue multi-métiers : vers le Gestion Sémantique de Contenu d' Entreprise au service du Collaboratif Opérationnel  le domaine « Qualité , Hygiène , sécurité et environnement » ( QHSE ) représenter à le heure actuel un vecteur de progrès majeur pourl'industrie européen . le prototype « Semantic Quality Environment » ( SQE ) introduire dans ce article viser à démontrer le validité d' un architecturesémantique cross-lingue vouer à le collaboration multi-métiers et multilingue , dans le cadre d' un système banaliser de gestion de contenu d' entreprise dédier àl'industrie naval européen . 	Un prototype cross-lingue multi-métiers : vers la Gestion Sémantique de Contenu d'Entreprise au service du Collaboratif Opérationnel	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un système pour l'extraction de corrélations linéaires dans des données de génomique médicale		Arriel Benis, Mélanie Courtine	http://editions-rnti.fr/render_pdf.php?p1&p=1000807	http://editions-rnti.fr/render_pdf.php?p=1000807	880	fr		@limbio-paris13.org	Un système pour l'extraction de corrélations linéaires dans des données de génomique médicale 	Un système pour l'extraction de corrélations linéaires dans des données de génomique médicale	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Une méthode de classification supervisée sans paramètre pour l'apprentissage sur les grandes bases de données	Dans ce papier, nous présentons une méthode de classification super-visée sans paramètre permettant d'attaquer les grandes volumétries. La méthodeest basée sur des estimateurs de densités univariés optimaux au sens de Bayes,sur un classifieur Bayesien naïf amélioré par une sélection de variables et unmoyennage de modèles exploitant un lissage logarithmique de la distribution aposteriori des modèles. Nous analysons en particulier la complexité algorith-mique de la méthode et montrons comment elle permet d'analyser des bases dedonnées nettement plus volumineuses que la mémoire vive disponible. Nous pré-sentons enfin les résultats obtenu lors du récent PASCAL Large Scale LearningChallenge, où notre méthode a obtenu des performances prédictives de premierplan avec des temps de calcul raisonnables.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000770	http://editions-rnti.fr/render_pdf.php?p=1000770	881	fr	fr	@orange-ftgroup.com	un méthode de classification superviser sans paramètre pour le apprentissage sur le grand base de données  Dans ce papier , nous présenter un méthode de classification super-visée sans paramètre permettre d' attaquer le grand volumétrie . le méthodeest baser sur un estimateur de densité univariés optimal au sens de Bayes , sur un classifieur Bayesien naïf améliorer par un sélection de variable et unmoyennage de modèle exploiter un lissage logarithmique de le distribution aposteriori des modèle . Nous analyser en particulier le complexité algorith-mique de le méthode et montrer comment elle permettre d' analyser un base dedonnées nettement plus volumineux que le mémoire vif disponible . Nous pré-sentons enfin le résultat obtenir lors du récent pascal Large Scale LearningChallenge , où son méthode avoir obtenir un performance prédictif de premierplan avec un temps de calcul raisonnable . 	Une méthode de classification supervisée sans paramètre pour l'apprentissage sur les grandes bases de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Une nouvelle approche pour la classification non supervisée en segmentation d'image	La segmentation des images en régions est un problème crucial pourl'analyse et la compréhension des images. Parmi les approches existantes pourrésoudre ce problème, la classification non supervisée est fréquemment em-ployée lors d'une première étape pour réaliser un partitionnement de l'espacedes intensités des pixels (qu'il s'agisse de niveaux de gris, de couleurs ou de ré-ponses spectrales). Puisqu'elle ignore complètement les notions de voisinagedes pixels, une seconde étape d'analyse spatiale (étiquetage en composantesconnexes par exemple) est ensuite nécessaire pour identifier les régions issuesde la segmentation. La non prise en compte de l'information spatiale est une li-mite majeure de ce type d'approche, ce qui a motivé de nombreux travaux où laclassification est couplée à d'autres techniques pour s'affranchir de ce problème.Dans cet article, nous proposons une nouvelle formulation de la classificationnon supervisée permettant d'effectuer la segmentation des images sans faire ap-pel à des techniques supplémentaires. Plus précisément, nous élaborons une mé-thode itérative de type k-means où les données à partitionner sont les pixels eux-mêmes (et non plus leurs intensités) et où les distances des points aux centresdes classes ne sont plus euclidiennes mais topographiques. La segmentation estalors un processus itératif, et à chaque itération, les classes obtenues peuvent êtreassimilées à des zones d'influence dans le contexte de la morphologie mathéma-tique. Ce parallèle nous permet de bénéficier des algorithmes efficaces proposésdans ce domaine (tels que ceux basés sur les files d'attente), tout en y ajoutantle caractère itératif des méthodes de classification non supervisée considéréesici. Nous illustrons finalement le potentiel de l'approche proposée par quelquesrésultats préliminaires de segmentation sur des images artificielles.	Sébastien Lefèvre	http://editions-rnti.fr/render_pdf.php?p1&p=1000745	http://editions-rnti.fr/render_pdf.php?p=1000745	882	fr	fr	@lsiit.u-strasbg.fr	un nouveau approche pour le classification non superviser en segmentation d' image  le segmentation des image en région être un problème crucial pourl'analyse et le compréhension des image . Parmi le approche existant pourrésoudre ce problème , le classification non superviser être fréquemment em-ployée lors d' un premier étape pour réaliser un partitionnement de le espacedes intensité des pixel ( qu' il clr agir de niveau de gris , de couleur ou de ré-ponses spectral ) . Puisqu' elle ignorer complètement le notion de voisinagedes pixel , un second étape d' analyse spatial ( étiquetage en composantesconnexes par exemple ) être ensuite nécessaire pour identifier le région issuesde le segmentation . le non prendre en compte de le information spatial être un li-mite majeur de ce type d' approche , ce qui avoir motiver un nombreux travail où laclassification être coupler à un autre technique pour clr affranchir de ce problème . Dans ce article , nous proposer un nouveau formulation de le classificationnon superviser permettre d' effectuer le segmentation des image sans faire ap-pel à un technique supplémentaire . plus précisément , nous élaborer un mé-thode itératif de type k-means où le donnée à partitionner être le pixel lui-même ( et non plus son intensité ) et où le distance des point aux centresdes classe ne être plus euclidien mais topographique . le segmentation estalors un processus itératif , et à chaque itération , le classe obtenir pouvoir êtreassimilées à un zone d' influence dans le contexte de le morphologie mathéma-tique . ce parallèle nous permettre de bénéficier un algorithme efficace proposésdans ce domaine ( tel que celui baser sur le file d' attente ) , tout en y ajoutantle caractère itératif des méthode de classification non superviser considéréesici . Nous illustrer finalement le potentiel de le approche proposer par quelquesrésultats préliminaire de segmentation sur un image artificiel . 	Une nouvelle approche pour la classification non supervisée en segmentation d'image	3
Revue des Nouvelles Technologies de l'Information	EGC	2009	Utilisation de l'analyse factorielle des correspondances pour la recherche d'images à grande échelle	Nous nous intéressons à l'utilisation de l'Analyse Factorielle des Cor-respondances (AFC) pour la recherche d'images par le contenu dans une base dedonnées d'images volumineuse. Nous adaptons l'AFC, méthode originellementdéveloppée pour l'Analyse des Données Textuelles (ADT), aux images en utili-sant des descripteurs locaux SIFT. En ADT, l'AFC permet de réduire le nombrede dimensions et de trouver des thèmes. Ici, l'AFC nous permettra de limiter lenombre d'images à examiner au cours de la recherche afin d'accélérer le tempsde réponse pour une requête. Pour traiter de grandes bases d'images, nous pro-posons une version incrémentale de l'algorithme AFC. Ce nouvel algorithmedécoupe une base d'images en blocs et les charge dans la mémoire l'un aprèsl'autre. Nous présentons aussi l'intégration des informations contextuelles (e.g.la Mesure de Dissimilarité Contextuelle (Jegou et al., 2007)) dans notre structurede recherche d'images. Cela améliore considérablement la précision. Nous ex-ploitons cette intégration dans deux axes: (i) hors ligne (la structure de voisinageest corrigée hors ligne) et (ii) à la volée (la structure de voisinage des images estcorrigée au cours de la recherche sur un petit ensemble d'images).	Nguyen-Khang Pham, Annie Morin, Patrick Gros, Quyet-Thang Le	http://editions-rnti.fr/render_pdf.php?p1&p=1000773	http://editions-rnti.fr/render_pdf.php?p=1000773	883	fr	fr	@irisa.fr, @cit.ctu.edu.vn	utilisation de le analyse factoriel des correspondance pour le recherche d' image à grand échelle  Nous nous intéresser à le utilisation de le analyse factorielle des Cor-respondances ( AFC ) pour le recherche d' image par le contenu dans un base dedonnées d' image volumineux . Nous adapter le AFC , méthode originellementdéveloppée pour le Analyse des Données Textuelles ( ADT ) , aux image en utili-sant un descripteur local SIFT . En ADT , le AFC permettre de réduire le nombrede dimension et de trouver un thème . ici , le AFC nous permettre de limiter lenombre d' image à examiner au cour de le recherche afin d' accélérer le tempsde réponse pour un requête . Pour traiter un grand base d' image , nous pro- poser un version incrémentale de le algorithme AFC . ce nouveau algorithmedécoupe un base d' image en bloc et le charge dans le mémoire l' un aprèsl'autre . Nous présenter aussi le intégration des information contextuel ( e.g. le mesure de Dissimilarité Contextuelle ( Jegou et al. , 2007 ) ) dans son structurede recherche d' image . cela améliorer considérablement le précision . Nous ex- ploitons ce intégration dans deux axe : ( i ) hors ligne ( le structure de voisinageest corriger hors ligne ) et ( ii ) à le volé ( le structure de voisinage des image estcorrigée au cour de le recherche sur un petit ensemble d' image ) . 	Utilisation de l'analyse factorielle des correspondances pour la recherche d'images à grande échelle	2
Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers la simulation et la détection des changements des données évolutives d'usage du Web	Dans le domaine des flux des données, la prise en compte du tempss'avère nécessaire pour l'analyse de ces données car leur distribution sous-jacentepeut changer au cours du temps. Un exemple typique concerne les modèles desprofils de navigation des internautes. Notre objectif est d'analyser l'évolutionde ces profils, celle-ci peut être liée au changement d'effectifs ou aux déplacementde clusters au cours du temps. Afin d'analyser la validité de notre approche,nous mettons en place uneméthodologie pour la simulation des données d'usageà partir de laquelle il est possible de contrôler l'occurrence des changements	Alzennyr Da Silva, Yves Lechevallier, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1000800	http://editions-rnti.fr/render_pdf.php?p=1000800	884	fr	fr	@inria.fr, @cin.ufpe.br	Vers le simulation et le détection des changement des donnée évolutif d' usage du Web  Dans le domaine des flux des donnée , le prise en compte du tempss'avère nécessaire pour le analyse de ce donnée car son distribution sous-jacentepeut changer au cour du temps . un exemple typique concerner le modèle desprofils de navigation des internaute . son objectif être d' analyser le évolutionde ce profil , celui _-ci pouvoir être lier au changement d' effectif ou aux déplacementde clusters au cour du temps . Afin d' analyser le validité de son approche , nous mettre en place uneméthodologie pour le simulation des donnée d' usageà partir de laquelle il être possible de contrôler le occurrence des changements 	Vers la simulation et la détection des changements des données évolutives d'usage du Web	1
Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers le traitement à grande échelle de données symboliques		Omar Merroun, Edwin Diday, Philippe Rigaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000791	http://editions-rnti.fr/render_pdf.php?p=1000791	885	fr		@gmail.com, @ceremade.dauphine.fr, @lamsade.dauphine.fr	Vers le traitement à grande échelle de données symboliques 	Vers le traitement à grande échelle de données symboliques	0
Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers une utilisation améliorée de relations spatiales pour l'apprentissage de données dans les modèles graphiques	Nous nous intéressons dans cet article aux représentations des relationsspatiales pour l'extraction d'information et la modélisation des donnéesvisuelles, en particulier dans le contexte de la catégorisation d'images. Nousmontrons comment la prise en compte d'une relation spatiale entre deux élémentsentraîne l'apparition d'une information supplémentaire entre ces élémentset le reste de l'ensemble à modéliser, ce qui est rarement exploité explicitement.Une représentation floue des relations dans unmodèle graphique est bien adaptéepour les algorithmes d'apprentissage utilisés actuellement et permet d'intégrerce type d'information complémentaire qui concerne l'absence d'une interactionplutôt que sa présence. Nous tentons d'évaluer les bénéfices de cette approchesur un problème de traitement d'images.	Isabelle Bloch, Emanuel Aldea	http://editions-rnti.fr/render_pdf.php?p1&p=1000772	http://editions-rnti.fr/render_pdf.php?p=1000772	886	fr	fr	@telecom-paristech.fr	Vers un utilisation améliorer de relation spatial pour le apprentissage de donnée dans le modèle graphiques  Nous nous intéresser dans ce article aux représentation des relationsspatiales pour le extraction d' information et le modélisation des donnéesvisuelles , en particulier dans le contexte de le catégorisation d' image . Nousmontrons comment le prise en compte d' un relation spatial entre deux élémentsentraîne le apparition d' un information supplémentaire entre ce élémentset le reste de le ensemble à modéliser , ce qui être rarement exploiter explicitement . un représentation flou des relation dans unmodèle graphique être bien adaptéepour le algorithme d' apprentissage utiliser actuellement et permettre d' intégrerce type d' information complémentaire qui concerner le absence d' un interactionplutôt que son présence . Nous tenter d' évaluer le bénéfice de ce approchesur un problème de traitement d' image . 	Vers une utilisation améliorée de relations spatiales pour l'apprentissage de données dans les modèles graphiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	A spatial rough set for extracting the periurban fringe	To date the availability of spatial data is increasing together withtechniques and methods adopted in geographical analysis. Despite this tendency,classifying in a sharp way every part of the city is more and more complicated.This is due to the growth of city complexity. Rough Set theory maybe a useful method to employ in combining great amounts of data in order tobuild complex knowledge about territory. It represents a different mathematicalapproach to uncertainty by capturing the indiscernibility. Two differentphenomena can be indiscernible in some contexts and classified in the sameway when combining available information about them. Several experiencesexist in the use of Rough Set theory in data mining, knowledge analysis andapproximate pattern classification, but the spatial component lacks in all theseresearch streams.This paper aims to the use of Rough Set methods in geographical analyses.This approach has been applied in a case of study, comparing the resultsachieved by means of both Map Algebra technique and Spatial Rough set. Thestudy case area, Potenza Province, is particularly suitable for the application ofthis theory, because it includes 100 municipalities with a different number ofinhabitants and morphologic features.	Beniamino Murgante, Giuseppe Las Casas, Anna Sansone	http://editions-rnti.fr/render_pdf.php?p1&p=1001234	http://editions-rnti.fr/render_pdf.php?p=1001234	997	en	en	@unibas.it	spatial rough set extract periurban fringe date availability spatial data increase together withtechnique method adopt geographical analysis despite tendency classify sharp way every part city complicate this due growth city complexity rough set theory maybe useful method employ combine great amount data order tobuild complex knowledge territory represent different mathematicalapproach uncertainty capture indiscernibility two differentphenomena indiscernible context classify sameway combine available information them several experiencesexist use rough set theory datum mining knowledge analysis andapproximate pattern classification spatial component lack theseresearch stream this paper aim use rough set method geographical analysis this approach apply case study compare resultsachieve means map algebra technique spatial rough set thestudy case area potenza province particularly suitable application ofthis theory include 100 municipality different number ofinhabitant morphologic feature	A spatial rough set for extracting the periurban fringe	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Algorithmes rapides de boosting de SVM	Les algorithmes de boosting de Newton Support Vector Machine (NSVM), Proximal Support Vector Machine (PSVM) et Least-Squares Support Vector Machine (LS-SVM) que nous présentons visent à la classification de très grands ensembles de données sur des machines standard. Nous présentons une extension des algorithmes de NSVM, PSVM et LS-SVM, pour construire des algorithmes de boosting. A cette fin, nous avons utilisé un terme de régularisation de Tikhonov et le théorème Sherman-Morrison- Woodbury pour adapter ces algorithmes au traitement d'ensembles de données ayant un grand nombre de dimensions. Nous les avons ensuite étendus par construction d'algorithmes de boosting de NSVM, PSVM et LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances des algorithmes sont évaluées sur des grands ensembles de données de l'UCI comme Adult, KDDCup 1999, Forest Covertype, Reuters-21578 et RCV1-binary sur une machine standard (PC-P4, 2,4 GHz, 1024 Mo RAM).	Thanh-Nghi Do, Jean-Daniel Fekete, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000617	http://editions-rnti.fr/render_pdf.php?p=1000617	998	fr	fr		algorithme rapide de boosting de SVM  le algorithme de boosting de Newton Support Vector Machine ( NSVM ) , Proximal Support Vector Machine ( PSVM ) et Least-Squares Support Vector Machine ( LS-SVM ) que nous présenter viser à le classification de très grand ensemble de donnée sur un machine standard . Nous présenter un extension des algorithme de NSVM , PSVM et LS-SVM , pour construire un algorithme de boosting . A ce fin , nous avoir utiliser un terme de régularisation de Tikhonov et le théorème Sherman-Morrison-Woodbury pour adapter ce algorithme au traitement d' ensemble de donnée avoir un grand nombre de dimension . Nous les avoir ensuite étendre par construction d' algorithme de boosting de NSVM , PSVM et LS-SVM afin de traiter un donnée avoir simultanément un grand nombre d' individu et de dimension . le performance des algorithme être évaluer sur un grand ensemble de donnée de le UCI comme Adult , KDDCup 1999 , Forest Covertype , Reuters- 21578 et RCV1-binary sur un machine standard ( PC-P4 , 2_,_4 GHz , 1024 Mo RAM ) . 	Algorithmes rapides de boosting de SVM	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Analyse exploratoire d'opinions cinématographiques : co-clustering de corpus textuels communautaires	Les sites communautaires sont un endroit privilégié pour s'exprimer et publier des opinions. Le site www.flixster.com est un exemple de site participatif sur lequel se rassemblent plus de 20 millions de cinéphiles qui partagent des commentaires sur les films qu'ils ont ou non aimés. Explorer les contenus autoproduits est un challenge pour qui veut comprendre les attentes des internautes. Par une méthode d'apprentissage non supervisée, nous montrerons qu'il est possible de mieux comprendre le vocabulaire utilisé pour décrire des opinions. En particulier, grâce à une méthode de co-clustering, nous montrerons qu'un rapprochement peut être fait entre des films particuliers sur la base de l'usage d'un vocabulaire particulier. L'analyse des résultats peut conduire à retrouver une certaine typologie de films ou encore des rapprochements entre films. Cette étude peut être complémentaire avec des analyses linguistiques des corpus, ou encore être exploitée dans un contexte applicatif de recommandation de contenus multimédias.	Damien Poirier, Cécile Bothorel, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000654	http://editions-rnti.fr/render_pdf.php?p=1000654	999	fr	fr		analyse exploratoire d' opinion cinématographique : co- clustering de corpus textuel communautaires  le site communautaire être un endroit privilégier pour clr exprimer et publier un opinion . le site www.flixster.com être un exemple de site participatif sur lequel clr rassembler plus de 20 million de cinéphile qui partager un commentaire sur le film qu' ils avoir ou non aimer . explorer le contenu autoproduire être un challenge pour qui vouloir comprendre le attente des internaute . Par un méthode d' apprentissage non superviser , nous montrer qu' il être possible de mieux comprendre le vocabulaire utiliser pour décrire un opinion . En particulier , grâce à un méthode de co- clustering , nous montrer qu' un rapprochement pouvoir être faire entre un film particulier sur le base de le usage d' un vocabulaire particulier . le analyse des résultat pouvoir conduire à retrouver un certain typologie de film ou encore un rapprochement entre film . ce étude pouvoir être complémentaire avec un analyse linguistique des corpus , ou encore être exploiter dans un contexte applicatif de recommandation de contenu multimédia . 	Analyse exploratoire d'opinions cinématographiques : co-clustering de corpus textuels communautaires	4
Revue des Nouvelles Technologies de l'Information	EGC	2008	Apport des traitements morpho-syntaxiques pour l'alignement des définitions par une classification SVM	Cet article propose une méthode d'alignement automatique de définitions destinée à améliorer la fusion entre des terminologies spécialisées et un vocabulaire médical généraliste par un classifieur de type SVM (Support Vecteur Machine) et une représentation compacte et pertinente d'un couple de définitions par concaténation d'un ensemble de mesures de similarité, afin de tenir compte de leur complémentarité, auquelle nous ajoutons les longueurs de chacune des définitions. Trois niveaux syntaxiques ont été investigués. Le modèle fondé sur un apprentissage à partir des groupes nominaux de type Noms-Adjectifs aboutit aux meilleures performances.	Laura Diosan, Alexandrina Rogozan, Jean-Pierre Pécuchet	http://editions-rnti.fr/render_pdf.php?p1&p=1000582	http://editions-rnti.fr/render_pdf.php?p=1000582	1000	fr	fr		apport des traitement morpho- syntaxique pour le alignement des définition par un classification SVM  ce article proposer un méthode d' alignement automatique de définition destiner à améliorer le fusion entre un terminologie spécialiser et un vocabulaire médical généraliste par un classifieur de type SVM ( Support Vecteur Machine ) et un représentation compact et pertinent d' un couple de définition par concaténation d' un ensemble de mesure de similarité , afin de tenir compte de son complémentarité , auquelle nous ajouter le longueur de chacun des définition . Trois niveau syntaxique avoir être investigués . le modèle fonder sur un apprentissage à partir un groupe nominal de type Noms-Adjectifs aboutir aux meilleur performance . 	Apport des traitements morpho-syntaxiques pour l'alignement des définitions par une classification SVM	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Approche d'annotation automatique des événements	"Quotidiennement, plusieurs agences de presse publient des milliers d'articles contenant plusieurs événements de toutes sortes (politiques, économiques, culturels, etc.). Les preneurs de décision, se trouvent face à ce grand nombre d'événements dont seulement quelques uns les concernent. Le traitement automatique de tels événements devient de plus en plus nécessaires. Pour cela, nous proposons une approche, qui se base sur l'apprentissage automatique, et qui permet d'annoter les articles de presse pour générer un résumé automatique contenant les principaux événements. Nous avons validé notre approche par le développement du système ""AnnotEv""."	Rim Faiz, Aymen Elkhlifi	http://editions-rnti.fr/render_pdf.php?p1&p=1000554	http://editions-rnti.fr/render_pdf.php?p=1000554	1001	fr	fr		approche d' annotation automatique des événements  " quotidiennement , plusieurs agence de presse publier un millier d' article contenir plusieurs événement de tout sorte ( politique , économique , culturel , etc. ) . . le preneur de décision , clr trouver face à ce grand nombre d' événement dont seulement quelque un les concerner . . le traitement automatique de tel événement devenir de plus en plus nécessaire . . Pour cela , nous proposer un approche , qui clr baser sur le apprentissage automatique , et qui permettre d' annoter le article de presse pour générer un résumé automatique contenir le principal événement . . Nous avoir valider son approche par le développement du système " " AnnotEv " " . " 	Approche d'annotation automatique des événements	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Approche hybride de classification à base de treillis de Galois: application à la reconnaissance de visages	La recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de l'apprentissage automatique. Le problème de classification de visages peut être considéré comme un problème d'apprentissage supervisé où les exemples d'apprentissage sont les visages étiquetés. Notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme d'apprentissage automatique supervisé. Ainsi, en se basant sur le fondement mathématique des treillis de Galois et leur utilisation pour la classification supervisée, nous proposons un nouvel algorithme de classification baptisé CITREC ainsi que son application pour la reconnaissance de visages. L'originalité de notre approche provient de la combinaison de l'analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne ou à plus proches voisins. Une validation expérimentale est décrite sur un benchmark du domaine de la reconnaissance de visages.	Yahya Slimani, Cherif Chiraz Latiri, Brahim Douar	http://editions-rnti.fr/render_pdf.php?p1&p=1000618	http://editions-rnti.fr/render_pdf.php?p=1000618	1002	fr	fr		approche hybride de classification à base de treillis de Galois : application à le reconnaissance de visages  le recherche dans le domaine de le reconnaissance de visage profiter un solution obtenir dans le domaine de le apprentissage automatique . le problème de classification de visage pouvoir être considérer comme un problème d' apprentissage superviser où le exemple d' apprentissage être le visage étiqueter . son article introduire dans ce contexte un nouveau approche hybride de classification qui utiliser le paradigme d' apprentissage automatique superviser . ainsi , en clr baser sur le fondement mathématique des treillis de Galois et son utilisation pour le classification superviser , nous proposer un nouveau algorithme de classification baptiser CITREC ainsi que son application pour le reconnaissance de visage . le originalité de son approche provenir de le combinaison de le analyse formel de concept avec le approche de classification superviser à inférence bayésien ou à plus proche voisin . un validation expérimental être décrire sur un benchmark du domaine de le reconnaissance de visage . 	Approche hybride de classification à base de treillis de Galois: application à la reconnaissance de visages	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Approches de type n-grammes pour l'analyse de parcours de vie familiaux	Cet article porte sur l'analyse de parcours de vie représentés sous forme de séquences d'événements. Plus spécifiquement, on examine les possibilités d'exploiter des codages de type n-grammes de ces séquences pour en extraire des connaissances. En fait, compte tenu de la simultanéité de certains événements, une procédure stricte de n-grammes comme on peut par exemple l'appliquer sur des textes, n'est pas applicable ici. Nous discutons diverses alternatives qui s'avèrent finalement plus proches de la fouille de séquences fréquentes. Les concepts discutés sont illustrés sur des données de l'enquête biographique rétrospective réalisée par le Panel suisse de ménages en 2002. Enfin, on précisera sur quels aspects l'approche proposée peut apporter un éclairage complémentaire utile par rapport à d'autres techniques plus classiques d'analyse exploratoire de parcours de vie.	Matthias Studer, Alexis Gabadinho, Nicolas S. Müller, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1000639	http://editions-rnti.fr/render_pdf.php?p=1000639	1003	fr	fr		approche de type n-gramme pour le analyse de parcours de vie familiaux  ce article porter sur le analyse de parcours de vie représenter sous forme de séquence d' événement . plus spécifiquement , on examiner le possibilité d' exploiter un codage de type n-gramme de ce séquence pour en extraire un connaissance . En fait , compter tenir de le simultanéité de certain événement , un procédure strict de n-gramme comme on pouvoir par exemple l' appliquer sur un texte , n' être pas applicable ici . Nous discuter divers alternatif qui clr avérer finalement plus proche de le fouille de séquence fréquent . le concept discuter être illustrer sur un donnée de le enquête biographique rétrospectif réaliser par le Panel suisse de ménage en 2002 . enfin , on préciser sur quel aspect le approche proposer pouvoir apporter un éclairage complémentaire utile par rapport à un autre technique plus classique d' analyse exploratoire de parcours de vie . 	Approches de type n-grammes pour l'analyse de parcours de vie familiaux	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Assignation automatique de solutions à des classes de plaintes liées aux ambiances intérieures polluées	Nous présentons dans cet article un système informatique pour le traitement des plaintes en lien avec des situations de pollution domestique écrites en français. Après la construction automatique d'une base de scenarii de plaintes, un module de recherche apparie la plainte à traiter à la thématique de la plainte la plus similaire. Enfin, il s'agit d'assigner au problème courant la solution correspondante au scénario de pollution auquel est affectée la plainte pertinente. Nous montrons ici l'intérêt de l'introduction dans l'appariement des textes de l'aspect sémantique géré par un dictionnaire généraliste de synonymes et en quoi il n'est pas réalisable pour notre problème particulier de construire une ontologie.	Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1000656	http://editions-rnti.fr/render_pdf.php?p=1000656	1004	fr	fr		assignation automatique de solution à un classe de plainte lier aux ambiance intérieur polluées  Nous présenter dans ce article un système informatique pour le traitement des plainte en lien avec un situation de pollution domestique écrire en français . Après le construction automatique d' un base de scenarii de plainte , un module de recherche apparier le plainte à traiter à le thématique de le plainte le plus similaire . enfin , il clr agir d' assigner au problème courir le solution correspondant au scénario de pollution auquel être affecter le plainte pertinent . Nous montrer ici le intérêt de le introduction dans le appariement des texte de le aspect sémantique gérer par un dictionnaire généraliste de synonyme et en quoi il n' être pas réalisable pour son problème particulier de construire un ontologie . 	Assignation automatique de solutions à des classes de plaintes liées aux ambiances intérieures polluées	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Binary Block GTM : Carte auto-organisatrice probabiliste pour les grands tableaux binaires	Ce papier présente un modèle génératif et son estimation permettant la visualisation de données binaires. Notre approche est basée sur un modèle de mélange de lois de Bernoulli par blocs et les cartes de Kohonen probabilistes. La méthode obtenue se montre à la fois parcimonieuse et pertinente en pratique.	Rodolphe Priam, Mohamed Nadif, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1000614	http://editions-rnti.fr/render_pdf.php?p=1000614	1005	fr	fr		Binary Block GTM : carte auto- organisatrice probabiliste pour le grand tableau binaires  ce papier présenter un modèle génératif et son estimation permettre le visualisation de donnée binaire . son approche être baser sur un modèle de mélange de loi de Bernoulli par bloc et le carte de Kohonen probabiliste . le méthode obtenir clr montrer à le foi parcimonieux et pertinent en pratique . 	Binary Block GTM : Carte auto-organisatrice probabiliste pour les grands tableaux binaires	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Cas d'utilisation réelle de Nautilus : calculs d'indicateurs chez un opérateur télécom	Nautilus est un logiciel d'analyse de bases de données. Le but de cette application est de généraliser l'utilisation de données clients au sein des entreprises. Elle facilite l'accès aux données en permettant de visualiser et manipuler les données du SGBD sous forme de concepts métiers. Elle inclut un générateur de requêtes SQL et un outil de gestion de tâches désignées pour l'agrégation de grands volumes de données. Le principe de fonctionnement est basé sur l'enchaînement de phases permettant la création des données d'analyse : importation des métadonnées du SGBD ; construction d'un dictionnaire de des concepts métiers ; spécification des champs à calculer. Les différents traitements tels que les jointures et l'alimentation des tables sont optimisés afin de rendre l'application utilisable sur des SGBD d'entreprise	Adrien Schmidt, Serge Fantino	http://editions-rnti.fr/render_pdf.php?p1&p=1000601	http://editions-rnti.fr/render_pdf.php?p=1000601	1006	fr	fr		cas d' utilisation réel de Nautilus : calcul d' indicateur chez un opérateur télécom  Nautilus être un logiciel d' analyse de base de donnée . le but de ce application être de généraliser le utilisation de donnée client au sein des entreprise . Elle faciliter le accès aux donnée en permettre de visualiser et manipuler le donnée du SGBD sous forme de concept métier . Elle inclure un générateur de requête SQL et un outil de gestion de tâche désigner pour le agrégation de grand volume de donnée . le principe de fonctionnement être baser sur le enchaînement de phase permettre le création des donnée d' analyse : importation des métadonnées du SGBD ; construction d' un dictionnaire de un concept métier ; spécification des champ à calculer . le différent traitement tel que le jointure et le alimentation des table être optimiser afin de rendre le application utilisable sur un SGBD d' entreprise 	Cas d'utilisation réelle de Nautilus : calculs d'indicateurs chez un opérateur télécom	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Classification adaptative de séries temporelles : application à l'identification des gènes exprimés au cours du cycle cellulaire	Ce travail s'inscrit dans le cadre de l'étude de la division cellulaire assurant la prolifération des cellules. Une meilleure compréhension de ce phénomène biologique nécessite l'identification des gènes caractérisant chaque phase du cycle cellulaire. Le procédé d'identification est généralement basé sur un ensemble de gènes dits gènes de référence, sélectionnés expérimentalement et considérés comme caractérisant les phases du cycle cellulaire. Les niveaux d'expression des gènes étudiés sont mesurés durant le cycle de la division cellulaire et permettent de construire des profils d'expression. Chaque gène étudié est affecté à la phase du cycle cellulaire correspondant au groupe de gènes de référence le plus similaire. Cette approche classique souffre de deux limites. D'une part les mesures de proximité les plus couramment utilisés entre profils d'expression de gènes sont basées sur les écarts en valeurs sans tenir compte de la forme des profils. D'autre part, dans la littérature, il n'y a pas consensus quant à l'ensemble des gènes de référence à considérer. Dans cet article, notre but est de proposer une classification adaptative, basée sur un indice de dissimilarité incluant les proximités en valeurs et en forme des profils d'expression de gènes, permettant d'identifier les phases d'expression des gènes étudiés, et de présenter un nouvel ensemble de gènes de référence validé par une connaissance biologique.	Alpha Diallo, Ahlame Douzal-Chouakria, Françoise Giroud	http://editions-rnti.fr/render_pdf.php?p1&p=1000637	http://editions-rnti.fr/render_pdf.php?p=1000637	1007	fr	fr		classification adaptatif de série temporel : application à le identification des gène exprimer au cour du cycle cellulaire  ce travail clr inscrire dans le cadre de le étude de le division cellulaire assurer le prolifération des cellule . un meilleur compréhension de ce phénomène biologique nécessiter le identification des gène caractériser chaque phase du cycle cellulaire . le procédé d' identification être généralement baser sur un ensemble de gène dire gène de référence , sélectionner expérimentalement et considérer comme caractériser le phase du cycle cellulaire . le niveau d' expression des gène étudier être mesurer durant le cycle de le division cellulaire et permettre de construire un profil d' expression . chaque gène étudier être affecter à le phase du cycle cellulaire correspondre au groupe de gène de référence le plus similaire . ce approche classique souffrir de deux limite . D' un part le mesure de proximité le plus couramment utiliser entre profil d' expression de gène être baser sur le écart en valeur sans tenir compte de le forme des profil . D' autre part , dans le littérature , il n' y avoir pas consensus quant à le ensemble des gène de référence à considérer . Dans ce article , son but être de proposer un classification adaptatif , baser sur un indice de dissimilarité inclure le proximité en valeur et en forme des profil d' expression de gène , permettre d' identifier le phase d' expression des gène étudier , et de présenter un nouveau ensemble de gène de référence valider par un connaissance biologique . 	Classification adaptative de séries temporelles : application à l'identification des gènes exprimés au cours du cycle cellulaire	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Classification de documents en réseaux petits mondes en vue d'apprentissage		Mohamed Khazri, Mohamed Tmar, Mohand Boughanem, Mohamed Abid	http://editions-rnti.fr/render_pdf.php?p1&p=1000581	http://editions-rnti.fr/render_pdf.php?p=1000581	1008	fr			Classification de documents en réseaux petits mondes en vue d'apprentissage 	Classification de documents en réseaux petits mondes en vue d'apprentissage	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Clustering en haute dimension par accumulation de clusterings locaux	"Le clustering est une tâche fondamentale de la fouille de données. Ces dernières années, les méthodes de type cluster ensembles ont été l'objet d'une attention soutenue. Il s'agit d'agréger plusieurs clusterings d'un jeu de données afin d'obtenir un clustering ""moyen"". Les clusterings individuels peuvent être le résultat de différents algorithmes. Ces méthodes sont particulièrement utiles lorsque la dimensionalité des données ne permet pas aux méthodes classiques basées sur la distance et/ou la densité de fonctionner correctement. Dans cet article, nous proposons une méthode pour obtenir des clusterings individuels à faible coût, à partir de projections partielles du jeu de données. Nous évaluons empiriquement notre méthode et la comparons à trois méthodes de différents types. Nous constatons qu'elle donne des résultats sensiblement supérieurs aux autres."	Marc-Ismaël Akodjènou-Jeannin, Kavé Salamatian, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000612	http://editions-rnti.fr/render_pdf.php?p=1000612	1009	fr	fr		Clustering en haut dimension par accumulation de clusterings locaux  " le clustering être un tâche fondamental de le fouille de donnée . . ce dernier année , le méthode de type cluster ensemble avoir être le objet d' un attention soutenir . . Il clr agir d' agréger plusieurs clusterings d' un jeu de donnée afin d' obtenir un clustering " " moyen " " . . le clusterings individuel pouvoir être le résultat de différent algorithme . . ce méthode être particulièrement utile lorsque le dimensionalité des donnée ne permettre pas aux méthode classique baser sur le distance et le densité de fonctionner correctement . . Dans ce article , nous proposer un méthode pour obtenir un clusterings individuel à faible coût , à partir de projection partiel du jeu de donnée . . Nous évaluer empiriquement son méthode et le comparons à trois méthode de différent type . . Nous constater qu' elle donner un résultat sensiblement supérieur aux autre . " 	Clustering en haute dimension par accumulation de clusterings locaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Clustering Visuel Semi-Supervisé pour des systèmes en coordonnées en étoiles 3D	Dans cet article, nous proposons une approche qui combine les méthodes statistiques avancées et la flexibilité des approches interactives manuelles en clustering visuel. Nous présentons l'interface Semi-Supervised Visual Clustering (SSVC). Sa contribution principale est l'apprentissage d'une métrique de projection optimale pour la visualisation en coordonnées en étoiles ainsi que pour l'extension 3D que nous avons développée. La métrique de distance de projection est apprise à partir des retours de l'utilisateur soit en termes de similarité/ dissimilarité entre les items, soit par l'annotation directe. L'interface SSVC permet, de plus, une utilisation hybride dans laquelle un ensemble de paramètres sont manuellement fixés par l'utilisateur tandis que les autres paramètres sont déterminés par un algorithme de distance optimale.	Loïc Lecerf, Boris Chidlovskii	http://editions-rnti.fr/render_pdf.php?p1&p=1000559	http://editions-rnti.fr/render_pdf.php?p=1000559	1010	fr	fr		Clustering Visuel Semi-Supervisé pour un système en coordonnée en étoile 3D  Dans ce article , nous proposer un approche qui combiner le méthode statistique avancer et le flexibilité des approche interactif manuel en clustering visuel . Nous présenter le interface Semi-Supervised Visual Clustering ( SSVC ) . son contribution principal être le apprentissage d' un métrique de projection optimal pour le visualisation en coordonnée en étoile ainsi que pour le extension 3D que nous avoir développer . le métrique de distance de projection être apprendre à partir un retour de le utilisateur être en terme de similarité  dissimilarité entre le item , soit par le annotation direct . le interface SSVC permettre , de plus , un utilisation hybride dans laquelle un ensemble de paramètre être manuellement fixer par le utilisateur tandis que le autre paramètre être déterminer par un algorithme de distance optimal . 	Clustering Visuel Semi-Supervisé pour des systèmes en coordonnées en étoiles 3D	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Co-classification sous contraintes par la somme des résidus quadratiques	"Dans de nombreuses applications, une co-classification est plus facile à interpréter qu'une classification mono-dimensionnelle. Il s'agit de calculer une bi-partition ou collection de co-clusters : chaque co-cluster est un groupe d'objets associé à un groupe d'attributs et les interprétations peuvent s'appuyer naturellement sur ces associations. Pour exploiter la connaissance du domaine et ainsi améliorer la pertinence des partitions, plusieurs méthodes de classification sous contraintes ont été proposées pour le cas mono-dimensionnel, e.g., l'exploitation de contraintes ""must-link"" et ""cannot-link"". Nous considérons ici la co-classification sous contraintes avec la gestion de telles contraintes étendues aux dimensions des objets et des attributs, mais aussi l'expression de contraintes de contiguité dans le cas de domaines ordonnés. Nous proposons un algorithme itératif qui minimise la somme des résidus quadratiques et permet l'exploitation active des contraintes spécifiées par les analystes. Nous montrons la valeur ajoutée de ce type d'extraction sur deux applications en analyse du transcriptome."	Ruggero G. Pensa, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000665	http://editions-rnti.fr/render_pdf.php?p=1000665	1011	fr	fr		Co- classification sous contrainte par le somme des résidu quadratiques  " Dans un nombreux application , un co- classification être plus facile à interpréter qu' un classification mono- dimensionnel . . Il clr agir de calculer un bi-partition ou collection de co- clusters : chaque co- cluster être un groupe d' objet associer à un groupe d' attribut et le interprétation pouvoir clr appuyer naturellement sur ce association . . Pour exploiter le connaissance du domaine et ainsi améliorer le pertinence des partition , plusieurs méthode de classification sous contrainte avoir être proposer pour le cas mono- dimensionnel , exempli gratia , le exploitation de contrainte " " must-link " " et " " cannot-link " " . . Nous considérer ici le co- classification sous contrainte avec le gestion de tel contrainte étendre aux dimension des objet et des attribut , mais aussi le expression de contrainte de contiguité dans le cas de domaine ordonner . . Nous proposer un algorithme itératif qui minimiser le somme des résidu quadratiques et permettre le exploitation actif des contrainte spécifier par le analyste . . Nous montrer le valeur ajouter de ce type d' extraction sur deux application en analyse du transcriptome . " 	Co-classification sous contraintes par la somme des résidus quadratiques	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Conception de systèmes d'information spatio-temporelle adaptatifs avec ASTIS	Les avancées technologiques récentes du Web et du sans fil,conjuguées au succès des applications spatialisées grand public, sont àl'origine d'un accès accru aux systèmes d'information spatio-temporelle(SIST) par une grande diversité d'utilisateurs, munis des dispositifs d'accèset dans des contextes d'utilisation variés. Adapter ces systèmes à l'utilisateurdevient donc une nécessité, un gage d'utilisabilité et de pérennité. Cet articleprésente une approche générique pour la conception et la génération desystèmes d'information spatio-temporelle adaptés à l'utilisateur, appeléASTIS. ASTIS offre des modalités générales de mise en oeuvre del'adaptation à l'utilisateur, visant tant le contenu que la présentation desapplications. Elle permet aux concepteurs d'intégrer ces modalitésd'adaptation dans des applications traitant des données spatio-temporelles.Afin de définir les besoins et types d'adaptation propres à leur application, ilsuffit aux concepteurs de créer des modèles conceptuels, par spécialisation etinstanciation des modèles offerts par notre architecture	Bogdan Moisuc, Jérôme Gensel, Hervé Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001232	http://editions-rnti.fr/render_pdf.php?p=1001232	1012	fr	fr	@imag.fr	conception de système d' information spatio- temporel adaptatif avec ASTIS  le avancée technologique récent du Web et du sans fil , conjuguer au succès des application spatialiser grand public , être àl'origine d' un accès accroître aux système d' information spatio- temporel ( SIST ) par un grand diversité d' utilisateur , munir des dispositif d' accèset dans un contexte d' utilisation varier . adapter ce système à le utilisateurdevient donc un nécessité , un gage d' utilisabilité et de pérennité . ce articleprésente un approche générique pour le conception et le génération desystèmes d' information spatio- temporel adapter à le utilisateur , appeléASTIS . asti offrir un modalité général de mise en oeuvre del'adaptation à le utilisateur , viser tant le contenu que le présentation desapplications . Elle permettre aux concepteur d' intégrer ce modalitésd'adaptation dans un application traitant des donnée spatio- temporel . Afin de définir le besoin et type d' adaptation propre à son application , ilsuffit aux concepteur de créer un modèle conceptuel , par spécialisation etinstanciation des modèle offrir par son architecture 	Conception de systèmes d'information spatio-temporelle adaptatifs avec ASTIS	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Data mining for activity extraction in video data	The exploration of large video data is a task which is now possible because of the advances made on object detection and tracking. Data mining techniques such as clustering are typically employed. Such techniques have mainly been applied for segmentation/indexation of video but knowledge extraction of the activity contained in the video has been only partially addressed. In this paper we present how video information is processed with the ultimate aim to achieve knowledge discovery of people activity in the video. First, objects of interest are detected in real time. Then, in an off-line process, we aim to perform knowledge discovery at two stages: 1) finding the main trajectory patterns of people in the video. 2) finding patterns of interaction between people and contextual objects in the scene. An agglomerative hierarchical clustering is employed at each stage. We present results obtained on real videos of the Torino metro (Italy).	Monique Thonnat, Jose Luis Patino, Etienne Corvée, François Brémond	http://editions-rnti.fr/render_pdf.php?p1&p=1000632	http://editions-rnti.fr/render_pdf.php?p=1000632	1013	en	en		datum mining activity extraction video datum exploration large video data task possible advance make object detection tracking data mining technique cluster typically employ technique mainly applied segmentation indexation video knowledge extraction activity contain video partially addressed paper present video information processed ultimate aim achieve knowledge discovery person activity video first object interest detected real time then off line process aim perform knowledge discovery two stage 1 finding main trajectory pattern person video 2 find pattern interaction person contextual object scene agglomerative hierarchical cluster employ stage present result obtained real video torino metro italy	Data mining for activity extraction in video data	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Découverte de motifs séquentiels et de règles inattendus	Les travaux autour de l'extraction de motifs séquentiels se sont particulièrement focalisés sur la définition d'approches efficaces pour extraire, en fonction d'une fréquence d'apparition, des corrélations entre des éléments dans des séquences. Même si ce critère de fréquence est déterminant, le décideur est également de plus en plus intéressé par des connaissances qui sont représentatives d'un comportement inattendu dans ces données (erreurs dans les données, fraudes, nouvelles niches, ... ). Dans cet article, nous introduisons le problème de la détection de motifs séquentiels inattendus par rapport aux croyances du domaine. Nous proposons l'approche USER dont l'objectif est d'extraire les motifs séquentiels et les règles inattendues dans une base de séquences.	Dong (Haoyuan) Li, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000641	http://editions-rnti.fr/render_pdf.php?p=1000641	1014	fr	fr		découverte de motif séquentiel et de règle inattendus  le travail autour de le extraction de motif séquentiel clr être particulièrement focaliser sur le définition d' approche efficace pour extraire , en fonction d' un fréquence d' apparition , un corrélation entre un élément dans un séquence . même si ce critère de fréquence être déterminant , le décideur être également de plus en plus intéresser par un connaissance qui être représentatif d' un comportement inattendu dans ce donnée ( erreur dans le donnée , fraude , nouveau niche , ... ) . Dans ce article , nous introduire le problème de le détection de motif séquentiel inattendu par rapport aux croyance du domaine . Nous proposer le approche USER dont le objectif être d' extraire le motif séquentiel et le règle inattendu dans un base de séquence . 	Découverte de motifs séquentiels et de règles inattendus	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Délestage pour l'analyse multidimensionnelle de flux de données	Dans le contexte de la gestion de flux de données, les données entrent dans le système à leur rythme. Des mécanismes de délestage sont à mettre en place pour qu'un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement. Le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié. Dans cet article, nous nous plaçons dans le cas où le système est un cube de données, dont la structure est connue a priori, alimenté par un flux de données. Nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube. Nous exploitons l'inégalité de Hoeffding pour obtenir une borne probabiliste sur l'écart entre la valeur attendue et la valeur estimée.	Sylvain Ferrandiz, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000580	http://editions-rnti.fr/render_pdf.php?p=1000580	1015	fr	fr		délestage pour le analyse multidimensionnel de flux de données  Dans le contexte de le gestion de flux de donnée , le donnée entrer dans le système à son rythme . un mécanisme de délestage être à mettre en place pour qu' un tel système pouvoir faire face aux situation où le débit des donnée dépasser son capacité de traitement . le lien entre réduction de le charge et dégradation de le qualité des résultat devoir alors être quantifier . Dans ce article , nous clr placer dans le cas où le système être un cube de donnée , dont le structure être connaître avoir priori , alimenter par un flux de donnée . Nous proposer un mécanisme de délestage pour le situation de surcharge et quantifier le dégradation de le qualité des résultat dans le cellule du cube . Nous exploiter le inégalité de Hoeffding pour obtenir un borner probabiliste sur le écart entre le valeur attendre et le valeur estimer . 	Délestage pour l'analyse multidimensionnelle de flux de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Détection de groupes atypiques pour une variable cible quantitative	"Une tâche importante en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes d'individus. Quelles sont les catégories d'individus qui gagnent de particulièrement forts salaires ou au contraire, quelles sont celles qui ont de très faibles salaires ? Nous présentons le problème d'extraction de tels groupes atypiques vis-à-vis d'une variable cible quantitative, comme par exemple la variable ""salaire"", et plus particulièrement pour les faibles et fortes valeurs d'un intervalle déterminé par l'utilisateur. Il s'agit donc de rechercher des conjonctions de variables dont la distribution diffère significativement de celle de l'ensemble d'apprentissage pour les faibles et fortes valeurs de l'intervalle de cette variable cible. Une adaptation d'une mesure statistique existante, l'intensité d'inclination, nous permet de découvrir de tels groupes atypiques. Cette mesure nous libère de l'étape de transformation des variables quantitatives, à savoir l'étape de discrétisation suivie d'un codage disjonctif complet. Nous proposons donc un algorithme d'extraction de tels groupes avec des règles d'élagage pour réduire la complexité du problème. Cet algorithme a été développé et intégré au logiciel d'extraction de connaissances WEKA. Nous terminons par un exemple d'extraction sur la base de données IPUMS du bureau de recensement américain."	Sylvie Guillaume, Florian Guillochon, Michel Schneider	http://editions-rnti.fr/render_pdf.php?p1&p=1000627	http://editions-rnti.fr/render_pdf.php?p=1000627	1016	fr	fr		détection de groupe atypique pour un variable cible quantitative  " un tâche important en analyse des donnée être le compréhension de comportement inattendu ou atypique de groupe d' individu . . Quelles être le catégorie d' individu qui gagner de particulièrement fort salaire ou au contraire , quelles être celui qui avoir de très faible salaire ? ? Nous présenter le problème d' extraction de tel groupe atypique vis-à-vis d' un variable cible quantitatif , comme par exemple le variable " " salaire " " , et plus particulièrement pour le faible et fort valeur d' un intervalle déterminer par le utilisateur . . Il clr agir donc de rechercher un conjonction de variable dont le distribution différer significativement de celui de le ensemble d' apprentissage pour le faible et fort valeur de le intervalle de ce variable cible . . un adaptation d' un mesure statistique existant , le intensité d' inclination , nous permettre de découvrir de tel groupe atypique . . ce mesure nous libérer de le étape de transformation des variable quantitatif , à savoir le étape de discrétisation suivre d' un codage disjonctif complet . . Nous proposer donc un algorithme d' extraction de tel groupe avec un règle d' élagage pour réduire le complexité du problème . . ce algorithme avoir être développer et intégrer au logiciel d' extraction de connaissance WEKA . . Nous terminer par un exemple d' extraction sur le base de donnée IPUMS du bureau de recensement américain . " 	Détection de groupes atypiques pour une variable cible quantitative	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Discretization of Continuous Features by Resampling	Les arbres de décision sont largement utilisés pour générer des classificateurs à partir d'un ensemble de données. Le processus de construction est une partitionnement récursif de l'ensemble d'apprentissage. Dans ce contexte, les attributs continus sont discrétisés. Il s'agit alors, pour chaque variable à discrétiser de trouver l'ensemble des points de coupure. Dans ce papier nous montrons que la recherche des ces points de coupure par une méthode de ré-échantillonnage, comme le BOOTSTRAP conduit à des meilleurs résultats. Nous avons testé cette approche avec les méthodes principales de discrétisation comme MDLPC, FUSBIN, FUSINTER, CONTRAST, Chi-Merge et les résultats sont systématiquement meilleurs en utilisant le bootstrap. Nous exposons ces principaux résultats et ouvrons de nouvelles pistes pour la construction d'arbres de décision.	Taimur Qureshi, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000622	http://editions-rnti.fr/render_pdf.php?p=1000622	1017	en	fr		le arbre de décision être largement utiliser pour générer un classificateur à partir d' un ensemble de donnée . le processus de construction être un partitionnement récursif de le ensemble d' apprentissage . Dans ce contexte , le attribut continu être discrétiser . Il clr agir alors , pour chaque variable à discrétiser de trouver le ensemble des point de coupure . Dans ce papier nous montrer que le recherche des ce point de coupure par un méthode de ré-échantillonnage , comme le BOOTSTRAP conduire à un meilleur résultat . Nous avoir tester ce approche avec le méthode principal de discrétisation comme MDLPC , FUSBIN , FUSINTER , CONTRAST , Chi-Merge et le résultat être systématiquement meilleur en utiliser le bootstrap . Nous exposer ce principal résultat et ouvrons de nouveau piste pour le construction d' arbre de décision . 	Discretization of Continuous Features by Resampling	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Echantillonnage adaptatif de jeux de données déséquilibrés pour les forêts aléatoires	Dans nombre d'applications, les données présentent un déséquilibre entre les classes. La prédiction est alors souvent détériorée pour la classe minoritaire. Pour contourner cela, nous proposons un échantillonnage guidé, lors des itérations successives d'une forêt aléatoire, par les besoins de l'utilisateur.	Elie Prudhomme, Julien Thomas, Pierre-Emmanuel Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1000588	http://editions-rnti.fr/render_pdf.php?p=1000588	1018	fr	fr		Echantillonnage adaptatif de jeu de donnée déséquilibrer pour le forêt aléatoires  Dans nombre d' application , le donnée présenter un déséquilibre entre le classe . le prédiction être alors souvent détériorer pour le classe minoritaire . Pour contourner cela , nous proposer un échantillonnage guider , lors un itération successif d' un forêt aléatoire , par le besoin de le utilisateur . 	Echantillonnage adaptatif de jeux de données déséquilibrés pour les forêts aléatoires	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Echantillonnage pour l'extraction de motifs séquentiels : des bases de données statiques aux flots de données	"Depuis quelques années, la communauté fouille de données s'est intéressée à la problématique de l'extraction de motifs séquentiels à partir de grandes bases de données en considérant comme hypothèse que les données pouvaient être chargées en mémoire centrale. Cependant, cette hypothèse est mise en défaut lorsque les bases manipulées sont trop volumineuses. Dans cet article, nous étudions une technique d'échantillonnage basée sur des réservoirs et montrons comment cette dernière est particulièrement bien adaptée pour résumer de gros volumes de données. Nous nous intéressons ensuite à la problématique plus récente de la fouille sur des données disponibles sous la forme d'un flot continu et éventuellement infini (""data stream""). Nous étendons l'approche d'échantillonnage à ce nouveau contexte et montrons que nous sommes à même d'extraire des motifs séquentiels de flots tout en garantissant les taux d'erreurs sur les résultats. Les différentes expérimentations menées confirment nos résultats théoriques."	Chedy Raïssi, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000567	http://editions-rnti.fr/render_pdf.php?p=1000567	1019	fr	fr		Echantillonnage pour le extraction de motif séquentiel : un base de donnée statique aux flot de données  " Depuis quelque année , le communauté fouiller de donnée clr être intéresser à le problématique de le extraction de motif séquentiel à partir un grand base de donnée en considérer comme hypothèse que le donnée pouvoir être charger en mémoire central . . cependant , ce hypothèse être mettre en défaut lorsque le base manipuler être trop volumineux . . Dans ce article , nous étudier un technique d' échantillonnage baser sur un réservoir et montrer comment ce dernier être particulièrement bien adapter pour résumer un gros volume de donnée . . Nous nous intéresser ensuite à le problématique plus récent de le fouille sur un donnée disponible sous le forme d' un flot continu et éventuellement infini ( " " dater stream " " ) . . Nous étendre le approche d' échantillonnage à ce nouveau contexte et montrer que nous sommer à même d' extraire un motif séquentiel de flot tout en garantir le taux d' erreur sur le résultat . . le différent expérimentation mener confirmer son résultat théorique . " 	Echantillonnage pour l'extraction de motifs séquentiels : des bases de données statiques aux flots de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Echantillonnage spatio-temporel de flux de données distribués	Ces dernières années, sont apparues de nombreuses applications, utilisant des données potentiellement infinies, provenant de façon continue de capteurs distribués. On retrouve ces capteurs dans des domaines aussi divers que la météorologie (établir des prévisions), le domaine militaire (surveiller des zones sensibles), l'analyse des consommations électriques (transmettre des alertes en cas de consommation anormale),... Pour faire face à la volumétrie et au taux d'arrivée des flux de données, des traitements sont effectués 'à la volée' sur les flux. En particulier, si le système n'est pas assez rapide pour traiter toutes les données d'un flux, il est possible de construire des résumés de l'information. Cette communication a pour objectif de faire un premier point sur nos travaux d'échantillonnage dans un environnement de flux de données fortement distribués. Notre approche est basée sur la théorie des sondages, l'analyse des données fonctionnelles et la gestion de flux de données. Cette approche sera illustrée par un cas réel : celui des mesures de consommations électriques	Raja Chiky, Jérôme Cubillé, Alain Dessertaine, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000569	http://editions-rnti.fr/render_pdf.php?p=1000569	1020	fr	fr		Echantillonnage spatio- temporel de flux de donnée distribués  ce dernier année , être apparaître de nombreux application , utiliser un donnée potentiellement infini , provenir de façon continuer de capteur distribuer . On retrouver ce capteur dans un domaine aussi divers que le météorologie ( établir un prévision ) , le domaine militaire ( surveiller un zone sensible ) , le analyse des consommation électrique ( transmettre un alerte en cas de consommation anormal ) , ... Pour faire face à le volumétrie et au taux d' arrivé des flux de donnée , un traitement être effectuer ' à le volé ' sur le flux . En particulier , si le système n' être pas assez rapide pour traiter tout le donnée d' un flux , il être possible de construire un résumé de le information . ce communication avoir pour objectif de faire un premier point sur son travail d' échantillonnage dans un environnement de flux de donnée fortement distribuer . son approche être baser sur le théorie des sondage , le analyse des donnée fonctionnel et le gestion de flux de donnée . ce approche être illustrer par un cas réel : celui des mesure de consommation électriques 	Echantillonnage spatio-temporel de flux de données distribués	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Enhancing Personal File Retrieval in Semantic File Systems with Tag-Based Context	Recently, tagging systems are widely used on the Internet. On desktops, tags are also supported by some semantic file systems and desktop search tools. In this paper, we focus on personal tag organization to enhance personal file retrieval. Our approach is based on the notion of context. A context is a set of tags assigned to a file by a user. Based on tag popularity and relationships between tags, our proposed algorithm creates a hierarchy of contexts on which a user can navigate to retrieve files in an effective manner.	Ba-Hung Ngo, Frédérique Silber-Chaussumier, Christian Bac	http://editions-rnti.fr/render_pdf.php?p1&p=1000558	http://editions-rnti.fr/render_pdf.php?p=1000558	1021	en	en		enhance personal file retrieval semantic file system tag base context recently tag system widely used internet desktop tag also support semantic file system desktop search tool paper focus personal tag organization enhance personal file retrieval approach base notion context context set tag assign file user base tag popularity relationship tag propose algorithm create hierarchy context user navigate retrieve file effective manner	Enhancing Personal File Retrieval in Semantic File Systems with Tag-Based Context	4
Revue des Nouvelles Technologies de l'Information	EGC	2008	Étude comparative de deux approches de classification recouvrante : MOC vs. OKM	La classification recouvrante désigne les techniques de regroupements de données en classes pouvant s'intersecter. Particulièrement adaptés à des domaines d'application actuels (e.g. Recherche d'Information, Bioinformatique) quelques modèles théoriques de classification recouvrante ont été proposés très récemment parmi lesquels le modèle MOC (Banerjee et al. (2005a)) utilisant les modèles de mélanges et l'approche OKM (Cleuziou (2007)) consistant à généraliser l'algorithme des k-moyennes. La présente étude vise d'une part à étudier les limites théoriques et pratiques de ces deux modèles, et d'autre part à proposer une formulation de l'approche OKM en terme de modèles de mélanges gaussiens, laissant ainsi entrevoir des perspectives intéressantes quant à la variabilité des schémas de recouvrements envisageables.	Guillaume Cleuziou, Jacques-Henri Sublemontier	http://editions-rnti.fr/render_pdf.php?p1&p=1000666	http://editions-rnti.fr/render_pdf.php?p=1000666	1022	fr	fr		Étude comparatif de deux approche de classification recouvrante : MOC vs. OKM  le classification recouvrante désigner le technique de regroupement de donnée en classe pouvoir clr intersecter . particulièrement adapter à un domaine d' application actuel ( e.g. recherche d' information , Bioinformatique ) quelque modèle théorique de classification recouvrante avoir être proposer très récemment parmi lesquels le modèle MOC ( Banerjee et al. ( 2005a ) ) utiliser le modèle de mélange et le approche OKM ( Cleuziou ( 2007 ) ) consister à généraliser le algorithme des k-moyennes . le présent étude viser d' un part à étudier le limite théorique et pratique de ce deux modèle , et d' autre part à proposer un formulation de le approche OKM en terme de modèle de mélange gaussiens , laisser ainsi entrevoir un perspective intéressant quant à le variabilité des schéma de recouvrement envisageable . 	Étude comparative de deux approches de classification recouvrante : MOC vs. OKM	5
Revue des Nouvelles Technologies de l'Information	EGC	2008	Étude de l'interaction entre variables pour l'extraction des règles d'influence	Cet article présente une méthode efficace pour l'extraction de règles d'influence quantitatives positives et négatives. Ces règles d'influence introduisent une nouvelle sémantique qui vise à faciliter l'analyse d'un volume important de données. Cette sémantique fixe la direction de la règle entre deux variables en positionnant, au préalable, l'une comme étant l'influent et l'autre comme étant l'influé. Elle permet, de ce fait, d'exprimer la nature de l'influence : positive, en maximisant le nombre d'éléments en commun ou négative, en maximisant le nombre d'éléments qui violent l'influé. Notre approche s'appuie sur une stratégie qui comporte cinq étapes dont deux exécutées en parallèle. Ces deux étapes constituent les étapes clé de notre approche. La première combine une méthode d'élagage et de regroupement tabulaire basée sur les tableaux de contingence. Cette dernière construit et classe les zones potentiellement intéressantes. La seconde, injecte la sémantique et évalue le degré d'influence que produirait l'introduction d'une nouvelle variable sur un ensemble de variables en utilisant une nouvelle mesure d'intérêt, l'Influence. Cette étape vient affiner les résultats de la première étape, et permet de se focaliser sur des zones valides par rapport aux contraintes spécifiées. Enfin, un système de règles d'influence jugées intéressantes est construit basé sur la juxtaposition des résultats des deux étapes clé de notre approche.	Leila Nemmiche Alachaher, Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1000629	http://editions-rnti.fr/render_pdf.php?p=1000629	1023	fr	fr		Étude de le interaction entre variable pour le extraction des règle d' influence  ce article présenter un méthode efficace pour le extraction de règle d' influence quantitatif positif et négatif . ce règle d' influence introduire un nouveau sémantique qui viser à faciliter le analyse d' un volume important de donnée . ce sémantique fixer le direction de le règle entre deux variable en positionner , au préalable , le un comme être le influent et le autre comme être le influer . Elle permettre , de ce fait , d' exprimer le nature de le influence : positif , en maximiser le nombre d' élément en commun ou négatif , en maximiser le nombre d' élément qui violent le influer . son approche clr appuyer sur un stratégie qui comporter cinq étape dont deux exécuter en parallèle . ce deux étape constituer le étape clé de son approche . le premier combiner un méthode d' élagage et de regroupement tabulaire baser sur le tableau de contingence . ce dernier construire et classe le zone potentiellement intéressant . le second , injecter le sémantique et évaluer le degré d' influence que produire le introduction d' un nouveau variable sur un ensemble de variable en utiliser un nouveau mesure d' intérêt , le Influence . ce étape venir affiner le résultat de le premier étape , et permettre de clr focaliser sur un zone valide par rapport aux contrainte spécifier . enfin , un système de règle d' influence juger intéressant être construire baser sur le juxtaposition des résultat des deux étape clé de son approche . 	Étude de l'interaction entre variables pour l'extraction des règles d'influence	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Évaluation des critères asymétriques pour les arbres de décision	Pour construire des arbres de décision sur des données déséquilibrées, des auteurs ont proposés des mesures d'entropie asymétriques. Le problème de l'évaluation de ces arbres se pose ensuite. Cet article propose d'évaluer la qualité d'arbres de décision basés sur une mesure d'entropie asymétrique.	Gilbert Ritschard, Simon Marcellin, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000587	http://editions-rnti.fr/render_pdf.php?p=1000587	1024	fr	fr		Évaluation des critère asymétrique pour le arbre de décision  Pour construire un arbre de décision sur un donnée déséquilibrer , un auteur avoir proposer un mesure d' entropie asymétrique . le problème de le évaluation de ce arbre clr poser ensuite . ce article proposer d' évaluer le qualité d' arbre de décision baser sur un mesure d' entropie asymétrique . 	Évaluation des critères asymétriques pour les arbres de décision	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	ExpLSA : utilisation d'informations syntaxico-sémantiques associées à LSA pour améliorer les méthodes de classification conceptuelle	L'analyse sémantique latente (LSA - Latent Semantic Analysis) est aujourd'hui utilisée dans de nombreux domaines comme la modélisation cognitive, les applications éducatives mais aussi pour la classification. L'approche présentée dans cet article consiste à ajouter des informations grammaticales à LSA. Différentes méthodes pour exploiter ces informations grammaticales sont étudiées dans le cadre d'une tâche de classification conceptuelle.	Nicolas Béchet, Mathieu Roche, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000658	http://editions-rnti.fr/render_pdf.php?p=1000658	1025	fr	fr		ExpLSA : utilisation d' information syntaxico- sémantique associer à LSA pour améliorer le méthode de classification conceptuelle  le analyse sémantique latent ( LSA - Latent Semantic Analysis ) être aujourd' hui utiliser dans un nombreux domaine comme le modélisation cognitif , le application éducatif mais aussi pour le classification . le approche présenter dans ce article consister à ajouter un information grammatical à LSA . différent méthode pour exploiter ce information grammatical être étudier dans le cadre d' un tâche de classification conceptuel . 	ExpLSA : utilisation d'informations syntaxico-sémantiques associées à LSA pour améliorer les méthodes de classification conceptuelle	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction d'itemsets compacts	L'extraction d'itemsets fréquents est un sujet majeur de l'ECD et son but est de découvrir des corrélations entre les enregistrements d'un ensemble de données. Cependant, le support est calculé en fonction de la taille de la base dans son intégralité. Dans cet article, nous montrons qu'il est possible de prendre en compte des périodes difficiles à déceler dans l'organisation des données et qui contiennent des itemsets fréquents sur ces périodes. Nous proposons ainsi la définition des itemsets compacts, qui représentent un comportement cohérent sur une période spécifique et nous présentons l'algorithme DEICO qui permet leur découverte.	Bashar Saleh, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000628	http://editions-rnti.fr/render_pdf.php?p=1000628	1026	fr	fr		extraction d' itemsets compacts  le extraction d' itemsets fréquent être un sujet majeur de le ECD et son but être de découvrir un corrélation entre le enregistrement d' un ensemble de donnée . cependant , le support être calculer en fonction de le taille de le base dans son intégralité . Dans ce article , nous montrer qu' il être possible de prendre en compte des période difficile à déceler dans le organisation des donnée et qui contenir un itemsets fréquent sur ce période . Nous proposer ainsi le définition des itemsets compact , qui représenter un comportement cohérent sur un période spécifique et nous présenter le algorithme DEICO qui permettre son découverte . 	Extraction d'itemsets compacts	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction de Motifs Séquentiels Multidimensionnels Clos sans Gestion d'Ensemble de Candidats	L'extraction de motifs séquentiels permet de découvrir des corrélations entre événements au cours du temps. Introduisant plusieurs dimensions d'analyse, les motifs séquentiels multidimensionnels permettent de découvrir des motifs plus pertinents. Mais le nombre de motifs obtenus peut devenir très important. C'est pourquoi nous proposons, dans cet article, de définir une représentation condensée garantie sans perte d'information : les motifs séquentiels multidimensionnels clos extraits ici sans gestion d'ensemble de candidats.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000642	http://editions-rnti.fr/render_pdf.php?p=1000642	1027	fr	fr		extraction de motif Séquentiels Multidimensionnels Clos sans Gestion d' ensemble de Candidats  le extraction de motif séquentiel permettre de découvrir un corrélation entre événement au cour du temps . introduire plusieurs dimension d' analyse , le motif séquentiel multidimensionnel permettre de découvrir un motif plus pertinent . Mais le nombre de motif obtenir pouvoir devenir très important . C' être pourquoi nous proposer , dans ce article , de définir un représentation condenser garantir sans perte d' information : le motif séquentiel multidimensionnel clore extrait ici sans gestion d' ensemble de candidat . 	Extraction de Motifs Séquentiels Multidimensionnels Clos sans Gestion d'Ensemble de Candidats	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction d'un modèle numérique de terrain à partir de photographies par drone	Dans le suivi et la modélisation de l'érosion en montagne, lareprésentation fine du relief est une composante importante. En effet, laconnaissance des zones de concentration des eaux, notamment à traversl'apparition de rigoles élémentaires, est fondamentale pour bien décrire lesconnectivités entre les zones de mobilisation des sédiments sur le versant et leréseau hydrographique stabilisé. La résolution au sol permise par lesphotographies aériennes classiques ne permet pas d'accéder à unereprésentation 3D suffisamment fine des ravines élémentaires. Nous testonsl'utilisation de photographies stéréoscopiques à résolution centimétrique prisesà basse altitude par un drone pour obtenir un MNT précis. La question majeureconcerne les règles à suivre pour un meilleur compromis entre précision etfacilité d'élaboration, et l'évaluation de l'importance relative de chaque étapesur la qualité finale de la restitution. La zone d'étude est située dans lesBadlands de Draix (Alpes de Haute Provence).	Andres Jacome, Christian Puech, Damien Raclot, Jean-Stéphane Bailly, Bruno Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001233	http://editions-rnti.fr/render_pdf.php?p=1001233	1028	fr	fr	@teledetection.fr, @supagro.inra.fr, @cemagref.fr	extraction d' un modèle numérique de terrain à partir de photographie par drone  Dans le suivi et le modélisation de le érosion en montagne , lareprésentation fin du relief être un composante important . En effet , laconnaissance un zone de concentration des eau , notamment à traversl'apparition de rigole élémentaire , être fondamental pour bien décrire lesconnectivités entre le zone de mobilisation des sédiment sur le versant et leréseau hydrographique stabiliser . le résolution au sol permettre par lesphotographies aérien classique ne permettre pas d' accéder à unereprésentation 3D suffisamment fin des ravine élémentaire . Nous testonsl'utilisation de photographie stéréoscopique à résolution centimétrique prisesà bas altitude par un drone pour obtenir un MNT précis . le question majeureconcerne le règle à suivre pour un meilleur compromis entre précision etfacilité d' élaboration , et le évaluation de le importance relatif de chaque étapesur le qualité final de le restitution . le zone d' étude être situer dans lesBadlands de Draix ( Alpes de haut Provence ) . 	Extraction d'un modèle numérique de terrain à partir de photographies par drone	6
Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction et exploitation des annotations contextuelles	Dans la perspective d'offrir un web sémantique, des travaux ont cherché à automatiser l'extraction des annotations sémantiques à partir de textes pour représenter au mieux la sémantique que vise à transmettre une page web. Dans cet article nous proposons une approche d'extraction des annotations qui représentent le plus précisément possible le contenu d'un document. Nous proposons de prendre en compte la notion de contexte modélisé par des relations contextuelles émanant, à la fois, de la structure et de la sémantique du texte.	Noureddine Mokhtari, Rose Dieng-Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000551	http://editions-rnti.fr/render_pdf.php?p=1000551	1029	fr	fr		extraction et exploitation des annotation contextuelles  Dans le perspective d' offrir un web sémantique , un travail avoir chercher à automatiser le extraction des annotation sémantique à partir de texte pour représenter au mieux le sémantique que viser à transmettre un page web . Dans ce article nous proposer un approche d' extraction des annotation qui représenter le plus précisément possible le contenu d' un document . Nous proposer de prendre en compte le notion de contexte modéliser par un relation contextuel émaner , à le foi , de le structure et de le sémantique du texte . 	Extraction et exploitation des annotations contextuelles	6
Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction et validation par croisement des relations d'une ontologie de domaine		Lobna Karoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000594	http://editions-rnti.fr/render_pdf.php?p=1000594	1030	fr			Extraction et validation par croisement des relations d'une ontologie de domaine 	Extraction et validation par croisement des relations d'une ontologie de domaine	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	FIASCO : un nouvel algorithme d'extraction d'itemsets fréquents dans les flots de données	Nous présentons dans cet article un nouvel algorithme permettant la construction et la mise à jour incrémentale du FIA : FIASCO. Notre algorithme effectue un seul passage sur les données et permet de prendre en compte les nouveaux batches, itemset par itemset et pour chaque itemset, item par item.	Lionel Vinceslas, Jean-Emile Symphor, Alban Mancheron, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000603	http://editions-rnti.fr/render_pdf.php?p=1000603	1031	fr	fr		fiasco : un nouveau algorithme d' extraction d' itemsets fréquent dans le flot de données  Nous présenter dans ce article un nouveau algorithme permettre le construction et le mise à jour incrémentale du FIA : fiasco . son algorithme effectuer un seul passage sur le donnée et permettre de prendre en compte le nouveau batches , itemset par itemset et pour chaque itemset , item par item . 	FIASCO : un nouvel algorithme d'extraction d'itemsets fréquents dans les flots de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Fouille de données audio pour la classification automatique de mots homophones	Cet article présente une contribution à la modélisation acoustique des mots à partir de grands corpus oraux, faisant appel aux techniques de fouilles de données. En transcription automatique, de nombreuses erreurs concernent des mots fréquents homophones. Deux paires de mots (quasi-)homophones à/a et et/est sont sélectionnées dans les corpus, pour lesquels sont définis et examinés 41 descripteurs acoustiques permettant potentiellement de les distinguer. 17 algorithmes de classification, mis à l'épreuve pour la discrimination automatique de ces deux paires de mots, donnent en moyenne 77% de classification correcte sur les 5 meilleurs algorithmes. En réduisant le nombre de descripteurs à 10 (sélectionnés par l'algorithme le plus performant), les résultats de classification restent proches du résultat obtenu avec 41 attributs. Cette comparaison met en évidence le caractère discriminant de certains attributs, qui pourront venir enrichir à la fois la modélisation acoustique et nos connaissances des prononciations de l'oral.	Rena Nemoto, Martine Adda-Decker, Iona Vasilescu	http://editions-rnti.fr/render_pdf.php?p1&p=1000633	http://editions-rnti.fr/render_pdf.php?p=1000633	1032	fr	fr		fouille de donnée audio pour le classification automatique de mot homophones  ce article présenter un contribution à le modélisation acoustique des mot à partir un grand corpus oral , faire appel aux technique de fouille de donnée . En transcription automatique , de nombreux erreur concerner un mot fréquent homophone . Deux paire de mot ( quasi- ) homophone à  avoir et et  être être sélectionner dans le corpus , pour lesquels être définir et examiner 41 descripteur acoustique permettre potentiellement de les distinguer . 17 algorithme de classification , mettre à le épreuve pour le discrimination automatique de ce deux paire de mot , donner en moyenne 77 \% de classification correct sur le 5 meilleur algorithme . En réduire le nombre de descripteur à 10 ( sélectionner par le algorithme le plus performant ) , le résultat de classification rester proche du résultat obtenir avec 41 attribut . ce comparaison mettre en évidence le caractère discriminant de certain attribut , qui pouvoir venir enrichir à le foi le modélisation acoustique et son connaissance des prononciation de le oral . 	Fouille de données audio pour la classification automatique de mots homophones	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	From Mining the Web to Inventing the New Sciences Underlying the Internet	As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business. However, because of its novelty and the scale of data sets involved, few companies have figured out how to properly make use of this data. In this talk, I will review some of the challenges and opportunities in the utilization of data to drive this new generation of marketing systems. I will provide several examples of how data is utilized in critical ways to drive some of these capabilities. The discussion will be framed with theMore general framework of Grand Challenges for data mining : pragmatic and technical. I will conclude this presentation with a consideration of the larger issues surrounding the Internet as a technology that is ubiquitous in our lives, yet one where very little is understood, at the scientific level, in defining and understanding many of the basics the Internet enables : Community, Personalization, and the new Microeconomics of the web. This leads to an overview of the new Yahoo ! Research organization and its aims : inventing the new sciences underlying what we do on the Internet, focusing on areas that have received little attention in the traditional academic circles. Some illustrative examples will be reviewed to make the ultimate goals more concrete.	Usama M. Fayyad	http://editions-rnti.fr/render_pdf.php?p1&p=1000550	http://editions-rnti.fr/render_pdf.php?p=1000550	1033	en	en		mining web invent new science underlie internet internet continue change way live find information communicate business also take dramatically increase role marketing advertising unlike prior mass medium internet unique medium come interactivity offer ability target program messaging individual level couple uniqueness richness datum available measurability variety way utilize datum great dependence effective marketing application heavily data drive make datum mine statistical data analysis modeling report essential mission critical part run on line business however novelty scale datum set involved company figure properly make use data talk review challenge opportunity utilization data drive new generation marketing system provide several example data utilize critical way drive capability discussion frame themore general framework grand challenge datum mining pragmatic technical conclude presentation consideration larger issue surround internet technology ubiquitous life yet one little understood scientific level define understand many basic internet enable community personalization new microeconomic web lead overview new yahoo research organization aim invent new science underlie internet focuse area receive little attention traditional academic circle illustrative example review make ultimate goal concrete	From Mining the Web to Inventing the New Sciences Underlying the Internet	8
Revue des Nouvelles Technologies de l'Information	EGC	2008	Génération de séquence résumé par une nouvelle approche basée sur le Soft Computing	Cet article propose une approche d'abstraction des séquences vidéo basée sur le soft computing. Etant donné une longueur cible du condensé vidéo, on cherche les segments vidéo qui couvrent le maximum du visuel de la vidéo originale en respectant la longueur du condensé.	Youssef Hadi, Rachid El Meziane, Rachid Oulad Haj Thami	http://editions-rnti.fr/render_pdf.php?p1&p=1000586	http://editions-rnti.fr/render_pdf.php?p=1000586	1034	fr	fr		génération de séquence résumer par un nouveau approche baser sur le Soft Computing  ce article proposer un approche d' abstraction des séquence vidéo baser sur le soft computing . Etant donner un longueur cible du condenser vidéo , on chercher le segment vidéo qui couvrir le maximum du visuel de le vidéo original en respecter le longueur du condensé . 	Génération de séquence résumé par une nouvelle approche basée sur le Soft Computing	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Gradients de prototypicalité conceptuelle et lexicale	Longtemps les ontologies ont été limitées à des domaines scientifiques et techniques, favorisant au passage l'essor du concept de « connaissances universelles et objectives ». Avec l'émergence et l'engouement actuel pour les sciences cognitives, couplés à l'application des ontologies à des domaines relatifs aux Sciences Humaines et Sociales (SHS), la subjectivité des connaissances devient une dimension incontournable qui se doit d'être intégrée et prise en compte dans le processus d'ingénierie ontologique (IO). L'objectif de nos travaux est de développer la notion d'Ontologie Pragmatisée Vernaculaire de Domaine (OPVD). Le principe sous-jacent à de telles ressources consiste à considérer que chaque ontologie est non seulement propre à un domaine, mais également à un endogroupe donné, doté d'une pragmatique qui est fonction tant de la culture que de l'apprentissage et de l'état émotionnel du dit endogroupe. Cette pragmatique, qui traduit un processus d'appropriation et de personnalisation de l'ontologie considérée, est qualifiée à l'aide de deux mesures : un gradient de prototypicalité conceptuelle et un gradient de prototypicalité lexicale	Xavier Aimé, Frédéric Fürst, Pascale Kuntz, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000564	http://editions-rnti.fr/render_pdf.php?p=1000564	1035	fr	fr		gradient de prototypicalité conceptuel et lexicale  longtemps le ontologie avoir être limiter à un domaine scientifique et technique , favoriser au passage le essor du concept de « connaissance universel et objectif » . Avec le émergence et le engouement actuel pour le science cognitif , coupler à le application des ontologie à un domaine relatif aux science Humaines et Sociales ( SHS ) , le subjectivité des connaissance devenir un dimension incontournable qui clr devoir d' être intégrer et prendre en compte dans le processus d' ingénierie ontologique ( IO ) . le objectif de son travail être de développer le notion d' ontologie Pragmatisée Vernaculaire de Domaine ( OPVD ) . le principe sous-jacent à un tel ressource consister à considérer que chaque ontologie être non seulement propre à un domaine , mais également à un endogroupe donner , doter d' un pragmatique qui être fonction tant de le culture que de le apprentissage et de le état émotionnel du dire endogroupe . ce pragmatique , qui traduire un processus d' appropriation et de personnalisation de le ontologie considérer , être qualifier à le aide de deux mesure : un gradient de prototypicalité conceptuel et un gradient de prototypicalité lexicale 	Gradients de prototypicalité conceptuelle et lexicale	4
Revue des Nouvelles Technologies de l'Information	EGC	2008	HyperSmooth : calcul et visualisation de cartes de potentiel interactives	Le groupe de recherche Hypercarte propose HyperSmooth,un nouvel outil cartographique pour l'analyse spatiale de phénomènessociaux économiques mettant en oeuvre une méthode de calcul depotentiel. L'objectif est de pouvoir représenter de façon continue et enchangeant d'échelle d'analyse une information statistiqueéchantillonnée sur toutes sortes de maillages, réguliers ou non. Le défitechnologique est de fournir un outil accessible sur le Web, interactifet rapide, ceci malgré le coût élevé du calcul, et qui assure laconfidentialité des données. Nous présentons notre solution basée surune architecture client serveur : le serveur calcule les cartes depotentiel en utilisant des techniques d'optimisation particulières, alorsque le client est en charge de la visualisation et du paramétrage del'analyse, et les deux parties communiquent via un protocole Web.	Christine Plumejeaud, Jean-Marc Vincent, Claude Grasland, Jérôme Gensel, Hélène Mathian, Serge Guelton, Joël Boulier	http://editions-rnti.fr/render_pdf.php?p1&p=1001230	http://editions-rnti.fr/render_pdf.php?p=1001230	1036	fr	fr	@imag.fr, @parisgeo.cnrs.fr	HyperSmooth : calcul et visualisation de carte de potentiel interactives  le groupe de recherche Hypercarte proposer HyperSmooth , un nouveau outil cartographique pour le analyse spatial de phénomènessociaux économique mettre en oeuvre un méthode de calcul depotentiel . le objectif être de pouvoir représenter de façon continu et enchangeant d' échelle d' analyse un information statistiqueéchantillonnée sur tout sorte de maillage , régulier ou non . le défitechnologique être de fournir un outil accessible sur le Web , interactifet rapide , ceci malgré le coût élever du calcul , et qui assurer laconfidentialité un donnée . Nous présenter son solution baser surune architecture client serveur : le serveur calculer le carte depotentiel en utiliser un technique d' optimisation particulier , alorsque le client être en charge de le visualisation et du paramétrage del'analyse , et le deux partie communiquer via un protocole Web . 	HyperSmooth : calcul et visualisation de cartes de potentiel interactives	5
Revue des Nouvelles Technologies de l'Information	EGC	2008	Industrialiser le data mining : enjeux et perspectives	L'informatique décisionnelle est un secteur en forte croissance dans toutes les entreprises. Les techniques classiques (reporting simple & Olap), qui s'intéressent essentiellement à présenter les données, sont aujourd'hui très largement déployées. Le data mining commence à se répandre, apportant des capacités de prévision à forte valeur ajoutée pour les entreprises les plus compétitives. Ce développement est rendu possible par la disponibilité croissante de masses de données importantes et la puissance de calcul dorénavant disponible. Cependant, la mise en IJuvre industrielle des projets de data mining pose des contraintes tant théoriques (quels algorithmes utiliser pour produire des modèles d'analyses exploitant des milliers de variables pour des millions d'exemples) qu'opérationnelles (comment mettre en production et contrôler le bon fonctionnement de centaines de modèles). Je présenterai ces contraintes issues des besoins des entreprises ; je montrerai comment exploiter des résultats théoriques (provenant des travaux de Vladimir Vapnik) pour produire des modèles robustes ; je donnerai des exemples d'applications réelles en gestion de la relation client et en analyse de qualité. Je conclurai en présentant quelques perspectives (utilisation du texte et des réseaux sociaux).	Françoise Fogelman-Soulié	http://editions-rnti.fr/render_pdf.php?p1&p=1000548	http://editions-rnti.fr/render_pdf.php?p=1000548	1037	fr	fr		industrialiser le dater mining : enjeu et perspectives  le informatique décisionnel être un secteur en fort croissance dans tout le entreprise . le technique classique ( reporting simple & Olap ) , qui clr intéresser essentiellement à présenter le donnée , être aujourd' hui très largement déployer . le dater mining commencer à clr répandre , apporter un capacité de prévision à fort valeur ajouter pour le entreprise le plus compétitif . ce développement être rendre possible par le disponibilité croissant de masse de donnée important et le puissance de calcul dorénavant disponible . cependant , le mise en IJuvre industriel des projet de data mining poser un contrainte tant théorique ( quel algorithme utiliser pour produire un modèle d' analyse exploiter un millier de variable pour un million d' exemple ) qu' opérationnel ( comment mettre en production et contrôler le bon fonctionnement de centaine de modèle ) . Je présenter ce contrainte issir des besoin des entreprise ; je montrer comment exploiter un résultat théorique ( provenir un travail de Vladimir Vapnik ) pour produire un modèle robuste ; je donner un exemple d' application réel en gestion de le relation client et en analyse de qualité . Je conclure en présenter quelque perspective ( utilisation du texte et des réseau social ) . 	Industrialiser le data mining : enjeux et perspectives	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Intégration de contraintes dans les cartes auto-organisatrices	Le travail présenté dans cet article décrit une nouvelle version des cartes topologiques que nous appelons CrTM. Cette version consiste à modifier l'algorithme de Kohonen de telle façon à ce qu'il contrôle les violations des contraintes lors de la construction de la topologie de la carte. Nous validons notre approche sur des données connues de la littérature en utilisant des contraintes artificielles. Une validation supplémentaire sera faite sur des données réelles issues d'images médicales pour la classification des mélanomes chez l'humain sous contraintes médicales.	Anouar Benhassena, Khalid Benabdeslem, Fazia Bellal, Alexandre Aussem, Bruno Canitia	http://editions-rnti.fr/render_pdf.php?p1&p=1000663	http://editions-rnti.fr/render_pdf.php?p=1000663	1038	fr	fr		intégration de contrainte dans le carte auto- organisatrices  le travail présenter dans ce article décrire un nouveau version des carte topologique que nous appeler CrTM . ce version consister à modifier le algorithme de Kohonen de tel façon à ce qu' il contrôler le violation des contrainte lors de le construction de le topologie de le carte . Nous valider son approche sur un donnée connaître de le littérature en utiliser un contrainte artificiel . un validation supplémentaire être faire sur un donnée réel issir d' image médical pour le classification des mélanome chez le humain sous contrainte médical . 	Intégration de contraintes dans les cartes auto-organisatrices	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Intégration de la structure dans un modèle probabiliste de document	En fouille de textes comme en recherche d'information, différents modèles, de type probabiliste, vectoriel ou booléen, se sont révélés bien adaptés pour représenter des documents textuels mais, ces modèles présentent l'inconvénient de ne pas tenir compte de la structure du document. Or la plupart des informations disponibles aujourd'hui sur Internet ou dans des bases documentaires sont fortement structurées. Dans cet article, nous proposons d'étendre le modèle probabiliste de représentation des documents de façon à tenir compte du poids d'une certaine catégorie d'éléments structurels : les balises représentant la structure logique et la structure de mise en forme. Ce modèle a été évalué à l'aide de la collection de la campagne d'évaluation INEX 2006.	Mathias Géry, Christine Largeron, Franck Thollard	http://editions-rnti.fr/render_pdf.php?p1&p=1000660	http://editions-rnti.fr/render_pdf.php?p=1000660	1039	fr	fr		intégration de le structure dans un modèle probabiliste de document  En fouille de texte comme en recherche d' information , différent modèle , de type probabiliste , vectoriel ou booléen , clr être révéler bien adapter pour représenter un document textuel mais , ce modèle présenter le inconvénient de ne pas tenir compte de le structure du document . Or le plupart des information disponible aujourd' hui sur Internet ou dans un base documentaire être fortement structurer . Dans ce article , nous proposer d' étendre le modèle probabiliste de représentation des document de façon à tenir compte du poids d' un certain catégorie d' élément structurel : le balise représenter le structure logique et le structure de mise en forme . ce modèle avoir être évaluer à le aide de le collection de le campagne d' évaluation INEX 2006 . 	Intégration de la structure dans un modèle probabiliste de document	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Interprétation automatique d'itinéraires à partir d'un corpus de récits de voyages pilotée par un usage pédagogique.	De larges corpus à fort ancrage territorial deviennent disponibles sousforme numérique dans les médiathèques et plus particulièrement dans les médiathèquesde dimension régionale. Les défis qu'offrent ces gigas octets de documentsbruts sont énormes en terme de traitement automatique des contenus.Nous proposons dans cet article deux modèles computationnels et une méthodecomplète permettant de réaliser un traitement automatique afin d'extraire des itinérairesdans des textes relatant des récits de voyage. Le premier modèle est unmodèle des attendus. Il s'intéresse au concept d'itinéraire et adopte le point devue du pédagogue et fait intervenir très tôt les usages envisagés. Le deuxièmemodèle est un modèle d'extraction, il permet de modéliser l'expression du déplacementdans des textes du genre récit de voyage. Nous proposons alors uneméthode automatique pour : d'une part extraire et interpréter automatiquementles déplacements d'un récit et d'autre part passer des déplacements à l'itinéraire,c'est-à-dire alimenter de manière automatique le modèle des attendus à partir dumodèle d'extraction. Nous montrons également comment les itinéraires extraitsinterviennent soit dans la phase de construction d'activités pédagogiques soitdirectement comme matériau dans une activité d'apprentissage. Nous présentonsenfin ¼R, un Prototype pour l'Interprétation d'Itinéraires dans des Récitsde voyages, qui implémente notre approche. Il prend en entrée un texte brut etfournit l'interprétation de l'itinéraire décrit dans le texte. Il permet également devisualiser sur un fond cartographique l'itinéraire extrait.	Pierre Loustau, Mauro Gaio, Thierry Nodenot	http://editions-rnti.fr/render_pdf.php?p1&p=1001237	http://editions-rnti.fr/render_pdf.php?p=1001237	1040	fr	fr	@univ-pau.fr, @univ-pau.fr	interprétation automatique d' itinéraire à partir d' un corpus de récit de voyage piloter par un usage pédagogique .  un large corpus à fort ancrage territorial devenir disponible sousforme numérique dans le médiathèque et plus particulièrement dans le médiathèquesde dimension régional . le défi qu' offrir ce giga-octet octet de documentsbruts être énorme en terme de traitement automatique des contenu . Nous proposer dans ce article deux modèle computationnels et un méthodecomplète permettre de réaliser un traitement automatique afin d' extraire un itinérairesdans des texte relater un récit de voyage . le premier modèle être unmodèle des attendu . Il clr intéresser au concept d' itinéraire et adopter le point devue du pédagogue et faire intervenir très tôt le usage envisager . le deuxièmemodèle être un modèle d' extraction , il permettre de modéliser le expression du déplacementdans des texte du genre récit de voyage . Nous proposer alors uneméthode automatique pour : d' un part extraire et interpréter automatiquementles déplacement d' un récit et d' autre part passer un déplacement à le itinéraire , c' est-à-dire alimenter de manière automatique le modèle des attendu à partir dumodèle d' extraction . Nous montrer également comment le itinéraire extraitsinterviennent soit dans le phase de construction d' activité pédagogique soitdirectement comme matériau dans un activité d' apprentissage . Nous présentonsenfin ¼R , un prototype pour le interprétation d' Itinéraires dans un Récitsde voyage , qui implémenter son approche . Il prendre en entrée un texte brut etfournit le interprétation de le itinéraire décrire dans le texte . Il permettre également devisualiser sur un fond cartographique le itinéraire extraire . 	Interprétation automatique d'itinéraires à partir d'un corpus de récits de voyages pilotée par un usage pédagogique.	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Interprétation d'images basée sur une approche évolutive guidée par une ontologie	Les approches de fouille et d'interprétation d'images consistant à considérer les pixels de façon indépendante ont montré leurs limites pour l'analyse d'images complexes. Pour résoudre ce problème, de nouvelles méthodes s'appuient sur une segmentation préalable de l'image qui consiste en une agrégation des pixels connexes afin de former des régions homogènes au sens d'un certain critère. Cependant le lien est souvent complexe entre la connaissance de l'expert sur les objets qu'il souhaite identifier dans l'image et les paramètres nécessaires à l'étape segmentation permettant de les identifier. Dans cet article la connaissance de l'expert est modélisée dans une ontologie qui est ensuite utilisée pour guider un processus de segmentation par une approche évolutive. Cette méthode trouve automatiquement des paramètres de segmentation permettant d'identifier les objets décrits par l'expert dans l'ontologie.	Germain Forestier, Sébastien Derivaux, Cédric Wemmert, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000635	http://editions-rnti.fr/render_pdf.php?p=1000635	1041	fr	fr		interprétation d' image baser sur un approche évolutif guider par un ontologie  le approche de fouille et d' interprétation d' image consister à considérer le pixel de façon indépendant avoir montrer son limite pour le analyse d' image complexe . Pour résoudre ce problème , un nouveau méthode clr appuyer sur un segmentation préalable de le image qui consister en un agrégation des pixel connexe afin de former un région homogène au sens d' un certain critère . cependant le lien être souvent complexe entre le connaissance de le expert sur le objet qu' il souhaiter identifier dans le image et le paramètre nécessaire à le étape segmentation permettre de les identifier . Dans ce article le connaissance de le expert être modéliser dans un ontologie qui être ensuite utiliser pour guider un processus de segmentation par un approche évolutif . ce méthode trouver automatiquement un paramètre de segmentation permettre d' identifier le objet décrire par le expert dans le ontologie . 	Interprétation d'images basée sur une approche évolutive guidée par une ontologie	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Khiops: outil de préparation et modélisation des données pour la fouille des grandes bases de données	Khiops est un outil de préparation des données et de modélisation pour l'apprentissage supervisé et non supervisé. L'outil permet d'évaluer de façon non paramétrique la corrélation entre tous types de variables dans le cas non supervisé et l'importance prédictive des variables et paires de variables dans le cas de la classification supervisée. Ces évaluations sont effectuées au moyen de modèles de discrétisation dans le cas numérique et de groupement de valeurs dans le cas catégoriel, ce qui permet de rechercher une représentation des données efficace au moyen d'un recodage des variables. L'outil produit également un modèle de scoring pour les tâches d'apprentissage supervisé, selon un classifieur Bayesien naif avec sélection de variables et moyennage de modèles. L'outil est adapté à l'analyse des grandes bases de données, avec des centaines de milliers d'individus et des dizaines de milliers de variables, et a permis de participer avec succès à plusieurs challenges internationaux récents.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000598	http://editions-rnti.fr/render_pdf.php?p=1000598	1042	fr	fr		Khiops : outil de préparation et modélisation des donnée pour le fouille des grand base de données  Khiops être un outil de préparation des donnée et de modélisation pour le apprentissage superviser et non superviser . le outil permettre d' évaluer de façon non paramétrique le corrélation entre tout type de variable dans le cas non superviser et le importance prédictif des variable et paire de variable dans le cas de le classification superviser . ce évaluation être effectuer au moyen de modèle de discrétisation dans le cas numérique et de groupement de valeur dans le cas catégoriel , ce qui permettre de rechercher un représentation des donnée efficace au moyen d' un recodage des variable . le outil produire également un modèle de scoring pour le tâche d' apprentissage superviser , selon un classifieur Bayesien naif avec sélection de variable et moyennage de modèle . le outil être adapter à le analyse des grand base de donnée , avec un centaine de millier d' individu et des dizaine de millier de variable , et avoir permettre de participer avec succès à plusieurs challenge international récent . 	Khiops: outil de préparation et modélisation des données pour la fouille des grandes bases de données	4
Revue des Nouvelles Technologies de l'Information	EGC	2008	La prise en compte de la dimension temporelle dans la classification de données	Dans un contexte d'ingénierie de la connaissance, l'analyse des données relationnelles évolutives est une question centrale. La représentation de ce type de données sous forme de graphe optimisé en facilite l'analyse et l'interprétation par l'utilisateur non expert. Cependant, ces graphes peuvent rapidement devenir trop complexes pour être étudiés dans leur globalité, il faut alors les décomposer de manière à en faciliter la lecture et l'analyse. Pour cela, une solution est de les simplifier, dans un premier temps, en un graphe réduit dont les sommets représentent chacun un groupe distinct de sommets : acteurs ou termes du domaine étudié. Dans un second temps, il faut les décomposer en instances (un graphe par période) afin de prendre en compte la dimension temporelle.La plateforme de veille stratégique Tétralogie, développée dans notre laboratoire, permet de synthétiser les données relationnelles évolutives sous forme de matrices de cooccurrence 3D et VisuGraph, son module de visualisation, permet de les représenter sous forme de graphes évolutifs.VisuGraph assimile les différentes périodes à des repères temporels et chaque sommet est placé en fonction de son degré d'appartenance aux différentes périodes. Ce prototype est aussi doté d'un module de la classification interactive de données relationnelles basé sur une technique de Markov Clustering, qui conduit à une visualisation sous forme de graphe réduit. Nous proposons ici de prendre en compte la dimension temporelle dans notre processus de classification des données. Ainsi, par la visualisation successive des différentes instances, il devient plus facile d'analyser l'évolution des classes au niveau intra mais aussi au niveau inter classes.	Eloïse Loubier, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1000653	http://editions-rnti.fr/render_pdf.php?p=1000653	1043	fr	fr		le prise en compte de le dimension temporel dans le classification de données  Dans un contexte d' ingénierie de le connaissance , le analyse des donnée relationnel évolutif être un question central . le représentation de ce type de donnée sous forme de graphe optimiser en faciliter le analyse et le interprétation par le utilisateur non expert . cependant , ce graphe pouvoir rapidement devenir trop complexe pour être étudier dans son globalité , il faillir alors les décomposer de manière à en faciliter le lecture et le analyse . Pour cela , un solution être de les simplifier , dans un premier temps , en un graphe réduire dont le sommet représenter chacun un groupe distinct de sommet : acteur ou terme du domaine étudier . Dans un second temps , il faillir les décomposer en instance ( un graphe par période ) afin de prendre en compte le dimension temporel . le plateforme de veille stratégique Tétralogie , développer dans son laboratoire , permettre de synthétiser le donnée relationnel évolutif sous forme de matrice de cooccurrence 3D et VisuGraph , son module de visualisation , permettre de les représenter sous forme de graphe évolutif . VisuGraph assimiler le différent période à un repère temporel et chaque sommet être placer en fonction de son degré d' appartenance aux différent période . ce prototype être aussi doter d' un module de le classification interactif de donnée relationnel baser sur un technique de Markov Clustering , qui conduire à un visualisation sous forme de graphe réduire . Nous proposer ici de prendre en compte le dimension temporel dans son processus de classification des donnée . ainsi , par le visualisation successif des différent instance , il devenir plus facile d' analyser le évolution des classe au niveau intra mais aussi au niveau inter classe . 	La prise en compte de la dimension temporelle dans la classification de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Le FIA: un nouvel automate permettant l'extraction efficace d'itemsets fréquents dans les flots de données	Le FIA (Frequent Itemset Automaton) est un nouvel automate qui permet de traiter de façon efficace la problématique de l'extraction des itemsets fréquents dans les flots de données. Cette structure de données est très compacte et informative, et elle présente également des propriétés incrémentales intéressantes pour les mises à jour avec une granularité très fine. L'algorithme développé pour la mise à jour du FIA effectue un unique passage sur les données qui sont prises en compte tout d'abord par batch (i.e., itemset par itemset), puis pour chaque itemset, item par item. Nous montrons que dans le cadre d'une approche prédictive et par l'intermédiaire de la bordure statistique, le FIA permet d'indexer les itemsets véritablement fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur la pertinence statistique des itemsets indexés avec la P-valeur.	Jean-Emile Symphor, Alban Mancheron, Lionel Vinceslas, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000568	http://editions-rnti.fr/render_pdf.php?p=1000568	1044	fr	fr		le FIA : un nouveau automate permettre le extraction efficace d' itemsets fréquent dans le flot de données  le FIA ( Frequent Itemset Automaton ) être un nouveau automate qui permettre de traiter de façon efficace le problématique de le extraction des itemsets fréquent dans le flot de donnée . ce structure de donnée être très compact et informatif , et elle présenter également un propriété incrémentales intéressant pour le mise à jour avec un granularité très fin . le algorithme développer pour le mise à jour du FIA effectuer un unique passage sur le donnée qui être prendre en compte tout d' abord par batch ( i.e. , itemset par itemset ) , puis pour chaque itemset , item par item . Nous montrer que dans le cadre d' un approche prédictif et par le intermédiaire de le bordure statistique , le FIA permettre d' indexer le itemsets véritablement fréquent du flot en maximiser le rappel et en fournir à tout moment un information sur le pertinence statistique des itemsets indexer avec le P-valeur . 	Le FIA: un nouvel automate permettant l'extraction efficace d'itemsets fréquents dans les flots de données	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Le forage de réseaux sociaux	L'exploitation des réseaux sociaux pour l'extraction de connaissances n'est pas nouvelle. Les anthropologues, sociologues et épidémiologies se sont déjà penchés sur la question. C'est probablement le succès du moteur de recherche Google qui a vulgarisé l'utilisation des parcours aléatoires des réseaux sociaux pour l'ordonnancement par pertinence. Plusieurs applications ont depuis vu naissance. La découverte des communautés dans les réseaux sociaux est aussi une nouvelle tendance de recherche très prisée. Durant cet exposé nous parlerons de l'analyse des réseaux sociaux, la découverte de communautés, et présenterons quelques applications dont l'ordonnancement dans les bases de données	Osmar R. Zaïane	http://editions-rnti.fr/render_pdf.php?p1&p=1000549	http://editions-rnti.fr/render_pdf.php?p=1000549	1045	fr	fr		le forage de réseau sociaux  le exploitation des réseau social pour le extraction de connaissance n' être pas nouveau . le anthropologue , sociologue et épidémiologie clr être déjà pencher sur le question . C' être probablement le succès du moteur de recherche Google qui avoir vulgariser le utilisation des parcours aléatoire des réseau social pour le ordonnancement par pertinence . plusieurs application avoir depuis voir naissance . le découverte des communauté dans le réseau social être aussi un nouveau tendance de recherche très priser . Durant ce exposé nous parler de le analyse des réseau social , le découverte de communauté , et présenter quelque application dont le ordonnancement dans le base de données 	Le forage de réseaux sociaux	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Le logiciel SODAS : avancées récentes Un outil pour analyser et visualiser des données symboliques		Myriam Touati, Mohamed Rahal, Filipe Afonso, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000607	http://editions-rnti.fr/render_pdf.php?p=1000607	1046	fr			Le logiciel SODAS : avancées récentes Un outil pour analyser et visualiser des données symboliques 	Le logiciel SODAS : avancées récentes Un outil pour analyser et visualiser des données symboliques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Les cartes cognitives hiérarchiques	Une carte cognitive fournit une représentation graphique d'un réseau d'influence entre des concepts. Les cartes cognitives de dimensions importantes ont l'inconvénient d'être difficiles à appréhender, interpréter et exploiter. Cet article présente un modèle de cartes cognitives hiérarchiques permettant au concepteur d'effectuer des regroupements de concepts qui sont ensuite utilisés dans un mécanisme permettant à l'utilisateur d'obtenir des vues partielles et synthétiques d'une carte.	Lionel Chauvin, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000560	http://editions-rnti.fr/render_pdf.php?p=1000560	1047	fr	fr		le carte cognitif hiérarchiques  un carte cognitif fournir un représentation graphique d' un réseau d' influence entre un concept . le carte cognitif de dimension important avoir le inconvénient d' être difficile à appréhender , interpréter et exploiter . ce article présenter un modèle de carte cognitif hiérarchique permettre au concepteur d' effectuer un regroupement de concept qui être ensuite utiliser dans un mécanisme permettre à le utilisateur d' obtenir un vue partiel et synthétique d' un carte . 	Les cartes cognitives hiérarchiques	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	L'intelligence collective géospatiale au service du diagnostic de territoire : GEOdoc	Le diagnostic de territoire constitue une étape obligatoire dans toutprojet d'aménagement ou dans toute volonté politique de modifier durablementl'espace. Les décideurs politiques doivent avoir une vision objective des actionsà mener en fondant leurs réflexions sur des études et des documents ;qu'ils soient à caractère géographique ou non. Il est donc fondamentald'améliorer l'accès et la consultation, par les décideurs stratégiques, de ce quel'on peut appeler des documents géographiques. Le but de cet article est deprésenter certains concepts et solutions technologiques qui peuvent être utilisésafin de mieux organiser, de naviguer (dans) et de visualiser ces documents. Ilpropose une mise en perspective commune de certaines de ces approches, surlaquelle est fondée la conception d'une première maquette d'un outil de visualisation(et de navigation) de documents géographiques nommé GEOdoc.	Stéphane Roche, Benoit Kiene, Claude Caron	http://editions-rnti.fr/render_pdf.php?p1&p=1001231	http://editions-rnti.fr/render_pdf.php?p=1001231	1048	fr	fr	@scg.ulaval.ca, @ign.fr, @usherbrooke.ca	le intelligence collectif géospatiale au service du diagnostic de territoire : GEOdoc  le diagnostic de territoire constituer un étape obligatoire dans toutprojet d' aménagement ou dans tout volonté politique de modifier durablementl'espace . le décideur politique devoir avoir un vision objectif des actionsà mener en fonder son réflexion sur un étude et des document ; qu' ils être à caractère géographique ou non . Il être donc fondamentald'améliorer le accès et le consultation , par le décideur stratégique , de ce quel'on pouvoir appeler un document géographique . le but de ce article être deprésenter certain concept et solution technologique qui pouvoir être utilisésafin de mieux organiser , de naviguer ( dans ) et de visualiser ce document . Ilpropose un mise en perspective commun de certains de ce approche , surlaquelle être fonder le conception d' un premier maquette d' un outil de visualisation ( et de navigation ) de document géographique nommer GEOdoc . 	L'intelligence collective géospatiale au service du diagnostic de territoire : GEOdoc	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Mesures hiérarchiques pondérées pour l'évaluation d'un système semi-automatique d'annotation de génomes utilisant des arbres de décision	L'annotation d'une protéine consiste, entre autres, à lui attribuer une classe dans une hiérarchie fonctionnelle. Celle-ci permet d'organiser les connaissances biologiques et d'utiliser un vocabulaire contrôlé. Pour estimer la pertinence des annotations, des mesures telles que la précision, le rappel, la spécificité et le Fscore sont utilisées. Cependant ces mesures ne sont pas toujours bien adaptées à l'évaluation de données hiérarchiques, car elles ne permettent pas de distinguer les erreurs faites aux différents niveaux de la hiérarchie. Nous proposons ici une représentation formelle pour les différents types d'erreurs adaptés à notre problème.	Lucie Gentils, Jérôme Azé, Claire Toffano-Nioche, Valentin Loux, Anne Poupon, Jean-François Gibrat, Christine Froidevaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000565	http://editions-rnti.fr/render_pdf.php?p=1000565	1049	fr	fr		mesure hiérarchique pondérer pour le évaluation d' un système semi-automatique d' annotation de génome utiliser un arbre de décision  le annotation d' un protéine consister , entre autre , à lui attribuer un classe dans un hiérarchie fonctionnel . Celle _-ci permettre d' organiser le connaissance biologique et d' utiliser un vocabulaire contrôler . Pour estimer le pertinence des annotation , un mesure tel que le précision , le rappel , le spécificité et le Fscore être utiliser . cependant ce mesure ne être pas toujours bien adapter à le évaluation de donnée hiérarchique , car elles ne permettre pas de distinguer le erreur faire aux différent niveau de le hiérarchie . Nous proposer ici un représentation formel pour le différent type d' erreur adapter à son problème . 	Mesures hiérarchiques pondérées pour l'évaluation d'un système semi-automatique d'annotation de génomes utilisant des arbres de décision	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Méthodologie de définition de e-services pour la gestion des connaissances à partir d'un plateau de créativité : application au e-learning instrumental	En s'appuyant sur la théorie de l'activité, nous avons mis au point une méthodologie de gestion des connaissances à base de e-services sur un plateau de créativité visant à faire piloter le processus de fabrication métier par celui des usages. Nous l'avons testé avec la réalisation d'un e-service d'apprentissage instrumental de pièces de musique à la guitare (E-guitare).	David Grosser, Noël Conruyt, Olivier Sebastien	http://editions-rnti.fr/render_pdf.php?p1&p=1000591	http://editions-rnti.fr/render_pdf.php?p=1000591	1050	fr	fr		méthodologie de définition de e-services pour le gestion des connaissance à partir d' un plateau de créativité : application au e-learning instrumental  En clr appuyer sur le théorie de le activité , nous avoir mettre au point un méthodologie de gestion des connaissance à base de e-services sur un plateau de créativité viser à faire piloter le processus de fabrication métier par celui des usage . Nous l' avoir tester avec le réalisation d' un e-service d' apprentissage instrumental de pièce de musique à le guitare ( E-guitare ) . 	Méthodologie de définition de e-services pour la gestion des connaissances à partir d'un plateau de créativité : application au e-learning instrumental	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Méthodologie d'Evaluation Intelligente des Concepts Ontologiques	"Un des problèmes majeurs dans la gestion des ontologies est son évaluation. Cet article traite l'évaluation des concepts ontologiques qui sont extraits de pages Web. Pour cela, nous avons proposé une méthodologie d'évaluation des concepts basée trois critères révélateurs : ""le degré de crédibilité""; ""le degré de cohésion"" et ""le degré d'éligibilité"". Chaque critère correspond à un apport de connaissance pour la tâche d'évaluation. Notre méthode d'évaluation assure une évaluation qualitative grâce aux associations de mots ainsi qu'une évaluation quantitative par le biais des trois degrés. Nos résultats et discussions avec les experts et les utilisateurs ont montré que notre méthode facilite la tâche d'évaluation."	Lobna Karoui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000566	http://editions-rnti.fr/render_pdf.php?p=1000566	1051	fr	fr		méthodologie d' Evaluation intelligent des concept Ontologiques  " un des problème majeur dans le gestion des ontologie être son évaluation . . ce article traire le évaluation des concept ontologique qui être extrait de page Web . . Pour cela , nous avoir proposer un méthodologie d' évaluation des concept baser trois critère révélateur : " " le degré de crédibilité " " ; ; " " le degré de cohésion " " et " " le degré d' éligibilité " " . . chaque critère correspondre à un apport de connaissance pour le tâche d' évaluation . . son méthode d' évaluation assurer un évaluation qualitatif grâce aux association de mot ainsi qu' un évaluation quantitatif par le biais des trois degré . . son résultat et discussion avec le expert et le utilisateur avoir montrer que son méthode faciliter le tâche d' évaluation . " 	Méthodologie d'Evaluation Intelligente des Concepts Ontologiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Mining Implications from Lattices of Closed Trees	We propose a way of extracting high-confidence association rules from datasets consisting of unlabeled trees. The antecedents are obtained through a computation akin to a hypergraph transversal, whereas the consequents follow from an application of the closure operators on unlabeled trees developed in previous recent works of the authors. We discuss in more detail the case of rules that always hold, independently of the dataset, since these are more complex than in itemsets due to the fact that we are no longer working on a lattice.	José L. Balcázar, Albert Bifet, Antoni Lozano	http://editions-rnti.fr/render_pdf.php?p1&p=1000625	http://editions-rnti.fr/render_pdf.php?p=1000625	1052	en	en		mining implication lattice close tree propose way extract high confidence association rule dataset consist unlabeled tree antecedent obtained computation akin hypergraph transversal whereas consequent follow application closure operator unlabeled tree develop previous recent work author discuss detail case rule always hold independently dataset since complex itemset due fact longer working lattice	Mining Implications from Lattices of Closed Trees	6
Revue des Nouvelles Technologies de l'Information	EGC	2008	Modélisation conceptuelle des trajectoires	Une perception intelligente du mouvement d'objets mobiles(personnes, voitures, colis, etc.) est à la base de nombreuses applications (parexemple le suivi d'une distribution postale à travers le monde, l'optimisation dutrafic routier ou l'étude de la migration d'animaux). Les systèmes de gestion debases de données actuels n'offrent ni les concepts ni les fonctions nécessaires àune analyse sémantique du mouvement, se limitant au stockage et àl'interrogation de positions spatiales individuelles, hors contexte temporel. Destravaux de recherche précédents ont introduit et développé le concept d'objetmobile ou spatio-temporel. Dans cet article nous allons plus loin en proposantle concept de trajectoire comme unité sémantique de mouvement sur laquellese construit la vision applicative. Nous proposons de décrire les trajectoires, auniveau conceptuel, avec leurs aspects géométriques, temporels et sémantiqueset leurs composants structurels : point de départ, point d'arrivée, arrêts etdéplacements intermédiaires. Chaque élément, trajectoire, arrêt, déplacement,voire partie de déplacement, peut recevoir des annotations sémantiques sousforme de valeurs d'attributs ou de liens vers des objets de la base. L'approchede modélisation décrite dans cet article est basée sur les patrons demodélisation, qui permettent une solution générique pour modéliser lescaractéristiques standard des trajectoires tout en étant ouverte auxcaractéristiques spécifiques à l'application envisagée. Enfin, l'implémentationdans une base de données relationnelle étendue est présentée.	Christine Parent, Stefano Spaccapietra, Christelle Vangenot, Maria-Luisa Damiani, José de Macedo, Fabio Porto	http://editions-rnti.fr/render_pdf.php?p1&p=1001236	http://editions-rnti.fr/render_pdf.php?p=1001236	1053	fr	fr	@unil.ch, @epfl.ch, @dico.unimi.it	modélisation conceptuel des trajectoires  un perception intelligent du mouvement d' objet mobile ( personne , voiture , colis , etc. ) être à le base de nombreux application ( parexemple le suivi d' un distribution postal à travers le monde , le optimisation dutrafic routier ou le étude de le migration d' animal ) . le système de gestion debases de donnée actuel n' offrir ni le concept ni le fonction nécessaire àune analyse sémantique du mouvement , clr limiter au stockage et àl'interrogation de position spatial individuel , hors contexte temporel . Destravaux de recherche précédent avoir introduire et développer le concept d' objetmobile ou spatio- temporel . Dans ce article nous aller plus loin en proposantle concept de trajectoire comme unité sémantique de mouvement sur laquellese construire le vision applicatif . Nous proposer de décrire le trajectoire , auniveau conceptuel , avec son aspect géométrique , temporel et sémantiqueset son composant structurel : point de départ , point d' arrivé , arrêt etdéplacements intermédiaire . chaque élément , trajectoire , arrêt , déplacement , voire partie de déplacement , pouvoir recevoir un annotation sémantique sousforme de valeur d' attribut ou de lien vers un objet de le base . le approchede modélisation décrire dans ce article être baser sur le patron demodélisation , qui permettre un solution générique pour modéliser lescaractéristiques standard des trajectoire tout en être ouvrir auxcaractéristiques spécifique à le application envisager . enfin , le implémentationdans un base de donnée relationnel étendre être présenter . 	Modélisation conceptuelle des trajectoires	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Nouvelle approche pour la recherche d'images par le contenu	On utilise l'analyse factorielle des correspondances (AFC) pour la recherche d'images par le contenu en s'inspirant directement de son utilisation en analyse des données textuelles (ADT). L'AFC permet ici de réduire les dimensions du problème et de sélectionner des indicateurs pertinents pour la recherche par le contenu. En ADT, l'AFC est appliquée à un tableau de contingence croisant mots et documents. La première étape consiste donc à définir des « mots visuels » dans les images (analogue des mots dans les textes). Ces mots sont construits à partir des descripteurs locaux (SIFT) des images. La méthode a été testée sur la base Caltech4 (Sivic et al., 2005) sur laquelle elle fournit de meilleurs résultats (qualité des résultats de recherche et temps d'exécution) que des méthodes plus classiques comme TF*IDF/Rocchio (Rocchio, 1971) ou pLSA (Hofmann, 1999a, 1999b). Enfin, pour passer à l'échelle et améliorer la qualité de recherche, nous proposons un nouveau prototype de recherche qui utilise des fichiers inversés basés sur la qualité de représentation des images sur les axes après avoir fait une AFC. Chaque fichier inversé est associé à une partie d'un axe (positive ou négative) et contient des images ayant une bonne qualité de représentation sur cet axe. Les tests réalisés montrent que ce nouveau prototype réduit le temps de recherche sans perte de qualité de résultat et dans certains cas, améliore le taux de précision par rapport à la méthode exhaustive.	Nguyen-Khang Pham, Annie Morin	http://editions-rnti.fr/render_pdf.php?p1&p=1000636	http://editions-rnti.fr/render_pdf.php?p=1000636	1054	fr	fr		nouveau approche pour le recherche d' image par le contenu  On utiliser le analyse factoriel des correspondance ( AFC ) pour le recherche d' image par le contenu en clr inspirer directement de son utilisation en analyse des donnée textuel ( ADT ) . le AFC permettre ici de réduire le dimension du problème et de sélectionner un indicateur pertinent pour le recherche par le contenu . En ADT , le AFC être appliquer à un tableau de contingence croiser mot et document . le premier étape consister donc à définir un « mot visuel » dans le image ( analogue des mot dans le texte ) . ce mot être construire à partir un descripteur local ( SIFT ) un image . le méthode avoir être tester sur le base Caltech4 ( Sivic et al. , 2005 ) sur laquelle elle fournir de meilleur résultat ( qualité des résultat de recherche et temps d' exécution ) que un méthode plus classique comme TF * IDF  Rocchio ( Rocchio , 1971 ) ou pLSA ( Hofmann , 1999a , 1999b ) . enfin , pour passer à le échelle et améliorer le qualité de recherche , nous proposer un nouveau prototype de recherche qui utiliser un fichier inverser baser sur le qualité de représentation des image sur le axe après avoir faire un AFC . chaque fichier inverser être associer à un partie d' un axe ( positif ou négatif ) et contenir un image avoir un bon qualité de représentation sur ce axe . le test réaliser montrer que ce nouveau prototype réduire le temps de recherche sans perte de qualité de résultat et dans certain cas , améliorer le taux de précision par rapport à le méthode exhaustif . 	Nouvelle approche pour la recherche d'images par le contenu	4
Revue des Nouvelles Technologies de l'Information	EGC	2008	Ontologies et raisonnement à partir de cas : Application à l'analyse des risques industriels	L'analyse de risques est un processus visant à décrire les scénarios conduisant à des phénomènes dangereux et à des accidents potentiels sur une installation industrielle. Pour réaliser une analyse de risques, un expert dispose de nombreuses ressources : rapports, études de dangers, bases d'accidents, etc. Ces ressources sont cependant souvent difficiles à exploiter parce qu'elles ne sont pas suffisamment structurées ni formalisées. Dans le cadre du projet KMGR (Knowledge Management pour la Gestion des Risques), mené en partenariat avec l'Institut National de l'Environnement industriel et des RISques (INERIS), nous proposons de traiter ce problème en développant un système de recherche d'information basé sur des ontologies, et de le compléter par un système de raisonnement à partir de cas (RàPC) pour tenir compte des expériences passées.	Amjad Abou Assali, Dominique Lenne, Bruno Debray	http://editions-rnti.fr/render_pdf.php?p1&p=1000595	http://editions-rnti.fr/render_pdf.php?p=1000595	1055	fr	fr		ontologie et raisonnement à partir de cas : application à le analyse des risque industriels  le analyse de risque être un processus viser à décrire le scénario conduire à un phénomène dangereux et à un accident potentiel sur un installation industriel . Pour réaliser un analyse de risque , un expert disposer un nombreux ressource : rapport , étude de danger , base d' accident , etc. ce ressource être cependant souvent difficile à exploiter parce qu' elles ne être pas suffisamment structurer ni formaliser . Dans le cadre du projet KMGR ( Knowledge Management pour le gestion des risque ) , mener en partenariat avec le institut national de le environnement industriel et des risque ( INERIS ) , nous proposer de traiter ce problème en développer un système de recherche d' information baser sur un ontologie , et de le compléter par un système de raisonnement à partir de cas ( RàPC ) pour tenir compte des expérience passer . 	Ontologies et raisonnement à partir de cas : Application à l'analyse des risques industriels	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Optimisation du Primal pour les SVM	L'apprentissage de SVM par optimisation directe du primal est très étudié depuis quelques temps car il ouvre de nouvelles perspectives notamment pour le traitement de données structurées. Nous proposons un nouvel algorithme de ce type qui combine de façon originale un certain nombre de techniques et idées comme la méthode du sous-gradient, l'optimisation de fonctions continues non partout différentiables, et une heuristique de shrinking.	Trinh Minh Tri Do, Thierry Artières	http://editions-rnti.fr/render_pdf.php?p1&p=1000615	http://editions-rnti.fr/render_pdf.php?p=1000615	1056	fr	fr		optimisation du Primal pour le SVM  le apprentissage de SVM par optimisation direct du primal être très étudier depuis quelque temps car il ouvrer un nouveau perspective notamment pour le traitement de donnée structurer . Nous proposer un nouveau algorithme de ce type qui combiner de façon original un certain nombre de technique et idée comme le méthode du sous-gradient , le optimisation de fonction continu non partout différentiables , et un heuristique de shrinking . 	Optimisation du Primal pour les SVM	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Optimisation incrémentale de réseaux de neurones RBF pour la régression via un algorithme évolutionnaire : RBF-Gene	Les réseaux de neurones RBF sont d'excellents régresseurs. Ils sont cependant difficiles à utiliser en raison du nombre de paramètres libres : nombre de neurones, poids des connexions, ... Des algorithmes évolutionnaires permettent de les optimiser mais ils sont peu nombreux et complexes.Nous proposons ici un nouvel algorithme, RBF-Gene, qui permet d'optimiser la structure et les poids du réseau, grâce à une inspiration biologique. Il est compétitif avec les autres techniques de régression mais surtout l'évolution peut choisir dynamiquement le nombre de neurones et la précision des différents paramètres.	Virginie Lefort, Guillaume Beslon	http://editions-rnti.fr/render_pdf.php?p1&p=1000620	http://editions-rnti.fr/render_pdf.php?p=1000620	1057	fr	fr		optimisation incrémentale de réseau de neurone RBF pour le régression via un algorithme évolutionnaire : RBF-Gene  le réseau de neurone RBF être d' excellent régresseurs . Ils être cependant difficile à utiliser en raison du nombre de paramètre libre : nombre de neurone , poids des connexion , ... un algorithme évolutionnaires permettre de les optimiser mais ils être peu nombreux et complexe . Nous proposer ici un nouveau algorithme , RBF-Gene , qui permettre d' optimiser le structure et le poids du réseau , grâce à un inspiration biologique . Il être compétitif avec le autre technique de régression mais surtout le évolution pouvoir choisir dynamiquement le nombre de neurone et le précision des différent paramètre . 	Optimisation incrémentale de réseaux de neurones RBF pour la régression via un algorithme évolutionnaire : RBF-Gene	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Pondération locale des variables en apprentissage numérique non-supervisé	Dans cet article, nous proposons une nouvelle approche de pondérations des variables durant un processus d'apprentissage non supervisé. Cette méthode se base sur l'algorithme « batch » des cartes auto-organisatrices. L'estimation des coefficients de pondération se fait en parallèle avec la classification automatique. Ces pondérations sont locales et associées à chaque référent de la carte auto-organisatrice. Elles reflètent l'importance locale de chaque variable pour la classification. Les pondérations locales sont utilisées pour la segmentation de la carte topologique permettant ainsi un découpage plus riche tenant compte des pertinences des variables. Les résultats de l'évaluation montrent que l'approche proposée, comparée à d'autres méthodes de classification, offre une segmentation plus fine de la carte et de meilleure qualité.	Nistor Grozavu, Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000619	http://editions-rnti.fr/render_pdf.php?p=1000619	1058	fr	fr		pondération local des variable en apprentissage numérique non- supervisé  Dans ce article , nous proposer un nouveau approche de pondération des variable durant un processus d' apprentissage non superviser . ce méthode clr baser sur le algorithme « batch » un carte auto- organisatrice . le estimation des coefficient de pondération clr faire en parallèle avec le classification automatique . ce pondération être local et associer à chaque référent de le carte auto- organisatrice . Elles refléter le importance local de chaque variable pour le classification . le pondération local être utiliser pour le segmentation de le carte topologique permettre ainsi un découpage plus riche tenir compte des pertinence des variable . le résultat de le évaluation montrer que le approche proposer , comparer à un autre méthode de classification , offrir un segmentation plus fin de le carte et de meilleur qualité . 	Pondération locale des variables en apprentissage numérique non-supervisé	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Prétraitement des bases de données de réactions chimiques pour la fouille de schémas de réactions	Un grand nombre de réactions chimiques sont aujourd'hui répertoriées dans des bases de données. Les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents. Deux obstacles s'opposent à cela : d'une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes d'extraire les schémas de réactions fréquents. D'autre part les bases de données contiennent des descriptions de réactions souvent incomplètes, ambiguës ou erronées. Le présent article décrit un processus de prétraitement opérationnel qui permet de filtrer, compléter puis transformer le contenu d'une base de réactions en des données fiables constituées de graphes abstraits répondant au problème de la fouille de schémas de réactions. Le processus place ainsi les bases de réactions à portée des techniques de fouille de graphes comme en attestent les résultats expérimentaux.	Frédéric Pennerath, Géraldine Polaillon, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000652	http://editions-rnti.fr/render_pdf.php?p=1000652	1059	fr	fr		Prétraitement un base de donnée de réaction chimique pour le fouille de schéma de réactions  un grand nombre de réaction chimique être aujourd' hui répertorier dans un base de donnée . le chimiste aimer pouvoir fouiller le graphe moléculaire contenir dans ce donnée pour en extraire un schéma de réaction fréquent . Deux obstacle clr opposer à cela : d' un part le manière dont le chimiste représenter le réaction par un graphe ne permettre pas aux technique de fouille de graphe d' extraire le schéma de réaction fréquent . D' autre part le base de donnée contenir un description de réaction souvent incomplet , ambigu ou erroné . le présent article décrire un processus de prétraitement opérationnel qui permettre de filtrer , compléter puis transformer le contenu d' un base de réaction en un donnée fiable constituer de graphe abstraire répondre au problème de le fouille de schéma de réaction . le processus placer ainsi le base de réaction à portée des technique de fouille de graphe comme en attester le résultat expérimental . 	Prétraitement des bases de données de réactions chimiques pour la fouille de schémas de réactions	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Principes d'Analyse des données symboliques et application à la détection d'anomalies sur des ouvrages publics	L'analyse des données Symboliques a pour objectif de fournir des résultatscomplémentaires à ceux fournis par la fouille de données classique encréant des concepts issus de données simples ou complexes puis en analysantces concepts par des descriptions symboliques où les variables expriment lavariation des instances de ces concepts en prenant des valeurs intervalle, histogramme,suites, munies de règles et de taxonomies, etc.	Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001800	http://editions-rnti.fr/render_pdf.php?p=1001800	1060	fr	fr		principe d' analyse des donnée symbolique et application à le détection d' anomalie sur un ouvrage publics  le analyse des donnée Symboliques avoir pour objectif de fournir un résultatscomplémentaires à celui fournir par le fouille de donnée classique encréant des concept issu de donnée simple ou complexe puis en analysantces concept par un description symbolique où le variable exprimer lavariation un instance de ce concept en prendre des valeur intervalle , histogramme , suite , munir de règle et de taxonomie , etc . 	Principes d'Analyse des données symboliques et application à la détection d'anomalies sur des ouvrages publics	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Processus d'acquisition d'un dictionnaire de sigles et de leurs définitions à partir d'un corpus	Le logiciel présenté dans cet article s'appuie sur une approche d'acquisition de sigles à partir de données textuelles	Vladislav Matviico, Nicolas Muret, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000599	http://editions-rnti.fr/render_pdf.php?p=1000599	1061	fr	fr		processus d' acquisition d' un dictionnaire de sigle et de son définition à partir d' un corpus  le logiciel présenter dans ce article clr appuyer sur un approche d' acquisition de sigle à partir de donnée textuelles 	Processus d'acquisition d'un dictionnaire de sigles et de leurs définitions à partir d'un corpus	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Proposition d'une nouvelle approche de détection d'intrusions basée sur les règles associatives génériques de classification	Les systèmes de détection d'intrusions (SDIs) ont pour objectif la sécurité des réseaux informatiques. Dans ce papier, nous proposons une nouvelle approche de détection d'intrusions basée sur des règles associatives génériques de classification pour améliorer la qualité de la détection d'intrusions.	Imen Brahmi, Sadok Ben Yahia, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1000589	http://editions-rnti.fr/render_pdf.php?p=1000589	1062	fr	fr		proposition d' un nouveau approche de détection d' intrusion baser sur le règle associatif générique de classification  le système de détection d' intrusion ( SDIs ) avoir pour objectif le sécurité des réseau informatique . Dans ce papier , nous proposer un nouveau approche de détection d' intrusion baser sur un règle associatif générique de classification pour améliorer le qualité de le détection d' intrusion . 	Proposition d'une nouvelle approche de détection d'intrusions basée sur les règles associatives génériques de classification	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Proposition pour l'intégration de l'analyse spatiale et de l'analyse multidimensionnelle	L'introduction de l'information spatiale dans les modèlesmultidimensionnels a donné naissance au concept de Spatial OLAP (SOLAP).Dans cet article, nous montrons en quoi les spécificités de l'informationgéographique et de l'analyse spatiale ne sont pas entièrement prises en comptedans l'analyse et les modèles multidimensionnels SOLAP. Pour pallier ceslimites, nous proposons le concept de dimension géographique et décrivons lesdifférents types de hiérarchies associées. Nous proposons l'introduction denouveaux opérateurs qui permettent d'adapter les opérateurs d'analyse spatialeau paradigme multidimensionnel. Enfin, nous présentons notre prototype quioffre une interface web de navigation spatiale et multidimensionnelle, etpermet l'intégration de ces nouveaux concepts.	Sandro Bimonte,  Anne Tchounikine, Maryvonne Miquel, Robert Laurini	http://editions-rnti.fr/render_pdf.php?p1&p=1001235	http://editions-rnti.fr/render_pdf.php?p=1001235	1063	fr	fr	@imag.fr, @insa-lyon.fr	proposition pour le intégration de le analyse spatial et de le analyse multidimensionnelle  le introduction de le information spatial dans le modèlesmultidimensionnels avoir donner naissance au concept de Spatial OLAP ( SOLAP ) . Dans ce article , nous montrer en quoi le spécificité de le informationgéographique et de le analyse spatial ne être pas entièrement prendre en comptedans le analyse et le modèle multidimensionnel SOLAP . Pour pallier ceslimites , nous proposer le concept de dimension géographique et décrire lesdifférents type de hiérarchie associer . Nous proposer le introduction denouveaux opérateur qui permettre d' adapter le opérateur d' analyse spatialeau paradigme multidimensionnel . enfin , nous présenter son prototype quioffre un interface web de navigation spatial et multidimensionnel , etpermet le intégration de ce nouveau concept . 	Proposition pour l'intégration de l'analyse spatiale et de l'analyse multidimensionnelle	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Recherche adaptative de structures de régulation génétique	Nous avons proposé un algorithme original de Fouille de Données, LICORN, afin d'inférer des relations de régulation coopérative à partir de données d'expression. LICORN donne de bons résultats s'il est appliqué à des données de levure, mais le passage à l'échelle sur des données plus complexes (e.g., humaines) est difficile. Dans cet article, nous proposons une extension de LICORN afin qu'il puisse gérer une contrainte de co-régulation adaptative. Une évaluation préliminaire sur des données de transcriptome de tumeurs de vessie montre que les réseaux significatifs sont obtenus à l'aide d'une contrainte de corégulation adaptative de manière beaucoup plus efficace, et qu'ils ont des performances de prédiction équivalentes voire meilleures que celles obtenues par LICORN.	Mohamed Elati, Céline Rouveirol	http://editions-rnti.fr/render_pdf.php?p1&p=1000631	http://editions-rnti.fr/render_pdf.php?p=1000631	1064	fr	fr		recherche adaptatif de structure de régulation génétique  Nous avoir proposer un algorithme original de fouille de Données , LICORN , afin d' inférer un relation de régulation coopératif à partir de donnée d' expression . LICORN donne de bon résultat s' il être appliquer à un donnée de levure , mais le passage à le échelle sur un donnée plus complexe ( exempli gratia , humain ) être difficile . Dans ce article , nous proposer un extension de LICORN afin qu' il pouvoir gérer un contrainte de co- régulation adaptatif . un évaluation préliminaire sur un donnée de transcriptome de tumeur de vessie montrer que le réseau significatif être obtenir à le aide d' un contrainte de corégulation adaptatif de manière beaucoup plus efficace , et qu' ils avoir un performance de prédiction équivalent voire meilleur que celui obtenir par LICORN . 	Recherche adaptative de structures de régulation génétique	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Recherche de motifs spatio-temporels de cas atypiques pour le trafic routier urbain	Un large panel de domaines d'application utilise des réseaux de capteurs géoréférencés pour mesurer divers évènements. Les séries temporelles fournies par ces réseaux peuvent être utilisées dans le but de dégager des connaissances sur les relations spatio-temporelles de l'activité mesurée. Dans cet article, nous proposons une méthode permettant d'abord de détecter des situations atypiques (au sens de l'occurrence) puis de construire des motifs spatio-temporels relatant leur propagation sur un réseau. Le cas étudié est celui du trafic routier urbain. Notre raisonnement se fonde sur l'application de la méthode Space-Time Principal Component Analysis (STPCA) et de la combinaison entre l'information mutuelle et l'algorithme Isomap. Les résultats expérimentaux exécutés sur des données réelles de trafic routier démontrent l'efficacité de la méthode introduite à identifier la propagation de cas atypiques fournissant ainsi un outil performant de prédiction de la circulation intraday à court et moyen terme.	Marc Joliveau, Florian De Vuyst	http://editions-rnti.fr/render_pdf.php?p1&p=1000640	http://editions-rnti.fr/render_pdf.php?p=1000640	1065	fr	fr		recherche de motif spatio- temporel de cas atypique pour le trafic routier urbain  un large panel de domaine d' application utiliser un réseau de capteur géoréférencés pour mesurer divers évènements . le série temporel fournir par ce réseau pouvoir être utiliser dans le but de dégager un connaissance sur le relation spatio- temporel de le activité mesurer . Dans ce article , nous proposer un méthode permettre d' abord de détecter un situation atypique ( au sens de le occurrence ) puis de construire un motif spatio- temporel relater son propagation sur un réseau . le cas étudier être celui du trafic routier urbain . son raisonnement clr fonder sur le application de le méthode Space-Time principal Component Analysis ( STPCA ) et de le combinaison entre le information mutuel et le algorithme Isomap . le résultat expérimental exécuter sur un donnée réel de trafic routier démontrer le efficacité de le méthode introduire à identifier le propagation de cas atypique fournir ainsi un outil performant de prédiction de le circulation intraday à court et moyen terme . 	Recherche de motifs spatio-temporels de cas atypiques pour le trafic routier urbain	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Recherche d'images par noyaux sur graphes de régions	Dans le cadre de la recherche interactive d'images dans une base de données, nous nous intéressons à des mesures de similarité d'image qui permettent d'améliorer l'apprentissage et utilisables en temps réel lors de la recherche. Les images sont représentées sous la forme de graphes d'adjacence de régions floues. Pour comparer des graphes valués nous employons des noyaux de graphes s'appuyant sur des ensembles de chaînes, extraites des graphes comparés. Nous proposons un cadre général permettant l'emploi de différents noyaux et différents types de chaînes(sans cycle, avec boucles) autorisant des appariements inexacts. Nous avons effectué des comparaisons sur deux bases issues de Columbia et Caltech et montré que des chaînes de très faible dimension (longueur inférieur à 3) sont les plus efficaces pour retrouver des classes d'objets.	Philippe-Henri Gosselin, Justine Lebrun, Sylvie Philipp-Foliguet	http://editions-rnti.fr/render_pdf.php?p1&p=1000634	http://editions-rnti.fr/render_pdf.php?p=1000634	1066	fr	fr		recherche d' image par noyau sur graphe de régions  Dans le cadre de le recherche interactif d' image dans un base de donnée , nous clr intéresser à un mesure de similarité d' image qui permettre d' améliorer le apprentissage et utilisable en temps réel lors de le recherche . le image être représenter sous le forme de graphe d' adjacence de région flou . Pour comparer un graphe valués nous employer un noyau de graphe clr appuyer sur un ensemble de chaîne , extraire des graphe comparer . Nous proposer un cadre général permettre le emploi de différent noyau et différent type de chaîne ( sans cycle , avec boucle ) autoriser un appariement inexact . Nous avoir effectuer un comparaison sur deux base issu de Columbia et Caltech et montrer que un chaîne de très faible dimension ( longueur inférieur à 3 ) être le plus efficace pour retrouver un classe d' objet . 	Recherche d'images par noyaux sur graphes de régions	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Recherche d'information personnalisée dans les bibliothèques numériques scientifiques	Dans cet article nous présentons nos travaux sur la recherche d'information personnalisée dans les bibliothèques numériques. Nous utilisons des profils utilisateurs qui représentent des intérêts et des préférences des utilisateurs. Les résultats de recherche peuvent être retriés en tenant compte des besoins d'informations spécifiques de différentes personnes, ce qui donne une meilleure précision. Nous étudions différentes méthodes basées sur les citations, sur le contenu textuel des documents et des approches hybrides. Les résultats des expérimentations montrent que nos approches sont efficaces et applicables dans le cadre des bibliothèques numériques.	Thanh-Trung Van, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1000556	http://editions-rnti.fr/render_pdf.php?p=1000556	1067	fr	fr		recherche d' information personnaliser dans le bibliothèque numérique scientifiques  Dans ce article nous présenter son travail sur le recherche d' information personnaliser dans le bibliothèque numérique . Nous utiliser un profil utilisateur qui représenter un intérêt et des préférence des utilisateur . le résultat de recherche pouvoir être retriés en tenir compte des besoin d' information spécifique de différent personne , ce qui donner un meilleur précision . Nous étudier différent méthode baser sur le citation , sur le contenu textuel des document et des approche hybride . le résultat des expérimentation montrer que son approche être efficace et applicable dans le cadre des bibliothèque numérique . 	Recherche d'information personnalisée dans les bibliothèques numériques scientifiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Requêtes alternatives dans le contexte d'un entrepôt de données génomiques	Afin d'aider les biologistes à annoter des génomes, ce qui nécessite l'analyse, le croisement, et la comparaison de données provenant de sources diverses, nous avons conçu un entrepôt de données de génomique microbienne. Nous présentons la structure globale flexible de l'entrepôt et son architecture multi-niveaux et définissons des correspondances entre ces niveaux. Nous introduisons ensuite la notion de requête alternative et montrons comment le système peut construire l'ensemble des requêtes alternatives à une requête initiale. Pour cela, nous introduisons un mécanisme d'interrogation qui repose sur l'architecture multi-niveaux, et donnons un algorithme de calcul des requêtes alternatives.	Christine Froidevaux, Frédéric Lemoine	http://editions-rnti.fr/render_pdf.php?p1&p=1000557	http://editions-rnti.fr/render_pdf.php?p=1000557	1068	fr	fr		requête alternatif dans le contexte d' un entrepôt de donnée génomiques  Afin d' aider le biologiste à annoter un génome , ce qui nécessiter le analyse , le croisement , et le comparaison de donnée provenir de source divers , nous avoir concevoir un entrepôt de donnée de génomique microbien . Nous présenter le structure global flexible de le entrepôt et son architecture multi-niveaux et définir un correspondance entre ce niveau . Nous introduire ensuite le notion de requête alternatif et montrer comment le système pouvoir construire le ensemble des requête alternatif à un requête initial . Pour cela , nous introduire un mécanisme d' interrogation qui reposer sur le architecture multi-niveaux , et donner un algorithme de calcul des requête alternatif . 	Requêtes alternatives dans le contexte d'un entrepôt de données génomiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Segmentation hiérarchique des cartes topologiques	Dans ce papier, nous présentons une nouvelle mesure de similarité pour la classification des référents de la carte auto-organisatrice qui sera réalisée à l'aide d'une nouvelle approche de classification hiérarchique. (1) La mesure de similarité est composée de deux termes : la distance de Ward pondérée et la distance euclidienne pondérée par la fonction de voisinage sur la carte topologique. (2) Un algorithme à base de fourmis artificielles nommé AntTree sera utilisé pour segmenter la carte auto-organisatrice.Cet algorithme a l'avantage de prendre en compte le voisinage entre les référents et de fournir une hiérarchie des référents avec une complexité proche du nlog(n). La segmentation incluant la nouvelle mesure est validée sur plusieurs bases de données publiques.	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1000662	http://editions-rnti.fr/render_pdf.php?p=1000662	1069	fr	fr		segmentation hiérarchique des carte topologiques  Dans ce papier , nous présenter un nouveau mesure de similarité pour le classification des référent de le carte auto- organisatrice qui être réaliser à le aide d' un nouveau approche de classification hiérarchique . ( 1 ) le mesure de similarité être composer de deux terme : le distance de Ward pondérer et le distance euclidien pondérer par le fonction de voisinage sur le carte topologique . ( 2 ) un algorithme à base de fourmi artificiel nommer AntTree être utiliser pour segmenter le carte auto- organisatrice . ce algorithme avoir le avantage de prendre en compte le voisinage entre le référent et de fournir un hiérarchie des référent avec un complexité proche du nlog ( n ) . le segmentation inclure le nouveau mesure être valider sur plusieurs base de donnée public . 	Segmentation hiérarchique des cartes topologiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Semantics of Spatial Window over Spatio-Temporal Data Stream	Dans les systèmes DSMS (Data Stream Management Systems), les données en entrée sont infinies et les requêtes sur celles-ci sont actives tout le temps. Dans le but de satisfaire ces caractéristiques, le fenêtrage temporel est largement utilisée pour convertir le flux infini de données sous forme de relations finies. Mais cette technique est inadaptée pour de nombreuses applications émergentes, en particulier les services de localisation. De nombreuses requêtes ne peuvent pas être traitées en utilisant le fenêtrage temporel, ou seraient traitées plus ecacement à l'aide d'un fenêtrage basé sur l'espace (fenêtrage spatial). Dans cet article, nous analysons la nécessité d'un fenêtrage spatial sur des flux de données spatio-temporels, et proposons, sur la base du langage de requêtes CQL (Continuous Query Language), une syntaxe et une sémantique associées au fenêtrage spatial.	Yi Yu, Talel Abdessalem	http://editions-rnti.fr/render_pdf.php?p1&p=1000570	http://editions-rnti.fr/render_pdf.php?p=1000570	1070	en	fr		Dans le système DSMS ( Data Stream Management Systems ) , le donnée en entrée être infini et le requête sur celui _-ci être actif tout le temps . Dans le but de satisfaire ce caractéristique , le fenêtrage temporel être largement utiliser pour convertir le flux infini de donnée sous forme de relation fini . Mais ce technique être inadapté pour un nombreux application émergent , en particulier le service de localisation . un nombreux requête ne pouvoir pas être traiter en utiliser le fenêtrage temporel , ou être traiter plus ecacement à le aide d' un fenêtrage baser sur le espace ( fenêtrage spatial ) . Dans ce article , nous analyser le nécessité d' un fenêtrage spatial sur un flux de donnée spatio- temporel , et proposer , sur le base du langage de requête CQL ( Continuous Query Language ) , un syntaxe et un sémantique associer au fenêtrage spatial . 	Semantics of Spatial Window over Spatio-Temporal Data Stream	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Sémantique et Réutilisation d'ontologie générique	Dans ce papier, nous enrichissons la méthode Terminae de construction d'ontologie à partir de textes en proposant une semi-automatisation de la construction du modèle conceptuel. Nous présentons un algorithme permettant la conceptualisation d'un terme en s'appuyant sur les informations linguistiques contenues dans l'ontologie générique de référence.	Sylvie Desprès, Sylvie Szulman	http://editions-rnti.fr/render_pdf.php?p1&p=1000563	http://editions-rnti.fr/render_pdf.php?p=1000563	1071	fr	fr		sémantique et réutilisation d' ontologie générique  Dans ce papier , nous enrichir le méthode Terminae de construction d' ontologie à partir de texte en proposer un semi-automatisation de le construction du modèle conceptuel . Nous présenter un algorithme permettre le conceptualisation d' un terme en clr appuyer sur le information linguistique contenir dans le ontologie générique de référence . 	Sémantique et Réutilisation d'ontologie générique	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	SOM pour la Classification Automatique Non supervisée de Documents Textuels basés sur Wordnet	Dans cet article, nous proposons la méthode des SOM (cartes auto-organisatrices de Kohonen) pour la classification non supervisée de documents textuels basés sur les n-grammes. La même méthode basée sur les synsets de WordNet comme termes pour la représentation des documents est étudiée par la suite. Ces combinaisons sont évaluées et comparées.	Mimoun Malki, Abdelmalek Amine, Zakaria Elberrichi, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1000596	http://editions-rnti.fr/render_pdf.php?p=1000596	1072	fr	fr		SOM pour le classification automatique non superviser de Documents Textuels baser sur Wordnet  Dans ce article , nous proposer le méthode des SOM ( carte auto- organisatrice de Kohonen ) pour le classification non superviser de document textuel baser sur le n-gramme . le même méthode baser sur le synsets de WordNet comme terme pour le représentation des document être étudier par le suite . ce combinaison être évaluer et comparer . 	SOM pour la Classification Automatique Non supervisée de Documents Textuels basés sur Wordnet	5
Revue des Nouvelles Technologies de l'Information	EGC	2008	Stratégies de classification non supervisée basées sur fenêtres superposées : application aux données d'usage du Web	Un problème majeur se pose dans le domaine des flux de données : la distribution sous-jacente des données peut changer sur le temps. Dans cet article, nous proposons trois stratégies de classification non supervisée basée sur des fenêtres superposées. Notre objectif est de pouvoir repérer ces changements dans le temps. Notre approche est appliquée sur un benchmark de données réelles et les conclusions obtenues sont basées sur deux indices de comparaison de partitions.	Alzennyr Da Silva, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1000592	http://editions-rnti.fr/render_pdf.php?p=1000592	1073	fr	fr		Stratégies de classification non superviser baser sur fenêtre superposer : application aux donnée d' usage du Web  un problème majeur clr poser dans le domaine des flux de donnée : le distribution sous-jacent des donnée pouvoir changer sur le temps . Dans ce article , nous proposer trois stratégie de classification non superviser baser sur un fenêtre superposer . son objectif être de pouvoir repérer ce changement dans le temps . son approche être appliquer sur un benchmark de donnée réel et le conclusion obtenir être baser sur deux indice de comparaison de partition . 	Stratégies de classification non supervisée basées sur fenêtres superposées : application aux données d'usage du Web	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Structure Inference of Bayesian Networks from Data: A New Approach Based on Generalized Conditional Entropy	We propose a novel algorithm for extracting the structure of a Bayesian network from a dataset. Our approach is based on generalized conditional entropies, a parametric family of entropies that extends the usual Shannon conditional entropy. Our results indicate that with an appropriate choice of a generalized conditional entropy we obtain Bayesian networks that have superior scores compared to similar structures obtained by classical inference methods.	Dan A. Simovici, Saaid Baraty	http://editions-rnti.fr/render_pdf.php?p1&p=1000621	http://editions-rnti.fr/render_pdf.php?p=1000621	1074	en	en		structure inference bayesian network data new approach base generalized conditional entropy propose novel algorithm extract structure bayesian network dataset approach base generalized conditional entropy parametric family entropies extend usual shannon conditional entropy result indicate appropriate choice generalized conditional entropy obtain bayesian network superior score compare similar structure obtain classical inference method	Structure Inference of Bayesian Networks from Data: A New Approach Based on Generalized Conditional Entropy	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Suppression des Itemsets Clés Non Essentiels en Classification basée sur les Règles d'Association	En classification basée sur les règles d'association, les itemsets clés sont essentiels : la suppression des itemsets non clés n'affecte pas la précision du classifieur en construction. Ce travail montre que parmi ces itemsets clés, on peut s'intéresser seulement à ceux de petites tailles. Plus loin encore, il étudie une généralisation d'une propriété importante des itemsets non clés et montre que parmi les itemsets clés de petites tailles, il y a ceux qui ne sont pas significatifs pour la classification. Ces itemsets clés sont dits non essentiels. Ils sont définis via un test de 2. Les expériences menées sur les grands jeux de données montrent que l'optimisation par la suppression de ces itemsets est correcte et efficace.	Viet Phan Luong	http://editions-rnti.fr/render_pdf.php?p1&p=1000626	http://editions-rnti.fr/render_pdf.php?p=1000626	1075	fr	fr		suppression des Itemsets Clés non Essentiels en Classification baser sur le règle d' Association  En classification baser sur le règle d' association , le itemsets clé être essentiel : le suppression des itemsets non clé n' affecter pas le précision du classifieur en construction . ce travail montrer que parmi ce itemsets clé , on pouvoir clr intéresser seulement à celui de petit taille . plus loin encore , il étudier un généralisation d' un propriété important des itemsets non clé et montrer que parmi le itemsets clé de petit taille , il y avoir celui qui ne être pas significatif pour le classification . ce itemsets clé être dire non essentiel . Ils être définir via un test de 2 . le expérience mener sur le grand jeu de donnée montrer que le optimisation par le suppression de ce itemsets être correct et efficace . 	Suppression des Itemsets Clés Non Essentiels en Classification basée sur les Règles d'Association	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Système multi-agent argumentatif pour la classification des connaissances cruciales	Dans cet article, nous proposons une approche multi-agent argumentative permettant d'automatiser la résolution des conflits entre décideurs dans un système d'aide à l'identification des connaissances cruciales nommé K-DSS. En effet, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. Notre objectif à travers ce travail est de proposer une approche argumentative permettant de résoudre les conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs.	Inès Saad, Imène Brigui-Chtioui	http://editions-rnti.fr/render_pdf.php?p1&p=1000667	http://editions-rnti.fr/render_pdf.php?p=1000667	1076	fr	fr		système multi-agent argumentatif pour le classification des connaissance cruciales  Dans ce article , nous proposer un approche multi-agent argumentatif permettre d' automatiser le résolution des conflit entre décideur dans un système d' aide à le identification des connaissance crucial nommer K-DSS . En effet , un divergence concernant le crucialité des connaissance pouvoir apparaître entre le décideur et aboutir ainsi à un incohérence dans le base commun de connaissance le rendre inexploitable . son objectif à travers ce travail être de proposer un approche argumentatif permettre de résoudre le conflit entre décideur . Afin de concevoir ce approche , nous clr appuyer sur le théorie multi-agents pour représenter le acteur humain par un agent logiciel connaître son préférence et son règle de décision et pouvoir ainsi argumenter son choix ou mettre à jour son croyance en fonction des argument qu' ils recevoir un autre agent décideur . 	Système multi-agent argumentatif pour la classification des connaissances cruciales	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un algorithme de classification topographique non supervisée à deux niveaux simultanés	Une des questions les plus importantes pour la plupart des applications réelles de la classification est de déterminer un nombre approprié de groupes (clusters). Déterminer le nombre optimal de groupes est un problème difficile, puisqu'il n'y a pas de moyen simple pour connaître ce nombre sans connaissance a priori. Dans cet article, nous proposons un nouvel algorithme de classification non supervisée à deux niveaux, appelé S2L-SOM (Simultaneous Twolevel Clustering - Self Organizing Map), qui permet de déterminer automatiquement le nombre optimal de groupes, pendant l'apprentissage d'une carte auto-organisatrice. L'estimation du nombre correct de groupes est en relation avec la stabilité de la segmentation et la validité des groupes générés. Pour mesurer cette stabilité nous utilisons une méthode de sous-échantillonnage. Le principal avantage de l'algorithme proposé, comparé aux méthodes classiques de classification, est qu'il n'est pas limité à la détection de groupes convexes, mais est capable de détecter des groupes de formes arbitraires. La validation expérimentale de cet algorithme sur un ensemble de problèmes fondamentaux pour la classification montre sa supériorité sur les méthodes standards de classification à deux niveaux comme SOM+K-Moyennes et SOM+Hierarchical- Agglomerative-Clustering.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000661	http://editions-rnti.fr/render_pdf.php?p=1000661	1077	fr	fr		un algorithme de classification topographique non superviser à deux niveau simultanés  un des question le plus important pour le plupart des application réel de le classification être de déterminer un nombre approprier de groupe ( clusters ) . déterminer le nombre optimal de groupe être un problème difficile , puisqu' il n' y avoir pas un moyen simple pour connaître ce nombre sans connaissance avoir priori . Dans ce article , nous proposer un nouveau algorithme de classification non superviser à deux niveau , appeler S2L-SOM ( Simultaneous Twolevel Clustering - Self Organizing Map ) , qui permettre de déterminer automatiquement le nombre optimal de groupe , pendant le apprentissage d' un carte auto- organisatrice . le estimation du nombre correct de groupe être en relation avec le stabilité de le segmentation et le validité des groupe générer . Pour mesurer ce stabilité nous utiliser un méthode de sous-échantillonnage . le principal avantage de le algorithme proposer , comparer aux méthode classique de classification , être qu' il n' être pas limiter à le détection de groupe convexe , mais être capable de détecter un groupe de forme arbitraire . le validation expérimental de ce algorithme sur un ensemble de problème fondamentaux pour le classification montrer son supériorité sur le méthode standard de classification à deux niveau comme SOM + K-Moyennes et SOM + Hierarchical-Agglomerative-Clustering . 	Un algorithme de classification topographique non supervisée à deux niveaux simultanés	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un cyber cartogramme gravitationnel pour l'analyse visuelle de données spatiotemporelles complexes	Le cartogramme présenté dans cet article est destiné à faciliterl'analyse visuelle de données spatiotemporelles complexes. Pour cela, il offrela possibilité de représenter simultanément les trois dimensions nécessaires àtoute forme d'analyse géographique que sont les dimensions spatiale (où),thématique (quoi) et temporelle (quand), à partir de trois composantes principales: (1) une représentation unidimensionnelle (1D) de l'espace géographiquede forme semi-circulaire centrée sur une origine (ex. le Canada) ; (2) desentités géographiques (ex. pays) qui viennent graviter autour de cette origineen fonction de valeurs attributaires ; et (3) une ligne de temps interactive permettantd'explorer la dimension temporelle de l'information représentée. Lacombinaison de ces trois composantes offre de multiples potentialités pourl'analyse spatio-temporelle de différentes formes de proximités qu'elles soientéconomiques, culturelles, sociales ou démographiques. Les fonctionnalités etpotentialités de ce cartogramme développé en source ouverte sont illustrées àpartir d'exemples issus de l'atlas cybercartographique du commerce Canadien.Cet article reprend les grandes lignes d'une communication présentée lors de laconférence SAGEO 2007.	Sébastien Caquard, Jean-Pierre Fiset	http://editions-rnti.fr/render_pdf.php?p1&p=1001229	http://editions-rnti.fr/render_pdf.php?p=1001229	1078	fr	fr	@connect.carleton.ca	un cyber cartogramme gravitationnel pour le analyse visuel de donnée spatiotemporelles complexes  le cartogramme présenter dans ce article être destiner à faciliterl'analyse visuel de donnée spatiotemporelles complexe . Pour cela , il offrela possibilité de représenter simultanément le trois dimension nécessaire àtoute forme d' analyse géographique que être le dimension spatial ( où ) , thématique ( quoi ) et temporel ( quand ) , à partir de trois composante principal : ( 1 ) un représentation unidimensionnel ( 1D ) de le espace géographiquede forme semi-circulaire centrer sur un origine ( ex. le Canada ) ; ( 2 ) desentités géographique ( ex. pays ) qui venir graviter autour de ce origineen fonction de valeur attributaire ; et ( 3 ) un ligne de temps interactif permettantd'explorer le dimension temporel de le information représenter . Lacombinaison de ce trois composante offrir de multiple potentialité pourl'analyse spatio- temporel de différent forme de proximité qu' elles soientéconomiques , culturel , social ou démographique . le fonctionnalité etpotentialités de ce cartogramme développer en source ouvert être illustrer àpartir d' exemple issu de le atlas cybercartographique du commerce canadien . ce article reprendre le grand ligne d' un communication présenter lors de laconférence SAGEO 2007 . 	Un cyber cartogramme gravitationnel pour l'analyse visuelle de données spatiotemporelles complexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un modèle d'espace vectoriel de concepts pour noyaux sémantiques	Les noyaux ont été largement utilisés pour le traitement de données textuelles comme mesure de similarité pour des algorithmes tels que les Séparateurs à VasteMarge (SVM). Le modèle de l'espace vectoriel (VSM) a été amplement utilisé pour la représentation spatiale des documents. Cependant, le VSM est une représentation purement statistique. Dans ce papier, nous présentons un modèle d'espace vectoriel de concepts (CVSM) qui se base sur des connaissances linguistiques a priori pour capturer le sens des documents. Nous proposons aussi un noyau linéaire et un noyau latent pour cet espace. Le noyau linéaire exploite les concepts linguistiques pour l'extraction du sens alors que le noyau latent combine les concepts statistiques et linguistiques. En effet, le noyau latent utilise des concepts latents extraits par l'Analyse Sémantique Latente (LSA) dans le CVSM. Les noyaux sont évalués sur une tâche de catégorisation de texte dans le domaine biomédical. Le corpus Ohsumed, bien connu pour sa difficulté de catégorisation, a été utilisé. Les résultats ont montré que les performances de catégorisation sont améliorées dans le CSVM.	Sujeevan Aseervatham	http://editions-rnti.fr/render_pdf.php?p1&p=1000659	http://editions-rnti.fr/render_pdf.php?p=1000659	1079	fr	fr		un modèle d' espace vectoriel de concept pour noyau sémantiques  le noyau avoir être largement utiliser pour le traitement de donnée textuel comme mesure de similarité pour un algorithme tel que le séparateur à VasteMarge ( SVM ) . le modèle de le espace vectoriel ( VSM ) avoir être amplement utiliser pour le représentation spatial des document . cependant , le VSM être un représentation purement statistique . Dans ce papier , nous présenter un modèle d' espace vectoriel de concept ( CVSM ) qui clr baser sur un connaissance linguistique avoir priori pour capturer le sens des document . Nous proposer aussi un noyau linéaire et un noyau latent pour ce espace . le noyau linéaire exploiter le concept linguistique pour le extraction du sens alors que le noyau latent combiner le concept statistique et linguistique . En effet , le noyau latent utiliser un concept latent extrait par le analyse sémantique latent ( LSA ) dans le CVSM . le noyau être évaluer sur un tâche de catégorisation de texte dans le domaine biomédical . le corpus Ohsumed , bien connaître pour son difficulté de catégorisation , avoir être utiliser . le résultat avoir montrer que le performance de catégorisation être améliorer dans le CSVM . 	Un modèle d'espace vectoriel de concepts pour noyaux sémantiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un modèle et une algèbre pour les systèmes de gestion d'ontologies	Nous présentons ici une approche pour la gestion de bases d'ontologies basée sur un modèle comprenant, outre la définition formelle des concepts (sous forme d'axiomes de logique de description), d'autres éléments descriptifs (termes, commentaires et arguments), ainsi que leurs liens d'alignement avec des concepts d'autres ontologies. L'adaptation ou la combinaison d'ontologies se font grâce à une algèbre comprenant des opérations telles que la sélection, la projection, l'union ou la jointure d'ontologies. Ces opérations agissent au niveau des axiomes, des éléments descriptifs et des liens d'alignement.	Gilles Falquet, Claire-Lise Mottaz Jiang, Jacques Guyot	http://editions-rnti.fr/render_pdf.php?p1&p=1000670	http://editions-rnti.fr/render_pdf.php?p=1000670	1080	fr	fr		un modèle et un algèbre pour le système de gestion d' ontologies  Nous présenter ici un approche pour le gestion de base d' ontologie baser sur un modèle comprendre , outre le définition formel des concept ( sous forme d' axiome de logique de description ) , un autre élément descriptif ( terme , commentaire et argument ) , ainsi que son lien d' alignement avec un concept d' autre ontologie . le adaptation ou le combinaison d' ontologie clr faire grâce à un algèbre comprendre un opération tel que le sélection , le projection , le union ou le jointure d' ontologie . ce opération agir au niveau des axiome , un élément descriptif et des lien d' alignement . 	Un modèle et une algèbre pour les systèmes de gestion d'ontologies	5
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un nouveau système immunitaire artificiel pour l'apprentissage non supervisé	Nous proposons dans ce papier un nouveau système immunitaire artificiel (SIA) appelé système NK, pour la détection de comportement du soi non soi avec une approche non supervisée basée sur le mécanisme de cellule NK (Naturel Killer). Dans ce papier, le système NK est appliqué à la détection de fraude en téléphonie mobile.	Rachid Elmeziane, Ilham Berrada, Ismail Kassou	http://editions-rnti.fr/render_pdf.php?p1&p=1000585	http://editions-rnti.fr/render_pdf.php?p=1000585	1081	fr	fr		un nouveau système immunitaire artificiel pour le apprentissage non supervisé  Nous proposer dans ce papier un nouveau système immunitaire artificiel ( SIA ) appeler système NK , pour le détection de comportement du lui non lui avec un approche non superviser baser sur le mécanisme de cellule NK ( Naturel Killer ) . Dans ce papier , le système NK être appliquer à le détection de fraude en téléphonie mobile . 	Un nouveau système immunitaire artificiel pour l'apprentissage non supervisé	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un processus d'acquisition d'information pour les besoins de l'enrichissement des BDG	Les données constituent l'élément central d'un Système d'Information Géographiques (SIG) et leur coût est souvent élevé en raison de l'investissement substantiel qui permet leur production. Cependant, ces données sont souvent restreintes à un service ou pour une catégorie d'utilisateurs. Ce qui a fait ressortir la nécessité de proposer des moyens d'enrichissement en informations pertinentes pour un nombre plus important d'utilisateurs. Nous présentons dans ce papier notre approche d'enrichissement de données qui se déroule selon trois étapes : une identification de segments et de thèmes associés, une délégation et enfin, un filtrage textuel. Un processus de raffinement est également offert. Notre approche globale a été intégrée à un SIG. Son évaluation a été accomplie montrant ainsi sa performance.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1000668	http://editions-rnti.fr/render_pdf.php?p=1000668	1082	fr	fr		un processus d' acquisition d' information pour le besoin de le enrichissement des BDG  le donnée constituer le élément central d' un système d' information Géographiques ( SIG ) et son coût être souvent élever en raison de le investissement substantiel qui permettre son production . cependant , ce donnée être souvent restreindre à un service ou pour un catégorie d' utilisateur . Ce qui avoir faire ressortir le nécessité de proposer un moyen d' enrichissement en information pertinent pour un nombre plus important d' utilisateur . Nous présenter dans ce papier son approche d' enrichissement de donnée qui clr dérouler selon trois étape : un identification de segment et de thème associer , un délégation et enfin , un filtrage textuel . un processus de raffinement être également offrir . son approche global avoir être intégrer à un SIG . son évaluation avoir être accomplir montrer ainsi son performance . 	Un processus d'acquisition d'information pour les besoins de l'enrichissement des BDG	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un système de vote pour la classification de textes d'opinion	Les tâches de classification textuelle ont souvent pour objectif de regrouper thématiquement différents textes. Dans cet article, nous nous sommes intéressés à la classification de documents en fonction des opinions et jugements de valeurs qu'ils contiennent. L'approche proposée est fondée sur un système de vote utilisant plusieurs méthodes de classification.	Michel Plantié, Mathieu Roche, Gérard Dray	http://editions-rnti.fr/render_pdf.php?p1&p=1000657	http://editions-rnti.fr/render_pdf.php?p=1000657	1083	fr	fr		un système de vote pour le classification de texte d' opinion  le tâche de classification textuel avoir souvent pour objectif de regrouper thématiquement différent texte . Dans ce article , nous clr sommer intéresser à le classification de document en fonction des opinion et jugement de valeur qu' ils contenir . le approche proposer être fonder sur un système de vote utiliser plusieurs méthode de classification . 	Un système de vote pour la classification de textes d'opinion	5
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une aide à la découverte de mappings dans SomeRDFS	Dans cet article, nous nous intéressons à la découverte de mises en correspondance entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données P2P SomeRDFS. Plus précisément, nous montrons comment exploiter les mécanismes de raisonnement mis en oeuvre dans SomeRDFS pour aider à découvrir des mappings entre ontologies. Ce travail est réalisé dans le cadre du projet MediaD en partenariat avec France Telecom R&D.	François-Élie Calvier, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000671	http://editions-rnti.fr/render_pdf.php?p=1000671	1084	fr	fr		un aide à le découverte de mappings dans SomeRDFS  Dans ce article , nous clr intéresser à le découverte de mise en correspondance entre ontologie distribuer modéliser le connaissance de pair du système de gestion de donnée P2P SomeRDFS . plus précisément , nous montrer comment exploiter le mécanisme de raisonnement mettre en oeuvre dans SomeRDFS pour aider à découvrir un mappings entre ontologie . ce travail être réaliser dans le cadre du projet MediaD en partenariat avec France Telecom R_&_D . 	Une aide à la découverte de mappings dans SomeRDFS	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une approche ensembliste inspirée du boosting en classification non supervisée	En classification supervisée, de nombreuses méthodes ensemblistes peuvent combiner plusieurs hypothèses de base afin de créer une règle de décision finale plus performante. Ainsi, il a été montré que des méthodes comme le bagging ou le boosting pouvaient se révéler intéressantes, tant dans la phase d'apprentissage qu'en généralisation. Dès lors, il est tentant de vouloir s'inspirer des grands principes d'une méthode comme le boosting en classification non supervisée. Or, il convient préalablement de se confronter aux difficultés connues de la thématique des ensembles de regroupeurs (correspondance des classes, agrégation des résultats, qualité) puis d'introduire l'idée du boosting dans un processus itératif. Cet article propose une méthode ensembliste inspirée du boosting, qui, à partir d'un partitionnement flou obtenu par les c-moyennes floues (fuzzy-c-means), va insister itérativement sur les exemples difficiles pour former une partition dure finale plus pertinente.	Romain Billot, Henri-Maxime Suchier, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000624	http://editions-rnti.fr/render_pdf.php?p=1000624	1085	fr	fr		un approche ensembliste inspirer du boosting en classification non supervisée  En classification superviser , de nombreux méthode ensembliste pouvoir combiner plusieurs hypothèse de base afin de créer un règle de décision final plus performant . ainsi , il avoir être montrer que un méthode comme le bagging ou le boosting pouvoir clr révéler intéressant , tant dans le phase d' apprentissage qu' en généralisation . Dès lors , il être tentant de vouloir clr inspirer un grand principe d' un méthode comme le boosting en classification non superviser . Or , il convenir préalablement de clr confronter aux difficulté connaître de le thématique des ensemble de regroupeurs ( correspondance des classe , agrégation des résultat , qualité ) puis d' introduire le idée du boosting dans un processus itératif . ce article proposer un méthode ensembliste inspirer du boosting , qui , à partir d' un partitionnement flou obtenir par le c-moyennes flou ( fuzzy-c-means ) , aller insister itérativement sur le exemple difficile pour former un partition dur final plus pertinent . 	Une approche ensembliste inspirée du boosting en classification non supervisée	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une approche ontologique pour automatiser le contrôle de conformité dans le domaine du bâtiment	Cet article présente la méthode et le système C3R pour vérifier de façon semi-automatique la conformité d'un projet de construction par rapport à des normes du bâtiment. Les projets de construction sont représentés par des graphes RDF et les normes par des requêtes SPARQL ; le processus de contrôle consiste en l'appariement des requêtes et des graphes. Son efficacité repose sur l'acquisition de connaissances ontologiques et sur un processus d'extraction de connaissances guidé par ce but spécifique de contrôle de conformité qui prend en compte les connaissances ontologiques acquises. Elle repose ensuite sur des méta-connaissances acquises auprès des experts du CSTB qui permettent de guider le contrôle lui-même : les requêtes représentant les normes sont annotées et organisées selon ces annotations. Ces annotations sont également utilisées dans les interactions avec l'utilisateur de C3R pour expliquer les résultats du processus de validation, en particulier en cas d'échec.	Anastasiya Yurchyshyna, Catherine Faron-Zucker, Nhan Le Thanh, Celson Lima	http://editions-rnti.fr/render_pdf.php?p1&p=1000562	http://editions-rnti.fr/render_pdf.php?p=1000562	1086	fr	fr		un approche ontologique pour automatiser le contrôle de conformité dans le domaine du bâtiment  ce article présenter le méthode et le système C3R pour vérifier de façon semi-automatique le conformité d' un projet de construction par rapport à un norme du bâtiment . le projet de construction être représenter par un graphe RDF et le norme par un requête SPARQL ; le processus de contrôle consister en le appariement des requête et des graphe . son efficacité reposer sur le acquisition de connaissance ontologique et sur un processus d' extraction de connaissance guider par ce but spécifique de contrôle de conformité qui prendre en compte le connaissance ontologique acquérir . Elle reposer ensuite sur un méta-connaissances acquérir auprès un expert du CSTB qui permettre de guider le contrôle lui-même : le requête représenter le norme être annoter et organiser selon ce annotation . ce annotation être également utiliser dans le interaction avec le utilisateur de C3R pour expliquer le résultat du processus de validation , en particulier en cas d' échec . 	Une approche ontologique pour automatiser le contrôle de conformité dans le domaine du bâtiment	5
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une J-mesure orientée pour élaguer des modèles de chroniques		Marc Le Goc, Nabil Benayadi	http://editions-rnti.fr/render_pdf.php?p1&p=1000593	http://editions-rnti.fr/render_pdf.php?p=1000593	1087	fr			Une J-mesure orientée pour élaguer des modèles de chroniques 	Une J-mesure orientée pour élaguer des modèles de chroniques	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une mesure de similarité contextuelle pour l'aide à la navigation dans un treillis	La recherche d'information et la navigation dans les pages web s'avèrent complexes du fait du volume croissant des données et de leur manque de structure. La formalisation conceptuelle d'un contexte associé à une ontologie rend possible l'amélioration de ce processus. Nous définissons un contexte conceptuel comme étant l'association d'un treillis de concepts construit à partir de pages web avec des ontologies. La recherche et la navigation peuvent alors s'effectuer à plusieurs niveaux d'abstraction : le niveau des données, le niveau conceptuel et le niveau sémantique. Cet article s'intéresse essentiellement au niveau conceptuel grâce à une représentation par les treillis de concepts des documents selon les termes qu'ils ont en commun. Notre objectif est de proposer une mesure de similarité permettant à l'utilisateur de mieux naviguer dans le treillis. En effet, une bonne interprétation du treillis devrait passer par un choix rigoureux des concepts, objets, relations et propriétés les plus intéressants. Pour faciliter la navigation, il faut pouvoir indiquer à l'utilisateur les concepts les plus pertinents par rapport au concept correspondant à sa requête ou pouvoir lui proposer un point de départ. L'originalité de notre proposition réside dans le fait de considérer un lien sémantique entre les concepts du treillis, basé sur une extension des mesures de similarité utilisées dans le cadre des ontologies, afin de permettre une meilleure exploitation de ce treillis. Nous présentons les résultats expérimentaux de l'application de cette mesure sur des treillis construits à partir de pages web dans le domaine du tourisme.	Saoussen Sakji, Marie-Aude Aufaure, Géraldine Polaillon, Bénédicte Le Grand, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1000561	http://editions-rnti.fr/render_pdf.php?p=1000561	1088	fr	fr		un mesure de similarité contextuel pour le aide à le navigation dans un treillis  le recherche d' information et le navigation dans le page web clr avérer complexe du fait du volume croissant des donnée et de son manque de structure . le formalisation conceptuel d' un contexte associer à un ontologie rendre possible le amélioration de ce processus . Nous définir un contexte conceptuel comme être le association d' un treillis de concept construire à partir de page web avec un ontologie . le recherche et le navigation pouvoir alors clr effectuer à plusieurs niveau d' abstraction : le niveau des donnée , le niveau conceptuel et le niveau sémantique . ce article clr intéresser essentiellement au niveau conceptuel grâce à un représentation par le treillis de concept des document selon le terme qu' ils avoir en commun . son objectif être de proposer un mesure de similarité permettre à le utilisateur de mieux naviguer dans le treillis . En effet , un bon interprétation du treillis devoir passer par un choix rigoureux des concept , objet , relation et propriété le plus intéressant . Pour faciliter le navigation , il faillir pouvoir indiquer à le utilisateur le concept le plus pertinent par rapport au concept correspondre à son requête ou pouvoir lui proposer un point de départ . le originalité de son proposition résider dans le fait de considérer un lien sémantique entre le concept du treillis , baser sur un extension des mesure de similarité utiliser dans le cadre des ontologie , afin de permettre un meilleur exploitation de ce treillis . Nous présenter le résultat expérimental de le application de ce mesure sur un treillis construire à partir de page web dans le domaine du tourisme . 	Une mesure de similarité contextuelle pour l'aide à la navigation dans un treillis	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une nouvelle approche du boosting face aux données bruitées	La réduction de l'erreur en généralisation est l'une des principales motivations de la recherche en apprentissage automatique. De ce fait, un grand nombre de travaux ont été menés sur les méthodes d'agrégation de classifieurs afin d'améliorer, par des techniques de vote, les performances d'un classifieur unique. Parmi ces méthodes d'agrégation, le boosting est sans doute le plus performant grâce à la mise à jour adaptative de la distribution des exemples visant à augmenter de façon exponentielle le poids des exemples mal classés. Cependant, en cas de données fortement bruitées, cette méthode est sensible au sur-apprentissage et sa vitesse de convergence est affectée. Dans cet article, nous proposons une nouvelle approche basée sur des modifications de la mise à jour des exemples et du calcul de l'erreur apparente effectuées au sein de l'algorithme classique d'AdaBoost. Une étude expérimentale montre l'intérêt de cette nouvelle approche, appelée Approche Hybride, face à AdaBoost et à BrownBoost, une version d'AdaBoost adaptée aux données bruitées.	Emna Bahri, Mondher Maddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000623	http://editions-rnti.fr/render_pdf.php?p=1000623	1089	fr	fr		un nouveau approche du boosting face aux donnée bruitées  le réduction de le erreur en généralisation être le un des principal motivation de le recherche en apprentissage automatique . De ce fait , un grand nombre de travail avoir être mener sur le méthode d' agrégation de classifieur afin d' améliorer , par un technique de vote , le performance d' un classifieur unique . Parmi ce méthode d' agrégation , le boosting être sans doute le plus performant grâce à le mise à jour adaptatif de le distribution des exemple viser à augmenter de façon exponentiel le poids des exemple mal classer . cependant , en cas de donnée fortement bruiter , ce méthode être sensible au sur-apprentissage et son vitesse de convergence être affecter . Dans ce article , nous proposer un nouveau approche baser sur un modification de le mise à jour des exemple et du calcul de le erreur apparent effectuer au sein de le algorithme classique d' AdaBoost . un étude expérimental montrer le intérêt de ce nouveau approche , appeler approche Hybride , face à AdaBoost et à BrownBoost , un version d' AdaBoost adapter aux donnée bruiter . 	Une nouvelle approche du boosting face aux données bruitées	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une nouvelle méthode divisive en classification non supervisée pour des données symboliques intervalles	Dans cet article nous présentons une nouvelle méthode de classification non supervisée pour des données symboliques intervalles. Il s'agit de l'extension d'une méthode de classification non supervisée classique à des données intervalles. La méthode classique suppose que les points observés sont la réalisation d'un processus de Poisson homogène dans k domaines convexes disjoints de Rp. La première partie de la nouvelle méthode est une procédure monothétique divisive. La règle de coupure est basée sur une extension à des données intervalles du critère de classification des Hypervolumes. L'étape d'élagage utilise un test statistique basé sur le processus de Poisson homogène. Le résultat est un arbre de décision. La seconde partie de la méthode consiste en une étape de recollement, qui permet, dans certains cas, d'améliorer la classification obtenue à la fin de la première partie de l'algorithme. La méthode est évaluée sur un ensemble de données réelles.	Nathanaël Kasoro, André Hardy	http://editions-rnti.fr/render_pdf.php?p1&p=1000664	http://editions-rnti.fr/render_pdf.php?p=1000664	1090	fr	fr		un nouveau méthode divisive en classification non superviser pour un donnée symbolique intervalles  Dans ce article nous présenter un nouveau méthode de classification non superviser pour un donnée symbolique intervalle . Il clr agir de le extension d' un méthode de classification non superviser classique à un donnée intervalles . le méthode classique supposer que le point observer être le réalisation d' un processus de Poisson homogène dans k domaine convexe disjoindre de Rp . le premier partie de le nouveau méthode être un procédure monothétique divisive . le règle de coupure être baser sur un extension à un donnée intervalle du critère de classification des Hypervolumes . le étape d' élagage utiliser un test statistique baser sur le processus de Poisson homogène . le résultat être un arbre de décision . le second partie de le méthode consister en un étape de recollement , qui permettre , dans certain cas , d' améliorer le classification obtenir à le fin de le premier partie de le algorithme . le méthode être évaluer sur un ensemble de donnée réel . 	Une nouvelle méthode divisive en classification non supervisée pour des données symboliques intervalles	
Revue des Nouvelles Technologies de l'Information	EGC	2008	Une proposition pour l'extraction de relations non prédicatives	Les relations sémantiques généralement reconnues par les méthodes d'extraction sont portées par des structures de type prédicats-arguments. Or, l'information recherchée est souvent répartie sur plusieurs phrases. Pour détecter ces relations dites complexes, nous proposons un modèle de représentation des connaissances basé sur les graphes conceptuels.	Mouna Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1000590	http://editions-rnti.fr/render_pdf.php?p=1000590	1091	fr	fr		un proposition pour le extraction de relation non prédicatives  le relation sémantique généralement reconnaître par le méthode d' extraction être porter par un structure de type prédicats-arguments . Or , le information rechercher être souvent répartir sur plusieurs phrase . Pour détecter ce relation dire complexe , nous proposer un modèle de représentation des connaissance baser sur le graphe conceptuel . 	Une proposition pour l'extraction de relations non prédicatives	2
Revue des Nouvelles Technologies de l'Information	EGC	2008	Utilisation du Web Sémantique pour la gestion d'une liste de diffusion d'une CoP	Cet article décrit une approche de création semi-automatique d'ontologies et d'annotations sémantiques à partir de messages électroniques échangés dans une liste de diffusion dédiée au support informatique. Les ressources sémantiques générées permettront d'identifier les questions fréquemment posées (FAQ) à travers une recherche guidée par cette ontologie.	Bassem Makni, Khaled Khelif, Rose Dieng-Kuntz, Hacène Cherfi	http://editions-rnti.fr/render_pdf.php?p1&p=1000553	http://editions-rnti.fr/render_pdf.php?p=1000553	1092	fr	fr		utilisation du Web Sémantique pour le gestion d' un liste de diffusion d' un CoP  ce article décrire un approche de création semi-automatique d' ontologie et d' annotation sémantique à partir de message électronique échanger dans un liste de diffusion dédier au support informatique . le ressource sémantique générer permettre d' identifier le question fréquemment poser ( FAQ ) à travers un recherche guider par ce ontologie . 	Utilisation du Web Sémantique pour la gestion d'une liste de diffusion d'une CoP	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	"Vers des Machines à Vecteurs de Support ""Actionnables"" : Une Approche Fondée sur le Classement"	"Une des principales critiques que l'on puisse faire aux Séparateurs à Vaste Marge (SVM) est le manque d'intelligibilité des résultats. En effet, il s'agit d'une technique ""boite noire"" qui ne fournit pas d'explications ni d'indices quant aux raisons d'une classification. Les résultats doivent être pris tels quels en faisant confiance au système qui les a produits. Pourtant selon notre expérience pratique, les experts du domaine préfèrent largement une méthode d'apprentissage avec explications et recommandation d'actions plutôt qu'une boite noire, aussi performante et prédictive soit-elle. Dans cette thématique, nous proposons une nouvelle approche qui consiste a rendre les SVM plus ""actionnables"". Ce but est atteint en couplant des modèles de classement des résultats des SVM à des méthodes d'apprentissage de concepts. Nous présentons une application de notre méthode sur diverses données dont des données médicales concernant des patients de l'athérosclérose. Nos résultats empiriques semblent très prometteurs et montrent l'utilité de notre approche quant à l'intelligibilité et l'actionnabilité des résultats produits par SVM."	Ansaf Salleb-Aouissi, Bert C. Huang, David L. Waltz	http://editions-rnti.fr/render_pdf.php?p1&p=1000616	http://editions-rnti.fr/render_pdf.php?p=1000616	1093	fr	fr		" ver des Machines à Vecteurs de Support " " Actionnables " " : : un approche fonder sur le classement "  " un des principal critique que le on pouvoir faire aux séparateur à vaste Marge ( SVM ) être le manque d' intelligibilité des résultat . . En effet , il clr agir d' un technique " " boiter noir " " qui ne fournir pas d' explication ni d' indice quant aux raison d' un classification . . le résultat devoir être prendre tel quel en faire confiance au système qui les avoir produire . . pourtant selon son expérience pratique , le expert du domaine préférer largement un méthode d' apprentissage avec explication et recommandation d' action plutôt qu' un boite noir , aussi performant et prédictif être -elle . . Dans ce thématique , nous proposer un nouveau approche qui consister avoir rendre le SVM plus " " actionnable " " . . ce but être atteindre en coupler un modèle de classement des résultat des SVM à un méthode d' apprentissage de concept . . Nous présenter un application de son méthode sur divers donnée dont des donnée médical concerner un patient de le athérosclérose . . son résultat empirique sembler très prometteur et montrer le utilité de son approche quant à le intelligibilité et le actionnabilité des résultat produire par SVM . " 	"Vers des Machines à Vecteurs de Support ""Actionnables"" : Une Approche Fondée sur le Classement"	0
Revue des Nouvelles Technologies de l'Information	EGC	2008	Vers l'exploitation de grandes masses de données	Une tendance lourde depuis la fin du siècle dernier est l'augmentation exponentielle du volume des données stockées. Cette augmentation ne se traduit pas nécessairement par une information plus riche puisque la capacité à traiter ces données ne progresse pas aussi rapidement. Avec les technologies actuelles, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l'information produite. Nous proposons une approche industrielle permettant d'augmenter considérablement notre capacité à transformer des données en information grâce à l'automatisation des traitements et à la focalisation sur les seules données pertinentes.	Raphaël Feraud, Marc Boullé, Fabrice Clérot, Françoise Fessant	http://editions-rnti.fr/render_pdf.php?p1&p=1000609	http://editions-rnti.fr/render_pdf.php?p=1000609	1094	fr	fr		Vers le exploitation de grand masse de données  un tendance lourd depuis le fin du siècle dernier être le augmentation exponentiel du volume des donnée stocker . ce augmentation ne clr traduire pas nécessairement par un information plus riche puisque le capacité à traiter ce donnée ne progresser pas aussi rapidement . Avec le technologie actuel , un difficile compromis devoir être trouver entre le coût de mise en oeuvre et le qualité de le information produire . Nous proposer un approche industriel permettre d' augmenter considérablement son capacité à transformer un donnée en information grâce à le automatisation des traitement et à le focalisation sur le seul donnée pertinent . 	Vers l'exploitation de grandes masses de données	8
Revue des Nouvelles Technologies de l'Information	EGC	2008	Vers l'intégration de la prédiction dans les cubes OLAP		Anouck Bodin-Niemczuk, Riadh Ben Messaoud, Sabine Loudcher, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000584	http://editions-rnti.fr/render_pdf.php?p=1000584	1095	fr			Vers l'intégration de la prédiction dans les cubes OLAP 	Vers l'intégration de la prédiction dans les cubes OLAP	7
Revue des Nouvelles Technologies de l'Information	EGC	2008	Vers une fouille sémantique des brevets : Application au domaine biomédical	"Les brevets sont une source d'information très riche puisque ce sont des documents qui servent à décrire les inventions. L'accès aux documents de brevets en ligne est possible grâce aux efforts des offices nationaux de la propriété intellectuelle. Par ailleurs, ayant des objectifs différents, la présentation de ces documents a pris des formes variées loin d'être unifiées. Ce papier présente une méthode et un système permettant l'analyse de brevets ""Patent Mining"" pour générer des annotations sémantiques. L'idée principale est de pouvoir prendre en considération la structure des brevets pour pouvoir trouver un lien entre le contenu du brevet et les concepts des différentes ontologies."	Nizar Ghoula, Khaled Khelif, Rose Dieng-Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000552	http://editions-rnti.fr/render_pdf.php?p=1000552	1096	fr	fr		Vers un fouille sémantique des brevet : application au domaine biomédical  " le brevet être un source d' information très riche puisque ce être un document qui servir à décrire le invention . . le accès aux document de brevet en ligne être possible grâce aux effort des office national de le propriété intellectuel . . Par ailleurs , avoir un objectif différent , le présentation de ce document avoir prendre un forme varier loin d' être unifier . . ce papier présenter un méthode et un système permettre le analyse de brevet " " Patent Mining " " pour générer un annotation sémantique . . le idée principal être de pouvoir prendre en considération le structure des brevet pour pouvoir trouver un lien entre le contenu du brevet et le concept des différent ontologie . " 	Vers une fouille sémantique des brevets : Application au domaine biomédical	1
Revue des Nouvelles Technologies de l'Information	EGC	2008	Visualisation des motifs séquentiels extraits à partir d'un corpus en Ancien Français	Cet article présente une interface permettant de visualiser des motifs séquentiels extraits à partir de données textuelles en Ancien Français.	Julien Rabatel, Yuan Lin, Yoann Pitarch, Hassan Saneifar, Claire Serp, Mathieu Roche, Anne Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000605	http://editions-rnti.fr/render_pdf.php?p=1000605	1097	fr	fr		visualisation des motif séquentiel extrait à partir d' un corpus en ancien Français  ce article présenter un interface permettre de visualiser un motif séquentiel extrait à partir de donnée textuel en ancien français . 	Visualisation des motifs séquentiels extraits à partir d'un corpus en Ancien Français	3
Revue des Nouvelles Technologies de l'Information	EGC	2008	Visualisation et classification des parcours de vie	Cet article propose une méthodologie pour la visualisation et la classification des parcours de vie. Plus spécifiquement, nous considérons les parcours de vie d'individus suisses nés durant la première moitié du XXème siècle en utilisant les données provenant de l'enquête biographique rétrospective menée en 2002 par le Panel suisse de ménages. Nous nous sommes concentrés sur ces événements du parcours de vie : le départ du foyer parental, la naissance du premier enfant, le premier mariage et le premier divorce. A partir des données de base sur ces événements, nous discutons de leur transformation en séquences d'états. Nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie. Cette méthodologie repose sur des distances calculées par un algorithme d'optimal matching. Ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à l'aide de techniques de « Multi Dimensional Scaling ». Cet article s'intéresse en particulier aux problématiques entourant l'application de ces méthodes aux données de parcours de vie.	Nicolas S. Müller, Sylvain Lespinats, Gilbert Ritschard, Matthias Studer, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1000638	http://editions-rnti.fr/render_pdf.php?p=1000638	1098	fr	fr		visualisation et classification des parcours de vie  ce article proposer un méthodologie pour le visualisation et le classification des parcours de vie . plus spécifiquement , nous considérer le parcours de vie d' individu suisse naître durant le premier moitié du XXème siècle en utiliser le donnée provenir de le enquête biographique rétrospectif mener en 2002 par le Panel suisse de ménage . Nous nous sommer concentrer sur ce événement du parcours de vie : le départ du foyer parental , le naissance du premier enfant , le premier mariage et le premier divorce . A partir un donnée de base sur ce événement , nous discuter de son transformation en séquence d' état . Nous présenter ensuite son méthodologie pour extraire de le connaissance des parcours de vie . ce méthodologie reposer sur un distance calculer par un algorithme d' optimal matching . ce distance être ensuite utiliser pour le classification des parcours de vie et son visualisation à le aide de technique de « Multi Dimensional Scaling » . ce article clr intéresser en particulier aux problématique entourer le application de ce méthode aux donnée de parcours de vie . 	Visualisation et classification des parcours de vie	6
Revue des Nouvelles Technologies de l'Information	EGC	2008	Web Content Data Mining : la classification croisée pour l'analyse textuelle d'un site Web	Notre objectif dans cet article est l'analyse textuelle d'un site Web indépendamment de son usage. Notre approche se déroule en trois étapes. La première étape consiste au typage des pages afin de distinguer les pages de navigation ou pages « auxiliaires » des pages de contenu. La deuxième étape consiste au prétraitement du contenu des pages de contenu afin de représenter chaque page par un vecteur de descripteurs. La dernière étape consiste au block clustering ou la classification simultanée des lignes et des colonnes de la matrice croisant les pages aux descripteurs de pages afin de découvrir des biclasses de pages et de descripteurs. L'application de cette approche au site de tourisme de Metz prouve son efficacité et son applicabilité. L'ensemble de classes de pages groupés en thèmes facilitera l'analyse ultérieure de l'usage du site.	Malika Charrad, Yves Lechevallier, Gilbert Saporta, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000555	http://editions-rnti.fr/render_pdf.php?p=1000555	1099	fr	fr		web Content Data Mining : le classification croiser pour le analyse textuel d' un site Web  son objectif dans ce article être le analyse textuel d' un site Web indépendamment de son usage . son approche clr dérouler en trois étape . le premier étape consister au typage des page afin de distinguer le page de navigation ou page « auxiliaire » un page de contenu . le deuxième étape consister au prétraitement du contenu des page de contenu afin de représenter chaque page par un vecteur de descripteur . le dernier étape consister au block clustering ou le classification simultané des ligne et des colonne de le matrice croiser le page aux descripteur de page afin de découvrir un biclasses de page et de descripteur . le application de ce approche au site de tourisme de Metz prouver son efficacité et son applicabilité . le ensemble de classe de page grouper en thème faciliter le analyse ultérieur de le usage du site . 	Web Content Data Mining : la classification croisée pour l'analyse textuelle d'un site Web	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Alignement de ressources sémantiques à partir de règles	Ce papier présente une approche automatique pour aligner des ressources sémantiques. L'alignement se traduit par la mise en correspondance des entités (termes, concepts, rôles) appartenant à des ressources d'un même domaine qui peuvent avoir des niveaux de formalisation différents. Les entités correspondantes sont de même nature et un coefficient caractérise leur degré de ressemblance.L'approche proposée est fondée sur des règles d'appariement entre les entités des deux ressources. Dans une première phase, ces règles d'appariement sont identifiées empiriquement. Des algorithmes combinant les différentes règles identifiées sont ensuite définis afin d'établir des correspondances entre les entités des ressources considérées.Ce papier présente un ensemble de règles d'appariement exploitant des éléments situés à différents niveaux conceptuels. Cet ensemble constitue un cadre pour l'alignement automatique des ressources sémantiques. Les résultats d'une première expérimentation qui a porté sur l'alignement de deux ressources du domaine de l'accidentologie sont également présentés.	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1001444	http://editions-rnti.fr/render_pdf.php?p=1001444	1199	fr	fr	@univ-paris5.fr, @univ-paris5.fr	alignement de ressource sémantique à partir de règles  ce papier présenter un approche automatique pour aligner un ressource sémantique . le alignement clr traduire par le mise en correspondance des entité ( terme , concept , rôle ) appartenir à un ressource d' un même domaine qui pouvoir avoir un niveau de formalisation différent . le entité correspondant être de même nature et un coefficient caractériser son degré de ressemblance . le approche proposer être fonder sur un règle d' appariement entre le entité des deux ressource . Dans un premier phase , ce règle d' appariement être identifier empiriquement . un algorithme combiner le différent règle identifier être ensuite définir afin d' établir un correspondance entre le entité des ressource considérer . ce papier présenter un ensemble de règle d' appariement exploiter un élément situer à différents niveau conceptuel . ce ensemble constituer un cadre pour le alignement automatique des ressource sémantique . le résultat d' un premier expérimentation qui avoir porter sur le alignement de deux ressource du domaine de le accidentologie être également présenter . 	Alignement de ressources sémantiques à partir de règles	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Annotation et navigation de données archéologiques	Dans cet article, nous proposons un cadre et un outil pour l'annotation et la navigation de données archéologiques. L'objectif principal est de structurer les annotations de façon à permettre une navigation incrémentale où l'utilisateur peut, à partir d'un ensemble d'objets initialement retournés par une requête, découvrir des liens approximatifs avec d'autres objets de la base. L'approche a été implémentée et est en cours de validation.	Bernardo Lopez, Samira Hammiche, Samir Sebahi, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001350	http://editions-rnti.fr/render_pdf.php?p=1001350	1200	fr	fr	@univ-lyon1.fr	annotation et navigation de donnée archéologiques  Dans ce article , nous proposer un cadre et un outil pour le annotation et le navigation de donnée archéologique . le objectif principal être de structurer le annotation de façon à permettre un navigation incrémentale où le utilisateur pouvoir , à partir d' un ensemble d' objet initialement retourner par un requête , découvrir un lien approximatif avec un autre objet de le base . le approche avoir être implémenter et être en cour de validation . 	Annotation et navigation de données archéologiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Annotation sémantique floue de tableaux guidée par une ontologie	Nous présentons dans cet article différentes étapes de l'annotation de tableaux de données à l'aide d'une ontologie. Tout d'abord, nous distinguons les colonnes de données numériques et symboliques. Les données symboliques sont ensuite annotées de manière floue à l'aide des termes de l'ontologie. Cette annotation nous permet de déduire le type des colonnes de données symboliques. Pour trouver le type des colonnes de données numériques, nous utilisons à la fois le titre de la colonne et les valeurs numériques et unités présentes dans la colonne. Chaque étape de notre annotation est validée expérimentalement.	Gaëlle Hignette, Patrice Buche, Juliette Dibie-Barthélemy, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1001441	http://editions-rnti.fr/render_pdf.php?p=1001441	1201	fr	fr	@risk, @inapg.fr, @univ-tlse2.fr	annotation sémantique flou de tableau guider par un ontologie  Nous présenter dans ce article différent étape de le annotation de tableau de donnée à le aide d' un ontologie . tout d' abord , nous distinguer le colonne de donnée numérique et symbolique . le donnée symbolique être ensuite annoter de manière flou à le aide des terme de le ontologie . ce annotation nous permettre de déduire le type des colonne de donnée symbolique . Pour trouver le type des colonne de donnée numérique , nous utiliser à le foi le titre de le colonne et le valeur numérique et unité présent dans le colonne . chaque étape de son annotation être valider expérimentalement . 	Annotation sémantique floue de tableaux guidée par une ontologie	5
Revue des Nouvelles Technologies de l'Information	EGC	2007	Application des réseaux bayésiens à l'analyse des facteurs impliqués dans le cancer du Nasopharynx	L'apprentissage de la structure des réseaux bayésien à partir de données est un problème NP-difficile. Une nouvelle heuristique de complexité polynômiale, intitulée Polynomial Max-Min Skeleton (PMMS), a été proposée en 2005 par Tsamardinos et al. et validée avec succès sur de nombreux bancs d'essai. PMMS présente, en outre, l'avantage d'être performant avec des jeux de données réduits. Néanmoins, comme tous les algorithmes sous contraintes, celui-ci échoue lorsque des dépendances fonctionnelles (déterministes) existent entre des groupes de variables. Il ne s'applique, par ailleurs, qu'aux données complètes. Aussi, dans cet article, nous apportons quelques modifications pour remédier à ces deux problèmes. Après validation sur le banc d'essai Asia, nous l'appliquons aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC) de 1289 observations, 61 variables et 5% de données manquantes issues d'un questionnaire. L'objectif est de dresser un profil statistique type de la population étudiée et d'apporter un éclairage utile sur les différents facteurs impliqués dans le NPC	Alexandre Aussem, Sergio Rodrigues de Morais, Marilys Corbex	http://editions-rnti.fr/render_pdf.php?p1&p=1001318	http://editions-rnti.fr/render_pdf.php?p=1001318	1202	fr	fr	@univ-lyon1.fr, @iarc.fr	application des réseau bayésien à le analyse des facteur impliquer dans le cancer du Nasopharynx  le apprentissage de le structure des réseau bayésien à partir de donnée être un problème NP-difficile . un nouveau heuristique de complexité polynômiale , intituler Polynomial Max-Min Skeleton ( PMMS ) , avoir être proposer en 2005 par Tsamardinos et al. et valider avec succès sur un nombreux banc d' essai . PMMS présenter , en outre , le avantage d' être performant avec un jeu de donnée réduire . néanmoins , comme tout le algorithme sous contrainte , celui _-ci échouer lorsque un dépendance fonctionnel ( déterministe ) exister entre un groupe de variable . Il ne clr appliquer , par ailleurs , qu' aux donnée complet . aussi , dans ce article , nous apporter quelque modification pour remédier à ce deux problème . Après validation sur le banc d' essai Asia , nous le appliquons aux donnée d' un étude épidémiologique cas-témoins du cancer du nasopharynx ( NPC ) de 1289 observation , 61 variable et 5 \% de donnée manquant issir d' un questionnaire . le objectif être de dresser un profil statistique type de le population étudier et d' apporter un éclairage utile sur le différent facteur impliquer dans le NPC 	Application des réseaux bayésiens à l'analyse des facteurs impliqués dans le cancer du Nasopharynx	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apport du Web sémantique dans la réalisation d'un moteur de recherche géo-localisé à usage des entreprises	La recherche d'une entreprise sur le Web, relative à un savoir-faire particulier, n'est pas une tâche toujours facile à mener. Les outils mis à la disposition de l'internaute ne donnent pas entièrement satisfaction. D'un côté les moteurs de recherche éprouvent des difficultés à faire ressortir clairement le résultat escompté. De l'autre côté, les annuaires spécialisés (type Pages Jaunes) sont tributaires d'une organisation figée, nuisant à leur efficacité. Face à ce constat, nous nous proposons de créer un nouveau moteur spécialisé dans la recherche d'entreprise, associant Web sémantique et géo-localisation. Cette approche novatrice nécessite l'implémentation d'une ontologie ayant pour objectif la formalisation des connaissances du domaine. Cette tâche a mis en évidence l'intérêt des structures économiques, maintenues par l'INSEE, et leur utilisation au sein de l'ontologie. Les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises. La structure des unités administratives, telle que gérée au sein du fichier SIRENE, s'est avérée judicieuse pour répondre à la problématique de géo-localisation des entreprises. Une opération de désambiguïsation est réalisée en associant à chaque noeud d'activité les mots clés et synonymes lui correspondant. Enfin, nous comparons les résultats obtenus par notre moteur à ceux obtenu par le principal moteur de recherche d'activités géo-localisées en France : les Pages jaunes. Que ce soit au niveau de la précision et du rappel, notre moteur obtient des résultats significativement meilleurs.	Frédéric Triou, Fabien Picarougne, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1001307	http://editions-rnti.fr/render_pdf.php?p=1001307	1203	fr	fr	@univ-nantes.fr	apport du Web sémantique dans le réalisation d' un moteur de recherche géo- localiser à usage des entreprises  le recherche d' un entreprise sur le Web , relatif à un savoir-faire particulier , n' être pas un tâche toujours facile à mener . le outil mettre à le disposition de le internaute ne donner pas entièrement satisfaction . D' un côté le moteur de recherche éprouver un difficulté à faire ressortir clairement le résultat escompter . De le autre côté , le annuaire spécialiser ( type Pages Jaunes ) être tributaire d' un organisation figer , nuire à son efficacité . face à ce constat , nous clr proposer de créer un nouveau moteur spécialiser dans le recherche d' entreprise , associer Web sémantique et géo- localisation . ce approche novateur nécessiter le implémentation d' un ontologie avoir pour objectif le formalisation des connaissance du domaine . ce tâche avoir mettre en évidence le intérêt des structure économique , maintenir par le INSEE , et son utilisation au sein de le ontologie . le nomenclature économique avoir être retenir pour gérer le classification des activité et produit pouvoir être dispenser par le entreprise . le structure des unité administratif , tel que gérer au sein du fichier SIRENE , clr être avérer judicieux pour répondre à le problématique de géo- localisation des entreprise . un opération de désambiguïsation être réaliser en associer à chaque noeud d' activité le mot clé et synonyme lui correspondre . enfin , nous comparer le résultat obtenir par son moteur à celui obtenir par le principal moteur de recherche d' activité géo- localiser en France : le page jaune . Que ce être au niveau de le précision et du rappel , son moteur obtenir un résultat significativement meilleur . 	Apport du Web sémantique dans la réalisation d'un moteur de recherche géo-localisé à usage des entreprises	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage actif d'émotions dans les dialogues Homme-Machine	La prise en compte des émotions dans les interactions Homme-machine permet de concevoir des systèmes intelligents, capables de s'adapter aux utilisateurs. Les techniques de redirection d'appels dans les centres téléphoniques automatisés se basent sur la détection des émotions dans la parole. Les principales difficultés pour mettre en oeuvre de tels systèmes sont l'acquisition et l'étiquetage des données d'apprentissage. Cet article propose l'application de deux stratégies d'apprentissage actif à la détection d'émotions dans des dialogues en interaction homme-machine. L'étude porte sur des données réelles issues de l'utilisation d'un serveur vocal et propose des outils adaptés à la conception de systèmes automatisés de redirection d'appels.	Alexis Bondu, Vincent Lemaire, Barbara Poulain	http://editions-rnti.fr/render_pdf.php?p1&p=1001428	http://editions-rnti.fr/render_pdf.php?p=1001428	1204	fr	fr	@orange-ft.com	apprentissage actif d' émotion dans le dialogue Homme-Machine  le prise en compte des émotion dans le interaction Homme-machine permettre de concevoir un système intelligent , capable de clr adapter aux utilisateur . le technique de redirection d' appel dans le centre téléphonique automatiser clr baser sur le détection des émotion dans le parole . le principal difficulté pour mettre en oeuvre de tel système être le acquisition et le étiquetage des donnée d' apprentissage . ce article proposer le application de deux stratégie d' apprentissage actif à le détection d' émotion dans un dialogue en interaction homme-machine . le étude porter sur un donnée réel issir de le utilisation d' un serveur vocal et proposer un outil adapter à le conception de système automatiser de redirection d' appel . 	Apprentissage actif d'émotions dans les dialogues Homme-Machine	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage semi-supervisé de fonctions d'ordonnancement	Nous présentons dans cet article un algorithme inductif semi-supervisé pour la tâche d'ordonnancement bipartite. Les algorithmes semi-supervisés proposés jusqu'à maintenant ont été étudiés dans le cadre strict de la classification. Récemment des travaux ont été réalisés dans le cadre transductif pour étendre les modèles existants en classification au cadre d'ordonnancement. L'originalité de notre approche est qu'elle est capable d'inférer un ordre sur une base test non- utilisée pendant la phase d'apprentissage, ce qui la rend plus générique qu'une méthode transductive pure. Les résultats empiriques sur la base CACM contenant les titres et les résumés du journal Communications of the Association for Computer Machinery montrent que les données non-étiquetées sont bénéfiques pour l'apprentissage de fonctions d'ordonnancement.	Tuong-Vinh Truong, Massih-Reza Amini	http://editions-rnti.fr/render_pdf.php?p1&p=1001425	http://editions-rnti.fr/render_pdf.php?p=1001425	1205	fr	fr	@lip6.fr	apprentissage semi-supervisé de fonction d' ordonnancement  Nous présenter dans ce article un algorithme inductif semi-supervisé pour le tâche d' ordonnancement bipartite . le algorithme semi-supervisés proposer jusqu' à maintenir avoir être étudier dans le cadre strict de le classification . récemment un travail avoir être réaliser dans le cadre transductif pour étendre le modèle existant en classification au cadre d' ordonnancement . le originalité de son approche être qu' elle être capable d' inférer un ordre sur un base test non- utiliser pendant le phase d' apprentissage , ce qui la rendre plus générique qu' un méthode transductive pur . le résultat empirique sur le base CACM contenir le titre et le résumé du journal Communications of the Association for Computer Machinery montrer que le donnée non- étiqueter être bénéfique pour le apprentissage de fonction d' ordonnancement . 	Apprentissage semi-supervisé de fonctions d'ordonnancement	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage statistique de la topologie d'un ensemble de données étiquetées	Découvrir la topologie d'un ensemble de données étiquetées dans un espace Euclidien peut aider à construire un meilleur système de décision. Dans ce papier, nous proposons un modèle génératif basé sur le graphe de Delaunay de plusieurs prototypes représentant les données étiquetées dans le but d'extraire de ce graphe la topologie des classes.	Pierre Gaillard, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001419	http://editions-rnti.fr/render_pdf.php?p=1001419	1206	fr	fr	@cea.fr, @cea.fr, @utc.fr	apprentissage statistique de le topologie d' un ensemble de donnée étiquetées  découvrir le topologie d' un ensemble de donnée étiqueter dans un espace Euclidien pouvoir aider à construire un meilleur système de décision . Dans ce papier , nous proposer un modèle génératif baser sur le graphe de Delaunay de plusieurs prototype représenter le donnée étiqueter dans le but d' extraire de ce graphe le topologie des classe . 	Apprentissage statistique de la topologie d'un ensemble de données étiquetées	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Approche connexionniste pour l'extraction de profils cas-témoins du cancer du Nasopharynx à partir des données issues d'une étude épidémiologique	Dans cet article, nous présentons un système de découverte de connaissances à partir de données issues d'une étude épidémiologique cas-témoins du cancer du Nasopharynx (NPC). Ces données étant obtenues par une collecte de questionnaires, elles ont d'une part, la particularité d'être qualitatives et, d'autre part, de présenter des valeurs manquantes. Prenant en compte ces deux dernières contraintes, le système que nous proposons suit une démarche d'exploration de données qui consiste à (1) définir une procédure de codage des données qualitatives en présence de valeurs manquantes ; (2) étudier les propriétés de l'algorithme des cartes auto-organisatrices de Kohonen et son adaptation à ce type de données dans un cadre de découverte et de visualisation de groupes homogènes des cas cancer / non-cancer ; (3) post-traiter le resultat de cet algorithme par une classification automatique pour optimiser le nombre de groupes ainsi trouvés, et (4) donner une interprétation sémantique des profils extraits de chaque groupe. L'objectif général de cette étude est d'éclater le profil statistique global de la population étudiée en un ensemble de profils types (cancer ou non-cancer) et d'extraire pour chaque profil l'ensemble de variables explicatives du NPC à partir d'une cartographie bidimensionnelle.	Khalid Benabdeslem, Mustapha Lebbah, Alexandre Aussem, Marilys Corbex	http://editions-rnti.fr/render_pdf.php?p1&p=1001418	http://editions-rnti.fr/render_pdf.php?p=1001418	1207	fr	fr	@univ-lyon1.fr, @limbio-paris13.org, @iarc.fr	approche connexionniste pour le extraction de profil cas-témoins du cancer du Nasopharynx à partir un donnée issir d' un étude épidémiologique  Dans ce article , nous présenter un système de découverte de connaissance à partir de donnée issir d' un étude épidémiologique cas-témoins du cancer du Nasopharynx ( NPC ) . ce donnée être obtenir par un collecte de questionnaire , elles avoir d' un part , le particularité d' être qualitatif et , d' autre part , de présenter un valeur manquant . prendre en compte ce deux dernier contrainte , le système que nous proposer suivre un démarche d' exploration de donnée qui consister à ( 1 ) définir un procédure de codage des donnée qualitatif en présence de valeur manquant ; ( 2 ) étudier le propriété de le algorithme des carte auto- organisatrice de Kohonen et son adaptation à ce type de donnée dans un cadre de découverte et de visualisation de groupe homogène des cas cancer  non- cancer ; ( 3 ) post-traiter le resultat de ce algorithme par un classification automatique pour optimiser le nombre de groupe ainsi trouver , et ( 4 ) donner un interprétation sémantique des profil extrait de chaque groupe . le objectif général de ce étude être d' éclater le profil statistique global de le population étudier en un ensemble de profil type ( cancer ou non- cancer ) et d' extraire pour chaque profil le ensemble de variable explicatif du NPC à partir d' un cartographie bidimensionnelle . 	Approche connexionniste pour l'extraction de profils cas-témoins du cancer du Nasopharynx à partir des données issues d'une étude épidémiologique	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Approche logique pour la réconciliation de références	Le problème de réconciliation de références consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel. Dans cet article, nous étudions ce problème quand le schéma des données est décrit en RDFS étendu par certaines primitives de OWL-DL. Nous décrivons et montrons l'intérêt d'une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma. Ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma. Les premiers résultats ont été obtenus sur des données réelles dans le cadre du projet PICSEL 3 en collaboration avec France Telecom R&D.	Fatiha Saïs, Nathalie Pernelle, Marie-Christine Rousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001449	http://editions-rnti.fr/render_pdf.php?p=1001449	1208	fr	fr	@lri.fr, @imag.fr	approche logique pour le réconciliation de références  le problème de réconciliation de référence consister à décider si deux description provenir de source distinct référer ou non à le même entité du monde réel . Dans ce article , nous étudier ce problème quand le schéma des donnée être décrire en RDFS étendre par certain primitif de OWL-DL . Nous décrire et montrer le intérêt d' un approche logique baser sur un règle de réconciliation qui pouvoir être générer automatiquement à partir un axiome du schéma . ce règle traduire de façon déclaratif le dépendance entre réconciliation qui découler de le sémantique du schéma . le premier résultat avoir être obtenir sur un donnée réel dans le cadre du projet PICSEL 3 en collaboration avec France Telecom R_&_D . 	Approche logique pour la réconciliation de références	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	Calcul et représentation efficace de cubes de données pour une visualisation orientée pixel	Les cubes de données fournissent une aide non négligeable lorsqu'il s'agit d'interroger des entrepôts de données. Un cube de données représente un pré-calcul de toutes les requêtes OLAP et ainsi améliore leur temps de réponses. Les approches proposées jusqu'à présent réduisent les temps de calcul et d'entrée sortie mais leur utilisation reste très coûteuse. D'autres travaux de recherche se sont intéressés à la visualisation de données pour les exploiter de façon interactive.Nous proposons une adaptation de la représentation condensée des cubes de données basée sur le modèle partitionnel. Cette technique nous permet de calculer efficacement un cube de données et de représenter les liens entre les données pour la visualisation. La visualisation proposée dans cet article est basée sur des techniques de visualisation orientée pixel et sur des techniques de diagramme de liens entre noeuds pour offrir à la fois une vision globale et locale pour l'exploitation. Cette nouvelle approche utilise d'une part les calculs efficaces de cubes de données et d'autre part les techniques avancées de visualisation.	Noel Novelli, David Auber	http://editions-rnti.fr/render_pdf.php?p1&p=1001362	http://editions-rnti.fr/render_pdf.php?p=1001362	1209	fr	fr	@lif.univ-mrs.fr, @labri.fr	calcul et représentation efficace de cube de donnée pour un visualisation orienter pixel  le cube de donnée fournir un aide non négligeable lorsqu' il clr agir d' interroger un entrepôt de donnée . un cube de donnée représenter un pré-calcul de tout le requête OLAP et ainsi améliorer son temps de réponse . le approche proposer jusqu' à présent réduire le temps de calcul et d' entrée sortir mais son utilisation rester très coûteux . un autre travail de recherche clr être intéresser à le visualisation de donnée pour les exploiter de façon interactif . Nous proposer un adaptation de le représentation condenser des cube de donnée baser sur le modèle partitionnel . ce technique nous permettre de calculer efficacement un cube de donnée et de représenter le lien entre le donnée pour le visualisation . le visualisation proposer dans ce article être baser sur un technique de visualisation orienter pixel et sur un technique de diagramme de lien entre noeud pour offrir à le foi un vision global et local pour le exploitation . ce nouveau approche utiliser d' un part le calcul efficace de cube de donnée et d' autre part le technique avancée de visualisation . 	Calcul et représentation efficace de cubes de données pour une visualisation orientée pixel	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Caractérisation des transitions temporisées dans les logs de conversation de services Web	La connaissance du protocole de conversation d'un service Web est importante pour les utilisateurs et les fournisseurs, car il en modélise le comportement externe ; mais, il n'est souvent pas spécifié lors de la conception. Notre travail s'inscrit dans une thématique d'extraction du protocole de conversation d'un service existant à partir de ses données d'exécution. Nous en étudions un sous-problème important qui est la découverte des transitions temporisées (i.e. les changements d'état liés à des contraintes temporelles). Nous proposons un cadre formel aboutissant à la définition des expirations propres, qui représentent un équivalent dans les logs des transitions temporisées. A notre connaissance, ceci représente la première contribution à la résolution de ce problème.	Didier Devaurs, Fabien De Marchi, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001302	http://editions-rnti.fr/render_pdf.php?p=1001302	1210	fr	fr	@irisa.fr, @liris.cnrs.fr	caractérisation des transition temporiser dans le logs de conversation de service Web  le connaissance du protocole de conversation d' un service Web être important pour le utilisateur et le fournisseur , car il en modéliser le comportement externe ; mais , il n' être souvent pas spécifier lors de le conception . son travail clr inscrire dans un thématique d' extraction du protocole de conversation d' un service existant à partir de son donnée d' exécution . Nous en étudier un sous-problème important qui être le découverte des transition temporiser ( i.e. le changement d' état lier à un contrainte temporel ) . Nous proposer un cadre formel aboutir à le définition des expiration propre , qui représenter un équivalent dans le logs des transition temporiser . A son connaissance , ceci représenter le premier contribution à le résolution de ce problème . 	Caractérisation des transitions temporisées dans les logs de conversation de services Web	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Cartographie de l'organisation : une approche topologique des connaissances	La gestion des connaissances est devenue aujourd'hui un enjeu majeur pour toute organisation. Celle-ci a pour but de capitaliser et de rendre accessible à ses acteurs la connaissance détenue par l'organisation. Cet article s'intéresse particulièrement à la visualisation à deux niveaux de ces connaissances (macroscopique - relatif aux connaissances globales détenues par l'organisation - et microscopique - relatif aux connaissances locales détenues par chaque membre organisationnel). La caractérisation des connaissances détenues par les acteurs repose sur quatre dimensions complémentaires (formelle, conative, cognitive, et socio-cognitive). Les deux types de visualisation proposés s'appuient sur les cartes auto-organisatrices et permettent une navigation dans différentes représentations des connaissances de l'organisation.	Marc Boyer, Marie-Françoise Canut, Max Chevalier, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1001437	http://editions-rnti.fr/render_pdf.php?p=1001437	1211	fr	fr	@iut-tlse3.fr, @iut-blagnac.fr, @iut-blagnac.fr, @irit.fr, @irit.fr	cartographie de le organisation : un approche topologique des connaissances  le gestion des connaissance être devenir aujourd' hui un enjeu majeur pour tout organisation . Celle _-ci avoir pour but de capitaliser et de rendre accessible à son acteur le connaissance détenir par le organisation . ce article clr intéresser particulièrement à le visualisation à deux niveau de ce connaissance ( macroscopique - relatif aux connaissance global détenir par le organisation - et microscopique - relatif aux connaissance local détenir par chaque membre organisationnel ) . le caractérisation des connaissance détenir par le acteur reposer sur quatre dimension complémentaire ( formel , conatif , cognitif , et socio- cognitif ) . le deux type de visualisation proposer clr appuyer sur le carte auto- organisatrice et permettre un navigation dans différent représentation des connaissance de le organisation . 	Cartographie de l'organisation : une approche topologique des connaissances	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Choix des conclusions et validation des règles issues d'arbres de classification	Cet article traite de la validation de règles dans un contexte de ciblage où il s'agit de déterminer les profils type des différentes valeurs de la variable à prédire. Les concepts de l'analyse statistique implicative fondée sur la différence entre nombre observé de contre-exemples et nombre moyen que produirait le hasard, s'avèrent particulièrement bien adaptés à ce contexte. Le papier montre comment les notions d'indice et d'intensité d'implication de Gras s'appliquent aux règles produites par les arbres de décision et présente des alternatives inspirées de résidus utilisés en modélisation de tables de contingence. Nous discutons ensuite sur un jeu de données réelles deux usages de ces indicateurs de force d'implication pour les règles issues d'arbres. Il s'agit d'une part de l'évaluation individuelle des règles, et d'autre part de leur utilisation comme critère pour le choix de la conclusion de la règle.	Vincent Pisetta, Gilbert Ritschard, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001424	http://editions-rnti.fr/render_pdf.php?p=1001424	1212	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr, @unige.ch	choix des conclusion et validation des règle issu d' arbre de classification  ce article traire de le validation de règle dans un contexte de ciblage où il clr agir de déterminer le profil type des différent valeur de le variable à prédire . le concept de le analyse statistique implicative fonder sur le différence entre nombre observer de contre-exemple et nombre moyen que produire le hasard , clr avérer particulièrement bien adapter à ce contexte . le papier montrer comment le notion d' indice et d' intensité d' implication de Gras clr appliquer aux règle produire par le arbre de décision et présenter un alternative inspirer de résidu utiliser en modélisation de table de contingence . Nous discuter ensuite sur un jeu de donnée réel deux usage de ce indicateur de force d' implication pour le règle issu d' arbre . Il clr agir d' un part de le évaluation individuel des règle , et d' autre part de son utilisation comme critère pour le choix de le conclusion de le règle . 	Choix des conclusions et validation des règles issues d'arbres de classification	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classement des fragments de documents XML par une méthode d'aide à la décision	Vu l'accroissement constant du volume d'information accessible en ligne sous format XML, il devient primordial de proposer des modèles adaptés à la recherche d'information dans les documents XML. Tandis que la recherche d'information classique repose sur l'indexation du contenu des documents, la recherche d'information dans les documents XML tente d'améliorer la qualité des résultats en tirant profit de la sémantique véhiculée par la structure des documents. Dans cet article, nous présentons une méthode de classement des items (éléments XML) retournés lors d'une recherche dans une collection de documents XML. Le classement repose sur la prise en compte d'un ensemble de critères discriminants. La particularité de notre approche réside dans la façon dont nous les utilisons : Nous employons une méthode décisionnelle pour classer les items en les comparant deux-à-deux là où en général une fonction de scoring globale est utilisée.	Faiza Abbaci, Pascal Francq	http://editions-rnti.fr/render_pdf.php?p1&p=1001393	http://editions-rnti.fr/render_pdf.php?p=1001393	1213	fr	fr	@ulb.ac.be, @ulb.ac.be	classement des fragment de document XML par un méthode d' aide à le décision  Vu le accroissement constant du volume d' information accessible en ligne sous format XML , il devenir primordial de proposer un modèle adapter à le recherche d' information dans le document XML . Tandis que le recherche d' information classique reposer sur le indexation du contenu des document , le recherche d' information dans le document XML tenter d' améliorer le qualité des résultat en tirer profit de le sémantique véhiculer par le structure des document . Dans ce article , nous présenter un méthode de classement des item ( élément XML ) retourner lors d' un recherche dans un collection de document XML . le classement reposer sur le prise en compte d' un ensemble de critère discriminant . le particularité de son approche résider dans le façon dont nous le utilisons : Nous employer un méthode décisionnel pour classer le item en les comparer deux-à-deux là où en général un fonction de scoring global être utiliser . 	Classement des fragments de documents XML par une méthode d'aide à la décision	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification de fonctions continues à l'aide d'une distribution et d'une densité définies dans un espace de dimension infinie	Il n'est pas rare que des données individu soient caractérisées par une distribution continue et non une seule valeur. Ces données fonctionnelles peuvent être utilisées pour classer les individus. Une solution élémentaire est de réduire les distributions à leurs moyennes et variances. Une solution plus riche a été proposée par Diday (2002) et mise en oeuvre par Vrac et al. (2001) et Cuvelier et Noirhomme-Fraiture (2005). Elle utilise des points de coupures dans les distributions et modélise ces valeurs conjointes par une distribution multidimensionnelle construite à l'aide d'une copule. Nous avons montré dans un précédent travail que, si cette technique apporte de bons résultats, la qualité de la classification dépend néanmoins du nombre et de l'emplacement des coupures. Les questions du choix du nombre et de l'emplacement des coupures restaient des questions ouvertes. Nous proposons une solution à ces questions, lorsque le nombre de coupures tend vers l'infini, en proposant une nouvelle distribution de probabilité adaptée à l'espace de dimension infinie que forment les données fonctionnelles. Nous proposons aussi une densité de probabilité adaptée à la nature de cette distribution en utilisant la dérivée directionnelle de Gâteaux. La direction choisie pour cette dérivée est celle de la dispersion des fonctions à classer. Les résultats sont encourageants et offrent des perspectives multiples dans tous les domaines où une distribution de données fonctionnelles est nécessaire.	Etienne Cuvelier, Monique Noirhomme-Fraiture	http://editions-rnti.fr/render_pdf.php?p1&p=1001456	http://editions-rnti.fr/render_pdf.php?p=1001456	1214	fr	fr	@fundp.ac.be	classification de fonction continu à le aide d' un distribution et d' un densité définir dans un espace de dimension infinie  Il n' être pas rare que un donnée individu être caractériser par un distribution continu et non un seul valeur . ce donnée fonctionnel pouvoir être utiliser pour classer le individu . un solution élémentaire être de réduire le distribution à son moyen et variance . un solution plus riche avoir être proposer par Diday ( 2002 ) et mettre en oeuvre par Vrac et al. ( 2001 ) et Cuvelier et Noirhomme-Fraiture ( 2005 ) . Elle utiliser un point de coupure dans le distribution et modéliser ce valeur conjoindre par un distribution multidimensionnel construire à le aide d' un copule . Nous avoir montrer dans un précédent travail que , si ce technique apporter un bon résultat , le qualité de le classification dépendre néanmoins du nombre et de le emplacement des coupure . le question du choix du nombre et de le emplacement des coupure rester un question ouvert . Nous proposer un solution à ce question , lorsque le nombre de coupure tendre vers le infini , en proposer un nouveau distribution de probabilité adapter à le espace de dimension infini que former le donnée fonctionnel . Nous proposer aussi un densité de probabilité adapter à le nature de ce distribution en utiliser le dérivée directionnel de Gâteaux . le direction choisir pour ce dérivée être celui de le dispersion des fonction à classer . le résultat être encourageant et offrir un perspective multiple dans tout le domaine où un distribution de donnée fonctionnel être nécessaire . 	Classification de fonctions continues à l'aide d'une distribution et d'une densité définies dans un espace de dimension infinie	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification de grands ensembles de données avec un nouvel algorithme de SVM	Le nouvel algorithme de boosting de Least-Squares Support Vector Machine (LS-SVM) que nous présentons vise à la classification de très grands ensembles de données sur des machines standard. Les méthodes de SVM et de noyaux permettent d'obtenir de bons résultats en ce qui concerne la précision mais la tâche d'apprentissage pour de grands ensembles de données demande une grande capacité mémoire et un temps relativement long. Nous présentons une extension de l'algorithme de LS-SVM proposé par Suykens et Vandewalle pour le boosting de LS-SVM. A cette fin, nous avons ajouté un terme de régularisation de Tikhonov et utilisé la formule de Sherman-Morrison-Woodbury pour traiter des ensembles de données ayant un grand nombre de dimensions. Nous l'avons ensuite étendu par application du boosting de LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances de l'algorithme sont évaluées sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et NDC sur une machine standard (PC-P4, 3GHz, 512 Mo RAM).	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001466	http://editions-rnti.fr/render_pdf.php?p=1001466	1215	fr	fr	@lri.fr, @esiea-ouest.fr	classification de grand ensemble de donnée avec un nouveau algorithme de SVM  le nouveau algorithme de boosting de Least-Squares Support Vector Machine ( LS-SVM ) que nous présenter viser à le classification de très grand ensemble de donnée sur un machine standard . le méthode de SVM et de noyau permettre d' obtenir un bon résultat en ce qui concerner le précision mais le tâche d' apprentissage pour un grand ensemble de donnée demander un grand capacité mémoire et un temps relativement long . Nous présenter un extension de le algorithme de LS-SVM proposer par Suykens et Vandewalle pour le boosting de LS-SVM . A ce fin , nous avoir ajouter un terme de régularisation de Tikhonov et utiliser le formule de Sherman-Morrison-Woodbury pour traiter un ensemble de donnée avoir un grand nombre de dimension . Nous l' avoir ensuite étendre par application du boosting de LS-SVM afin de traiter un donnée avoir simultanément un grand nombre d' individu et de dimension . le performance de le algorithme être évaluer sur le ensemble de donnée de le UCI , Twonorm , Ringnorm , Reuters- 21578 et NDC sur un machine standard ( PC-P4 , 3GHz , 512 Mo RAM ) . 	Classification de grands ensembles de données avec un nouvel algorithme de SVM	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification supervisée de séquences biologiques basée sur les motifs et les matrices de substitution	La classification des séquences biologiques est l'un des importants défis ouverts dans la bioinformatique, tant pour les séquences protéiques que pour les séquences nucléiques. Cependant, la présence de ces données sous la forme de chaînes de caractères ne permet pas de les traiter par les outils standards de classification supervisée, qui utilisent souvent le format relationnel. Pour remédier à ce problème de codage, plusieurs travaux se sont basés sur l'extraction des motifs pour construire une nouvelle représentation des séquences biologiques sous la forme d'un tableau binaire. Nous décrivons une nouvelle approche qui étend les méthodes précédents par l'utilisation de matrices de substitution dans les cas des séquences protéiques. Nous présentons ensuite une étude comparative qui prend en compte l'effet de chaque méthode sur la précision de la classification mais aussi le nombre d'attributs générés et le temps de calcul.	Rabie Saidi, Mondher Maddouri, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001409	http://editions-rnti.fr/render_pdf.php?p=1001409	1216	fr	fr	@univ-artois.fr, @fsegt.mu.tn, @univ-artois.fr	classification superviser de séquence biologique baser sur le motif et le matrice de substitution  le classification des séquence biologique être le un des important défi ouvrir dans le bioinformatique , tant pour le séquence protéique que pour le séquence nucléiques . cependant , le présence de ce donnée sous le forme de chaîne de caractère ne permettre pas de les traiter par le outil standard de classification superviser , qui utiliser souvent le format relationnel . Pour remédier à ce problème de codage , plusieurs travail clr être baser sur le extraction des motif pour construire un nouveau représentation des séquence biologique sous le forme d' un tableau binaire . Nous décrire un nouveau approche qui étendre le méthode précédent par le utilisation de matrice de substitution dans le cas des séquence protéique . Nous présenter ensuite un étude comparatif qui prendre en compte le effet de chaque méthode sur le précision de le classification mais aussi le nombre d' attribut générer et le temps de calcul . 	Classification supervisée de séquences biologiques basée sur les motifs et les matrices de substitution	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Clustering : from model-based approaches to heuristic algorithms	Les méthodes du 'clustering' ont pour but de diviser un ensemble (large) d'objets dans un petit nombre de groupes homogènes (clusters), basé sur des données relevées ou observées qui décrivent les (dis-)similarités qui existent entre les objets - en espérant que ces clusters soient utiles pour l'application concernée. Il existe une multitude d'approches, et cette contribution présente quelques-unes qui sont les plus importantes ou actuelles.	Hans-Hermann Bock	http://editions-rnti.fr/render_pdf.php?p1&p=1001287	http://editions-rnti.fr/render_pdf.php?p=1001287	1217	en	fr	@stochastik.rwth-aachen.de	le méthode du ' clustering ' avoir pour but de diviser un ensemble ( large ) d' objet dans un petit nombre de groupe homogène ( clusters ) , baser sur un donnée relever ou observer qui décrire le ( dis- ) similarité qui exister entre le objet - en espérer que ce clusters être utile pour le application concerner . Il exister un multitude d' approche , et ce contribution présent quelques-uns qui être le plus important ou actuel . 	Clustering : from model-based approaches to heuristic algorithms	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Combinaison des cartes topologiques mixtes et des machines à vecteurs de support : une application pour la prédiction de perte de poids chez les obèses	Cet article présente un modèle pour aborder les problèmes de classement difficiles, en particulier dans le domaine médical. Ces problèmes ont souvent la particularité d'avoir des taux d'erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problèmes, nous proposons d'utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. La combinaison de ces deux modèles permet non seulement d'améliorer la visualisation des données mais aussi en les performances en généralisation. Ce modèle (CT-SVM) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation expérimentale de notre modèle (CT-SVM), nous avons utilisé quatre jeux de données. La première base est un extrait d'une grande base médicale sur l'étude de l'obésité réalisée à l'Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature.	Mohamed Ramzi Temanni, Mustapha Lebbah, Christine Poitou-Bernert, Karine Clément, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001298	http://editions-rnti.fr/render_pdf.php?p=1001298	1218	fr	fr	@limbio-paris13.org, @htp.aphp.fr	combinaison des carte topologique mixte et un machine à vecteur de support : un application pour le prédiction de perte de poids chez le obèses  ce article présenter un modèle pour aborder le problème de classement difficile , en particulier dans le domaine médical . ce problème avoir souvent le particularité d' avoir un taux d' erreur en généralisation très élever et ce quel que être le méthode utiliser . Pour ce genre de problème , nous proposer d' utiliser un modèle de classement combiner le modèle de partitionnement un carte topologique mixte et le machine à vecteur de support ( SVM ) . le modèle non superviser être dédier à le visualisation et au partitionnement un donnée composer de variable quantitatif et qualitatif . le deuxième modèle superviser , être dédier au classement . le combinaison de ce deux modèle permettre non seulement d' améliorer le visualisation des donnée mais aussi en le performance en généralisation . ce modèle ( CT-SVM ) consister à entraîner un carte auto- organisatrice pour construire un partition organiser des donnée , constituer de plusieurs sous-ensemble qui aller servir à reformuler le problème de classement initial en sous-problème de classement . Pour chaque sous-ensemble , on entraîner un classeur SVM spécifique . Pour le validation expérimental de son modèle ( CT-SVM ) , nous avoir utiliser quatre jeu de donnée . le premier base être un extraire d' un grand base médical sur le étude de le obésité réaliser à le Hôpital Hôtel-Dieu de Paris , et le trois dernier base être issir de le littérature . 	Combinaison des cartes topologiques mixtes et des machines à vecteurs de support : une application pour la prédiction de perte de poids chez les obèses	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction coopérative de carte de thèmes : vers une modélisation de l'activité socio-sémantique	Nous présentons dans cette contribution un cadre de modélisation recourant conjointement au modèle Hypertopic (Cahier et al., 2004) pour la représentation des connaissances de domaine et au modèle SeeMe (Herrmann et al., 1999) pour la représentation de l'activité. Ces deux approches apparaissent complémentaires, et nous montrons comment elles peuvent être combinées, pour mieux ancrer, sur les plans formel et méthodologique, les approches de cartographie collective des connaissances.	L'Hédi Zaher, Jean-Pierre Cahier, Christophe Lejeune, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1001304	http://editions-rnti.fr/render_pdf.php?p=1001304	1219	fr	fr	@utt.fr	construction coopératif de carte de thème : vers un modélisation de le activité socio- sémantique  Nous présenter dans ce contribution un cadre de modélisation recourir conjointement au modèle Hypertopic ( Cahier et al. , 2004 ) pour le représentation des connaissance de domaine et au modèle SeeMe ( Herrmann et al. , 1999 ) pour le représentation de le activité . ce deux approche apparaître complémentaire , et nous montrer comment elles pouvoir être combiner , pour mieux ancrer , sur le plan formel et méthodologique , le approche de cartographie collectif des connaissance . 	Construction coopérative de carte de thèmes : vers une modélisation de l'activité socio-sémantique	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction d'ontologie à partir de corpus de textes	"Cet article présente une méthode semi-automatique de construction d'ontologie à partir de corpus de textes sur un domaine spécifique. Cette méthode repose en premier lieu sur un analyseur syntaxique partiel et robuste des textes, et en second lieu, sur l'utilisation de l'analyse formelle de concepts ""FCA"" pour la construction de classes d'objets en un treillis de Galois. La construction de l'ontologie, c'est à dire d'une hiérarchie de concepts et d'instances, est réalisée par une transformation formelle de la structure du treillis. Cette méthode s'applique dans le domaine de l'astronomie."	Amedeo Napoli, Yannick Toussaint, Rokia Bendaoud	http://editions-rnti.fr/render_pdf.php?p1&p=1001364	http://editions-rnti.fr/render_pdf.php?p=1001364	1220	fr	fr	@loria.fr	construction d' ontologie à partir de corpus de textes  " ce article présenter un méthode semi-automatique de construction d' ontologie à partir de corpus de texte sur un domaine spécifique . . ce méthode reposer en premier lieu sur un analyseur syntaxique partiel et robuste des texte , et en second lieu , sur le utilisation de le analyse formel de concept " " FCA " " pour le construction de classe d' objet en un treillis de Galois . . le construction de le ontologie , c' être à dire d' un hiérarchie de concept et d' instance , être réaliser par un transformation formel de le structure du treillis . . ce méthode clr appliquer dans le domaine de le astronomie . " 	Construction d'ontologie à partir de corpus de textes	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction et analyse de résumés de données évolutives : application aux données d'usage du Web	La manière dont une visite est réalisée sur un site Web peut changer en raison de modifications liées à la structure et au contenu du site lui-même, ou bien en raison du changement de comportement de certains groupes d'utilisateurs ou de l'émergence de nouveaux comportements. Ainsi, les modèles associés à ces comportements dans la fouille d'usage du Web doivent être mis à jour continuellement afin de mieux refléter le comportement actuel des internautes. Une solution, proposée dans cet article, est de mettre à jour ces modèles à l'aide des résumés obtenus par une approche évolutive des méthodes de classification.	Alzennyr Da Silva, Yves Lechevallier, Fabrice Rossi, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001434	http://editions-rnti.fr/render_pdf.php?p=1001434	1221	fr	fr	@inria.fr, @cin.ufpe.br	construction et analyse de résumé de donnée évolutif : application aux donnée d' usage du Web  le manière dont un visite être réaliser sur un site Web pouvoir changer en raison de modification lier à le structure et au contenu du site lui-même , ou bien en raison du changement de comportement de certain groupe d' utilisateur ou de le émergence de nouveau comportement . ainsi , le modèle associer à ce comportement dans le fouille d' usage du Web devoir être mettre à jour continuellement afin de mieux refléter le comportement actuel des internaute . un solution , proposer dans ce article , être de mettre à jour ce modèle à le aide des résumé obtenir par un approche évolutif des méthode de classification . 	Construction et analyse de résumés de données évolutives : application aux données d'usage du Web	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction incrémentale et visualisation de graphes de voisinage par des fourmis artificielles	Cet article décrit un nouvel algorithme incrémental nommé AntGraph pour la construction de graphes de voisinage. Il s'inspire du comportement d'autoassemblage observé chez des fourmis réelles où ces dernières se fixent progressivement à un support fixe puis successivement aux fourmis déjà fixées afin de créer une structure vivante. Nous utilisons ainsi une approche à base de fourmis artificielles où chaque fourmi représente une donnée. Nous indiquons comment ce comportement peut être utilisé pour construire de manière incrémentale un graphe à partir d'une mesure de similarité entre les données. Nous montrons finalement que notre algorithme obtient de meilleurs résultats en comparaison avec le graphe de Voisins Relatifs, notamment en terme de temps de calcul.	Julien Lavergne, Hanane Azzag, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001323	http://editions-rnti.fr/render_pdf.php?p=1001323	1222	fr	fr	@univ-tours.fr, @univ-paris13.fr, @ceries-lab.com	construction incrémentale et visualisation de graphe de voisinage par un fourmi artificielles  ce article décrire un nouveau algorithme incrémental nommer AntGraph pour le construction de graphe de voisinage . Il clr inspirer du comportement d' autoassemblage observer chez un fourmi réel où ce dernier clr fixer progressivement à un support fixe puis successivement aux fourmi déjà fixer afin de créer un structure vivant . Nous utiliser ainsi un approche à base de fourmi artificiel où chaque fourmi représenter un donner . Nous indiquer comment ce comportement pouvoir être utiliser pour construire de manière incrémentale un graphe à partir d' un mesure de similarité entre le donnée . Nous montrer finalement que son algorithme obtenir un meilleur résultat en comparaison avec le graphe de Voisins Relatifs , notamment en terme de temps de calcul . 	Construction incrémentale et visualisation de graphes de voisinage par des fourmis artificielles	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Découverte de chroniques à partir de séquences d'événements pour la supervision de processus dynamiques	Ce papier adresse le problème de la découverte de connaissances temporelles à partir des données datées, générées par le système de supervision d'un processus de fabrication. Par rapport aux approches existantes qui s'appliquent directement aux données, notre méthode d'extraction des connaissances se base sur un modèle global construit à partir des données. L'approche de modélisation adoptée, dite stochastique, considère les données datées comme une séquence d'occurrences de classes d'événements discrets. Cette séquence est représentée sous les formes duales d'une chaîne de Markov homogène et d'une superposition de processus de Poisson. L'algorithme proposé, appelé BJT4R, permet d'identifier les motifs séquentiels, les plus probables entre deux classes d'événements discrets et les représentent sous la forme de modèles de chroniques. Ce papier présente les premiers résultats de l'application de cet algorithme sur des données générées par un processus de fabrication de semi-conducteur d'un site de production du groupe STMicroelectronics.	Nabil Benayadi, Marc Le Goc, Philippe Bouché	http://editions-rnti.fr/render_pdf.php?p1&p=1001389	http://editions-rnti.fr/render_pdf.php?p=1001389	1223	fr	fr	@lsis.org	découverte de chronique à partir de séquence d' événement pour le supervision de processus dynamiques  ce papier adresser le problème de le découverte de connaissance temporel à partir un donnée dater , générer par le système de supervision d' un processus de fabrication . Par rapport aux approche existant qui clr appliquer directement aux donnée , son méthode d' extraction des connaissance clr baser sur un modèle global construire à partir un donnée . le approche de modélisation adopter , dire stochastique , considérer le donnée dater comme un séquence d' occurrence de classe d' événement discret . ce séquence être représenter sous le forme dual d' un chaîne de Markov homogène et d' un superposition de processus de Poisson . le algorithme proposer , appeler BJT4R , permettre d' identifier le motif séquentiel , le plus probable entre deux classe d' événement discret et les représenter sous le forme de modèle de chronique . ce papier présenter le premier résultat de le application de ce algorithme sur un donnée générer par un processus de fabrication de semi-conducteur d' un site de production du groupe STMicroelectronics . 	Découverte de chroniques à partir de séquences d'événements pour la supervision de processus dynamiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Des fonctions d'oubli intelligentes dans les entrepôts de données	"Les entrepôts de données stockent des quantités de données de plus en plus massives et arrivent vite à saturation. Un langage de spécifications de fonctions d'oubli est défini pour résoudre ce problème. Dans le but d'offrir la possibilité d'effectuer des analyses sur l'historique des données, les spécifications définissent des résumés par agrégation et par échantillonnage à conserver parmi les données à ""oublier"". Cette communication présente le langage de spécifications ainsi que les principes et les algorithmes pour assurer de façon mécanique la gestion des fonctions d'oubli."	Aliou Boly, Sabine Goutier, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001380	http://editions-rnti.fr/render_pdf.php?p=1001380	1224	fr	fr	@enst.fr, @enst.fr, @edf.fr, @edf.fr	un fonction d' oubli intelligent dans le entrepôt de données  " le entrepôt de donnée stocker un quantité de donnée de plus en plus massif et arriver vite à saturation . . un langage de spécification de fonction d' oubli être définir pour résoudre ce problème . . Dans le but d' offrir le possibilité d' effectuer un analyse sur le historique des donnée , le spécification définir un résumé par agrégation et par échantillonnage à conserver parmi le donnée à " " oublier " " . . ce communication présenter le langage de spécification ainsi que le principe et le algorithme pour assurer de façon mécanique le gestion des fonction d' oubli . " 	Des fonctions d'oubli intelligentes dans les entrepôts de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Détermination du niveau de consommation des abonnés en téléphonie mobile par la théorie des ensembles flous	La détermination du niveau de consommation chez les clients est essentielle pour tout objectif de segmentation stratégique et de churn. Nous présentons sur un cas réel l'utilisation de la théorie des ensembles flous pour la définition d'une fonction d'appartenance permettant d'évaluer, de manière précise, le niveau de consommation, des abonnés en téléphonie mobile.	Rachid El Meziane, Ilham Berrada, Ismail Kassou, Karim Baïna	http://editions-rnti.fr/render_pdf.php?p1&p=1001378	http://editions-rnti.fr/render_pdf.php?p=1001378	1225	fr	fr	@ensias.ma	détermination du niveau de consommation des abonné en téléphonie mobile par le théorie des ensemble flous  le détermination du niveau de consommation chez le client être essentiel pour tout objectif de segmentation stratégique et de churn . Nous présenter sur un cas réel le utilisation de le théorie des ensemble flou pour le définition d' un fonction d' appartenance permettre d' évaluer , de manière précis , le niveau de consommation , un abonné en téléphonie mobile . 	Détermination du niveau de consommation des abonnés en téléphonie mobile par la théorie des ensembles flous	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Ensemble prédicteur fondé sur les cartes auto-organisatrices adapté aux données volumineuses	Le stockage massif des données noie l'information pertinente et engendre des problèmes théoriques liés à la volumétrie des données disponibles. Ces problèmes dégradent la capacité prédictive des algorithmes d'extraction des connaissances à partir des données. Dans cet article, nous proposons une méthodologie adaptée à la représentation et à la prédiction des données volumineuses. A cette fin, suite à un partitionnement des attributs, des groupes d'attributs non-corrélés sont créés qui permettent de contourner les problèmes liés aux espaces de grandes dimensions. Un Ensemble est alors mis en place, apprenant chaque groupe par une carte auto-organisatrice. Outre la prédiction, ces cartes ont pour objectif une représentation pertinente des données. Enfin, la prédiction est réalisée par un vote des différentes cartes. Une expérimentation est menée qui confirme le bien-fondé de cette approche.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001422	http://editions-rnti.fr/render_pdf.php?p=1001422	1226	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	ensemble prédicteur fonder sur le carte auto- organisatrice adapter aux donnée volumineuses  le stockage massif des donnée noyer le information pertinent et engendrer un problème théorique lier à le volumétrie des donnée disponible . ce problème dégrader le capacité prédictif des algorithme d' extraction des connaissance à partir un donnée . Dans ce article , nous proposer un méthodologie adapter à le représentation et à le prédiction des donnée volumineux . A ce fin , suite à un partitionnement des attribut , un groupe d' attribut non- corréler être créer qui permettre de contourner le problème lier aux espace de grand dimension . un ensemble être alors mettre en place , apprendre chaque groupe par un carte auto- organisatrice . Outre le prédiction , ce carte avoir pour objectif un représentation pertinent des donnée . enfin , le prédiction être réaliser par un vote des différent carte . un expérimentation être mener qui confirmer le bien-fondé de ce approche . 	Ensemble prédicteur fondé sur les cartes auto-organisatrices adapté aux données volumineuses	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	Evaluation d'une approche de classification conceptuelle	L'objectif de ce travail est d'évaluer la perte d'information au sens de l'inertie entre des méthodes de partitionnement ou de classification hiérarchiques et une approche de classification conceptuelle. Nous voulons répondre à la question suivante : l'aspect simpliste du processus monothétique d'une méthode conceptuelle implique-t-il des partitions de moins bonne qualité au sens du critère de l'inertie ? Nous proposons de réaliser cette expérience sur 6 bases de l'UCI, trois de ces bases sont des tableaux de données quantitatives, les trois autres sont des tableaux de données qualitatives.	Marie Chavent, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001455	http://editions-rnti.fr/render_pdf.php?p=1001455	1227	fr	fr	@math.u-bordeaux1.fr, @inria.fr	Evaluation d' un approche de classification conceptuelle  le objectif de ce travail être d' évaluer le perte d' information au sens de le inertie entre un méthode de partitionnement ou de classification hiérarchique et un approche de classification conceptuel . Nous vouloir répondre à le question suivant : le aspect simpliste du processus monothétique d' un méthode conceptuel impliquer -t-il un partition de moins bon qualité au sens du critère de le inertie ? Nous proposer de réaliser ce expérience sur 6 base de le UCI , trois de ce base être un tableau de donnée quantitatif , le trois autre être un tableau de donnée qualitatif . 	Evaluation d'une approche de classification conceptuelle	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Evaluation supervisée de métrique : application à la préparation de données séquentielles	De nos jours, le statisticien n'a plus nécessairement le contrôle sur la récolte des données. Le besoin d'une analyse statistique vient dans un second temps, une fois les données récoltées. Par conséquent, un travail est à fournir lors de la phase de préparation des données afin de passer d'une représentation informatique à une représentation statistique adaptée au problème considéré. Dans cet article, nous étudions un procédé de sélection d'une bonne représentation en nous basant sur des travaux antérieurs. Nous proposons un protocole d'évaluation de la pertinence d'une représentation par l'intermédiaire d'une métrique, dans le cas de la classification supervisée. Ce protocole exploite une méthode de classification non paramétrique régularisée, garantissant l'automaticité et la fiabilité de l'évaluation. Nous illustrons le fonctionnement et les apports de ce protocole par un problème réel de préparation de données de consommation téléphonique. Nous montrons également la fiabilité et l'interprétabilité des décisions qui en résultent.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001392	http://editions-rnti.fr/render_pdf.php?p=1001392	1228	fr	fr	@francetelecom.com, @francetelecom.com	Evaluation superviser de métrique : application à le préparation de donnée séquentielles  De son jour , le statisticien n' avoir plus nécessairement le contrôle sur le récolte des donnée . le besoin d' un analyse statistique venir dans un second temps , un foi le donnée récolter . Par conséquent , un travail être à fournir lors de le phase de préparation des donnée afin de passer d' un représentation informatique à un représentation statistique adapter au problème considérer . Dans ce article , nous étudier un procédé de sélection d' un bon représentation en nous baser sur un travail antérieur . Nous proposer un protocole d' évaluation de le pertinence d' un représentation par le intermédiaire d' un métrique , dans le cas de le classification superviser . ce protocole exploiter un méthode de classification non paramétrique régulariser , garantir le automaticité et le fiabilité de le évaluation . Nous illustrer le fonctionnement et le apport de ce protocole par un problème réel de préparation de donnée de consommation téléphonique . Nous montrer également le fiabilité et le interprétabilité des décision qui en résulter . 	Evaluation supervisée de métrique : application à la préparation de données séquentielles	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Evolution de l'ontologie et gestion des annotations sémantiques inconsistantes	Les ontologies et les annotations sémantiques sont deux composants importants dans un système de gestion des connaissances basé sur le Web sémantique. Dans l'environement dynamique et distribué du Web sémantique, les ontologies et les annotations pourraient être changées pour s'adapter à l'évolution de l'organisation concernée. Ces changements peuvent donc entraîner des inconsistances à détecter et traiter. Dans cet article, nous nous focalisons principalement sur l'évolution des annotations sémantiques en soulignant le contexte où les modifications de l'ontologie entraînent des inconsistances sur ces annotations. Nous présentons une approche basée sur des règles permettant de détecter les inconsistances dans les annotations sémantiques devenues obsolètes par rapport à l'ontologie modifiée. Nous décrivons aussi les stratégies d'évolution nécessaires pour guider le processus de résolution de ces inconsistances grâce à des règles correctives.	Phuc-Hiep Luong, Rose Dieng-Kuntz, Alain Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001450	http://editions-rnti.fr/render_pdf.php?p=1001450	1229	fr	fr	@inria.fr, @auf.org	Evolution de le ontologie et gestion des annotation sémantique inconsistantes  le ontologie et le annotation sémantique être deux composant important dans un système de gestion des connaissance baser sur le web sémantique . Dans le environement dynamique et distribuer du Web sémantique , le ontologie et le annotation pouvoir être changer pour clr adapter à le évolution de le organisation concerner . ce changement pouvoir donc entraîner un inconsistance à détecter et traiter . Dans ce article , nous clr focaliser principalement sur le évolution des annotation sémantique en souligner le contexte où le modification de le ontologie entraîner un inconsistance sur ce annotation . Nous présenter un approche baser sur un règle permettre de détecter le inconsistance dans le annotation sémantique devenir obsolète par rapport à le ontologie modifier . Nous décrire aussi le stratégie d' évolution nécessaire pour guider le processus de résolution de ce inconsistance grâce à un règle correctif . 	Evolution de l'ontologie et gestion des annotations sémantiques inconsistantes	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extension sémantique du modèle de similarité basé sur la proximité floue des termes	Le modèle flou de proximité repose sur l'hypothèse que plus les occurrences des termes d'une requête se trouvent proches dans un document, plus ce dernier est pertinent. Cette mesure floue est très avantageuse dans le traitement des documents à textes courts, toutefois elle ne tient pas compte de la sémantique des termes. Nous présentons dans cet article l'intégration d'une métrique conceptuelle au modèle de proximité floue des termes pour la formalisation de notre propre modèle.	Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1001399	http://editions-rnti.fr/render_pdf.php?p=1001399	1230	fr	fr	@cstb.fr, @math-info.univ	extension sémantique du modèle de similarité baser sur le proximité flou des termes  le modèle flou de proximité reposer sur le hypothèse que plus le occurrence des terme d' un requête clr trouver proche dans un document , plus ce dernier être pertinent . ce mesure flou être très avantageux dans le traitement des document à texte court , toutefois elle ne tenir pas compte de le sémantique des terme . Nous présenter dans ce article le intégration d' un métrique conceptuel au modèle de proximité flou des terme pour le formalisation de son propre modèle . 	Extension sémantique du modèle de similarité basé sur la proximité floue des termes	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction d'entités dans des collections évolutives	Nous nous intéressons à l'extraction d'entités nommées avec comme but d'exploiter un ensemble de rapports pour en extraire une liste de partenaires. À partir d'une liste initiale, nous utilisons un premier ensemble de documents pour identifier des schémas de phrase qui sont ensuite validés par apprentissage supervisé sur des documents annotés pour en mesurer l'efficacité avant d'être utilisés sur l'ensemble des documents à explorer. Cette approche est inspirée de celle utilisée pour l'extraction de données dans les documents semi-structurés (wrappers) et ne nécessite pas de ressources linguistiques particulières ni de larges collections de tests. Notre collection de documents évoluant annuellement, nous espérons de plus une amélioration de notre extraction dans le temps.	Thierry Despeyroux, Eduardo Fraschini, Anne-Marie Vercoustre	http://editions-rnti.fr/render_pdf.php?p1&p=1001433	http://editions-rnti.fr/render_pdf.php?p=1001433	1231	fr	fr	@inria.fr	extraction d' entité dans un collection évolutives  Nous nous intéresser à le extraction d' entité nommer avec comme but d' exploiter un ensemble de rapport pour en extraire un liste de partenaire . À partir d' un liste initial , nous utiliser un premier ensemble de document pour identifier un schéma de phrase qui être ensuite valider par apprentissage superviser sur un document annoter pour en mesurer le efficacité avant d' être utiliser sur le ensemble des document à explorer . ce approche être inspirer de celui utiliser pour le extraction de donnée dans le document semi-structurés ( wrappers ) et ne nécessiter pas un ressource linguistique particulier ni de large collection de test . son collection de document évoluer annuellement , nous espérer de plus un amélioration de son extraction dans le temps . 	Extraction d'entités dans des collections évolutives	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de connaissances d'adaptation par analyse de la base de cas	En raisonnement à partir de cas, l'adaptation d'un cas source pour résoudre un problème cible est une étape à la fois cruciale et difficile à réaliser. Une des raisons de cette difficulté tient au fait que les connaissances d'adaptation sont généralement dépendantes du domaine d'application. C'est ce qui motive la recherche sur l'acquisition de connaissances d'adaptation (ACA). Cet article propose une approche originale de l'ACA fondée sur des techniques d'extraction de connaissances dans des bases de données (ECBD). Nous présentons CABAMAKA, une application qui réalise l'ACA par analyse de la base de cas, en utilisant comme technique d'apprentissage l'extraction de motifs fermés fréquents. L'ensemble du processus d'extraction des connaissances est détaillé, puis nous examinons comment organiser les résultats obtenus de façon à faciliter la validation des connaissances extraites par l'analyste.	Amedeo Napoli, Jean Lieber, Fadi Badra	http://editions-rnti.fr/render_pdf.php?p1&p=1001464	http://editions-rnti.fr/render_pdf.php?p=1001464	1232	fr	fr	@loria.fr	extraction de connaissance d' adaptation par analyse de le base de cas  En raisonnement à partir de cas , le adaptation d' un cas source pour résoudre un problème cible être un étape à le foi crucial et difficile à réaliser . un des raison de ce difficulté tenir au fait que le connaissance d' adaptation être généralement dépendant du domaine d' application . C' être ce qui motiver le recherche sur le acquisition de connaissance d' adaptation ( ACA ) . ce article proposer un approche original de le ACA fonder sur un technique d' extraction de connaissance dans un base de donnée ( ECBD ) . Nous présenter CABAMAKA , un application qui réaliser le ACA par analyse de le base de cas , en utiliser comme technique d' apprentissage le extraction de motif fermer fréquent . le ensemble du processus d' extraction des connaissance être détailler , puis nous examiner comment organiser le résultat obtenir de façon à faciliter le validation des connaissance extraire par le analyste . 	Extraction de connaissances d'adaptation par analyse de la base de cas	6
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de données sur Internet avec Retroweb	Ce document décrit Retroweb, une boite à outils qui permet l'extraction de données structurées à partir de pages Web. Notre solution est semi-automatique car les données à extraire sont préalablement dénies par l'utilisateur. L'intérêt de cette approche est qu'elle permet l'extraction de données ciblées et conformes aux besoins de l'application utilisatrice (migrateur, moteur de recherche, outil de veille). Retroweb se caractérise aussi par une grande facilité d'utilisation car il ne nécessite aucune connaissance de langage particulier, la définition des règles d'extraction se faisant directement de manière interactive dans le navigateur Internet. Ce document décrit les trois principaux processus de notre méthode.	Fabrice Estievenart, Jean-Roch Meurisse	http://editions-rnti.fr/render_pdf.php?p1&p=1001338	http://editions-rnti.fr/render_pdf.php?p=1001338	1233	fr	fr	@cetic.be, @fundp.ac.be	extraction de donnée sur Internet avec Retroweb  ce document décrire Retroweb , un boite à outil qui permettre le extraction de donnée structurer à partir de page Web . son solution être semi-automatique car le donnée à extraire être préalablement dénies par le utilisateur . le intérêt de ce approche être qu' elle permettre le extraction de donnée cibler et conforme aux besoin de le application utilisateur ( migrateur , moteur de recherche , outil de veille ) . Retroweb clr caractériser aussi par un grand facilité d' utilisation car il ne nécessiter aucun connaissance de langage particulier , le définition des règle d' extraction clr faire directement de manière interactif dans le navigateur Internet . ce document décrire le trois principal processus de son méthode . 	Extraction de données sur Internet avec Retroweb	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de séquences multidimensionnelles convergentes et divergentes	"Les motifs séquentiels sont un domaine de la fouille de données très étudié depuis leur introduction par Agrawal et Srikant.Même s'il existe de nombreux travaux (algorithmes, domaines d'application), peu d'entre eux se situent dans un contexte multidimensionnel avec la prise en compte de ses spécificités : plusieurs dimensions, relations hiérarchiques entre les éléments de chaque dimension, etc. Dans cet article, nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue : du général au particulier ou vice et versa. Nous définissons ainsi le concept de séquences multidimensionnelles convergentes ou divergentes ainsi que l'algorithme associé, M2S_CD, basé sur le paradigme ""pattern growth"". Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l'intérêt de notre approche aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits."	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001388	http://editions-rnti.fr/render_pdf.php?p=1001388	1234	fr	fr	@lirmm.fr	extraction de séquence multidimensionnel convergent et divergentes  " le motif séquentiel être un domaine de le fouille de donnée très étudier depuis son introduction par Agrawal et Srikant . . même clr il exister un nombreux travail ( algorithme , domaine d' application ) , peu d' entre lui clr situer dans un contexte multidimensionnel avec le prise en compte de son spécificité : plusieurs dimension , relation hiérarchique entre le élément de chaque dimension , etc. Dans ce article , nous proposer un méthode original pour extraire un connaissance multidimensionnel définir sur plusieurs niveau de hiérarchie mais selon un certain point de vue : du général au particulier ou vice et verser . . Nous définir ainsi le concept de séquence multidimensionnel convergent ou divergent ainsi que le algorithme associer , M2S _ CD , baser sur le paradigme " " pattern growth " " . . un expérimentation , sur un jeu de donnée synthétique et réel , montrer le intérêt de son approche aussi bien en terme de robustesse des algorithme que de pertinence des motif extrait . " 	Extraction de séquences multidimensionnelles convergentes et divergentes	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction des Top-k Motifs par Approximer-et-Pousser	Cet article porte sur l'extraction de motifs sous contraintes globales. Contrairement aux contraintes usuelles comme celle de fréquence minimale, leur vérification est problématique car elle entraine de multiples comparaisons entre les motifs. Typiquement, la localisation des k motifs maximisant une mesure d'intérêt, i.e. satisfaisant la contrainte top-k, est difficile. Pourtant, cette contrainte globale se révèle très utile pour trouver les motifs les plus significatifs au regard d'un critère choisi par l'utilisateur. Dans cet article, nous proposons une méthode générale d'extraction de motifs sous contraintes globales, appelée Approximer-et-Pousser. Cette méthode peut être vue comme une méthode de relaxation d'une contrainte globale en une contrainte locale évolutive. Nous appliquons alors cette approche à l'extraction des top-k motifs selon une mesure d'intérêt. Les expérimentations montrent l'efficacité de l'approche Approximer-et-Pousser.	Arnaud Soulet, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1001387	http://editions-rnti.fr/render_pdf.php?p=1001387	1235	fr	fr	@unicaen.fr	extraction des Top-k motif par Approximer-et-Pousser  ce article porter sur le extraction de motif sous contrainte global . contrairement aux contrainte usuel comme celui de fréquence minimal , son vérification être problématique car elle entraine de multiple comparaison entre le motif . typiquement , le localisation des k motif maximiser un mesure d' intérêt , id est satisfaire le contrainte top-k , être difficile . pourtant , ce contrainte global clr révéler très utile pour trouver le motif le plus significatif au regard d' un critère choisir par le utilisateur . Dans ce article , nous proposer un méthode général d' extraction de motif sous contrainte global , appeler Approximer-et-Pousser . ce méthode pouvoir être voir comme un méthode de relaxation d' un contrainte global en un contrainte local évolutif . Nous appliquer alors ce approche à le extraction des top-k motif selon un mesure d' intérêt . le expérimentation montrer le efficacité de le approche Approximer-et-Pousser . 	Extraction des Top-k Motifs par Approximer-et-Pousser	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	Filtrage des sites Web à caractère violent par analyse du contenu textuel et structurel	"Dans cet article, nous proposons une solution pour la classification et le filtrage des sites Web à caractère violent. A la différence de la majorité de systèmes commerciaux basés essentiellement sur la détection de mots indicatifs ou l'utilisation d'une liste noire manuellement collectée, notre solution baptisée, ""WebAngels Filter"", s'appuie sur un apprentissage automatique par des techniques de data mining et une analyse conjointe du contenu textuel et structurel de la page Web. Les résultats expérimentaux obtenus lors de l'évaluation de notre approche sur une base de test sont assez bons. Comparé avec des logiciels, parmi les plus populaires, ""WebAngels Filter"" montre sa performance en terme de classification."	Abdelmajid Ben Hamadou, Mohamed Hammami, Radhouane Guermazi	http://editions-rnti.fr/render_pdf.php?p1&p=1001395	http://editions-rnti.fr/render_pdf.php?p=1001395	1236	fr	fr	@laposte.net, @isimsf.rnu.tn, @ec-lyon.fr	filtrage des site Web à caractère violent par analyse du contenu textuel et structurel  " Dans ce article , nous proposer un solution pour le classification et le filtrage des site Web à caractère violent . . A le différence de le majorité de système commercial baser essentiellement sur le détection de mot indicatif ou le utilisation d' un liste noir manuellement collecter , son solution baptiser , " " WebAngels Filter " " , clr appuyer sur un apprentissage automatique par un technique de data mining et un analyse conjoindre du contenu textuel et structurel de le page Web . . le résultat expérimental obtenir lors de le évaluation de son approche sur un base de test être assez bon . . Comparé avec un logiciel , parmi le plus populaire , " " WebAngels Filter " " montrer son performance en terme de classification . " 	Filtrage des sites Web à caractère violent par analyse du contenu textuel et structurel	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Finding interesting queries in relational databases	La découverte de motifs dans des bases de données relationnelles quelconques est un problème intéressant pour lequel il existe très peu de méthodes efficaces. Nous présentons un cadre dans lequel des paires de requêtes sur les données sont utilisées comme des motifs et nous discutons du problème de la découverte d'associations utiles entre elles. Plus spécifiquement, nous considérons des petites sous-classes de requêtes conjonctives qui nous permettent de découvrir des motifs intéressants de manière efficace.	Bart Goethals	http://editions-rnti.fr/render_pdf.php?p1&p=1001285	http://editions-rnti.fr/render_pdf.php?p=1001285	1237	en	fr		le découverte de motif dans un base de donnée relationnel quelconque être un problème intéressant pour lequel il exister très peu de méthode efficace . Nous présenter un cadre dans lequel un paire de requête sur le donnée être utiliser comme un motif et nous discuter du problème de le découverte d' association utile entre lui . plus spécifiquement , nous considérer un petit sous-classe de requête conjonctif qui nous permettre de découvrir un motif intéressant de manière efficace . 	Finding interesting queries in relational databases	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Fusion des approches visuelles et contextuelles pour l'annotation des images médicales	Dans le contexte de la recherche d'information sur Internet, nous proposons une architecture d'annotation automatique des images médicales, extraites à partir des documents de santé en ligne. Notre système est conçu pour extraire des informations médicales spécifiques (i.e. modalité médicale, région anatomique) à partir du contenu et du contexte des images. Nous proposons une architecture de fusion des approches contenu/contexte adaptée aux images médicales. L'approche orientée sur le contenu des images, consiste à annoter des images inconnues par la catégorisation des représentations visuelles compactes. Nous utilisons en même temps le contexte des images (les régions textuelles) ainsi que des ontologies médicales spécialement adaptées aux informations recherchées. Finalement, nous démontrons qu'en fusionnant les décisions des deux approches, nous améliorons les performances globales du système d'annotation.	Filip Florea, Valeriu Cornea, Alexandrina Rogozan, Abdelaziz Bensrhair, Stéfan Jacques Darmoni	http://editions-rnti.fr/render_pdf.php?p1&p=1001416	http://editions-rnti.fr/render_pdf.php?p=1001416	1238	fr	fr	@insa-rouen.fr, @chu-rouen.fr	fusion des approche visuel et contextuel pour le annotation des image médicales  Dans le contexte de le recherche d' information sur Internet , nous proposer un architecture d' annotation automatique des image médical , extraire à partir un document de santé en ligne . son système être concevoir pour extraire un information médical spécifique ( i.e. modalité médical , région anatomique ) à partir du contenu et du contexte des image . Nous proposer un architecture de fusion des approche contenir  contexte adapter aux image médical . le approche orienter sur le contenu des image , consister à annoter un image inconnu par le catégorisation des représentation visuel compact . Nous utiliser en même temps le contexte des image ( le région textuel ) ainsi que un ontologie médical spécialement adapter aux information rechercher . finalement , nous démontrer qu' en fusionner le décision des deux approche , nous améliorer le performance global du système d' annotation . 	Fusion des approches visuelles et contextuelles pour l'annotation des images médicales	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Génération et enrichissement automatique de listes de patrons de phrases pour les moteurs de questions-réponses	Nous utilisons un algorithme d'amorce mutuelle (Riloff et Jones 99), entre des couples de termes d'une relation et des patrons de phrase. À partir de couples d'amorce, le système génère des listes de patrons qui sont ensuite enrichies de façon semi-supervisée, puis utilisées pour trouver de nouveaux couples. Ces couples sont à leur tour réutilisés pour générer, par itérations successives, de nouveaux patrons. L'originalité de l'étude réside dans l'interprétation du rappel, estimé comme la couverture d'un patron sur l'ensemble des exemples auxquels il s'applique	Cédric Vidrequin, Juan-Manuel Torres-Moreno, Jean-Jacques Schneider, Marc El-Bèze	http://editions-rnti.fr/render_pdf.php?p1&p=1001363	http://editions-rnti.fr/render_pdf.php?p=1001363	1239	fr	fr	@univ-avignon.fr, @semantia.com	génération et enrichissement automatique de liste de patron de phrase pour le moteur de questions-réponses  Nous utiliser un algorithme d' amorce mutuel ( Riloff et Jones 99 ) , entre un couple de terme d' un relation et des patron de phrase . À partir de couple d' amorce , le système générer un liste de patron qui être ensuite enrichir de façon semi-supervisée , puis utiliser pour trouver un nouveau couple . ce couple être à son tour réutiliser pour générer , par itération successif , un nouveau patron . le originalité de le étude résider dans le interprétation du rappel , estimer comme le couverture d' un patron sur le ensemble des exemple auxquels il clr applique 	Génération et enrichissement automatique de listes de patrons de phrases pour les moteurs de questions-réponses	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Intégration des connaissances utilisateurs pour des analyses personnalisées dans les entrepôts de données évolutifs	"Dans cet article, nous proposons une approche d'évolution de schéma dans les entrepôts de données qui permet aux utilisateurs d'intégrer leurs propres connaissances du domaine afin d'enrichir les possibilités d'analyse de l'entrepôt. Nous représentons cette connaissance sous la forme de règles de type ""si-alors"". Ces règles sont utilisées pour créer de nouveaux axes d'analyse en générant de nouveaux niveaux de granularité dans les hiérarchies de dimension. Notre approche est fondée sur un modèle formel d'entrepôts de données évolutif qui permet de gérer la mise à jour des hiérarchies de dimension."	Cécile Favre, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001379	http://editions-rnti.fr/render_pdf.php?p=1001379	1240	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	intégration des connaissance utilisateur pour un analyse personnaliser dans le entrepôt de donnée évolutifs  " Dans ce article , nous proposer un approche d' évolution de schéma dans le entrepôt de donnée qui permettre aux utilisateur d' intégrer son propre connaissance du domaine afin d' enrichir le possibilité d' analyse de le entrepôt . . Nous représenter ce connaissance sous le forme de règle de type " " si-alors " " . . ce règle être utiliser pour créer un nouveau axe d' analyse en générer un nouveau niveau de granularité dans le hiérarchie de dimension . . son approche être fonder sur un modèle formel d' entrepôt de donnée évolutif qui permettre de gérer le mise à jour des hiérarchie de dimension . " 	Intégration des connaissances utilisateurs pour des analyses personnalisées dans les entrepôts de données évolutifs	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Interestingness in Data Mining	Interestingness measures play an important role in data mining regardless of the kind of patterns being mined. These measures are intended for selecting and ranking patterns according to their potential interest to the user. Good measures also allow the time and space cost of the mining process to be reduced. Measuring the interestingness of discovered patterns is an active and important area of data mining research. Although much work has been conducted in this area, so far there is no widespread agreement on a formal definition of interestingness in this context. Based on the diversity of definitions presented to date, interestingness is perhaps best treated as a broad concept, which emphasizes conciseness, coverage, reliability, peculiarity, diversity, novelty, surprisingness, utility, and actionability. This presentation reviews interestingness measures for rules and summaries, classifies them from several perspectives, compares their properties, identifies their roles in the data mining process, gives strategies for selecting appropriate measures for applications, and identifies opportunities for future research in this area.	Howard J. Hamilton	http://editions-rnti.fr/render_pdf.php?p1&p=1001283	http://editions-rnti.fr/render_pdf.php?p=1001283	1241	en	en		interestingness datum mining interestingness measure play important role data mining regardless kind pattern mine measure intend selecting ranking pattern accord potential interest user good measure also allow time space cost mining process reduce measure interestingness discover pattern active important area datum mining research although much work conduct area far widespread agreement formal definition interestingness context base diversity definition presented date interestingness perhaps best treat broad concept emphasize conciseness coverage reliability peculiarity diversity novelty surprisingness utility actionability presentation reviews interestingness measure rule summary classify several perspective compare property identify role datum mining process give strategy select appropriate measure application identify opportunity future research area	Interestingness in Data Mining	20
Revue des Nouvelles Technologies de l'Information	EGC	2007	L'émergence de connaissances dans les communautés de pratique	Cet article est le résultat d'une recherche sur le processus, peu explicité dans la littérature, de création de connaissances dans les communautés de pratique. Nous commençons par établir une définition de travail pour ce concept de communauté de pratique qui permet l'échange et le partage de connaissances au sein de groupes de plus en plus virtuels. Nous analysons ensuite les communautés de pratique sous l'angle de la théorie de l'émergence. Nous proposons, alors, la modélisation d'un outil de support pour ces communautés qui améliore les échanges entre les membres et favorise l'émergence de nouvelles connaissances. Cet outil manipule les connaissances implicites ainsi qu'explicites et propose des possibilités pour la publication et la recherche d'informations. De plus, il s'adapte à chaque membre de la communauté par un processus de personnalisation.	Caroline Wintergerst, Thomas Ludwig, Danielle Boulanger	http://editions-rnti.fr/render_pdf.php?p1&p=1001442	http://editions-rnti.fr/render_pdf.php?p=1001442	1242	fr	fr	@univ-lyon3.fr, @univ-lyon3.fr, @web.de	le émergence de connaissance dans le communauté de pratique  ce article être le résultat d' un recherche sur le processus , peu expliciter dans le littérature , de création de connaissance dans le communauté de pratique . Nous commencer par établir un définition de travail pour ce concept de communauté de pratique qui permettre le échange et le partage de connaissance au sein de groupe de plus en plus virtuel . Nous analyser ensuite le communauté de pratique sous le angle de le théorie de le émergence . Nous proposer , alors , le modélisation d' un outil de support pour ce communauté qui améliorer le échange entre le membre et favoriser le émergence de nouveau connaissance . ce outil manipuler le connaissance implicite ainsi qu' explicite et proposer un possibilité pour le publication et le recherche d' information . De plus , il clr adapter à chaque membre de le communauté par un processus de personnalisation . 	L'émergence de connaissances dans les communautés de pratique	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	L'outil SDET pour le complètement des données descriptives liées aux bases de données géographiques	L'enrichissement des bases de données est un moyen visant à offrir un supplément informationnel aux utilisateurs. Dans le cas des données géographiques, cette activité représente de nos jours un problème crucial. Sa résolution permettrait de meilleures prises de décisions ne reposant pas uniquement sur les informations limitées. Notre outil SDET (Semantic Data Enrichment Tool) vient proposer une solution d'enrichissement faisant du Système d'Information Géographiques (SIG) initial une source riche d'informations.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001337	http://editions-rnti.fr/render_pdf.php?p=1001337	1243	fr	fr	@insat.rnu.tn, @insat.rnu.tn	le outil SDET pour le complètement des donnée descriptif lier aux base de donnée géographiques  le enrichissement des base de donnée être un moyen viser à offrir un supplément informationnel aux utilisateur . Dans le cas des donnée géographique , ce activité représenter de son jour un problème crucial . son résolution permettre de meilleur prise de décision ne reposer pas uniquement sur le information limiter . son outil SDET ( Semantic Data Enrichment Tool ) venir proposer un solution d' enrichissement faire du système d' information Géographiques ( SIG ) initial un source riche d' information . 	L'outil SDET pour le complètement des données descriptives liées aux bases de données géographiques	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Les itemsets essentiels fermés : une nouvelle représentation concise	Devant l'accroissement constant des grandes bases de données, plusieurs travaux de recherche en fouille de données s'orientent vers le développement de techniques de représentation compacte. Ces recherches se développent suivant deux axes complémentaires : l'extraction de bases génériques de règles d'association et l'extraction de représentations concises d'itemsets fréquents.Dans ce papier, nous introduisons une nouvelle représentation concise exacte des itemsets fréquents. Elle se situe au croisement de chemins de deux autres représentations concises, à savoir les itemsets fermés et ceux dits essentiels. L'idée intuitive est de profiter du fait que tout opérateur de fermeture induit une fonction surjective. Dans ce contexte, nous introduisons un nouvel opérateur de fermeture permettant de calculer les fermetures des itemsets essentiels. Ceci a pour but d'avoir une représentation concise de taille réduite tout en permettant l'extraction des supports négatif et disjonctif d'un itemset en plus de son support conjonctif. Un nouvel algorithme appelé D-CLOSURE permettant d'extraire les itemsets essentiels fermés est aussi présenté. L'étude expérimentale que nous avons menée a permis de confirmer que la nouvelle approche présente un bon taux de compacité comparativement aux autres représentations concises exactes.	Tarek Hamrouni, Islem Denden, Sadok Ben Yahia, Engelbert Mephu Nguifo, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001384	http://editions-rnti.fr/render_pdf.php?p=1001384	1244	fr	fr	@fst.rnu.tn, @univ-artois.fr	le itemsets essentiel fermer : un nouveau représentation concise  Devant le accroissement constant des grand base de donnée , plusieurs travail de recherche en fouille de donnée clr orienter vers le développement de technique de représentation compact . ce recherche clr développer suivant deux axe complémentaire : le extraction de base générique de règle d' association et le extraction de représentation concis d' itemsets fréquent . Dans ce papier , nous introduire un nouveau représentation concis exact des itemsets fréquent . Elle clr situer au croisement de chemin de deux autre représentation concis , à savoir le itemsets fermer et celui dire essentiel . le idée intuitif être de profiter du fait que tout opérateur de fermeture induire un fonction surjective . Dans ce contexte , nous introduire un nouveau opérateur de fermeture permettre de calculer le fermeture des itemsets essentiel . ceci avoir pour but d' avoir un représentation concis de taille réduire tout en permettre le extraction des support négatif et disjonctif d' un itemset en plus de son support conjonctif . un nouveau algorithme appeler D-CLOSURE permettre d' extraire le itemsets essentiel fermer être aussi présenter . le étude expérimental que nous avoir mener avoir permettre de confirmer que le nouveau approche présent un bon taux de compacité comparativement aux autre représentation concis exact . 	Les itemsets essentiels fermés : une nouvelle représentation concise	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Logiciel d'aide à l'évaluation des catégorisations	"Les méthodes de classification automatique sont employées dans des domaines variés et de nombreux algorithmes ont été proposés dans la littérature. Au milieu de cette ""jungle"", il semble parfois difficile à un simple utilisateur de choisir quel algorithme est le plus adapté à ses besoins. Depuis le milieu des années 90, une nouvelle thématique de recherches, appelée clustering validity, tente de répondre à ce genre d'interrogation en proposant des indices pour juger de la qualité des catégorisations obtenues. Mais le choix est parfois difficile entre ces indices et il peut s'avérer délicat de prendre la bonne décision. C'est pourquoi nous proposons un logiciel adapté à cette problématique d'évaluation."	Julien Velcin, William Vacher, Jean-Gabriel Ganascia	http://editions-rnti.fr/render_pdf.php?p1&p=1001333	http://editions-rnti.fr/render_pdf.php?p=1001333	1245	fr	fr	@lip6.fr, @free.fr	logiciel d' aide à le évaluation des catégorisations  " le méthode de classification automatique être employer dans un domaine varier et de nombreux algorithme avoir être proposer dans le littérature . . Au milieu de ce " " jungle " " , il sembler parfois difficile à un simple utilisateur de choisir quel algorithme être le plus adapter à son besoin . . Depuis le milieu des année 90 , un nouveau thématique de recherche , appeler clustering validity , tenter de répondre à ce genre d' interrogation en proposer un indice pour juger de le qualité des catégorisation obtenir . . Mais le choix être parfois difficile entre ce indice et il pouvoir clr avérer délicat de prendre le bon décision . . C' être pourquoi nous proposer un logiciel adapter à ce problématique d' évaluation . " 	Logiciel d'aide à l'évaluation des catégorisations	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Mesure d'entropie asymétrique et consistante	Les mesures d'entropie, dont la plus connue est celle de Shannon, ont été proposées dans un contexte de codage et de transmission d'information. Néanmoins, dès le milieu des années soixante, elles ont été utilisées dans d'autres domaines comme l'apprentissage et plus particulièrement pour construire des graphes d'induction et des arbres de décision. L'usage brut de ces mesures n'est cependant pas toujours bien approprié pour engendrer des modèles de prédiction ou d'explication pertinents. Cette faiblesse résulte des propriétés des entropies, en particulier le maximum nécessairement atteint pour la distribution uniforme et l'insensibilité à la taille de l'échantillon. Nous commençons par rappeler ces propriétés classiques. Nous définissons ensuite une nouvelle axiomatique mieux adaptée à nos besoins et proposons une mesure empirique d'entropie plus flexible vérifiant ces axiomes.	Djamel Abdelkader Zighed, Simon Marcellin, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1001309	http://editions-rnti.fr/render_pdf.php?p=1001309	1246	fr	fr	@univ-lyon2.fr, @unige.ch	mesure d' entropie asymétrique et consistante  le mesure d' entropie , dont le plus connaître être celui de Shannon , avoir être proposer dans un contexte de codage et de transmission d' information . néanmoins , dès le milieu des année soixante , elles avoir être utiliser dans un autre domaine comme le apprentissage et plus particulièrement pour construire un graphe d' induction et des arbre de décision . le usage brut de ce mesure n' être cependant pas toujours bien approprier pour engendrer un modèle de prédiction ou d' explication pertinent . ce faiblesse résulter un propriété des entropie , en particulier le maximum nécessairement atteindre pour le distribution uniforme et le insensibilité à le taille de le échantillon . Nous commencer par rappeler ce propriété classique . Nous définir ensuite un nouveau axiomatique mieux adapter à son besoin et proposer un mesure empirique d' entropie plus flexible vérifier ce axiome . 	Mesure d'entropie asymétrique et consistante	19
Revue des Nouvelles Technologies de l'Information	EGC	2007	Mesure non symétrique pour l'évaluation de modèles, utilisation pour les jeux de données déséquilibrés	Les critères servant à l'évaluation de modèles d'apprentissage supervisé ainsi que ceux utilisés pour bâtir des arbres de décision sont, pour la plupart, symétriques. De manière pragmatique, cela signifie que chacune des modalités de la variable endogène se voit assigner une importance identique. Or, dans nombre de cas pratiques cela n'est pas le cas. Ainsi, on peut notamment prendre l'exemple de jeux de données fortement déséquilibrés pour lesquels l'objectif principal est l'identification des objets représentatifs de la modalité minoritaire (Aide au diagnostic, identification de phénomènes inhabituels : fraudes, pannes...). Dans ce type de situation il apparaît clairement qu'assigner une importance identique aux erreurs de prédiction ne constitue pas la meilleure des solutions. Nous proposons dans cet article un critère (pouvant servir à la fois pour l'évaluation de modèles d'apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision) prenant en compte cet aspect non symétrique de l'importance associée à chacune des modalités de la variable endogène. Nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés.	Julien Thomas, Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1001426	http://editions-rnti.fr/render_pdf.php?p=1001426	1247	fr	fr		mesure non symétrique pour le évaluation de modèle , utilisation pour le jeu de donnée déséquilibrés  le critère servir à le évaluation de modèle d' apprentissage superviser ainsi que celui utiliser pour bâtir un arbre de décision être , pour le plupart , symétrique . De manière pragmatique , cela signifier que chacun des modalité de le variable endogène clr voir assigner un importance identique . Or , dans nombre de cas pratique cela n' être pas le cas . ainsi , on pouvoir notamment prendre le exemple de jeu de donnée fortement déséquilibrer pour lesquels le objectif principal être le identification des objet représentatif de le modalité minoritaire ( aide au diagnostic , identification de phénomène inhabituel : fraude , panne ... ) . Dans ce type de situation il apparaître clairement qu' assigner un importance identique aux erreur de prédiction ne constituer pas le meilleur des solution . Nous proposer dans ce article un critère ( pouvoir servir à le foi pour le évaluation de modèle d' apprentissage superviser ou encore de critère utiliser pour bâtir un arbre de décision ) prendre en compte ce aspect non symétrique de le importance associer à chacun des modalité de le variable endogène . Nous proposer ensuite un évolution des modèle de type forêt aléatoire utiliser ce critère pour le jeu de donnée fortement déséquilibrer . 	Mesure non symétrique pour l'évaluation de modèles, utilisation pour les jeux de données déséquilibrés	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Méthodes statistiques et modèles thermiques compacts	Dans le domaine thermique, la plupart des études reposent sur des modèles à éléments finis. Cependant, le coût en calcul et donc en temps de ces méthodes ont renforcé le besoin de modèles plus compacts. Le réseau RC équivalent est la solution la plus souvent utilisée. Toutefois, ses paramètres doivent souvent être ajustés à l'aide de mesures ou de simulation. Dans ce contexte d'identification de système, les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique.	Hubert Polaert, Philippe Leray, Grégory Mallet	http://editions-rnti.fr/render_pdf.php?p1&p=1001377	http://editions-rnti.fr/render_pdf.php?p=1001377	1248	fr	fr	@insa-rouen.fr	méthode statistique et modèle thermique compacts  Dans le domaine thermique , le plupart des étude reposer sur un modèle à élément fini . cependant , le coût en calcul et donc en temps de ce méthode avoir renforcer le besoin de modèle plus compact . le réseau RC équivalent être le solution le plus souvent utiliser . toutefois , son paramètre devoir souvent être ajuster à le aide de mesure ou de simulation . Dans ce contexte d' identification de système , le méthode statistique être comparer aux méthode classiquement utiliser pour le prédiction thermique . 	Méthodes statistiques et modèles thermiques compacts	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Navigation et appariement d'objets géographiques dans une ontologie	L'ACI FoDoMuSt se propose d'élaborer un processus de fouille de données multi-stratégies pour la reconnaissance automatique d'objets géographiques sur des images satellitaires ou aériennes. Ces dernières sont segmentées afin d'isoler des polygones définis par un ensemble de descripteurs de bas niveaux. Afin de leur affecter une sémantique, on applique dans un premier temps une classification. Si aucun objet géographique n'est identifié, on tente alors un appariement du polygone avec les concepts d'une ontologie d'objets géographiques. Un algorithme de navigation dans l'ontologie et une mesure de comparaison sémantique ont ainsi été développés, paramétrables selon le contexte d'appariement. Cette mesure évalue la pertinence d'un appariement et comprend une composante locale (comparaison au niveau du concept) et une composante globale (combinaison linéaire de mesures locales). La méthode proposée a été développée en JAVA et intégrée à la plate-forme FoDoMuSt. Les premières expérimentations et évaluations humaines sont très encourageantes.	Rémy Brisson, Omar Boussaid, Pierre Gançarski, Anne Puissant, Nicolas Durand	http://editions-rnti.fr/render_pdf.php?p1&p=1001402	http://editions-rnti.fr/render_pdf.php?p=1001402	1249	fr	fr	@yahoo.fr, @univ-lyon2.fr, @lsiit.u-strasbg.fr, @lsiit.u-strasbg.fr, @unicaen.fr	navigation et appariement d' objet géographique dans un ontologie  le ACI FoDoMuSt clr proposer d' élaborer un processus de fouille de donnée multi-stratégies pour le reconnaissance automatique d' objet géographique sur un image satellitaire ou aérien . ce dernier être segmenter afin d' isoler un polygone définir par un ensemble de descripteur de bas niveau . Afin de leur affecter un sémantique , on appliquer dans un premier temps un classification . Si aucun objet géographique n' être identifier , on tenter alors un appariement du polygone avec le concept d' un ontologie d' objet géographique . un algorithme de navigation dans le ontologie et un mesure de comparaison sémantique avoir ainsi être développer , paramétrables selon le contexte d' appariement . ce mesure évaluer le pertinence d' un appariement et comprendre un composante local ( comparaison au niveau du concept ) et un composante global ( combinaison linéaire de mesure local ) . le méthode proposer avoir être développer en JAVA et intégrer à le plate-forme FoDoMuSt . le premier expérimentation et évaluation humain être très encourageant . 	Navigation et appariement d'objets géographiques dans une ontologie	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Notion de conversation dans les communications interpersonnelles instantanées sur IP	Dans cet article nous étudions la contribution des techniques de fouille de données à l'amélioration des services de communications instantanées sur IP tel que la messagerie instantanée (IM) et la téléphonie sur IP (ToIP).	Alexandre Bouchacourt, Luigi Lancieri	http://editions-rnti.fr/render_pdf.php?p1&p=1001359	http://editions-rnti.fr/render_pdf.php?p=1001359	1250	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com	notion de conversation dans le communication interpersonnel instantané sur IP  Dans ce article nous étudier le contribution des technique de fouille de donnée à le amélioration des service de communication instantané sur IP tel que le messagerie instantané ( IM ) et le téléphonie sur IP ( ToIP ) . 	Notion de conversation dans les communications interpersonnelles instantanées sur IP	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	OKM : une extension des k-moyennes pour la recherche de classes recouvrantes	Dans cet article nous abordons le problème de la classification (ou clustering) dans le but de découvrir des classes avec recouvrements. Malgré quelques avancées récentes dans ce domaines, motivées par des besoins applicatifs importants (traitements des données multimédia par exemple), nous constatons l'absence de solutions théoriques à ce problème. Notre étude consiste alors à proposer une nouvelle formulation du problème de classification par partitionnement, adaptée à la recherche d'un recouvrement des données en classes d'objets similaires. Cette approche se fonde sur la dénition d'un critère objectif de qualité d'un recouvrement et d'une solution algorithmique visant à optimiser ce critère. Nous proposons deux évaluations de ce travail permettant d'une part d'appréhender le fonctionnement global de l'algorithme sur des données simples (vitesse de convergence, visualisation des résultats) et d'autre part d'évaluer quantitativement le bénéfice d'une telle approche sur une application de classification de documents textuels.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001458	http://editions-rnti.fr/render_pdf.php?p=1001458	1251	fr	fr	@univ-orleans.fr	OKM : un extension des k-moyennes pour le recherche de classe recouvrantes  Dans ce article nous aborder le problème de le classification ( ou clustering ) dans le but de découvrir un classe avec recouvrement . Malgré quelque avancée récent dans ce domaine , motiver par un besoin applicatif important ( traitement des donnée multimédia par exemple ) , nous constater le absence de solution théorique à ce problème . son étude consister alors à proposer un nouveau formulation du problème de classification par partitionnement , adapter à le recherche d' un recouvrement des donnée en classe d' objet similaire . ce approche clr fonder sur le dénition d' un critère objectif de qualité d' un recouvrement et d' un solution algorithmique viser à optimiser ce critère . Nous proposer deux évaluation de ce travail permettre d' un part d' appréhender le fonctionnement global de le algorithme sur un donnée simple ( vitesse de convergence , visualisation des résultat ) et d' autre part d' évaluer quantitativement le bénéfice d' un tel approche sur un application de classification de document textuel . 	OKM : une extension des k-moyennes pour la recherche de classes recouvrantes	19
Revue des Nouvelles Technologies de l'Information	EGC	2007	Optimal histogram representation of large data sets: Fisher vs piecewise linear approximation	Histogram representation of a large set of data is a good way to summarize and visualize data and is frequently performed in order to optimize query estimation in DBMS. In this paper, we show the performance and the properties of two strategies for an optimal construction of histograms on a single real valued descriptor on the base of a prior choice of the number of buckets. The first one is based on the Fisher algorithm, while the second one is based on a geometrical procedure for the interpolation of the empirical distribution function by a piecewise linear function. The goodness of fit is computed using the Wasserstein metric between distributions. We compare the proposed method performances against some existing ones on artificial and real datasets.	Antonio Irpino, Elvira Romano	http://editions-rnti.fr/render_pdf.php?p1&p=1001314	http://editions-rnti.fr/render_pdf.php?p=1001314	1252	en	en	@unina.it, @unina.it	optimal histogram representation large datum set fisher vs piecewise linear approximation histogram representation large set datum good way summarize visualize datum frequently perform order optimize query estimation dbms paper show performance property two strategy optimal construction histogram single real value descriptor base prior choice number bucket first one base fisher algorithm second one base geometrical procedure interpolation empirical distribution function piecewise linear function goodness fit compute used wasserstein metric distribution compare propose method performance exist one artificial real dataset	Optimal histogram representation of large data sets: Fisher vs piecewise linear approximation	26
Revue des Nouvelles Technologies de l'Information	EGC	2007	Partitionnement d'un réseau de sociabilité à fort coefficient de clustering	Afin de comparer l'organisation sociale d'une paysannerie médiévale avant et après la guerre de Cent Ans nous étudions la structure de réseaux sociaux construits à partir d'un corpus de contrats agraires. Faibles diamètres et fort clustering révèlent des graphes en petit monde. Comme beaucoup de grands réseaux d'interaction étudiés ces dernières années ces graphes sont sans échelle typique. Les distributions des degrés de leurs sommets sont bien ajustées par une loi de puissance tronquée par une coupure exponentielle. Ils possèdent en outre un club-huppé, c'est à dire un noyau dense et de faible diamètre regroupant les individus à forts degrés. La forme particulière des éléments propres du laplacien permet d'extraire des communautés qui se répartissent en étoile autour du club huppé.	Romain Boulet, Bertrand Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1001438	http://editions-rnti.fr/render_pdf.php?p=1001438	1253	fr	fr	@univ-tlse2.fr	Partitionnement d' un réseau de sociabilité à fort coefficient de clustering  Afin de comparer le organisation social d' un paysannerie médiéval avant et après le guerre de Cent Ans nous étudier le structure de réseau social construire à partir d' un corpus de contrat agraire . faible diamètre et fort clustering révéler un graphe en petit monde . Comme beaucoup de grand réseau d' interaction étudier ce dernier année ce graphe être sans échelle typique . le distribution des degré de son sommet être bien ajuster par un loi de puissance tronquer par un coupure exponentiel . Ils posséder en outre un club-huppé , c' être à dire un noyau dense et de faible diamètre regrouper le individu à fort degré . le forme particulier des élément propre du laplacien permettre d' extraire un communauté qui clr répartir en étoile autour du club huppé . 	Partitionnement d'un réseau de sociabilité à fort coefficient de clustering	7
Revue des Nouvelles Technologies de l'Information	EGC	2007	Peut-on capturer la sémantique à travers la syntaxe ? - découverte des règles d'exception simultanée	L'objectif de la fouille de données est la découverte sophistiquée de connaissances lisibles, surprenantes et possiblement utiles. Les aspects surprenant et utile font partie de la sémantique et nécessitent l'utilisation des connaissances du domaine, ce qui cause souvent le problème d'acquisition de la connaissance. Notre découverte des règles d'exception simultanée peut être une réponse à ce problème. Nous envisageons de trouver les connaissances surprenantes et possiblement utiles à travers notre forme de paire de règles d'exception. Les autres méthodes inventées concernent l'index d'évaluation et la recherche exhaustive. Plusieurs applications médicales seront présentées sur lesquelles nos propositions ont été appliquées.	Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001280	http://editions-rnti.fr/render_pdf.php?p=1001280	1254	fr	fr		pouvoir -on capturer le sémantique à travers le syntaxe ? - découverte des règle d' exception simultanée  le objectif de le fouille de donnée être le découverte sophistiquer de connaissance lisible , surprenant et possiblement utile . le aspect surprenant et utile faire partie de le sémantique et nécessiter le utilisation des connaissance du domaine , ce qui cause souvent le problème d' acquisition de le connaissance . son découverte des règle d' exception simultané pouvoir être un réponse à ce problème . Nous envisager de trouver le connaissance surprenant et possiblement utile à travers son forme de paire de règle d' exception . le autre méthode inventer concerner le index d' évaluation et le recherche exhaustif . plusieurs application médical être présenter sur lesquelles son proposition avoir être appliquer . 	Peut-on capturer la sémantique à travers la syntaxe ? - découverte des règles d'exception simultanée	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Préservation de l'Intimité dans les Protocoles de Conversations	Le travail présenté dans cet article, rentre dans le cadre de la gestion des données privées en vue de la substitution, appelée remplaçabilité, dynamique des services Web. Trois contributions sont apportées, (1) modélisation des politiques privées spécifiant les règles d'utilisation des données privées, prenant en compte des aspects se rapportant aux services Web, (2) étendre les protocoles de conversations des services Web par le modèle proposé, afin d'apporter les primitives nécessaires pour l'analyse des protocoles en présence de ces règles, (3) définition d'un mécanisme d'analyse de la remplaçabilité d'un service par un autre en vue de ses politiques privées.	Nawal Guermouche, Salima Benbernou, Emmanuel Coquery, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001360	http://editions-rnti.fr/render_pdf.php?p=1001360	1255	fr	fr	@loria.fr, @liris.cnrs.fr	préservation de le intimité dans le protocole de Conversations  le travail présenter dans ce article , rentrer dans le cadre de le gestion des donnée priver en vue de le substitution , appeler remplaçabilité , dynamique des service Web . Trois contribution être apporter , ( 1 ) modélisation des politique priver spécifier le règle d' utilisation des donnée priver , prendre en compte des aspect clr rapporter aux service Web , ( 2 ) étendre le protocole de conversation des service Web par le modèle proposer , afin d' apporter le primitif nécessaire pour le analyse des protocole en présence de ce règle , ( 3 ) définition d' un mécanisme d' analyse de le remplaçabilité d' un service par un autre en vue de son politique priver . 	Préservation de l'Intimité dans les Protocoles de Conversations	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	RAS : Un outil pour l'annotation de documents basée sur les liens de citation	RAS (Reference Annotation System) est un outil d'annotation de documents. Cet outil est le résultat de l'implémentation de notre approche d'annotation basée sur le contexte de citation. L'approche est indépendante du contenu et utilise un regroupement thématique des références construit à partir d'une classification floue non-supervisée. L'outil présenté dans cet article a été expérimentée et évaluée avec la base de documents scientifiques Citeseer.	Lylia Abrouk, Danièle Hérin	http://editions-rnti.fr/render_pdf.php?p1&p=1001341	http://editions-rnti.fr/render_pdf.php?p=1001341	1256	fr	fr	@lirmm.fr	ras : un outil pour le annotation de document baser sur le lien de citation  ras ( Reference Annotation System ) être un outil d' annotation de document . ce outil être le résultat de le implémentation de son approche d' annotation baser sur le contexte de citation . le approche être indépendant du contenu et utiliser un regroupement thématique des référence construire à partir d' un classification flou non- superviser . le outil présenter dans ce article avoir être expérimenter et évaluer avec le base de document scientifique Citeseer . 	RAS : Un outil pour l'annotation de documents basée sur les liens de citation	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Ré-ordonnancement pour l'apprentissage de transformations de documents HTML	Notre objectif est de transformer les documents Web vers un schéma médiateur XML défini a priori. C'est une étape nécessaire pour de nombreuses tâches de recherche d'information concernant le Web Sémantique, les documents semi-structurés, le traitement de sources hétérogènes, etc. Elle permet d'associer une structure sémantiquement riche à des documents dont le formats ne contient que des informations de présentation. Nous proposons de traiter ce problème comme un problème d'apprentissage structuré en le formalisant comme une transformation d'arbre en arbre.Notre méthode de transformation comporte deux étapes. Dans une première étape, une grammaire hors-contexte probabiliste permet de générer un ensemble de solutions candidates. Dans une deuxième étape, ces solutions candidates sont ordonnées grâce à un algorithme de ré-ordonnancement à base de perceptron à noyau. Cette étape d'ordonnancement nous permet d'utiliser de manière efficace des caractéristiques complexes définies à partir du document d'entrée et de la solution candidate.	Guillaume Wisniewski, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1001461	http://editions-rnti.fr/render_pdf.php?p=1001461	1257	fr	fr	@lip6.fr	Ré-ordonnancement pour le apprentissage de transformation de document HTML  son objectif être de transformer le document Web vers un schéma médiateur XML définir avoir priori . C' être un étape nécessaire pour un nombreux tâche de recherche d' information concerner le Web Sémantique , le document semi-structurés , le traitement de source hétérogène , etc. Elle permettre d' associer un structure sémantiquement riche à un document dont le format ne contenir que un information de présentation . Nous proposer de traiter ce problème comme un problème d' apprentissage structurer en le formaliser comme un transformation d' arbre en arbre . son méthode de transformation comporter deux étape . Dans un premier étape , un grammaire hors-contexte probabiliste permettre de générer un ensemble de solution candidat . Dans un deuxième étape , ce solution candidat être ordonner grâce à un algorithme de ré-ordonnancement à base de perceptron à noyau . ce étape d' ordonnancement nous permettre d' utiliser de manière efficace des caractéristique complexe définir à partir du document d' entrée et de le solution candidat . 	Ré-ordonnancement pour l'apprentissage de transformations de documents HTML	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Réduction de dimension pour l'analyse de données vidéo	Les données vidéo ont la particularité d'être très volumineuses alors qu'elles contiennent peu d'information sémantique. Pour les analyser, il faut réduire la quantité d'information dans l'espace de recherche. Les données vidéo sont souvent considérées comme l'ensemble des pixels d'une succession d'images analysées séquentiellement. Dans cet article, nous proposons d'utiliser une analyse en composantes principales (ACP) pour réduire la dimensionnalité des informations sans perdre la nature tridimensionnelle des données initiales. Nous commençons par considérer des sous-séquences, dont le nombre de trames est le nombre de dimensions dans l'espace de représentation. Nous appliquons une ACP pour obtenir un espace de faible dimension où les points similaires sémantiquement sont proches. La sous-séquence est ensuite divisée en blocs tridimensionnels dont on projette l'ellipsoïde d'inertie dans le premier plan factoriel. Nous déduisons enfin le mouvement présent dans les blocs à partir des ellipses ainsi obtenues. Nous présenterons les résultats obtenus pour un problème de vidéosurveillance.	Nicolas Verbeke, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001404	http://editions-rnti.fr/render_pdf.php?p=1001404	1258	fr	fr	@univ-paris5.fr	réduction de dimension pour le analyse de donnée vidéo  le donnée vidéo avoir le particularité d' être très volumineux alors qu' elles contenir peu d' information sémantique . Pour les analyser , il faillir réduire le quantité d' information dans le espace de recherche . le donnée vidéo être souvent considérer comme le ensemble des pixel d' un succession d' image analyser séquentiellement . Dans ce article , nous proposer d' utiliser un analyse en composante principal ( ACP ) pour réduire le dimensionnalité des information sans perdre le nature tridimensionnel des donnée initial . Nous commencer par considérer un sous-séquences , dont le nombre de trame être le nombre de dimension dans le espace de représentation . Nous appliquer un ACP pour obtenir un espace de faible dimension où le point similaire sémantiquement être proche . le sous-séquence être ensuite diviser en bloc tridimensionnel dont on projeter le ellipsoïde d' inertie dans le premier plan factoriel . Nous déduire enfin le mouvement présent dans le bloc à partir un ellipse ainsi obtenir . Nous présenter le résultat obtenir pour un problème de vidéosurveillance . 	Réduction de dimension pour l'analyse de données vidéo	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Régression floue et crédibiliste par SVM pour la classification des images sonar	La classification des images sonar est d'une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins. En effet, le sonar offre des capacités d'imagerie plus performantes que les capteurs optiques en milieu sous-marin. La classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu. De nombreuses approches ont été proposées sans donner de bons résultats, celles-ci ne tenant pas compte des imperfections des données. Pour modéliser ce type de données, il est judicieux d'utiliser les théories de l'incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance. Les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation. Il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de l'algorithme de classification. L'approche de la régression par SVM que nous avons introduite permet cette modélisation des imperfections. Nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes, dans le cadre de la classification des images sonar.	Hicham Laanaya, Arnaud Martin, Driss Aboutajdine, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001295	http://editions-rnti.fr/render_pdf.php?p=1001295	1259	fr	fr	@fsr.ac.ma, @ensieta.fr	régression flou et crédibiliste par SVM pour le classification des image sonar  le classification des image sonar être d' un grand importance par exemple pour le navigation sous-marine ou pour le cartographie des fond marin . En effet , le sonar offrir un capacité d' imagerie plus performant que le capteur optique en milieu sous-marin . le classification de ce type de donnée rencontrer plusieurs difficulté en raison des imprécision et incertitude lier au capteur et au milieu . un nombreux approche avoir être proposer sans donner un bon résultat , celui _-ci ne tenir pas compte des imperfection des donnée . Pour modéliser ce type de donnée , il être judicieux d' utiliser le théorie de le incertain comme le théorie des sous-ensemble flou ou le théorie des fonction de croyance . le machine à vecteur de support être de plus en plus utiliser pour le classification automatique aux vue son simplicité et son capacité de généralisation . Il être ainsi possible de proposer un approche qui tenir compte de ce imprécision et de ce incertitude au coeur même de le algorithme de classification . le approche de le régression par SVM que nous avoir introduire permettre ce modélisation des imperfection . Nous proposer ici un application de ce nouveau approche sur un donnée réel particulièrement complexe , dans le cadre de le classification des image sonar . 	Régression floue et crédibiliste par SVM pour la classification des images sonar	5
Revue des Nouvelles Technologies de l'Information	EGC	2007	Segmentation thématique par calcul de distance thématique	Dans cet article, nous présentons une approche de la segmentation thématique fondée sur une représentation en vecteurs sémantiques des phrases et des calculs de distance entre ces vecteurs. Les vecteurs sémantiques sont générés par le système SYGFRAN, un analyseur morpho-syntaxique et conceptuel de la langue française. La segmentation thématique s'effectue elle en recherchant des zones de transition au sein du texte grâce aux vecteurs sémantiques. L'évaluation de cette méthode s'est faite sur les données du défi DEFT'06.	Jacques Chauché, Alexandre Labadié	http://editions-rnti.fr/render_pdf.php?p1&p=1001398	http://editions-rnti.fr/render_pdf.php?p=1001398	1260	fr	fr	@lirmm.fr, @lirmm.fr	segmentation thématique par calcul de distance thématique  Dans ce article , nous présenter un approche de le segmentation thématique fonder sur un représentation en vecteur sémantique des phrase et des calcul de distance entre ce vecteur . le vecteur sémantique être générer par le système SYGFRAN , un analyseur morpho- syntaxique et conceptuel de le langue français . le segmentation thématique clr effectuer elle en rechercher un zone de transition au sein du texte grâce aux vecteur sémantique . le évaluation de ce méthode clr être faire sur le donnée du défi DEFT'06 . 	Segmentation thématique par calcul de distance thématique	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Sémantique et contextes conceptuels pour la recherche d'information	Cet article propose une méthodologie de recherche d'information qui utilise l'analyse conceptuelle conjointement avec la sémantique dans le but de fournir des réponses contextuelles à des requêtes sur le web. Le contexte conceptuel défini dans cet article peut être global - c'est-à-dire stable - ou instantané - c'est-à-dire borné par le contexte global. Notre méthodologie consiste en une première phase de pré traitement permettant de construire le contexte global, et une seconde phase de traitement en ligne des requêtes des utilisateurs, associées au contexte instantané. Notre processus de recherche d'information est illustré à travers une expérimentation dans le domaine du tourisme.	Michel Soto, Bénédicte Le Grand, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001440	http://editions-rnti.fr/render_pdf.php?p=1001440	1261	fr	fr	@supelec.fr, @inria.fr, @lip6.fr	sémantique et contexte conceptuel pour le recherche d' information  ce article proposer un méthodologie de recherche d' information qui utiliser le analyse conceptuel conjointement avec le sémantique dans le but de fournir un réponse contextuel à un requête sur le web . le contexte conceptuel définir dans ce article pouvoir être global - c' est-à-dire stable - ou instantané - c' est-à-dire borner par le contexte global . son méthodologie consister en un premier phase de pré traitement permettre de construire le contexte global , et un second phase de traitement en ligne des requête des utilisateur , associer au contexte instantané . son processus de recherche d' information être illustrer à travers un expérimentation dans le domaine du tourisme . 	Sémantique et contextes conceptuels pour la recherche d'information	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Sous-bases k-faibles pour des règles d'association valides au sens de la confiance	Nous introduisons la notion de sous-base k-faible pour les règles d'association valides au sens de la confiance. Ces sous-bases k-faibles sont caractérisées en termes d'opérateurs de fermeture correspondant à des familles de Moore k-faiblement hiérarchiques.	Jean Diatta, Régis Girard	http://editions-rnti.fr/render_pdf.php?p1&p=1001383	http://editions-rnti.fr/render_pdf.php?p=1001383	1262	fr	fr	@univ-reunion.fr	Sous-bases k-faibles pour un règle d' association valide au sens de le confiance  Nous introduire le notion de sous-base k-faible pour le règle d' association valide au sens de le confiance . ce sous-bases k-faibles être caractériser en terme d' opérateur de fermeture correspondre à un famille de Moore k-faiblement hiérarchique . 	Sous-bases k-faibles pour des règles d'association valides au sens de la confiance	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	SPoID : Extraction de motifs séquentiels pour les bases de données incomplètes	Les bases de données issues du monde réel contiennent souvent de nombreuses informations non renseignées. Durant le processus d'extraction de connaissances dans les bases de données, une phase de traitement spécifique de ces données est souvent nécessaire, permettant de les supprimer ou de les compléter. Lors de l'extraction de séquences fréquentes, ces données incomplètes sont la plupart du temps occultées. Ceci conduit parfois à l'élimination de plus de la moitié de la base et l'information extraite n'est plus représentative. Nous proposons donc de ne plus éliminer les enregistrements incomplets, mais d'utiliser l'information partielle qu'ils contiennent. La méthode proposée ignore en fait temporairement certaines données incomplètes pour les séquences recherchées. Les expérimentations sur jeux de données synthétiques montrent la validité de notre proposition aussi bien en terme de qualité des motifs extraits que de robustesse aux valeurs manquantes.	Céline Fiot, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001460	http://editions-rnti.fr/render_pdf.php?p=1001460	1263	fr	fr	@lirmm.fr	SPoID : extraction de motif séquentiel pour le base de donnée incomplètes  le base de donnée issu du monde réel contenir souvent un nombreux information non renseigner . Durant le processus d' extraction de connaissance dans le base de donnée , un phase de traitement spécifique de ce donnée être souvent nécessaire , permettre de les supprimer ou de les compléter . lors de le extraction de séquence fréquent , ce donnée incomplet être le plupart du temps occulter . ceci conduire parfois à le élimination de plus de le moitié de le base et le information extraire n' être plus représentatif . Nous proposer donc de ne plus éliminer le enregistrement incomplet , mais d' utiliser le information partiel qu' ils contenir . le méthode proposer ignorer en fait temporairement certain donnée incomplet pour le séquence rechercher . le expérimentation sur jeu de donnée synthétique montrer le validité de son proposition aussi bien en terme de qualité des motif extrait que de robustesse aux valeur manquant . 	SPoID : Extraction de motifs séquentiels pour les bases de données incomplètes	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	SyRQuS - Recherche par combinaison de graphes RDF	Nous nous intéressons à un mécanisme permettant la construction de réponses combinés à partir de plusieurs graphes RDF. Nous imposons, par souci de cohérence, que cette combinaison soit réalisée uniquement si les graphes RDF ne se contredisent pas. Pour déterminer la non-contradiction entre deux graphes RDF nous utilisons une mesure de similarité, calculée au moment de l'ajout de documents RDF dans la base de documents.	Adrian Tanasescu	http://editions-rnti.fr/render_pdf.php?p1&p=1001347	http://editions-rnti.fr/render_pdf.php?p=1001347	1264	fr	fr	@liris.cnrs.fr	SyRQuS - recherche par combinaison de graphe RDF  Nous nous intéresser à un mécanisme permettre le construction de réponse combiner à partir de plusieurs graphe RDF . Nous imposer , par souci de cohérence , que ce combinaison être réaliser uniquement si le graphe RDF ne clr contredire pas . Pour déterminer le non- contradiction entre deux graphe RDF nous utiliser un mesure de similarité , calculer au moment de le ajout de document RDF dans le base de document . 	SyRQuS - Recherche par combinaison de graphes RDF	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Traitement de données de consommation électrique par un Système de Gestion de Flux de Données	Avec le développement de compteurs communicants, les consommations d'énergie électrique pourront à terme être télérelevées par les fournisseurs d'électricité à des pas de temps pouvant aller jusqu'à la seconde. Ceci générera des informations en continu, à un rythme rapide et en quantité importante. Les Systèmes de Gestion de Flux de Données (SGFD), aujourd'hui disponibles sous forme de prototypes, ont vocation à faciliter la gestion de tels flux. Cette communication décrit une étude expérimentale pour analyser les avantages et limites de l'utilisation de deux prototypes de SGFD (STREAM et TelegraphCQ) pour la gestion de données de consommation électrique.	Talel Abdessalem, Raja Chiky, Georges Hébrail, Jean Louis Vitti	http://editions-rnti.fr/render_pdf.php?p1&p=1001432	http://editions-rnti.fr/render_pdf.php?p=1001432	1265	fr	fr	@enst.fr, @edf.fr	traitement de donnée de consommation électrique par un système de gestion de Flux de Données  Avec le développement de compteur communicant , le consommation d' énergie électrique pouvoir à terme être télérelevées par le fournisseur d' électricité à un pas de temps pouvoir aller jusqu' à le second . ceci générer un information en continu , à un rythme rapide et en quantité important . le système de gestion de Flux de Données ( SGFD ) , aujourd' hui disponible sous forme de prototype , avoir vocation à faciliter le gestion de tel flux . ce communication décrire un étude expérimental pour analyser le avantage et limite de le utilisation de deux prototype de SGFD ( STREAM et TelegraphCQ ) pour le gestion de donnée de consommation électrique . 	Traitement de données de consommation électrique par un Système de Gestion de Flux de Données	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	Traitement et exploration du fichier Log du Serveur Web pour l'extraction des connaissances : Web Usage Mining	Le but dans ce travail consiste à concevoir et réaliser un Outil Logiciel, en utilisant les concepts du Web Usage Mining pour offrir aux web masters l'ensemble des connaissances, y inclut les statistiques sur leurs sites, afin de prendre les décisions adéquates. Il s'agit en fait, d'extraire de l'information à partir du fichier log du serveur Web, hébergeant le site Web, et de prendre les décisions pour découvrir les habitudes des internautes, et de répondre à leurs besoins en adaptant le contenu, la forme et l'agencement des pages web.	Mostafa Hanoune, Faouzia Benabbou	http://editions-rnti.fr/render_pdf.php?p1&p=1001344	http://editions-rnti.fr/render_pdf.php?p=1001344	1266	fr	fr	@yahoo.fr, @menara.ma	traitement et exploration du fichier Log du Serveur Web pour le extraction des connaissance : web Usage Mining  le but dans ce travail consister à concevoir et réaliser un outil Logiciel , en utiliser le concept du Web Usage Mining pour offrir aux web masters le ensemble des connaissance , y inclure le statistique sur son site , afin de prendre le décision adéquat . Il clr agir en fait , d' extraire de le information à partir du fichier logarithme du serveur Web , héberger le site Web , et de prendre le décision pour découvrir le habitude des internaute , et de répondre à son besoin en adapter le contenu , le forme et le agencement des page web . 	Traitement et exploration du fichier Log du Serveur Web pour l'extraction des connaissances : Web Usage Mining	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un algorithme multi-agent de classification pour la construction d'ontologies dynamiques	La construction d'ontologies à partir de textes reste une tâche coûteuse en temps qui justifie l'émergence de l'Ontology Learning. Notre système, Dynamo, s'inscrit dans cette mouvance, en apportant une approche originale basée sur une architecture multi-agent adaptative. En particulier, l'article présente le coeur de notre approche, un algorithme distribué de classification hiérarchique qui s'applique sur les résultats d'un analyseur syntaxique. Cet algorithme est évalué et comparé à un algorithme centralisé plus conventionnel. Forts de ces résultats, nous discutons ses limites et dressons en perspective les aménagements à effectuer pour aller vers une solution complète de construction d'ontologies.	Kévin Ottens, Nathalie Aussenac-Gilles	http://editions-rnti.fr/render_pdf.php?p1&p=1001452	http://editions-rnti.fr/render_pdf.php?p=1001452	1267	fr	fr	@irit.fr	un algorithme multi-agent de classification pour le construction d' ontologie dynamiques  le construction d' ontologie à partir de texte rester un tâche coûteux en temps qui justifier le émergence de le Ontology Learning . son système , Dynamo , clr inscrire dans ce mouvance , en apporter un approche original baser sur un architecture multi-agent adaptatif . En particulier , le article présent le coeur de son approche , un algorithme distribuer de classification hiérarchique qui clr appliquer sur le résultat d' un analyseur syntaxique . ce algorithme être évaluer et comparer à un algorithme centraliser plus conventionnel . fort de ce résultat , nous discuter son limite et dresser en perspective le aménagement à effectuer pour aller vers un solution complet de construction d' ontologie . 	Un algorithme multi-agent de classification pour la construction d'ontologies dynamiques	3
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un cadre théorique pour la gestion de grandes bases de motifs	Les algorithmes de fouille de données sont maintenant capables de traiter de grands volumes de données mais les utilisateurs sont souvent submergés par la quantité de motifs générés. En outre, dans certains cas, que ce soit pour des raisons de confidentialité ou de coûts, les utilisateurs peuvent ne pas avoir accès directement aux données et ne disposer que des motifs. Les utilisateurs n'ont plus alors la possibilité d'approfondir à partir des données initiales le processus de fouille de façon à extraire des motifs plus spécifiques. Pour remédier à cette situation, une solution consiste à gérer les motifs. Ainsi, dans cet article, nous présentons un cadre théorique permettant à un utilisateur de manipuler, en post-traitement, une collection de motifs préalablement extraite. Nous proposons de représenter la collection sous la forme d'un graphe qu'un utilisateur pourra ensuite exploiter à l'aide d'opérateurs algébriques pour y retrouver des motifs ou en chercher de nouveaux.	François Jacquenet, Baptiste Jeudy, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001385	http://editions-rnti.fr/render_pdf.php?p=1001385	1268	fr	fr	@univ-st-etienne.fr	un cadre théorique pour le gestion de grand base de motifs  le algorithme de fouille de donnée être maintenant capable de traiter un grand volume de donnée mais le utilisateur être souvent submerger par le quantité de motif générer . En outre , dans certain cas , que ce être pour un raison de confidentialité ou de coût , le utilisateur pouvoir ne pas avoir accès directement aux donnée et ne disposer que un motif . le utilisateur n' avoir plus alors le possibilité d' approfondir à partir un donnée initial le processus de fouille de façon à extraire un motif plus spécifique . Pour remédier à ce situation , un solution consister à gérer le motif . ainsi , dans ce article , nous présenter un cadre théorique permettre à un utilisateur de manipuler , en post-traitement , un collection de motif préalablement extraire . Nous proposer de représenter le collection sous le forme d' un graphe qu' un utilisateur pouvoir ensuite exploiter à le aide d' opérateur algébrique pour y retrouver un motif ou en chercher un nouveau . 	Un cadre théorique pour la gestion de grandes bases de motifs	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un outil pour la visualisation de relations entre gènes	La reconstruction de réseaux de gènes est un des défis majeurs de la post-génomique. A partir de données d'expression issues de puces à ADN, différentes techniques existent pour inférer des réseaux de gènes. Nous proposons dans ce papier une approche pour la visualisation de réseaux d'interactions entre gènes à partir de données d'expression. L'originalité de notre approche est de superposer des règles avec des sémantiques différentes au sein d'un même support visuel et de ne générer que les règles qui impliquent des gènes dits centraux. Ceux-ci sont spécifiés en amont par les experts et permettent de limiter la génération des règles aux seuls gènes qui intéressent les spécialistes. Une implémentation a été réalisée dans le logiciel libre MeV de l'institut TIGR.	Jean-Marc Petit, Marie Agier	http://editions-rnti.fr/render_pdf.php?p1&p=1001342	http://editions-rnti.fr/render_pdf.php?p=1001342	1269	fr	fr	@isima, @insa-lyon.fr	un outil pour le visualisation de relation entre gènes  le reconstruction de réseau de gène être un des défi majeur de le post-génomique . A partir de donnée d' expression issu de puce à ADN , différentes technique exister pour inférer un réseau de gène . Nous proposer dans ce papier un approche pour le visualisation de réseau d' interaction entre gène à partir de donnée d' expression . le originalité de son approche être de superposer un règle avec un sémantique différent au sein d' un même support visuel et de ne générer que le règle qui impliquer un gène dire central . celui _-ci être spécifier en amont par le expert et permettre de limiter le génération des règle aux seul gène qui intéresser le spécialiste . un implémentation avoir être réaliser dans le logiciel libre MeV de le institut TIGR . 	Un outil pour la visualisation de relations entre gènes	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un segmenteur de texte en phrases guidé par l'utilisateur	Ce programme effectue une segmentation en phrases d'un texte. Contrairement aux procédures classiques, nous n'utilisons pas d'annotations préliminaires et tirons parti d'un apprentissage guidé par l'utilisateur.	Thomas Heitz	http://editions-rnti.fr/render_pdf.php?p1&p=1001334	http://editions-rnti.fr/render_pdf.php?p=1001334	1270	fr	fr	@lri.fr	un segmenteur de texte en phrase guider par le utilisateur  ce programme effectuer un segmentation en phrase d' un texte . contrairement aux procédure classique , nous n' utiliser pas un annotation préliminaire et tirer partir d' un apprentissage guider par le utilisateur . 	Un segmenteur de texte en phrases guidé par l'utilisateur	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche de classification non supervisée basée sur la détection de singularités et la corrélation de séries temporelles pour la recherche d'états : application à un bioprocédé fed-batch	Nous proposons dans cet article une méthode de clustering qui combine l'analyse dynamique et l'analyse statistique pour caractériser des états. Il s'agit d'une méthode de fouille de données qui travaille sur des ensembles de séries temporelles pour détecter des états; ces états représentent les informations les plus significatives du système. L'objectif de cette méthode non supervisée est d'extraire de la connaissance à partir de l'analyse des séries temporelles multiples. Elle s'appuie sur la détection de singularités dans les séries temporelles et sur l'analyse des corrélations des séries entre les intervalles définis par ces singularités. Pour l'application présentée, les séries temporelles sont des signaux biochimiques mesurés durant un bioprocédé. Cette approche est donc utilisée pour confirmer et enrichir la connaissance des experts du domaine des bioprocédés sans utiliser la connaissance a priori de ces experts. Elle est appliquée à la recherche d'états physiologiques dans un bioprocédé de type fed-batch.	Sébastien Régis	http://editions-rnti.fr/render_pdf.php?p1&p=1001453	http://editions-rnti.fr/render_pdf.php?p=1001453	1271	fr	fr	@univ-ag.fr	un approche de classification non superviser baser sur le détection de singularité et le corrélation de série temporel pour le recherche d' état : application à un bioprocédé fed-batch  Nous proposer dans ce article un méthode de clustering qui combiner le analyse dynamique et le analyse statistique pour caractériser un état . Il clr agir d' un méthode de fouille de donnée qui travailler sur un ensemble de série temporel pour détecter un état ; ce état représenter le information le plus significatif du système . le objectif de ce méthode non superviser être d' extraire de le connaissance à partir de le analyse des série temporel multiple . Elle clr appuyer sur le détection de singularité dans le série temporel et sur le analyse des corrélation des série entre le intervalle définir par ce singularité . Pour le application présenter , le série temporel être un signal biochimique mesurer durant un bioprocédé . ce approche être donc utiliser pour confirmer et enrichir le connaissance des expert du domaine des bioprocédés sans utiliser le connaissance avoir priori de ce expert . Elle être appliquer à le recherche d' état physiologique dans un bioprocédé de type fed-batch . 	Une approche de classification non supervisée basée sur la détection de singularités et la corrélation de séries temporelles pour la recherche d'états : application à un bioprocédé fed-batch	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche non paramétrique Bayésienne pour l'estimation de densité conditionnelle sur les rangs	Nous nous intéressons à l'estimation de la distribution des rangs d'une variable cible numérique conditionnellement à un ensemble de prédicteurs numériques. Pour cela, nous proposons une nouvelle approche non paramétrique Bayesienne pour effectuer une partition rectangulaire optimale de chaque couple (cible, prédicteur) uniquement à partir des rangs des individus. Nous montrons ensuite comment les effectifs de ces grilles nous permettent de construire un estimateur univarié de la densité conditionnelle sur les rangs et un estimateur multivarié utilisant l'hypothèse Bayesienne naïve. Ces estimateurs sont comparés aux meilleures méthodes évaluées lors d'un récent Challenge sur l'estimation d'une densité prédictive. Si l'estimateur Bayésien naïf utilisant l'ensemble des prédicteurs se révèle peu performant, l'estimateur univarié et l'estimateur combinant deux prédicteurs donne de très bons résultats malgré leur simplicité.	Marc Boullé, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1001317	http://editions-rnti.fr/render_pdf.php?p=1001317	1272	fr	fr	@orange-ftgroup.com, @orange-ftgroup.com	un approche non paramétrique Bayésienne pour le estimation de densité conditionnel sur le rangs  Nous nous intéresser à le estimation de le distribution des rang d' un variable cible numérique conditionnellement à un ensemble de prédicteurs numérique . Pour cela , nous proposer un nouveau approche non paramétrique Bayesienne pour effectuer un partition rectangulaire optimal de chaque couple ( cible , prédicteur ) uniquement à partir un rang des individu . Nous montrer ensuite comment le effectif de ce grille nous permettre de construire un estimateur univarié de le densité conditionnel sur le rang et un estimateur multivarié utiliser le hypothèse Bayesienne naïf . ce estimateur être comparer aux meilleur méthode évaluer lors d' un récent challenge sur le estimation d' un densité prédictif . Si le estimateur Bayésien naïf utiliser le ensemble des prédicteurs clr révéler peu performant , le estimateur univarié et le estimateur combiner deux prédicteurs donne de très bon résultat malgré son simplicité . 	Une approche non paramétrique Bayésienne pour l'estimation de densité conditionnelle sur les rangs	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche sociotechnique pour le Knowledge Management (KM)	Cet article présente un cadre sociotechnique pour le KM. Cette vision sociotechnique du KM permet : (1) d'écarter le KM d'un souci commercial ; (2) faire le clivage des différentes technologies du KM ; et (3) de s'interroger sur les paradigmes associés aux composants social et technique du KM. C'est précisément ce dernier point que cet article développe afin d'identifier les mécanismes génériques du KM. Plus précisément, l'aspect social est décrit à travers l'approche organisationnelle du KM, l'approche managériale du KM, et l'approche biologique du KM, alors que l'aspect technique est décrit à travers l'approche ingénierie des connaissances et compétences du KM. Ces approches nous conduisent aussi à donner un tableau comparatif entre ces visions organisationnelles, managériales et biologiques du KM.	Leoncio Jiménez	http://editions-rnti.fr/render_pdf.php?p1&p=1001436	http://editions-rnti.fr/render_pdf.php?p=1001436	1273	fr	fr	@spock.ucm.cl	un approche sociotechnique pour le Knowledge Management ( KM )  ce article présenter un cadre sociotechnique pour le KM . ce vision sociotechnique du KM permettre : ( 1 ) d' écarter le KM d' un souci commercial ; ( 2 ) faire le clivage des différent technologie du KM ; et ( 3 ) de clr interroger sur le paradigme associer aux composant social et technique du KM . C' être précisément ce dernier point que ce article développer afin d' identifier le mécanisme générique du KM . plus précisément , le aspect social être décrire à travers le approche organisationnel du KM , le approche managérial du KM , et le approche biologique du KM , alors que le aspect technique être décrire à travers le approche ingénierie des connaissance et compétence du KM . ce approche nous conduire aussi à donner un tableau comparatif entre ce vision organisationnel , managérial et biologique du KM . 	Une approche sociotechnique pour le Knowledge Management (KM)	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une étude des algorithmes de construction d'architecture des réseaux de neurones multicouches	Le problème de choix d'architecture d'un réseau de neurones multicouches reste toujours très difficile à résoudre dans un processus de fouille de données. Ce papier recense quelques algorithmes de recherche d'architectures d'un réseau de neurones pour les tâches de classification. Il présente également une analyse théorique et expérimentale de ces algorithmes. Ce travail confirme les difficultés de choix des paramètres d'apprentissage (modèle, nombre de couches, nombre de neurones par couches, taux d'apprentissage, algorithme d'apprentissage,...) communs à tout processus de construction de réseaux de neurones et les difficultés de choix de paramètres propres à certains algorithmes.	Norbert Tsopzé, Engelbert Mephu Nguifo, Gilbert Tindo	http://editions-rnti.fr/render_pdf.php?p1&p=1001288	http://editions-rnti.fr/render_pdf.php?p=1001288	1274	fr	fr	@univ-artois.fr, @gmail.com, @uycdc.uninet.cm	un étude des algorithme de construction d' architecture des réseau de neurone multicouches  le problème de choix d' architecture d' un réseau de neurone multicouches rester toujours très difficile à résoudre dans un processus de fouille de donnée . ce papier recenser quelque algorithme de recherche d' architecture d' un réseau de neurone pour le tâche de classification . Il présenter également un analyse théorique et expérimental de ce algorithme . ce travail confirmer le difficulté de choix des paramètre d' apprentissage ( modèle , nombre de couche , nombre de neurone par couche , taux d' apprentissage , algorithme d' apprentissage , ... ) commun à tout processus de construction de réseau de neurone et le difficulté de choix de paramètre propre à certain algorithme . 	Une étude des algorithmes de construction d'architecture des réseaux de neurones multicouches	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une extension de XQuery pour la recherche textuelle d'information dans des documents XML	Nous présentons dans cet article une extension de XQuery que nous avons développée pour interroger le contenu et la structure de documents XML. Cette extension consiste à intégrer dans XQuery le langage NEXI, un sous-ensemble de XPath, défini dans le cadre de l'initiative INEX. Notre proposition est double : (i) équiper NEXI d'une sémantique floue, (ii) intégrer NEXI dans XQuery au moyen d'une métafonction appelée nexi, ayant une requête NEXI comme paramètre, et d'une extension de la clause for de l'opérateur FLWOR de XQuery. De plus, nous décrivons le prototype paramétrable que nous avons développé au dessus de deux moteurs XQuery classiques : Galax et Saxon.	Jacques Le Maitre, Nicolas Faessel	http://editions-rnti.fr/render_pdf.php?p1&p=1001401	http://editions-rnti.fr/render_pdf.php?p=1001401	1275	fr	fr	@lsis.org, @univ-tln.fr	un extension de XQuery pour le recherche textuel d' information dans un document XML  Nous présenter dans ce article un extension de XQuery que nous avoir développer pour interroger le contenu et le structure de document XML . ce extension consister à intégrer dans XQuery le langage NEXI , un sous-ensemble de XPath , définir dans le cadre de le initiative INEX . son proposition être double : ( i ) équiper NEXI d' un sémantique flou , ( ii ) intégrer NEXI dans XQuery au moyen d' un métafonction appeler nexi , avoir un requête NEXI comme paramètre , et d' un extension de le clause for de le opérateur FLWOR de XQuery . De plus , nous décrire le prototype paramétrable que nous avoir développer au dessus de deux moteur XQuery classique : Galax et Saxon . 	Une extension de XQuery pour la recherche textuelle d'information dans des documents XML	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une méthode d'interprétation de scores	Cet article présente une méthode permettant d'interpréter la sortie d'un modèle de classification ou de régression. L'interprétation se base sur l'importance de la variable et l'importance de la valeur de la variable. Cette approche permet d'interpréter la sortie du modèle pour chaque instance.	Raphaël Feraud, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001348	http://editions-rnti.fr/render_pdf.php?p=1001348	1276	fr	fr	@orange-ft.com	un méthode d' interprétation de scores  ce article présenter un méthode permettre d' interpréter le sortie d' un modèle de classification ou de régression . le interprétation clr baser sur le importance de le variable et le importance de le valeur de le variable . ce approche permettre d' interpréter le sortie du modèle pour chaque instance . 	Une méthode d'interprétation de scores	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une méthode optimale d'évaluation bivariée pour la classification supervisée	En préparation des données pour la classification supervisée, les méthodes filtres usuellement utilisées pour la sélection de variables sont efficaces en temps de calcul. Néanmoins, leur nature univariée ne permet pas de détecter les redondances ou les interactions constructives entre variables. Cet article présente une nouvelle méthode permettant d'évaluer l'importance prédictive jointe d'une paire de variables de façon automatique, rapide et fiable. Elle est basée sur un partitionnement de chaque variable exogène, en intervalles dans le cas numérique et groupes de valeurs dans le cas catégoriel. La grille de données exogène résultante permet alors d'évaluer la corrélation entre la paire de variables exogènes et la variable endogène. Le meilleur partitionnement bivarié est recherché au moyen d'une approche Bayésienne de la sélection de modèle. Les expérimentations démontrent les apports de la méthode, notamment une amélioration significative des performances en classification.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001420	http://editions-rnti.fr/render_pdf.php?p=1001420	1277	fr	fr	@orange-ft.com	un méthode optimal d' évaluation bivariée pour le classification supervisée  En préparation des donnée pour le classification superviser , le méthode filtre usuellement utiliser pour le sélection de variable être efficace en temps de calcul . néanmoins , son nature univariée ne permettre pas de détecter le redondance ou le interaction constructif entre variable . ce article présenter un nouveau méthode permettre d' évaluer le importance prédictif joindre d' un paire de variable de façon automatique , rapide et fiable . Elle être baser sur un partitionnement de chaque variable exogène , en intervalle dans le cas numérique et groupe de valeur dans le cas catégoriel . le grille de donnée exogène résultante permettre alors d' évaluer le corrélation entre le paire de variable exogène et le variable endogène . le meilleur partitionnement bivarié être rechercher au moyen d' un approche Bayésienne de le sélection de modèle . le expérimentation démontrer le apport de le méthode , notamment un amélioration significatif des performance en classification . 	Une méthode optimale d'évaluation bivariée pour la classification supervisée	7
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une nouvelle approche de la programmation DC et DCA pour la classification floue	Dans cet article, nous nous intéressons à Fuzzy C-Means (FCM), une technique très connue pour la classification floue. Nous proposons un algorithme efficace basé sur la programmation DC (Difference of Convexe functions) et DCA (DC Algorithm) pour résoudre ce problème. Les expériences numériques comparatives avec l'algorithme standard FCM sur les données réelles montrent la robustesse, la performance de cet nouvel algorithme DCA et sa supériorité par rapport à FCM.	Le Thi Hoai An, Le Hoai Minh, Pham Dinh Tao	http://editions-rnti.fr/render_pdf.php?p1&p=1001459	http://editions-rnti.fr/render_pdf.php?p=1001459	1278	fr	fr	@univ-metz.fr, @univ-metz.fr, @insa-rouen.fr	un nouveau approche de le programmation DC et DCA pour le classification floue  Dans ce article , nous clr intéresser à Fuzzy C-Means ( FCM ) , un technique très connaître pour le classification flou . Nous proposer un algorithme efficace baser sur le programmation DC ( Difference of Convexe functions ) et DCA ( DC Algorithm ) pour résoudre ce problème . le expérience numérique comparatif avec le algorithme standard FCM sur le donnée réel montrer le robustesse , le performance de ce nouveau algorithme DCA et son supériorité par rapport à FCM . 	Une nouvelle approche de la programmation DC et DCA pour la classification floue	4
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une nouvelle méthode d'alignement et de visualisation d'ontologies OWL-Lite	Dans ce papier, une nouvelle plate-forme d'alignement et de visualisation des ontologies, appelée POVA (Prototype OWL-Lite Visual Alignment), est décrite. Le module d'alignement implémente une nouvelle approche d'alignement d'ontologies remédiant au problème de la circularité et de l'intervention de l'utilisateur.	Sami Zghal, Karim Kamoun, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001355	http://editions-rnti.fr/render_pdf.php?p=1001355	1279	fr	fr	@fst.rnu.tn, @univ-artois.fr	un nouveau méthode d' alignement et de visualisation d' ontologie OWL-Lite  Dans ce papier , un nouveau plate-forme d' alignement et de visualisation des ontologie , appeler POVA ( Prototype OWL-Lite Visual Alignment ) , être décrire . le module d' alignement implémenter un nouveau approche d' alignement d' ontologie remédier au problème de le circularité et de le intervention de le utilisateur . 	Une nouvelle méthode d'alignement et de visualisation d'ontologies OWL-Lite	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une règle d'exception en Analyse Statistique Implicative	En fouille de règles, certaines situations exceptionnelles défient le bon sens. C'est le cas de la règle R : a --> c et b --> c et (a et b) --> non c. Une telle règle, que nous étudions dans l'article, est appelée règle d'exception. A la suite des travaux précurseurs de E. Suzuki et Y. Kodratoff (1999), qui ont étudié un autre type de règle d'exception, nous cherchons ici à caractériser les conditions d'apparition de la règle R dans le cadre de l'Analyse Statistique Implicative.	Régis Gras, Pascale Kuntz, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001312	http://editions-rnti.fr/render_pdf.php?p=1001312	1280	fr	fr	@club-internet.fr, @univ-nantes.fr, @i.kyushu-u.ac.jp	un règle d' exception en analyse statistique Implicative  En fouille de règle , certain situation exceptionnel défier le bon sens . C' être le cas de le règle R : avoir -- c et b -- c et ( avoir et b ) -- non c . un tel règle , que nous étudier dans le article , être appeler règle d' exception . A le suite des travail précurseur de E. Suzuki et Y. Kodratoff ( 1999 ) , qui avoir étudier un autre type de règle d' exception , nous chercher ici à caractériser le condition d' apparition de le règle R dans le cadre de le analyse statistique Implicative . 	Une règle d'exception en Analyse Statistique Implicative	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Utilisation de WordNet dans la catégorisation de textes multilingues	Cet article est consacré au problème de la catégorisation multilingue qui consiste à catégoriser des documents de différentes langues en utilisant le même classifieur. L'approche que nous proposons est basée sur l'idée d'étendre l'utilisation de WordNet dans la catégorisation monolingue vers la catégorisation multilingue.	Mohamed Amine Bentaallah, Mimoun Malki	http://editions-rnti.fr/render_pdf.php?p1&p=1001353	http://editions-rnti.fr/render_pdf.php?p=1001353	1281	fr	fr	@univ-sba.dz, @yahoo.com	utilisation de WordNet dans le catégorisation de texte multilingues  ce article être consacrer au problème de le catégorisation multilingue qui consister à catégoriser un document de différent langue en utiliser le même classifieur . le approche que nous proposer être baser sur le idée d' étendre le utilisation de WordNet dans le catégorisation monolingue vers le catégorisation multilingue . 	Utilisation de WordNet dans la catégorisation de textes multilingues	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Validation des visualisations par axes principaux de données numériques et textuelles	Parmi les outils de visualisation de données multidimensionnelles figurent d'une part les méthodes fondées sur la décomposition aux valeurs singulières, et d'autre part les méthodes de classification, incluant les cartes auto-organisées de Kohonen. Comment valider ces visualisations ? On présente sept procédures de validation par bootstrap qui dépendent des données, des hypothèses, des outils : a) le bootstrap partiel, qui considère les réplications comme des variables supplémentaires; b) le bootstrap total de type 1, qui réanalyse les réplications avec changements éventuels de signes des axes; c) le bootstrap total de type 2 qui corrige aussi les interversions d'axes; d) le bootstrap total de type 3, sur lequel on insistera, qui corrige les réplications par rotations procrustéenne; e) le bootstrap spécifique (cas des hiérarchies d'individus statistiques et des données textuelles). f) le bootstrap sur variables. g) les extensions des procédures précédentes à certaines cartes auto-organisées.	Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001332	http://editions-rnti.fr/render_pdf.php?p=1001332	1282	fr	fr	@enst.fr	validation des visualisation par axe principal de donnée numérique et textuelles  Parmi le outil de visualisation de donnée multidimensionnel figurer d' un part le méthode fonder sur le décomposition aux valeur singulier , et d' autre part le méthode de classification , inclure le carte auto- organiser de Kohonen . Comment valider ce visualisation ? On présenter sept procédure de validation par bootstrap qui dépendre un donnée , un hypothèse , un outil : avoir ) le bootstrap partiel , qui considérer le réplication comme un variable supplémentaire ; b ) le bootstrap total de type 1 , qui réanalyser le réplication avec changement éventuel de signe des axe ; c ) le bootstrap total de type 2 qui corriger aussi le interversion d' axe ; d ) le bootstrap total de type 3 , sur lequel on insister , qui corriger le réplication par rotation procrustéenne ; e ) le bootstrap spécifique ( cas des hiérarchie d' individu statistique et des donnée textuel ) . f ) le bootstrap sur variable . gramme ) le extension des procédure précédent à certain carte auto- organiser . 	Validation des visualisations par axes principaux de données numériques et textuelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers un algorithme multi-agents de clustering dynamique	Dans cet article, nous présentons un algorithme multi-agents de clustering dynamique. Ce type de clustering doit permettre de gérer des données évolutives et donc être capable d'adapter en permanence les clusters construits.	Bruno Mermet, Gaële Simon, Dominique Fournier	http://editions-rnti.fr/render_pdf.php?p1&p=1001357	http://editions-rnti.fr/render_pdf.php?p=1001357	1283	fr	fr	@univ-lehavre.fr, @univ-lehavre.fr	Vers un algorithme multi-agents de clustering dynamique  Dans ce article , nous présenter un algorithme multi-agents de clustering dynamique . ce type de clustering devoir permettre de gérer un donnée évolutif et donc être capable d' adapter en permanence le clusters construire . 	Vers un algorithme multi-agents de clustering dynamique	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers un système hybride pour l'annotation sémantique d'images IRM du cerveau	Cet article montre l'intérêt de combiner des méthodes numériques et symboliques pour obtenir une annotation sémantique des images IRM du cerveau humain. Il s'agit d'identifier des structures anatomiques du cortex cérébral humain, en utilisant conjointement des connaissances a priori de nature numérique et une ontologie des structures corticales du cerveau représentée en OWL DL, étendue par des règles SWRL. Ces connaissances symboliques a priori représentées dans des langages standards du Web deviennent non seulement partageables mais permettent aussi un raisonnement automatique qui aide l'utilisateur à la labellisation des structures anatomiques mises en évidence dans des images IRM du cerveau d'un individu donné.	Ammar Mechouche, Christine Golbreich, Bernard Gibaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001417	http://editions-rnti.fr/render_pdf.php?p=1001417	1284	fr	fr	@irisa.fr, @univ-rennes1.fr	Vers un système hybride pour le annotation sémantique d' image IRM du cerveau  ce article montrer le intérêt de combiner un méthode numérique et symbolique pour obtenir un annotation sémantique des image IRM du cerveau humain . Il clr agir d' identifier un structure anatomique du cortex cérébral humain , en utiliser conjointement un connaissance avoir priori de nature numérique et un ontologie des structure cortical du cerveau représenter en OWL DL , étendre par un règle SWRL . ce connaissance symbolique avoir priori représenter dans un langage standard du Web devenir non seulement partageable mais permettre aussi un raisonnement automatique qui aider le utilisateur à le labellisation des structure anatomique mettre en évidence dans un image IRM du cerveau d' un individu donner . 	Vers un système hybride pour l'annotation sémantique d'images IRM du cerveau	1
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une base de connaissances biographique : extraction d'information et ontologie	Le projet B-Ontology a pour but l'extraction, l'organisation et l'exploitation de connaissances biographiques à partir de dépêches de presse. Sa réalisation requiert l'intégration de diverses technologies, principalement l'extraction d'information, les ontologies et bases de connaissances, les techniques de data mining. Cet article propose un aperçu des choix réalisés dans le cadre du projet. Cette démarche permet également de définir un environnement d'outils utiles pour les applications d'extraction et de gestion de connaissances.	Laurent Kevers, Cédrick Fairon	http://editions-rnti.fr/render_pdf.php?p1&p=1001400	http://editions-rnti.fr/render_pdf.php?p=1001400	1285	fr	fr	@uclouvain.be, @uclouvain.be	Vers un base de connaissance biographique : extraction d' information et ontologie  le projet B-Ontology avoir pour but le extraction , le organisation et le exploitation de connaissance biographique à partir de dépêche de presse . son réalisation requérir le intégration de divers technologie , principalement le extraction d' information , le ontologie et base de connaissance , le technique de data mining . ce article proposer un aperçu des choix réaliser dans le cadre du projet . ce démarche permettre également de définir un environnement d' outil utile pour le application d' extraction et de gestion de connaissance . 	Vers une base de connaissances biographique : extraction d'information et ontologie	2
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une nouvelle approche d'extraction des motifs séquentiels non-dérivables	L'extraction de motifs séquentiels est un défi important pour la communauté fouille de données. Même si les représentations condensées ont montré leur intérêt dans le domaine des itemsets, à l'heure actuelle peu de travaux considèrent ce type de représentation pour extraire des motifs. Cet article propose d'établir les premières bases formelles pour obtenir les bornes inférieures et supérieures du support d'une séquence S. Nous démontrons que ces bornes peuvent être dérivées à partir des sous-séquences de S et prouvons que ces règles de dérivation permettent la construction d'une nouvelle représentation condensée de l'ensemble des motifs fréquents. Les différentes expérimentations menées montrent que notre approche offre une meilleure représentation condensée que celles des motifs clos et cela sans perte d'information.	Pascal Poncelet, Chedy Raïssi	http://editions-rnti.fr/render_pdf.php?p1&p=1001391	http://editions-rnti.fr/render_pdf.php?p=1001391	1286	fr	fr	@lirmm.fr, @ema.fr	Vers un nouveau approche d' extraction des motif séquentiel non- dérivables  le extraction de motif séquentiel être un défi important pour le communauté fouiller de donnée . même si le représentation condenser avoir montrer son intérêt dans le domaine des itemsets , à le heure actuel peu de travail considérer ce type de représentation pour extraire un motif . ce article proposer d' établir le premier base formel pour obtenir le borne inférieur et supérieur du support d' un séquence S. Nous démontrer que ce borne pouvoir être dériver à partir un sous-séquences de S et prouver que ce règle de dérivation permettre le construction d' un nouveau représentation condenser de le ensemble des motif fréquent . le différent expérimentation mener montrer que son approche offrir un meilleur représentation condenser que celui des motif clore et cela sans perte d' information . 	Vers une nouvelle approche d'extraction des motifs séquentiels non-dérivables	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une plate-forme interactive pour la visualisation de grands ensembles de règles d'association	"La recherche de règles d'association est une question centrale en Extraction de Connaissances dans les Données (ECD). Dans cet article, nous nous intéressons plus particulièrement à la restitution visuelle de règles pertinentes dans un corpus très important. Nous proposons ainsi un prototype basé sur une approche de type ""wrapper"" par intégration des phases d'extraction et de visualisation de l'ECD. Tout d'abord, le processus d'extraction génère une base générique de règles et dans un second temps, la tâche de visualisation s'appuie sur un processus de regroupement (""clustering"") permettant de grouper et de visualiser un sous-ensemble de règles d'association génériques. Le rendu visuel à l'écran exploite une représentation de type ""Fisheye view"" de manière à obtenir simultanément une représentation globale des différents groupes de règles et une vue détaillée du groupe sélectionné."	Olivier Couturier, Tarek Hamrouni, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001381	http://editions-rnti.fr/render_pdf.php?p=1001381	1287	fr	fr	@univ-artois.fr, @fst.rnu.tn	Vers un plate-forme interactif pour le visualisation de grand ensemble de règle d' association  " le recherche de règle d' association être un question central en extraction de connaissance dans le Données ( ECD ) . . Dans ce article , nous clr intéresser plus particulièrement à le restitution visuel de règle pertinent dans un corpus très important . . Nous proposer ainsi un prototype baser sur un approche de type " " wrapper " " par intégration des phase d' extraction et de visualisation de le ECD . . tout d' abord , le processus d' extraction générer un base générique de règle et dans un second temps , le tâche de visualisation clr appuyer sur un processus de regroupement ( " " clustering " " ) permettre de grouper et de visualiser un sous-ensemble de règle d' association générique . . le rendre visuel à le écran exploiter un représentation de type " " Fisheye view " " de manière à obtenir simultanément un représentation global des différent groupe de règle et un vue détailler du groupe sélectionner . " 	Vers une plate-forme interactive pour la visualisation de grands ensembles de règles d'association	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	Visualisation de graphes avec Tulip : exploration interactive de grandes masses de données en appui à la fouille de données et à l'extraction de connaissances	Cet article décrit une étude de cas exhibant les qualités de la plateforme de visualisation de graphes Tulip, démontrant l'apport de la visualisation à la fouille de données interactive et à l'extraction de connaissances. Le calcul d'un graphe à partir d'indices de similarité est un exemple typique où l'exploration visuelle et interactive de graphes vient en appui au travail de fouille de données. Nous penchons sur le cas où l'on souhaite étudier une collection de documents afin d'avoir une idée des thématiques abordées dans la collection.	David Auber, Yves Chiricota, Maylis Delest, Jean-Philippe Domenger, Patrick Mary, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001330	http://editions-rnti.fr/render_pdf.php?p=1001330	1288	fr	fr	@labri.fr, @uqac.ca, @lirmm.fr	visualisation de graphe avec Tulip : exploration interactif de grand masse de donnée en appui à le fouille de donnée et à le extraction de connaissances  ce article décrire un étude de cas exhiber le qualité de le plateforme de visualisation de graphe Tulip , démontrer le apport de le visualisation à le fouille de donnée interactif et à le extraction de connaissance . le calcul d' un graphe à partir d' indice de similarité être un exemple typique où le exploration visuel et interactif de graphe venir en appui au travail de fouille de donnée . Nous pencher sur le cas où le on souhaiter étudier un collection de document afin d' avoir un idée des thématique aborder dans le collection . 	Visualisation de graphes avec Tulip : exploration interactive de grandes masses de données en appui à la fouille de données et à l'extraction de connaissances	9
Revue des Nouvelles Technologies de l'Information	EGC	2007	Visualisation exploratoire des résultats d'algorithmes d'arbre de décision	Nous présentons une méthode d'exploration des résultats des algorithmes d'apprentissage par arbre de décision (comme C4.5). La méthode présentée utilise simultanément une visualisation radiale, focus+context, fisheye et hiérarchique pour la représentation et l'exploration des résultats des algorithmes d'arbre de décision. L'utilisateur peut ainsi extraire facilement des règles d'induction et élaguer l'arbre obtenu dans une phase de post-traitement. Cela lui permet d'avoir une meilleure compréhension des résultats obtenus. Les résultats des tests numériques avec des ensembles de données réelles montrent que la méthode proposée permet une bien meilleure compréhension des résultats des arbres de décision.	Thanh-Nghi Do, Nguyen-Khang Pham, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001331	http://editions-rnti.fr/render_pdf.php?p=1001331	1289	fr	fr	@lri.fr, @irisa.fr, @esiea-ouest.fr	visualisation exploratoire des résultat d' algorithme d' arbre de décision  Nous présenter un méthode d' exploration des résultat des algorithme d' apprentissage par arbre de décision ( comme C4.5 ) . le méthode présenter utiliser simultanément un visualisation radiale , focus + context , fisheye et hiérarchique pour le représentation et le exploration des résultat des algorithme d' arbre de décision . le utilisateur pouvoir ainsi extraire facilement un règle d' induction et élaguer le arbre obtenir dans un phase de post-traitement . cela lui permettre d' avoir un meilleur compréhension des résultat obtenir . le résultat des test numérique avec un ensemble de donnée réel montrer que le méthode proposer permettre un bien meilleur compréhension des résultat des arbre de décision . 	Visualisation exploratoire des résultats d'algorithmes d'arbre de décision	0
Revue des Nouvelles Technologies de l'Information	EGC	2007	WebDocEnrich : enrichissement sémantique flexible de documents semi-structurés	WebdocEnrich est une approche d'enrichissement sémantique automatique de documents HTML hétérogènes qui exploite une description du domaine pour enrichir le contenu des documents et les représenter en XML.	Mouhamadou Thiam, Nacéra Bennacer, Nathalie Pernelle	http://editions-rnti.fr/render_pdf.php?p1&p=1001365	http://editions-rnti.fr/render_pdf.php?p=1001365	1290	fr	fr	@lri.fr, @supelec.fr	WebDocEnrich : enrichissement sémantique flexible de document semi-structurés  WebdocEnrich être un approche d' enrichissement sémantique automatique de document HTML hétérogène qui exploiter un description du domaine pour enrichir le contenu des document et les représenter en XML . 	WebDocEnrich : enrichissement sémantique flexible de documents semi-structurés	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Accès aux connaissances orales par le résumé automatique	Le temps nécessaire pour écouter un flux audio est un facteur réduisant l'accès efficace àde grandes archives de parole. Une première approche, la structuration automatique des données,permet d'utiliser un moteur de recherche pour cibler plus rapidement l'information. Leslistes de résultats générées sont longues dans un souci d'exhaustivité. Alors que pour des documentstextuels, un coup d'oeil discrimine un résultat interessant d'un résultat non pertinant,il faut écouter l'audio dans son intégralité pour en capturer le contenu. Nous proposons doncd'utiliser le résumé automatique afin de structurer les résultats des recherches et d'en réduirela redondance.	Benoît Favre, Jean-François Bonastre, Patrice Bellot, François Capman	http://editions-rnti.fr/render_pdf.php?p1&p=1000358	http://editions-rnti.fr/render_pdf.php?p=1000358	1360	fr	fr	@thalesgroup.com, @univ-avignon.fr, @univ-avignon.fr, @univ-avignon.fr	accès aux connaissance oral par le résumé automatique  le temps nécessaire pour écouter un flux audio être un facteur réduire le accès efficace àde grand archive de parole . un premier approche , le structuration automatique des donnée , permettre d' utiliser un moteur de recherche pour cibler plus rapidement le information . Leslistes de résultat générer être long dans un souci d' exhaustivité . alors que pour un documentstextuels , un coup d' oeil discriminer un résultat interessant d' un résultat non pertinant , il faillir écouter le audio dans son intégralité pour en capturer le contenu . Nous proposer doncd'utiliser le résumé automatique afin de structurer le résultat des recherche et d' en réduirela redondance . 	Accès aux connaissances orales par le résumé automatique	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Affectation pondérée sur des données de type intervalle	On s'intéresse à la construction d'arbres de décision sur des données symboliques de type intervalle en utilisant le critère de découpage binaire de Kolmogorov-Smirnov. Nous proposons une approche permettant d'affecter un individu à la fois aux deux noeuds fils générés par le partitionnement d'un noeud non terminal. Le but de cette méthode est de prendre en compte le positionnement de la donnée à classer par rapport à la donnée seuil de coupure.	Edwin Diday, Chérif Mballo	http://editions-rnti.fr/render_pdf.php?p1&p=1000373	http://editions-rnti.fr/render_pdf.php?p=1000373	1361	fr	fr	@esiea-ouest.fr, @ceremade.dauphine.fr	affectation pondérer sur un donnée de type intervalle  On clr intéresser à le construction d' arbre de décision sur un donnée symbolique de type intervalle en utiliser le critère de découpage binaire de Kolmogorov-Smirnov . Nous proposer un approche permettre d' affecter un individu à le foi aux deux noeud fil générer par le partitionnement d' un noeud non terminal . le but de ce méthode être de prendre en compte le positionnement de le donner à classer par rapport à le donner seuil de coupure . 	Affectation pondérée sur des données de type intervalle	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Aide en gestion hospitalière par visualisation des composantes de non-pertinence		Bernard Huet	http://editions-rnti.fr/render_pdf.php?p1&p=1000431	http://editions-rnti.fr/render_pdf.php?p=1000431	1362	fr		@lip6.fr	Aide en gestion hospitalière par visualisation des composantes de non-pertinence 	Aide en gestion hospitalière par visualisation des composantes de non-pertinence	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Algorithme semi-interactif pour la sélection de dimensions	"Nous présentons un algorithme génétique semi-interactif de sélectionde dimensions dans les grands ensembles de données pour la détectiond'individus atypiques (outliers). Les ensembles de données possédant unnombre élevé de dimensions posent de nombreux problèmes aux algorithmesde fouille de données, une solution est d'effectuer un pré-traitement afin de neretenir que les dimensions ""intéressantes"". Nous utilisons un algorithmegénétique pour le choix du sous-ensemble de dimensions à retenir. Par ailleursnous souhaitons donner un rôle plus important à l'utilisateur dans le processusde fouille, nous avons donc développé un algorithme génétique semi-interactifoù l'évaluation des solutions n'élimine pas complètement la fonctiond'évaluation mais la couple avec une évaluation de l'utilisateur. Enfin,l'importante réduction du nombre de dimensions nous permet de visualiser lesrésultats de l'algorithme de détection d'outlier. Cette visualisation permet àl'expert des données d'étiqueter les éléments atypiques (erreurs ou simplementdes individus différents de la masse)."	Lydia Boudjeloud, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000366	http://editions-rnti.fr/render_pdf.php?p=1000366	1363	fr	fr	@esiea-ouest.fr	algorithme semi-interactif pour le sélection de dimensions  " Nous présenter un algorithme génétique semi-interactif de sélectionde dimension dans le grand ensemble de donnée pour le détectiond'individus atypique ( outliers ) . . le ensemble de donnée posséder unnombre élever de dimension poser un nombreux problème aux algorithmesde fouiller de donnée , un solution être d' effectuer un pré-traitement afin de neretenir que le dimension " " intéressant " " . . Nous utiliser un algorithmegénétique pour le choix du sous-ensemble de dimension à retenir . . Par ailleursnous souhaiter donner un rôle plus important à le utilisateur dans le processusde fouille , nous avoir donc développer un algorithme génétique semi-interactifoù le évaluation des solution n' éliminer pas complètement le fonctiond'évaluation mais le couple avec un évaluation de le utilisateur . . enfin , le important réduction du nombre de dimension nous permettre de visualiser lesrésultats de le algorithme de détection d' outlier . . ce visualisation permettre àl'expert un donnée d' étiqueter le élément atypique ( erreur ou simplementdes individu différent de le masse ) . " 	Algorithme semi-interactif pour la sélection de dimensions	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Alignement extensionnel et asymétrique de hiérarchies conceptuelles par découverte d'implications entre concepts	Dans la littérature, de nombreux travaux traitent de méthodes d'alignementd'ontologies. Ils utilisent, pour la plupart, des relations basées sur desmesures de similarité qui ont la particularité d'être symétriques. Cependant, peude travaux évaluent l'intérêt d'utiliser des mesures d'appariement asymétriquesdans le but d'enrichir l'alignement produit. Ainsi, nous proposons dans ce papierune méthode d'alignement extensionnelle et asymétrique basée sur la découvertedes implications significatives entre deux ontologies. Notre approche,basée sur le modèle probabiliste d'écart à l'indépendance appelé intensité d'implication,est divisée en deux parties consécutives : (1) l'extraction, à partir ducorpus textuel associé à l'ontologie, et l'association des termes aux concepts;(2) la découverte et sélection des implications génératrices les plus significativesentre les concepts. La méthode proposée est évaluée sur deux jeux de donnéesréels portant respectivement sur des profils d'entreprises et sur des cataloguesde cours d'universités. Les résultats obtenus montrent que l'on peut trouver desrelations pertinentes qui sont ignorées par un alignement basé seulement sur desmesures de similarité.	Jérôme David, Fabrice Guillet, Régis Gras, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000337	http://editions-rnti.fr/render_pdf.php?p=1000337	1364	fr	fr	@univ-nantes.fr	alignement extensionnel et asymétrique de hiérarchie conceptuel par découverte d' implication entre concepts  Dans le littérature , de nombreux travail traiter de méthode d' alignementd'ontologies . Ils utiliser , pour le plupart , un relation baser sur desmesures de similarité qui avoir le particularité d' être symétrique . cependant , peude travail évaluer le intérêt d' utiliser un mesure d' appariement asymétriquesdans le but d' enrichir le alignement produire . ainsi , nous proposer dans ce papierune méthode d' alignement extensionnelle et asymétrique baser sur le découvertedes implication significatif entre deux ontologie . son approche , baser sur le modèle probabiliste d' écart à le indépendance appeler intensité d' implication , être diviser en deux partie consécutif : ( 1 ) le extraction , à partir ducorpus textuel associer à le ontologie , et le association des terme aux concept ; ( 2 ) le découverte et sélection des implication générateur le plus significativesentre le concept . le méthode proposer être évaluer sur deux jeu de donnéesréels porter respectivement sur un profil d' entreprise et sur un cataloguesde cour d' université . le résultat obtenir montrer que le on pouvoir trouver desrelations pertinent qui être ignorer par un alignement baser seulement sur desmesures de similarité . 	Alignement extensionnel et asymétrique de hiérarchies conceptuelles par découverte d'implications entre concepts	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Amélioration des indicateurs techniques pour l'analyse du marché financier	La technique des motifs fréquents a été utilisée pour améliorer lepouvoir prédictif des stratégies quantitatives. Innovant dans le contexte desmarchés financiers, notre méthode associe une signature aux configurations demarché fréquentes. Un système de « trading » automatique sélectionne lesmeilleures signatures par une procédure de « back testing » itérative et les utiliseen combinaison avec l'indicateur technique pour améliorer sa performance.L'application des motifs fréquents à cette problématique des indicateurstechniques est une contribution originale. Au sens du test t de Student,notre méthode améliore nettement les approches sans signatures. La techniquea été testé sur des données journalières type taux d'intérêt et actions. Notreanalyse des indicateurs (Williams%R, BN et croisement des moments) a montréque qu'une approche par signatures est particulièrement bien adaptée auxstratégies à mémoire courte.	Hunor Albert-Lorincz, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000428	http://editions-rnti.fr/render_pdf.php?p=1000428	1365	fr	fr	@sdm.cic.fr, @insa-lyon.fr	amélioration des indicateur technique pour le analyse du marché financier  le technique des motif fréquent avoir être utiliser pour améliorer lepouvoir prédictif des stratégie quantitatif . innover dans le contexte desmarchés financier , son méthode associer un signature aux configuration demarché fréquent . un système de « trading » automatique sélectionner lesmeilleures signature par un procédure de « back testing » itératif et le utiliseen combinaison avec le indicateur technique pour améliorer son performance . le application des motif fréquent à ce problématique des indicateurstechniques être un contribution original . Au sens du test t de Student , son méthode améliorer nettement le approche sans signature . le techniquea être tester sur un donnée journalier type taux d' intérêt et action . Notreanalyse des indicateur ( Williams \% R , BN et croisement des moment ) avoir montréque qu' un approche par signature être particulièrement bien adapter auxstratégies à mémoire court . 	Amélioration des indicateurs techniques pour l'analyse du marché financier	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Analyse du Comportement des utilisateurs exploitant une base de données vidéo	Dans cet article, nous présentons un modèle de fouille des usages dela vidéo pour améliorer la qualité de l'indexation. Nous proposons une approchebasée sur un modèle à deux niveaux représentant le comportement des utilisateursexploitant un moteur de recherche vidéo. Le premier niveau consiste àmodéliser le comportement lors de la lecture d'une vidéo unique (comportementintra vidéo), le second à modéliser le comportement sur l'ensemble d'une session(comportement inter video). A partir de cette représentation, nous avonsdéveloppé un algorithme de regroupement, adapté à la nature particulière de cesdonnées. L'analyse des usages de la vidéo nous permet d'affiner l'indexationvidéo sur la base de l'intérêt des utilisateurs.	Sylvain Mongy	http://editions-rnti.fr/render_pdf.php?p1&p=1000376	http://editions-rnti.fr/render_pdf.php?p=1000376	1366	fr	fr	@lifl.fr	analyse du comportement des utilisateur exploiter un base de donnée vidéo  Dans ce article , nous présenter un modèle de fouille des usage dela vidéo pour améliorer le qualité de le indexation . Nous proposer un approchebasée sur un modèle à deux niveau représenter le comportement des utilisateursexploitant un moteur de recherche vidéo . le premier niveau consister àmodéliser le comportement lors de le lecture d' un vidéo unique ( comportementintra vidéo ) , le second à modéliser le comportement sur le ensemble d' un session ( comportement inter video ) . A partir de ce représentation , nous avonsdéveloppé un algorithme de regroupement , adapter à le nature particulier de cesdonnées . le analyse des usage de le vidéo nous permettre d' affiner le indexationvidéo sur le base de le intérêt des utilisateur . 	Analyse du Comportement des utilisateurs exploitant une base de données vidéo	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Annotation sémantique de pages web	Cet article présente un système automatique d'annotation sémantiquede pages web. Les systèmes d'annotation automatique existants sont essentiellementsyntaxiques, même lorsque les travaux visent à produire une annotationsémantique. La prise en compte d'informations sémantiques sur le domaine pourl'annotation d'un élément dans une page web à partir d'une ontologie supposed'aborder conjointement deux problèmes : (1) l'identification de la structuresyntaxique caractérisant cet élément dans la page web et (2) l'identification duconcept le plus spécifique (en termes de subsumption) dans l'ontologie dontl'instance sera utilisée pour annoter cet élément. Notre démarche repose sur lamise en oeuvre d'une technique d'apprentissage issue initialement des wrappersque nous avons articulée avec des raisonnements exploitant la structure formellede l'ontologie.	Sylvain Tenier, Amedeo Napoli, Xavier Polanco, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1001499	http://editions-rnti.fr/render_pdf.php?p=1001499	1367	fr	fr	@inist.fr, @loria.fr	annotation sémantique de page web  ce article présenter un système automatique d' annotation sémantiquede page web . le système d' annotation automatique existant être essentiellementsyntaxiques , même lorsque le travail viser à produire un annotationsémantique . le prise en compte d' information sémantique sur le domaine pourl'annotation d' un élément dans un page web à partir d' un ontologie supposed'aborder conjointement deux problème : ( 1 ) le identification de le structuresyntaxique caractériser ce élément dans le page web et ( 2 ) le identification duconcept le plus spécifique ( en terme de subsumption ) dans le ontologie dontl'instance être utiliser pour annoter ce élément . son démarche reposer sur lamise en oeuvre d' un technique d' apprentissage issue initialement des wrappersque nous avoir articuler avec un raisonnement exploiter le structure formellede le ontologie . 	Annotation sémantique de pages web	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Apprentissage de la structure des réseaux bayésiens à partir des motifs fréquents corrélés : application à l'identification des facteurs environnementaux du cancer du Nasopharynx	L'apprentissage de structure des réseaux bayésien à partir de donnéesest un problème NP-difficile pour lequel de nombreuses heuristiques ont été proposées.Dans cet article, nous proposons une nouvelle méthode inspirée des travauxsur la recherche de motifs fréquents corrélés pour identifier les causalitésentre les variables. L'algorithme opère en quatre temps : (1) la découvertepar niveau des motifs fréquents corrélés minimaux ; (2) la construction d'ungraphe non orienté à partir de ces motifs ; (3) la détection des V_structures etl'orientation partielle du graphe ; (4) l'élimination des arêtes superflues par destests d'indépendance conditionnelle. La méthode, appliquée au réseau Asia, permetde retrouver la structure du graphe initial. Nous l'appliquons ensuite auxdonnées d'une étude épidémiologique cas-témoins du cancer du nasopharynx(NPC). L'objectif est de dresser un profil statistique type de la population étudiéeet d'apporter un éclairage utile sur les différents facteurs impliqués dans leNPC.	Alexandre Aussem, Zahra Kebaili, Marilys Corbex, Fabien De Marchi	http://editions-rnti.fr/render_pdf.php?p1&p=1000420	http://editions-rnti.fr/render_pdf.php?p=1000420	1368	fr	fr	@univ-lyon1.fr, @iarc.fr, @liris.cnrs.fr	apprentissage de le structure des réseau bayésien à partir un motif fréquent corréler : application à le identification des facteur environnemental du cancer du Nasopharynx  le apprentissage de structure des réseau bayésien à partir de donnéesest un problème NP-difficile pour lequel de nombreux heuristique avoir être proposer . Dans ce article , nous proposer un nouveau méthode inspirer des travauxsur le recherche de motif fréquent corréler pour identifier le causalitésentre le variable . le algorithme opérer en quatre temps : ( 1 ) le découvertepar niveau des motif fréquent corréler minimal ; ( 2 ) le construction d' ungraphe non orienter à partir de ce motif ; ( 3 ) le détection des V _ structure etl'orientation partiel du graphe ; ( 4 ) le élimination des arête superflu par destests d' indépendance conditionnel . le méthode , appliquer au réseau Asia , permetde retrouver le structure du graphe initial . Nous l' appliquer ensuite auxdonnées d' un étude épidémiologique cas-témoins du cancer du nasopharynx ( NPC ) . le objectif être de dresser un profil statistique type de le population étudiéeet d' apporter un éclairage utile sur le différent facteur impliquer dans leNPC . 	Apprentissage de la structure des réseaux bayésiens à partir des motifs fréquents corrélés : application à l'identification des facteurs environnementaux du cancer du Nasopharynx	5
Revue des Nouvelles Technologies de l'Information	EGC	2006	Approche entropique pour l'analyse de modèle de chroniques	Cet article propose d'utiliser l'entropie informationnelle pouranalyser des modèles de chroniques découverts selon une approchestochastique (Bouché et Le Goc, 2005). Il décrit une adaptation de l'algorithmeTemporalID3 (Console et Picardi, 2003) permettant de découvrir des modèlesde chroniques à partir d'un ensemble d'apprentissage contenant des séquencesd'occurrences d'événements discrets. Ces séquences représentent des suitesd'alarmes générées par un système à base de connaissance de monitoring et dediagnostic de systèmes dynamiques. On montre sur un exemple que l'approcheentropique complète l'approche stochastique en identifiant les classesd'événements qui contribuent le plus significativement à la prédiction d'uneoccurrence d'une classe particulière.	Nabil Benayadi, Marc Le Goc, Philippe Bouché	http://editions-rnti.fr/render_pdf.php?p1&p=1000397	http://editions-rnti.fr/render_pdf.php?p=1000397	1369	fr	fr	@lsis.org	approche entropique pour le analyse de modèle de chroniques  ce article proposer d' utiliser le entropie informationnel pouranalyser des modèle de chronique découvert selon un approchestochastique ( Bouché et le Goc , 2005 ) . Il décrire un adaptation de le algorithmeTemporalID3 ( console et Picardi , 2003 ) permettre de découvrir un modèlesde chronique à partir d' un ensemble d' apprentissage contenir un séquencesd'occurrences d' événement discret . ce séquence représenter un suitesd'alarmes générer par un système à base de connaissance de monitoring et dediagnostic de système dynamique . On montrer sur un exemple que le approcheentropique compléter le approche stochastique en identifier le classesd'événements qui contribuer le plus significativement à le prédiction d' uneoccurrence d' un classe particulier . 	Approche entropique pour l'analyse de modèle de chroniques	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	ARABASE : Base de données Web pour l'exploitation en reconnaissance optique de l'écriture Arabe		Noura Bouzrara, Nacéra Madani Aissaoui, Najoua Essoukri Ben Amara	http://editions-rnti.fr/render_pdf.php?p1&p=1000448	http://editions-rnti.fr/render_pdf.php?p=1000448	1370	fr		@yahoo.fr, @fsm.rnu.tn, @enim.rnu.tn	ARABASE : Base de données Web pour l'exploitation en reconnaissance optique de l'écriture Arabe 	ARABASE : Base de données Web pour l'exploitation en reconnaissance optique de l'écriture Arabe	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Arbres de décision multi modes et multi cibles.	Nous présentons une nouvelle méthode d'induction d'arbre de décision appelée MuMTree (pour Multi Models Tree) utilisable pour les modes d'apprentissage supervisé, non supervisé, supervisé à plusieurs variables cibles. Nous présentons les différents principes nécessaires pour réaliser un tel arbre de décision. Nous illustrons ensuite, sur un cas de modélisation multi-cibles, les avantages de cette méthode par rapport à un arbre de décision classique.	Frank Meyer, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000401	http://editions-rnti.fr/render_pdf.php?p=1000401	1371	fr	fr	@francetelecom.com, @francetelecom.com	arbre de décision multi mode et multi cible .  Nous présenter un nouveau méthode d' induction d' arbre de décision appeler MuMTree ( pour Multi Models Tree ) utilisable pour le mode d' apprentissage superviser , non superviser , superviser à plusieurs variable cible . Nous présenter le différent principe nécessaire pour réaliser un tel arbre de décision . Nous illustrer ensuite , sur un cas de modélisation multi-cibles , le avantage de ce méthode par rapport à un arbre de décision classique . 	Arbres de décision multi modes et multi cibles.	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Archiview, un outil de visualisation topographique des paramètres d'un hôpital		Pierre P. Lévy, Jean-Philippe Villaréal, Pierre-Paul Couka, Fabrice Gallois, Laurence Herbin, Antoine Flahault	http://editions-rnti.fr/render_pdf.php?p1&p=1000449	http://editions-rnti.fr/render_pdf.php?p=1000449	1372	fr		@tnn.aphp.fr	Archiview, un outil de visualisation topographique des paramètres d'un hôpital 	Archiview, un outil de visualisation topographique des paramètres d'un hôpital	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Biclustering of Gene Expression Data Based on Local Nearness	The analysis of gene expression data in DNA chips is an importanttool used in genomic research whose main objectives range from the study ofthe functionality of specific genes and their participation in biological processto the reconstruction of diseases's conditions and their subsequent prognosis.Gene expression data are arranged in matrices where each gene corresponds toone row and every column represents one specific experimental condition. Thebiclustering techniques have the purpose of finding subsets of genes that showsimilar activity patterns under a subset of conditions. Our approach consists ofa biclustering algorithm based on local nearness. The algorithm searches forbiclusters in a greedy fashion, starting with two-genes biclusters and includingas much as possible depending on a distance threshold which guarantees thesimilarity of gene behaviors.	Jesús S. Aguilar-Ruiz, Domingo S. Rodríguez, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1000427	http://editions-rnti.fr/render_pdf.php?p=1000427	1373	en	en	@lsi.us.es, @cs.umb.edu	biclustering gene expression data base local nearness analysis gene expression data dna chip importanttool used genomic research whose main objective range study ofthe functionality specific gene participation biological processto reconstruction disease s condition subsequent prognosis gene expression data arrange matrix gene correspond toone row every column represent one specific experimental condition thebiclustering technique purpose find subsets gene showsimilar activity pattern subset condition approach consist ofa biclustering algorithm base local nearness algorithm search forbicluster greedy fashion start two gene bicluster includinga much possible depend distance threshold guarantee thesimilarity gene behavior	Biclustering of Gene Expression Data Based on Local Nearness	8
Revue des Nouvelles Technologies de l'Information	EGC	2006	Bordures statistiques pour la fouille incrémentale de données dans les Data Streams	Récemment la communauté Extraction de Connaissances s'est intéressée à de nouveaux modèles où les données arrivent séquentiellement sous la forme d'un flot rapide et continu, i.e. les data streams. L'une des particularités importantes de ces flots est que seule une quantité d'information partielle est disponible au cours du temps. Ainsi après différentes mises à jour successives, il devient indispensable de considérer l'incertitude inhérente à l'information retenue. Dans cet article, nous introduisons une nouvelle approche statistique en biaisant les valeurs supports pour les motifs fréquents. Cette dernière a l'avantage de maximiser l'un des deux paramètres (précision ou rappel) déterminés par l'utilisateur tout en limitant la dégradation sur le paramètre non choisi. Pour cela, nous définissons les notions de bordures statistiques. Celles-ci constituent les ensembles de motifs candidats qui s'avèrent très pertinents à utiliser dans le cas de la mise à jour incrémentale des streams. Les différentes expérimentations effectuées dans le cadre de recherche de motifs séquentiels ont montré l'intérêt de l'approche et le potentiel des techniques utilisées.	Jean-Emile Symphor, Pierre-Alain Laur	http://editions-rnti.fr/render_pdf.php?p1&p=1000417	http://editions-rnti.fr/render_pdf.php?p=1000417	1374	fr	fr	@univ-ag.fr	bordure statistique pour le fouille incrémentale de donnée dans le Data Streams  récemment le communauté Extraction de Connaissances clr être intéresser à un nouveau modèle où le donnée arriver séquentiellement sous le forme d' un flot rapide et continu , i.e. les dater stream . le un des particularité important de ce flot être que seul un quantité d' information partiel être disponible au cour du temps . ainsi après différent mise à jour successif , il devenir indispensable de considérer le incertitude inhérent à le information retenir . Dans ce article , nous introduire un nouveau approche statistique en biaiser le valeur support pour le motif fréquent . ce dernier avoir le avantage de maximiser le un des deux paramètre ( précision ou rappel ) déterminer par le utilisateur tout en limiter le dégradation sur le paramètre non choisir . Pour cela , nous définir le notion de bordure statistique . Celles _-ci constituer le ensemble de motif candidat qui clr avérer très pertinent à utiliser dans le cas de le mise à jour incrémentale des stream . le différent expérimentation effectuer dans le cadre de recherche de motif séquentiel avoir montrer le intérêt de le approche et le potentiel des technique utiliser . 	Bordures statistiques pour la fouille incrémentale de données dans les Data Streams	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Carte auto-organisatrice probabiliste sur données binaires	Lesméthodes factorielles d'analyse exploratoire statistique définissentdes directions orthogonales informatives à partir d'un ensemble de données.Elles conduisent par exemple à expliquer les proximités entre individus à l'aided'un groupe de variables caractéristiques.Dans le contexte du datamining lorsqueles tableaux de données sont de grande taille, une méthode de cartographie synthétiques'avère intéressante. Ainsi une carte auto-organisatrice (SOM) est uneméthode de partitionnement munie d'une structure de graphe de voisinage -surles classes- le plus souvent planaire. Des travaux récents sont développés pourétendre le SOM probabiliste Generative Topographic Mapping (GTM) aux modèlesde mélanges classiques pour données discrètes. Dans ce papier nous présentonset étudions un modèle génératif symétrique de carte auto-organisatricepour données binaires que nous appelons Bernoulli Aspect Topological Model(BATM). Nous introduisons un nouveau lissage et accélérons la convergence del'estimation par une initialisation originale des probabilités en jeu.	Mohamed Nadif, Rodolphe Priam	http://editions-rnti.fr/render_pdf.php?p1&p=1000386	http://editions-rnti.fr/render_pdf.php?p=1000386	1375	fr	fr		carte auto- organisatrice probabiliste sur donnée binaires  Lesméthodes factorielle d' analyse exploratoire statistique définissentdes direction orthogonal informatif à partir d' un ensemble de donnée . Elles conduire par exemple à expliquer le proximité entre individu à le aided'un groupe de variable caractéristique . Dans le contexte du datamining lorsqueles tableau de donnée être de grand taille , un méthode de cartographie synthétiques'avère intéressant . ainsi un carte auto- organisatrice ( SOM ) être uneméthode de partitionnement munir d' un structure de graphe de voisinage-surles classe -le plus souvent planaire . un travail récent être développer pourétendre le SOM probabiliste Generative Topographic Mapping ( GTM ) aux modèlesde mélange classique pour donnée discret . Dans ce papier nous présentonset étudier un modèle génératif symétrique de carte auto- organisatricepour donner binaire que nous appeler Bernoulli Aspect Topological Model ( BATM ) . Nous introduire un nouveau lissage et accélérer le convergence del'estimation par un initialisation original des probabilité en jeu . 	Carte auto-organisatrice probabiliste sur données binaires	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Champs de Markov conditionnels pour le traitement de séquences	Les modèles conditionnels du type modèles de Markov d'entropiemaximale et champs de Markov conditionnels apportent des réponses auxlacunes des modèles de Markov cachés traditionnellement employés pour laclassification et la segmentation de séquences. Ces modèles conditionnels ontété essentiellement utilisés jusqu'à présent dans des tâches d'extractiond'information ou d'étiquetage morphosyntaxique. Cette contribution explorel'emploi de ces modèles pour des données de nature différente, de type« signal », telles que la parole ou l'écriture en ligne. Nous proposons desarchitectures de modèles adaptées à ces tâches pour lesquelles nous avonsdérivé les algorithmes d'inférence et d'apprentissage correspondant. Nousfournissons des résultats expérimentaux pour deux tâches de classification etd'étiquetage de séquences.	Thierry Artières, Trinh Minh Tri Do	http://editions-rnti.fr/render_pdf.php?p1&p=1000419	http://editions-rnti.fr/render_pdf.php?p=1000419	1376	fr	fr	@lip6.fr, @lip6.fr	Champs de Markov conditionnel pour le traitement de séquences  le modèle conditionnel du type modèle de Markov d' entropiemaximale et champ de Markov conditionnel apporter un réponse auxlacunes des modèle de Markov cacher traditionnellement employer pour laclassification et le segmentation de séquence . ce modèle conditionnel ontété essentiellement utiliser jusqu' à présent dans un tâche d' extractiond'information ou d' étiquetage morphosyntaxique . ce contribution explorel'emploi de ce modèle pour un donnée de nature différent , de type « signal » , tel que le parole ou le écriture en ligne . Nous proposer desarchitectures de modèle adapter à ce tâche pour lesquelles nous avonsdérivé le algorithme d' inférence et d' apprentissage correspondant . Nousfournissons un résultat expérimental pour deux tâche de classification etd'étiquetage de séquence . 	Champs de Markov conditionnels pour le traitement de séquences	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Choix du taux d'élagage pour l'extraction de la terminologie. Une approche fondée sur les courbes ROC	Le choix du taux d'élagage est crucial dans le but d'acquérir une terminologiede qualité à partir de corpus de spécialité. Cet article présente uneétude expérimentale consistant à déterminer le taux d'élagage le plus adapté.Plusieurs mesures d'évaluation peuvent être utilisées pour déterminer ce tauxtels que la précision, le rappel et le Fscore. Cette étude s'appuie sur une autremesure d'évaluation qui semble particulièrement bien adaptée pour l'extractionde la terminologie : les courbes ROC (Receiver Operating Characteristics).	Mathieu Roche, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000347	http://editions-rnti.fr/render_pdf.php?p=1000347	1377	fr	fr	@lirmm.fr, @lri.fr	choix du taux d' élagage pour le extraction de le terminologie . un approche fonder sur le courbe ROC  le choix du taux d' élagage être crucial dans le but d' acquérir un terminologiede qualité à partir de corpus de spécialité . ce article présent uneétude expérimental consister à déterminer le taux d' élagage le plus adapter . plusieurs mesure d' évaluation pouvoir être utiliser pour déterminer ce tauxtels que le précision , le rappel et le Fscore . ce étude clr appuyer sur un autremesure d' évaluation qui sembler particulièrement bien adapter pour le extractionde le terminologie : le courbe ROC ( Receiver Operating Characteristics ) . 	Choix du taux d'élagage pour l'extraction de la terminologie. Une approche fondée sur les courbes ROC	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification de documents XML à partir d'une représentation linéaire des arbres de ces documents	Cet article présente un nouveau modèle de représentation pour la classificationde documents XML. Notre approche permet de prendre en compte soitla structure seule, soit la structure et le contenu de ces documents. L'idée estde représenter un document par l'ensemble des sous-chemins de l'arbre XMLde longueur comprise entre n et m, deux valeurs fixées a priori. Ces cheminssont ensuite considérés comme de simples mots sur lesquels on peut appliquerdes méthodes standards de classification, par exemple K-means. Nous évaluonsnotre méthode sur deux collections: la collection INEX et les rapports d'activitéde l'INRIA. Nous utilisons un ensemble de mesures bien connues dans le domainede la recherche d'information lorsque les classes sont connues a priori.Lorsqu'elles ne sont pas connues, nous proposons une analyse qualitative desrésultats qui s'appuie sur les mots (chemins) les plus caractéristiques des classesgénérées.	Anne-Marie Vercoustre, Mounir Fegas, Yves Lechevallier, Thierry Despeyroux	http://editions-rnti.fr/render_pdf.php?p1&p=1000384	http://editions-rnti.fr/render_pdf.php?p=1000384	1378	fr	fr	@inria.fr	classification de document XML à partir d' un représentation linéaire des arbre de ce documents  ce article présenter un nouveau modèle de représentation pour le classificationde document XML . son approche permettre de prendre en compte soitla structure seul , soit le structure et le contenu de ce document . le idée estde représenter un document par le ensemble des sous-chemins de le arbre XMLde longueur comprendre entre n et m , deux valeur fixer avoir priori . ce cheminssont ensuite considérer comme un simple mot sur lesquels on pouvoir appliquerdes méthode standard de classification , par exemple K-means . Nous évaluonsnotre méthode sur deux collection : le collection INEX et le rapport d' activitéde le INRIA . Nous utiliser un ensemble de mesure bien connaître dans le domainede le recherche d' information lorsque le classe être connaître avoir priori . Lorsqu' elles ne être pas connaître , nous proposer un analyse qualitatif desrésultats qui clr appuyer sur le mot ( chemin ) le plus caractéristique des classesgénérées . 	Classification de documents XML à partir d'une représentation linéaire des arbres de ces documents	13
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification des comptes-rendus mammographiques à partir d'une ontologie radiologique en OWL	Dans cet article, nous proposons un système de classification descomptes-rendus mammographiques, reposant sur une ontologie radiologiquedécrivant les signes radiologiques et les différentes classes de la classificationACR des systèmes BIRADS dans le langage OWL. Le système est conçu pour,extraire les faits issus des textes libres de comptes-rendus en étant dirigé parl'ontologie, puis inférer la classe correspondante et en déduire l'attitude à tenirà partir de la classification ACR. Ce travail présente la construction d'une ontologieradiologique mammaire dans le langage OWL et son intérêt pour classerautomatiquement les comptes-rendus de mammographies.	Amel Boustil, Zaïdi Sahnoun, Ziad Mansouri, Christine Golbreich	http://editions-rnti.fr/render_pdf.php?p1&p=1000346	http://editions-rnti.fr/render_pdf.php?p=1000346	1379	fr	fr	@yahoo.fr, @yahoo.fr, @univ-rennes1.fr	classification des compte-rendu mammographiques à partir d' un ontologie radiologique en OWL  Dans ce article , nous proposer un système de classification descomptes-rendus mammographiques , reposer sur un ontologie radiologiquedécrivant le signe radiologique et le différent classe de le classificationACR des système BIRADS dans le langage OWL . le système être concevoir pour , extraire le fait issir des texte libre de compte-rendu en être diriger parl'ontologie , puis inférer le classe correspondant et en déduire le attitude à tenirà partir de le classification ACR . ce travail présenter le construction d' un ontologieradiologique mammaire dans le langage OWL et son intérêt pour classerautomatiquement le compte-rendu de mammographie . 	Classification des comptes-rendus mammographiques à partir d'une ontologie radiologique en OWL	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification d'un tableau de contingence et modèle probabiliste	Ces dernières années, la classification croisée ou classification parblocs, c'est-à-dire la recherche simultanée d'une partition des lignes et d'unepartition des colonnes d'un tableau de données, est devenue un outil très utiliséen fouille de données. Dans ce domaine, l'information se présente souvent sousforme de tableaux de contingence ou tableaux de co-occurrence croisant les modalitésde deux variables qualitatives. Dans cet article, nous étudions le problèmede la classification croisée de ce type de données en nous appuyant sur un modèlede mélange probabiliste. En utilisant l'approche vraisemblance classifiante,nous proposons un algorithme de classification croisée basé sur la maximisationalternée de la vraisemblance associée à deux mélanges multinomiaux classiqueset nous montrons alors que sous certaines contraintes restrictives, on retrouveles critères du Chi2 et de l'information mutuelle. Des résultats sur des donnéessimulées et des données réelles illustrent et confirment l'efficacité et l'intérêt decette approche.	Gérard Govaert, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000387	http://editions-rnti.fr/render_pdf.php?p=1000387	1380	fr	fr	@utc.fr, @univ-metz.fr	classification d' un tableau de contingence et modèle probabiliste  ce dernier année , le classification croiser ou classification parblocs , c' est-à-dire le recherche simultané d' un partition des ligne et d' unepartition des colonne d' un tableau de donnée , être devenir un outil très utiliséen fouille de donnée . Dans ce domaine , le information clr présenter souvent sousforme de tableau de contingence ou tableau de co- occurrence croiser le modalitésde deux variable qualitatif . Dans ce article , nous étudier le problèmede le classification croiser de ce type de donnée en nous appuyer sur un modèlede mélange probabiliste . En utiliser le approche vraisemblance classifiante , nous proposer un algorithme de classification croiser baser sur le maximisationalternée de le vraisemblance associer à deux mélange multinomiaux classiqueset nous montrer alors que sous certain contrainte restrictif , on retrouveles critère du Chi2 et de le information mutuel . un résultat sur un donnéessimulées et des donnée réel illustrer et confirmer le efficacité et le intérêt decette approche . 	Classification d'un tableau de contingence et modèle probabiliste	7
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification non-supervisée de données relationnelles		Jérôme Maloberti, Shin Ando, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1000375	http://editions-rnti.fr/render_pdf.php?p=1000375	1381	fr			Classification non-supervisée de données relationnelles 	Classification non-supervisée de données relationnelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classifications hiérarchiques factorielles de variables	On présente deux méthodes de classification hiérarchique ascendantede variables quantitatives et de fréquences. Chaque noeud de ces hiérarchiesregroupe deux classes de variables à partir d'une analyse factorielle particulièrebasée sur les variables représentatives de ces deux classes. Par cette méthode,on dispose, à chaque pas, d'un plan factoriel permettant de représenter àla fois les variables des deux classes fusionnées et l'ensemble des individus.Ces derniers se positionnent dans ce plan suivant leurs valeurs pour les variablesconsidérées. Ainsi, l'interprétation des noeuds obtenus s'effectue facilementà partir de l'examen de ces représentations factorielles. La répartition desindividus observée dans chacun de ces plans factoriels permet également dedéfinir une segmentation des individus en total accord avec la hiérarchie desvariables obtenues. On montre le fonctionnement des méthodes sur des exemplesréels.	Sergio Camiz, Jean-Jacques Denimal 	http://editions-rnti.fr/render_pdf.php?p1&p=1000374	http://editions-rnti.fr/render_pdf.php?p=1000374	1382	fr	fr	@uniroma1.it, @univ-lille1.fr	classification hiérarchique factoriel de variables  On présenter deux méthode de classification hiérarchique ascendantede variable quantitatif et de fréquence . chaque noeud de ce hiérarchiesregroupe deux classe de variable à partir d' un analyse factoriel particulièrebasée sur le variable représentatif de ce deux classe . Par ce méthode , on disposer , à chaque pas , d' un plan factoriel permettre de représenter àla foi le variable des deux classe fusionner et le ensemble des individu . ce dernier clr positionner dans ce plan suivant son valeur pour le variablesconsidérées . ainsi , le interprétation des noeud obtenir clr effectuer facilementà partir de le examen de ce représentation factoriel . le répartition desindividus observer dans chacun de ce plan factoriel permettre également dedéfinir un segmentation des individu en total accord avec le hiérarchie desvariables obtenir . On montrer le fonctionnement des méthode sur un exemplesréels . 	Classifications hiérarchiques factorielles de variables	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Clustering dynamique d'un flot de données : un algorithme incrémental et optimal de détection des maxima de densité	L'extraction non supervisée et incrémentale de classes sur un flot dedonnées (data stream clustering) est un domaine en pleine expansion. La plupartdes approches visent l'efficacité informatique. La nôtre, bien que se prêtantà un passage à l'échelle en mode distribué, relève d'une problématiquequalitative, applicable en particulier au domaine de la veille informationnelle :faire apparaître les évolutions fines, les « signaux faibles », à partir des thématiquesextraites d'un flot de documents. Notre méthode GERMEN localise defaçon exhaustive les maxima du paysage de densité des données à l'instant t,en identifiant les perturbations locales du paysage à t-1 et modifications defrontières induites par le document présenté. Son caractère optimal provient deson exhaustivité (à une valeur du paramètre de localité correspond un ensembleunique de maxima, et un découpage unique des classes qui la rend indépendantede tout paramètre d'initialisation et de l'ordre des données.	Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1000320	http://editions-rnti.fr/render_pdf.php?p=1000320	1383	fr	fr	@univ-fcomte.fr	Clustering dynamique d' un flot de donnée : un algorithme incrémental et optimal de détection des maximum de densité  le extraction non superviser et incrémentale de classe sur un flot dedonnées ( dater stream clustering ) être un domaine en plein expansion . le plupartdes approche viser le efficacité informatique . le nôtre , bien que clr prêtantà un passage à le échelle en mode distribuer , relever d' un problématiquequalitative , applicable en particulier au domaine de le veille informationnel : faire apparaître le évolution fin , le « signal faible » , à partir un thématiquesextraites d' un flot de document . son méthode GERMEN localiser defaçon exhaustif le maximum du paysage de densité des donnée à le instant t , en identifier le perturbation local du paysage à t- 1 et modification defrontières induire par le document présenter . son caractère optimal provenir deson exhaustivité ( à un valeur du paramètre de localité correspondre un ensembleunique de maximum , et un découpage unique des classe qui la rendre indépendantede tout paramètre d' initialisation et de le ordre des donnée . 	Clustering dynamique d'un flot de données : un algorithme incrémental et optimal de détection des maxima de densité	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Combinaison de l'approche inductive (progressive) et linguistique pour l'étiquetage morphosyntaxique des corpus de spécialité	Les étiqueteurs morphosyntaxiques sont de plus en plus performantset cependant, un véritable problème apparaît lorsque nous voulons étiqueterdes corpus de spécialité pour lesquels nous n'avons pas de corpus annotés. Lacorrection des ambiguïtés difficiles est une étape importante pour obtenir uncorpus de spécialité parfaitement étiqueté. Pour corriger ces ambiguïtés et diminuerle nombre de fautes, nous utilisons une approche itérative appelée InductionProgressive. Cette approche est une combinaison d'apprentissage automatique,de règles rédigées par l'expert et de corrections manuelles qui secombinent itérativement afin d'obtenir une amélioration de l'étiquetage tout enrestreignant les actions de l'expert à la résolution de problèmes de plus en plusdélicats. L'approche proposée nous a permis d'obtenir un corpus de biologiemoléculaire « correctement » étiqueté. En utilisant ce corpus, nous avons effectuéune étude comparative de quatre étiqueteurs supervisés.	Ahmed Amrani, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000354	http://editions-rnti.fr/render_pdf.php?p=1000354	1384	fr	fr	@esiea.fr, @lri.fr	combinaison de le approche inductif ( progressif ) et linguistique pour le étiquetage morphosyntaxique des corpus de spécialité  le étiqueteur morphosyntaxiques être de plus en plus performantset cependant , un véritable problème apparaître lorsque nous vouloir étiqueterdes corpus de spécialité pour lesquels nous n' avoir pas un corpus annoter . Lacorrection des ambiguïté difficile être un étape important pour obtenir uncorpus de spécialité parfaitement étiqueter . Pour corriger ce ambiguïté et diminuerle nombre de faute , nous utiliser un approche itératif appeler InductionProgressive . ce approche être un combinaison d' apprentissage automatique , de règle rédiger par le expert et de correction manuel qui secombinent itérativement afin d' obtenir un amélioration de le étiquetage tout enrestreignant le action de le expert à le résolution de problème de plus en plusdélicats . le approche proposer nous avoir permettre d' obtenir un corpus de biologiemoléculaire « correctement » étiqueter . En utiliser ce corpus , nous avoir effectuéune étude comparatif de quatre étiqueteur superviser . 	Combinaison de l'approche inductive (progressive) et linguistique pour l'étiquetage morphosyntaxique des corpus de spécialité	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comment formaliser les connaissances tacites d'une organisation ? Le cas de la conduite du changement à la SNCF		Anne Remillieux, Christian Blatter	http://editions-rnti.fr/render_pdf.php?p1&p=1000440	http://editions-rnti.fr/render_pdf.php?p=1000440	1385	fr		@sncf.fr	Comment formaliser les connaissances tacites d'une organisation ? Le cas de la conduite du changement à la SNCF 	Comment formaliser les connaissances tacites d'une organisation ? Le cas de la conduite du changement à la SNCF	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison de deux modes de représentation de données faiblement structurées en sciences du vivant	Cet article présente deux modes de représentation de l'informationdans le cadre d'une problématique en sciences du vivant. Le premier, appliqué àla microbiologie prévisionnelle, s'appuie sur deux formalismes, le modèle relationnelet les graphes conceptuels, interrogés uniformément via une même interface.Le second, appliqué aux technologies des céréales, utilise le seul modèlerelationnel. Cet article décrit les caractéristiques des données et compare les solutionsde représentation adoptées dans les deux systèmes.	Rallou Thomopoulos, Patrice Buche, Ollivier Haemmerlé, Frédéric Mabille, Nongyao Mueangdee	http://editions-rnti.fr/render_pdf.php?p1&p=1000332	http://editions-rnti.fr/render_pdf.php?p=1000332	1386	fr	fr	@ensam.inra.fr, @risk, @inapg.fr, @univ-tlse2.fr	comparaison de deux mode de représentation de donnée faiblement structurer en science du vivant  ce article présenter deux mode de représentation de le informationdans le cadre d' un problématique en science du vivre . le premier , appliquer àla microbiologie prévisionnel , clr appuyer sur deux formalisme , le modèle relationnelet le graphe conceptuel , interroger uniformément via un même interface . le second , appliquer aux technologie des céréale , utiliser le seul modèlerelationnel . ce article décrire le caractéristique des donnée et comparer le solutionsde représentation adopter dans le deux système . 	Comparaison de deux modes de représentation de données faiblement structurées en sciences du vivant	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison de dissimilarités pour l'analyse de l'usage d'un site web	"L'obtention d'une classification des pages d'un site web en fonctiondes navigations extraites des fichiers ""logs"" du serveur peut s'avérer très utilepour évaluer l'adéquation entre la structure du site et l'attente des utilisateurs. Onconstruit une telle typologie en s'appuyant une mesure de dissimilarité entre lespages, définie à partir des navigations. Le choix de la mesure la plus appropriéeà l'analyse du site est donc fondamental. Dans cet article, nous présentons unsite de petite taille dont les pages sont classées en catégories sémantiques parun expert. Nous confrontons ce classement aux partitions obtenues à partir dediverses dissimilarités afin d'en étudier les avantages et inconvénients."	Fabrice Rossi, Francisco de Assis Tenório de Carvalho, Yves Lechevallier, Alzennyr Da Silva	http://editions-rnti.fr/render_pdf.php?p1&p=1000378	http://editions-rnti.fr/render_pdf.php?p=1000378	1387	fr	fr		comparaison de dissimilarités pour le analyse de le usage d' un site web  " le obtention d' un classification des page d' un site web en fonctiondes navigation extraire des fichier " " logs " " du serveur pouvoir clr avérer très utilepour évaluer le adéquation entre le structure du site et le attente des utilisateur . . Onconstruit un tel typologie en clr appuyer un mesure de dissimilarité entre lespages , définir à partir un navigation . . le choix de le mesure le plus appropriéeà le analyse du site être donc fondamental . . Dans ce article , nous présenter unsite de petit taille dont le page être classer en catégorie sémantique parun expert . . Nous confronter ce classement aux partition obtenir à partir dediverses dissimilarités afin d' en étudier le avantage et inconvénient . " 	Comparaison de dissimilarités pour l'analyse de l'usage d'un site web	5
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison des mammographies par des méthodes d'apprentissage		Irina Diana Coman, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000433	http://editions-rnti.fr/render_pdf.php?p=1000433	1388	fr		@gmail.com, @univ-lyon2.fr	Comparaison des mammographies par des méthodes d'apprentissage 	Comparaison des mammographies par des méthodes d'apprentissage	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison des mesures d'intérêt de règles d'association : une approche basée sur des graphes de corrélation	Le choix des mesures d'intérêt (MI) afin d'évaluer les règles d'associationest devenu une question importante pour le post-traitement des connaissanceen ECD. Dans la littérature, de nombreux auteurs ont discuté et comparéles propriétés des MI afin d'améliorer le choix des meilleures mesures. Cependant,il s'avère que la qualité d'une règle est contextuelle : elle dépend à la fois dela structure de données et des buts du décideur. Ainsi, certaines mesures peuventêtre appropriées dans un certain contexte, mais pas dans d'autres. Dans cet article,nous présentons une nouvelle approche contextuelle mise en applicationpar un nouvel outil, ARQAT, permettant à un décideur d'évaluer et de comparerle comportement des MI sur ses jeux de données spécifiques. Cette approche estbasée sur l'analyse visuelle d'un graphe de corrélation entre des MI objectives.Nous employons ensuite cette approche afin de comparer et de discuter le comportementde trente-six mesures d'intérêt sur deux ensembles de données a prioritrès opposés : un premier dont les données sont fortement corrélées et un secondaux données faiblement corrélées. Alors que nous attendions des différences importantesentre les graphes de corrélation de ces deux jeux d'essai, nous avonspu observer des stabilités de corrélation entre certaines MI qui sont révélatricesde propriétés indépendantes de la nature des données observées. Ces stabilitéssont récapitulées et analysées.	Xuan-Hiep Huynh, Fabrice Guillet, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000404	http://editions-rnti.fr/render_pdf.php?p=1000404	1389	fr	fr	@univ-nantes.fr	comparaison des mesure d' intérêt de règle d' association : un approche baser sur un graphe de corrélation  le choix des mesure d' intérêt ( MI ) afin d' évaluer le règle d' associationest devenir un question important pour le post-traitement des connaissanceen ECD . Dans le littérature , de nombreux auteur avoir discuter et comparéles propriété des mi afin d' améliorer le choix des meilleur mesure . cependant , il clr avérer que le qualité d' un règle être contextuel : elle dépendre à le foi dela structure de donnée et des but du décideur . ainsi , certain mesure peuventêtre approprier dans un certain contexte , mais pas dans un autre . Dans ce article , nous présenter un nouveau approche contextuel mettre en applicationpar un nouveau outil , ARQAT , permettre à un décideur d' évaluer et de comparerle comportement des mi sur son jeu de donnée spécifique . ce approche estbasée sur le analyse visuel d' un graphe de corrélation entre un mi objectif . Nous employer ensuite ce approche afin de comparer et de discuter le comportementde trente-six mesure d' intérêt sur deux ensemble de donnée avoir prioritrès opposer : un premier dont le donnée être fortement corréler et un secondaux donner faiblement corréler . alors que nous attendre un différence importantesentre le graphe de corrélation de ce deux jeu d' essai , nous avonspu observer un stabilité de corrélation entre certain MI qui être révélatricesde propriété indépendant de le nature des donnée observer . ce stabilitéssont récapituler et analyser . 	Comparaison des mesures d'intérêt de règles d'association : une approche basée sur des graphes de corrélation	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Confrontation de Points de Vue dans le système Porphyry		Samuel Gesche, Sylvie Calabretto, Guy Caplat	http://editions-rnti.fr/render_pdf.php?p1&p=1000444	http://editions-rnti.fr/render_pdf.php?p=1000444	1390	fr		@insa-lyon.fr	Confrontation de Points de Vue dans le système Porphyry 	Confrontation de Points de Vue dans le système Porphyry	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Credit scoring, statistique et apprentissage	"Basel 2 regulations brought new interest in supervised classification methodologies for predicting default probability for loans. An important feature of consumer credit is that predictors are generally categorical. Logistic regression and linear discriminant analysis are the most frequently used techniques but are often unduly opposed. Vapnik's statistical learning theory explains why a prior dimension reduction (eg by means of multiple correspondence analysis) improves the robustness of the score function. Ridge regression, linear SVM, PLS regression are also valuable competitors. Predictive capability is measured by AUC or Gini's index which are related to the well known non-parametric Wilcoxon-Mann-Whitney test. Among methodological problems, reject inference is an important one, since most samples are subject to a selection bias. There are many methods, none being satisfactory. Distinguish between good and bad customers is not enough, especially for long-term loans. The question is then not only ""if"", but ""when"" the customers default. Survival analysis provides new types of scores."	Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1000316	http://editions-rnti.fr/render_pdf.php?p=1000316	1391	fr	en	@cnam.fr	Credit scoring , statistique et apprentissage 	Credit scoring, statistique et apprentissage	7
Revue des Nouvelles Technologies de l'Information	EGC	2006	Critère VT100 de sélection des règles d'association	L'extraction de règles d'association génère souvent un grand nombrede règles. Pour les classer et les valider, de nombreuses mesures statistiquesont été proposées ; elles permettent de mettre en avant telles ou telles caractéristiquesdes règles extraites. Elles ont pour point commun d'être fonctioncroissante du nombre de transactions et aboutissent bien souvent àl'acceptation de toutes les règles lorsque la base de données est de grandetaille. Dans cet article, nous proposons une mesure inspirée de la notion de valeur-test. Elle présente comme principale caractéristique d'être insensible à lataille de la base, évitant ainsi l'écueil des règles fallacieusement significatives.Elle permet également de mettre sur un même pied, et donc de les comparer,des règles qui auront été extraites de bases de données différentes. Elle permetenfin de gérer différents seuils de signification des règles. Le comportement dela mesure est détaillé sur un exemple.	Alain Morineau, Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1000409	http://editions-rnti.fr/render_pdf.php?p=1000409	1392	fr	fr	@modulad.fr, @univ-lyon2.fr	critère VT100 de sélection des règle d' association  le extraction de règle d' association générer souvent un grand nombrede règle . Pour les classer et les valider , un nombreux mesure statistiquesont être proposer ; elles permettre de mettre en avant tel ou tel caractéristiquesdes règle extraire . Elles avoir pour point commun d' être fonctioncroissante du nombre de transaction et aboutir bien souvent àl'acceptation de tout le règle lorsque le base de donnée être de grandetaille . Dans ce article , nous proposer un mesure inspirer de le notion de valeur-test . Elle présenter comme principal caractéristique d' être insensible à lataille de le base , éviter ainsi le écueil des règle fallacieusement significatif . Elle permettre également de mettre sur un même pied , et donc de les comparer , un règle qui avoir être extraire de base de donnée différent . Elle permetenfin de gérer différents seuil de signification des règle . le comportement dela mesure être détailler sur un exemple . 	Critère VT100 de sélection des règles d'association	7
Revue des Nouvelles Technologies de l'Information	EGC	2006	De l'analyse didactique à la modélisation informatique pour la conception d'un EIAH en chirurgie orthopédique	L'objet de la recherche présentée est de concevoir un environnementinformatique d'apprentissage qui permette de réduire l'écart entre la formationthéorique des chirurgiens et leur formation pratique, qui se dérouleprincipalement sur le mode du compagnonnage. L'article expose laméthodologie et quelques illustrations du travail didactique d'analyse desconnaissances et du système d'enseignement / apprentissage en milieuhospitalier (chirurgie orthopédique) ainsi que partie de la formalisationinformatique de cette connaissance. Cette modélisation permet la prise encompte dans l'environnement informatique de connaissances pragmatiquespour le diagnostic des connaissances de l'utilisateur en fonction des actionsqu'il effectue à l'interface pendant la résolution d'un problème (pose de visdans le bassin), et la prise de décision didactique qui suit : quelle rétroactionfournir pour affiner le diagnostic, et/ou permettre l'apprentissage souhaité.	Vanda Luengo, Lucile Vadcard, Dima Mufti-Alchawafa	http://editions-rnti.fr/render_pdf.php?p1&p=1000422	http://editions-rnti.fr/render_pdf.php?p=1000422	1393	fr	fr	@imag.fr, @imag.fr, @imag.fr	De le analyse didactique à le modélisation informatique pour le conception d' un EIAH en chirurgie orthopédique  le objet de le recherche présenter être de concevoir un environnementinformatique d' apprentissage qui permettre de réduire le écart entre le formationthéorique des chirurgien et son formation pratique , qui clr dérouleprincipalement sur le mode du compagnonnage . le article exposer laméthodologie et quelque illustration du travail didactique d' analyse desconnaissances et du système d' enseignement  apprentissage en milieuhospitalier ( chirurgie orthopédique ) ainsi que partie de le formalisationinformatique de ce connaissance . ce modélisation permettre le prise encompte dans le environnement informatique de connaissance pragmatiquespour le diagnostic des connaissance de le utilisateur en fonction des actionsqu'il effectuer à le interface pendant le résolution d' un problème ( pose de visdans le bassin ) , et le prise de décision didactique qui suivre : quel rétroactionfournir pour affiner le diagnostic , et permettre le apprentissage souhaiter . 	De l'analyse didactique à la modélisation informatique pour la conception d'un EIAH en chirurgie orthopédique	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Définition et diffusion de signatures sémantiques dans les systèmes pair-à-pair	Les systèmes pair-à-pair (peer-to-peer, P2P, égal-à-égal) se sont popularisésces dernières années avec les systèmes de partage de fichiers sur Internet.De nombreuses recherches concernant l'optimisation de la localisationdes données ont émergé et constituent un axe de recherche très actif. La priseen compte de la sémantique du contenu des pairs dans le routage des requêtespermet d'améliorer considérablement la localisation des données. Nous nousconcentrons sur l'approche PlanetP, faisant usage de la notion de filtre de Bloom,qui consiste à propager une signature sémantique des pairs (filtres de Bloom) àtravers le réseau. Nous présentons cette approche et en proposons une amélioration: la création de filtres de Bloom dynamiques, dans le sens où leur tailledépend de la charge des pairs (nombre de documents partagés).	Raja Chiky, Bruno Defude, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000388	http://editions-rnti.fr/render_pdf.php?p=1000388	1394	fr	fr	@enst.fr, @enst.fr, @int-evry.fr	définition et diffusion de signature sémantique dans le système pair-à-pair  le système pair-à-pair ( peer-to- peer , P2P , égal-à-égal ) clr être popularisésces dernier année avec le système de partage de fichier sur Internet . un nombreux recherche concernant le optimisation de le localisationdes donner avoir émerger et constituer un axe de recherche très actif . le priseen compte de le sémantique du contenu des pair dans le routage des requêtespermet d' améliorer considérablement le localisation des donnée . Nous nousconcentrons sur le approche PlanetP , faire usage de le notion de filtre de Bloom , qui consister à propager un signature sémantique des pair ( filtre de Bloom ) àtravers le réseau . Nous présenter ce approche et en proposer un amélioration : le création de filtre de Bloom dynamique , dans le sens où son tailledépend de le charge des pair ( nombre de document partager ) . 	Définition et diffusion de signatures sémantiques dans les systèmes pair-à-pair	8
Revue des Nouvelles Technologies de l'Information	EGC	2006	Des motifs séquentiels généralisés aux contraintes de temps étendues	Dans de nombreux domaines, la recherche de connaissances temporellesest très appréciée. Des techniques ont été proposées aussi bien en fouille dedonnées qu'en apprentissage, afin d'extraire et de gérer de telles connaissances,en les associant également à la spécification de contraintes temporelles (e.g.: fenêtretemporelle maximale), notamment dans le contexte de la recherche de motifsséquentiels. Cependant, ces contraintes sont souvent trop rigides ou nécessitentune bonne connaissance du domaine pour ne pas extraire des informationserronées. C'est pourquoi nous proposons une approche basée sur la constructionde graphes de séquences afin de prendre en compte des contraintes de tempsplus souples. Ces contraintes sont relâchées par rapport aux contraintes de tempsprécédemment proposées. Elles permettent donc d'extraire plus de motifs pertinents.Afin de guider l'analyse des motifs obtenus, nous proposons égalementun niveau de précision des contraintes temporelles pour les motifs extraits.	Maguelonne Teisseire, Céline Fiot, Anne Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000415	http://editions-rnti.fr/render_pdf.php?p=1000415	1395	fr	fr	@lirmm.fr	un motif séquentiel généraliser aux contrainte de temps étendues  Dans un nombreux domaine , le recherche de connaissance temporellesest très apprécier . un technique avoir être proposer aussi bien en fouille dedonnées qu' en apprentissage , afin d' extraire et de gérer de tel connaissance , en les associer également à le spécification de contrainte temporel ( e.g. : fenêtretemporelle maximal ) , notamment dans le contexte de le recherche de motifsséquentiels . cependant , ce contrainte être souvent trop rigide ou nécessitentune bon connaissance du domaine pour ne pas extraire un informationserronées . C' être pourquoi nous proposer un approche baser sur le constructionde graphe de séquence afin de prendre en compte des contrainte de tempsplus souple . ce contrainte être relâcher par rapport aux contrainte de tempsprécédemment proposer . Elles permettre donc d' extraire plus de motif pertinent . Afin de guider le analyse des motif obtenir , nous proposer égalementun niveau de précision des contrainte temporel pour le motif extrait . 	Des motifs séquentiels généralisés aux contraintes de temps étendues	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	EDA : algorithme de désuffixation du langage médical		Didier Nakache, Elisabeth Métais, Annabelle Dierstein	http://editions-rnti.fr/render_pdf.php?p1&p=1000429	http://editions-rnti.fr/render_pdf.php?p=1000429	1396	fr		@wanadoo.fr, @cnam.fr	EDA : algorithme de désuffixation du langage médical 	EDA : algorithme de désuffixation du langage médical	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Enrichissement d'ontologies dans le secteur de l'eau douce en environnement Internet distribué et multilingue		Lylia Abrouk, Mathieu Lafourcade	http://editions-rnti.fr/render_pdf.php?p1&p=1000432	http://editions-rnti.fr/render_pdf.php?p=1000432	1397	fr		@lirmm.fr, @semide.org	Enrichissement d'ontologies dans le secteur de l'eau douce en environnement Internet distribué et multilingue 	Enrichissement d'ontologies dans le secteur de l'eau douce en environnement Internet distribué et multilingue	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	ESIEA Datalab Logiciel de Nettoyage et Préparation de Données		Christopher Corsia	http://editions-rnti.fr/render_pdf.php?p1&p=1000451	http://editions-rnti.fr/render_pdf.php?p=1000451	1398	fr		@esiea.fr	ESIEA Datalab Logiciel de Nettoyage et Préparation de Données 	ESIEA Datalab Logiciel de Nettoyage et Préparation de Données	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Exploration des paramètres discriminants pour les représentations vectorielles de la sémantique des mots	Les méthodes de représentation sémantique des mots à partir d'une analyse statistique sont basées sur des comptes de co-occurences entre mots et unités textuelles. Ces méthodes ont des paramétrages complexes, notamment le type d'unité textuelle utilisée comme contexte. Ces paramètres déterminent fortement la qualité des résultats obtenus. Dans cet article, nous nous intéressons au paramètrage de la technique dite Hyperspace Analogue to Language (HAL).Nous proposons une nouvelle méthode pour explorer ses paramètres discriminants. Cette méthode est basée sur l'analyse d'un graphe de voisinage d'une liste de mots de référence pré-classés. Nous expérimentons cette méthode et en donnons les premiers résultats qui renforcent et complètent des résultats issus de travaux précédents.	Frank Meyer, Vincent Dubois	http://editions-rnti.fr/render_pdf.php?p1&p=1000360	http://editions-rnti.fr/render_pdf.php?p=1000360	1399	fr	fr	@francetelecom.com, @francetelecom.com	exploration des paramètre discriminant pour le représentation vectoriel de le sémantique des mots  le méthode de représentation sémantique des mot à partir d' un analyse statistique être baser sur un compte de co- occurences entre mot et unité textuel . ce méthode avoir des paramétrages complexe , notamment le type d' unité textuel utiliser comme contexte . ce paramètre déterminer fortement le qualité des résultat obtenir . Dans ce article , nous clr intéresser au paramètrage de le technique dire Hyperspace Analogue to Language ( HAL ) . Nous proposer un nouveau méthode pour explorer son paramètre discriminant . ce méthode être baser sur le analyse d' un graphe de voisinage d' un liste de mot de référence pré-classés . Nous expérimenter ce méthode et en donner le premier résultat qui renforcer et compléter un résultat issu de travail précédent . 	Exploration des paramètres discriminants pour les représentations vectorielles de la sémantique des mots	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Exploration interactive de bases de connaissances : un retour d'expérience	La navigation au sein de bases de connaissances reste un problèmeouvert. S'il existe plusieurs paradigmes de visualisation, peu de travaux sur lesretours d'expérience sont disponibles. Dans le cadre de cet article nous noussommes intéressés aux différents paradigmes de navigation interactive au seinde bases documentaires annotées sémantiquement ; l'accès à la base deconnaissances s'effectuant à travers l'ontologie du domaine d'application. Cesparadigmes ont été évalués dans le cadre d'une application industrielle(mécanique des fluides et échangeurs thermiques) en fonction de critèresdéfinis par les utilisateurs. L'analyse des retours d'expérience1 nous a permisde spécifier et de réaliser un nouveau navigateur dédié à la gestion dedocuments techniques annotés par une ontologie de domaine : le « Eye Tree »,navigateur de type « polar fisheye view ».	Christophe Tricot, Christophe Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000362	http://editions-rnti.fr/render_pdf.php?p=1000362	1400	fr	fr	@univ-savoi	exploration interactif de base de connaissance : un retour d' expérience  le navigation au sein de base de connaissance rester un problèmeouvert . S' il exister plusieurs paradigme de visualisation , peu de travail sur lesretours d' expérience être disponible . Dans le cadre de ce article nous noussommes intéresser aux différent paradigme de navigation interactif au seinde base documentaire annoter sémantiquement ; le accès à le base deconnaissances clr effectuer à travers le ontologie du domaine d' application . Cesparadigmes avoir être évaluer dans le cadre d' un application industriel ( mécanique des fluide et échangeur thermique ) en fonction de critèresdéfinis par le utilisateur . le analyse des retour d' expérience1 nous avoir permisde spécifier et de réaliser un nouveau navigateur dédier à le gestion dedocuments technique annoter par un ontologie de domaine : le « eye Tree » , navigateur de type « polar fisheye view » . 	Exploration interactive de bases de connaissances : un retour d'expérience	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extension de l'algorithme CURE aux fouilles de données volumineuses		Jerzy Korczak, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000402	http://editions-rnti.fr/render_pdf.php?p=1000402	1401	fr		@lsiit.u-strasbg.fr	Extension de l'algorithme CURE aux fouilles de données volumineuses 	Extension de l'algorithme CURE aux fouilles de données volumineuses	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction automatique de champs numériques dans des documents manuscrits	Nous décrivons dans cet article une chaine de traitement complète etgénérique permettant d'extraire automatiquement les champs numériques (numérosde téléphone, codes clients, codes postaux) dans des documents manuscritslibres. Notre chaïne de traitement est constituée des trois étapes suivantes:localisation des champs numériques potentiels selon une approche markoviennesans reconnaissance chiffre ni segmentation, reconnaissance des séquences extraites,et vérification des hypothèses de localisation / reconnaissance en vue delimiter la fausse alarme génerée lors de l'étape de localisation. L'évaluation denotre système sur une base de 300 courriers manuscrits montre des performancesen rappel-précision intéressantes.	Clément Chatelain, Laurent Heutte, Thierry Paquet	http://editions-rnti.fr/render_pdf.php?p1&p=1000319	http://editions-rnti.fr/render_pdf.php?p=1000319	1402	fr	fr	@univ-rouen.fr	extraction automatique de champ numérique dans un document manuscrits  Nous décrire dans ce article un chaine de traitement complet etgénérique permettre d' extraire automatiquement le champ numérique ( numérosde téléphone , code client , code postal ) dans un document manuscritslibres . son chaïne de traitement être constituer des trois étape suivant : localisation des champ numérique potentiel selon un approche markoviennesans reconnaissance chiffre ni segmentation , reconnaissance des séquence extraire , et vérification des hypothèse de localisation  reconnaissance en vue delimiter le fausser alarme génerée lors de le étape de localisation . le évaluation denotre système sur un base de 300 courrier manuscrit montrer des performancesen rappel-précision intéressant . 	Extraction automatique de champs numériques dans des documents manuscrits	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction d'objets vidéo : Une approche combinant les contours actifs et le flot optique	Dans cet article, nous présentons une méthode mixte de segmentationd'objets visuels dans une séquence d'images d'une vidéo combinant à la foisune segmentation basée régions et l'estimation de mouvement par flot optique.L'approche développée est basé sur une minimisation d'une fonctionnelled'énergie (E) qui fait intervenir les probabilités d'appartenance (densité) avecune gaussienne, en tenant compte des informations perceptuelles de couleur etde texture des régions d'intérêt. Pour améliorer la méthode de détection et desuivi, nous avons étendu la formulation énergétique de notre modèle decontour actif en incluant une force supplémentaire issue du calcul du flot optique.Nous montrons l'intérêt de cette approche mixte en terme de temps de calculet d'extraction d'objets vidéo complexes, et nous présentons les résultatsobtenus sur des séquences de corpus vidéo couleur.	Youssef Zinbi, Youssef Chahir, Abder Elmoatz	http://editions-rnti.fr/render_pdf.php?p1&p=1000321	http://editions-rnti.fr/render_pdf.php?p=1000321	1403	fr	fr	@unicaen.fr, @greyc.ismra.fr	extraction d' objet vidéo : un approche combiner le contour actif et le flot optique  Dans ce article , nous présenter un méthode mixte de segmentationd'objets visuel dans un séquence d' image d' un vidéo combiner à le foisune segmentation baser région et le estimation de mouvement par flot optique . le approche développer être baser sur un minimisation d' un fonctionnelled'énergie ( E ) qui faire intervenir le probabilité d' appartenance ( densité ) avecune gaussienne , en tenir compte des information perceptuelles de couleur etde texture des région d' intérêt . Pour améliorer le méthode de détection et desuivi , nous avoir étendre le formulation énergétique de son modèle decontour actif en inclure un force supplémentaire issue du calcul du flot optique . Nous montrer le intérêt de ce approche mixte en terme de temps de calculet d' extraction d' objet vidéo complexe , et nous présenter le résultatsobtenus sur un séquence de corpus vidéo couleur . 	Extraction d'objets vidéo : Une approche combinant les contours actifs et le flot optique	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction de motifs séquentiels dans les flots de données d'usage du Web	"Ces dernières années, de nouvelles contraintes sont apparues pour lestechniques de fouille de données. Ces contraintes sont typiques d'un nouveaugenre de données : les ""data streams"". Dans un processus de fouille appliquésur un data stream, l'utilisation de la mémoire est limitée, de nouveaux élémentssont générés en permanence et doivent être traités le plus rapidement possible,aucun opérateur bloquant ne peut être appliqué sur les données et celles-ci nepeuvent être observées qu'une seule fois. A l'heure actuelle, la majorité des travauxrelatifs à l'extraction de motifs dans les data streams ne concernent pas lesmotifs temporels. Nous montrons dans cet article que cela est principalement dûau phénomène combinatoire qui est lié à l'extraction de motifs séquentiels. Nousproposons alors un algorithme basé sur l'alignement de séquences pour extraireles motifs séquentiels dans les data streams. Afin de respecter la contrainte d'unepasse unique sur les données, une heuristique gloutonne est proposée pour segmenterles séquences. Nous montrons enfin que notre proposition est capabled'extraire des motifs pertinents avec un support très faible."	Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000418	http://editions-rnti.fr/render_pdf.php?p=1000418	1404	fr	fr	@inria.fr	extraction de motif séquentiel dans le flot de donnée d' usage du Web  " ce dernier année , de nouveau contrainte être apparaître pour lestechniques de fouille de donnée . . ce contrainte être typique d' un nouveaugenre de donnée : le " " dater stream " " . . Dans un processus de fouille appliquésur un dater stream , le utilisation de le mémoire être limiter , de nouveau élémentssont générer en permanence et devoir être traiter le plus rapidement possible , aucun opérateur bloquer ne pouvoir être appliquer sur le donnée et celui _-ci nepeuvent être observer qu' un seul foi . . A le heure actuel , le majorité des travauxrelatifs à le extraction de motif dans le data stream ne concerner pas lesmotifs temporel . . Nous montrer dans ce article que cela être principalement dûau phénomène combinatoire qui être lier à le extraction de motif séquentiel . . Nousproposons alors un algorithme baser sur le alignement de séquence pour extraireles motif séquentiel dans le data stream . . Afin de respecter le contrainte d' unepasse unique sur le donnée , un heuristique glouton être proposer pour segmenterles séquence . . Nous montrer enfin que son proposition être capabled'extraire un motif pertinent avec un support très faible . " 	Extraction de motifs séquentiels dans les flots de données d'usage du Web	5
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction de relations dans les documents Web	Nous présentons un système pour l'inférence de programmes d'extraction de relations dans les documents Web. Il utilise les vues textuelle et structurelle sur les documents. L'extraction des relations est incrémentale et utilise des méthodes de composition et d'enrichissement. Nous montrons que notre système est capable d'extraire des relations pour les organisations existantes dans les documents Web (listes,  tables, tables tournées, tables croisées).	Rémi Gilleron, Patrick Marty, Marc Tommasi, Fabien Torre	http://editions-rnti.fr/render_pdf.php?p1&p=1000380	http://editions-rnti.fr/render_pdf.php?p=1000380	1405	fr	fr	@univ-lille3.fr	extraction de relation dans le document Web  Nous présenter un système pour le inférence de programme d' extraction de relation dans le document Web . Il utiliser le vue textuel et structurel sur le document . le extraction des relation être incrémentale et utiliser un méthode de composition et d' enrichissement . Nous montrer que son système être capable d' extraire un relation pour le organisation existant dans le document Web ( liste , table , table tourner , table croiser ) . 	Extraction de relations dans les documents Web	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction et identification d'entités complexes à partir de textes biomédicaux	Nous présentons ici un système d'extraction et d'identification d'entitésnommées complexes à l'intention des corpus de spécialité biomédicale. Nousavons développé une méthode qui repose sur une approche mixte à base d'ensemblede règles a priori et de dictionnaires contrôlés. Cet article expose lestechniques que nous avons mises en place pour éviter ou minimiser les problèmesde synonymie, de variabilité des termes et pour limiter la présence denoms ambigus. Nous décrivons l'intégration de ces méthodes au sein du processusde reconnaissance des entités nommées. L'intérêt de cet outil réside dans lacomplexité et l'hétérogénéité des entités extraites. Cette méthode ne se limitepas à la détection des noms des gènes ou des protéines, mais s'adapte à d'autresdescripteurs biomédicaux. Nous avons expérimenté cette approche en mesurantles performances obtenues sur le corpus de référence GENIA.	Julien Lorec, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1000350	http://editions-rnti.fr/render_pdf.php?p=1000350	1406	fr	fr	@nantes.inserm.fr, @univ-nantes.fr	extraction et identification d' entité complexe à partir de texte biomédicaux  Nous présenter ici un système d' extraction et d' identification d' entitésnommées complexe à le intention des corpus de spécialité biomédical . Nousavons développer un méthode qui reposer sur un approche mixte à base d' ensemblede règle avoir priori et de dictionnaire contrôler . ce article exposer lestechniques que nous avoir mettre en place pour éviter ou minimiser le problèmesde synonymie , de variabilité des terme et pour limiter le présence denoms ambigu . Nous décrire le intégration de ce méthode au sein du processusde reconnaissance des entité nommer . le intérêt de ce outil résider dans lacomplexité et le hétérogénéité des entité extraire . ce méthode ne clr limitepas à le détection des nom des gène ou un protéine , mais clr adapter à un autresdescripteurs biomédical . Nous avoir expérimenter ce approche en mesurantles performance obtenir sur le corpus de référence GENIA . 	Extraction et identification d'entités complexes à partir de textes biomédicaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction multilingue de termes à partir de leur structure morphologique		Delphine Bernhard	http://editions-rnti.fr/render_pdf.php?p1&p=1000356	http://editions-rnti.fr/render_pdf.php?p=1000356	1407	fr		@imag.fr	Extraction multilingue de termes à partir de leur structure morphologique 	Extraction multilingue de termes à partir de leur structure morphologique	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	FaBR-CL : méthode de classification croisée de protéines	Dans cet article, nous proposons une méthode de classification croiséepermettant de classer des protéines, d'une part, et de classer des descripteurs (3-grammes) selon leurs pertinences par rapport aux groupes de protéines obtenus,d'autres part.	Walid Erray, Faouzi Mhamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1000446	http://editions-rnti.fr/render_pdf.php?p=1000446	1408	fr	fr	@univ-lyon2.fr, @ensi.rnu.tn	FaBR-CL : méthode de classification croiser de protéines  Dans ce article , nous proposer un méthode de classification croiséepermettant de classer un protéine , d' un part , et de classer un descripteur ( 3- gramme ) selon son pertinence par rapport aux groupe de protéine obtenir , un autre part . 	FaBR-CL : méthode de classification croisée de protéines	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Faire vivre un référentiel métier dans l'industrie : le système de gestion de connaissances ICARE	La gestion des connaissances, enjeu majeur pour l'industrie, est entréedans une phase concrète de déploiement. La conjonction d'une maturitédes organisations dans la maîtrise de leur métier, la consolidation de méthodeset les outils évolutifs pour faire vivre un patrimoine de connaissances favorisentl'émergence de projets significatifs et leur diffusion opérationnelle au seinde grands groupes industriels. ICARE chez PSA Peugeot Citroën réalisé avecl'environnement Ardans Knowledge Maker en est ici l'exemple.	Alain Berger, Pierre Mariot, Christophe Coppens, Julien Laroque Malbert	http://editions-rnti.fr/render_pdf.php?p1&p=1000450	http://editions-rnti.fr/render_pdf.php?p=1000450	1409	fr	fr	@ardans.com, @ardans.com, @mpsa.com, @mpsa.com	faire vivre un référentiel métier dans le industrie : le système de gestion de connaissance ICARE  le gestion des connaissance , enjeu majeur pour le industrie , être entréedans un phase concret de déploiement . le conjonction d' un maturitédes organisation dans le maîtrise de son métier , le consolidation de méthodeset le outil évolutif pour faire vivre un patrimoine de connaissance favorisentl'émergence de projet significatif et son diffusion opérationnel au seinde grand groupe industriel . ICARE chez PSA Peugeot Citroën réaliser avecl'environnement Ardans Knowledge Maker en être ici le exemple . 	Faire vivre un référentiel métier dans l'industrie : le système de gestion de connaissances ICARE	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Fast-MGB : Nouvelle Base Générique Minimale de Règles Associatives	Le problème de l'exploitation des règles associatives est devenu primordial,puisque le nombre des règles associatives extraites des jeux de donnéesréelles devient très élevé. Une solution possible consiste à ne dériver qu'unebase générique de règles associatives. Cet ensemble de taille réduite permet degénérer toutes les règles associatives via un système axiomatique adéquat. Danscet article, nous proposons une nouvelle approche FAST-MGB qui permet dedériver, directement à partir du contexte d'extraction formel, une base génériqueminimale de règles associatives.	Cherif Chiraz Latiri, Lamia Ben Ghezaiel, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000349	http://editions-rnti.fr/render_pdf.php?p=1000349	1410	fr	fr	@gnet.tn, @riadi.rnu.tn, @riadi.rnu.tn	Fast-MGB : nouveau Base Générique Minimale de Règles Associatives  le problème de le exploitation des règle associatif être devenir primordial , puisque le nombre des règle associatif extraire des jeu de donnéesréelles devenir très élever . un solution possible consister à ne dériver qu' unebase générique de règle associatif . ce ensemble de taille réduire permettre degénérer tout le règle associatif via un système axiomatique adéquat . Danscet article , nous proposer un nouveau approche FAST-MGB qui permettre dedériver , directement à partir du contexte d' extraction formel , un base génériqueminimale de règle associatif . 	Fast-MGB : Nouvelle Base Générique Minimale de Règles Associatives	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	Finding fragments of orders and total orders from 0-1 data	High-dimensional collections of 0-1 data occur in many applications. The attributes insuch data sets are typically considered to be unordered. However, in many cases there is anatural total or partial order underlying the variables of the data set. Examples of variablesfor which such orders exist include terms in documents and paleontological sites in fossil datacollections. We describe methods for finding fragments of total orders from such data, basedon finding frequently occurring patterns. We also discuss techniques for finding good totalorderings (seriation) based on spectral ordering and MCMC methods	Heikki Mannila	http://editions-rnti.fr/render_pdf.php?p1&p=1000315	http://editions-rnti.fr/render_pdf.php?p=1000315	1411	en	en	@cs.helsinki.fi	find fragment order total order 0 1 datum high dimensional collection 0 1 datum occur many application attribute insuch datum set typically consider unorder however many case anatural total partial order underlie variable data set example variablesfor order exist include term document paleontological site fossil datacollection describe method find fragment total order data basedon finding frequently occur pattern also discuss technique find good totalordering seriation base spectral order mcmc method	Finding fragments of orders and total orders from 0-1 data	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Fouille de données dans les systèmes Pair-à-Pair pour améliorer la recherche de ressources	La quantité de sources d'information disponible sur Internet fait dessystèmes d'échanges pair-à-pair (P2P) un genre nouveau d'architecture qui offreà une large communauté des applications pour partager des fichiers, des calculs,dialoguer ou communiquer en temps réel. Dans cet article, nous proposonsune nouvelle approche pour améliorer la localisation d'une ressource sur un réseauP2P non structuré. En utilisant une nouvelle heuristique, nous proposonsd'extraire des motifs qui apparaissent dans un grand nombre de noeuds du réseau.Cette connaissance est très utile pour proposer aux utilisateurs des fichierssouvent demandés (en requête ou en téléchargement) et éviter une trop grandeconsommation de la bande passante.	Florent Masseglia, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000390	http://editions-rnti.fr/render_pdf.php?p=1000390	1412	fr	fr	@inria.fr, @ema.fr, @lirmm.fr	fouille de donnée dans le système Pair-à-Pair pour améliorer le recherche de ressources  le quantité de source d' information disponible sur Internet faire dessystèmes d' échange pair-à-pair ( P2P ) un genre nouveau d' architecture qui offreà un large communauté des application pour partager un fichier , un calcul , dialoguer ou communiquer en temps réel . Dans ce article , nous proposonsune nouveau approche pour améliorer le localisation d' un ressource sur un réseauP 2P non structurer . En utiliser un nouveau heuristique , nous proposonsd'extraire un motif qui apparaître dans un grand nombre de noeud du réseau . ce connaissance être très utile pour proposer aux utilisateur des fichierssouvent demander ( en requête ou en téléchargement ) et éviter un trop grandeconsommation de le bande passante . 	Fouille de données dans les systèmes Pair-à-Pair pour améliorer la recherche de ressources	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Fouille de données spatiales, Approche basée sur la programmation logique inductive	Ce qui caractérise la fouille de données spatiales est la nécessité de prendre en compte les interactions des objets dans l'espace. Les méthodes classiques de fouille de données sont mal adaptées pour ce type d'analyse. Nous proposons dans cet article une approche basée sur la programmation logique inductive. Elle se base sur deux idées. La première consiste à matérialiser ces interactions spatiales dans des tables de distances, ramenant ainsi la fouille de données spatiales à la fouille de fonnées multi-tables. La seconde transforme les données en logique du premier ordre et applique ensuite la programmation logique inductive. Cet article présentera cette approche. Il décrira son application à la classification supervisée par arbre de décision spatial. Il présentera aussi les expérimentations réalisées et les résultats obtenus sur l'analyse de la contamination des coquillages dans la lagune de Thau.	Nadjim Chelghoum, Karine Zeitouni, Thierry Laugier, Annie Fiandrino, Lionel Loubersac	http://editions-rnti.fr/render_pdf.php?p1&p=1000400	http://editions-rnti.fr/render_pdf.php?p=1000400	1413	fr	fr	@ifremer.fr, @prism.uvsq.fr	fouille de donnée spatial , Approche baser sur le programmation logique inductive  Ce qui caractériser le fouille de donnée spatial être le nécessité de prendre en compte le interaction des objet dans le espace . le méthode classique de fouille de donnée être mal adapter pour ce type d' analyse . Nous proposer dans ce article un approche baser sur le programmation logique inductif . Elle clr baser sur deux idée . le premier consister à matérialiser ce interaction spatial dans un table de distance , ramener ainsi le fouille de donnée spatial à le fouille de fonnées multi-tables . le second transformer le donnée en logique du premier ordre et appliquer ensuite le programmation logique inductif . ce article présenter ce approche . Il décrire son application à le classification superviser par arbre de décision spatial . Il présenter aussi le expérimentation réaliser et le résultat obtenir sur le analyse de le contamination des coquillage dans le lagune de Thau . 	Fouille de données spatiales, Approche basée sur la programmation logique inductive	12
Revue des Nouvelles Technologies de l'Information	EGC	2006	Gestion de connaissances : Compétences et ressources pédagogiques		Olivier Gerbé, Thierno Diarra, Jacques Raynauld	http://editions-rnti.fr/render_pdf.php?p1&p=1000335	http://editions-rnti.fr/render_pdf.php?p=1000335	1414	fr		@hec.ca	Gestion de connaissances : Compétences et ressources pédagogiques 	Gestion de connaissances : Compétences et ressources pédagogiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Graphes de voisinage pour l'indexation et l'interrogation d'images par le contenu	La découverte d'informations cachées dans les bases de données multimédiasest une tâche difficile à cause de leur structure complexe et à la subjectivitéliée à leur interprétation. Face à cette situation, l'utilisation d'un indexest primordiale. Un index multimédia permet de regrouper les données selondes critères de similarité. Nous proposons dans cet article d'apporter une améliorationà une approche déjà existante d'interrogation d'images par le contenu .Nous proposons une méthode efficace pour mettre à jour, localement, les graphesde voisinage qui constituent notre structure d'index multimédia. Cette méthodeest basée sur une manière intelligente de localisation de points dans un espacemultidimensionnel. Des résultats prometteurs sont obtenus après des expérimentationssur diverses bases de données.	Hakim Hacid, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000318	http://editions-rnti.fr/render_pdf.php?p=1000318	1415	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	graphe de voisinage pour le indexation et le interrogation d' image par le contenu  le découverte d' information cacher dans le base de donnée multimédiasest un tâche difficile à cause de son structure complexe et à le subjectivitéliée à son interprétation . face à ce situation , le utilisation d' un indexest primordial . un index multimédia permettre de regrouper le donnée selondes critère de similarité . Nous proposer dans ce article d' apporter un améliorationà un approche déjà existant d' interrogation d' image par le contenu . Nous proposer un méthode efficace pour mettre à jour , localement , le graphesde voisinage qui constituer son structure d' index multimédia . ce méthodeest baser sur un manière intelligent de localisation de point dans un espacemultidimensionnel . un résultat prometteur être obtenir après un expérimentationssur divers base de donnée . 	Graphes de voisinage pour l'indexation et l'interrogation d'images par le contenu	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	I-Semantec : une plateforme collaborative de capitalisation des connaissances métier en conception de produits industriels		Mohamed-Foued Sriti, Philippe Boutinaud, Nada Matta, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1000441	http://editions-rnti.fr/render_pdf.php?p=1000441	1416	fr		@cadesis.com, @utt.fr	I-Semantec : une plateforme collaborative de capitalisation des connaissances métier en conception de produits industriels 	I-Semantec : une plateforme collaborative de capitalisation des connaissances métier en conception de produits industriels	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Indexation de vues virtuelles dans un médiateur XML pour le traitement de XQuery Text	Intégrer le traitement de requêtes de recherche d'information dans unmédiateur XML est un problème difficile. Ceci est notamment dû au fait quecertaines sources de données ne permettent pas de recherche sur mot-clefs etdistance ni de classer les résultats suivant leur pertinence. Dans cet article nousabordons l'intégration des fonctionnalités principales du standard XQuery Textdans XLive, un médiateur XML/XQuery. Pour cela nous avons choisid'indexer des vues virtuelles de documents. Les documents virtuelssélectionnés sont transformés en objets des sources. L'opérateur de sélectiondu médiateur est étendu pour supporter des recherches d'information sur lesdocuments de la vue. La recherche sur mots-clefs et le classement de résultatsont ainsi supportés. Notre formule de classement de résultats est adaptée auformat de données semi-structurées, basé sur le nombre de mots-clefs dans lesdifférents éléments et la distance entre les éléments d'un résultat.	Clément Jamard, Georges Gardarin	http://editions-rnti.fr/render_pdf.php?p1&p=1000325	http://editions-rnti.fr/render_pdf.php?p=1000325	1417	fr	fr	@prism.uvsq.fr	indexation de vue virtuel dans un médiateur XML pour le traitement de XQuery Text  intégrer le traitement de requête de recherche d' information dans unmédiateur XML être un problème difficile . ceci être notamment devoir au faire quecertaines source de donnée ne permettre pas de recherche sur mot _-clef etdistance ni de classer le résultat suivre son pertinence . Dans ce article nousabordons le intégration des fonctionnalité principal du standard XQuery Textdans XLive , un médiateur XML  XQuery . Pour cela nous avoir choisid'indexer des vue virtuel de document . le document virtuelssélectionnés être transformer en objet des source . le opérateur de sélectiondu médiateur être étendre pour supporter un recherche d' information sur lesdocuments de le vue . le recherche sur mot _-clef et le classement de résultatsont ainsi supporter . son formule de classement de résultat être adapter auformat de donnée semi-structurées , baser sur le nombre de mot _-clef dans lesdifférents élément et le distance entre le élément d' un résultat . 	Indexation de vues virtuelles dans un médiateur XML pour le traitement de XQuery Text	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Interrogation et Vérification de documents OWL dans le modèle des Graphes Conceptuels	OWL est un langage pour la description d'ontologies sur le Web. Cependant,en tant que langage, OWL ne fournit aucun moyen pour interpréter lesontologies qu'il décrit, et étant orienté machine, il reste difficilement compréhensiblepar l'humain. On propose une approche de visualisation, d'interrogationet de vérification de documents OWL, regroupées dans un unique environnementgraphique : le modèle des graphes conceptuels.	Thomas Raimbault, Henri Briand, Rémi Lehn, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000342	http://editions-rnti.fr/render_pdf.php?p=1000342	1418	fr	fr	@univ-angers.fr, @univ-nantes.fr	interrogation et vérification de document OWL dans le modèle des graphe Conceptuels  OWL être un langage pour le description d' ontologie sur le Web . cependant , en tant que langage , OWL ne fournir aucun moyen pour interpréter lesontologies qu' il décrire , et être orienter machine , il rester difficilement compréhensiblepar le humain . On proposer un approche de visualisation , d' interrogationet de vérification de document OWL , regrouper dans un unique environnementgraphique : le modèle des graphe conceptuel . 	Interrogation et Vérification de documents OWL dans le modèle des Graphes Conceptuels	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	La fouille de graphes dans les bases de données réactionnelles au service de la synthèse en chimie organique	La synthèse en chimie organique consiste à concevoir de nouvellesmolécules à partir de réactifs et de réactions. Les experts de la synthèse s'appuientsur de très grandes bases de données de réactions qu'ils consultent à traversdes procédures d'interrogation standard. Un processus de découverte denouvelles réactions leur permettrait de mettre au point de nouveaux procédés desynthèse. Cet article présente une modélisation des réactions par des graphes etintroduit une méthode de fouille de ces graphes de réaction qui permet de faireémerger des motifs génériques utiles à la prédiction de nouvelles réactions. Enfinl'article fait le point sur l'état actuel de ce travail de recherche en présentantle modèle général dans lequel s'intégrera un nouvel algorithme de fouille deréactions chimiques.	Frédéric Pennerath, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000398	http://editions-rnti.fr/render_pdf.php?p=1000398	1419	fr	fr	@supelec.fr, @loria.fr	le fouille de graphe dans le base de donnée réactionnel au service de le synthèse en chimie organique  le synthèse en chimie organique consister à concevoir de nouvellesmolécules à partir de réactif et de réaction . le expert de le synthèse clr appuientsur de très grand base de donnée de réaction qu' ils consulter à traversdes procédure d' interrogation standard . un processus de découverte denouvelles réaction leur permettre de mettre au point de nouveau procédé desynthèse . ce article présenter un modélisation des réaction par un graphe etintroduit un méthode de fouille de ce graphe de réaction qui permettre de faireémerger des motif générique utile à le prédiction de nouveau réaction . Enfinl'article faire le point sur le état actuel de ce travail de recherche en présentantle modèle général dans lequel clr intégrer un nouveau algorithme de fouille deréactions chimique . 	La fouille de graphes dans les bases de données réactionnelles au service de la synthèse en chimie organique	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	Le forage distribué des données : une méthode simple, rapide et efficace	Dans cet article nous nous attaquons au problème du forage de trèsgrandes bases de données distribuées. Le résultat visé est un modèle qui soit etprédictif et descriptif, appelé méta-classificateur. Pour ce faire, nous proposonsde miner à distance chaque base de données indépendamment. Puis, il s'agitde regrouper les modèles produits (appelés classificateurs de base), sachant quechaque forage produira un modèle prédictif et descriptif, représenté pour nos besoinspar un ensemble de règles de classification. Afin de guider l'assemblage del'ensemble final de règles, qui sera l'union des ensembles individuels de règles,un coefficient de confiance est attribué à chaque règle de chaque ensemble. Cecoefficient, calculé par des moyens statistiques, représente la confiance que nouspouvons avoir dans chaque règle en fonction de sa couverture et de son taux d'erreurface à sa capacité d'être appliquée correctement sur de nouvelles données.Nous démontrons dans cet article que, grâce à ce coefficient de confiance, l'agrégationpure et simple de tous les classificateurs de base pour obtenir un agrégatde règles produit un méta-classificateur rapide et efficace par rapport aux techniquesexistantes.	Mohamed Aounallah, Guy Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000328	http://editions-rnti.fr/render_pdf.php?p=1000328	1420	fr	fr	@ift.ulaval.ca	le forage distribuer des donnée : un méthode simple , rapide et efficace  Dans ce article nous nous attaquer au problème du forage de trèsgrandes base de donnée distribuer . le résultat viser être un modèle qui être etprédictif et descriptif , appeler méta-classificateur . Pour ce faire , nous proposonsde miner à distance chaque base de donnée indépendamment . Puis , il clr agitde regrouper le modèle produire ( appelé classificateur de base ) , savoir quechaque forage produire un modèle prédictif et descriptif , représenter pour son besoinspar un ensemble de règle de classification . Afin de guider le assemblage del'ensemble final de règle , qui être le union des ensemble individuel de règle , un coefficient de confiance être attribuer à chaque règle de chaque ensemble . Cecoefficient , calculer par un moyen statistique , représenter le confiance que nouspouvons avoir dans chaque règle en fonction de son couverture et de son taux d' erreurface à son capacité d' être appliquer correctement sur un nouveau donnée . Nous démontrer dans ce article que , grâce à ce coefficient de confiance , le agrégationpure et simple de tout le classificateur de base pour obtenir un agrégatde règle produire un méta-classificateur rapide et efficace par rapport aux techniquesexistantes . 	Le forage distribué des données : une méthode simple, rapide et efficace	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Maintaining an Online Bibliographical Database: The Problem of Data Quality	CiteSeer and Google-Scholar are huge digital libraries which provideaccess to (computer-)science publications. Both collections are operated likespecialized search engines, they crawl the web with little human interventionand analyse the documents to classify them and to extract some metadata fromthe full texts. On the other hand there are traditional bibliographic data baseslike INSPEC for engineering and PubMed for medicine. For the field of computerscience the DBLP service evolved from a small specialized bibliographyto a digital library covering most subfields of computer science. The collectionsof the second group are maintained with massive human effort. On the longterm this investment is only justified if data quality of the manually maintainedcollections remains much higher than that of the search engine style collections.In this paper we discuss management and algorithmic issues of data quality. Wefocus on the special problem of person names	Michael Ley, Patrick Reuther	http://editions-rnti.fr/render_pdf.php?p1&p=1000317	http://editions-rnti.fr/render_pdf.php?p=1000317	1421	en	en	@uni-trier.de	maintain online bibliographical database problem datum quality citeseer google scholar huge digital library provideaccess computer science publication collection operate likespecialize search engine crawl web little human interventionand analyse document classify extract metadata fromthe full text hand traditional bibliographic datum baseslike inspec engineering pubm medicine field computerscience dblp service evolved small specialize bibliographyto digital library cover subfield computer science collectionsof second group maintain massive human effort longterm investment justified data quality manually maintainedcollection remain much higher search engine style collection in paper discuss management algorithmic issue data quality wefocu special problem person name	Maintaining an Online Bibliographical Database: The Problem of Data Quality	60
Revue des Nouvelles Technologies de l'Information	EGC	2006	Méthode de récolte de traces de navigation sur interface graphique et visualisation de parcours		Marc Damez	http://editions-rnti.fr/render_pdf.php?p1&p=1000452	http://editions-rnti.fr/render_pdf.php?p=1000452	1422	fr		@esiea.fr	Méthode de récolte de traces de navigation sur interface graphique et visualisation de parcours 	Méthode de récolte de traces de navigation sur interface graphique et visualisation de parcours	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	Modèle conceptuel pour bases de données multidimensionnelles annotées	Nos travaux visent à proposer une mémoire d'expertises décisionnellespermettant de conserver et de manipuler non seulement les données décisionnellesmais aussi l'expertise analytique des décideurs. Les données décisionnellessont représentées au travers de concepts multidimensionnels etl'expertise associée est matérialisée grâce au concept d'annotation	Guillaume Cabanac, Max Chevalier, Franck Ravat, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000330	http://editions-rnti.fr/render_pdf.php?p=1000330	1423	fr	fr	@irit.fr	modèle conceptuel pour base de donnée multidimensionnel annotées  son travail viser à proposer un mémoire d' expertise décisionnellespermettant de conserver et de manipuler non seulement le donnée décisionnellesmais aussi le expertise analytique des décideur . le donnée décisionnellessont représenter au travers de concept multidimensionnel etl'expertise associer être matérialiser grâce au concept d' annotation 	Modèle conceptuel pour bases de données multidimensionnelles annotées	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Modèle décisionnel basé sur la qualité des données pour sélectionner les règles d'associations légitimement intéressantes	Dans cet article nous proposons d'exploiter des mesures décrivant laqualité des données pour définir la qualité des règles d'associations résultantd'un processus de fouille. Nous proposons un modèle décisionnel probabilistebasé sur le coût de la sélection de règles légitimement, potentiellement intéressantesou inintéressantes si la qualité des données à l'origine de leur calcul estbonne, moyenne ou douteuse. Les expériences sur les données de KDD-CUP-98 montrent que les 10 meilleures règles sélectionnées d'après leurs mesuresde support et confiance ne sont intéressantes que dans le cas où la qualité deleurs données est correcte voire améliorée.	Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000410	http://editions-rnti.fr/render_pdf.php?p=1000410	1424	fr	fr	@irisa.fr	modèle décisionnel baser sur le qualité des donnée pour sélectionner le règle d' association légitimement intéressantes  Dans ce article nous proposer d' exploiter un mesure décrire laqualité un donnée pour définir le qualité des règle d' association résultantd'un processus de fouille . Nous proposer un modèle décisionnel probabilistebasé sur le coût de le sélection de règle légitimement , potentiellement intéressantesou inintéressant si le qualité des donnée à le origine de son calcul estbonne , moyen ou douteur . le expérience sur le donnée de KDD-CUP- 98 montrer que le 10 meilleur règle sélectionner d' après son mesuresde support et confiance ne être intéressant que dans le cas où le qualité deleurs donner être correct voire améliorer . 	Modèle décisionnel basé sur la qualité des données pour sélectionner les règles d'associations légitimement intéressantes	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Modélisation informationnelle : un cadre méthodologique pour représenter des connaissances évolutives spatialisables	Pour comprendre et représenter les évolutions du bâti, question renouvelée avec le développement des NTIC, l'analyste s'appuie sur des connaissances évolutives ayant dans notre champ d'application - le patrimoine architectural - un caractère spatialisable (par l'attachement à un lieu lambda) mais aussi des caractéristiques handicapantes (hétérogénéité, incertitudes et contradictions, etc.). En réponse, nous utilisons ce caractère spatialisable pour intégrer les ressources constituant le jeu de connaissances propre à chaque édifice: théorie, sources documentaires, observations. Cette démarche que nous nommons modélisation informationnelle a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées. Notre contribution introduit les filiations de cette démarche, le cadre méthodologique qui la matérialise, et discute de son application au cas concret de la place centrale de Cracovie (Rynek Glowny) pour en évaluer l'apport potentiel en matière de gestion et de visualisation de connaissances.	Jean-Yves Blaise, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1000368	http://editions-rnti.fr/render_pdf.php?p=1000368	1425	fr	fr	@gamsau.map.archi.fr	modélisation informationnel : un cadre méthodologique pour représenter un connaissance évolutif spatialisables  Pour comprendre et représenter le évolution du bâti , question renouveler avec le développement des NTIC , le analyste clr appuyer sur un connaissance évolutif avoir dans son champ d' application - le patrimoine architectural - un caractère spatialisable ( par le attachement à un lieu lambda ) mais aussi des caractéristique handicapantes ( hétérogénéité , incertitude et contradiction , etc. ) . En réponse , nous utiliser ce caractère spatialisable pour intégrer le ressource constituer le jeu de connaissance propre à chaque édifice : théorie , source documentaire , observation . ce démarche que nous nommer modélisation informationnel avoir pour objectif un gain de compréhension du lieu architectural et des information qui lui être associer . son contribution introduire le filiation de ce démarche , le cadre méthodologique qui la matérialiser , et discuter de son application au cas concret de le place central de Cracovie ( Rynek Glowny ) pour en évaluer le apport potentiel en matière de gestion et de visualisation de connaissance . 	Modélisation informationnelle : un cadre méthodologique pour représenter des connaissances évolutives spatialisables	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Multi-catégorisation de textes juridiques et retour de pertinence	La fouille de données textuelles constitue un champ majeur dutraitement automatique des données. Une large variété de conférences, commeTREC, lui sont consacrées. Dans cette étude, nous nous intéressons à la fouillede textes juridiques, dans l'objectif est le classement automatique de ces textes.Nous utilisons des outils d'analyses linguistiques (extraction de terminologie)dans le but de repérer les concepts présents dans le corpus. Ces conceptspermettent de construire un espace de représentation de faible dimensionnalité,ce qui nous permet d'utiliser des algorithmes d'apprentissage basés sur desmesures de similarité entre individus, comme les graphes de voisinage. Nouscomparons les résultats issus du graphe et de C4.5 avec les SVM qui eux sontutilisés sans réduction de la dimensionnalité.	Vincent Pisetta, Hakim Hacid, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000353	http://editions-rnti.fr/render_pdf.php?p=1000353	1426	fr	fr	@univ-lyon2.fr, @eric-univ.lyon2.fr, @univ-lyon2.fr	Multi-catégorisation de texte juridique et retour de pertinence  le fouille de donnée textuel constituer un champ majeur dutraitement automatique des donnée . un large variété de conférence , commeTREC , lui être consacrer . Dans ce étude , nous clr intéresser à le fouillede texte juridique , dans le objectif être le classement automatique de ce texte . Nous utiliser un outil d' analyse linguistique ( extraction de terminologie ) dans le but de repérer le concept présent dans le corpus . ce conceptspermettent de construire un espace de représentation de faible dimensionnalité , ce qui nous permettre d' utiliser un algorithme d' apprentissage baser sur desmesures de similarité entre individu , comme le graphe de voisinage . Nouscomparons le résultat issu du graphe et de C4 . 5 avec le SVM qui lui sontutilisés sans réduction de le dimensionnalité . 	Multi-catégorisation de textes juridiques et retour de pertinence	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Outil de datamining spatial appliqué à l'analyse des risques liés au territoire		Schahrazed Zeghache, Farida Admane, Kamel Elaraba Ziane	http://editions-rnti.fr/render_pdf.php?p1&p=1000442	http://editions-rnti.fr/render_pdf.php?p=1000442	1427	fr		@mail.cerist.dz, @wissal.dz, @mail.cerist.dz	Outil de datamining spatial appliqué à l'analyse des risques liés au territoire 	Outil de datamining spatial appliqué à l'analyse des risques liés au territoire	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Prédiction de solubilité de molécules à partir des seules données relationnelles	La recherche de médicaments passe par la synthèse de molécules candidatesdont l'efficacité est ensuite testée. Ce processus peut être accéléré enidentifiant les molécules non solubles, car celles-ci ne peuvent entrer dans lacomposition d'un médicament et ne devraient donc pas être étudiées. Des techniquesont été développées pour induire un modèle de prédiction de l'indice desolubilité, utilisant principalement des réseaux de neurones ou des régressionslinéaires multiples. La plupart des travaux actuels visent à enrichir les donnéesde caractéristiques supplémentaires sur les molécules. Dans cet article, nous étudionsl'intérêt de la construction automatique d'attributs basée sur la structureintrinsèquement multi-relationnelle des données. Les attributs obtenus sont utilisésdans un algorithme d'arbre de modèles, auquel on associe une méthodede bagging. Les tests réalisés montrent que ces méthodes donnent des résultatscomparables aux meilleures méthodes du domaine qui travaillent sur des attributsconstruits par les experts.	Sébastien Derivaux, Agnès Braud, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1000424	http://editions-rnti.fr/render_pdf.php?p=1000424	1428	fr	fr	@lsiit.u-strasbg.fr	prédiction de solubilité de molécule à partir un seul donnée relationnelles  le recherche de médicament passer par le synthèse de molécule candidatesdont le efficacité être ensuite tester . ce processus pouvoir être accélérer enidentifiant le molécule non soluble , car celui _-ci ne pouvoir entrer dans lacomposition d' un médicament et ne devoir donc pas être étudier . un techniquesont être développer pour induire un modèle de prédiction de le indice desolubilité , utiliser principalement un réseau de neurone ou des régressionslinéaires multiple . le plupart des travail actuel viser à enrichir le donnéesde caractéristique supplémentaire sur le molécule . Dans ce article , nous étudionsl'intérêt de le construction automatique d' attribut baser sur le structureintrinsèquement multi-relationnelle des donnée . le attribut obtenir être utilisésdans un algorithme d' arbre de modèle , auquel on associer un méthodede bagging . le test réaliser montrer que ce méthode donner des résultatscomparables aux meilleur méthode du domaine qui travailler sur un attributsconstruits par le expert . 	Prédiction de solubilité de molécules à partir des seules données relationnelles	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Préparation des données Radar pour la reconnaissance/identification de cibles aériennes	La problématique générale présentée dans ce papier concerne lessystèmes intelligents, dédiés pour l'aide à la prise de décision dans le domaineradar. Les premiers travaux ont donc consisté après avoir adapté le processusd'extraction de connaissances à partir de données (ECD) au domaine radar, àmettre en oeuvre les étapes en amont de la phase de fouille de données. Nousnous limitons dans ce papier à la phase de préparation des données (imagesISAR : Inverse Synthetic Aperture Radar). Nous introduisons ainsi la notion dequalité comme moyen d'évaluer l'imperfection dans les données radarsexpérimentales.	Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000426	http://editions-rnti.fr/render_pdf.php?p=1000426	1429	fr	fr	@ensieta.fr	préparation des donnée Radar pour le reconnaissance  identification de cible aériennes  le problématique général présenter dans ce papier concerner lessystèmes intelligent , dédier pour le aide à le prise de décision dans le domaineradar . le premier travail avoir donc consister après avoir adapter le processusd'extraction de connaissance à partir de donnée ( ECD ) au domaine radar , àmettre en oeuvre le étape en amont de le phase de fouille de donnée . Nousnous limiter dans ce papier à le phase de préparation des donnée ( imagesISAR : Inverse Synthetic Aperture Radar ) . Nous introduire ainsi le notion dequalité comme moyen d' évaluer le imperfection dans le donnée radarsexpérimentales . 	Préparation des données Radar pour la reconnaissance/identification de cibles aériennes	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	Prétraitement de grands ensembles de données pour la fouille visuelle	Nous présentons une nouvelle approche pour le traitement des ensemblesde données de très grande taille en fouille visuelle de données. Les limitesde l'approche visuelle concernant le nombre d'individus et le nombre dedimensions sont connues de tous. Pour pouvoir traiter des ensembles de donnéesde grande taille, une solution possible est d'effectuer un prétraitement del'ensemble de données avant d'appliquer l'algorithme interactif de fouille visuelle.Pour ce faire, nous utilisons la théorie du consensus (avec une affectationvisuelle des poids). Nous évaluons les performances de notre nouvelle approchesur des ensembles de données de l'UCI et du Kent Ridge Bio MedicalDataset Repository.	François Poulet, Edwige Fangseu Badjio	http://editions-rnti.fr/render_pdf.php?p1&p=1000324	http://editions-rnti.fr/render_pdf.php?p=1000324	1430	fr	fr	@esiea-ouest.fr, @esiea-ouest.fr	Prétraitement de grand ensemble de donnée pour le fouille visuelle  Nous présenter un nouveau approche pour le traitement des ensemblesde donner de très grand taille en fouille visuel de donnée . le limitesde le approche visuel concerner le nombre d' individu et le nombre dedimensions être connaître de tout . Pour pouvoir traiter un ensemble de donnéesde grand taille , un solution possible être d' effectuer un prétraitement del'ensemble de donnée avant d' appliquer le algorithme interactif de fouille visuel . Pour ce faire , nous utiliser le théorie du consensus ( avec un affectationvisuelle des poids ) . Nous évaluer le performance de son nouveau approchesur des ensemble de donnée de le UCI et du Kent Ridge Bio MedicalDataset Repository . 	Prétraitement de grands ensembles de données pour la fouille visuelle	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche de règles non redondantes par vecteurs de bits dans des grandes bases de motifs 		François Jacquenet, Christine Largeron, Cédric Udréa	http://editions-rnti.fr/render_pdf.php?p1&p=1000414	http://editions-rnti.fr/render_pdf.php?p=1000414	1431	fr		@univ-st-etienne.fr	Recherche de règles non redondantes par vecteurs de bits dans des grandes bases de motifs  	Recherche de règles non redondantes par vecteurs de bits dans des grandes bases de motifs 	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche de sous-structures fréquentes pour l'intégration de schémas XML	La recherche d'un schéma médiateur à partir d'un ensemble de schémasXML est une problématique actuelle où les résultats de recherche issusde la fouille de données arborescentes peuvent être adoptés. Dans ce contexte,plusieurs propositions ont été réalisées mais les méthodes de représentation desarborescences sont souvent trop coûteuses pour permettre un véritable passageà l'échelle. Dans cet article, nous proposons des algorithmes de recherche desous-schémas fréquents basés sur une méthode originale de représentation deschémas XML. Nous décrivons brièvement la structure adoptée pour ensuitedétailler les algorithmes de recherche de sous-arbres fréquents s'appuyant surune telle structure. La représentation proposée et les algorithmes associés ontété évalués sur différentes bases synthétiques de schémas XML montrant ainsil'intérêt de l'approche proposée	Federico Del Razo López, Anne Laurent, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000393	http://editions-rnti.fr/render_pdf.php?p=1000393	1432	fr	fr	@lirmm.fr, @ema.fr	recherche de sous-structures fréquent pour le intégration de schéma XML  le recherche d' un schéma médiateur à partir d' un ensemble de schémasXML être un problématique actuel où le résultat de recherche issusde le fouille de donnée arborescent pouvoir être adopter . Dans ce contexte , plusieurs proposition avoir être réaliser mais le méthode de représentation desarborescences être souvent trop coûteux pour permettre un véritable passageà le échelle . Dans ce article , nous proposer un algorithme de recherche desous-schémas fréquent baser sur un méthode original de représentation deschémas xml . Nous décrire brièvement le structure adopter pour ensuitedétailler le algorithme de recherche de sous-arbres fréquent clr appuyer surune tel structure . le représentation proposer et le algorithme associer ontété évaluer sur différent base synthétique de schéma XML montrer ainsil'intérêt de le approche proposée 	Recherche de sous-structures fréquentes pour l'intégration de schémas XML	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche en temps réel de préfixes massifs hiérarchiques dans un réseau IP à l'aide de techniques de stream mining	Au cours de ces dernières années, de nombreuses techniques de streammining ont été proposées afin d'analyser des flux de données en temps réel.Dans cet article, nous montrons comment nous avons utilisé des techniques destream mining permettant la recherche d'objets massifs hiérarchiques (hierarchicalheavy hitters) dans un flux de données pour identifier en temps réel dans unréseau IP les préfixes dont la contribution au trafic dépasse une certaine proportionde ce trafic pendant un intervalle de temps donné.	Pascal Cheung-Mon-Chan, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000323	http://editions-rnti.fr/render_pdf.php?p=1000323	1433	fr	fr	@francetelecom.com	recherche en temps réel de préfixe massif hiérarchique dans un réseau IP à le aide de technique de stream mining  Au cour de ce dernier année , de nombreux technique de streammining avoir être proposer afin d' analyser un flux de donnée en temps réel . Dans ce article , nous montrer comment nous avoir utiliser un technique destream mining permettre le recherche d' objet massif hiérarchique ( hierarchicalheavy hitters ) dans un flux de donnée pour identifier en temps réel dans unréseau IP le préfixe dont le contribution au trafic dépasser un certain proportionde ce trafic pendant un intervalle de temps donner . 	Recherche en temps réel de préfixes massifs hiérarchiques dans un réseau IP à l'aide de techniques de stream mining	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Reconnaissance automatique de concepts à partir d'une ontologie	Ce papier présente une approche qui s'appuie sur une ontologie pourreconnaître automatiquement des concepts spécifiques à un domaine dans uncorpus en langue naturelle. La solution proposée est non-supervisée et peuts'appliquer à tout domaine pour lequel une ontologie a été déjà construite. Uncorpus du domaine est utilisé dans lequel les concepts seront reconnus. Dansune première phase, des connaissances sont extraites de ce corpus en faisantappel à des fouilles de textes. Une ontologie du domaine est utilisée pour étiqueterces connaissance. Le papier donne un aperçu des techniques de fouillesemployées et décrit le processus d 'étiquetage. Les résultats d'une premièreexpérimentation dans le domaine de l'accidentologie sont aussi présentés	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1000351	http://editions-rnti.fr/render_pdf.php?p=1000351	1434	fr	fr	@univ-paris5.fr, @univ-paris5.fr	reconnaissance automatique de concept à partir d' un ontologie  ce papier présenter un approche qui clr appuyer sur un ontologie pourreconnaître automatiquement un concept spécifique à un domaine dans uncorpus en langue naturel . le solution proposer être non- superviser et peuts'appliquer à tout domaine pour lequel un ontologie avoir être déjà construire . Uncorpus du domaine être utiliser dans lequel le concept être reconnaître . Dansune premier phase , un connaissance être extraire de ce corpus en faisantappel à un fouille de texte . un ontologie du domaine être utiliser pour étiqueterces connaissance . le papier donner un aperçu des technique de fouillesemployées et décrire le processus d ' étiquetage . . le résultat d' un premièreexpérimentation dans le domaine de le accidentologie être aussi présentés 	Reconnaissance automatique de concepts à partir d'une ontologie	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Reconnaissance automatique d'évènements survenant sur patients en réanimation à l'aide d'une méthode adaptative d'extraction en ligne d'épisodes temporels	Ce papier présente la version adaptative d'un algorithmed'extraction d'épisodes temporels développé précédemment. Les trois paramè-tres de réglages de l'algorithme ne sont plus fixes. Ils sont modifiés en ligne enfonction de la variance estimée du signal que l'on veut décomposer en épiso-des temporels. La version adaptative de l'algorithme a été utilisée pour recon-naître automatiquement des aspirations trachéales à partir de plusieures varia-bles physiologiques enregistrés sur des patients hospitalisés en réanimation.Des résultats préliminaires sont présentés dans ce papier.	Sylvie Charbonnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000333	http://editions-rnti.fr/render_pdf.php?p=1000333	1435	fr	fr	@inpg.fr	reconnaissance automatique d' évènements survenir sur patient en réanimation à le aide d' un méthode adaptatif d' extraction en ligne d' épisode temporels  ce papier présenter le version adaptatif d' un algorithmed'extraction d' épisode temporel développer précédemment . le trois paramè-tres de réglage de le algorithme ne être plus fixe . Ils être modifier en ligne enfonction de le variance estimer du signal que le on vouloir décomposer en épiso- des temporel . le version adaptatif de le algorithme avoir être utiliser pour recon-naître automatiquement un aspiration trachéales à partir de plusieures varia-bles physiologique enregistrer sur un patient hospitaliser en réanimation . un résultat préliminaire être présenter dans ce papier . 	Reconnaissance automatique d'évènements survenant sur patients en réanimation à l'aide d'une méthode adaptative d'extraction en ligne d'épisodes temporels	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Règles d'association avec une prémisse composée : Mesure du gain d'information.	La communauté de fouille de données a développé un grand nombre d'indices permettantde mesurer la qualité des règles d'association (RA) selon diverses sémantiques (Guillet,2004). Cependant ces sémantiques, qui permettent d'interpréter les règles simples, s'avèrentd'utilisation trop complexe pour un expert dans le cas de règles à prémisse composée. Notreobjectif est donc de sélectionner les règles à prémisse composée de type AB&#8594;C quiapportent une information supplémentaire à celle des règles simples A&#8594;C et B&#8594;C. Pourcela nous définissons un indice de gain d'une règle composée par rapport aux règles simples.Dans l'application présentée, nous extrayons des RA de résultats de classifications pouren faciliter l'analyse . Le gain a permis de filtrer des règles d'interprétation simple	Martine Cadot, Pascal Cuxac, Claire François	http://editions-rnti.fr/render_pdf.php?p1&p=1000412	http://editions-rnti.fr/render_pdf.php?p=1000412	1436	fr	fr	@loria.fr, @inist.fr, @inist.fr	règle d' association avec un prémisse composer : mesure du gain d' information .  le communauté de fouille de donnée avoir développer un grand nombre d' indice permettantde mesurer le qualité des règle d' association ( RA ) selon divers sémantique ( Guillet , 2004 ) . cependant ce sémantique , qui permettre d' interpréter le règle simple , clr avèrentd'utilisation trop complexe pour un expert dans le cas de règle à prémisse composer . Notreobjectif être donc de sélectionner le règle à prémisse composer de type AB→C quiapportent un information supplémentaire à celui des règle simple A→C et B→C. Pourcela nous définir un indice de gain d' un règle composer par rapport aux règle simple . Dans le application présenter , nous extraire des ra de résultat de classification pouren faciliter le analyse . le gain avoir permettre de filtrer un règle d' interprétation simple 	Règles d'association avec une prémisse composée : Mesure du gain d'information.	6
Revue des Nouvelles Technologies de l'Information	EGC	2006	Représentation d'expertise psychologique sous la forme de graphes orientés, codés en RDF		Yves Fossé, Stéphane Daviet, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000434	http://editions-rnti.fr/render_pdf.php?p=1000434	1437	fr		@univ-nantes.fr	Représentation d'expertise psychologique sous la forme de graphes orientés, codés en RDF 	Représentation d'expertise psychologique sous la forme de graphes orientés, codés en RDF	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Représentation des connaissances appliquées à la géotechnique : une approche		Nicolas Faure	http://editions-rnti.fr/render_pdf.php?p1&p=1000436	http://editions-rnti.fr/render_pdf.php?p=1000436	1438	fr		@univ-lyon3.fr	Représentation des connaissances appliquées à la géotechnique : une approche 	Représentation des connaissances appliquées à la géotechnique : une approche	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Sélection de variables et modélisation d'expression d'émotion dans les dialogues Homme-Machine		Barbara Poulain	http://editions-rnti.fr/render_pdf.php?p1&p=1000438	http://editions-rnti.fr/render_pdf.php?p=1000438	1439	fr		@francetelecom.com	Sélection de variables et modélisation d'expression d'émotion dans les dialogues Homme-Machine 	Sélection de variables et modélisation d'expression d'émotion dans les dialogues Homme-Machine	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Sélection supervisée d'instances : une approche descriptive	La classification suivant le plus proche voisin est une règle simple etperformante. Sa mise en oeuvre pratique nécessite, tant pour des raisons de coûtde calcul que de robustesse, de sélectionner les instances à conserver. La partitionde Voronoi induite par les prototypes constitue la structure sous-jacente àcette règle. Dans cet article, on introduit un critère descriptif d'évaluation d'unetelle partition, quantifiant le compromis entre nombre de cellules et discriminationde la variable cible entre les cellules. Une heuristique d'optimisation estproposée, tirant partie des propriétés des partitions de Voronoi et du critère. Laméthode obtenue est comparée avec les standards sur une vingtaine de jeux dedonnées de l'UCI. Notre technique ne souffre d'aucun défaut de performanceprédictive, tout en sélectionnant un minimum d'instances. De plus, elle ne surapprendpas.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000382	http://editions-rnti.fr/render_pdf.php?p=1000382	1440	fr	fr	@francetelecom.com, @francetelecom.com	sélection superviser d' instance : un approche descriptive  le classification suivre le plus proche voisin être un règle simple etperformante . son mise en oeuvre pratique nécessiter , tant pour un raison de coûtde calcul que de robustesse , de sélectionner le instance à conserver . le partitionde Voronoi induire par le prototype constituer le structure sous-jacent àcette règle . Dans ce article , on introduire un critère descriptif d' évaluation d' unetelle partition , quantifier le compromis entre nombre de cellule et discriminationde le variable cible entre le cellule . un heuristique d' optimisation estproposée , tirer partie des propriété des partition de Voronoi et du critère . Laméthode obtenir être comparer avec le standard sur un vingtaine de jeu dedonnées de le UCI . son technique ne souffrir d' aucun défaut de performanceprédictive , tout en sélectionner un minimum d' instance . De plus , elle ne surapprendpas . 	Sélection supervisée d'instances : une approche descriptive	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	SVM incrémental, parallèle et distribué pour le traitement de grandes quantités de données	Nous présentons un nouvel algorithme de SVM (Support VectorMachine ou Séparateur à Vaste Marge) linéaire et non-linéaire, parallèle etdistribué permettant le traitement de grands ensembles de données dans untemps restreint sur du matériel standard. A partir de l'algorithme de Newton-GSVM proposé par Mangasarian, nous avons construit un algorithmeincrémental, parallèle et distribué permettant d'améliorer les performances entemps d'exécution et mémoire en s'exécutant sur un groupe d'ordinateurs. Cenouvel algorithme a la capacité de classifier un million d'individus en 20dimensions et deux classes en quelques secondes sur un ensemble de dix PC	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000322	http://editions-rnti.fr/render_pdf.php?p=1000322	1441	fr	fr	@cit.ctu.edu.vn, @esiea-ouest.fr	SVM incrémental , parallèle et distribuer pour le traitement de grand quantité de données  Nous présenter un nouveau algorithme de SVM ( Support VectorMachine ou Séparateur à vaste Marge ) linéaire et non- linéaire , parallèle etdistribué permettre le traitement de grand ensemble de donnée dans untemps restreindre sur du matériel standard . A partir de le algorithme de Newton-GSVM proposer par Mangasarian , nous avoir construire un algorithmeincrémental , parallèle et distribuer permettre d' améliorer le performance entemps d' exécution et mémoire en clr exécuter sur un groupe d' ordinateur . Cenouvel algorithme avoir le capacité de classifier un million d' individu en 20dimensions et deux classe en quelque second sur un ensemble de dix PC 	SVM incrémental, parallèle et distribué pour le traitement de grandes quantités de données	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Système d'aide à la décision pour la surveillance de la qualité de l'air intérieur		Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1000445	http://editions-rnti.fr/render_pdf.php?p=1000445	1442	fr		@univ-paris5.fr, @cstb.fr, @cstb.fr	Système d'aide à la décision pour la surveillance de la qualité de l'air intérieur 	Système d'aide à la décision pour la surveillance de la qualité de l'air intérieur	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Techniques de fouille de données pour la réécriture de requêtes en présence de contraintes de valeurs	Dans cet article, nous montrons comment les techniques de fouilles de données peuvent résoudre efficacement le problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs. A partir d'une formalisation du problème de la réécriture dans le cadre de la logique de description ALN(Ov), nous montrons comment ce problème se rattache à un cadre de découverte de connaissances dans les bases de données. L'exploitation de ce cadre nous permet de bénéficier de solutions algorithmiques existantes pour la résolution du problème de réécriture. Nous proposons une implémentation de cette approche, puis nous l'expérimentons. Les premiers résultats démontrent l'intérêt d'une telle approche en termes de capacité à traiter un grand nombre de sources de données.	Hélène Jaudoin, Frédéric Flouvat	http://editions-rnti.fr/render_pdf.php?p1&p=1000326	http://editions-rnti.fr/render_pdf.php?p=1000326	1443	fr	fr	@isima	technique de fouille de donnée pour le réécriture de requête en présence de contrainte de valeurs  Dans ce article , nous montrer comment le technique de fouille de donnée pouvoir résoudre efficacement le problème de le réécriture de requête en terme de vue en présence de contrainte de valeur . A partir d' un formalisation du problème de le réécriture dans le cadre de le logique de description ALN ( Ov ) , nous montrer comment ce problème clr rattacher à un cadre de découverte de connaissance dans le base de donnée . le exploitation de ce cadre nous permettre de bénéficier de solution algorithmique existant pour le résolution du problème de réécriture . Nous proposer un implémentation de ce approche , puis nous le expérimentons . le premier résultat démontrer le intérêt d' un tel approche en terme de capacité à traiter un grand nombre de source de donnée . 	Techniques de fouille de données pour la réécriture de requêtes en présence de contraintes de valeurs	1
Revue des Nouvelles Technologies de l'Information	EGC	2006	Teximus Expertise : un logiciel de gestion de connaissances	Le logiciel Teximus Expertise est un outil évolué de gestion dynamiquede connaissances basé sur les notions de référentiel sémantique. Cette suiteintégrée facilite le partage de connaissances et d'informations dans les entreprises.	Olivier Gerbé	http://editions-rnti.fr/render_pdf.php?p1&p=1000453	http://editions-rnti.fr/render_pdf.php?p=1000453	1444	fr	fr	@teximus.com	Teximus Expertise : un logiciel de gestion de connaissances  le logiciel Teximus Expertise être un outil évoluer de gestion dynamiquede connaissance baser sur le notion de référentiel sémantique . ce suiteintégrée faciliter le partage de connaissance et d' information dans le entreprise . 	Teximus Expertise : un logiciel de gestion de connaissances	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Typicalité et contribution des sujets et des variables supplémentaires en Analyse Statistique Implicative	L'analyse statistique implicative traite des tableaux sujets xvariables afin d'extraire règles et métarègles statistiques entre les variables.L'article interroge les structures obtenues représentées par graphe et hiérarchieorientés afin de dégager la responsabilité des sujets ou des groupes de sujets(variables supplémentaires) dans la constitution des chemins du graphe ou desclasses de la hiérarchie. On distingue les concepts de typicalité pour signifier laproximité des sujets avec le comportement moyen de la population envers lesrègles statistiques extraites, puis de contribution pour quantifier le rôlequ'auraient les sujets par rapport aux règles strictes associées. Un exemple dedonnées réelles, traité à l'aide du logiciel CHIC, illustre et montre l'intérêt deces deux concepts.	Régis Gras, Jérôme David, Jean-Claude Régnier, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000370	http://editions-rnti.fr/render_pdf.php?p=1000370	1445	fr	fr	@club-internet.fr, @univ-nantes.fr, @univ-lyon2.fr	Typicalité et contribution des sujet et des variable supplémentaire en analyse statistique Implicative  le analyse statistique implicative traite des tableau sujet xvariables afin d' extraire règle et métarègles statistique entre le variable . le article interroger le structure obtenir représenter par graphe et hiérarchieorientés afin de dégager le responsabilité des sujet ou un groupe de sujet ( variable supplémentaire ) dans le constitution des chemin du graphe ou desclasses de le hiérarchie . On distinguer le concept de typicalité pour signifier laproximité un sujet avec le comportement moyen de le population envers lesrègles statistique extraire , puis de contribution pour quantifier le rôlequ'auraient le sujet par rapport aux règle strict associer . un exemple dedonnées réel , traiter à le aide du logiciel CHIC , illustrer et montrer le intérêt deces deux concept . 	Typicalité et contribution des sujets et des variables supplémentaires en Analyse Statistique Implicative	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un automate pour évaluer la nature des textes	On ne peut s'intéresser aux textes sans s'intéresser à leur nature. La nature des textes permet de distinguer les textes d'un point de vue primaire. Elle est utilisée pour identifier les textes artificiels, pour la reconnaissance de la langue, afin d'identifier les SPAMS... En ce sens, la méthode la plus connue reste encore la méthode de Zipf. Cet article propose une nouvelle méthode basée sur un automate. L'automate construit un signal pour chaque texte. L'automate est présenté en détail et des expérimentations montrent son utilité dans les domaines aussi divers que ceux cités précédemment/	Hubert Marteau, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000355	http://editions-rnti.fr/render_pdf.php?p=1000355	1446	fr	fr	@univ-tours.fr, @univ-paris5.fr	un automate pour évaluer le nature des textes  On ne pouvoir clr intéresser aux texte sans clr intéresser à son nature . le nature des texte permettre de distinguer le texte d' un point de vue primaire . Elle être utiliser pour identifier le texte artificiel , pour le reconnaissance de le langue , afin d' identifier le spam ... En ce sens , le méthode le plus connaître rester encore le méthode de Zipf . ce article proposer un nouveau méthode baser sur un automate . le automate construire un signal pour chaque texte . le automate être présenter en détail et des expérimentation montrer son utilité dans le domaine aussi divers que celui cité précédemment  	Un automate pour évaluer la nature des textes	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un logiciel permettant d'apprendre des règles et leurs exceptions : Area		Sylvain Lagrue, Jérémie Lussiez, Julien Rossit	http://editions-rnti.fr/render_pdf.php?p1&p=1000454	http://editions-rnti.fr/render_pdf.php?p=1000454	1447	fr		@univ-artois.fr, @univ-artois.fr, @univ-artois.fr	Un logiciel permettant d'apprendre des règles et leurs exceptions : Area 	Un logiciel permettant d'apprendre des règles et leurs exceptions : Area	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un modèle de qualité de l'information	Ce travail s'intègre dans la problématique générale de la recherched'information ; et plus particulièrement dans la personnalisation et la qualitéd'information. Dans cet article nous proposons un modèle multidimensionnelde la qualité de l'information décrivant les différents facteurs de qualité influantsur la personnalisation de l'information. Ce modèle permet de structurerles différents facteurs de qualité de l'information dans une hiérarchie afind'assister l'utilisateur dans la construction de son propre profil selon ses besoinset ses exigences en termes de qualité.	Rami Harrathi, Sylvie Calabretto	http://editions-rnti.fr/render_pdf.php?p1&p=1000363	http://editions-rnti.fr/render_pdf.php?p=1000363	1448	fr	fr	@yahoo.fr, @insa-lyon.fr	un modèle de qualité de le information  ce travail clr intégrer dans le problématique général de le recherched'information ; et plus particulièrement dans le personnalisation et le qualitéd'information . Dans ce article nous proposer un modèle multidimensionnelde le qualité de le information décrire le différent facteur de qualité influantsur le personnalisation de le information . ce modèle permettre de structurerles différent facteur de qualité de le information dans un hiérarchie afind'assister le utilisateur dans le construction de son propre profil selon son besoinset son exigence en terme de qualité . 	Un modèle de qualité de l'information	8
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un modèle métier extensible adapté à la gestion de dépêches d'agences de presse		Frédéric Bertrand, Cyril Faucher, Marie-Christine Lafaye, Jean-Yves Lafaye, Alain Bouju	http://editions-rnti.fr/render_pdf.php?p1&p=1000447	http://editions-rnti.fr/render_pdf.php?p=1000447	1449	fr		@univ-lr.fr	Un modèle métier extensible adapté à la gestion de dépêches d'agences de presse 	Un modèle métier extensible adapté à la gestion de dépêches d'agences de presse	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche distribuée pour l'extraction de connaissances : Application à l'enrichissement de l'aspect factuel des BDG	Les systèmes d'informations géographiques (SIG) sont utilisés pouraméliorer l'efficacité des entreprises et des services publics, en associantméthodes d'optimisation et prise en compte de la dimension géographique.Cependant, les bases de données géographiques (BDG) stockées dans les SIGsont restreintes à l'application pour laquelle elles ont été conçues. Souvent, lesutilisateurs demeurent contraints de l'existant et se trouvent dans le besoin dedonnées complémentaires pour une prise de décision adéquate. D'où, l'idée del'enrichissement de l'aspect descriptif des BDG existantes. Pour atteindre cetobjectif, nous proposons une approche qui consiste à intégrer un module defouille de données textuelles au SIG lui même. Il s'agit de proposer uneméthode distribuée de résumé de documents multiples à partir de corpus enligne.L'idée est de faire coopérer un ensemble d'agents s'entraidant afind'aboutir à un résumé optimal.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1000329	http://editions-rnti.fr/render_pdf.php?p=1000329	1450	fr	fr	@insat.rnu.tn, @insat.rnu.tn	un approche distribuer pour le extraction de connaissance : application à le enrichissement de le aspect factuel des BDG  le système d' information géographique ( SIG ) être utiliser pouraméliorer le efficacité des entreprise et des service public , en associantméthodes d' optimisation et prise en compte de le dimension géographique . cependant , le base de donnée géographique ( BDG ) stocker dans le SIGsont restreindre à le application pour laquelle elles avoir être concevoir . souvent , lesutilisateurs demeurer contraindre de le existant et clr trouver dans le besoin dedonnées complémentaire pour un prise de décision adéquat . D' où , le idée del'enrichissement de le aspect descriptif des BDG existant . Pour atteindre cetobjectif , nous proposer un approche qui consister à intégrer un module defouille de donnée textuel au SIG lui même . Il clr agir de proposer uneméthode distribuer de résumé de document multiple à partir de corpus enligne . le idée être de faire coopérer un ensemble d' agent clr entraider afind'aboutir à un résumé optimal . 	Une approche distribuée pour l'extraction de connaissances : Application à l'enrichissement de l'aspect factuel des BDG	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche multi-agent adaptative pour la simulation de schémas tactiques	Ce papier est consacré à la simulation ou à la réalisation automatiquede schémas tactiques par un groupe d´agents footballeurs autonomes. Son objectifest de montrer ce que peuvent apporter des techniques d'apprentissagepar renforcement à des agents réactifs conçus pour cette tâche. Dans un premiertemps, nous proposons une plateforme et une architecture d'agents capabled'effectuer des schémas tactiques dans des cas relativement simples. Ensuite,nous mettons en oeuvre un algorithme d'apprentissage par renforcementpour permettre aux agents de faire face à des situations plus complexes. Enfin,une série d'expérimentations montrent le gain apporté aux agents réactifs parl'utilisation d'algorithmes d'apprentissage.	Aydano Machado, Yann Chevaleyre, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000334	http://editions-rnti.fr/render_pdf.php?p=1000334	1451	fr	fr	@lip6.fr, @lamsade.dauphine.fr	un approche multi-agent adaptatif pour le simulation de schéma tactiques  ce papier être consacrer à le simulation ou à le réalisation automatiquede schéma tactique par un groupe d' agent footballeur autonome . son objectifest de montrer ce que pouvoir apporter un technique d' apprentissagepar renforcement à un agent réactif concevoir pour ce tâche . Dans un premiertemps , nous proposer un plateforme et un architecture d' agent capabled'effectuer des schéma tactique dans un cas relativement simple . ensuite , nous mettre en oeuvre un algorithme d' apprentissage par renforcementpour permettre aux agent de faire face à un situation plus complexe . enfin , un série d' expérimentation montrer le gain apporter aux agent réactif parl'utilisation d' algorithme d' apprentissage . 	Une approche multi-agent adaptative pour la simulation de schémas tactiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche simple inspirée des réseaux sociaux pour la hiérarchisation des systèmes autonomes de l'Internet	"Le transit des flux d'information dans le réseau Internet à l'échellemondiale est régi par des accords commerciaux entre systèmes autonomes, accordsqui sont mis en oeuvre via le protocole de routage BGP. La négociationde ces accords commerciaux repose implicitement sur une hiérarchie des systèmesautonomes et la position relative de deux systèmes débouche sur un accordde type client/fournisseur (un des systèmes, le client, est nettement mieuxclassé que l'autre, le fournisseur, et le client paye le fournisseur pour le transitdes flux d'information) ou sur un accord de type ""peering"" (transit gratuit dutrafic entre les deux systèmes). En dépit de son importance, il n'existe pas dehiérarchie officielle de l'Internet (les clauses commerciales des accords entresystèmes autonomes ne sont pas nécessairement publiques) ni de consensus surla façon d'établir une telle hiérarchie. Nous proposons une heuristique simpleinspirée de la notion de ""centralité spectrale"" issue de l'analyse des réseaux sociauxpour analyser la position relative des systèmes autonomes de l'Internet àpartir des informations des seules informations de connectivité entre systèmesautonomes."	Fabrice Clérot, Quang Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000391	http://editions-rnti.fr/render_pdf.php?p=1000391	1452	fr	fr	@francetelecom.com, @francetelecom.com	un approche simple inspirer des réseau social pour le hiérarchisation des système autonome de le Internet  " le transit des flux d' information dans le réseau Internet à le échellemondiale être régir par un accord commercial entre système autonome , accordsqui être mettre en oeuvre via le protocole de routage BGP . . le négociationde ce accord commercial reposer implicitement sur un hiérarchie des systèmesautonomes et le position relatif de deux système déboucher sur un accordde type client  fournisseur ( un des système , le client , être nettement mieuxclassé que le autre , le fournisseur , et le client payer le fournisseur pour le transitdes flux d' information ) ou sur un accord de type " " peering " " ( transit gratuit dutrafic entre le deux système ) . . En dépit de son importance , il n' exister pas dehiérarchie officiel de le Internet ( le clause commercial des accord entresystèmes autonome ne être pas nécessairement public ) ni de consensus surla façon d' établir un tel hiérarchie . . Nous proposer un heuristique simpleinspirée de le notion de " " centralité spectral " " issue de le analyse des réseau sociauxpour analyser le position relatif des système autonome de le Internet àpartir des information des seul information de connectivité entre systèmesautonomes . " 	Une approche simple inspirée des réseaux sociaux pour la hiérarchisation des systèmes autonomes de l'Internet	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une comparaison de certains indices de pertinence des règles d'association	Cet article propose une comparaison graphique de certains indices depertinence pour évaluer l'intérêt des règles d'association. Nous nous sommesappuyés sur une étude existante pour sélectionner quelques indices auxquelsnous avons ajouté l'indice de Jaccard et l'indice d'accords désaccords (IAD).Ces deux derniers nous semblent plus adaptés pour discriminer les règles intéressantesdans le cas où les items sont des événements peu fréquents. Une applicationest réalisée sur des données réelles issues du secteur automobile	Marie Plasse, Ndeye Niang, Gilbert Saporta, Laurent Leblond	http://editions-rnti.fr/render_pdf.php?p1&p=1000405	http://editions-rnti.fr/render_pdf.php?p=1000405	1453	fr	fr	@cnam.fr, @cnam.fr, @mpsa.com, @mpsa.com	un comparaison de certain indice de pertinence des règle d' association  ce article proposer un comparaison graphique de certain indice depertinence pour évaluer le intérêt des règle d' association . Nous nous sommesappuyés sur un étude existant pour sélectionner quelque indice auxquelsnous avoir ajouter le indice de Jaccard et le indice d' accord désaccord ( IAD ) . ce deux dernier nous sembler plus adapter pour discriminer le règle intéressantesdans le cas où le item être un événement peu fréquent . un applicationest réaliser sur un donnée réel issir du secteur automobile 	Une comparaison de certains indices de pertinence des règles d'association	9
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une mesure de proximité et une méthode de regroupement pour l'aide à l'acquisition d'ontologies spécialisées	Cet article traite du regroupement d'unités textuelles dans une perspectived'aide à l'élaboration d'ontologies spécialisées. Le travail présenté s'inscritdans le cadre du projet BIOTIM. Nous nous concentrons ici sur l'une desétapes de construction semi-automatique d'une ontologie qui consiste à structurerun ensemble d'unités textuelles caractéristiques en classes susceptibles dereprésenter les concepts du domaine. L'approche que nous proposons s'appuiesur la dénition d'une nouvelle mesure non-symétrique permettant d'évaluer laproximité entre lemmes, en utilisant leurs contextes d'apparition dans les documents.En complément de cette mesure, nous présentons un algorithme declassication non-supervisée adapté à la problématique et aux données traitées.Les premières expérimentations présentées sur les données botaniques laissentpercevoir des résultats pertinents pouvant être utilisés pour assister l'expert dansla détermination et la structuration des concepts du domaine.	Guillaume Cleuziou, Sylvie Billot, Stanislas Lew, Lionel Martin, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000339	http://editions-rnti.fr/render_pdf.php?p=1000339	1454	fr	fr	@univ-orleans.fr	un mesure de proximité et un méthode de regroupement pour le aide à le acquisition d' ontologie spécialisées  ce article traire du regroupement d' unité textuel dans un perspectived'aide à le élaboration d' ontologie spécialiser . le travail présenter clr inscritdans le cadre du projet BIOTIM . Nous nous concentrer ici sur le un desétapes de construction semi-automatique d' un ontologie qui consister à structurerun ensemble d' unité textuel caractéristique en classe susceptible dereprésenter le concept du domaine . le approche que nous proposer clr appuiesur le dénition d' un nouveau mesure non- symétrique permettre d' évaluer laproximité entre lemme , en utiliser son contexte d' apparition dans le document . En complément de ce mesure , nous présenter un algorithme declassication non- superviser adapter à le problématique et aux donnée traiter . le premier expérimentation présenter sur le donnée botanique laissentpercevoir des résultat pertinent pouvoir être utiliser pour assister le expert dansla détermination et le structuration des concept du domaine . 	Une mesure de proximité et une méthode de regroupement pour l'aide à l'acquisition d'ontologies spécialisées	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une nouvelle mesure sémantique pour le calcul de la similarité entre deux concepts d'une même ontologie	Les ontologies sont au coeur du processus de gestion des connaissances.Différentes mesures sémantiques ont été proposées dans la littératurepour évaluer quantitativement l'importance de la liaison sémantique entre pairesde concepts. Cet article propose une synthèse analytique des principales mesuressémantiques basées sur une ontologie modélisée par un graphe et restreinte iciaux liens hiérarchiques is-a. Après avoir mis en évidence différentes limites desmesures actuelles, nous en proposons une nouvelle, la PSS (Proportion of SharedSpecificity), qui sans corpus externe, tient compte de la densité des liens dans legraphe reliant deux concepts	Emmanuel Blanchard, Mounira Harzallah, Pascale Kuntz, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000344	http://editions-rnti.fr/render_pdf.php?p=1000344	1455	fr	fr	@univ-nantes.fr	un nouveau mesure sémantique pour le calcul de le similarité entre deux concept d' un même ontologie  le ontologie être au coeur du processus de gestion des connaissance . différent mesure sémantique avoir être proposer dans le littératurepour évaluer quantitativement le importance de le liaison sémantique entre pairesde concept . ce article proposer un synthèse analytique des principal mesuressémantiques baser sur un ontologie modéliser par un graphe et restreindre iciaux lien hiérarchique is-a . Après avoir mettre en évidence différent limite desmesures actuel , nous en proposer un nouveau , le PSS ( proportion of SharedSpecificity ) , qui sans corpus externe , tenir compte de le densité des lien dans legraphe relier deux concepts 	Une nouvelle mesure sémantique pour le calcul de la similarité entre deux concepts d'une même ontologie	2
Revue des Nouvelles Technologies de l'Information	EGC	2006	Utilisation de métadonnées pour l'aide à l'interprétation de classes et de partitions	Les résultats des méthodes de fouille de données sont difficilementinterprétables par un utilisateur n'ayant pas l'expertise requise. Dans ce papiernous proposons un outil permettant aux utilisateurs d'interpréter les résultatsissus des méthodes de classification non supervisée. Cet outil est basé sur desmétadonnées utilisées pour formaliser le processus d'interprétationautomatique. Ces métadonnées vont servir à l'utilisateur pour comprendre dansquelles circonstances les données originales ont été collectées et de quellemanière elles ont été agrégées puis classifiées. L'intérêt de ce travail porte surla souplesse qu'auront les utilisateurs à pouvoir interpréter facilement lesclasses obtenues. Nous développons notre approche basée sur l'utilisation desmétadonnées. Nous traduirons notre méthodologie par un exemple concret.	Abdourahamane Baldé, Yves Lechevallier, Brigitte Trousse, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000371	http://editions-rnti.fr/render_pdf.php?p=1000371	1456	fr	fr	@inria.fr, @inria.fr, @supelec.fr	utilisation de métadonnées pour le aide à le interprétation de classe et de partitions  le résultat des méthode de fouille de donnée être difficilementinterprétables par un utilisateur n' avoir pas le expertise requérir . Dans ce papiernous proposer un outil permettre aux utilisateur d' interpréter le résultatsissus des méthode de classification non superviser . ce outil être baser sur desmétadonnées utiliser pour formaliser le processus d' interprétationautomatique . ce métadonnées aller servir à le utilisateur pour comprendre dansquelles circonstance le donnée original avoir être collecter et de quellemanière elles avoir être agréger puis classifier . le intérêt de ce travail porter surla souplesse qu' avoir le utilisateur à pouvoir interpréter facilement lesclasses obtenir . Nous développer son approche baser sur le utilisation desmétadonnées . Nous traduire son méthodologie par un exemple concret . 	Utilisation de métadonnées pour l'aide à l'interprétation de classes et de partitions	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Utilisation des réseaux bayésiens dans le cadre de l'extraction de règles d'association	Cet article aborde le problème de l'utilisation d'un modèle de connaissancedans un contexte de fouille de données. L'approche méthodologique proposéemontre l'intérêt de la mise en oeuvre de réseaux bayésiens couplée à l'extractionde règles d'association dites delta-fortes (membre gauche minimal, fréquenceminimale et niveau de confiance contrôlé). La découverte de règles potentiellementutiles est alors facilitée par l'exploitation des connaissances décritespar l'expert et représentées dans le réseau bayésien. Cette approche estvalidée sur un cas d'application concernant la fouille de données d'interruptionsopérationnelles dans l'industrie aéronautique.	Clément Fauré, Sylvie Delprat, Alain Mille, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000407	http://editions-rnti.fr/render_pdf.php?p=1000407	1457	fr	fr	@eads.net, @liris.cnrs.fr	utilisation des réseau bayésien dans le cadre de le extraction de règle d' association  ce article aborder le problème de le utilisation d' un modèle de connaissancedans un contexte de fouille de donnée . le approche méthodologique proposéemontre le intérêt de le mise en oeuvre de réseau bayésien coupler à le extractionde règle d' association dire delta-fortes ( membre gauche minimal , fréquenceminimale et niveau de confiance contrôler ) . le découverte de règle potentiellementutiles être alors faciliter par le exploitation des connaissance décritespar le expert et représenter dans le réseau bayésien . ce approche estvalidée sur un cas d' application concernant le fouille de donnée d' interruptionsopérationnelles dans le industrie aéronautique . 	Utilisation des réseaux bayésiens dans le cadre de l'extraction de règles d'association	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Vers l'extraction de motifs rares	Un certain nombre de travaux en fouille de données se sont intéressés à l'extraction de motifs et à la génération de règles d'association à partir de ces motifs. Cependant, ces travaux se sont jusqu'à présent, centrés sur la notion de motifs fréquents. Le premier algorithme à avoir permis l'extraction de tous les motifs fréquents est Apriori mais d'autres ont été mis au point par la suite, certains n'extrayant que des sous-ensembles de ces motifs (motifs fermés fréquents, motifs fréquents maximaux, générateurs minimaux). Dans cet article, nous nous intéressons aux motifs rares qui peuvent également véhiculer des informations importantes. Les motifs rares correspondent au complémentaire des motifs fréquents. A notre connaissance, ces motifs n'ont pas encore été étudiés, malgré l'intérêt que certains domaines pourraient tirer de ce genre de modèle. C'est en particulier le cas de la médecine, où par exemple, il est important pour un praticien de repérer les symptômes non usuels ou les effets indésirables exceptionnels qui peuvent se déclarer chez un patient pour une pathologie ou un traitement donné.	Laszlo Szathmary, Sandy Maumus, Pierre Petronin, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000396	http://editions-rnti.fr/render_pdf.php?p=1000396	1458	fr	fr	@loria.fr, @nancy.inserm.fr, @gmail.com	Vers le extraction de motif rares  un certain nombre de travail en fouille de donnée clr être intéresser à le extraction de motif et à le génération de règle d' association à partir de ce motif . cependant , ce travail clr être jusqu' à présent , centrer sur le notion de motif fréquent . le premier algorithme à avoir permettre le extraction de tout le motif fréquent être Apriori mais un autre avoir être mettre au point par le suite , certains n' extraire que un sous-ensemble de ce motif ( motif fermer fréquent , motif fréquent maximal , générateur minimal ) . Dans ce article , nous clr intéresser aux motif rare qui pouvoir également véhiculer un information important . le motif rare correspondre au complémentaire des motif fréquent . A son connaissance , ce motif n' avoir pas encore être étudier , malgré le intérêt que certain domaine pouvoir tirer de ce genre de modèle . C' être en particulier le cas de le médecine , où par exemple , il être important pour un praticien de repérer le symptôme non usuel ou le effet indésirable exceptionnel qui pouvoir clr déclarer chez un patient pour un pathologie ou un traitement donner . 	Vers l'extraction de motifs rares	11
Revue des Nouvelles Technologies de l'Information	EGC	2006	Visualisation en Gestion des Connaissances Développement d'un nouveau modèle graphique Graph'Atanor	Les systèmes de gestion des connaissances servent de support pour lacréation et la diffusion de mémoires d'entreprises qui permettent de capitaliser,conserver et enrichir les connaissances des experts. Dans ces systèmes, l'interactionavec les experts est effectuée avec des outils adaptés dans lesquels uneformalisation graphique des connaissances est utilisée. Cette formalisation estsouvent basée au niveau théorique sur des modèles de graphes mais de façonpratique, les représentations visuelles sont souvent des arbres et des limitationsapparaissent par rapport aux représentations basées sur des graphes. Dans cetarticle nous présentons le modèle utilisé par le serveur de connaissances Atanorqui utilise des arbres pour visualiser les connaissances, et nous développons unenouvelle approche qui permet de représenter les mêmes connaissances sous laforme de graphes en niveaux. Une analyse comparative des deux méthodes dansun contexte industriel de maintenance permet de mettre en valeur l'apport desgraphes dans le processus de visualisation graphique des connaissances.	Bruno Pinaud, Pascale Kuntz, Fabrice Guillet, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000364	http://editions-rnti.fr/render_pdf.php?p=1000364	1459	fr	fr	@knowesia.fr, @univ-nantes.fr	visualisation en gestion des connaissance Développement d' un nouveau modèle graphique Graph'Atanor  le système de gestion des connaissance servir de support pour lacréation et le diffusion de mémoire d' entreprise qui permettre de capitaliser , conserver et enrichir le connaissance des expert . Dans ce système , le interactionavec le expert être effectuer avec un outil adapter dans lesquels uneformalisation graphique des connaissance être utiliser . ce formalisation estsouvent baser au niveau théorique sur un modèle de graphe mais de façonpratique , le représentation visuel être souvent un arbre et des limitationsapparaissent par rapport aux représentation baser sur un graphe . Dans cetarticle nous présenter le modèle utiliser par le serveur de connaissance Atanorqui utiliser un arbre pour visualiser le connaissance , et nous développer unenouvelle approche qui permettre de représenter le même connaissance sous laforme de graphe en niveau . un analyse comparatif des deux méthode dansun contexte industriel de maintenance permettre de mettre en valeur le apport desgraphes dans le processus de visualisation graphique des connaissance . 	Visualisation en Gestion des Connaissances Développement d'un nouveau modèle graphique Graph'Atanor	0
Revue des Nouvelles Technologies de l'Information	EGC	2006	Visualisation interactive de données avec des méthodes à base de points d'intérêt	Nous présentons dans cet article une méthode de visualisation interactivede données numériques ou symboliques permettant à un utilisateur expertdu domaine d'obtenir des informations et des connaissances pertinentes. Nousproposons une approche nouvelle en adaptant l'utilisation des points d'intérêtsdans un contexte de fouille visuelle de données. A partir d'un ensemble de pointsd'intérêt disposés sur un cercle, les données sont visualisées à l'intérieur de cecercle en fonction de leur similarité à ces points d'intérêt. Des opérations interactivessont alors définies : sélectionner, zoomer, changer dynamiquement lespoints d'intérêts. Nous évaluons les propriétés d'une telle visualisation sur desdonnées aux caractéristiques connues. Nous décrivons une application réelle encours dans le domaine de l'exploration de données issues d'enquêtes de satisfaction.	David Da Costa, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000367	http://editions-rnti.fr/render_pdf.php?p=1000367	1460	fr	fr	@univ-tours.fr, @univ-tours.fr, @agicom.fr	visualisation interactif de donnée avec un méthode à base de point d' intérêt  Nous présenter dans ce article un méthode de visualisation interactivede donnée numérique ou symbolique permettre à un utilisateur expertdu domaine d' obtenir un information et des connaissance pertinent . Nousproposons un approche nouveau en adapter le utilisation des point d' intérêtsdans un contexte de fouille visuel de donnée . A partir d' un ensemble de pointsd'intérêt disposer sur un cercle , le donnée être visualiser à le intérieur de cecercle en fonction de son similarité à ce point d' intérêt . un opération interactivessont alors définir : sélectionner , zoomer , changer dynamiquement lespoints d' intérêt . Nous évaluer le propriété d' un tel visualisation sur desdonnées aux caractéristique connaître . Nous décrire un application réel encours dans le domaine de le exploration de donnée issu d' enquête de satisfaction . 	Visualisation interactive de données avec des méthodes à base de points d'intérêt	3
Revue des Nouvelles Technologies de l'Information	EGC	2006	Web sémantique pour la mémoire d'expériences d'une communauté scientifique : le projet MEAT	Cet article décrit le projet MEAT (Mémoire d'Expériences pourl'Analyse du Transcriptome) dont le but est d'assister les biologistes travaillantdans le domaine des puces à ADN, pour l'interprétation et la validation de leursrésultats. Nous proposons une aide méthodologique et logicielle pour construireune mémoire d'expériences pour ce domaine. Notre approche, basée surles technologies du web sémantique, repose sur l'utilisation des ontologies etdes annotations sémantiques sur des articles scientifiques et d'autres sourcesde connaissances du domaine. Notre approche peut être généralisée à d'autresdomaines requérant des expérimentations et traitant un grand flux de données(protéomique, chimie,etc.).	Khaled Khelif, Rose Dieng-Kuntz, Pascal Barbry	http://editions-rnti.fr/render_pdf.php?p1&p=1000340	http://editions-rnti.fr/render_pdf.php?p=1000340	1461	fr	fr	@inria.fr, @ipmc.fr	web sémantique pour le mémoire d' expérience d' un communauté scientifique : le projet MEAT  ce article décrire le projet MEAT ( mémoire d' Expériences pourl'Analyse du Transcriptome ) dont le but être d' assister le biologiste travaillantdans le domaine des puce à ADN , pour le interprétation et le validation de leursrésultats . Nous proposer un aide méthodologique et logiciel pour construireune mémoire d' expérience pour ce domaine . son approche , baser surles technologie du web sémantique , reposer sur le utilisation des ontologie etdes annotation sémantique sur un article scientifique et d' autre sourcesde connaissance du domaine . son approche pouvoir être généraliser à un autresdomaines requérir un expérimentation et traiter un grand flux de donnée ( protéomique , chimie , etc. ) . 	Web sémantique pour la mémoire d'expériences d'une communauté scientifique : le projet MEAT	4
Revue des Nouvelles Technologies de l'Information	EGC	2006	Web Usage Mining : extraction de périodes denses à partir des logs	"Les techniques de Web Usage Mining existantes sont actuellementbasées sur un découpage des données arbitraire (e.g. ""un log par mois"") ou guidépar des résultats supposés (e.g. ""quels sont les comportements des clients pourla période des achats de Noël ? ""). Ces approches souffrent des deux problèmessuivants. D'une part, elles dépendent de cette organisation arbitraire des donnéesau cours du temps. D'autre part elles ne peuvent pas extraire automatiquementdes ""pics saisonniers"" dans les données stockées. Nous proposons d'exploiterles données pour découvrir de manière automatique des périodes ""denses"" decomportements. Une période sera considérée comme ""dense"" si elle contient aumoins un motif séquentiel fréquent pour l'ensemble des utilisateurs qui étaientconnectés sur le site à cette période."	Florent Masseglia, Pascal Poncelet, Maguelonne Teisseire, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000377	http://editions-rnti.fr/render_pdf.php?p=1000377	1462	fr	fr	@inria.fr, @ema.fr, @lirmm.fr	web Usage Mining : extraction de période dense à partir un logs  " le technique de Web Usage Mining existant être actuellementbasées sur un découpage des donnée arbitraire ( e.g. " " un logarithme par moi " " ) ou guidépar un résultat supposer ( e.g. " " quels être le comportement des client pourla période des achat de Noël ? " " ) . . ce approche souffrir un deux problèmessuivants . . D' un part , elles dépendre de ce organisation arbitraire des donnéesau cour du temps . . D' autre part elles ne pouvoir pas extraire automatiquementdes " " pic saisonnier " " dans le donnée stocker . . Nous proposer d' exploiterles donner pour découvrir de manière automatique des période " " dense " " decomportements . . un période être considérer comme " " dense " " si elle contenir aumoins un motif séquentiel fréquent pour le ensemble des utilisateur qui étaientconnectés sur le site à ce période . " 	Web Usage Mining : extraction de périodes denses à partir des logs	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	ACKA : Une approche d'acquisition coopérative de connaissances pour la construction d'un modèle de simulation multi-agents	Cet article présente une approche (ACKA an Approach for Cooperative Knowledge Acquisition) participative et coopérative d'acquisition de connaissances nécessaires pour la construction d'un modèle de simulation basé sur des agents. Elle est basée sur le principe de jeu de rôles dans une réunion d'entreprise. Nous proposons de construire un modèle multi-acteurs, représentant un modèle initial du système multi-agents. Dans cette étude, Nous appliquons ACKA pour construire un modèle multi-acteurs pour la compréhension des processus de décision dans les ?rmes de la ?liere avicole. En particulier, nous cherchons à comprendre les impacts des comportements individuels sur la gestion de l'utilisation des matières premières agricoles.	Athmane Hamel, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1000403	http://editions-rnti.fr/render_pdf.php?p=1000403	1570	fr	fr	@tours.inra.fr, @lamsade.dauphine.fr	ACKA : un approche d' acquisition coopératif de connaissance pour le construction d' un modèle de simulation multi-agents  ce article présenter un approche ( ACKA an Approach for Cooperative Knowledge Acquisition ) participatif et coopératif d' acquisition de connaissance nécessaire pour le construction d' un modèle de simulation baser sur un agent . Elle être baser sur le principe de jeu de rôle dans un réunion d' entreprise . Nous proposer de construire un modèle multi-acteurs , représenter un modèle initial du système multi-agents . Dans ce étude , Nous appliquer ACKA pour construire un modèle multi-acteurs pour le compréhension des processus de décision dans le ? rmes de le ? liere avicole . En particulier , nous chercher à comprendre le impact des comportement individuel sur le gestion de le utilisation des matière premier agricole . 	ACKA : Une approche d'acquisition coopérative de connaissances pour la construction d'un modèle de simulation multi-agents	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Acquisition et exploitation de connaissances dans un contexte multi-experts pour un système d'aide à la décision	Nous présentons une méthodologie d'extraction, de gestion et d'exploitation de connaissances dans un contexte multi-experts. Elle repose sur trois étapes : extraction des connaissances de chaque expert, gestion des connaissances individuelles afin de constituer une base de connaissances commune et exploitation de cette base afin de fournir une aide à la décision aux experts. La méthodologie proposée a été mise en oeuvre au Cameroun avec cinq experts en micro-finance. Elle a donné des résultats en adéquation avec les pratiques des experts. Au-delà, on envisage de mettre en oeuvre un système de capitalisation des connaissances. Il doit permettre d'analyser rapidement un plus grand nombre de situations, les experts restant en nombre limité, et contribuer à un transfert de compétences pour former les décideurs locaux. En effet, les experts sont en général membres d'ONG et restent rarement plus de deux ans sur place.	Jean-Pierre Barthélemy, Jean-Robert Kala Kamdjoug, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1000220	http://editions-rnti.fr/render_pdf.php?p=1000220	1571	fr	fr	@enst-bretagne.fr	acquisition et exploitation de connaissance dans un contexte multi-experts pour un système d' aide à le décision  Nous présenter un méthodologie d' extraction , de gestion et d' exploitation de connaissance dans un contexte multi-experts . Elle reposer sur trois étape : extraction des connaissance de chaque expert , gestion des connaissance individuel afin de constituer un base de connaissance commun et exploitation de ce base afin de fournir un aide à le décision aux expert . le méthodologie proposer avoir être mettre en oeuvre au Cameroun avec cinq expert en micro- finance . Elle avoir donner un résultat en adéquation avec le pratique des expert . au-delà , on envisager de mettre en oeuvre un système de capitalisation des connaissance . Il devoir permettre d' analyser rapidement un plus grand nombre de situation , le expert rester en nombre limiter , et contribuer à un transfert de compétence pour former le décideur local . En effet , le expert être en général membre d' ONG et rester rarement plus de deux an sur place . 	Acquisition et exploitation de connaissances dans un contexte multi-experts pour un système d'aide à la décision	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	AID : Un framework intégré de conception d'un schéma objet-relationnel	Devant la prolifération des données complexes qui ne cessent de croître, et la diversité des structures qui se multiplient, la conception des schémas de base de données en général et des schémas objet-relationnels en particulier, est devenue une activité difficile et complexe, qui fait appel à des connaissances variées. Lors de la conception d'un schéma, l'utilisateur (non averti) doit connaître la théorie sous-jacente au modèle de données, de façon à énoncer son modèle, syntaxiquement correct lui permettant de construire un schéma de base de données objet-relationnel répondant à ses besoins. Plusieurs outils spécialisés dans la conception de schémas de base de données provenant aussi bien de la communauté académique que du monde industriel, tels Super, Totem, Rational/Rose, etc. ont été développés dans des contextes et avec des buts souvent très différents. Affin de répondre à ce besoin pressant, nous avons proposé une solution consistant en l'élaboration d'environnements intégrés facilitant la cohabitation de plusieurs modèles et techniques utilisés lors de la conception d'un schéma de base de données. Il s'agit d'offrir une plate-forme logicielle appelée AID (Aided Interface for Database design) offrant des mécanismes opératoires uniformes représentant un soutien graphique et interactif pour une conception incrémentale basée sur des manipulations directes et systémiques des graphes au travers d'une palette graphique d'opérateurs. L'innovation d'AID est son approche systémique qui facilite l'expression des besoins par le concepteur averti ou non, en lui automatisant sa tâche.	Etienne Pichat, Hassan Badir	http://editions-rnti.fr/render_pdf.php?p1&p=1000292	http://editions-rnti.fr/render_pdf.php?p=1000292	1572	fr	fr	@liris.cnrs.fr	AID : un framework intégrer de conception d' un schéma objet-relationnel  Devant le prolifération des donnée complexe qui ne cesser de croître , et le diversité des structure qui clr multiplier , le conception des schéma de base de donnée en général et des schéma objet-relationnels en particulier , être devenir un activité difficile et complexe , qui faire appel à un connaissance varier . lors de le conception d' un schéma , le utilisateur ( non averti ) devoir connaître le théorie sous-jacent au modèle de donnée , de façon à énoncer son modèle , syntaxiquement correct lui permettre de construire un schéma de base de donnée objet-relationnel répondre à son besoin . plusieurs outil spécialiser dans le conception de schéma de base de donnée provenir aussi bien de le communauté académique que du monde industriel , tel Super , Totem , Rational  Rose , etc. avoir être développer dans un contexte et avec un but souvent très différent . affin de répondre à ce besoin pressant , nous avoir proposer un solution consister en le élaboration d' environnement intégrer faciliter le cohabitation de plusieurs modèle et technique utiliser lors de le conception d' un schéma de base de donnée . Il clr agir d' offrir un plate-forme logiciel appeler AID ( Aided Interface for Database design ) offrir un mécanisme opératoire uniforme représenter un soutien graphique et interactif pour un conception incrémentale baser sur un manipulation direct et systémique des graphe au travers d' un palette graphique d' opérateur . le innovation d' AID être son approche systémique qui faciliter le expression des besoin par le concepteur averti ou non , en lui automatiser son tâche . 	AID : Un framework intégré de conception d'un schéma objet-relationnel	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Amélioration de la performance de l'Analyse de la Sémantique Latente pour des corpus de petite taille		Fadoua Ataa-Allah, Abderrahim El Qadi, Siham Boulaknadel, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1000278	http://editions-rnti.fr/render_pdf.php?p=1000278	1573	fr		@yahoo.fr, @fsr.ac.ma, @estm.ac.ma	Amélioration de la performance de l'Analyse de la Sémantique Latente pour des corpus de petite taille 	Amélioration de la performance de l'Analyse de la Sémantique Latente pour des corpus de petite taille	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse comparative de classifications : apport des règles d'association floues	Notre travail s'appuie sur l'analyse d'un corpus bibliographique dans le domaine de la géotechnique à l'aide de cartes réalisées avec la plateforme Stanalyst®. Celui-ci intègre un algorithme de classification automatique non hiérarchique (les K-means axiales) donnant des résultats dépendant du nombre de classes demandé. Cette instabilité rend difficile toute comparaison entre classifications, et laisse un doute quant au choix du nombre de classes nécessaire pour représenter correctement un domaine. Nous comparons les résultats de classifications selon 3 protocoles : (1) analyse des intitulés des classes ; (2) relations entre les classes à partir des membres communs ; (3) règles d'association floues. Les graphes obtenus présentant des similitudes remarquables, nous privilégions les règles d'association floues : elles sont extraites automatiquement et se basent sur la description des classes et non des membres. Ceci nous permet donc d'analyser des classifications issues de corpus différents.	Pascal Cuxac, Martine Cadot, Claire François	http://editions-rnti.fr/render_pdf.php?p1&p=1000359	http://editions-rnti.fr/render_pdf.php?p=1000359	1574	fr	fr	@inist.fr, @inist.fr, @loria.fr	analyse comparatif de classification : apport des règle d' association floues  son travail clr appuyer sur le analyse d' un corpus bibliographique dans le domaine de le géotechnique à le aide de carte réaliser avec le plateforme Stanalyst®. celui _-ci intégrer un algorithme de classification automatique non hiérarchique ( le K-means axial ) donner un résultat dépendant du nombre de classe demander . ce instabilité rendre difficile tout comparaison entre classification , et laisser un doute quant au choix du nombre de classe nécessaire pour représenter correctement un domaine . Nous comparer le résultat de classification selon 3 protocole : ( 1 ) analyse des intitulé des classe ; ( 2 ) relation entre le classe à partir un membre commun ; ( 3 ) règle d' association flou . le graphe obtenir présenter un similitude remarquable , nous privilégier le règle d' association flou : elles être extraire automatiquement et clr baser sur le description des classe et non des membre . ceci nous permettre donc d' analyser un classification issu de corpus différent . 	Analyse comparative de classifications : apport des règles d'association floues	6
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse de données symboliques et graphe de connaissances d'un agent	Dans cet article nous appliquons l'analyse de données symboliques au graphe de connaissances d'un agent. Nous présentons une mesure de similarité entre des données symboliques adaptée à nos graphes de connaissances. Nous utilisons les pyramides symboliques pour extraire un nouvel objet symbolique. Le nouvel objet est ensuite réinséré dans le graphe où il peut être utilisé par l'agent, faisant ainsi évoluer sa sémantique. Il peut alors servir d'individu lors des analyses ultérieures, permettant de découvrir de nouveaux concepts prenant en compte l'évolution de la sémantique.	Philippe Caillou, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000408	http://editions-rnti.fr/render_pdf.php?p=1000408	1575	fr	fr	@lamsade.dauphine.fr, @ceremade.dauphine.fr	analyse de donnée symbolique et graphe de connaissance d' un agent  Dans ce article nous appliquer le analyse de donnée symbolique au graphe de connaissance d' un agent . Nous présenter un mesure de similarité entre un donnée symbolique adapter à son graphe de connaissance . Nous utiliser le pyramide symbolique pour extraire un nouveau objet symbolique . le nouveau objet être ensuite réinsérer dans le graphe où il pouvoir être utiliser par le agent , faire ainsi évoluer son sémantique . Il pouvoir alors servir d' individu lors des analyse ultérieur , permettre de découvrir un nouveau concept prendre en compte le évolution de le sémantique . 	Analyse de données symboliques et graphe de connaissances d'un agent	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse géométrique des données pour l'affinement de la connaissance : cas des données EPGY (Education Program for Gifted Students, Stanford University)		Brigitte Le Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1000256	http://editions-rnti.fr/render_pdf.php?p=1000256	1576	fr		@math-info.univ	Analyse géométrique des données pour l'affinement de la connaissance : cas des données EPGY (Education Program for Gifted Students, Stanford University) 	Analyse géométrique des données pour l'affinement de la connaissance : cas des données EPGY (Education Program for Gifted Students, Stanford University)	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse stochastique de séquences d'événements discrets pour la découverte de signatures	"Cet article concerne la découverte de signatures (ou modèles de chroniques) à partir d'une séquence d'événements discrets (alarmes) générée par un agent cognitif de surveillance (Monitoring Cognitive Agent ou MCA).Considérant un couple (Processus, MCA) comme un générateur stochastique d'événements discrets, deux représentations complémentaires permettent de caractériser les propriétés stochastiques et temporelles d'un tel générateur : une chaîne de Markov à temps continu et une superposition de processus de Poisson. L'étude de ces deux représentations duales permet de découvrir des ""signatures"" décrivant les relations stochastiques et temporelles entre événements dans une séquence. Ces signatures peuvent alors être utilisées pour reconnaître des comportements spécifiques, comme le montre l'application de l'approche à un outil de production industriel piloté par un système Sachem, le MCA développé et utilisé par le groupe Arcelor pour aider au pilotage de ses outils de production."	Philippe Bouché, Marc Le Goc	http://editions-rnti.fr/render_pdf.php?p1&p=1000219	http://editions-rnti.fr/render_pdf.php?p=1000219	1577	fr	fr	@lsis.org, @lsis.org	analyse stochastique de séquence d' événement discret pour le découverte de signatures  " ce article concerner le découverte de signature ( ou modèle de chronique ) à partir d' un séquence d' événement discret ( alarme ) générer par un agent cognitif de surveillance ( Monitoring Cognitive Agent ou MCA ) . . Considérant un couple ( Processus , MCA ) comme un générateur stochastique d' événement discret , deux représentation complémentaire permettre de caractériser le propriété stochastiques et temporel d' un tel générateur : un chaîne de Markov à temps continu et un superposition de processus de Poisson . . le étude de ce deux représentation dual permettre de découvrir un " " signature " " décrire le relation stochastiques et temporel entre événement dans un séquence . . ce signature pouvoir alors être utiliser pour reconnaître un comportement spécifique , comme le montre le application de le approche à un outil de production industriel piloter par un système Sachem , le MCA développer et utiliser par le groupe Arcelor pour aider au pilotage de son outil de production . " 	Analyse stochastique de séquences d'événements discrets pour la découverte de signatures	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Annotation de textes par extraction d'informations lexicosyntaxiques  et acquisition de schémas conceptuels de causalité	Nous présentons la méthode INSYSE (INterface SYntaxe SEmantique) pour l'annotation de documents textuels. Notre objectif est de construire des annotations sémantiques de ces résumés pour interroger le corpus sur la fonction des gènes et leurs relations de causalité avec certaines maladies. Notre approche est semi-automatique, centrée sur (1) l'extraction d'informations lexico-syntaxiques à partir de certaines phrases du corpus comportant des lexèmes de causation, et (2) l'élaboration de règles basées sur des grammaires d'unification permettant d'acquérir à partir de ces informations des schémas conceptuels instanciés. Ceux-ci sont traduits en annotations RDF(S) sur la base desquelles le corpus de textes peut être interrogé avec le moteur de recherche sémantique Corese.	Laurent Alamarguy, Rose Dieng-Kuntz, Catherine Faron-Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000258	http://editions-rnti.fr/render_pdf.php?p=1000258	1578	fr	fr	@inria.fr, @essi.fr	annotation de texte par extraction d' information lexicosyntaxiques et acquisition de schéma conceptuel de causalité  Nous présenter le méthode INSYSE ( INterface SYntaxe SEmantique ) pour le annotation de document textuel . son objectif être de construire un annotation sémantique de ce résumé pour interroger le corpus sur le fonction des gène et son relation de causalité avec certain maladie . son approche être semi-automatique , centrer sur ( 1 ) le extraction d' information lexico- syntaxique à partir de certain phrase du corpus comporter un lexème de causation , et ( 2 ) le élaboration de règle baser sur un grammaire d' unification permettre d' acquérir à partir de ce information des schéma conceptuel instanciés . celui _-ci être traduire en annotation RDF ( S ) sur le base desquelles le corpus de texte pouvoir être interroger avec le moteur de recherche sémantique Corese . 	Annotation de textes par extraction d'informations lexicosyntaxiques  et acquisition de schémas conceptuels de causalité	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage automatique des modèles structurels d'objets cartographiques	Pour reconnaitre les objets cartographiques dans les images satellitales on a besoin d'un modèle d'objet qu'on recherche. Nous avons développé un système d'apprentissage qui construit le modèle structurel d'objets cartographiques automatiquement a partir des images satellitales segmentées. Les images contenants les objets sont décomposées en formes primitives et sont transformées en Graphes Relationnels Attribués (ARGs). Nous avons généré les modèles d'objets a partir de ces graphes, en utilisant des algorithmes d'appariement de graphes. La qualité d'un modèle est évaluée par la distance d'édition des exemples a ce modèle. Nous sommes parvenus a obtenir des modèles de ponts et de ronds-points qui sont compatibles avec les modèles construits manuellement.	Güray Erus, Nicolas Loménie	http://editions-rnti.fr/render_pdf.php?p1&p=1000336	http://editions-rnti.fr/render_pdf.php?p=1000336	1579	fr	fr	@univ-paris5.fr	apprentissage automatique des modèle structurel d' objet cartographiques  Pour reconnaitre le objet cartographique dans le image satellitales on avoir besoin d' un modèle d' objet qu' on rechercher . Nous avoir développer un système d' apprentissage qui construire le modèle structurel d' objet cartographique automatiquement avoir partir un image satellitales segmenter . le image contenant le objet être décomposer en forme primitif et être transformer en Graphes Relationnels Attribués ( ARGs ) . Nous avoir générer le modèle d' objet avoir partir de ce graphe , en utiliser un algorithme d' appariement de graphe . le qualité d' un modèle être évaluer par le distance d' édition des exemple avoir ce modèle . Nous sommer parvenir avoir obtenir un modèle de pont et de rond-point qui être compatible avec le modèle construire manuellement . 	Apprentissage automatique des modèles structurels d'objets cartographiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de scénarios à partir de séries temporelles multivariées		Thomas Guyet, Catherine Garbay, Michel Dojat	http://editions-rnti.fr/render_pdf.php?p1&p=1000224	http://editions-rnti.fr/render_pdf.php?p=1000224	1580	fr		@imag.fr, @ujf-grenoble.fr	Apprentissage de scénarios à partir de séries temporelles multivariées 	Apprentissage de scénarios à partir de séries temporelles multivariées	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de signatures de facteurs de transcription à partir de données d'expression	L'inférence de signatures de facteurs de transcription à partir des données puces à ADN a déjà été étudié dans la communauté bioinformatique. La principale difficulté à résoudre est de trouver un ensemble d'heuristiques pertinentes, afin de contrôler la complexité de résolution de ce problème NP-difficile. Nous proposons dans cet article une solution heuristique alternative à celles utilisées dans les approches bayésiennes, fondée sur la recherche de motifs fréquents maximaux dans une matrice discrétisée issue des données numériques de puces ADN. Notre méthode est appliquée sur des données de cancer de vessie de l'Institut Curie et de l'Hôpital Henri Mondor de Créteil.	Mohamed Elati, Céline Rouveirol, François Radvanyi	http://editions-rnti.fr/render_pdf.php?p1&p=1000416	http://editions-rnti.fr/render_pdf.php?p=1000416	1581	fr	fr	@lri.fr, @curie.fr	apprentissage de signature de facteur de transcription à partir de donnée d' expression  le inférence de signature de facteur de transcription à partir un donnée puce à ADN avoir déjà être étudier dans le communauté bioinformatique . le principal difficulté à résoudre être de trouver un ensemble d' heuristique pertinent , afin de contrôler le complexité de résolution de ce problème NP-difficile . Nous proposer dans ce article un solution heuristique alternatif à celui utiliser dans le approche bayésien , fonder sur le recherche de motif fréquent maximal dans un matrice discrétiser issue des donnée numérique de puce ADN . son méthode être appliquer sur un donnée de cancer de vessie de le Institut Curie et de le hôpital Henri Mondor de Créteil . 	Apprentissage de signatures de facteurs de transcription à partir de données d'expression	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de structures de réseaux Bayésiens et données incomplètes	"Le formalisme des modèles graphiques connait actuellement un essor dans les domaines du ""machine learning"". En particulier, les réseaux bayésiens sont capables d'effectuer des raisonnements probabilistes à partir de données incomplètes alors que peu de méthodes sont actuellement capables d'utiliser les bases d'exemples incomplètes pour leur apprentissage. En s'inspirant du principe de AMS-EM proposé par (Friedman, 1997) et des travaux de(Chow & Liu, 1968), nous proposons une méthode permettant de faire l'apprentissage de réseaux bayésiens particuliers, de structure arborescente, à partir de données incomplètes. Une étude expérimentale expose ensuite des résultats préliminaires qu'il est possible d'attendre d'une telle méthode, puis montre le gain potentiel apporté lorsque nous utilisons les arbres obtenus comme initialisation d'une méthode de recherche gloutonne comme AMS-EM."	Olivier François, Philippe Leray	http://editions-rnti.fr/render_pdf.php?p1&p=1000222	http://editions-rnti.fr/render_pdf.php?p=1000222	1582	fr	fr	@insa-rouen.fr	apprentissage de structure de réseau Bayésiens et donnée incomplètes  " le formalisme des modèle graphique connait actuellement un essor dans le domaine du " " machine learning " " . . En particulier , le réseau bayésien être capable d' effectuer un raisonnement probabiliste à partir de donnée incomplet alors que peu de méthode être actuellement capable d' utiliser le base d' exemple incomplet pour son apprentissage . . En clr inspirer du principe de AMS-EM proposer par ( Friedman , 1997 ) et des travail de ( Chow & Liu , 1968 ) , nous proposer un méthode permettre de faire le apprentissage de réseau bayésien particulier , de structure arborescent , à partir de donnée incomplet . . un étude expérimental exposer ensuite un résultat préliminaire qu' il être possible d' attendre d' un tel méthode , puis montrer le gain potentiel apporter lorsque nous utiliser le arbre obtenir comme initialisation d' un méthode de recherche glouton comme AMS-EM. " 	Apprentissage de structures de réseaux Bayésiens et données incomplètes	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage non supervisé de séries temporelles à l'aide des k-Means et d'une nouvelle méthode d'agrégation de séries	"L'utilisation d'un algorithme d'apprentissage non supervisé de type k-Means sur un jeu de séries temporelles amène à se poser deux questions : Celle du choix d'une mesure de similarité et celle du choix d'une méthode effectuant l'agrégation de plusieurs séries afin d'en estimer le centre (i.e. calculer les k moyennes). Afin de répondre à la première question, nous présentons dans cet article les principales mesures de similarité existantes puis nous expliquons pourquoi l'une d'entre elles (appelée Dynamic Time Warping) nous paraît la plus adaptée à l'apprentissage non supervisé. La deuxième question pose alors problème car nous avons besoin d'une méthode d'agrégation respectant les caractéristiques bien particulières du Dynamic Time Warping. Nous pensons que l'association de cette mesure de similarité avec l'agrégation Euclidienne peut générer une perte d'informations importante dans le cadre d'un apprentissage sur la ""forme"" des séries. Nous proposons donc une méthode originale d'agrégation de séries temporelles, compatible avec le Dynamic Time Warping, qui améliore ainsi les résultats obtenus à l'aide de l'algorithme des k-Means."	Nicolas Nicoloyannis, Rémi Gaudin	http://editions-rnti.fr/render_pdf.php?p1&p=1000250	http://editions-rnti.fr/render_pdf.php?p=1000250	1583	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	apprentissage non superviser de série temporel à le aide des k-Means et d' un nouveau méthode d' agrégation de séries  " le utilisation d' un algorithme d' apprentissage non superviser de type k-Means sur un jeu de série temporel amener à clr poser deux question : : Celle du choix d' un mesure de similarité et celui du choix d' un méthode effectuer le agrégation de plusieurs série afin d' en estimer le centre ( id est calculer le k moyen ) . . Afin de répondre à le premier question , nous présenter dans ce article le principal mesure de similarité existant puis nous expliquer pourquoi le un d' entre lui ( appeler Dynamic Time Warping ) nous paraître le plus adapter à le apprentissage non superviser . . le deuxième question poser alors problème car nous avoir besoin d' un méthode d' agrégation respecter le caractéristique bien particulier du Dynamic Time Warping . . Nous penser que le association de ce mesure de similarité avec le agrégation Euclidienne pouvoir générer un perte d' information important dans le cadre d' un apprentissage sur le " " forme " " des série . . Nous proposer donc un méthode original d' agrégation de série temporel , compatible avec le Dynamic Time Warping , qui améliorer ainsi le résultat obtenir à le aide de le algorithme des k-Means . " 	Apprentissage non supervisé de séries temporelles à l'aide des k-Means et d'une nouvelle méthode d'agrégation de séries	5
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage supervisé pour la classification des images basé sur la structure P-tree	Un problème important de la production automatique de règles de classification concerne la durée de génération de ces règles ; en effet, les algorithmes mis en oeuvre produisent souvent des règles pendant un certain temps assez long. Nous proposons une nouvelle méthode de classification à partir d'une base de données images. Cette méthode se situe à la jonction de deux techniques : l'algèbre de P-tree et l'arbre de décision en vue d'accélérer le processus de classification et de recherche dans de grandes bases d'images. La modélisation que nous proposons se base, d'une part, sur les descripteurs visuels tels que la couleur, la forme et la texture dans le but d'indexer les images et, d'autre part, sur la génération automatique des règles de classification à l'aide d'un nouvel algorithme C4.5(P-tree). Pour valider notre méthode, nous avons développé un système baptisé C.I.A.D.P-tree qui a été implémenté et confronté à une application réelle dans le domaine du traitement d'images. Les résultats expérimentaux montrent que cette méthode réduit efficacement le temps de classification.	Rim Faiz, Najeh Naffakhi, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1000343	http://editions-rnti.fr/render_pdf.php?p=1000343	1584	fr	fr	@ihec.rnu.tn, @ihec.rnu.tn, @isg.rnu.tn	apprentissage superviser pour le classification des image baser sur le structure P-tree  un problème important de le production automatique de règle de classification concerner le durée de génération de ce règle ; en effet , le algorithme mettre en oeuvre produire souvent un règle pendant un certain temps assez long . Nous proposer un nouveau méthode de classification à partir d' un base de donnée image . ce méthode clr situer à le jonction de deux technique : le algèbre de P-tree et le arbre de décision en vue d' accélérer le processus de classification et de recherche dans un grand base d' image . le modélisation que nous proposer clr baser , d' un part , sur le descripteur visuel tel que le couleur , le forme et le texture dans le but d' indexer le image et , d' autre part , sur le génération automatique des règle de classification à le aide d' un nouveau algorithme C4.5 ( P-tree ) . Pour valider son méthode , nous avoir développer un système baptiser C.I.A.D.P-tree qui avoir être implémenter et confronter à un application réel dans le domaine du traitement d' image . le résultat expérimental montrer que ce méthode réduire efficacement le temps de classification . 	Apprentissage supervisé pour la classification des images basé sur la structure P-tree	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Arbre de décision sur des données de type intervalle : évaluation et comparaison	Le critère de découpage binaire de Kolmogorov-Smirnov nécessite un ordre total des valeurs prises par les variables explicatives. Nous pouvons ordonner des intervalles fermés bornés de nombres réels de différentes façons. Notre contribution dans cet article consiste à évaluer et à comparer des arbres de décision obtenus sur des données de type intervalle à l'aide du critère de découpage binaire de Kolmogorov-Smirnov étendu à ce type de données (Mballo et al. 2004). Pour ce faire, nous axons notre attention sur le taux d'erreur mesuré sur l'échantillon de test. Pour estimer ce paramètre, nous divisons aléatoirement chaque base de données en deux parties égales en terme d'effectif (à un objet près) pour construire deux arbres. Ces deux arbres sont d'abord testés par un même échantillon puis par deux échantillons différents.	Chérif Mballo, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000215	http://editions-rnti.fr/render_pdf.php?p=1000215	1585	fr	fr	@esiea-ouest.fr, @ceremade.dauphine.fr	arbre de décision sur un donnée de type intervalle : évaluation et comparaison  le critère de découpage binaire de Kolmogorov-Smirnov nécessiter un ordre total des valeur prendre par le variable explicatives . Nous pouvoir ordonner un intervalle fermer borner de nombre réel de différent façon . son contribution dans ce article consister à évaluer et à comparer un arbre de décision obtenir sur un donnée de type intervalle à le aide du critère de découpage binaire de Kolmogorov-Smirnov étendre à ce type de donnée ( Mballo et al. 2004 ) . Pour ce faire , nous axer son attention sur le taux d' erreur mesurer sur le échantillon de test . Pour estimer ce paramètre , nous diviser aléatoirement chaque base de donnée en deux partie égal en terme d' effectif ( à un objet près ) pour construire deux arbre . ce deux arbre être d' abord tester par un même échantillon puis par deux échantillon différent . 	Arbre de décision sur des données de type intervalle : évaluation et comparaison	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Caractérisation d'une région d'intérêt dans les images	Une image est un support d'information qui a montré son efficacité. Néanmoins une image comporte souvent plusieurs zones, l'arrière plan et une zone d'intérêt privilégiée. La vision humaine permet la segmentation de manière naturelle et intégrant toute la connaissance que le sujet peut avoir de l'objectif visé par l'image. Nous proposons ici une méthode de détermination des régions d'intérêt d'une image numérique comme zones saillantes. Les lois de Zipf et Zipf inverse sont adaptées au traitement des images et permettent d'évaluer la complexité structurelle d'une image. Une comparaison des modèles locaux évalués sur des imagettes permet de mettre en évidence une région de l'image. Deux méthodes de classification ont été utilisées pour la détermination de la région d'intérêt : la partition d'un nuage de points représentant les caractéristiques associées aux imagettes, et les réseaux de neurones. Cette méthode de détection permet d'obtenir des zones d'intérêt conformes à la perception humaine. On opère une hiérarchisation sur les zones en fonction de la structuration de l'information élémentaire, les pixels.	Yves Caron, Pascal Makris, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000338	http://editions-rnti.fr/render_pdf.php?p=1000338	1586	fr	fr	@univ-tours.fr, @univ-tours.fr, @univ-paris5.fr	caractérisation d' un région d' intérêt dans le images  un image être un support d' information qui avoir montrer son efficacité . néanmoins un image comporter souvent plusieurs zone , le arrière plan et un zone d' intérêt privilégier . le vision humain permettre le segmentation de manière naturel et intégrer tout le connaissance que le sujet pouvoir avoir de le objectif viser par le image . Nous proposer ici un méthode de détermination des région d' intérêt d' un image numérique comme zone saillant . le loi de Zipf et Zipf inverse être adapter au traitement des image et permettre d' évaluer le complexité structurel d' un image . un comparaison des modèle local évaluer sur un imagettes permettre de mettre en évidence un région de le image . Deux méthode de classification avoir être utiliser pour le détermination de le région d' intérêt : le partition d' un nuage de point représenter le caractéristique associer aux imagettes , et le réseau de neurone . ce méthode de détection permettre d' obtenir un zone d' intérêt conforme à le perception humain . On opérer un hiérarchisation sur le zone en fonction de le structuration de le information élémentaire , le pixel . 	Caractérisation d'une région d'intérêt dans les images	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	CHIC : traitement de données avec l'analyse implicative	Cet article a pour but de montrer les possibilités offertes par le logiciel CHIC (Classification Hiérarchique Implicative et Cohésitive) pour effectuer certaines analyses de données. Il est basé sur la théorie de l'Analyse Statistique Implicative ou A.S.I. développée par Régis Gras et ses collaborateurs. Le principe premier de l'A.S.I. repose sur la problématique d'une mesure des règles d'association du type : «si a alors b» dans une population instanciant les variables a et b. CHIC enrichit sa réponse, établie sur des bases statistiques, en évaluant la responsabilité des sujets dans l'élection de la règle. L'article présent explique la démarche à suivre pour utiliser le logiciel ainsi que les possibilités offertes par celui-ci.	Raphaël Couturier, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000423	http://editions-rnti.fr/render_pdf.php?p=1000423	1587	fr	fr	@univ-fcomte.fr, @club-internet.fr	chic : traitement de donnée avec le analyse implicative  ce article avoir pour but de montrer le possibilité offrir par le logiciel CHIC ( Classification Hiérarchique Implicative et Cohésitive ) pour effectuer certain analyse de donnée . Il être baser sur le théorie de le analyse Statistique Implicative ou A.S.I. développer par Régis Gras et son collaborateur . le principe premier de le A.S.I. reposer sur le problématique d' un mesure des règle d' association du type : « si avoir alors b » dans un population instanciant le variable avoir et b . chic enrichir son réponse , établir sur un base statistique , en évaluer le responsabilité des sujet dans le élection de le règle . le article présent expliquer le démarche à suivre pour utiliser le logiciel ainsi que le possibilité offrir par celui _-ci . 	CHIC : traitement de données avec l'analyse implicative	33
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification 2-3 Hiéarchique de données du Web		Sergiu Theodor Chelcea, Brigitte Trousse	http://editions-rnti.fr/render_pdf.php?p1&p=1000253	http://editions-rnti.fr/render_pdf.php?p=1000253	1588	fr		@inria.fr	Classification 2-3 Hiéarchique de données du Web 	Classification 2-3 Hiéarchique de données du Web	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification d'un tableau de contingence et modèle probabiliste	Les modèles de mélange, qui supposent que l'échantillon est formé de sous-populations caractérisées par une distribution de probabilité, constitue un support théorique intéressant pour étudier la classification automatique. On peut ainsi montrer que l'algorithme des k-means peut être vu comme une version classifiante de l'algorithme d'estimation EM dans un cas particulièrement simple de mélange de lois normales. Lorsque l'on cherche à classifier les lignes (ou les colonnes) d'un tableau de contingence, il est possible d'utiliser une variante de l'algorithme des k-means, appelé Mndki2, en s'appuyant sur la notion de profil et sur la distance du khi-2. On obtient ainsi une méthode simple et efficace pouvant s'utiliser conjointement à l'analyse factorielle des correspondances qui s'appuie sur la même représentation des données. Malheureusement et contrairement à l'algorithme des k-means classique, les liens qui existent entre les modèles de mélange et la classification ne s'appliquent pas directement à cette situation. Dans ce travail, nous montrons que l'algorithme Mndki2 peut être associé, à une approximation près, à un modèle de mélange de lois multinomiales.	Gérard Govaert, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000251	http://editions-rnti.fr/render_pdf.php?p=1000251	1589	fr	fr	@utc.fr, @univ-metz.fr	classification d' un tableau de contingence et modèle probabiliste  le modèle de mélange , qui supposer que le échantillon être former de sous-populations caractériser par un distribution de probabilité , constituer un support théorique intéressant pour étudier le classification automatique . On pouvoir ainsi montrer que le algorithme des k-means pouvoir être voir comme un version classifiante de le algorithme d' estimation EM dans un cas particulièrement simple de mélange de loi normal . Lorsque le on chercher à classifier le ligne ( ou le colonne ) d' un tableau de contingence , il être possible d' utiliser un variante de le algorithme des k-means , appeler Mndki2 , en clr appuyer sur le notion de profil et sur le distance du khi- 2 . On obtenir ainsi un méthode simple et efficace pouvoir clr utiliser conjointement à le analyse factoriel des correspondance qui clr appuyer sur le même représentation des donnée . malheureusement et contrairement à le algorithme des k-means classique , le lien qui exister entre le modèle de mélange et le classification ne clr appliquer pas directement à ce situation . Dans ce travail , nous montrer que le algorithme Mndki2 pouvoir être associer , à un approximation près , à un modèle de mélange de loi multinomiales . 	Classification d'un tableau de contingence et modèle probabiliste	7
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification non supervisée et visualisation 3D de documents	Le nombre de documents issus d'une requête sur le Web devient de plus en plus important. Cela nous amène à chercher des solutions pour aider l'utilisateur qui est confronté à cette masse de données. Une alternative possible à un affichage linéaire non triée selon un critère, consiste à effectuer une classification des résultats. C'est dans ce but que l'on s'intéresse aux cartes auto-organisatrices de Kohonen qui sont issues d'un d'algorithme de classification non supervisée. Cependant, il faut ajouter des contraintes à cet algorithme afin qu'il soit adapté à la classification des résultats d'une requête. Par exemple, il doit être déterministe. De plus la classification obtenue dépend fortement de la distance utilisée pour comparer deux documents. On évalue alors l'impact de différentes distances ou dissimilarités, afin de trouver la plus adaptée à notre problème. Un compromis doit également être trouvé entre le temps d'exécution de l'algorithme et la qualité de la classification obtenue. Pour cela, l'utilisation d'un échantillonnage est envisagée. Enfin, ces travaux sont intégrés dans un prototype qui permet de visualiser les résultats en trois dimensions et d'interagir avec eux.	Nicolas Bonnel, Annie Morin, Alexandre Cotarmanac'h	http://editions-rnti.fr/render_pdf.php?p1&p=1000379	http://editions-rnti.fr/render_pdf.php?p=1000379	1590	fr	fr		classification non superviser et visualisation 3D de documents  le nombre de document issir d' un requête sur le Web devenir de plus en plus important . cela nous amener à chercher un solution pour aider le utilisateur qui être confronter à ce masse de donnée . un alternative possible à un affichage linéaire non trier selon un critère , consister à effectuer un classification des résultat . C' être dans ce but que le on clr intéresser aux carte auto- organisatrice de Kohonen qui être issir d' un d' algorithme de classification non superviser . cependant , il faillir ajouter un contrainte à ce algorithme afin qu' il être adapter à le classification des résultat d' un requête . Par exemple , il devoir être déterministe . De plus le classification obtenir dépendre fortement de le distance utiliser pour comparer deux document . On évaluer alors le impact de différent distance ou dissimilarités , afin de trouver le plus adapter à son problème . un compromis devoir également être trouver entre le temps d' exécution de le algorithme et le qualité de le classification obtenir . Pour cela , le utilisation d' un échantillonnage être envisager . enfin , ce travail être intégrer dans un prototype qui permettre de visualiser le résultat en trois dimension et d' interagir avec lui . 	Classification non supervisée et visualisation 3D de documents	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classifying XML Materialized Views for their maintenance on distributed Web sources	Ces dernières années ont mis en évidence la croissance et la diversité des informations électroniques accessibles sur le web. C'est ainsi que les systèmes d'intégration de données tels que des médiateurs ont été conçus pour intégrer ces données distribuées et hétérogènes dans une vue uniforme. Pour faciliter l'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'interrogation pour XML qui s'est imposé pour les systèmes basés sur XML. Ainsi XQuery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources. Pour optimiser l'évaluation de requêtes, les vues sont matérialisées lors de la mise à jour des sources, car dans le contexte de sources web, très peu d'informations sont fournies par les sources. Les méthodes habituellement proposées ne peuvent pas être appliquées. Cet article étudie comment mettre à jour des vues matérialisées XML sur des sources web, au sein d'une architecture de médiation.	Tuyet-Tram Dang-Ngoc, Virginie Sans, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000331	http://editions-rnti.fr/render_pdf.php?p=1000331	1591	en	fr	@dept-info.u	ce dernier année avoir mettre en évidence le croissance et le diversité des information électronique accessible sur le web . C' être ainsi que le système d' intégration de donnée tel que un médiateur avoir être concevoir pour intégrer ce donnée distribuer et hétérogène dans un vue uniforme . Pour faciliter le intégration des donnée à travers différent système , XML avoir être adopter comme format standard pour échanger un information . XQuery être un langage d' intégration des donnée à travers différent système , XML avoir être adopter comme format standard pour échanger un information . XQuery être un langage d' interrogation pour XML qui clr être imposer pour le système baser sur XML . ainsi XQuery être employer sur un système de médiation pour concevoir un vue définir sur plusieurs source . Pour optimiser le évaluation de requête , le vue être matérialiser lors de le mise à jour des source , car dans le contexte de source web , très peu d' information être fournir par le source . le méthode habituellement proposer ne pouvoir pas être appliquer . ce article étudier comment mettre à jour des vue matérialiser XML sur un source web , au sein d' un architecture de médiation . 	Classifying XML Materialized Views for their maintenance on distributed Web sources	33
Revue des Nouvelles Technologies de l'Information	EGC	2005	Combinaison de fonctions de préférence par Boosting pour la recherche de passages dans les systèmes de question/réponse	Nous proposons une méthode d'apprentissage automatique pour la sélection de passages susceptibles de contenir la réponse à une question dans les systèmes de Question-Réponse (QR). Les systèmes de RI ad hoc ne sont pas adaptés à cette tâche car les passages recherchés ne doivent pas uniquement traiter du même sujet que la question mais en plus contenir sa réponse. Pour traiter ce problème les systèmes actuels ré-ordonnent les passages renvoyés par un moteur de recherche en considérant des critères sous forme d'une somme pondérée de fonctions de scores. Nous proposons d'apprendre automatiquement les poids de cette combinaison, grâce à un algorithme de réordonnancement défini dans le cadre du Boosting, qui sont habituellement déterminés manuellement. En plus du cadre d'apprentissage proposé, l'originalité de notre approche réside dans la définition des fonctions allouant des scores de pertinence aux passages. Nous validons notre travail sur la base de questions et de réponses de l'évaluation TREC-11 des systèmes de QR. Les résultats obtenus montrent une amélioration significative des performances en terme de rappel et de précision par rapport à un moteur de recherche standard et à une méthode d'apprentissage issue du cadre de la classification.	Nicolas Usunier, Massih-Reza Amini, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000169	http://editions-rnti.fr/render_pdf.php?p=1000169	1592	fr	fr	@lip6.fr	combinaison de fonction de préférence par Boosting pour le recherche de passage dans le système de question  réponse  Nous proposer un méthode d' apprentissage automatique pour le sélection de passage susceptible de contenir le réponse à un question dans le système de Question-Réponse ( QR ) . le système de RI ad hoc ne être pas adapter à ce tâche car le passage rechercher ne devoir pas uniquement traiter du même sujet que le question mais en plus contenir son réponse . Pour traiter ce problème le système actuel ré-ordonnent le passage renvoyer par un moteur de recherche en considérer un critère sous forme d' un somme pondérer de fonction de score . Nous proposer d' apprendre automatiquement le poids de ce combinaison , grâce à un algorithme de réordonnancement définir dans le cadre du Boosting , qui être habituellement déterminer manuellement . En plus du cadre d' apprentissage proposer , le originalité de son approche résider dans le définition des fonction allouer un score de pertinence aux passage . Nous valider son travail sur le base de question et de réponse de le évaluation TREC- 11 des système de QR . le résultat obtenir montrer un amélioration significatif des performance en terme de rappel et de précision par rapport à un moteur de recherche standard et à un méthode d' apprentissage issue du cadre de le classification . 	Combinaison de fonctions de préférence par Boosting pour la recherche de passages dans les systèmes de question/réponse	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	De la statistique des données à la statistique des connaissances : avancées récentes en analyse des données symboliques		Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000437	http://editions-rnti.fr/render_pdf.php?p=1000437	1593	fr		@email	De la statistique des données à la statistique des connaissances : avancées récentes en analyse des données symboliques 	De la statistique des données à la statistique des connaissances : avancées récentes en analyse des données symboliques	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Élagage et aide à l'interprétation symbolique et graphique d'une pyramide	"Le but de ce travail est de faciliter l'interprétation d'une classification pyramidale construite sur un tableau de données symboliques. Alors que dans une hiérarchie binaire le nombre de paliers est égal à n-1, si n est le nombre d'individus à classer, dans le cas d'une pyramide ce dernier peut atteindre n(n-1)/2. Afin de réduire ce nombre, on élague la pyramide et on utilise un critère de sélection de paliers basé sur la hauteur. De plus on décrit tous les paliers retenus par des variables que l'on sélectionne également en utilisant ""le degré de généralité"" ainsi que des mesures de dissimilarités de type symbolique-numérique. L'aide à l'interprétation se sert d'outils graphiques et interactifs grâce à la bibliothèque OpenGL. Enfin une simulation montre comment évoluent ces sélections quand le nombre de classes et de variables croit."	Kutluhan Kemal Pak, Mohamed Cherif Rahal, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000226	http://editions-rnti.fr/render_pdf.php?p=1000226	1594	fr	fr	@ceremade.dauphine.fr	Élagage et aide à le interprétation symbolique et graphique d' un pyramide  " le but de ce travail être de faciliter le interprétation d' un classification pyramidal construire sur un tableau de donnée symbolique . . alors que dans un hiérarchie binaire le nombre de palier être égal à n-1 si n être le nombre d' individu à classer , dans le cas d' un pyramide ce dernier pouvoir atteindre n ( n-1 )  2 . . Afin de réduire ce nombre , on élaguer le pyramide et on utiliser un critère de sélection de palier baser sur le hauteur . . De plus on décrire tout le palier retenir par un variable que le on sélectionner également en utiliser " " le degré de généralité " " ainsi que un mesure de dissimilarités de type symbolique-numérique . . le aide à le interprétation clr servir d' outil graphique et interactif grâce à le bibliothèque OpenGL . . enfin un simulation montrer comment évoluer ce sélection quand le nombre de classe et de variable croire . " 	Élagage et aide à l'interprétation symbolique et graphique d'une pyramide	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Enrichissement sémantique de documents XML représentant des tableaux	Ce travail a pour objectif la construction automatique d'un entrepôt thématique de données, à partir de documents de format divers provenant du Web. L'exploitation de cet entrepôt est assurée par un moteur d'interrogation fondé sur une ontologie. Notre attention porte plus précisément sur les tableaux extraits de ces documents et convertis au format XML, aux tags exclusivement syntaxiques. Cet article présente la transformation de ces tableaux, sous forme XML, en un formalisme enrichi sémantiquement dont la plupart des tags et des valeurs sont des termes construits à partir de l'ontologie.	Fatiha Saïs, Hélène Gagliardi, Ollivier Haemmerlé, Nathalie Pernelle	http://editions-rnti.fr/render_pdf.php?p1&p=1000309	http://editions-rnti.fr/render_pdf.php?p=1000309	1595	fr	fr	@lri.fr, @inapg.fr	enrichissement sémantique de document XML représenter un tableaux  ce travail avoir pour objectif le construction automatique d' un entrepôt thématique de donnée , à partir de document de format divers provenir du Web . le exploitation de ce entrepôt être assurer par un moteur d' interrogation fonder sur un ontologie . son attention porter plus précisément sur le tableau extrait de ce document et convertir au format XML , aux tags exclusivement syntaxique . ce article présenter le transformation de ce tableau , sous forme XML , en un formalisme enrichir sémantiquement dont le plupart des tags et des valeur être un terme construire à partir de le ontologie . 	Enrichissement sémantique de documents XML représentant des tableaux	7
Revue des Nouvelles Technologies de l'Information	EGC	2005	Entrepôt de Données Spatiales basé sur GML : Politique  de Gestion de Cache		Lionel Savary, Georges Gardarin, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000290	http://editions-rnti.fr/render_pdf.php?p=1000290	1596	fr		@prism.uvsq.fr	Entrepôt de Données Spatiales basé sur GML : Politique  de Gestion de Cache 	Entrepôt de Données Spatiales basé sur GML : Politique  de Gestion de Cache	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Évaluation des algorithmes LEM et eLEM pour données continues	Très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange, l'algorithme EM présente l'inconvénient majeur de converger parfois lentement. Son application sur des tableaux de grande taille devient ainsi irréalisable. Afin de remédier à ce problème, plusieurs méthodes ont été proposées. Nous présentons ici le comportement d'une méthode connue, LEM, et d'une variante que nous avons proposée récemment eLEM. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification, et nous illustrons le bon comportement de notre variante sur des données continues simulées et réelles.	François-Xavier Jollois, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000231	http://editions-rnti.fr/render_pdf.php?p=1000231	1597	fr	fr	@univ-paris5.fr, @univ-metz.fr	Évaluation des algorithme LEM et eLEM pour donnée continues  très populaire et très efficace pour le estimation de paramètre d' un modèle de mélange , le algorithme EM présenter le inconvénient majeur de converger parfois lentement . son application sur un tableau de grand taille devenir ainsi irréalisable . Afin de remédier à ce problème , plusieurs méthode avoir être proposer . Nous présenter ici le comportement d' un méthode connaître , LEM , et d' un variante que nous avoir proposer récemment eLEM . Celles _-ci permettre d' accélérer le convergence de le algorithme , tout en obtenir un résultat similaire à celui _-ci . Dans ce travail , nous clr concentrer sur le aspect classification , et nous illustrer le bon comportement de son variante sur un donnée continu simuler et réel . 	Évaluation des algorithmes LEM et eLEM pour données continues	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Expériences de classification d'une collection de documents XML de structure homogène	Cet article présente différentes expériences de classification de documents XML de structure homogène, en vue d'expliquer et de valider une présentation organisationnelle pré-existante. Le problème concerne le choix des éléments et mots utilisés pour la classification et son impact sur la typologie induite. Pour cela nous combinons une sélection structurelle basée sur la nature des éléments XML et une sélection linguistique basée sur un typage syntaxique des mots. Nous illustrons ces principes sur la collection des rapports d'activité 2003 des équipes de recherche de l'Inria en cherchant des groupements d'équipes (Thèmes) à partir du contenu de différentes parties de ces rapports. Nous comparons nos premiers résultats avec les thèmes de recherche officiels de l'Inria.	Thierry Despeyroux, Yves Lechevallier, Brigitte Trousse, Anne-Marie Vercoustre	http://editions-rnti.fr/render_pdf.php?p1&p=1000243	http://editions-rnti.fr/render_pdf.php?p=1000243	1598	fr	fr	@inria.fr	expérience de classification d' un collection de document XML de structure homogène  ce article présent différent expérience de classification de document XML de structure homogène , en vue d' expliquer et de valider un présentation organisationnel pré-existante . le problème concerner le choix des élément et mot utiliser pour le classification et son impact sur le typologie induire . Pour cela nous combiner un sélection structurel baser sur le nature des élément XML et un sélection linguistique baser sur un typage syntaxique des mot . Nous illustrer ce principe sur le collection des rapport d' activité 2003 des équipe de recherche de le Inria en chercher un groupement d' équipe ( Thèmes ) à partir du contenu de différent partie de ce rapport . Nous comparer son premier résultat avec le thème de recherche officiel de le Inria . 	Expériences de classification d'une collection de documents XML de structure homogène	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Expérimentations sur un modèle de recherche d'information utilisant les liens hypertextes des pages Web	La fonction de correspondance, qui permet de sélectionner et de classer les documents par rapport à une requête est un composant essentiel dans tout système de recherche d'information. Nous proposons de modéliser une fonction de correspondance prenant en compte à la fois le contenu et les liens hypertextes des pages Web. Nous avons expérimenté notre système sur la collection de test TREC-9, et nous concluons que pour certains types de requêtes, inclure le texte ancre associé aux liens hypertextes des pages dans la fonction de similarité s'avère plus efficace.	Bich-Liên Doan, Idir Chibane	http://editions-rnti.fr/render_pdf.php?p1&p=1000264	http://editions-rnti.fr/render_pdf.php?p=1000264	1599	fr	fr	@supelec.fr, @supelec.fr	expérimentation sur un modèle de recherche d' information utiliser le lien hypertextes des page Web  le fonction de correspondance , qui permettre de sélectionner et de classer le document par rapport à un requête être un composer essentiel dans tout système de recherche d' information . Nous proposer de modéliser un fonction de correspondance prendre en compte à le foi le contenu et le lien hypertextes des page Web . Nous avoir expérimenter son système sur le collection de test TREC- 9 , et nous conclure que pour certain type de requête , inclure le texte ancrer associer aux lien hypertexte des page dans le fonction de similarité clr avérer plus efficace . 	Expérimentations sur un modèle de recherche d'information utilisant les liens hypertextes des pages Web	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles	Nous traitons l'extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles. La méthode proposée nous permet de découvrir des règles d'association au niveau des concepts. Cette extension implique notamment de nouvelles définitions pour le support et la confiance afin d'exploiter la structure symbolique des données. Au fil de l'article l'exemple classique du panier de la ménagère est développé. Ainsi, plutôt que d'extraire des règles entre différents articles appartenant à des mêmes transactions enregistrées dans un magasin comme dans le cas classique, nous extrayons des règles d'association au niveau des clients afin d'étudier leurs comportements d'achat.	Filipe Afonso, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000348	http://editions-rnti.fr/render_pdf.php?p=1000348	1600	fr	fr	@ceremade.dauphine.fr, @ceremade.dauphine.fr	extension de le algorithme Apriori et des règle d' association aux cas des donnée symbolique diagramme et intervalles  Nous traiter le extension de le algorithme Apriori et des règle d' association aux cas des donnée symbolique diagramme et intervalle . le méthode proposer nous permettre de découvrir un règle d' association au niveau des concept . ce extension impliquer notamment un nouveau définition pour le support et le confiance afin d' exploiter le structure symbolique des donnée . Au fil de le article le exemple classique du panier de le ménagère être développer . ainsi , plutôt que d' extraire un règle entre différents article appartenir à un même transaction enregistrer dans un magasin comme dans le cas classique , nous extraire un règle d' association au niveau des client afin d' étudier son comportement d' achat . 	Extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extension des bases de données inductives pour la découverte de chroniques	Les bases de données inductives intègrent le processus de fouille de données dans une base de données qui contient à la fois les données et les connaissances induites. Nous nous proposons d'étendre les données traitées afin de permettre l'extraction de motifs temporels fréquents et non fréquents à partir d'un ensemble de séquences d'évènements. Les motifs temporels visés sont des chroniques qui permettent d'exprimer des contraintes numériques sur les délais entre les occurrences d'évènements.	Alexandre Vautier, Marie-Odile Cordier, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1000314	http://editions-rnti.fr/render_pdf.php?p=1000314	1601	fr	fr	@irisa.fr	extension des base de donnée inductif pour le découverte de chroniques  le base de donnée inductif intégrer le processus de fouille de donnée dans un base de donnée qui contenir à le foi le donnée et le connaissance induire . Nous nous proposer d' étendre le donnée traiter afin de permettre le extraction de motif temporel fréquent et non fréquent à partir d' un ensemble de séquence d' évènements . le motif temporel viser être un chronique qui permettre d' exprimer un contrainte numérique sur le délai entre le occurrence d' évènements . 	Extension des bases de données inductives pour la découverte de chroniques	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction Bayésienne et intégration de patterns représentés suivant les K plus proches voisins pour le go 19x19	Cet article décrit la génération automatique et l'utilisation d'une base de patterns pour le go 19x19. La représentation utilisée est celle des K plus proches voisins. Les patterns sont engendrés en parcourant des parties de professionnels. Les probabilités d'appariement et de jeu des patterns sont également estimées à ce moment là. La base créée est intégrée dans un programme existant, Indigo. Soit elle est utilisée comme un livre d'ouvertures en début de partie, soit comme une extension des bases pré-existantes du générateur de coups du programme. En terme de niveau de jeu, le gain résultant est estimé à 15 points en moyenne.	Bruno Bouzy, Guillaume Chaslot	http://editions-rnti.fr/render_pdf.php?p1&p=1000212	http://editions-rnti.fr/render_pdf.php?p=1000212	1602	fr	fr	@univ-paris5.fr, @ec-lille.fr	extraction Bayésienne et intégration de pattern représenter suivant le K plus proche voisin pour le go 19x19  ce article décrire le génération automatique et le utilisation d' un base de pattern pour le go 19x19 . le représentation utiliser être celui des K plus proche voisin . le pattern être engendrer en parcourir un partie de professionnel . le probabilité d' appariement et de jeu des pattern être également estimer à ce moment là . le base créer être intégrer dans un programme existant , Indigo . Soit elle être utiliser comme un livre d' ouverture en début de partie , soit comme un extension des base pré-existantes du générateur de coup du programme . En terme de niveau de jeu , le gain résulter être estimer à 15 point en moyenne . 	Extraction Bayésienne et intégration de patterns représentés suivant les K plus proches voisins pour le go 19x19	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction bilingue de termes médicaux dans un corpus  parallèle anglais/français	Le Catalogue et Index des Sites Médicaux Francophones (CISMeF) recense les principales ressources institutionnelles de santé en français. La description de ces ressources, puis leur accès par les utilisateurs, se fait grâce à la terminologie CISMeF, fondée sur le thésaurus américain Medical Subject Headings (MeSH). La version française du MeSH comprend tous les descripteurs MeSH, mais de nombreux synonymes américains restent à traduire. Afin d'enrichir la terminologie, nous proposons ici une méthode de traduction automatique de ces synonymes. Pour ce faire, nous avons constitué deux corpus parallèles anglais/français du domaine médical. Après alignement semi-automatique des corpus paragraphe à paragraphe, nous avons procédé automatiquement à l'appariement bilingue des termes. Pour cela, le lexique constitué des descripteurs MeSH américains et de leur traduction en français a fourni les couples amorces qui ont servi de point de départ à la propagation syntaxique des liens d'appariement. 217 synonymes ont pu être traduits, avec une précision de 70%.	Aurélie Névéol, Sylwia Ozdowska	http://editions-rnti.fr/render_pdf.php?p1&p=1000413	http://editions-rnti.fr/render_pdf.php?p=1000413	1603	fr	fr	@insa-rouen.fr, @stics, @univ-tlse2.fr	extraction bilingue de terme médical dans un corpus parallèle anglais  français  le Catalogue et Index des Sites Médicaux Francophones ( CISMeF ) recenser le principal ressource institutionnel de santé en français . le description de ce ressource , puis son accès par le utilisateur , clr faire grâce à le terminologie CISMeF , fonder sur le thésaurus américain Medical Subject Headings ( MeSH ) . le version français du MeSH comprendre tout le descripteur MeSH , mais un nombreux synonyme américain rester à traduire . Afin d' enrichir le terminologie , nous proposer ici un méthode de traduction automatique de ce synonyme . Pour ce faire , nous avoir constituer deux corpus parallèle anglais  français du domaine médical . Après alignement semi-automatique des corpus paragraphe à paragraphe , nous avoir procéder automatiquement à le appariement bilingue des terme . Pour cela , le lexique constituer des descripteur MeSH américain et de son traduction en français avoir fournir le couple amorce qui avoir servir de point de départ à le propagation syntaxique des lien d' appariement . 217 synonyme avoir pouvoir être traduire , avec un précision de 70 \% . 	Extraction bilingue de termes médicaux dans un corpus  parallèle anglais/français	18
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de la localisation des termes pour le classement des documents	Trouver et classer les documents pertinents par rapport à une requête est fondamental dans le domaine de la recherche d'information. Notre étude repose sur la localisation des termes dans les documents. Nous posons l'hypothèse que plus les occurrences des termes d'une requête se retrouvent proches dans un document alors plus ce dernier doit être positionné en tête de la liste de réponses. Nous présentons deux variantes de notre modèle à zone d'influence, la première est basée sur une notion de proximité floue et la seconde sur une notion de pertinence locale.	Annabelle Mercier, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1000269	http://editions-rnti.fr/render_pdf.php?p=1000269	1604	fr	fr	@emse.fr	extraction de le localisation des terme pour le classement des documents  trouver et classer le document pertinent par rapport à un requête être fondamental dans le domaine de le recherche d' information . son étude reposer sur le localisation des terme dans le document . Nous poser le hypothèse que plus le occurrence des terme d' un requête clr retrouver proche dans un document alors plus ce dernier devoir être positionner en tête de le liste de réponse . Nous présenter deux variante de son modèle à zone d' influence , le premier être baser sur un notion de proximité flou et le second sur un notion de pertinence local . 	Extraction de la localisation des termes pour le classement des documents	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de règles d'association quantitatives application à des données médicales	L'extraction de règles d'association est devenue aujourd'hui une tâche populaire en fouille de données. Cependant, l'algorithme Apriori et ses variantes restent dédiés aux bases de données renfermant des informations catégoriques.Nous proposons dans cet article QuantMiner, qui est un outil que nous avons développé dans le but d'extraire des règles d'association gérant variables catégoriques et numériques. L'outil que nous proposons repose sur un algorithme génétique permettant de découvrir de façon dynamique les intervalles des variables numériques apparaissant dans les règles.Nous présentons également une application réelle de notre outil sur des données médicales relatives à la maladie de l'athérosclérose et donnons des résultats de notre expérience pour la description et la caractérisation de cette maladie.	Cyril Nortet, Ansaf Salleb-Aouissi, Teddy Turmeaux, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000352	http://editions-rnti.fr/render_pdf.php?p=1000352	1605	fr	fr	@lifo.univ, @irisa.fr	extraction de règle d' association quantitatif application à un donnée médicales  le extraction de règle d' association être devenir aujourd' hui un tâche populaire en fouille de donnée . cependant , le algorithme Apriori et son variante rester dédier aux base de donnée renfermer un information catégorique . Nous proposer dans ce article QuantMiner , qui être un outil que nous avoir développer dans le but d' extraire un règle d' association gérer variable catégorique et numérique . le outil que nous proposer reposer sur un algorithme génétique permettre de découvrir de façon dynamique le intervalle des variable numérique apparaître dans le règle . Nous présenter également un application réel de son outil sur un donnée médical relatif à le maladie de le athérosclérose et donner un résultat de son expérience pour le description et le caractérisation de ce maladie . 	Extraction de règles d'association quantitatives application à des données médicales	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de termes centrée autour de l'expert	Nous développons un logiciel, Exit, capable d'aider un expert à extraire des termes qu'il trouve pertinents dans des textes de spécialité. Tout est mis en place pour faciliter le travail de l'expert afin qu'il puisse consacrer son temps à la seule reconnaissance des termes pertinents. Pour cela, différentes mesures statistiques et de nombreuses options d'extraction sont disponibles dans Exit. Afin d'utiliser au mieux les connaissances de l'expert, notre approche est semi-automatique. De plus, l'expert construit des termes pouvant inclure des termes précédemment extraits ce qui rend itératif et constructif notre processus de formation des termes. Enfin, l'ergonomie du logiciel a profité des enseignements tirés lors de son utilisation pour une compétition internationale d'extraction de connaissances.	Thomas Heitz, Mathieu Roche, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000425	http://editions-rnti.fr/render_pdf.php?p=1000425	1606	fr	fr	@lri.fr	extraction de terme centrer autour de le expert  Nous développer un logiciel , Exit , capable d' aider un expert à extraire un terme qu' il trouver pertinent dans un texte de spécialité . tout être mettre en place pour faciliter le travail de le expert afin qu' il pouvoir consacrer son temps à le seul reconnaissance des terme pertinent . Pour cela , différentes mesure statistique et un nombreux option d' extraction être disponible dans Exit . Afin d' utiliser au mieux le connaissance de le expert , son approche être semi-automatique . De plus , le expert construire des terme pouvoir inclure un terme précédemment extrait ce qui rendre itératif et constructif son processus de formation des terme . enfin , le ergonomie du logiciel avoir profiter un enseignement tirer lors de son utilisation pour un compétition international d' extraction de connaissance . 	Extraction de termes centrée autour de l'expert	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction des connaissances pour l'enrichissement des bases  de données géographiques		Sami Faiz, Khaoula Mahmoudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000394	http://editions-rnti.fr/render_pdf.php?p=1000394	1607	fr		@insat.rnu.tn, @insat.rnu.tn	Extraction des connaissances pour l'enrichissement des bases  de données géographiques 	Extraction des connaissances pour l'enrichissement des bases  de données géographiques	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fonctions d'oubli et conservation de détail dans les entrepôts de données		Aliou Boly, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000287	http://editions-rnti.fr/render_pdf.php?p=1000287	1608	fr		@enst.fr, @enst.fr, @edf.fr	Fonctions d'oubli et conservation de détail dans les entrepôts de données 	Fonctions d'oubli et conservation de détail dans les entrepôts de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Forage distribué des données : une comparaison entre l'agrégation d'échantillons et l'agrégation de règles	Pour nous attaquer au problème du forage de très grandes bases de données distribuées, nous proposons d'étudier deux approches. La première est de télécharger seulement un échantillon de chaque base de données puis d'y effectuer le forage. La deuxième approche est de miner à distance chaque base de données indépendamment, puis de télécharger les modèles résultants, sous forme de règles de classification, dans un site central où l'agrégation de ces derniers est réalisée. Dans cet article, nous présentons une vue d'ensemble des techniques d'échantillonnage les plus communes. Nous présentons ensuite cette nouvelle technique de forage distribué des données où la mécanique d'agrégation est basée sur un coefficient de confiance attribué à chaque règle et sur de très petits échantillons de chaque base de données. Le coefficient de confiance d'une règle est calculé par des moyens statistiques en utilisant le théorème limite centrale. En conclusion, nous présentons une comparaison entre les meilleures techniques d'échantillonnage que nous avons trouvées dans la littérature, et notre approche de forage distribué des données (FDD) basée sur l'agrégation de modèles.	Mohamed Aounallah, Sébastien Quirion, Guy W. Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000211	http://editions-rnti.fr/render_pdf.php?p=1000211	1609	fr	fr	@ift.ulaval.ca, @ift.ulaval.ca, @gel.ulaval.ca	forage distribuer des donnée : un comparaison entre le agrégation d' échantillon et le agrégation de règles  Pour nous attaquer au problème du forage de très grand base de donnée distribuer , nous proposer d' étudier deux approche . le premier être de télécharger seulement un échantillon de chaque base de donnée puis d' y effectuer le forage . le deuxième approche être de miner à distance chaque base de donnée indépendamment , puis de télécharger le modèle résultants , sous forme de règle de classification , dans un site central où le agrégation de ce dernier être réaliser . Dans ce article , nous présenter un vue d' ensemble des technique d' échantillonnage le plus commun . Nous présenter ensuite ce nouveau technique de forage distribuer des donnée où le mécanique d' agrégation être baser sur un coefficient de confiance attribuer à chaque règle et sur de très petit échantillon de chaque base de donnée . le coefficient de confiance d' un règle être calculer par un moyen statistique en utiliser le théorème limite central . En conclusion , nous présenter un comparaison entre le meilleur technique d' échantillonnage que nous avoir trouver dans le littérature , et son approche de forage distribuer des donnée ( FDD ) baser sur le agrégation de modèle . 	Forage distribué des données : une comparaison entre l'agrégation d'échantillons et l'agrégation de règles	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de Données Relationnelles dans les SGBD		Cédric Udréa, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000288	http://editions-rnti.fr/render_pdf.php?p=1000288	1610	fr		@univ-lyon2.fr	Fouille de Données Relationnelles dans les SGBD 	Fouille de Données Relationnelles dans les SGBD	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de graphes et découverte de règles d'association : application à l'analyse d'images de document	Cet article présente une méthode permettant la découverte non supervisée de motifs fréquents représentatifs de symboles sur des images de documents. Les symboles sont considérés comme des entités graphiques porteurs d'information et les images de document sont représentées par des graphes relationnels attribués. Dans un premier temps, la méthode réalise la découverte de sous-graphes disjoints fréquents et fait correspondre pour chacun d'eux un symbole différent. Une recherche des règles d'association entre ces symboles permet alors d'accéder à une partie des connaissances du domaine décrit par ces symboles. L'objectif à terme est d'utiliser les symboles découverts pour la classification ou la recherche d'images dans un flux hétérogène de document là ou une approche supervisée n'est pas envisageable.	Eugen Barbu, Pierre Héroux, Sébastien Adam, Éric Trupin	http://editions-rnti.fr/render_pdf.php?p1&p=1000341	http://editions-rnti.fr/render_pdf.php?p=1000341	1611	fr	fr	@univ-rouen.fr	fouille de graphe et découverte de règle d' association : application à le analyse d' image de document  ce article présenter un méthode permettre le découverte non superviser de motif fréquent représentatif de symbole sur un image de document . le symbole être considérer comme un entité graphique porteur d' information et le image de document être représenter par un graphe relationnel attribuer . Dans un premier temps , le méthode réaliser le découverte de sous-graphes disjoint fréquent et faire correspondre pour chacun d' lui un symbole différent . un recherche des règle d' association entre ce symbole permettre alors d' accéder à un partie des connaissance du domaine décrire par ce symbole . le objectif à terme être d' utiliser le symbole découvert pour le classification ou le recherche d' image dans un flux hétérogène de document là ou un approche superviser n' être pas envisageable . 	Fouille de graphes et découverte de règles d'association : application à l'analyse d'images de document	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de textes pour orienter la construction d'une ressource terminologique	La finalité de ce papier est d'analyser l'apport de techniques de fouille de données textuelles à une méthodologie de construction d'ontologie à partir de textes. Le domaine d'application de cette expérimentation est celui de l'accidentologie routière. Dans ce contexte, les résultats des techniques de fouille de données textuelles sont utilisés pour orienter la construction d'une ressource terminologique à partir de procès-verbaux d'accidents. La méthode TERMINAE et l'outil du même nom offrent le cadre général pour la modélisation de la ressource. Le papier présente les techniques de fouille employées et l'intégration des résultats des fouilles dans les différentes étapes du processus de construction de la ressource.	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1000261	http://editions-rnti.fr/render_pdf.php?p=1000261	1612	fr	fr	@univ-paris5.fr, @univ-paris5.fr	fouille de texte pour orienter le construction d' un ressource terminologique  le finalité de ce papier être d' analyser le apport de technique de fouille de donnée textuel à un méthodologie de construction d' ontologie à partir de texte . le domaine d' application de ce expérimentation être celui de le accidentologie routier . Dans ce contexte , le résultat des technique de fouille de donnée textuel être utiliser pour orienter le construction d' un ressource terminologique à partir de procès-verbal d' accident . le méthode TERMINAE et le outil du même nom offrir le cadre général pour le modélisation de le ressource . le papier présent le technique de fouille employer et le intégration des résultat des fouille dans le différent étape du processus de construction de le ressource . 	Fouille de textes pour orienter la construction d'une ressource terminologique	4
Revue des Nouvelles Technologies de l'Information	EGC	2005	Hiérarchisation des règles d'association en fouille de textes	L'extraction de règles d'association est souvent exploitée comme méthode de fouille de données. Cependant, une des limites de cette approche vient du très grand nombre de règles extraites et de la difficulté pour l'analyste à appréhender la totalité de ces règles. Nous proposons donc de pallier ce problème en structurant l'ensemble des règles d'association en hiérarchies. La structuration des règles se fait à deux niveaux. Un niveau global qui a pour objectif de construire une hiérarchie structurant les règles extraites des données. Nous définissons donc un premier type de subsomption entre règles issue de la subsomption dans les treillis de Galois. Le second niveau correspond à une analyse locale des règles et génère pour une règle donnée une hiérarchie de généralisation de cette règle qui repose sur des connaissances complémentaires exprimées dans un modèle terminologique. Ce niveau fait appel à un second type de subsomption inspiré de la subsomption en programmation logique inductive. Nous définissons ces deux types de subsomptions, développons un exemple montrant l'intérêt de l'approche pour l'analyste et étudions les propriétés formelles des hiérarchies ainsi proposées.	Rokia Bendaoud, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000268	http://editions-rnti.fr/render_pdf.php?p=1000268	1613	fr	fr	@loria.fr	hiérarchisation des règle d' association en fouille de textes  le extraction de règle d' association être souvent exploiter comme méthode de fouille de donnée . cependant , un des limite de ce approche venir du très grand nombre de règle extraire et de le difficulté pour le analyste à appréhender le totalité de ce règle . Nous proposer donc de pallier ce problème en structurer le ensemble des règle d' association en hiérarchie . le structuration des règle clr faire à deux niveau . un niveau global qui avoir pour objectif de construire un hiérarchie structurer le règle extraire des donnée . Nous définir donc un premier type de subsomption entre règle issue de le subsomption dans le treillis de Galois . le second niveau correspondre à un analyse local des règle et générer pour un règle donner un hiérarchie de généralisation de ce règle qui reposer sur un connaissance complémentaire exprimer dans un modèle terminologique . ce niveau faire appel à un second type de subsomption inspirer de le subsomption en programmation logique inductif . Nous définir ce deux type de subsomption , développer un exemple montrer le intérêt de le approche pour le analyste et étudier le propriété formel des hiérarchie ainsi proposer . 	Hiérarchisation des règles d'association en fouille de textes	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Intégration efficace des arbres de décision dans les SGBD : utilisation des index bitmap	Nous présentons dans cet article une nouvelle approche de fouille qui permet d'appliquer des algorithmes de construction d'arbres de décision en répondant à deux objectifs : (1) traiter des bases volumineuses, (2) en des temps de traitement acceptables. Le premier objectif est atteint en intégrant ces algorithmes au coeur des SGBD, en utilisant uniquement les outils fournis par ces derniers. Toutefois, les temps de traitement demeurent longs, en raison des nombreuses lectures de la base. Nous montrons que, grâce aux index bitmap, nous réduisons à la fois la taille de la base d'apprentissage et les temps de traitements. Pour valider notre approche, nous avons implémenté la méthode ID3 sous forme d'une procédure stockée dans le SGBD Oracle.	Cécile Favre, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000281	http://editions-rnti.fr/render_pdf.php?p=1000281	1614	fr	fr	@univ-lyon2.fr	intégration efficace des arbre de décision dans le SGBD : utilisation des index bitmap  Nous présenter dans ce article un nouveau approche de fouille qui permettre d' appliquer un algorithme de construction d' arbre de décision en répondre à deux objectif : ( 1 ) traiter un base volumineux , ( 2 ) en un temps de traitement acceptable . le premier objectif être atteindre en intégrer ce algorithme au coeur des SGBD , en utiliser uniquement le outil fournir par ce dernier . toutefois , le temps de traitement demeurer long , en raison des nombreux lecture de le base . Nous montrer que , grâce aux index bitmap , nous réduire à le foi le taille de le base d' apprentissage et le temps de traitement . Pour valider son approche , nous avoir implémenter le méthode ID3 sous forme d' un procédure stocker dans le SGBD Oracle . 	Intégration efficace des arbres de décision dans les SGBD : utilisation des index bitmap	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	L'automate textuel pour la prise en compte de l'évolution du texte	Il n'est plus à rappeler que le corpus textuel, est tel qu'il est actuellement, intraitable à l'échelle que sa croissance nous confirme l'obligation d'utiliser des outils automatique de traitement. Cet article s'intéresse plus particulièrement à la caractérisation de textes et par là même à celle d'auteurs. A l'heure actuelle, toutes les méthodes existant travaillent sur le document fini, sans admettre qu'un cheminement existe entre le début du document et sa fin. Nous proposons une méthode tentant d'apporter cette notion d'évolution textuelle en traitant le texte par un automate et l'évaluation choisie. Puis nous présenterons des résultats validés par des experts, obtenus sur un corpus d'entretiens sociologiques.	Hubert Marteau, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000276	http://editions-rnti.fr/render_pdf.php?p=1000276	1615	fr	fr	@univ-tours.fr, @univ-paris5.fr	le automate textuel pour le prise en compte de le évolution du texte  Il n' être plus à rappeler que le corpus textuel , être tel qu' il être actuellement , intraitable à le échelle que son croissance nous confirmer le obligation d' utiliser un outil automatique de traitement . ce article clr intéresser plus particulièrement à le caractérisation de texte et par là même à celui d' auteur . A le heure actuel , tout le méthode existant travailler sur le document finir , sans admettre qu' un cheminement exister entre le début du document et son fin . Nous proposer un méthode tentant d' apporter ce notion d' évolution textuel en traiter le texte par un automate et le évaluation choisir . Puis nous présenter un résultat valider par un expert , obtenir sur un corpus d' entretien sociologique . 	L'automate textuel pour la prise en compte de l'évolution du texte	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	La démarche ontologique pour la gestion des compétences et des connaissances	"La gestion des ressources humaines repose d'une part sur la connaissance des individus et de leurs compétences et d'autre part sur la connaissance de l'organisation et de ses métiers. C'est par la ""mise en correspondance"" de ces connaissances qu'il est possible d'améliorer l'emploi, de valoriser les connaissances et les compétences individuelles et de mieux gérer l'organisation. Cette mise en correspondance nécessite une représentation explicite des connaissances, ce qui permet de répondre à de nouveaux besoins : annuaire de compétences, gestion des projets et des retours d'expériences, identification des connaissances à risques, etc.Nous verrons dans le cadre de cet article l'intérêt de l'approche ontologique tant d'un point de vue méthodologique pour la clarification des notions mises en jeu dans le cadre de la GPECC (Gestion Prévisionnelle des Emplois des Compétences et des Connaissances) que pour la construction, la représentation et la maintenance des référentiels des compétences, des connaissances et des métiers. Elle permet en particulier une gestion de l'information par la terminologie et le sens métier propre à l'organisation."	Christophe Roche, Charles Foveau, Samah Reguigui	http://editions-rnti.fr/render_pdf.php?p1&p=1000299	http://editions-rnti.fr/render_pdf.php?p=1000299	1616	fr	fr	@univ-savoi, @ontologos-corp.com, @ontologos-corp.com	le démarche ontologique pour le gestion des compétence et des connaissances  " le gestion des ressource humain reposer d' un part sur le connaissance des individu et de son compétence et d' autre part sur le connaissance de le organisation et de son métier . . C' être par le " " mettre en correspondance " " de ce connaissance qu' il être possible d' améliorer le emploi , de valoriser le connaissance et le compétence individuel et de mieux gérer le organisation . . ce mise en correspondance nécessiter un représentation explicite des connaissance , ce qui permettre de répondre à un nouveau besoin : annuaire de compétence , gestion des projet et des retour d' expérience , identification des connaissance à risque , etc. Nous voir dans le cadre de ce article le intérêt de le approche ontologique tant d' un point de vue méthodologique pour le clarification des notion mettre en jeu dans le cadre de le GPECC ( Gestion Prévisionnelle des Emplois des Compétences et des connaissance ) que pour le construction , le représentation et le maintenance des référentiel des compétence , un connaissance et des métier . . Elle permettre en particulier un gestion de le information par le terminologie et le sens métier propre à le organisation . " 	La démarche ontologique pour la gestion des compétences et des connaissances	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	La réussite universitaire : prédictions par génération de règles		Nadine Meskens, Jean-Philippe Vandamme, Abdelhakim Artiba	http://editions-rnti.fr/render_pdf.php?p1&p=1000361	http://editions-rnti.fr/render_pdf.php?p=1000361	1617	fr		@fucam.ac.be, @i4e2.org	La réussite universitaire : prédictions par génération de règles 	La réussite universitaire : prédictions par génération de règles	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Les NTIC au services de la capitalisation des connaissances		Jean-Paul Barthès	http://editions-rnti.fr/render_pdf.php?p1&p=1000439	http://editions-rnti.fr/render_pdf.php?p=1000439	1618	fr		@utc.fr	Les NTIC au services de la capitalisation des connaissances 	Les NTIC au services de la capitalisation des connaissances	
Revue des Nouvelles Technologies de l'Information	EGC	2005	Logiciel d'aide à l'étiquetage morpho-syntaxique de  textes de spécialité	La compréhension de textes de spécialité nécessite un étiquetage morpho-syntaxique de bonne qualité. Or, lorsque les textes étudiés sont issus de domaines spécifiques et peu usités, il est rare de disposer de dictionnaires et autres ressources lexicales fiables. Le logiciel que nous proposons permet d'utiliser un étiquetage réalisé par un étiqueteur généraliste, puis d'améliorer cet étiquetage en intégrant des connaissances d'experts du domaine étudié. Grâce au logiciel développé, il est relativement aisé pour un expert du domaine de détecter des erreurs d'étiquetage et de mettre en place des règles de ré-étiquetage. Ces règles peuvent être obtenues de deux manières différentes : (1) soit en utilisant un langage de programmation permettant d'exprimer des règles complexes de ré-étiquetage, (2) soit par apprentissage automatique des règles à partir d'exemples corrigés au moyen d'une interface dédiée. Cet apprentissage propose de nouvelles règles à l'expert, acquises automatiquement.	Ahmed Amrani, Jérôme Azé, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000421	http://editions-rnti.fr/render_pdf.php?p=1000421	1619	fr	fr	@esiea.fr, @lri.fr	logiciel d' aide à le étiquetage morpho- syntaxique de texte de spécialité  le compréhension de texte de spécialité nécessiter un étiquetage morpho- syntaxique de bon qualité . Or , lorsque le texte étudier être issir de domaine spécifique et peu usiter , il être rare de disposer de dictionnaire et autre ressource lexical fiable . le logiciel que nous proposer permettre d' utiliser un étiquetage réaliser par un étiqueteur généraliste , puis d' améliorer ce étiquetage en intégrer un connaissance d' expert du domaine étudier . grâce au logiciel développer , il être relativement aisé pour un expert du domaine de détecter un erreur d' étiquetage et de mettre en place des règle de ré-étiquetage . ce règle pouvoir être obtenir de deux manière différent : ( 1 ) soit en utiliser un langage de programmation permettre d' exprimer un règle complexe de ré-étiquetage , ( 2 ) soit par apprentissage automatique des règle à partir d' exemple corriger au moyen d' un interface dédier . ce apprentissage proposer un nouveau règle à le expert , acquérir automatiquement . 	Logiciel d'aide à l'étiquetage morpho-syntaxique de  textes de spécialité	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Manipulation et fusion de données multidimensionnelles	Cet article définit une algèbre permettant de manipuler des tables dimensionnelles extraites d'une base de données multidimensionnelles. L'algèbre intègre un noyau minimum d'opérateurs unaires permettant d'effectuer les analyses décisionnelles par combinaison d'opérateurs. Cette algèbre intègre un opérateur binaire permettant la fusion de tables dimensionnelles facilitant les corrélations des sujets analysés.	Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000286	http://editions-rnti.fr/render_pdf.php?p=1000286	1620	fr	fr	@irit.fr	manipulation et fusion de donnée multidimensionnelles  ce article définir un algèbre permettre de manipuler un table dimensionnel extraire d' un base de donnée multidimensionnel . le algèbre intégrer un noyau minimum d' opérateur unaires permettre d' effectuer le analyse décisionnel par combinaison d' opérateur . ce algèbre intégrer un opérateur binaire permettre le fusion de table dimensionnel faciliter le corrélation des sujet analyser . 	Manipulation et fusion de données multidimensionnelles	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Méthode de construction d'ontologie de termes à partir du treillis de l'iceberg de Galois	"L'approche présentée dans cet article a pour objectif la construction d'une ontologie à partir du treillis de l'iceberg de Galois. Nous entendons par ontologie un ensemble de termes structurés entre eux par un ensemble de liens de divers types. Dans notre cas d'étude, cette ontologie constitue un support de connaissances ""documentaires"". En effet, elle peut être utilisée dans diverses applications en Recherche d'Information (RI), telles que l'indexation automatique et l'expansion de requêtes ainsi qu'en text-mining. La méthode de construction que nous proposons est fondée sur l'analyse formelle de concepts (AFC) et plus précisément, la structure du treillis de l'iceberg de Galois. En utilisant cette structure hiérarchique partiellement ordonnée, nous présentons une translation directe des relations laticielles vers celles ontologiques. Nous proposons ainsi d'enrichir l'ontologie dérivée par des règles associatives génériques entre termes, découvertes dans le cadre d'un processus de text-mining."	Cherif Chiraz Latiri, Mehdi Mtir, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1000304	http://editions-rnti.fr/render_pdf.php?p=1000304	1621	fr	fr	@gnet.tn, @fst.rnu.tn, @voila.fr	méthode de construction d' ontologie de terme à partir du treillis de le iceberg de Galois  " le approche présenter dans ce article avoir pour objectif le construction d' un ontologie à partir du treillis de le iceberg de Galois . . Nous entendre par ontologie un ensemble de terme structurer entre lui par un ensemble de lien de divers type . . Dans son cas d' étude , ce ontologie constituer un support de connaissance " " documentaire " " . . En effet , elle pouvoir être utiliser dans divers application en recherche d' Information ( RI ) , tel que le indexation automatique et le expansion de requête ainsi qu' en text-mining . . le méthode de construction que nous proposer être fonder sur le analyse formel de concept ( AFC ) et plus précisément , le structure du treillis de le iceberg de Galois . . En utiliser ce structure hiérarchique partiellement ordonner , nous présenter un translation direct des relation laticielles vers celui ontologique . . Nous proposer ainsi d' enrichir le ontologie dériver par un règle associatif générique entre terme , découvrir dans le cadre d' un processus de text-mining . " 	Méthode de construction d'ontologie de termes à partir du treillis de l'iceberg de Galois	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Microarray data mining : recent advances		Gregory Piatetsky-Shapiro	http://editions-rnti.fr/render_pdf.php?p1&p=1000443	http://editions-rnti.fr/render_pdf.php?p=1000443	1622	en			Microarray data mining : recent advances 	Microarray data mining : recent advances	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Mining Frequent Queries in Star Schemes	L'extraction de toutes les requêtes fréquentes dans une base de données relationnelle est un problème di±cile, même si l'on ne considère que des requêtes conjonctives. Nous montrons que ce problème devient possible dans le cas suivant : le schéma de la base est un schéma en étoile, et les données satisfont un ensemble de dépendances fonctionnelles et de contraintes référentielles. De plus, les schémas en étoile sont appropriés pour les entrepôts de données et que les dépendances fonctionnelles et les contraintes référentielles sont les contraintes les plus usuelles dans les bases de données. En considérant le modèle des instances faibles, nous montrons que les requêtes fréquentes exprimées par sélection-projection peuvent être extraites par des algorithmes de type Apriori.	Tao-Yuan Jen, Dominique Laurent, Nicolas Spyratos, Oumar Sy	http://editions-rnti.fr/render_pdf.php?p1&p=1000283	http://editions-rnti.fr/render_pdf.php?p=1000283	1623	en	fr	@dept-info.u, @lri.fr, @ugb.sn	le extraction de tout le requête fréquent dans un base de donnée relationnel être un problème di±cile , même si le on ne considérer que un requête conjonctif . Nous montrer que ce problème devenir possible dans le cas suivant : le schéma de le base être un schéma en étoile , et le donnée satisfaire un ensemble de dépendance fonctionnel et de contrainte référentiel . De plus , le schéma en étoile être approprier pour le entrepôt de donnée et que le dépendance fonctionnel et le contrainte référentiel être le contrainte le plus usuel dans le base de donnée . En considérer le modèle des instance faible , nous montrer que le requête fréquent exprimer par sélection-projection pouvoir être extraire par un algorithme de type Apriori . 	Mining Frequent Queries in Star Schemes	11
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation de connaissances pour un système de médiation	Travaillant sur l'élaboration d'une méthodologie de développement de systèmes de médiation intégrés dans des systèmes coopératifs, nous avons proposé une architecture à 3 composants : le premier concerne la coopération, le second l'assistance et le troisième est relatif aux connaissances nécessaires aux 2 précédents. Dans cet article nous présentons plus particulièrement le point de vue des connaissances. Ces connaissances sont de 2 natures : des connaissances statiques, sur le domaine par exemple, et des connaissances acquises pendant l'utilisation coopérative du système, notamment la mémoire des activités et les descriptions des actes de résolutions de problèmes. Pour illustrer cette modélisation de connaissances, nous nous intéresserons aux activités coopératives de suivi, de gestion et d'évaluation de projets d'étudiants, assistées par l'outil iPédagogique.	Victoria Eugenia Ospina, Alain-Jérôme Fougères, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1000383	http://editions-rnti.fr/render_pdf.php?p=1000383	1624	fr	fr	@utbm.fr, @utt.fr	modélisation de connaissance pour un système de médiation  travailler sur le élaboration d' un méthodologie de développement de système de médiation intégrer dans un système coopératif , nous avoir proposer un architecture à 3 composant : le premier concerner le coopération , le second le assistance et le troisième être relatif aux connaissance nécessaire aux 2 précédent . Dans ce article nous présenter plus particulièrement le point de vue des connaissance . ce connaissance être de 2 nature : un connaissance statique , sur le domaine par exemple , et des connaissance acquérir pendant le utilisation coopératif du système , notamment le mémoire des activité et le description des acte de résolution de problème . Pour illustrer ce modélisation de connaissance , nous nous intéresser aux activité coopératif de suivi , de gestion et d' évaluation de projet d' étudiant , assister par le outil iPédagogique . 	Modélisation de connaissances pour un système de médiation	6
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation de la cognition sociale - propositions autour de  l'utilisation de schémas cognitifs		Jorge Louçã	http://editions-rnti.fr/render_pdf.php?p1&p=1000395	http://editions-rnti.fr/render_pdf.php?p=1000395	1625	fr		@iscte.pt	Modélisation de la cognition sociale - propositions autour de  l'utilisation de schémas cognitifs 	Modélisation de la cognition sociale - propositions autour de  l'utilisation de schémas cognitifs	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation des individus et de leurs relations pour l'aide à l'intégration des individus dans l'organisation	L'objectif de ce papier est de présenter une contribution à la modélisation des individus et de leurs relations pour permettre l'aide à l'intégration des acteurs dans une organisation. Nous étudions en particulier le cas du remplacement d'un acteur (« turn-over »). Dans ce cadre, nous proposons un modèle regroupant un ensemble de données relatives à un individu, aux relations que celui-ci entretient avec les autres acteurs et à son espace informationnel. L'étude porte sur la mise en oeuvre de mécanismes d'aide fournissant à un acteur les moyens de son intégration : la mise à disposition d'une image des espaces informationnels et relationnels de son prédécesseur ainsi que la mise en relation de l'acteur avec les autres acteurs de l'organisation. Cette étude est menée en partenariat avec des experts en GRH.	Marie-Françoise Canut, Max Chevalier, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1000392	http://editions-rnti.fr/render_pdf.php?p=1000392	1626	fr	fr	@iut-blagnac.fr, @iut-blagnac.fr, @irit.fr, @irit.fr	modélisation des individu et de son relation pour le aide à le intégration des individu dans le organisation  le objectif de ce papier être de présenter un contribution à le modélisation des individu et de son relation pour permettre le aide à le intégration des acteur dans un organisation . Nous étudier en particulier le cas du remplacement d' un acteur ( « turn-over » ) . Dans ce cadre , nous proposer un modèle regrouper un ensemble de donnée relatif à un individu , aux relation que celui _-ci entretenir avec le autre acteur et à son espace informationnel . le étude porter sur le mise en oeuvre de mécanisme d' aide fournir à un acteur le moyen de son intégration : le mise à disposition d' un image des espace informationnel et relationnel de son prédécesseur ainsi que le mise en relation de le acteur avec le autre acteur de le organisation . ce étude être mener en partenariat avec un expert en GRH . 	Modélisation des individus et de leurs relations pour l'aide à l'intégration des individus dans l'organisation	4
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation des interactions entre individus avec AgentUML	Pour faciliter l'étude de certains phénomènes, des outils de simulation ont été créés dans de nombreux domaines. L'étude du comportement humain à jusque là échappé à cette tendance. Aujourd'hui, les systèmes multi-agents couplés aux avancées des sciences humaines fournissent les bases nécessaires à l'élaboration de ce type d'outil. Cet article s'inscrit ainsi dans cette dynamique avec l'objectif de développer un outil de simulation du comportement d'individus traumatisés crâniens sur une chaîne de production. Cet outil doit permettre la collecte de la connaissance relative au système étudié et fournir une aide à la décision pour les responsables de l'entreprise. Cet article propose une modélisation des interactions entre individus dans le formalisme AgentUML. Une implémentation du modèle au sein d'un outil de simulation fonctionnel et les résultats obtenus seront également présentés. A terme, le but est la production de données de simulation exploitables par des techniques d'ECD.	Stéphane Daviet, Fabrice Guillet, Henri Briand, Adina Magda Florea, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000399	http://editions-rnti.fr/render_pdf.php?p=1000399	1627	fr	fr	@kowesis.fr, @univ-nantes.fr, @cs.pub.ro	modélisation des interaction entre individu avec AgentUML  Pour faciliter le étude de certain phénomène , un outil de simulation avoir être créer dans un nombreux domaine . le étude du comportement humain à jusque là échapper à ce tendance . Aujourd' hui , le système multi-agents coupler aux avancée des science humain fournir le base nécessaire à le élaboration de ce type d' outil . ce article clr inscrire ainsi dans ce dynamique avec le objectif de développer un outil de simulation du comportement d' individu traumatiser crânien sur un chaîne de production . ce outil devoir permettre le collecte de le connaissance relatif au système étudier et fournir un aide à le décision pour le responsable de le entreprise . ce article proposer un modélisation des interaction entre individu dans le formalisme AgentUML . un implémentation du modèle au sein d' un outil de simulation fonctionnel et le résultat obtenir être également présenter . A terme , le but être le production de donnée de simulation exploitable par un technique d' ECD . 	Modélisation des interactions entre individus avec AgentUML	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation d'objets mobiles dans un entrepôt de données	La gestion d'objets mobiles a connu un regain d'intérêt ces dernières années, particulièrement dans le but de gérer et de prédire la localisation d'objets mobiles. Cependant, il y a peu de recherches sur l'exploitation d'historiques de bases d'objets mobiles. La première étape dans ce processus est la mise en oeuvre d'un entrepôt d'objets mobiles. Seulement, les modèles d'entrepôts existants ne permettent pas de traiter directement ce type de données complexes. Cet article présente une approche originale pour pallier ce problème. Cette approche offre la puissance de l'algèbre OLAP sur toute combinaison de données classiques, spatiales et/ou temporelles et mobiles. Elle a été validée par un prototype et appliquée à l'analyse de la mobilité urbaine1. Les résultats de l'expérimentation montrent la validité de l'approche et les tests de performances son efficacité.	Tao Wan, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000284	http://editions-rnti.fr/render_pdf.php?p=1000284	1628	fr	fr	@prism.uvsq.fr, @prism.uvsq.fr	modélisation d' objet mobile dans un entrepôt de données  le gestion d' objet mobile avoir connaître un regain d' intérêt ce dernier année , particulièrement dans le but de gérer et de prédire le localisation d' objet mobile . cependant , il y avoir peu de recherche sur le exploitation d' historique de base d' objet mobile . le premier étape dans ce processus être le mise en oeuvre d' un entrepôt d' objet mobile . seulement , le modèle d' entrepôt existant ne permettre pas de traiter directement ce type de donnée complexe . ce article présenter un approche original pour pallier ce problème . ce approche offrir le puissance de le algèbre OLAP sur tout combinaison de donnée classique , spatial et temporel et mobile . Elle avoir être valider par un prototype et appliquer à le analyse de le mobilité urbaine1 . le résultat de le expérimentation montrer le validité de le approche et le test de performance son efficacité . 	Modélisation d'objets mobiles dans un entrepôt de données	7
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation d'un agent émotionnel en UML et RDF	Pouvoir extraire de la connaissance à partir d'une plate-forme de simulation est aujourd'hui envisageable en conjuguant les avancées obtenues en Intelligence Artificielle autour des systèmes multi-agents et les méthodes de formalisation et d'extraction des connaissances. C'est donc dans un cadre général de gestion des connaissances que nous proposons de modéliser un agent artificiel doté de connaissances et d'émotions. Pour cela, une expertise psychologique a été recueillie et formalisée de manière à être stockée dans une base de connaissances sous forme de règles et de classes en UML et RDF. L'implémentation du modèle permet d'entrevoir les perspectives d'une telle simulation : enrichissement par des données issues de simulations, découverte de nouvelles connaissances par l'application de processus d'ECD.	Hélène Desmier, Fabrice Guillet, Adina Magda Florea, Henri Briand, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000406	http://editions-rnti.fr/render_pdf.php?p=1000406	1629	fr	fr	@performanse.fr, @univ-nantes.fr, @cs.pub.ro	modélisation d' un agent émotionnel en UML et RDF  pouvoir extraire de le connaissance à partir d' un plate-forme de simulation être aujourd' hui envisageable en conjuguer le avancée obtenir en Intelligence Artificielle autour un système multi-agents et le méthode de formalisation et d' extraction des connaissance . C' être donc dans un cadre général de gestion des connaissance que nous proposer de modéliser un agent artificiel doter de connaissance et d' émotion . Pour cela , un expertise psychologique avoir être recueillir et formaliser de manière à être stocker dans un base de connaissance sous forme de règle et de classe en UML et RDF . le implémentation du modèle permettre d' entrevoir le perspective d' un tel simulation : enrichissement par un donnée issu de simulation , découverte de nouveau connaissance par le application de processus d' ECD . 	Modélisation d'un agent émotionnel en UML et RDF	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Motifs séquentiels flous : un peu, beaucoup, passionément	La plupart des bases de données issues du monde réel sont constituées de données numériques et historiées (données de capteurs, données scientifiques, données démographiques). Dans ce cadre les algorithmes d'extraction de motifs séquentiels, s'ils sont adaptés au caractère temporel des données ne permettent pas le traitement de données numériques. es données sont alors pré-traitées pour les transformer en données binaire, ce qui entraîne une perte d'information. Des algorithmes ont donc été proposés pour traiter les données numériques sous forme d'intervalles et d'intervalles flous notamment. En ce qui concerne la recherche de motifs séquentiels fondée sur des intervalles flous, les deux méthodes de la littérature ne sont pas satisfaisantes car incomplètes soit dans le traitement des séquences soit dans le calcul du support. Dans cet article, nous proposons donc trois méthodes d'extraction de motifs séquentiels flous {SPEEDYFUZZY, MINIFUZZY et TOTALLYFUZZY) et en détaillons les algorithmes sous-jacents en soulignant les différents niveaux de fuzzification. Ces algorithmes sont implémentés et évalués à travers différentes expérimentations menées sur des jeux de tests synthétiques.	Céline Fiot, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000357	http://editions-rnti.fr/render_pdf.php?p=1000357	1630	fr	fr		motif séquentiel flou : un peu , beaucoup , passionément  le plupart des base de donnée issu du monde réel être constituer de donnée numérique et historier ( donnée de capteur , donnée scientifique , donnée démographique ) . Dans ce cadre le algorithme d' extraction de motif séquentiel , s' ils être adapter au caractère temporel des donnée ne permettre pas le traitement de donnée numérique . être donner être alors pré-traitées pour les transformer en donnée binaire , ce qui entraîner un perte d' information . un algorithme avoir donc être proposer pour traiter le donnée numérique sous forme d' intervalle et d' intervalle flou notamment . En ce qui concerner le recherche de motif séquentiel fonder sur un intervalle flou , le deux méthode de le littérature ne être pas satisfaisant car incomplet être dans le traitement des séquence être dans le calcul du support . Dans ce article , nous proposer donc trois méthode d' extraction de motif séquentiel flou { SPEEDYFUZZY , MINIFUZZY et TOTALLYFUZZY ) et en détailler le algorithme sous-jacent en souligner le différent niveau de fuzzification . ce algorithme être implémenter et évaluer à travers différent expérimentation mener sur un jeu de test synthétique . 	Motifs séquentiels flous : un peu, beaucoup, passionément	6
Revue des Nouvelles Technologies de l'Information	EGC	2005	Notion de sémantiques bien-formées pour les règles	La notion de règles entre attributs est très générale, allant des règles d'association en fouille de données aux dépendances fonctionnelles (DF) en bases de données. Malgré cette diversité, la syntaxe des règles est toujours la même, seule leur sémantique diffère. Pour une sémantique donnée, en fonction des propriétés induites, des techniques algorithmiques sont mises en oeuvre pour découvrir les règles à partir des données. A partir d'un ensemble de règles, il est aussi utile en pratique de raisonner sur ces règles, comme cela est le cas par exemple avec les axiomes d'Armstrong pour les dépendances fonctionnelles. Dans cet article, nous proposons un cadre qui permet de s'assurer qu'une sémantique donnée pour les règles est bien-formée, i.e. les axiomes d'Armstrong sont justes et complets pour cette sémantique. Les propositions faites dans ce papier proviennent du contexte applicatif de l'analyse de données de biopuces. A partir de plusieurs sémantiques pour les données d'expression de gènes, nous montrons comment ces sémantiques s'intègrent dans le cadre présenté.	Marie Agier, Jean-Marc Petit	http://editions-rnti.fr/render_pdf.php?p1&p=1000207	http://editions-rnti.fr/render_pdf.php?p=1000207	1631	fr	fr		notion de sémantique bien-formées pour le règles  le notion de règle entre attribut être très général , aller un règle d' association en fouille de donnée aux dépendance fonctionnel ( DF ) en base de donnée . Malgré ce diversité , le syntaxe des règle être toujours le même , seul son sémantique différer . Pour un sémantique donner , en fonction des propriété induire , un technique algorithmique être mettre en oeuvre pour découvrir le règle à partir un donnée . A partir d' un ensemble de règle , il être aussi utile en pratique de raisonner sur ce règle , comme cela être le cas par exemple avec le axiome d' Armstrong pour le dépendance fonctionnel . Dans ce article , nous proposer un cadre qui permettre de clr assurer qu' un sémantique donner pour le règle être bien-formée , i.e. le axiome d' Armstrong être juste et complet pour ce sémantique . le proposition faire dans ce papier provenir du contexte applicatif de le analyse de donnée de biopuces . A partir de plusieurs sémantique pour le donnée d' expression de gène , nous montrer comment ce sémantique clr intégrer dans le cadre présenter . 	Notion de sémantiques bien-formées pour les règles	5
Revue des Nouvelles Technologies de l'Information	EGC	2005	Outil de classification et de visualisation de grands volumes de données mixtes	Nous avons conçu un outil de classification de données original que nous détaillons dans le présent article. Cet outil comporte un module de création de résumés et un module d'affichage. Le module de visualisation permet une lecture aisée des résumés grâce à une interface graphique évoluée permettant la présentation et l'exploration des résumés sous forme d'une hiérarchie de profils ou d'un tableau de profils. Chaque profil donne de manière claire les informations relatives au résumé de données correspondant. La lecture de la hiérarchie et du tableau est aussi grandement facilitée par les choix d'un ordre optimal pour la présentation des variables et des résumés.	Christophe Candillier, Noureddine Mouaddib	http://editions-rnti.fr/render_pdf.php?p1&p=1000369	http://editions-rnti.fr/render_pdf.php?p=1000369	1632	fr	fr	@univ-nantes.fr, @univ-nantes.fr	outil de classification et de visualisation de grand volume de donnée mixtes  Nous avoir concevoir un outil de classification de donnée original que nous détailler dans le présent article . ce outil comporter un module de création de résumé et un module d' affichage . le module de visualisation permettre un lecture aisé des résumer grâce à un interface graphique évoluer permettre le présentation et le exploration des résumé sous forme d' un hiérarchie de profil ou d' un tableau de profil . chaque profil donner de manière clair le information relatif au résumé de donnée correspondant . le lecture de le hiérarchie et du tableau être aussi grandement faciliter par le choix d' un ordre optimal pour le présentation des variable et des résumé . 	Outil de classification et de visualisation de grands volumes de données mixtes	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Prise en compte des « Points de Vue » pour l'annotation d'un processus d'Extraction de Connaissances à partir de Données	Dans cet article on propose une nouvelle approche qui rend explicite la notion de point de vue dans une analyse multivues issue d'un processus d'Extraction de Connaissances à partir de Données (ECD). Par point de vue, nous entendons la vision particulière d'un analyste lors de son processus ECD, vision référant à un corps de connaissances qui lui est spécifique. On cherche, d'une part, à faciliter la réutilisabilité et l'adaptabilité du processus, et d'autre part à garder une trace des points de vues sous-jacents aux analyses faites. Le processus d'ECD sera vu comme un processus de génération et de transformation de vues qui seront annotées par des métadonnées pour garder la sémantique de la connaissance extraite. Un positionnement de notre approche vis-à-vis des travaux méthodologiques du processus d'ECD sera donné. Des éléments de modélisation du processus ECD basé sur les points de vue seront décrits au niveau ontologique. Enfin, on illustrera notre approche sur l'analyse des usages d'un site web à partir des fichiers log, selon le point de vue fiabilité.	Hicham Behja, Brigitte Trousse, Abdelaziz Marzak	http://editions-rnti.fr/render_pdf.php?p1&p=1000263	http://editions-rnti.fr/render_pdf.php?p=1000263	1633	fr	fr	@inria.fr, @hotmail.com	prendre en compte des « point de Vue » pour le annotation d' un processus d' extraction de connaissance à partir de Données  Dans ce article on proposer un nouveau approche qui rendre expliciter le notion de point de vue dans un analyse multivues issue d' un processus d' extraction de connaissance à partir de Données ( ECD ) . Par point de vue , nous entendre le vision particulier d' un analyste lors de son processus ECD , vision référer à un corps de connaissance qui lui être spécifique . On chercher , d' un part , à faciliter le réutilisabilité et le adaptabilité du processus , et d' autre part à garder un trace des point de vue sous-jacent aux analyse faire . le processus d' ECD être voir comme un processus de génération et de transformation de vue qui être annoter par un métadonnées pour garder le sémantique de le connaissance extraire . un positionnement de son approche vis-à-vis un travail méthodologique du processus d' ECD être donner . un élément de modélisation du processus ECD baser sur le point de vue être décrire au niveau ontologique . enfin , on illustrer son approche sur le analyse des usage d' un site web à partir un fichier logarithme , selon le point de vue fiabilité . 	Prise en compte des « Points de Vue » pour l'annotation d'un processus d'Extraction de Connaissances à partir de Données	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Problématiques de gestion de connaissances dans le cadre de l'enseignement à distance sur l'Internet.	Le développement des réseaux à haut-débit et de l'Internet fournit un nouveau support à l'enseignement à distance. Aujourd'hui, de nombreux acteurs dans le domaine de l'enseignement ont mis en place des dispositifs de formation en ligne. Ceux-ci se composent généralement d'une sélection de matériaux organisés et présentés de manière à suivre un programme pédagogique particulier, de mécanismes de communication entre apprenants et enseignants, et d'outils de suivi des apprenants. Les plates-formes d'enseignement à distance devenant de plus en plus génériques, des nouveaux modèles ont été définis, standardisés ou normalis és, permettant la formalisation de méta-données pédagogiques ou tentant d'évaluer les connaissances acquises par les apprenants. En nous appuyant sur ces modèles, nous proposons de construire une base de connaissances, associant notamment les termes des domaines enseignés en relations à sémantique pédagogique. L'exploitation de cette base de connaissances fournit un premier niveau d'aide à l'ingénierie pédagogique, en particulier lorsque le volume de contenus en ligne est important. Des inférences mettant en jeu ces connaissances permettent alors un meilleur suivi du dispositif d'enseignement.	Romain Dailly, Christian Chervet, Rémi Lehn, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000389	http://editions-rnti.fr/render_pdf.php?p=1000389	1634	fr	fr	@univ-nantes.fr	problématique de gestion de connaissance dans le cadre de le enseignement à distance sur le Internet .  le développement des réseau à haut-débit et de le Internet fournir un nouveau support à le enseignement à distance . Aujourd' hui , de nombreux acteur dans le domaine de le enseignement avoir mettre en place des dispositif de formation en ligne . celui _-ci clr composer généralement d' un sélection de matériau organiser et présenter de manière à suivre un programme pédagogique particulier , de mécanisme de communication entre apprenant et enseignant , et d' outil de suivi des apprenant . le plate-forme d' enseignement à distance devenir de plus en plus générique , un nouveau modèle avoir être définir , standardiser ou normalis és , permettre le formalisation de méta-données pédagogique ou tenter d' évaluer le connaissance acquérir par le apprenant . En nous appuyer sur ce modèle , nous proposer de construire un base de connaissance , associer notamment le terme des domaine enseigner en relation à sémantique pédagogique . le exploitation de ce base de connaissance fournir un premier niveau d' aide à le ingénierie pédagogique , en particulier lorsque le volume de contenu en ligne être important . un inférence mettre en jeu ce connaissance permettre alors un meilleur suivi du dispositif d' enseignement . 	Problématiques de gestion de connaissances dans le cadre de l'enseignement à distance sur l'Internet.	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Processus de traitement de données radar pour la reconnaissance/identification de cibles aériennes		Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000345	http://editions-rnti.fr/render_pdf.php?p=1000345	1635	fr		@ensieta.fr	Processus de traitement de données radar pour la reconnaissance/identification de cibles aériennes 	Processus de traitement de données radar pour la reconnaissance/identification de cibles aériennes	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Raisonnement en gestion des compétences	Nous nous intéressons au raisonnement sur les compétences des ressources humaines pour simplifier leur gestion. Dans cet article, nous proposons une méthode de raisonnement pour l'aide à l'identification des compétences d'un individu. Un processus de knowledge-mining défini par analogie avec l'extraction de règles d'association en data-mining est proposé afin d'induire une base de règles à partir d'une base de connaissances sur le domaine. De plus, un prototype a été développé pour expérimenter notre approche sur un exemple académique.	Emmanuel Blanchard, Mounira Harzallah, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000385	http://editions-rnti.fr/render_pdf.php?p=1000385	1636	fr	fr	@univ-nantes.fr, @univ-nantes.fr	raisonnement en gestion des compétences  Nous nous intéresser au raisonnement sur le compétence des ressource humain pour simplifier son gestion . Dans ce article , nous proposer un méthode de raisonnement pour le aide à le identification des compétence d' un individu . un processus de knowledge-mining définir par analogie avec le extraction de règle d' association en data-mining être proposer afin d' induire un base de règle à partir d' un base de connaissance sur le domaine . De plus , un prototype avoir être développer pour expérimenter son approche sur un exemple académique . 	Raisonnement en gestion des compétences	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	RASMA : Une approche Multi-Agent pour l'amélioration de l'Algorithme des Règles d'Associations Spatiales		Hajer Baazaoui Zghal, Radhia Ben Hamed, Sami Faiz, Henda Ben Ghézala	http://editions-rnti.fr/render_pdf.php?p1&p=1000365	http://editions-rnti.fr/render_pdf.php?p=1000365	1637	fr		@riadi.rnu.tn	RASMA : Une approche Multi-Agent pour l'amélioration de l'Algorithme des Règles d'Associations Spatiales 	RASMA : Une approche Multi-Agent pour l'amélioration de l'Algorithme des Règles d'Associations Spatiales	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Réécriture de requêtes multimédias : approche basée sur l'usage d'une ontologie	Nous proposons dans cet article une stratégie de réécriture de requêtes sur des données multimédias décrites moyennant le standard MPEG-7. Ce standard se base sur XML schéma qui permet de décrire la structure des données. Cependant, aucune sémantique n'est assignée à cette structure. Nous proposons d'étendre ce standard d'une ontologie permettant d'exprimer les connaissances du domaine. Ainsi, l'ontologie sera utilisée durant l'indexation des données multimédias et la réécriture de requêtes. Le but de la réécriture de requêtes est de transformer une requête initiale en une ou plusieurs requêtes équivalentes ou sémantiquement proches compte tenu des connaissances représentées dans l'ontologie.	Samira Hammiche, Salima Benbernou, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1000306	http://editions-rnti.fr/render_pdf.php?p=1000306	1638	fr	fr	@univ-lyon1.fr	réécriture de requête multimédia : approche baser sur le usage d' un ontologie  Nous proposer dans ce article un stratégie de réécriture de requête sur un donnée multimédia décrire moyennant le standard MPEG- 7 . ce standard clr baser sur XML schéma qui permettre de décrire le structure des donnée . cependant , aucun sémantique n' être assigner à ce structure . Nous proposer d' étendre ce standard d' un ontologie permettre d' exprimer le connaissance du domaine . ainsi , le ontologie être utiliser durant le indexation des donnée multimédia et le réécriture de requête . le but de le réécriture de requête être de transformer un requête initial en un ou plusieurs requête équivalent ou sémantiquement proche compte tenir des connaissance représenter dans le ontologie . 	Réécriture de requêtes multimédias : approche basée sur l'usage d'une ontologie	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Règles de propagation pour la création d'ontologies d'annotation de ressources	L'annotation se distingue de l'indexation automatique par l'utilisation d'une ou plusieurs ontologies qui définissent un domaine global de référence permettant de cadrer et de normaliser les annotations effectuées, par ailleurs une ressource annotée doit l'être non pas par une liste de mots clefs, mais bien par une ou plusieurs ontologies. Malheureusement, il est peu réaliste de penser que les centaines de millions de ressources mises à disposition sur le Web puissent être annotées par leurs auteurs. Pour résoudre ce problème, notre démarche consiste à indexer les documents en se basant sur l'ontologie globale et ensuite propager les annotations en utilisant des documents déjà annotés pour annoter d'autres documents référencés par ceux-ci. La propagation des annotations suit des règles que nous proposons dans cet article. L'illustration est effectuée sur un corpus de livres dont le thème relève de l'informatique.	Lylia Abrouk, Pierre Pompidor, Danièle Hérin, Michel Sala	http://editions-rnti.fr/render_pdf.php?p1&p=1000305	http://editions-rnti.fr/render_pdf.php?p=1000305	1639	fr	fr	@lirmm.fr	règle de propagation pour le création d' ontologie d' annotation de ressources  le annotation clr distinguer de le indexation automatique par le utilisation d' un ou plusieurs ontologie qui définir un domaine global de référence permettre de cadrer et de normaliser le annotation effectuer , par ailleurs un ressource annoter devoir le être non pas par un liste de mot clef , mais bien par un ou plusieurs ontologie . malheureusement , il être peu réaliste de penser que le centaine de million de ressource mettre à disposition sur le Web pouvoir être annoter par son auteur . Pour résoudre ce problème , son démarche consister à indexer le document en clr baser sur le ontologie global et ensuite propager le annotation en utiliser un document déjà annoter pour annoter un autre document référencer par celui _-ci . le propagation des annotation suivre un règle que nous proposer dans ce article . le illustration être effectuer sur un corpus de livre dont le thème relever de le informatique . 	Règles de propagation pour la création d'ontologies d'annotation de ressources	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Réponses coopératives dans l'interrogation de documents RDF	Le développement du Web Sémantique a conduit à l'élaboration de standards pour la représentation des connaissances sur le Web. RDF, comme un de ces standards, est devenu une recommandation du W3C. Même s'il a été conçu pour être interprétable par l'homme et la machine (encodage XML, triplets, graphes étiquetés), RDF n'a pas été fourni avec des services d'interrogation et de raisonnement. La plupart des travaux concernant l'interrogation de documents RDF se sont concentrés sur l'usage de techniques issues de la programmation logique et sur des extensions de SQL. Nous portons un nouveau regard sur les techniques d'interrogation et de raisonnement sur les documents RDF et nous montrons que la sémantique des termes OSF (Order Sorted Features) est compatible avec la représentation isomorphique (triplets) des propositions RDF. Cette transformation permet l'ordonnancement des ressources en ontologies et, à travers ceci, des meilleurs mécanismes de réponses (par approximation et recouvrement) aux interrogations de documents RDF.	Adrian Tanasescu, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1000307	http://editions-rnti.fr/render_pdf.php?p=1000307	1640	fr	fr	@univ-lyon1.fr	réponse coopératif dans le interrogation de document RDF  le développement du Web Sémantique avoir conduire à le élaboration de standard pour le représentation des connaissance sur le Web . RDF , comme un de ce standard , être devenir un recommandation du W3C. même clr il avoir être concevoir pour être interprétable par le homme et le machine ( encodage XML , triplet , graphe étiqueter ) , RDF n' avoir pas être fournir avec un service d' interrogation et de raisonnement . le plupart des travail concernant le interrogation de document RDF clr être concentrer sur le usage de technique issue de le programmation logique et sur un extension de SQL . Nous porter un nouveau regard sur le technique d' interrogation et de raisonnement sur le document RDF et nous montrer que le sémantique des terme OSF ( Order Sorted Features ) être compatible avec le représentation isomorphique ( triplet ) des proposition RDF . ce transformation permettre le ordonnancement des ressource en ontologie et , à travers ceci , un meilleur mécanisme de réponse ( par approximation et recouvrement ) aux interrogation de document RDF . 	Réponses coopératives dans l'interrogation de documents RDF	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Restructuration automatique de documents dans les corpus semi-structurés hétérogènes	L'interrogation de grandes bases de documents semi-structurés (type XML) est un problème ouvert important. En effet, pour interroger un document dont le schéma est nouveau, un système doit pouvoir soit adapter la requête posée au document, soit adapter le document pour pouvoir lui appliquer la requête. Nous nous positionnons ici dans le cadre de la restructuration de documents qui consiste à transformer des documents semi-structurés issus de diverses sources dans un schéma de médiation connu. Nous proposons un cadre statistique général à la problématique de la restructuration de documents et détaillons une instance d'un modèle stochastique de documents structurés appliquée à cette problématique. Nous détaillons enfin un ensemble d'expériences effectuées sur les documents du corpus INEX afin de mesurer la capacité de notre modèle.	Guillaume Wisniewski, Ludovic Denoyer, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000260	http://editions-rnti.fr/render_pdf.php?p=1000260	1641	fr	fr	@lip6.fr	restructuration automatique de document dans le corpus semi-structurés hétérogènes  le interrogation de grand base de document semi-structurés ( type XML ) être un problème ouvert important . En effet , pour interroger un document dont le schéma être nouveau , un système devoir pouvoir être adapter le requête poser au document , soit adapter le document pour pouvoir lui appliquer le requête . Nous nous positionner ici dans le cadre de le restructuration de document qui consister à transformer un document semi-structurés issir de divers source dans un schéma de médiation connaître . Nous proposer un cadre statistique général à le problématique de le restructuration de document et détailler un instance d' un modèle stochastique de document structurer appliquer à ce problématique . Nous détailler enfin un ensemble d' expérience effectuer sur le document du corpus INEX afin de mesurer le capacité de son modèle . 	Restructuration automatique de documents dans les corpus semi-structurés hétérogènes	4
Revue des Nouvelles Technologies de l'Information	EGC	2005	Sélection de modèles par des méthodes à noyaux pour la classification de données séquentielles	Ce travail concerne le développement de méthodes de classification discriminantes pour des données séquentielles. Quelques techniques ont été proposées pour étendre aux séquences les méthodes discriminantes, comme les machines à vecteurs supports, par nature plus adaptées aux données en dimension fixe. Elles permettent de classifier des séquences complètes mais pas de réaliser la segmentation, qui consiste à reconnaître la séquence d'unités, phonèmes ou lettres par exemple, correspondant à un signal. En utilisant une correspondance donnée / modèle nous transformons le problème de l'apprentissage des modèles à partir de données par un problème de sélection de modèles, qui peut être attaqué via des méthodes du type machines à vecteurs supports. Nous proposons et évaluons divers noyaux pour cela et fournissons des résultats expérimentaux pour deux problèmes de classification.	Trinh Minh Tri Do, Thierry Artières, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000236	http://editions-rnti.fr/render_pdf.php?p=1000236	1642	fr	fr	@lip6.fr	sélection de modèle par un méthode à noyau pour le classification de donnée séquentielles  ce travail concerner le développement de méthode de classification discriminant pour un donnée séquentiel . quelque technique avoir être proposer pour étendre aux séquence le méthode discriminant , comme le machine à vecteur support , par nature plus adapter aux donnée en dimension fixe . Elles permettre de classifier un séquence complet mais pas de réaliser le segmentation , qui consister à reconnaître le séquence d' unité , phonème ou lettre par exemple , correspondre à un signal . En utiliser un correspondance donner  modèle nous transformer le problème de le apprentissage des modèle à partir de donnée par un problème de sélection de modèle , qui pouvoir être attaquer via un méthode du type machine à vecteur support . Nous proposer et évaluer divers noyau pour cela et fournir un résultat expérimental pour deux problème de classification . 	Sélection de modèles par des méthodes à noyaux pour la classification de données séquentielles	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Semi-supervised incremental clustering of categorical data	"Le clustering semi-supervisé combine l'apprentissage supervisé et non-supervisé pour produire meilleurs clusterings. Dans la phase initiale supervisée de l'algorithme, un échantillon d'apprentissage est produit par sélection aléatoire. On suppose que les exemples de l'échantillon d'apprentissage sont étiquetés par un attribut de classe. Puis, un algorithme incrémentiel développé pour les données catégoriques est utilisé pour produire un ensemble de clusters pur (tels que les exemple de chaque cluster ont la même étiquette), qui servent de ""seeding clusters"" pour la deuxième phase non-supervisée de l'algorithme. Dans cette phase, l'algorithme incrémentiel est appliqué aux données non étiquetées. La qualité du clustering est évaluée par l'index de Gini moyen des clusters. Les expériences démontrent que des très bons clusterings peuvent être obtenus avec des petits échantillons d'apprentissage."	Dan A. Simovici, Natima Singla	http://editions-rnti.fr/render_pdf.php?p1&p=1000246	http://editions-rnti.fr/render_pdf.php?p=1000246	1643	en	fr	@cs.umb.edu, @cs.umb.edu	" le clustering semi-supervisé combiner le apprentissage superviser et non- superviser pour produire meilleur clusterings . . Dans le phase initial superviser de le algorithme , un échantillon d' apprentissage être produire par sélection aléatoire . . On supposer que le exemple de le échantillon d' apprentissage être étiqueter par un attribut de classe . . Puis , un algorithme incrémentiel développer pour le donnée catégorique être utiliser pour produire un ensemble de clusters pur ( tel que le exemple de chaque cluster avoir le même étiquette ) , qui servir de " " seeding clusters " " pour le deuxième phase non- superviser de le algorithme . . Dans ce phase , le algorithme incrémentiel être appliquer aux donnée non étiqueter . . le qualité du clustering être évaluer par le index de Gini moyen des clusters . . le expérience démontrer que un très bon clusterings pouvoir être obtenir avec un petit échantillon d' apprentissage . " 	Semi-supervised incremental clustering of categorical data	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	SEQTREE, un outil de fouille de données séquentielles par visualisation	Dans cet article, nous présentons un outil de visualisation de séquences modélisées par des arbres de suffixes probabilistes (Prediction suffix trees - PST). Ce type d'arbre permet de représenter une chaîne de Markov d'ordre variable. Dans différentes application, il s'est avéré plus efficace qu'une chaîne de Markov d'ordre fixe avec un coût calculatoire moindre. Pour ces raisons, il nous a paru intéressant d'exploiter le caractère arborescent de ce mode de représentation non seulement d'un point de vue algorithmique, mais aussi d'un point de vue visuel.	Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1000430	http://editions-rnti.fr/render_pdf.php?p=1000430	1644	fr	fr		SEQTREE , un outil de fouille de donnée séquentiel par visualisation  Dans ce article , nous présenter un outil de visualisation de séquence modéliser par un arbre de suffixe probabiliste ( Prediction suffix trees - PST ) . ce type d' arbre permettre de représenter un chaîne de Markov d' ordre variable . Dans différent application , il clr être avérer plus efficace qu' un chaîne de Markov d' ordre fixer avec un coût calculatoire moindre . Pour ce raison , il nous avoir paraître intéressant d' exploiter le caractère arborescent de ce mode de représentation non seulement d' un point de vue algorithmique , mais aussi d' un point de vue visuel . 	SEQTREE, un outil de fouille de données séquentielles par visualisation	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	SSC : Statistical Subspace Clustering	Cet article se place dans le cadre du subspace clustering, dont la problématique est double : identifier simultanément les clusters et le sous-espace spécifique dans lequel chacun est défini, et caractériser chaque cluster par un nombre minimal de dimensions, permettant ainsi une présentation des résultats compréhensible par un expert du domaine d'application. Les méthodes proposées jusqu'à présent pour cette tâche ont le défaut de se restreindre à un cadre numérique. L'objectif de cet article est de proposer un algorithme de subspace clustering capable de traiter des données décrites à la fois par des attributs continus et des attributs catégoriels. Nous présentons une méthode basée sur l'algorithme classique EM mais opérant sur un modelé simplifié des données et suivi d'une technique originale de sélection d'attributs pour ne garder que les dimensions pertinentes de chaque cluster. Les expérimentations présentées ensuite, menées sur des bases de données aussi bien artificielles que réelles, montrent que notre algorithme présente des résultats robustes en termes de qualité de la classification et de compréhensibilité des clusters obtenus.	Laurent Candillier, Isabelle Tellier, Fabien Torre, Olivier Bousquet	http://editions-rnti.fr/render_pdf.php?p1&p=1000241	http://editions-rnti.fr/render_pdf.php?p=1000241	1645	en	fr	@univ-lille3.fr, @pertinence.com	ce article clr placer dans le cadre du subspace clustering , dont le problématique être double : identifier simultanément le clusters et le sous-espace spécifique dans lequel chacun être définir , et caractériser chaque cluster par un nombre minimal de dimension , permettre ainsi un présentation des résultat compréhensible par un expert du domaine d' application . le méthode proposer jusqu' à présent pour ce tâche avoir le défaut de clr restreindre à un cadre numérique . le objectif de ce article être de proposer un algorithme de subspace clustering capable de traiter un donnée décrire à le foi par un attribut continu et des attribut catégoriel . Nous présenter un méthode baser sur le algorithme classique EM mais opérer sur un modelé simplifier des donnée et suivre d' un technique original de sélection d' attribut pour ne garder que le dimension pertinent de chaque cluster . le expérimentation présenter ensuite , mener sur un base de donnée aussi bien artificiel que réel , montrer que son algorithme présent des résultat robuste en terme de qualité de le classification et de compréhensibilité des clusters obtenir . 	SSC : Statistical Subspace Clustering	24
Revue des Nouvelles Technologies de l'Information	EGC	2005	SVM et visualisation pour la fouille de grands ensembles de données	Nous présentons un algorithme de SVM et des méthodes graphiques pour le traitement de grands ensembles de données. Pour pouvoir traiter de tels ensembles de données, nous utilisons une représentation des données de plus haut niveau (sous forme symbolique). L'algorithme de séparateur à vaste marge (SVM) est adapté pour pouvoir traiter ce nouveau type de données. Nous construisons un nouveau noyau RBF (Radial Basis Function) que l'algorithme utilise à la fois pour la classification, la régression et la détection d'individus atypiques dans des données de type intervalle. Nous utilisons ensuite des méthodes de visualisation interactive (elles aussi adaptées au cas des variables de type intervalle) pour expliquer les résultats obtenus par les SVM. La méthode est évaluée sur des ensembles de données symboliques existant ou créés artificiellement.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000372	http://editions-rnti.fr/render_pdf.php?p=1000372	1646	fr	fr	@esiea-ouest.fr	SVM et visualisation pour le fouille de grand ensemble de données  Nous présenter un algorithme de SVM et des méthode graphique pour le traitement de grand ensemble de donnée . Pour pouvoir traiter de tel ensemble de donnée , nous utiliser un représentation des donnée de plus haut niveau ( sous forme symbolique ) . le algorithme de séparateur à vaste marge ( SVM ) être adapter pour pouvoir traiter ce nouveau type de donnée . Nous construire un nouveau noyau RBF ( Radial Basis Function ) que le algorithme utiliser à le foi pour le classification , le régression et le détection d' individu atypique dans un donnée de type intervalle . Nous utiliser ensuite un méthode de visualisation interactif ( lui aussi adapter au cas des variable de type intervalle ) pour expliquer le résultat obtenir par le SVM . le méthode être évaluer sur un ensemble de donnée symbolique existant ou créer artificiellement . 	SVM et visualisation pour la fouille de grands ensembles de données	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Tableau de Bits Indexé (TBI)  pour la Recherche de Séquences Fréquentes		Lionel Savary, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000279	http://editions-rnti.fr/render_pdf.php?p=1000279	1647	fr		@prism.uvsq.fr	Tableau de Bits Indexé (TBI)  pour la Recherche de Séquences Fréquentes 	Tableau de Bits Indexé (TBI)  pour la Recherche de Séquences Fréquentes	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	TANAGRA : un logiciel gratuit pour l'enseignement et la recherche	TANAGRA est un logiciel « open source » librement accessible sur le web, il tente de concilier deux types d'utilisation. D'une part, en proposant une interface suffisamment conviviale, il est accessible aux utilisateurs non spécialistes qui veulent effectuer des études sur des données réelles. D'autre part, en définissant une architecture simplifiée à l'extrême, les efforts de développement portent sur l'essentiel, à savoir la mise au point et l'intégration d'algorithmes de fouille de données, les chercheurs peuvent ainsi mener des expérimentations sur les méthodes. Dans cet article, nous présentons les principales fonctionnalités du logiciel en essayant de le positionner sur l'échiquier des (très) nombreux logiciels diffusés actuellement.	Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1000435	http://editions-rnti.fr/render_pdf.php?p=1000435	1648	fr	fr	@univ-lyon2.fr	tanagra : un logiciel gratuit pour le enseignement et le recherche  tanagra être un logiciel « open source » librement accessible sur le web , il tenter de concilier deux type d' utilisation . D' un part , en proposer un interface suffisamment convivial , il être accessible aux utilisateur non spécialiste qui vouloir effectuer un étude sur un donnée réel . D' autre part , en définir un architecture simplifier à le extrême , le effort de développement porter sur le essentiel , à savoir le mise au point et le intégration d' algorithme de fouille de donnée , le chercheur pouvoir ainsi mener un expérimentation sur le méthode . Dans ce article , nous présenter le principal fonctionnalité du logiciel en essayer de le positionner sur le échiquier des ( très ) nombreux logiciel diffuser actuellement . 	TANAGRA : un logiciel gratuit pour l'enseignement et la recherche	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Tendances dans les expressions de gènes :  application à l'analyse du transcriptome  de Plasmodium Falciparum	L'étude de l'expression des gènes est depuis quelques années révolutionnée par les puces à ADN. Les méthodes habituellement mises en oeuvre pour analyser ces données s'appuient sur des algorithmes de partitionnement, comme les clustering hiérarchiques, et sur une hypothèse communément admise qui associe à un ensemble de profils d'expression similaires, une fonction identique. Cette analyse étudie l'ensemble des gènes sans distinction. L'approche que nous proposons deux catégories de gènes : connus ou putatifs. Pour chaque gène n'ayant pas d'information rattachée, nous étudions son voisinage afin d'y trouver des motifs fréquents (itemsets). Ensuite, l'Analyse est guidée par l'interprétation biologique afin de faire émerger des propriétés intéressantes. Un premier jeu de test sur Plasmodium Falciparum (agent de la Malaria) nous a permis de mettre en évidence, en nous intéressant aux items relatifs à la glycolyse, un transporteur de nucléosides qui intervient au niveau énergétique dans la phase ring (précoce) du parasite.	Philippe Collet, Vincent Derozier, Gérard Dray, François Trousset, Pascal Poncelet, Michel Crampes	http://editions-rnti.fr/render_pdf.php?p1&p=1000411	http://editions-rnti.fr/render_pdf.php?p=1000411	1649	fr	fr	@ema.fr	tendance dans le expression de gène : application à le analyse du transcriptome de Plasmodium Falciparum  le étude de le expression des gène être depuis quelque année révolutionner par le puce à ADN . le méthode habituellement mettre en oeuvre pour analyser ce donnée clr appuyer sur un algorithme de partitionnement , comme le clustering hiérarchique , et sur un hypothèse communément admettre qui associer à un ensemble de profil d' expression similaire , un fonction identique . ce analyse étudier le ensemble des gène sans distinction . le approche que nous proposer deux catégorie de gène : connaître ou putatif . Pour chaque gène n' avoir pas un information rattacher , nous étudier son voisinage afin d' y trouver un motif fréquent ( itemsets ) . ensuite , le Analyse être guider par le interprétation biologique afin de faire émerger un propriété intéressant . un premier jeu de test sur Plasmodium Falciparum ( agent de le Malaria ) nous avoir permettre de mettre en évidence , en nous intéressant aux item relatif à le glycolyse , un transporteur de nucléosides qui intervenir au niveau énergétique dans le phase ring ( précoce ) du parasite . 	Tendances dans les expressions de gènes :  application à l'analyse du transcriptome  de Plasmodium Falciparum	2
Revue des Nouvelles Technologies de l'Information	EGC	2005	Un automate pour la génération complète ou partielle des concepts du treillis de Galois	"Cet article se situe dans le domaine de l'analyse formelle de concepts et du treillis de concepts (treillis de Galois) lequel est un cadre théorique intéressant pour le regroupement conceptuel des données et la génération des règles d'association. Puisque la prospection de données (data mining) est utilisée comme support à la prise de décision par des analystes rarement intéressés par la liste exhaustive (souvent très longue) des concepts et des règles, l'élaboration d'une solution approximative sera dans la plupart des cas un compromis satisfaisant et relativement moins coûteux qu'une solution exhaustive. Dans cet article, on propose une approche appelée CIGA (Closed Itemset Generation using an Automata) de génération partielle ou complète de concepts par la construction et le parcours d'un automate à états finis. La génération des concepts permet l'identification des ""itemsets"" fermés fréquents, étape cruciale pour l'extraction des règles d'association."	Ganaël Jatteau, Rokia Missaoui, Madenda Sarifuddin	http://editions-rnti.fr/render_pdf.php?p1&p=1000229	http://editions-rnti.fr/render_pdf.php?p=1000229	1650	fr	fr	@uqo.ca	un automate pour le génération complet ou partiel des concept du treillis de Galois  " ce article clr situer dans le domaine de le analyse formel de concept et du treillis de concept ( treillis de Galois ) lequel être un cadre théorique intéressant pour le regroupement conceptuel des donnée et le génération des règle d' association . . Puisque le prospection de donnée ( data mining ) être utiliser comme support à le prise de décision par un analyste rarement intéresser par le liste exhaustif ( souvent très long ) des concept et des règle , le élaboration d' un solution approximatif être dans le plupart des cas un compromis satisfaisant et relativement moins coûteux qu' un solution exhaustif . . Dans ce article , on proposer un approche appeler CIGA ( Closed Itemset Generation using an Automata ) de génération partiel ou complet de concept par le construction et le parcours d' un automate à état fini . . le génération des concept permettre le identification des " " itemsets " " fermer fréquent , étape crucial pour le extraction des règle d' association . " 	Un automate pour la génération complète ou partielle des concepts du treillis de Galois	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Un critère d'évaluation pour la sélection de variables	Cet article aborde le problème de la sélection de variables dans le cadre de la classification supervisée. Les méthodes de sélection reposent sur un algorithme de recherche et un critère d'évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables. Nous présentons un nouveau critère d'évaluation fondé sur une mesure d'ambigüité. Cette mesure est fondée sur une combinaison d'étiquettes représentant le degré de spécificité ou d'appartenance aux classes en présence. Les tests menés sur de nombreux jeux de données réels et artificiels montrent que notre méthode est capable de sélectionner les variables pertinentes et d'augmenter dans la plupart des cas les taux de bon classement.	Dahbia Semani, Carl Frélicot, Pierre Courtellemont	http://editions-rnti.fr/render_pdf.php?p1&p=1000218	http://editions-rnti.fr/render_pdf.php?p=1000218	1651	fr	fr	@univ-lr.fr	un critère d' évaluation pour le sélection de variables  ce article aborder le problème de le sélection de variable dans le cadre de le classification superviser . le méthode de sélection reposer sur un algorithme de recherche et un critère d' évaluation pour mesurer le pertinence des sous-ensemble potentiel de variable . Nous présenter un nouveau critère d' évaluation fonder sur un mesure d' ambigüité . ce mesure être fonder sur un combinaison d' étiquette représenter le degré de spécificité ou d' appartenance aux classe en présence . le test mener sur un nombreux jeu de donnée réel et artificiel montrer que son méthode être capable de sélectionner le variable pertinent et d' augmenter dans le plupart des cas le taux de bon classement . 	Un critère d'évaluation pour la sélection de variables	1
Revue des Nouvelles Technologies de l'Information	EGC	2005	Un système d'aide à la navigation dans des hypermédias	Avec le développement d'Internet et d'applications hypermédias, la construction et l'exploitation de profils ou modèles des utilisateurs deviennent capitaux dans de nombreux domaines. Pouvoir cibler un utilisateur d'un hypermédia ou d'un site web afin de lui proposer ce qu'il attend devient essentiel, par exemple lorsque l'on veut lui présenter les produits qu'il est le plus susceptible d'acheter, ou bien plus généralement à chaque fois que l'on veut éviter de noyer l'utilisateur dans un flot d'informations. Nous présentons un système d'aide à la navigation, intégrant un système de modélisation du comportement de navigation et un stratège qui met en oeuvre, en fonction du comportement détecté, une aide visant à recommander des liens particuliers.	Julien Blanchard, Bertrand Petitjean, Thierry Artières, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000272	http://editions-rnti.fr/render_pdf.php?p=1000272	1652	fr	fr	@lip6.fr	un système d' aide à le navigation dans un hypermédias  Avec le développement d' Internet et d' application hypermédias , le construction et le exploitation de profil ou modèle des utilisateur devenir capital dans un nombreux domaine . pouvoir cibler un utilisateur d' un hypermédia ou d' un site web afin de lui proposer ce qu' il attendre devenir essentiel , par exemple lorsque le on vouloir lui présenter le produit qu' il être le plus susceptible d' acheter , ou bien plus généralement à chaque foi que le on vouloir éviter de noyer le utilisateur dans un flot d' information . Nous présenter un système d' aide à le navigation , intégrer un système de modélisation du comportement de navigation et un stratège qui mettre en oeuvre , en fonction du comportement détecter , un aide viser à recommander un lien particulier . 	Un système d'aide à la navigation dans des hypermédias	6
Revue des Nouvelles Technologies de l'Information	EGC	2005	Une approche filtre pour la sélection de variables en apprentissage non supervisé	"La Sélection de Variable (SV) constitue une technique efficace pour réduire la dimension des espaces d'apprentissage et s'avère être une méthode essentielle pour le pré-traitement de données afin de supprimer les variables bruitées et/ou inutiles. Peu de méthodes de SV ont été proposées dans le cadre de l'apprentissage non supervisé, et, la plupart d'entre elles, sont des méthodes dites ""enveloppes"" nécessitant l'utilisation d'un algorithme d'apprentissage pour évaluer les sous ensembles de variables. Or, l'approche ""enveloppe"" est largement mal adaptée à une utilisation lors de cas ""réels"". En effet, d'une part ces méthodes ne sont pas indépendantes vis à vis des algorithmes d'apprentissage non supervisé qui nécessitent le plus souvent de fixer un certain nombre de paramètres ; mais surtout, il n'existe pas de critères bien adaptés à l'évaluation de la qualité d'apprentissage non supervisé dans des sous espaces différents. Nous proposons et évaluons dans ce papier une méthode ""filtre"" et donc indépendante des algorithmes d'apprentissage non supervisé. Cette méthode s'appuie sur deux indices permettant d'évaluer l'adéquation entre deux ensembles de variables (entre deux sous espaces)."	Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000209	http://editions-rnti.fr/render_pdf.php?p=1000209	1653	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	un approche filtre pour le sélection de variable en apprentissage non supervisé  " le sélection de Variable ( SV ) constituer un technique efficace pour réduire le dimension des espace d' apprentissage et clr avérer être un méthode essentiel pour le pré-traitement de donnée afin de supprimer le variable bruiter et inutile . . peu de méthode de SV avoir être proposer dans le cadre de le apprentissage non superviser , et , le plupart d' entre lui , être un méthode dire " " enveloppe " " nécessiter le utilisation d' un algorithme d' apprentissage pour évaluer le sous ensemble de variable . . Or , le approche " " enveloppe " " être largement mal adapter à un utilisation lors de cas " " réel " " . . En effet , d' un part ce méthode ne être pas indépendant vis à vis des algorithme d' apprentissage non superviser qui nécessiter le plus souvent de fixer un certain nombre de paramètre ; ; mais surtout , il n' exister pas un critère bien adapter à le évaluation de le qualité d' apprentissage non superviser dans un sous espace différent . . Nous proposer et évaluer dans ce papier un méthode " " filtre " " et donc indépendant des algorithme d' apprentissage non superviser . . ce méthode clr appuyer sur deux indice permettre d' évaluer le adéquation entre deux ensemble de variable ( entre deux sous espace ) . " 	Une approche filtre pour la sélection de variables en apprentissage non supervisé	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Une méthode d'évaluation de la pertinence des pages Web dans WebSum	Dans cet article nous présentons une méthode d'évaluation de la pertinence des pages Web retournées par un moteur de recherche.	Olfa Jenhani el Jed	http://editions-rnti.fr/render_pdf.php?p1&p=1000312	http://editions-rnti.fr/render_pdf.php?p=1000312	1654	fr	fr	@irit.fr	un méthode d' évaluation de le pertinence des page Web dans WebSum  Dans ce article nous présenter un méthode d' évaluation de le pertinence des page Web retourner par un moteur de recherche . 	Une méthode d'évaluation de la pertinence des pages Web dans WebSum	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Usage non classificatoire d'arbres de classification : enseignements d'une analyse de la participation féminine à l'emploi en Suisse	Cet article présente une application en grandeur réelle des arbres de classification dans un contexte non classificatoire. Les arbres générés visent à mettre en lumière les différences régionales dans la façon dont les femmes décident de leur participation au marché du travail. L'accent est donc mis sur la capacité descriptive plutôt que prédictive des arbres. L'application porte sur des données relatives à la participation féminine au marché du travail issues du Recensement Suisse de la Population de l'an 2000. Ce vaste ensemble de données a été analysé en deux phases. Un premier arbre exploratoire a mis en évidence la nécessité de procéder à des études séparées pour les non mères, les mères mariées ou veuves, et les mères célibataires ou divorcées. Nous nous limitons ici aux résultats de ce dernier groupe, pour lequel nous avons généré un arbre séparé pour chacune des trois régions linguistiques principales. Les arbres obtenus font apparaître des différences culturelles fondamentales entre régions. Du point de vue méthodologique, la principale difficulté de cet usage non classificatoire des arbres concerne leur validation, puisque le taux d'erreur de classification généralement retenu perd tout son sens dans ce contexte. Nous commentons cet aspect et illustrons l'usage d'alternatives plus pertinentes et facilement calculables.	Gilbert Ritschard, Pau Origoni, Fabio B. Losa	http://editions-rnti.fr/render_pdf.php?p1&p=1000172	http://editions-rnti.fr/render_pdf.php?p=1000172	1655	fr	fr	@ti.ch, @ti.ch, @unige.ch	usage non classificatoire d' arbre de classification : enseignement d' un analyse de le participation féminin à le emploi en Suisse  ce article présenter un application en grandeur réel des arbre de classification dans un contexte non classificatoire . le arbre générer viser à mettre en lumière le différence régional dans le façon dont le femme décider de son participation au marché du travail . le accent être donc mettre sur le capacité descriptif plutôt que prédictif des arbre . le application porter sur un donnée relatif à le participation féminin au marché du travail issu du recensement Suisse de le population de le an 2000 . ce vaste ensemble de donnée avoir être analyser en deux phase . un premier arbre exploratoire avoir mettre en évidence le nécessité de procéder à un étude séparer pour le non mère , le mère marier ou veuf , et le mère célibataire ou divorcé . Nous nous limiter ici aux résultat de ce dernier groupe , pour lequel nous avoir générer un arbre séparer pour chacun des trois région linguistique principal . le arbre obtenir faire apparaître un différence culturel fondamental entre région . Du point de vue méthodologique , le principal difficulté de ce usage non classificatoire des arbre concerner son validation , puisque le taux d' erreur de classification généralement retenir perdre tout son sens dans ce contexte . Nous commenter ce aspect et illustrer le usage d' alternatif plus pertinent et facilement calculable . 	Usage non classificatoire d'arbres de classification : enseignements d'une analyse de la participation féminine à l'emploi en Suisse	7
Revue des Nouvelles Technologies de l'Information	EGC	2005	Utilisation des technologies XML pour la formalisation de l'ontologie de modèles e-business	Notre travail de recherche consiste à représenter l'ontologie des modèles e-business e-BMO par le langage BM²L spécifié sur la base d'un méta-modèle XML. BM²L est comparé à d'autres langages de définition d'ontologie à savoir, RDF(S), DAML + OIL et OWL et ce, selon un framework établis sur les spécificités de cette ontologie. Aussi, introduisons nous une application Web e-BMH pour la conception et l'exploitation des modèles e-business conformément à l'ontologie.	Rim Djedidi Hannachi, Sarra Ben Lagha, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000311	http://editions-rnti.fr/render_pdf.php?p=1000311	1656	fr	fr	@alinto.com, @unil.ch, @riadi.rnu.tn	utilisation des technologie XML pour le formalisation de le ontologie de modèle e-business  son travail de recherche consister à représenter le ontologie des modèle e-business e-BMO par le langage BM²L spécifier sur le base d' un méta-modèle XML . BM²L être comparer à un autre langage de définition d' ontologie à savoir , RDF ( S ) , DAML + OIL et OWL et ce , selon un framework établir sur le spécificité de ce ontologie . aussi , introduire nous un application Web e-BMH pour le conception et le exploitation des modèle e-business conformément à le ontologie . 	Utilisation des technologies XML pour la formalisation de l'ontologie de modèles e-business	0
Revue des Nouvelles Technologies de l'Information	EGC	2005	Validation statistique des cartes de Kohonen en apprentissage supervisé	En apprentissage supervisé, la prédiction de la classe est le but ultime. Plus largement, on attend d'une bonne méthodologie d'apprentissage qu'elle permette une représentation des données susceptible de faciliter la navigation de l'utilisateur dans la base d'exemples et d'aider au choix des exemples et des variables pertinents tout en assurant une pré-diction de qualité dont on comprenne les ressorts. Différents travaux ont montré l'aptitude des graphes de voisinage issus des prédicteurs à fonder une telle méthodologie, ainsi le graphe des voisins relatifs de Toussaint. Cependant, la complexité de leur construction, en O(n3), reste élevée. Dans le cas de données volumineuses, nous proposons de substituer aux graphes de voisinage les cartes de Kohonen construites sur les prédicteurs. Après un bref rappel du principe des cartes de Kohonen en apprentissage non supervisé, nous montrons comment celles-ci peuvent fonder une stratégie d'apprentissage optimisée. Nous proposons ensuite d'évaluer la qualité de cette stratégie par une statistique originale qui est étroitement corrélée au taux d'erreur en généralisation. Différentes expérimentations montrent la faisabilité de cette approche. On dispose alors d'un critère fiable pour sélectionner les individus et les attributs pertinents.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000217	http://editions-rnti.fr/render_pdf.php?p=1000217	1657	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	validation statistique des carte de Kohonen en apprentissage supervisé  En apprentissage superviser , le prédiction de le classe être le but ultime . plus largement , on attendre d' un bon méthodologie d' apprentissage qu' elle permettre un représentation des donnée susceptible de faciliter le navigation de le utilisateur dans le base d' exemple et d' aider au choix des exemple et des variable pertinent tout en assurer un pré-diction de qualité dont on comprendre le ressort . différent travail avoir montrer le aptitude des graphe de voisinage issir des prédicteurs à fonder un tel méthodologie , ainsi le graphe des voisin relatif de Toussaint . cependant , le complexité de son construction , en O ( n3 ) , rester élever . Dans le cas de donnée volumineux , nous proposer de substituer aux graphe de voisinage le carte de Kohonen construire sur le prédicteurs . Après un bref rappel du principe des carte de Kohonen en apprentissage non superviser , nous montrer comment celui _-ci pouvoir fonder un stratégie d' apprentissage optimiser . Nous proposer ensuite d' évaluer le qualité de ce stratégie par un statistique original qui être étroitement corréler au taux d' erreur en généralisation . différent expérimentation montrer le faisabilité de ce approche . On disposer alors d' un critère fiable pour sélectionner le individu et le attribut pertinent . 	Validation statistique des cartes de Kohonen en apprentissage supervisé	3
Revue des Nouvelles Technologies de l'Information	EGC	2005	Visualisation de la perception d'un site web par ses utilisateurs.	Nous proposons dans cet article une méthode de visualisation de l'activité des utilisateurs d'un site web qui permet d'évaluer qualitativement l'adéquation entre son architecture logique et la perception de celle-ci par les internautes. Nous travaillons sur les parcours des internautes sur le site étudié, après reconstruction de ceux-ci grâce aux fichiers logs des serveurs concernés. Nous utilisons la structure logique des sites étudiés pour simplifier la représentation des parcours, en ne tenant pas compte de l'ordre de visite des catégories sémantiques du site. Les parcours simplifiés sont utilisés pour calculer une dissimilarité entre les catégories sémantiques qui sont ensuite représentées dans un plan par Multi Dimensional Scaling. Nous complétons cette visualisation d'ensemble par une représentation de l'arbre couvrant minimal des catégories sémantiques qui permet de mieux appréhender certaines interactions. Nous illustrons l'intérêt de la méthode en l'appliquant au site de l'INRIA.	Fabrice Rossi, Yves Lechevallier, Aïcha El Golli	http://editions-rnti.fr/render_pdf.php?p1&p=1000381	http://editions-rnti.fr/render_pdf.php?p=1000381	1658	fr	fr	@inria.fr	visualisation de le perception d' un site web par son utilisateur .  Nous proposer dans ce article un méthode de visualisation de le activité des utilisateur d' un site web qui permettre d' évaluer qualitativement le adéquation entre son architecture logique et le perception de celui _-ci par le internaute . Nous travailler sur le parcours des internaute sur le site étudier , après reconstruction de celui _-ci grâce aux fichier logs des serveur concerner . Nous utiliser le structure logique des site étudier pour simplifier le représentation des parcours , en ne tenir pas compter de le ordre de visite des catégorie sémantique du site . le parcours simplifier être utiliser pour calculer un dissimilarité entre le catégorie sémantique qui être ensuite représenter dans un plan par Multi Dimensional Scaling . Nous compléter ce visualisation d' ensemble par un représentation de le arbre couvrir minimal des catégorie sémantique qui permettre de mieux appréhender certain interaction . Nous illustrer le intérêt de le méthode en le appliquer au site de le INRIA . 	Visualisation de la perception d'un site web par ses utilisateurs.	11
Revue des Nouvelles Technologies de l'Information	EGC	2005	« La connaissance de la connaissance » : une réflexion sur la triangulation des analyses textuelles à partir d'un corpus spécialisé en gouvernance d'entreprise	Suite à la survenue récente de scandales financiers, la synthèse des idées mobilisables en gouvernance d'entreprise semble désormais essentielle si l'on veut sécuriser les investisseurs. Dans cette perspective, le présent projet de recherche consiste à mettre en oeuvre un panel d'outils d'analyse de données textuelles (Alceste, Syntex, Tropes-Zoom/Decision Explorer, Wordmapper, Weblex) afin d'évaluer les moyens dont peut disposer un analyste désireux d'extraire des connaissances contenues dans un ensemble d'articles académiques. La qualité de représentation du corpus dans sa globalité est tout d'abord testée. L'étude est ensuite centrée sur le concept même de connaissance, mobilisé dans la théorie de la gouvernance des entreprises. La convergence et la complémentarité des approches méthodologiques sont alors explicitées. Il en est de même pour ce qui concerne la capacité d'extraction d'une connaissance pertinente à partir des textes étudiés.	Stéphane Trébucq	http://editions-rnti.fr/render_pdf.php?p1&p=1000274	http://editions-rnti.fr/render_pdf.php?p=1000274	1659	fr	fr	@u-bordeaux4.fr	« le connaissance de le connaissance » : un réflexion sur le triangulation des analyse textuel à partir d' un corpus spécialiser en gouvernance d' entreprise  suite à le survenue récent de scandale financier , le synthèse des idée mobilisable en gouvernance d' entreprise sembler désormais essentiel si le on vouloir sécuriser le investisseur . Dans ce perspective , le présent projet de recherche consister à mettre en oeuvre un panel d' outil d' analyse de donnée textuel ( Alceste , Syntex , Tropes-Zoom  Decision Explorer , Wordmapper , Weblex ) afin d' évaluer le moyen dont pouvoir disposer un analyste désireux d' extraire un connaissance contenir dans un ensemble d' article académique . le qualité de représentation du corpus dans son globalité être tout d' abord tester . le étude être ensuite centrer sur le concept même de connaissance , mobiliser dans le théorie de le gouvernance des entreprise . le convergence et le complémentarité des approche méthodologique être alors expliciter . Il en être de même pour ce qui concerner le capacité d' extraction d' un connaissance pertinent à partir un texte étudier . 	« La connaissance de la connaissance » : une réflexion sur la triangulation des analyses textuelles à partir d'un corpus spécialisé en gouvernance d'entreprise	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	A Galois connecion semantics-based approach for deriving generic bases of association rules	"L'augmentation vertigineuse de la taille des données (textuelles ou transactionnelles) est un défi constant pour la ""scalabilité"" des techniques d'extraction des connaissances. Dans ce papier, on présente une approche pour la dérivation des bases génériques de règles associatives. Les principales caract éristiques de cette approches sont les suivantes. D'une part, l'introduction d'une structure de données appelée ""Trie-itemset"" pour le stockage de la relation en entrée. D'autre part, on utilise une méthode ""Diviser pour régner"" pour réduire le coût de construction de structures partiellement ordonnées, à partir desquelles les bases génériques de règles sont directement extraites."	Sadok Ben Yahia, Narjes Doggaz, Yahya Slimani, Jihem Rezgui	http://editions-rnti.fr/render_pdf.php?p1&p=1001033	http://editions-rnti.fr/render_pdf.php?p=1001033	1679	en	fr	@fst.rnu.tn, @yahoo.fr	" le augmentation vertigineux de le taille des donnée ( textuel ou transactionnel ) être un défi constant pour le " " scalabilité " " des technique d' extraction des connaissance . . Dans ce papier , on présenter un approche pour le dérivation des base générique de règle associatif . . le principal caract éristique de ce approche être le suivant . . D' un part , le introduction d' un structure de donnée appeler " " Trie-itemset " " pour le stockage de le relation en entrée . . D' autre part , on utiliser un méthode " " diviser pour régner " " pour réduire le coût de construction de structure partiellement ordonner , à partir desquelles le base générique de règle être directement extraire . " 	A Galois connecion semantics-based approach for deriving generic bases of association rules	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	A metric approach to supervised discretization	Nous présentons une nouvelle approche à la discrétisation supervisée des attributs continues qui se sert de l'espace métrique des partitions d'un ensemble fini. Nous discutons deux nouvelles idées fondamentales : une généralisation des techniques de discrétisation de Fayyad-Irani basée sur une distance sur des partitions, dérivée de l'entropie généralisée de Daroczy, et un nouveau critère géométrique pour arrêter l'algorithme de discrétisation. Les arbres de décision résultants sont plus petits, ont moins de feuilles, et montrent des niveaux plus élevés d'exactitude établis par la validation croisée stratifiée.	Dan A. Simovici, Richard Butterworth	http://editions-rnti.fr/render_pdf.php?p1&p=1000966	http://editions-rnti.fr/render_pdf.php?p=1000966	1680	en	fr	@cs.umb.edu, @cs.umb.edu	Nous présenter un nouveau approche à le discrétisation superviser des attribut continu qui clr servir de le espace métrique des partition d' un ensemble finir . Nous discuter deux nouveau idée fondamental : un généralisation des technique de discrétisation de Fayyad-Irani baser sur un distance sur un partition , dériver de le entropie généraliser de Daroczy , et un nouveau critère géométrique pour arrêter le algorithme de discrétisation . le arbre de décision résultants être plus petit , avoir moins de feuille , et montrer un niveau plus élever d' exactitude établir par le validation croiser stratifier . 	A metric approach to supervised discretization	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	A robust method for partitioning the values of categorical attributes	Dans le domaine de l'apprentissage supervisé, les méthodes de groupage des modalités d'un attribut symbolique permettent de construire un nouvel attribut synthétique conservant au maximum la valeur informationnelle de l'attribut initial et diminuant le nombre de modalités. Nous proposons ici une généralisation de l'algorithme de discrétisation Khiops pour le problème du groupage des modalités. L'algorithme proposé permet de contrôler a priori le risque de sur-apprentissage et d'améliorer significativement la robustesse des groupages produits. Cette caractéristique de robustesse a été obtenue en étudiant la statistique des variations du critère du Khi2 lors de regroupements de lignes d'un tableau de contingence et en modélisant le comportement statistique de l'algorithme Khiops. Des expérimentations intensives ont permis de valider cette approche et ont montré que la méthode de groupage Khiops aboutit à des groupages performants, à la fois en terme de qualité prédictive et de faible nombre de groupes.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000950	http://editions-rnti.fr/render_pdf.php?p=1000950	1681	en	fr	@francetelecom.com	Dans le domaine de le apprentissage superviser , le méthode de groupage des modalité d' un attribut symbolique permettre de construire un nouveau attribut synthétique conserver au maximum le valeur informationnel de le attribut initial et diminuer le nombre de modalité . Nous proposer ici un généralisation de le algorithme de discrétisation Khiops pour le problème du groupage des modalité . le algorithme proposer permettre de contrôler avoir priori le risque de sur-apprentissage et d' améliorer significativement le robustesse des groupage produire . ce caractéristique de robustesse avoir être obtenir en étudier le statistique des variation du critère du Khi2 lors de regroupement de ligne d' un tableau de contingence et en modéliser le comportement statistique de le algorithme Khiops . un expérimentation intensif avoir permettre de valider ce approche et avoir montrer que le méthode de groupage Khiops aboutir à un groupage performant , à le foi en terme de qualité prédictif et de faible nombre de groupe . 	A robust method for partitioning the values of categorical attributes	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Accélération de EM pour données qualitatives : études comparative de différentes versions	L'algorithme EM est très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange. L'inconvénient majeur de cet algorithme est la lenteur de sa convergence. Son application sur des tableaux de grande taille pourrait ainsi prendre énormément de temps. Afin de remédier à ce problème, nous étudions ici le comportement de plusieurs variantes connus de EM, ainsi qu'une nouvelle méthode. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification. Nous réalisons une étude comparative entre les différentes variantes sur des données simulées et réelles et proposons une stratégie d'utilisation de notre méthode qui s'avère très efficace.	Mohamed Nadif, François-Xavier Jollois	http://editions-rnti.fr/render_pdf.php?p1&p=1001019	http://editions-rnti.fr/render_pdf.php?p=1001019	1682	fr	fr	@univ-metz.fr	accélération de EM pour donnée qualitatif : étude comparatif de différent versions  le algorithme EM être très populaire et très efficace pour le estimation de paramètre d' un modèle de mélange . le inconvénient majeur de ce algorithme être le lenteur de son convergence . son application sur un tableau de grand taille pouvoir ainsi prendre énormément de temps . Afin de remédier à ce problème , nous étudier ici le comportement de plusieurs variante connaître de EM , ainsi qu' un nouveau méthode . Celles _-ci permettre d' accélérer le convergence de le algorithme , tout en obtenir un résultat similaire à celui _-ci . Dans ce travail , nous clr concentrer sur le aspect classification . Nous réaliser un étude comparatif entre le différent variante sur un donnée simuler et réel et proposer un stratégie d' utilisation de son méthode qui clr avérer très efficace . 	Accélération de EM pour données qualitatives : études comparative de différentes versions	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Acquisition de données vs gestion de connaissances  patrimoniales : le cas des vestiges du théâtre antique d'Arles	Qu'y a t'il de commun aujourd'hui entre l'acquisition de données 3D, la gestion d'informations patrimoniales, ou encore la modélisation tridimensionnelle en temps réel ? Bien peu, force est de le constater, si ce n'est que l'édifice patrimonial sert là souvent de terrain d'expérimentation. Pourtant, il ne saurait être réduit à ce seul statut : il est objet de connaissances dont l'étude doit bénéficier de différents jeux de technologies. Notre proposition, expérimentée sur des vestiges du théâtre antique d'Arles, place cet édifice au centre d'un dispositif visant à intégrer, au sein d'un système d'informations architecturales 3D en devenir, les résultats de différentes phases de son étude. Un jeu de connaissances formalisé sur l'édifice sert de dénominateur commun depuis l'acquisition de données 3D jusqu'à la représentation dans une maquette temps réel pour la toile. Cette maquette devient outil de navigation dans le jeu d'informations et de savoirs qui caractérise l'édifice.	Jean-Yves Blaise, Francesca De Domenico, Livio De Luca, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1001149	http://editions-rnti.fr/render_pdf.php?p=1001149	1683	fr	fr	@gamsau.map.archi.fr	acquisition de donnée vs gestion de connaissance patrimonial : le cas des vestige du théâtre antique d' Arles  Qu' y avoir t' il de commun aujourd' hui entre le acquisition de donnée 3D , le gestion d' information patrimonial , ou encore le modélisation tridimensionnel en temps réel ? bien peu , force être de le constater , si ce n' être que le édifice patrimonial servir là souvent de terrain d' expérimentation . pourtant , il ne saurer être réduire à ce seul statut : il être objet de connaissance dont le étude devoir bénéficier un différent jeu de technologie . son proposition , expérimenter sur un vestige du théâtre antique d' Arles , place ce édifice au centre d' un dispositif viser à intégrer , au sein d' un système d' information architectural 3D en devenir , le résultat de différent phase de son étude . un jeu de connaissance formaliser sur le édifice servir de dénominateur commun depuis le acquisition de donnée 3D jusqu' à le représentation dans un maquette temps réel pour le toile . ce maquette devenir outil de navigation dans le jeu d' information et de savoir qui caractériser le édifice . 	Acquisition de données vs gestion de connaissances  patrimoniales : le cas des vestiges du théâtre antique d'Arles	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Analyse d'information relationnelle par des graphes interactifs de grandes tailles	La découverte de connaissances à partir d'importantes masses de données hétérogènes débouche le plus souvent sur l'analyse relationnelle. La recherche d'informations stratégiques s'appuie en effet sur les liens fonctionnels et sémantiques entre documents, acteurs, terminologie et concepts d'un domaine sans oublier le paramètre temps. De nombreuses méthodes sont proposées pour identifier, analyser et visualiser les mécanismes mis à jour : analyse relationnelle, classifications supervisées et non supervisées, analyse factorielle, analyse sémantique, cartes, dendogrammes, ... Mais ces approches demandent souvent une expertise non négligeable pour être comprises et ne s'adressent donc pas aux non initiés. Par contre, la vue d'un graphe mettant en relation une ou deux classes d'éléments interdépendants est directement assimilable par tout le monde. Nous proposons donc un ensemble de visualisations interactives de graphes dont la manipulation doit permettre une découverte de connaissances intuitive et basée sur un langage graphique naturel. Nous illustrons notre propos de nombreux exemples tirés de cas réels d'analyses stratégiques qui ont permis d'évaluer cette approche sur un panel très large de données.	Saïd Karouach, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001086	http://editions-rnti.fr/render_pdf.php?p=1001086	1684	fr	fr	@irit.fr	analyse d' information relationnel par un graphe interactif de grand tailles  le découverte de connaissance à partir un important masse de donnée hétérogène déboucher le plus souvent sur le analyse relationnel . le recherche d' information stratégique clr appuyer en effet sur le lien fonctionnel et sémantique entre document , acteur , terminologie et concept d' un domaine sans oublier le paramètre temps . un nombreux méthode être proposer pour identifier , analyser et visualiser le mécanisme mettre à jour : analyse relationnel , classification superviser et non superviser , analyse factoriel , analyse sémantique , carte , dendogrammes , ... Mais ce approche demander souvent un expertise non négligeable pour être comprendre et ne clr adresser donc pas aux non initié . Par contre , le vue d' un graphe mettre en relation un ou deux classe d' élément interdépendant être directement assimilable par tout le monde . Nous proposer donc un ensemble de visualisation interactif de graphe dont le manipulation devoir permettre un découverte de connaissance intuitif et baser sur un langage graphique naturel . Nous illustrer son propos de nombreux exemple tirer de cas réel d' analyse stratégique qui avoir permettre d' évaluer ce approche sur un panel très large de donnée . 	Analyse d'information relationnelle par des graphes interactifs de grandes tailles	9
Revue des Nouvelles Technologies de l'Information	EGC	2004	Annotation automatique de documents XML	Nous proposons dans cet article un mécanisme automatique d'annotation de documents. Ce mécanisme s'appuie sur une opération de composition permettant de créer de nouveaux documents à partir de documents existants et sur un algorithme permettant d'inférer l'annotation d'un document composé à partir d'annotation de ses parties. Notre modèle est illustré par une étude de cas consacrée à la mise en commun de documents pédagogiques au format XML, dans un environnement coopératif d'enseignement à distance. Nous décrivons un prototype permettant d'annoter ces documents, et d'engendrer une description RDF contenant les annotations.	Birahim Gueye, Philippe Rigaux, Nicolas Spyratos	http://editions-rnti.fr/render_pdf.php?p1&p=1001138	http://editions-rnti.fr/render_pdf.php?p=1001138	1685	fr	fr	@lri.fr	annotation automatique de document XML  Nous proposer dans ce article un mécanisme automatique d' annotation de document . ce mécanisme clr appuyer sur un opération de composition permettre de créer un nouveau document à partir de document existant et sur un algorithme permettre d' inférer le annotation d' un document composer à partir d' annotation de son partie . son modèle être illustrer par un étude de cas consacrer à le mise en commun de document pédagogique au format XML , dans un environnement coopératif d' enseignement à distance . Nous décrire un prototype permettre d' annoter ce document , et d' engendrer un description RDF contenir le annotation . 	Annotation automatique de documents XML	4
Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage des réseaux bayésiens avec des graphes chaînés maximaux		Paul Munteanu, Mohamed Bendou	http://editions-rnti.fr/render_pdf.php?p1&p=1001093	http://editions-rnti.fr/render_pdf.php?p=1001093	1686	fr		@esiea-ouest.fr	Apprentissage des réseaux bayésiens avec des graphes chaînés maximaux 	Apprentissage des réseaux bayésiens avec des graphes chaînés maximaux	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage et optimisation conjoints : extraction de connaissances pertinentes sur les systèmes de production		Anne-Lise Huyet, Jean-Luc Paris	http://editions-rnti.fr/render_pdf.php?p1&p=1001169	http://editions-rnti.fr/render_pdf.php?p=1001169	1687	fr		@ifma.fr, @ifma.fr	Apprentissage et optimisation conjoints : extraction de connaissances pertinentes sur les systèmes de production 	Apprentissage et optimisation conjoints : extraction de connaissances pertinentes sur les systèmes de production	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage incrémental des profils dans un système de filtrage d'information	Cet article présente une méthode d'apprentissage des profils dans les systèmes de filtrage d'information. Le processus d'apprentissage est effectué d'une manière incrémentale au fur et à mesure que les informations sont filtrées et jugées par l'utilisateur. Des expérimentations effectuées sur une collection de test de référence TREC, montrent que la méthode permet effectivement l'amélioration des profils.	Mohand Boughanem, Hamid Tebri, Mohamed Tmar	http://editions-rnti.fr/render_pdf.php?p1&p=1001123	http://editions-rnti.fr/render_pdf.php?p=1001123	1688	fr	fr		apprentissage incrémental des profil dans un système de filtrage d' information  ce article présenter un méthode d' apprentissage des profil dans le système de filtrage d' information . le processus d' apprentissage être effectuer d' un manière incrémentale au fur et à mesure que le information être filtrer et juger par le utilisateur . un expérimentation effectuer sur un collection de test de référence TREC , montrer que le méthode permettre effectivement le amélioration des profil . 	Apprentissage incrémental des profils dans un système de filtrage d'information	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Approche binaire pour la génération de fortes règles d'association	Dans ce papier, nous proposons une nouvelle méthode d'extraction des règles d'association dans des bases de données relationnelles basée sur la technologie des arbres de Peano (Ptree). La structure de données utilisée pour représenter la base de données est un ensemble de Ptrees de base représentant chacun un vecteur binaire et tous ces Ptrees sont stockés dans des fichiers binaires. Nous montrons que la structure Ptree combinée avec la technique de réduction appelée élagage par support minimum produisent des règles d'association fortes et réduisent considérablement le temps de construction de l'association. En effet, notre approche présente l'avantage de ne pas effectuer des parcours coûteux de la base de données. Cette approche a été testée à travers un prototype que nous avons implémenté. Les résultats expérimentaux montrent que les règles d'association fortes sont générées dans un temps minimum comparativement à d'autres travaux.	Thabet Slimani, Boutheina Ben Yaghlane, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1001052	http://editions-rnti.fr/render_pdf.php?p=1001052	1689	fr	fr	@issatm.rnu.tn, @ihec.rnu.tn, @ihec.rnu.tn	approche binaire pour le génération de fort règle d' association  Dans ce papier , nous proposer un nouveau méthode d' extraction des règle d' association dans un base de donnée relationnel baser sur le technologie des arbre de Peano ( Ptree ) . le structure de donnée utiliser pour représenter le base de donnée être un ensemble de Ptrees de base représenter chacun un vecteur binaire et tout ce Ptrees être stocker dans un fichier binaire . Nous montrer que le structure Ptree combiner avec le technique de réduction appeler élagage par support minimum produire un règle d' association fort et réduire considérablement le temps de construction de le association . En effet , son approche présent le avantage de ne pas effectuer un parcours coûteux de le base de donnée . ce approche avoir être tester à travers un prototype que nous avoir implémenter . le résultat expérimental montrer que le règle d' association fort être générer dans un temps minimum comparativement à un autre travail . 	Approche binaire pour la génération de fortes règles d'association	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Approche innovante pour la recherche et l'extractin coopérative et dynamique d'informations sur Internet	Il existe de nombreuses techniques qui permettent de classifier les documents textuels en fonction de l'intérêt d'un utilisateur (kNN, SVM, ...). Malheureusement, l'intégration de ces méthodes dans les plates-formes de textmining est souvent très statique au cours du temps. Le but de cet article est de présenter une plate-forme de webmining dans laquelle les données hétérogènes sont représentées uniformément selon un formalisme XML/TEI et où l'utilisateur peut interagir sur les processus de récupération et d'analyse de ces données. Pour cela, les modules de traitements sont représentés par des agents fonctionnant sur la plate-forme MadKit et l'apprentissage se fait par une méthode dérivée de VSM et TFIDF utilisant un principe de listes noires pondérées permettant la reconnaissance de documents indésirables. La dynamique de la plate-forme repose principalement sur la possibilité d'ajouter à la volée des agents de traitement et de pouvoir modifier l'ordre et les paramètres d'analyse des documents.	Xavier Denis, Gaële Simon, Nicolas Chanchevrier	http://editions-rnti.fr/render_pdf.php?p1&p=1001129	http://editions-rnti.fr/render_pdf.php?p=1001129	1690	fr	fr	@operamail.com, @univ-lehavre.fr, @tiscali.fr	approche innovant pour le recherche et le extractin coopératif et dynamique d' information sur Internet  Il exister de nombreux technique qui permettre de classifier le document textuel en fonction de le intérêt d' un utilisateur ( kNN , SVM , ... ) . malheureusement , le intégration de ce méthode dans le plate-forme de textmining être souvent très statique au cour du temps . le but de ce article être de présenter un plate-forme de webmining dans laquelle le donnée hétérogène être représenter uniformément selon un formalisme XML  TEI et où le utilisateur pouvoir interagir sur le processus de récupération et d' analyse de ce donnée . Pour cela , le module de traitement être représenter par un agent fonctionner sur le plate-forme MadKit et le apprentissage clr faire par un méthode dériver de VSM et TFIDF utiliser un principe de liste noir pondérer permettre le reconnaissance de document indésirable . le dynamique de le plate-forme reposer principalement sur le possibilité d' ajouter à le volé des agent de traitement et de pouvoir modifier le ordre et le paramètre d' analyse des document . 	Approche innovante pour la recherche et l'extractin coopérative et dynamique d'informations sur Internet	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	BELUGA : un outil pour l'analyse dynamique des connaissances de la littérature scientifique d'un domaine. Première application au cas des maladies à prions	Un projet ciblé sur l'étude du domaine des maladies à prions à permis de formaliser une méthodologie commune, sociologique et informatique, de compréhension de sa dynamique par l'analyse thématique. Nous avons créé une plate forme d'indexation de notices bibliographiques dont le but est d'extraire des associations évoluant à travers des intervalles de temps. Beluga propose une chaîne de traitement basée sur l'indexation des documents en unités de base : références, auteurs, termes simples et composés, organismes. L'outil est fondé sur une double approche d'apprentissage et de visualisation qui automatise les processus d'extraction de groupes d'auteurs et de termes, et permet à l'utilisateur de revenir aux données documentaires sources. L'analyse diachronique de corpus de documents électroniques nous permet d'analyser comment la terminologie est structurée en thématiques émergentes.	Nicolas Turenne, Marc Barbier	http://editions-rnti.fr/render_pdf.php?p1&p=1001096	http://editions-rnti.fr/render_pdf.php?p=1001096	1691	fr	fr	@jouy.inra.fr, @grignon.inra.fr	beluga : un outil pour le analyse dynamique des connaissance de le littérature scientifique d' un domaine . premier application au cas des maladie à prions  un projet cibler sur le étude du domaine des maladie à prion à permis de formaliser un méthodologie commun , sociologique et informatique , de compréhension de son dynamique par le analyse thématique . Nous avoir créer un plate forme d' indexation de notice bibliographique dont le but être d' extraire un association évoluer à travers des intervalle de temps . Beluga proposer un chaîne de traitement baser sur le indexation des document en unité de base : référence , auteur , terme simple et composer , organisme . le outil être fonder sur un double approche d' apprentissage et de visualisation qui automatiser le processus d' extraction de groupe d' auteur et de terme , et permettre à le utilisateur de revenir aux donnée documentaire source . le analyse diachronique de corpus de document électronique nous permettre d' analyser comment le terminologie être structurer en thématique émergent . 	BELUGA : un outil pour l'analyse dynamique des connaissances de la littérature scientifique d'un domaine. Première application au cas des maladies à prions	8
Revue des Nouvelles Technologies de l'Information	EGC	2004	BooLoader : un chargeur efficace dédié aux bases de transactions denses	Nous nous intéressons à la représentation et au chargement de bases de transactions en mémoire. Pour cela, nous proposons d'utiliser un format condensé fondé sur les diagrammes de décision binaires et nous présentons un algorithme que nous avons implanté en un système baptisé BooLoader, pour charger des bases de transactions. Nous donnons également des résultats expérimentaux de notre système sur des bases éparses et denses.	Zahir Maazouzi, Ansaf Salleb-Aouissi, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000898	http://editions-rnti.fr/render_pdf.php?p=1000898	1692	fr	fr	@lifo.univ	BooLoader : un chargeur efficace dédier aux base de transaction denses  Nous nous intéresser à le représentation et au chargement de base de transaction en mémoire . Pour cela , nous proposer d' utiliser un format condenser fonder sur le diagramme de décision binaire et nous présenter un algorithme que nous avoir implanter en un système baptiser BooLoader , pour charger un base de transaction . Nous donner également un résultat expérimental de son système sur un base épars et dense . 	BooLoader : un chargeur efficace dédié aux bases de transactions denses	
Revue des Nouvelles Technologies de l'Information	EGC	2004	Caractérisation de signatures complexes dans des familles de protéines distantes	L'identification de signatures de protéines est un problème majeur pour la découverte de nouveaux membres dans des familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles distantes sont trop hétérogènes pour qu'on puisse identifier les régions conservées à partir des algorithmes classiques de la bioinformatique. Nous proposons une approche génétique pour la découverte de motifs hiérarchiques; l'algorithme suit une démarche descendante en s'appuyant dans une première phase sur les classes physico-chimiques des acides aminés. Les signatures sont ensuite définies par des séquences des motifs ainsi obtenus. Elles sont extraites au moyen d'un algorithme de découverte d'itemsets séquentiels où les motifs jouent le rôle d'items. Une dernière étape consiste à fouiller dans cette base d'itemsets pour n'en retenir qu'un ensemble réduit de signatures. Plusieurs stratégies sont proposés pour déterminer un ensemble optimal de signatures qui respecte des contraintes de complétude, de cardinalité et de spécificité. Nous appliquons notre démarche sur la famille des cytokines. L'analyse de la base de protéines SCOP a montré que les groupes de signatures que nous avons extrait cible spécifiquement cette famille d'intérêt.	Jérôme Mikolajczak, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1001049	http://editions-rnti.fr/render_pdf.php?p=1001049	1693	fr	fr	@nantes.inserm.fr, @nantes.inserm.fr, @univ-nantes.fr	caractérisation de signature complexe dans un famille de protéine distantes  le identification de signature de protéine être un problème majeur pour le découverte de nouveau membre dans un famille de protéine connaître . le concept de signature qui permettre de caractériser ce famille être généralement baser sur le définition de motif commun . Il clr avérer que le famille de protéine connaître . le concept de signature qui permettre de caractériser ce famille être généralement baser sur le définition de motif commun . Il clr avérer que le famille distant être trop hétérogène pour qu' on pouvoir identifier le région conserver à partir un algorithme classique de le bioinformatique . Nous proposer un approche génétique pour le découverte de motif hiérarchique ; le algorithme suivre un démarche descendant en clr appuyer dans un premier phase sur le classe physico- chimique des acide aminer . le signature être ensuite définir par un séquence des motif ainsi obtenir . Elles être extraire au moyen d' un algorithme de découverte d' itemsets séquentiel où le motif jouer le rôle d' item . un dernier étape consister à fouiller dans ce base d' itemsets pour n' en retenir qu' un ensemble réduire de signature . plusieurs stratégie être proposer pour déterminer un ensemble optimal de signature qui respecter un contrainte de complétude , de cardinalité et de spécificité . Nous appliquer son démarche sur le famille des cytokines . le analyse de le base de protéine SCOP avoir montrer que le groupe de signature que nous avoir extraire cible spécifiquement ce famille d' intérêt . 	Caractérisation de signatures complexes dans des familles de protéines distantes	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Caractérisation globale de l'exécution de jobs	La caractérisation globale de l'exécution de jobs passe par l'exploitation de mesures recueillies sur les machines en production. Afin de répondre à la problématique, il est nécessaire de tenir compte des différents types de données, ainsi que de la dualité de la caractérisation : statique et dynamique. Une solution technique répondant aux contraintes est proposée. Elle repose sur l'utilisation de SVM afin de détecter des phases, et à un niveau supérieur, à un réseau bayésien afin d'automatiser l'analyse de modèles de Markov enrichis. Ceux-ci sont introduits comme la base formelle et synthétique de description du comportement du job, aussi bien sur un système batch, que parallèle. Enfin, les résultats obtenus à l'aide d'un prototype sont discutés.	Fabrice Gadaud, Guillaume Duquesnay	http://editions-rnti.fr/render_pdf.php?p1&p=1001161	http://editions-rnti.fr/render_pdf.php?p=1001161	1694	fr	fr	@cgg.com	caractérisation global de le exécution de jobs  le caractérisation global de le exécution de job passer par le exploitation de mesure recueillir sur le machine en production . Afin de répondre à le problématique , il être nécessaire de tenir compte des différent type de donnée , ainsi que de le dualité de le caractérisation : statique et dynamique . un solution technique répondre aux contrainte être proposer . Elle reposer sur le utilisation de SVM afin de détecter un phase , et à un niveau supérieur , à un réseau bayésien afin d' automatiser le analyse de modèle de Markov enrichir . celui _-ci être introduire comme le base formel et synthétique de description du comportement du job , aussi bien sur un système batch , que parallèle . enfin , le résultat obtenir à le aide d' un prototype être discuter . 	Caractérisation globale de l'exécution de jobs	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Cartographie sémantique des connaissances à la carte		Christophe Tricot, Cristophe Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000916	http://editions-rnti.fr/render_pdf.php?p=1000916	1695	fr		@univ-savoi, @univ-savoi	Cartographie sémantique des connaissances à la carte 	Cartographie sémantique des connaissances à la carte	23
Revue des Nouvelles Technologies de l'Information	EGC	2004	Classer pour découvrir : une nouvelle méthode d'analyse du comportement de tous les utilisateurs d'un site Web	"L'analyse du comportement des utilisateurs d'un site Web est un domaine riche et complexe. Le grand nombre de méthodes d'extraction de connaissances appliquées aux logs Web, ainsi que la diversité du type de ces méthodes en est une preuve. Cependant, compte tenu de cette complexité, nous posons dans cet article la question suivante : Est-il possible de combiner des méthodes existantes pour proposer une analyse qui tire profit des résultats de plusieurs spécialités et extraire par exemple des comportements fréquents minoritaires ?Notre étude à donc porté sur une nouvelle approche hybride (issue de la classification neuronale et de la recherche de motifs séquentiels) visant à classer les navigations des utilisateurs d'un site (à l'aide de leurs résumés sémantiques) puis, pour chaque classe de navigations, d'en extraire les comportements fréquents. Notre objectif est 1) de pallier les limites de l'extraction de motifs fréquents par rapport à la quantité de données à traiter et aussi par rapport à la qualité des résultats et 2) de pallier les limites d'une première méthode d'analyse du comportement appelée ""Diviser pour Découvrir"", que nous avons proposé en 2003. Nous avons mené des expérimentations sur les logs HTTP des sites INRIA. Les résultats obtenus confirment le bien fondé de notre approche vis à vis de l'état de l'art."	Doru Tanasa, Brigitte Trousse, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1001146	http://editions-rnti.fr/render_pdf.php?p=1001146	1696	fr	fr		classer pour découvrir : un nouveau méthode d' analyse du comportement de tout le utilisateur d' un site Web  " le analyse du comportement des utilisateur d' un site Web être un domaine riche et complexe . . le grand nombre de méthode d' extraction de connaissance appliquer aux logs Web , ainsi que le diversité du type de ce méthode en être un preuve . . cependant , compter tenir de ce complexité , nous poser dans ce article le question suivant : : Est -il possible de combiner un méthode existant pour proposer un analyse qui tirer profit des résultat de plusieurs spécialité et extraire par exemple des comportement fréquent minoritaire ? ? son étude à donc porter sur un nouveau approche hybride ( issue de le classification neuronal et de le recherche de motif séquentiel ) viser à classer le navigation des utilisateur d' un site ( à le aide de son résumé sémantique ) puis , pour chaque classe de navigation , d' en extraire le comportement fréquent . . son objectif être 1 ) de pallier le limite de le extraction de motif fréquent par rapport à le quantité de donnée à traiter et aussi par rapport à le qualité des résultat et 2 ) de pallier le limite d' un premier méthode d' analyse du comportement appeler " " diviser pour Découvrir " " , que nous avoir proposer en 2003 . . Nous avoir mener un expérimentation sur le logs HTTP des site INRIA . . le résultat obtenir confirmer le bien fonder de son approche vivre à vis de le état de le article " 	Classer pour découvrir : une nouvelle méthode d'analyse du comportement de tous les utilisateurs d'un site Web	9
Revue des Nouvelles Technologies de l'Information	EGC	2004	Classification automatique d'images		Mohamed Hammami, Boulbaba Ben Amor, Liming Chen	http://editions-rnti.fr/render_pdf.php?p1&p=1001027	http://editions-rnti.fr/render_pdf.php?p=1001027	1697	fr			Classification automatique d'images 	Classification automatique d'images	15
Revue des Nouvelles Technologies de l'Information	EGC	2004	Construction de variables et arbre de décision		Gaëlle Legrand, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000996	http://editions-rnti.fr/render_pdf.php?p=1000996	1698	fr		@univ-lyon2.fr, @univ-lyon2.fr	Construction de variables et arbre de décision 	Construction de variables et arbre de décision	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Contrôle du risque multiple pour la sélection de règles d'association significatives	Les algorithmes d'extraction de règles d'association parcourent efficacement le treillis des itemsets pour constituer une base de règles admissibles à des seuils de support et de confiance, mais donnent une multitude de règles peu exploitables. Nous suggérons d'épurer de telles bases en éliminant les règles non statistiquement significatives. La multitude de tests pratiqués conduit mécaniquement à multiplier les règles sélectionnées à tort. après avoir présenté des procédures issues de la biostatistique qui contrôlent non pas le risque, mais le nombre de fausses découvertes, nous proposons BS_DF, un algorithme original fondé sur le bootstrat qui sélectionne les règles significatives en contrôlant le nombre de fausses découvertes. Des expérimentations montrer l'efficacité de ces procédures.	Elie Prudhomme, Stéphane Lallich, Olivier Teytaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001046	http://editions-rnti.fr/render_pdf.php?p=1001046	1699	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr, @artelys.com	contrôle du risque multiple pour le sélection de règle d' association significatives  le algorithme d' extraction de règle d' association parcourir efficacement le treillis des itemsets pour constituer un base de règle admissible à un seuil de support et de confiance , mais donner un multitude de règle peu exploitable . Nous suggérer d' épurer un tel base en éliminer le règle non statistiquement significatif . le multitude de test pratiquer conduire mécaniquement à multiplier le règle sélectionner à tort . après avoir présenter un procédure issu de le biostatistique qui contrôler non pas le risque , mais le nombre de faux découverte , nous proposer BS _ DF , un algorithme original fonder sur le bootstrat qui sélectionner le règle significatif en contrôler le nombre de faux découverte . un expérimentation montrer le efficacité de ce procédure . 	Contrôle du risque multiple pour la sélection de règles d'association significatives	14
Revue des Nouvelles Technologies de l'Information	EGC	2004	Découverte de régularités pour l'intégration de  données semi structurées	Cet article présente l'utilisation d'une technique de fouille de données pour aider à la spécification de vues sur des sources XML. Notre langage de vues permet d'intégrer des données XML provenant de sources hétérogènes. Cependant, la définition de motifs sur les sources permettant de spécifier les données à extraire est souvent difficile, car la structure des données n'est pas toujours connue. Nous proposons d'extraire les structures fréquentes dans les données des sources pour spécifier des motifs pertinents à utiliser dans la spécification des vues.	Pierre-Alain Laur, Xavier Baril	http://editions-rnti.fr/render_pdf.php?p1&p=1001139	http://editions-rnti.fr/render_pdf.php?p=1001139	1700	fr	fr	@lirmm.fr	découverte de régularité pour le intégration de donnée semi structurées  ce article présenter le utilisation d' un technique de fouille de donnée pour aider à le spécification de vue sur un source XML . son langage de vue permettre d' intégrer un donnée XML provenir de source hétérogène . cependant , le définition de motif sur le source permettre de spécifier le donnée à extraire être souvent difficile , car le structure des donnée n' être pas toujours connaître . Nous proposer d' extraire le structure fréquent dans le donnée des source pour spécifier un motif pertinent à utiliser dans le spécification des vue . 	Découverte de régularités pour l'intégration de  données semi structurées	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Détection par SVM - Application à la détection de Churn en téléphonie mobile prépayée		Cédric Archaux, Arnaud Martin, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001165	http://editions-rnti.fr/render_pdf.php?p=1001165	1701	fr		@bouyguestelecom.fr, @ensieta.fr	Détection par SVM - Application à la détection de Churn en téléphonie mobile prépayée 	Détection par SVM - Application à la détection de Churn en téléphonie mobile prépayée	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	ETIQ, un étiqueteur inductif convivail pour les corpus de spécialité		Ahmed Amrani, Oriane Matte-Tailliez, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001120	http://editions-rnti.fr/render_pdf.php?p=1001120	1702	fr		@esiea.fr, @lri.fr, @lri.fr	ETIQ, un étiqueteur inductif convivail pour les corpus de spécialité 	ETIQ, un étiqueteur inductif convivail pour les corpus de spécialité	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Étude de textes par leur image	Nous proposons une méthode automatique de comparaison de textes reposant sur une technique de transformation d'un texte en une image de taille donnée et l'analyse à l'aide des outils de la géométrie fractale. Nous présentons une application à l'étude d'un corpus de 90 textes longs.	Hubert Marteau, Alexandre Lefevre, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001095	http://editions-rnti.fr/render_pdf.php?p=1001095	1703	fr	fr	@univ-tours.fr, @univ-paris5.fr	Étude de texte par son image  Nous proposer un méthode automatique de comparaison de texte reposer sur un technique de transformation d' un texte en un image de taille donner et le analyse à le aide des outil de le géométrie fractal . Nous présenter un application à le étude d' un corpus de 90 texte long . 	Étude de textes par leur image	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Étude expérimentale de mesures de qualité de règles d'association	La validation des connaissances extraites d'un processus d'ECD par un expert métier nécessite de filtrer ces connaissances. Pour ce faire, de nombreuses mesures ont été proposées, chacune répondant à des besoins spécifiques. Ces mesures présentent des caractéristiques variées et parfois contradictoires qu'il convient alors d'examiner. Arguant du fait que la sélection des bonnes connaissances passe aussi par l'utilisation d'un ensemble de mesures adaptées au contexte, nous présentons dans cet article une étude expérimentale de différentes mesures. Cette étude est mise en regard d'une étude formelle synthétisant les qualités des mesures.	Benoît Vaillant, Philippe Lenca, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001056	http://editions-rnti.fr/render_pdf.php?p=1001056	1704	fr	fr	@enst	Étude expérimental de mesure de qualité de règle d' association  le validation des connaissance extraire d' un processus d' ECD par un expert métier nécessiter de filtrer ce connaissance . Pour ce faire , un nombreux mesure avoir être proposer , chacun répondre à un besoin spécifique . ce mesure présenter un caractéristique varier et parfois contradictoire qu' il convenir alors d' examiner . arguer du fait que le sélection des bon connaissance passer aussi par le utilisation d' un ensemble de mesure adapter au contexte , nous présenter dans ce article un étude expérimental de différent mesure . ce étude être mettre en regard d' un étude formel synthétiser le qualité des mesure . 	Étude expérimentale de mesures de qualité de règles d'association	7
Revue des Nouvelles Technologies de l'Information	EGC	2004	EXIT : EXtraction Itérative de la Terminologie		Mathieu Roche, Thomas Heitz, Oriane Matte-Tailliez, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001121	http://editions-rnti.fr/render_pdf.php?p=1001121	1705	fr			EXIT : EXtraction Itérative de la Terminologie 	EXIT : EXtraction Itérative de la Terminologie	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction de connaissances grâce à un outil de text-mining. Application à la veille informationnelle dans le cadre policier		Marc Borry, Annick Castiaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001167	http://editions-rnti.fr/render_pdf.php?p=1001167	1706	fr		@fundp.ac.be, @intelstrat.net	Extraction de connaissances grâce à un outil de text-mining. Application à la veille informationnelle dans le cadre policier 	Extraction de connaissances grâce à un outil de text-mining. Application à la veille informationnelle dans le cadre policier	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction de processus fonctionnels en génétique des  microbes à partir de résumés MEDLINE	Après l'ère du décodage des génomes, les biologistes sont de plus en plus confrontés à l'intégration de myriades de connaissances parcellaires, stockées majoritairement sous forme textuelle. Nous montrons, à travers un exemple concret, que la conjonction de deux chaînes de traitement faisant appel de façon modérée à l'expertise humaine offre au biologiste une aide utile pour parcourir cette littérature, à partir d'une structuration sans a priori de son corpus ; il s'agit ici de résumés Medline indexés par les gènes et protéines qu'ils citent, et que l'algorithme structure (sans superviseur) en principales voies métaboliques et de régulation présentes dans le corpus choisi. 1) Une chaîne d'indexation par les noms de gènes et protéines inclut un expert pour valider, 2) Un environnement interactif de clustering thématique attribue des valeurs graduées de centralité dans chaque thème aux résumés comme aux noms, comme à toute autre variable illustrative (autres termes bio., MeSH, ...).	Alain Lelu, Philippe Bessières, Alain Zasadzinski, Dominique Besagni	http://editions-rnti.fr/render_pdf.php?p1&p=1001107	http://editions-rnti.fr/render_pdf.php?p=1001107	1707	fr	fr	@jouy.inra.fr, @jouy.inra.fr, @inist.fr, @inist.fr	extraction de processus fonctionnel en génétique des microbe à partir de résumé MEDLINE  Après le ère du décodage des génome , le biologiste être de plus en plus confronter à le intégration de myriade de connaissance parcellaire , stocker majoritairement sous forme textuel . Nous montrer , à travers un exemple concret , que le conjonction de deux chaîne de traitement faire appel de façon modérer à le expertise humain offrir au biologiste un aide utile pour parcourir ce littérature , à partir d' un structuration sans avoir priori de son corpus ; il clr agir ici de résumé Medline indexer par le gène et protéine qu' ils citer , et que le algorithme structure ( sans superviseur ) en principal voie métabolique et de régulation présent dans le corpus choisi. 1 ) un chaîne d' indexation par le nom de gène et protéine inclure un expert pour valider , 2 ) un environnement interactif de clustering thématique attribuer un valeur graduer de centralité dans chaque thème aux résumé comme aux nom , comme à tout autre variable illustratif ( autre terme bio. , MeSH , ... ) . 	Extraction de processus fonctionnels en génétique des  microbes à partir de résumés MEDLINE	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction of text summary using latent semantic indexing and information retrieval technique : comparison of four strategies	In this paper, we present four generic text summarization techniques. Each technique extracts a text summary by ranking and extracting sentences from an original document. The first method, SUMMARIZER 1, uses standard information retrieval (IR) methods to rank sentences. The second method, SUMMARIZER 2, uses the Latent Semantic Analysis (LSA) technique to identify semantically important sentences, for summary creations. The third method, SUMMARIZER 3, uses a combination of the latent semantic analysis technique, reduction and relevance measure. The fourth method simply uses the TF*IDF (Term frequency * Inverse Document frequency) weighting scheme. Evaluations of the four methods are conducted using Document Understanding Conferences (DUC) datasets from NIST. We have compared the summary of each method with the manual summaries. Summarizer 4, with its lowest overhead, has comparable performance to summarizer 1. Analysis shows that a combination of LSA technique and the relevance measure (Summarizer 3) has the best performance on an average.	Abdelghani Bellaachia, Anand Mahajan	http://editions-rnti.fr/render_pdf.php?p1&p=1001111	http://editions-rnti.fr/render_pdf.php?p=1001111	1708	en	en	@gwu.edu	extraction text summary used latent semantic indexing information retrieval technique comparison four strategy paper present four generic text summarization technique technique extract text summary ranking extract sentence original document first method summarizer 1 used standard information retrieval ir method rank sentence second method summarizer 2 used latent semantic analysis lsa technique identify semantically important sentence summary creation third method summarizer 3 used combination latent semantic analysis technique reduction relevance measure fourth method simply used tf idf term frequency inverse document frequency weighting scheme evaluation four method conduct used document understanding conference duc dataset nist compare summary method manual summary summarizer 4 lowest overhead comparable performance summarizer 1 analysis show combination lsa technique relevance measure summarizer 3 best performance average	Extraction of text summary using latent semantic indexing and information retrieval technique : comparison of four strategies	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fonctionnement d'un Système Informatique d'Aide à la Décision (SIAD)	Cet article présente le fonctionnement d'un SIAD : alimentation, traitement des données qui l'alimentent, production des résultats, outils de consultation mis à la disposition des utilisateurs, exploitation éditoriale.	Michel Volle	http://editions-rnti.fr/render_pdf.php?p1&p=1000886	http://editions-rnti.fr/render_pdf.php?p=1000886	1709	fr	fr	@volle.com	fonctionnement d' un système informatique d' aide à le Décision ( SIAD )  ce article présent le fonctionnement d' un SIAD : alimentation , traitement des donnée qui l' alimenter , production des résultat , outil de consultation mettre à le disposition des utilisateur , exploitation éditorial . 	Fonctionnement d'un Système Informatique d'Aide à la Décision (SIAD)	4
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fonctions d'oubli dans les entrpôts de données	Les entrepôts de données stockent des quantités de données de plus en plus massives, en particulier du fait de la constitution d'historiques. Nous proposons ici une solution pour éviter la saturation des entrepôts de données. Nous définissons un langage de spécifications de fonctions d'oubli des données les plus anciennes, permettant de déterminer ce qui doit être présent dans l'entrepôt de données à chaque instant. Ces spécifications de fonctions d'oubli se traduisent par des opérations de résumé par agrégation, et par des opérations de suppression des données anciennes réalisées de façon mécanique à chaque pas de mise à jour. La communication présente tout d'abord une description syntaxique du langage de spécifications des fonctions d'oubli. Les contraintes à vérifier pour assurer la cohérence du langage sont ensuite décrites. Enfin, nous proposons des structures de données adaptées au stockage des données nécessaires à la gestion des fonctions d'oubli.	Aliou Boly, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000891	http://editions-rnti.fr/render_pdf.php?p=1000891	1710	fr	fr	@enst.fr, @enst.fr, @edf.fr	fonction d' oubli dans le entrpôts de données  le entrepôt de donnée stocker un quantité de donnée de plus en plus massif , en particulier du fait de le constitution d' historique . Nous proposer ici un solution pour éviter le saturation des entrepôt de donnée . Nous définir un langage de spécification de fonction d' oubli des donnée le plus ancien , permettre de déterminer ce qui devoir être présent dans le entrepôt de donnée à chaque instant . ce spécification de fonction d' oubli clr traduire par un opération de résumé par agrégation , et par un opération de suppression des donnée ancien réaliser de façon mécanique à chaque pas de mise à jour . le communication présent tout d' abord un description syntaxique du langage de spécification des fonction d' oubli . le contrainte à vérifier pour assurer le cohérence du langage être ensuite décrire . enfin , nous proposer un structure de donnée adapter au stockage des donnée nécessaire à le gestion des fonction d' oubli . 	Fonctions d'oubli dans les entrpôts de données	
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fouille dans la structure de documents XML	La prolifération des documents XML appelle des techniques appropriées pour extraire et exploiter l'information contenue dans ces documents. On distingue deux approches de fouille : XML Content Mining portant sur le contenu et XML Structure Lining qui a trait à la structure des documents. Combiner ces deux approches est très intéressant. Les informations contenues dans la structure orientent la fouille sur le contenu. Nous présentons la première étape de cette démarche : une nouvelle méthode d'extraction des règles d'association à partir de la structure des documents XML qui permet de gérer les aspects hiérarchiques de ces documents tout en améliorant les mécanismes d'extraction grâce à la création d'une structure spéciale représentant la hiérarchie des balises rencontrées.	Amandine Duffoux, Omar Boussaid, Stéphane Lallich, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1001134	http://editions-rnti.fr/render_pdf.php?p=1001134	1711	fr	fr	@univ-lyon2.fr	fouille dans le structure de document XML  le prolifération des document XML appeler un technique approprier pour extraire et exploiter le information contenir dans ce document . On distinguer deux approche de fouille : xml Content Mining porter sur le contenu et xml Structure Lining qui avoir trait à le structure des document . combiner ce deux approche être très intéressant . le information contenir dans le structure orienter le fouille sur le contenu . Nous présenter le premier étape de ce démarche : un nouveau méthode d' extraction des règle d' association à partir de le structure des document XML qui permettre de gérer le aspect hiérarchique de ce document tout en améliorer le mécanisme d' extraction grâce à le création d' un structure spécial représenter le hiérarchie des balise rencontrer . 	Fouille dans la structure de documents XML	4
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fouille de grands ensembles de données avec un boosting proximal SVM	Les SVM (support vector machines) ont montré leur efficacité dans plusieurs domaines d'application. L'apprentissage des SVM se ramène à résoudre un programme quadratique, dont la mise en oeuvre est en général coûteuse en temps. Une reformulation plus récente des SVM (proximal SVM), proposée par Fung et Mangasarian, ne nécessite que la résolution d'un système linéaire, cet algorithme de PSVM est plus efficace et permet de traiter des données dont le nombre d'individus est très important (109) et le nombre d'attributs plus restreint (104). Nous proposons d'utiliser la formule de Sherman-Morrison-Woodbury pour adapter le PSVM à la fouille d'ensembles de données dont le nombre d'attributs est très important et le nombre d'individus plus restreint sur un matériel standard. Puis nous présentons un algorithme de boosting de PSVM pour classifier des données de très grandes tailles en nombre d'individus et d'attributs. Nous évaluons les performances du nouvel algorithme sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et Ndc.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001011	http://editions-rnti.fr/render_pdf.php?p=1001011	1712	fr	fr	@esiea-ouest.fr	fouille de grand ensemble de donnée avec un boosting proximal SVM  le SVM ( support vector machine ) avoir montrer son efficacité dans plusieurs domaine d' application . le apprentissage des SVM clr ramener à résoudre un programme quadratique , dont le mise en oeuvre être en général coûteux en temps . un reformulation plus récent des SVM ( proximal SVM ) , proposer par Fung et Mangasarian , ne nécessiter que le résolution d' un système linéaire , ce algorithme de PSVM être plus efficace et permettre de traiter un donnée dont le nombre d' individu être très important ( 109 ) et le nombre d' attribut plus restreindre ( 104 ) . Nous proposer d' utiliser le formule de Sherman-Morrison-Woodbury pour adapter le PSVM à le fouille d' ensemble de donnée dont le nombre d' attribut être très important et le nombre d' individu plus restreindre sur un matériel standard . Puis nous présenter un algorithme de boosting de PSVM pour classifier un donnée de très grand taille en nombre d' individu et d' attribut . Nous évaluer le performance du nouveau algorithme sur le ensemble de donnée de le UCI , Twonorm , Ringnorm , Reuters- 21578 et Ndc . 	Fouille de grands ensembles de données avec un boosting proximal SVM	
Revue des Nouvelles Technologies de l'Information	EGC	2004	Gestion de données hétérogènes dans un entrepôt de données		Laurence Duval	http://editions-rnti.fr/render_pdf.php?p1&p=1000907	http://editions-rnti.fr/render_pdf.php?p=1000907	1713	fr		@ensai.fr	Gestion de données hétérogènes dans un entrepôt de données 	Gestion de données hétérogènes dans un entrepôt de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	GVSR : un annuaire de logiciels de manipulation et d'édition de graphes		Bruno Pinaud, Pascale Kuntz, Magalie Delépine, Julien Barberet	http://editions-rnti.fr/render_pdf.php?p1&p=1001094	http://editions-rnti.fr/render_pdf.php?p=1001094	1714	fr		@univ-nantes.fr	GVSR : un annuaire de logiciels de manipulation et d'édition de graphes 	GVSR : un annuaire de logiciels de manipulation et d'édition de graphes	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	How well go Lattice algorithms on currently used machine leaning TestBeds ?	Many research papers in classification or association rules increase the interest of Concept lattices structures for data mining (DM) and machine learning (ML). To increase the efficiency of concept lattice-bases algorithms in ML, it is necessary to make us of an efficient algorithms to build concept lattices. In fact, more than ten algorithms for generating concept lattices were published. As real data sets for data mining are very large, concept lattice structure suffers form its complexity issues on such data. The efficiency and performance of concept lattices algorithms are very different from one to another. So we need to compare the existing lattice algorithms with large data. We implemented the four first algorithms in Java environment and compared these algorithms on about 30 datasets of the UCI repository that are well established to be used to compare ML algorithms. Preliminary results give preference to Ganter's algorithm, and then to Bordat's algorithm, which do not fil well with the recommendations of Kuznetsov and Obiedkov. Furthermore, we analyzed the duality of lattice-based algorithms.	Huaiguo Fu, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001078	http://editions-rnti.fr/render_pdf.php?p=1001078	1715	en	en	@univ-artois.fr	well go lattice algorithm currently used machine lean testbed many research paper classification association rule increase interest concept lattice structure datum mining dm machine learn ml increase efficiency concept lattice basis algorithm ml necessary make us efficient algorithms build concept lattice fact ten algorithm generate concept lattice published real datum set datum mining large concept lattice structure suffer form complexity issue data efficiency performance concept lattice algorithm different one another need compare exist lattice algorithms large data implement four first algorithms java environment compare algorithm 30 dataset uci repository well establish used compare ml algorithm preliminary result give preference ganter s algorithm bordat s algorithm fil well recommendation kuznetsov obiedkov furthermore analyze duality lattice base algorithms	How well go Lattice algorithms on currently used machine leaning TestBeds ?	16
Revue des Nouvelles Technologies de l'Information	EGC	2004	Identification de blocs homogènes sur des données continues	Contrairement aux méthodes usuelles de classification ne cherchant généralement qu'une seule partition, soit des instances, soit des attributs, les méthodes de classification croisée et de classification directe fournissent des blocs de données liant des instances à des attributs. Les premières consistent à chercher simultanément une partition en lignes et une partition en colonnes. Les secondes, elles, s'appliquent directement sur les données, et permettent d'obtenir des blocs de données homogènes de toutes tailles, ainsi que des hiérarchies de classes en lignes et en colonnes. Combinant les avantages des deux méthodes, nous présentons ici une méthodologie permettant de travailler sur de grandes bases de données.	François-Xavier Jollois, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1001015	http://editions-rnti.fr/render_pdf.php?p=1001015	1716	fr	fr	@univ-metz.fr	identification de bloc homogène sur un donnée continues  contrairement aux méthode usuel de classification ne chercher généralement qu' un seul partition , soit un instance , soit un attribut , le méthode de classification croiser et de classification direct fournir un bloc de donnée liant des instance à un attribut . le premier consister à chercher simultanément un partition en ligne et un partition en colonne . le second , lui , clr appliquer directement sur le donnée , et permettre d' obtenir un bloc de donnée homogène de tout taille , ainsi que un hiérarchie de classe en ligne et en colonne . combiner le avantage des deux méthode , nous présenter ici un méthodologie permettre de travailler sur un grand base de donnée . 	Identification de blocs homogènes sur des données continues	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Induction extensionnelle : définition et application à l'acquisition de concepts à partir de textes	"Lorsque des outils inductifs sont inclus dans un système d'acquisition des connaissances, on dit que l'on construit un système apprenti. C'est dans le but de soulager la charge de travail de l'expert du domaine que cette forme d'apprentissage comporte des outils inductifs. La difficulté tient en ce que l'énumération des connaissances expertes produit des données peu bruitées mais très incomplètes que les itérations successives d'induction vont compléter, toutefois en y ajoutant de grandes quantités de bruit. Il en résulte qu'on doit utiliser des procédures inductives spéciales, adaptées à l'apprentissage par croissance de noyaux de connaissance supervisée. En particulier, pour résoudre le problème difficile de la reconnaissance de concepts dans les textes, nous avons défini une forme d'apprentissage qui intègre l'apprentissage à partir d'instances et les systèmes apprentis, que nous nommons ""Induction Extensionnelle"", un oxymoron qui souligne que malgré l'absence de création d'un modèle explicite, une induction prend effectivement place."	Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001017	http://editions-rnti.fr/render_pdf.php?p=1001017	1717	fr	fr	@lri.fr	induction extensionnelle : définition et application à le acquisition de concept à partir de textes  " Lorsque des outil inductif être inclure dans un système d' acquisition des connaissance , on dire que le on construire un système apprenti . . C' être dans le but de soulager le charge de travail de le expert du domaine que ce forme d' apprentissage comporter un outil inductif . . le difficulté tenir en ce que le énumération des connaissance expert produire un donnée peu bruiter mais très incomplet que le itération successif d' induction aller compléter , toutefois en y ajouter un grand quantité de bruit . . Il en résulter qu' on devoir utiliser un procédure inductif spécial , adapter à le apprentissage par croissance de noyau de connaissance superviser . . En particulier , pour résoudre le problème difficile de le reconnaissance de concept dans le texte , nous avoir définir un forme d' apprentissage qui intégrer le apprentissage à partir d' instance et le système apprenti , que nous nommer " " Induction Extensionnelle " " , un oxymoron qui souligner que malgré le absence de création d' un modèle expliciter , un induction prendre effectivement place . " 	Induction extensionnelle : définition et application à l'acquisition de concepts à partir de textes	6
Revue des Nouvelles Technologies de l'Information	EGC	2004	Intégration efficace de méthodes  de fouille de données dans les SGBD	Cet article présente une nouvelle approche permettant d'appliquer des algorithmes de fouille, en particulier d'apprentissage supervisé, à de grandes bases de données et en des temps de traitement acceptables. Cet objectif est atteint en intégrant ces algorithmes dans un SGBD. Ainsi, nous ne sommes limités que par la taille du disque et plus par celle de la mémoire. Cependant, les entrées-sorties nécessaires pour accéder à la base engendrent des temps de traitement longs. Nous proposons donc dans cet article une méthode originale pour réduire la taille de la base d'apprentissage en construisant sa table de contingence. Les algorithmes d'apprentissage sont alors adaptés pour s'appliquer à la table de contingence. Afin de valider notre approche, nous avons implémenté la méthode de construction d'arbre de décision ID3 et montré que l'utilisation de la table de contingence permet d'obtenir des temps de traitements équivalents à ceux des logiciels classiques.	Cédric Udréa, Fadila Bentayeb, Jérôme Darmont, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000899	http://editions-rnti.fr/render_pdf.php?p=1000899	1718	fr	fr	@univ-lyon2.fr	intégration efficace de méthode de fouille de donnée dans le SGBD  ce article présenter un nouveau approche permettre d' appliquer un algorithme de fouille , en particulier d' apprentissage superviser , à un grand base de donnée et en un temps de traitement acceptable . ce objectif être atteindre en intégrer ce algorithme dans un SGBD . ainsi , nous ne sommer limiter que par le taille du disque et plus par celui de le mémoire . cependant , le entrées-sorties nécessaire pour accéder à le base engendrer un temps de traitement long . Nous proposer donc dans ce article un méthode original pour réduire le taille de le base d' apprentissage en construire son table de contingence . le algorithme d' apprentissage être alors adapter pour clr appliquer à le table de contingence . Afin de valider son approche , nous avoir implémenter le méthode de construction d' arbre de décision ID3 et montrer que le utilisation de le table de contingence permettre d' obtenir un temps de traitement équivalent à celui des logiciel classique . 	Intégration efficace de méthodes  de fouille de données dans les SGBD	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Interrogation de sources biomédicales : prise en compte des préférences de l'utilisateur	Nous nous plaçons dans le cadre d'un projet de constitution d'une plate-forme intégrative de données biomédicales pour l'étude génomique des cancers. La plate-forme comporte, entre autres, un certain nombre de scénarios d'analyse qui sont proposés à l'utilisateur. A chaque étape d'un scénario qu'il a choisi de réaliser pour les besoins de son étude, l'utilisateur peut être amené à poser une requête nécessitant d'accéder à différentes sources et il doit alors choisir les sources pertinentes. Nous proposons un guide à l'utilisateur sous forme d'un algorithme de sélection de sources adapté à sa requête et à ses préférences. Pour cela, nous explorons quelques spécificités des banques de données biomédicales et définissons différents critères de préférence utiles pour les biologistes. Nous illustrons notre démarche avec un exemple de requête biomédicale.	Sarah Cohen Boulakia, Christine Froidevaux, Séverine Lair	http://editions-rnti.fr/render_pdf.php?p1&p=1000893	http://editions-rnti.fr/render_pdf.php?p=1000893	1719	fr	fr	@lri.fr, @curie.fr	interrogation de source biomédical : prise en compte des préférence de le utilisateur  Nous nous placer dans le cadre d' un projet de constitution d' un plate-forme intégrative de donnée biomédical pour le étude génomique des cancer . le plate-forme comporter , entre autre , un certain nombre de scénario d' analyse qui être proposer à le utilisateur . A chaque étape d' un scénario qu' il avoir choisir de réaliser pour le besoin de son étude , le utilisateur pouvoir être amener à poser un requête nécessiter d' accéder à différent source et il devoir alors choisir le source pertinent . Nous proposer un guide à le utilisateur sous forme d' un algorithme de sélection de source adapter à son requête et à son préférence . Pour cela , nous explorer quelque spécificité des banque de donnée biomédical et définir différent critère de préférence utile pour le biologiste . Nous illustrer son démarche avec un exemple de requête biomédical . 	Interrogation de sources biomédicales : prise en compte des préférences de l'utilisateur	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Le e-lien une solution pour l'extraction et le partage de connaissances structurées dans les documents hypertextuels		Gilles Verley, Jean-Pierre Asselin de Beauville	http://editions-rnti.fr/render_pdf.php?p1&p=1000917	http://editions-rnti.fr/render_pdf.php?p=1000917	1720	fr		@univ-tours.fr, @auf.org	Le e-lien une solution pour l'extraction et le partage de connaissances structurées dans les documents hypertextuels 	Le e-lien une solution pour l'extraction et le partage de connaissances structurées dans les documents hypertextuels	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Les règles d'association comme outil de catégorisation textuelle		Simon Jaillet, Maguelonne Teisseire, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1001127	http://editions-rnti.fr/render_pdf.php?p=1001127	1721	fr			Les règles d'association comme outil de catégorisation textuelle 	Les règles d'association comme outil de catégorisation textuelle	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Maintenance de bases de connaissances terminologiques	L'acquisition des connaissances terminologiques de l'entreprise se fait souvent à partir des textes qu'elle utilise. Dans le cadre de ce travail, la base de connaissances terminologiques repose sur la modélisation des concepts-métier sous la forme d'une ontologie. Le problème de la maintenance de cette base et de cette ontologie doit alors être traité.Dans cet article, après avoir donné une définition d'une base de connaissances terminologiques (BCT) et des problèmes de diachronie, nous présentons notre modèle et notre méthode d'acquisition des connaissances terminologiques de l'entreprise. Nous exposons alors notre proposition pour maintenir au cours du temps la base de connaissances terminologiques ainsi construite.Nous illustrons ce travail sur une base de connaissance terminologique sur le cinéma d'animation en décrivant le problème de la maintenance dans une reconstitution historique de différents états de cette base lors de l'apparition des techniques numériques d'animation.	Daniel Beauchêne, Christophe Roche, Cécile Million-Rousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000910	http://editions-rnti.fr/render_pdf.php?p=1000910	1722	fr	fr	@univ-savoi, @univ-savoi, @ontologos-corp.com	maintenance de base de connaissance terminologiques  le acquisition des connaissance terminologique de le entreprise clr faire souvent à partir un texte qu' elle utiliser . Dans le cadre de ce travail , le base de connaissance terminologique reposer sur le modélisation des concepts-métier sous le forme d' un ontologie . le problème de le maintenance de ce base et de ce ontologie devoir alors être traiter . Dans ce article , après avoir donner un définition d' un base de connaissance terminologique ( BCT ) et des problème de diachronie , nous présenter son modèle et son méthode d' acquisition des connaissance terminologique de le entreprise . Nous exposer alors son proposition pour maintenir au cour du temps le base de connaissance terminologique ainsi construire . Nous illustrer ce travail sur un base de connaissance terminologique sur le cinéma d' animation en décrire le problème de le maintenance dans un reconstitution historique de différent état de ce base lors de le apparition des technique numérique d' animation . 	Maintenance de bases de connaissances terminologiques	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Manipulation de représentations de cubes de données		Arnaud Giacometti, Patrick Marcel, Hassina Mouloudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000908	http://editions-rnti.fr/render_pdf.php?p=1000908	1723	fr		@univ-tours.fr, @univ-tours.fr	Manipulation de représentations de cubes de données 	Manipulation de représentations de cubes de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mediating the Semantic Web	Cet article développe une extension d'une architecture de médiation pour intégrer le Web sémantique. Plus précisément, XLive est un médiateur tout XML développé à PRiSM. Il permet d'exécuter des XQuery sur des sources de données hétérogènes. Après une rapide présentation de XLive et du Web sémantique, une architecture à trois niveaux d'ontologies et de schémas est introduite pour connecter des adaptateurs pour le Web sémantique. Cette architecture vise à intégrer des sources de type Web service d'information conformément à une ontologie globale de référence. Elle conduit à étendre XLive avec le support de vues, un outil de conception de vues et de mappings, et des adaptateurs pour les Web services.	Georges Gardarin, Tuyet-Tram Dang-Ngoc	http://editions-rnti.fr/render_pdf.php?p1&p=1000883	http://editions-rnti.fr/render_pdf.php?p=1000883	1724	en	fr	@gardarin.org, @prism.uvsq.fr	ce article développer un extension d' un architecture de médiation pour intégrer le Web sémantique . plus précisément , XLive être un médiateur tout XML développer à PRiSM . Il permettre d' exécuter un XQuery sur un source de donnée hétérogène . Après un rapide présentation de XLive et du Web sémantique , un architecture à trois niveau d' ontologie et de schéma être introduire pour connecter un adaptateur pour le Web sémantique . ce architecture viser à intégrer un source de type Web service d' information conformément à un ontologie global de référence . Elle conduire à étendre XLive avec le support de vue , un outil de conception de vue et de mappings , et des adaptateur pour le Web service . 	Mediating the Semantic Web	7
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mesurer la qualité des règles et de leurs contraposées avec le taux informationnel TIC	La validation des connaissances est l'une des étapes les plus problématiques d'un processus de découverte de règles d'association. Pour que le décideur (expert des données) puisse trouver des connaissances intéressantes dans les grandes quantités de règles produites par les algorithmes de fouille de données, il est nécessaire de mesurer la qualité des règles. Nous insérant dans le cadre de l'analyse statistique implicative, nous proposons dans cet article d'évaluer les règles en considérant leur contenu informationnel à travers un nouvel indice de qualité fondé sur l'entropie de Shannon : TIC (Taux Informationnel modulé par la Contraposée). Cet indice a l'avantage d'être bien adapté à la sémantique des règles, puisque d'une part il respecte leur caractère asymétrique et d'autre part il tire profit de leurs contraposées. Par ailleurs, c'est à notre connaissance la seule mesure de qualité de règles qui intègre à la fois indépendance et déséquilibre, c'est-à-dire qui permette de rejeter simultanément les règles entre variables corrélées négativement et les règles qui possèdent plus de contre-exemples que d'exemples. Des comparaisons de TIC avec la J-mesure, l'information mutuelle, l'indice de Gini, et la confiance sont réalisées sur des simulations numériques.	Julien Blanchard, Fabrice Guillet, Régis Gras, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1001035	http://editions-rnti.fr/render_pdf.php?p=1001035	1725	fr	fr	@univ-nantes.fr	mesurer le qualité des règle et de son contraposées avec le taux informationnel TIC  le validation des connaissance être le un des étape le plus problématique d' un processus de découverte de règle d' association . Pour que le décideur ( expert des donnée ) pouvoir trouver un connaissance intéressant dans le grand quantité de règle produire par le algorithme de fouille de donnée , il être nécessaire de mesurer le qualité des règle . Nous insérer dans le cadre de le analyse statistique implicative , nous proposer dans ce article d' évaluer le règle en considérer son contenu informationnel à travers un nouveau indice de qualité fonder sur le entropie de Shannon : tic ( taux Informationnel moduler par le Contraposée ) . ce indice avoir le avantage d' être bien adapter à le sémantique des règle , puisque d' un part il respecter son caractère asymétrique et d' autre part il tirer profit de son contraposées . Par ailleurs , c' être à son connaissance le seul mesure de qualité de règle qui intégrer à le foi indépendance et déséquilibre , c' est-à-dire qui permettre de rejeter simultanément le règle entre variable corréler négativement et le règle qui posséder plus de contre-exemple que d' exemple . un comparaison de TIC avec le J-mesure , le information mutuel , le indice de Gini , et le confiance être réaliser sur un simulation numérique . 	Mesurer la qualité des règles et de leurs contraposées avec le taux informationnel TIC	22
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mesurer les usages d'internet	Nous rendons compte d'une démarche mise en place pour construire une représentation fine des usages d'internet et de leur évolution, en procédant à du traitement secondaire de données de trafic, provenant de panels représentatifs d'internautes. Après avoir présenté les caractéristiques des cohortes étudiées et les différents modes d'enrichissement des données de trafic mis en place, nous présentons quelques résultats construits à partir de ces données enrichies, et en particulier une segmentation des internautes construite sur la base de l'entrelacement des pratiques de communication et de navigation.	Valérie Beaudouin	http://editions-rnti.fr/render_pdf.php?p1&p=1000887	http://editions-rnti.fr/render_pdf.php?p=1000887	1726	fr	fr	@francetelecom.com	mesurer le usage d' internet  Nous rendre compter d' un démarche mettre en place pour construire un représentation fin des usage d' internet et de son évolution , en procéder à du traitement secondaire de donnée de trafic , provenir de panel représentatif d' internaute . Après avoir présenter le caractéristique des cohorte étudier et le différent mode d' enrichissement des donnée de trafic mettre en place , nous présenter quelque résultat construire à partir de ce donnée enrichi , et en particulier un segmentation des internaute construire sur le base de le entrelacement des pratique de communication et de navigation . 	Mesurer les usages d'internet	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mise en oeuvre des méthodes de fouille de données spatiales alternatives et performances	La fouille de données spatiales nécessite l'analyse des interactions dans l'espace. Ces interactions peuvent être matérialisées dans des tables de distances, ramenant ainsi la fouille de données spatiales à l'analyse multitables. Or, les méthodes de fouilles de données traditionnelles considèrent une seule table en entrée où chaque tuple est une observation à analyser. De simples jointures entre ces tables ne résoud pas le problème et fausse les résultats en raison du comptage multiple des observations. Nous proposons trois alternatives de fouille de données multi-tables dans le cadre de la fouille des données spatiales. La première consiste à interroger à la volée les différentes tables et modifie en dur les algorithmes existants. La seconde est une optimisation de la première qui pré -calcule les jointures et adapte les algorithmes existants. La troisième réorganise les données dans une table unique en complétant - et non en joignant- la table d'analyse par les données présentes dans les autres tables, ensuite applique un algorithme standard sans modification. Cet article présente ces trois alternatives. Il décrit leur implémentation pour la classification supervisée et compare leur performance.	Nadjim Chelghoum, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1001003	http://editions-rnti.fr/render_pdf.php?p=1001003	1727	fr	fr	@prism.uvsq.fr, @prism.uvsq.fr	mise en oeuvre des méthode de fouille de donnée spatial alternatif et performances  le fouille de donnée spatial nécessiter le analyse des interaction dans le espace . ce interaction pouvoir être matérialiser dans un table de distance , ramener ainsi le fouille de donnée spatial à le analyse multitables . Or , le méthode de fouille de donnée traditionnel considérer un seul table en entrée où chaque tuple être un observation à analyser . un simple jointure entre ce table ne résoud pas le problème et fausser le résultat en raison du comptage multiple des observation . Nous proposer trois alternatif de fouille de donnée multi-tables dans le cadre de le fouille des donnée spatial . le premier consister à interroger à le volé le différent table et modifier en dur le algorithme existant . le second être un optimisation de le premier qui pré-calcule le jointure et adapter le algorithme existant . le troisième réorganiser le donnée dans un table unique en compléter - et non en joindre -la table d' analyse par le donnée présent dans le autre table , ensuite appliquer un algorithme standard sans modification . ce article présent ce trois alternatif . Il décrire son implémentation pour le classification superviser et comparer son performance . 	Mise en oeuvre des méthodes de fouille de données spatiales alternatives et performances	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Modèle de gestion intégrée des compétences et connaissances	La compétence et la connaissance sont deux concepts qui nous semblent fortement conjoints, cependant, ils sont rarement étudiés et gérés ensemble. Nous cherchons donc à identifier les liens et frontières qui peuvent exister entre eux. Ceci a pour objectif de développer un modèle de représentation et de gestion, intégré aux connaissances et aux compétences. Dans cet article, est tout d'abord présentée, une synthèse sur les concepts de compétence et de connaissance. Ensuite, les modèles et outils de gestion de ces concepts sont exposés. Puis, le modèle CKIM (Competency and Knowledge Integrated Model) développé, est défini. Les utilités de ce modèle et son exploitation sont discutées en quatrième partie. La dernière partie représente un prototype d'implantation du modèle CKIM réalisé sur le serveur de connaissances ATHANOR.	Nathalie Vergnaud, Mounira Harzallah, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000915	http://editions-rnti.fr/render_pdf.php?p=1000915	1728	fr	fr	@univ-nantes.fr	modèle de gestion intégrer des compétence et connaissances  le compétence et le connaissance être deux concept qui nous sembler fortement conjoindre , cependant , ils être rarement étudier et gérer ensemble . Nous chercher donc à identifier le lien et frontière qui pouvoir exister entre lui . ceci avoir pour objectif de développer un modèle de représentation et de gestion , intégrer aux connaissance et aux compétence . Dans ce article , être tout d' abord présenter , un synthèse sur le concept de compétence et de connaissance . ensuite , le modèle et outil de gestion de ce concept être exposer . Puis , le modèle CKIM ( Competency and Knowledge Integrated Model ) développer , être définir . le utilité de ce modèle et son exploitation être discuter en quatrième partie . le dernier partie représenter un prototype d' implantation du modèle CKIM réaliser sur le serveur de connaissance ATHANOR . 	Modèle de gestion intégrée des compétences et connaissances	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Modèle topologique pour l'interrogation des bases d'images	Nous proposons dans cet article un modèle topologique de représentation de bases d'images. Chaque image est représentée à l'aide d'un vecteur de caractéristiques dans R^p et figure comme noeud dans un graphe de voisinage. L'exploration du graphe correspond à la navigation dans la base de données, les voisins d'un noeud représentent des images similaires. Afin de pouvoir traiter des requêtes, nous définissons un modèle topologique. L'image requête est représentée par un vecteur de caractéristiques dans R^p et insérée dans le graphe en mettant à jour localement les relations de voisinage. Ce travail se positionne dans le domaine de la fouille de données complexes.	Mihaela Scuturici, Jérémy Clech, Vasile-Marian Scuturici, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001092	http://editions-rnti.fr/render_pdf.php?p=1001092	1729	fr	fr	@univ-lyon2.fr	modèle topologique pour le interrogation des base d' images  Nous proposer dans ce article un modèle topologique de représentation de base d' image . chaque image être représenter à le aide d' un vecteur de caractéristique dans R^p et figurer comme noeud dans un graphe de voisinage . le exploration du graphe correspondre à le navigation dans le base de donnée , le voisin d' un noeud représenter un image similaire . Afin de pouvoir traiter un requête , nous définir un modèle topologique . le image requêter être représenter par un vecteur de caractéristique dans R^p et insérer dans le graphe en mettre à jour localement le relation de voisinage . ce travail clr positionner dans le domaine de le fouille de donnée complexe . 	Modèle topologique pour l'interrogation des bases d'images	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Modélisation dynamique et temporelle de l'utilisateur pour un filtrage personnalisé de documents textuels	L'apprentissage efficace du profil utilisateur est un challenge car il évolue sans cesse. Dans cet article nous proposons une nouvelle approche pour l'apprentissage du profil long-terme de l'utilisateur pour le filtrage de documents textuels. Dans ce cadre, les documents consultés sont classés de manière dynamique et nous analysons la répartition dans le temps des classes de documents afin de déterminer le mieux possible les classes d'intérêts de l'utilisateur. L'étude empirique confirme la pertinence de notre approche pour une meilleure personnalisation de documents.	Rachid Arezki, Abdenour Mokrane, Pascal Poncelet, Gérard Dray, David William Pearson	http://editions-rnti.fr/render_pdf.php?p1&p=1001122	http://editions-rnti.fr/render_pdf.php?p=1001122	1730	fr	fr	@ema.fr	modélisation dynamique et temporel de le utilisateur pour un filtrage personnaliser de document textuels  le apprentissage efficace du profil utilisateur être un challenge car il évoluer sans cesse . Dans ce article nous proposer un nouveau approche pour le apprentissage du profil long-terme de le utilisateur pour le filtrage de document textuel . Dans ce cadre , le document consulter être classer de manière dynamique et nous analyser le répartition dans le temps des classe de document afin de déterminer le mieux possible le classe d' intérêt de le utilisateur . le étude empirique confirmer le pertinence de son approche pour un meilleur personnalisation de document . 	Modélisation dynamique et temporelle de l'utilisateur pour un filtrage personnalisé de documents textuels	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	MUSETTE : a framework for knowledge capture from experience	Nous présentons dans cet article une nouvelle approche de modélisation de l'expérience d'utilisation d'un système informatique, avec pour objectif de réutiliser cette expérience en contexte pour assister l'utilisateur à effectuer sa tâche. Quatre scénarios illustrent cette approche.	Pierre-Antoine Champin, Yannick Prié, Alain Mille	http://editions-rnti.fr/render_pdf.php?p1&p=1000912	http://editions-rnti.fr/render_pdf.php?p=1000912	1731	en	fr	@liris.cnrs.fr	Nous présenter dans ce article un nouveau approche de modélisation de le expérience d' utilisation d' un système informatique , avec pour objectif de réutiliser ce expérience en contexte pour assister le utilisateur à effectuer son tâche . Quatre scénario illustrer ce approche . 	MUSETTE : a framework for knowledge capture from experience	56
Revue des Nouvelles Technologies de l'Information	EGC	2004	OpAC : Opérateur d'analyse en ligne basé sur une technique de fouilles de données	L'analyse en ligne OLAP (On-Line Analysis Processing) et la fouille de données (Data Mining) sont deux champs de recherche qui ont connu, depuis quelques années, des évolutions parallèles et indépendantes. De récentes études ont montré l'importance et l'intérêt de l'association entre ces deux domaines scientifiques. A l'heure actuelle, on assiste à l'accroissement du besoin d'une analyse en ligne plus élaborée. Nous pensons que le couplage entre OLAP et la fouille de données pourra apporter des réponses à ce besoin. Dans cet article, nous proposons d'adopter ce couplage en vue de créer un nouvel opérateur, baptisé OpAC (Opérateur d'Agrégation par Classification), d'analyse en ligne des données multidimensionnelles. OpAC consiste particulièrement en l'agrégation sémantique des modalités d'une dimension d'un cube de données en se basant sur la technique de la classification ascendante hiérarchique.	Riadh Ben Messaoud, Sabine Rabaseda, Omar Boussaid, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000889	http://editions-rnti.fr/render_pdf.php?p=1000889	1732	fr	fr	@univ-lyon2.fr	OpAC : opérateur d' analyse en ligne baser sur un technique de fouille de données  le analyse en ligne OLAP ( On-Line Analysis Processing ) et le fouille de donnée ( Data Mining ) être deux champ de recherche qui avoir connaître , depuis quelque année , un évolution parallèle et indépendant . un récent étude avoir montrer le importance et le intérêt de le association entre ce deux domaine scientifique . A le heure actuel , on assister à le accroissement du besoin d' un analyse en ligne plus élaborer . Nous penser que le couplage entre OLAP et le fouille de donnée pouvoir apporter un réponse à ce besoin . Dans ce article , nous proposer d' adopter ce couplage en vue de créer un nouveau opérateur , baptiser OpAC ( opérateur d' agrégation par classification ) , d' analyse en ligne des donnée multidimensionnel . OpAC consister particulièrement en le agrégation sémantique des modalité d' un dimension d' un cube de donnée en clr baser sur le technique de le classification ascendant hiérarchique . 	OpAC : Opérateur d'analyse en ligne basé sur une technique de fouilles de données	6
Revue des Nouvelles Technologies de l'Information	EGC	2004	Optimisation des requêtes temporelles sur le web		Rim Faiz, Nizar Khayati, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1001126	http://editions-rnti.fr/render_pdf.php?p=1001126	1733	fr		@ihec.rnu.tn, @isg.rnu.tn, @ihec.rnu.tn	Optimisation des requêtes temporelles sur le web 	Optimisation des requêtes temporelles sur le web	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Outil de représentation des évolutions de communautés d'intérêts	Cet article présente un système de visualisation permettant l'observation des comportements collectifs implicites. Il s'agit de reconnaître et de représenter des communautés à partir des connexions Internet des utilisateurs : les utilisateurs sont répartis en communautés en fonction des similarités entre des listes de termes établies sur l'analyse des documents consultés par chacun d'eux. L'étude est rendue dynamique par la comparaison des communautés reconnues sur des périodes de temps connexes. L'outil décrit ci après offre deux représentations différentes de ces communautés : une vision des liaisons thématiques entre les utilisateurs sur chaque période étudiée et une vue comparative des communautés reconnues sur toute la durée de l'étude.	Anne Lavallard, Luigi Lancieri	http://editions-rnti.fr/render_pdf.php?p1&p=1001140	http://editions-rnti.fr/render_pdf.php?p=1001140	1734	fr	fr	@francetelecom.com, @francetelecom.com	outil de représentation des évolution de communauté d' intérêts  ce article présenter un système de visualisation permettre le observation des comportement collectif implicite . Il clr agir de reconnaître et de représenter un communauté à partir un connexion Internet des utilisateur : le utilisateur être répartir en communauté en fonction des similarité entre un liste de terme établir sur le analyse des document consulter par chacun d' lui . le étude être rendre dynamique par le comparaison des communauté reconnaître sur un période de temps connexe . le outil décrire ci après offre deux représentation différent de ce communauté : un vision des liaison thématique entre le utilisateur sur chaque période étudier et un vue comparatif des communauté reconnaître sur tout le durée de le étude . 	Outil de représentation des évolutions de communautés d'intérêts	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	PoBOC : un algorithme de 	"Nous décrivons l'algorithme PoBOC (Pole-Based Overlapping Clustering) qui génère un ensemble de clusters non-disjoints (ou ""softclusters"") présentés sous forme d'une hiérarchie de concepts à partir de la seule matrice de similarités sur les données considérées. Nous évaluons l'approche sur deux situations d'apprentissage : la classification par apprentissage de règles et l'organisation de données plus complexes et peu structurées telles que les données textuelles.La validation des méthodes de clustering est une étape difficile résolue le plus souvent par une évaluation d'experts. Les deux applications proposées permettent de valider la méthode d'organisation selon deux points de vue : d'une part quantitativement en évaluant l'influence de la méthode pour la classification, d'autre part en permettant une analyse ""humaine"" du résultat dans le cas des données textuelles. Nous mettons en évidence l'intérêt de PoBOC comparativement à d'autres approches d'apprentissage non-supervisé."	Guillaume Cleuziou, Lionel Martin, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001007	http://editions-rnti.fr/render_pdf.php?p=1001007	1735	fr	fr	@lifo.univ	PoBOC : un algorithme de  " Nous décrire le algorithme PoBOC ( Pole-Based Overlapping Clustering ) qui générer un ensemble de clusters non- disjoint ( ou " " softclusters " " ) présenter sous forme d' un hiérarchie de concept à partir de le seul matrice de similarité sur le donnée considérer . . Nous évaluer le approche sur deux situation d' apprentissage : le classification par apprentissage de règle et le organisation de donnée plus complexe et peu structurer tel que le donnée textuel . . le validation des méthode de clustering être un étape difficile résoudre le plus souvent par un évaluation d' expert . . le deux application proposer permettre de valider le méthode d' organisation selon deux point de vue : d' un part quantitativement en évaluer le influence de le méthode pour le classification , d' autre part en permettre un analyse " " humain " " du résultat dans le cas des donnée textuel . . Nous mettre en évidence le intérêt de PoBOC comparativement à un autre approche d' apprentissage non- superviser . " 	PoBOC : un algorithme de 	5
Revue des Nouvelles Technologies de l'Information	EGC	2004	Positionnement multidimensionnel et partitionnement pour la visualisation de données multivariées		Antoine Naud	http://editions-rnti.fr/render_pdf.php?p1&p=1001030	http://editions-rnti.fr/render_pdf.php?p=1001030	1736	fr		@phys.uni.torun.pl	Positionnement multidimensionnel et partitionnement pour la visualisation de données multivariées 	Positionnement multidimensionnel et partitionnement pour la visualisation de données multivariées	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Qualité et datawarehouse dans le milieu hospitalier		Mireille Cosquer, François Gros, Alain Livartowski	http://editions-rnti.fr/render_pdf.php?p1&p=1000905	http://editions-rnti.fr/render_pdf.php?p=1000905	1737	fr		@curie.net	Qualité et datawarehouse dans le milieu hospitalier 	Qualité et datawarehouse dans le milieu hospitalier	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche ciblée de documents sur le web	Les langages de requêtes mots-clés pour le web manquent souvent de précision lorsqu'il s'agit de rechercher des documents particuliers difficilement caractérisables par de simples mots-clés (exemple : des cours java ou des photos de formule 1). Nous proposons un langage multi-critères de type attribut-valeur pour augmenter la précision de la recherche de documents sur le web.Nous avons expérimentalement montré le gain de précision de la recherche de documents basé sur ce langage.	Amar-Djalil Mezaour	http://editions-rnti.fr/render_pdf.php?p1&p=1001124	http://editions-rnti.fr/render_pdf.php?p=1001124	1738	fr	fr	@lri.fr	recherche cibler de document sur le web  le langage de requête mot _-clé pour le web manquer souvent de précision lorsqu' il clr agir de rechercher un document particulier difficilement caractérisables par un simple mot _-clé ( exemple : des cour java ou des photo de formule 1 ) . Nous proposer un langage multi-critères de type attribut-valeur pour augmenter le précision de le recherche de document sur le web . Nous avoir expérimentalement montrer le gain de précision de le recherche de document baser sur ce langage . 	Recherche ciblée de documents sur le web	5
Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche dans de grandes bases d'images fixes : une nouvelle approche guidée par les règles d'association	"Une base d'images fixes peut être décrite de plusieurs façons, notamment par des descripteurs visuels globaux de couleur, de texture, ou de forme. Les requêtes les plus fréquentes impliquent et combinent les résultats de plusieurs types de descripteurs : par exemple, ""retrouver toutes les images ayant une couleur et une texture semblables à celles d'une image requête donnée"". Pour retrouver plus efficacement et plus rapidement une image dans une grande base, nous exploitons des combinaisons appropriées de descripteurs et étudions l'intérêt des règles d'association entre clusters de descripteurs pour accélérer le temps de réponse à des requêtes sur de grandes bases d'images fixes."	Anicet Kouomou Choupo, Annie Morin, Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000895	http://editions-rnti.fr/render_pdf.php?p=1000895	1739	fr	fr	@irisa.fr	recherche dans un grand base d' image fixe : un nouveau approche guider par le règle d' association  " un base d' image fixe pouvoir être décrire de plusieurs façon , notamment par un descripteur visuel global de couleur , de texture , ou de forme . . le requête le plus fréquent impliquer et combiner le résultat de plusieurs type de descripteur : par exemple , " " retrouver tout le image avoir un couleur et un texture semblable à celui d' un image requête donner " " . . Pour retrouver plus efficacement et plus rapidement un image dans un grand base , nous exploiter un combinaison approprier de descripteur et étudier le intérêt des règle d' association entre clusters de descripteur pour accélérer le temps de réponse à un requête sur un grand base d' image fixe . " 	Recherche dans de grandes bases d'images fixes : une nouvelle approche guidée par les règles d'association	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche de règles d'association hiérarchiques par une approche anthropocentrée	L'Extraction de Connaissances dans la Bases de Données est devenue, pour les banques, une alternative au problème lié à la quantité de données qui sont stockées et qui ne cessent d'augmenter. Ceci aboutit à un paradoxe puisqu'il faut mieux cibler la clientèle susceptible d'être intéressée par une offre en utilisant des méthodes qui ne permettent plus de traiter le nombre croissant d'enregistrements des bases de données. Nos travaux se situent dans la continuité d'une étude que nous avons réalisée sur la recherche de règles d'association appliquée au marketing bancaire. En effet, des premiers résultats encourageants nous ont conduit à approfondir nos travaux vers une recherche de règles d'association hiérarchiques utilisant non plus une approche automatique mais une approche anthropocentrée. Il s'agit d'une approche dans laquelle l'expert fait partie intégrante du processus en jouant le rôle d'heuristique évolutive. Cet article présente les résultats de notre démarche de recherche.	Olivier Couturier, Engelbert Mephu Nguifo, Brigitte Noiret	http://editions-rnti.fr/render_pdf.php?p1&p=1001154	http://editions-rnti.fr/render_pdf.php?p=1001154	1740	fr	fr	@cepdc.caisse-epargne.fr	recherche de règle d' association hiérarchique par un approche anthropocentrée  le extraction de connaissance dans le base de Données être devenir , pour le banque , un alternatif au problème lier à le quantité de donnée qui être stocker et qui ne cesser d' augmenter . ceci aboutir à un paradoxe puisqu' il faillir mieux cibler le clientèle susceptible d' être intéresser par un offre en utiliser un méthode qui ne permettre plus de traiter le nombre croissant d' enregistrement des base de donnée . son travail clr situer dans le continuité d' un étude que nous avoir réaliser sur le recherche de règle d' association appliquer au marketing bancaire . En effet , un premier résultat encourageant nous avoir conduire à approfondir son travail vers un recherche de règle d' association hiérarchique utiliser non plus un approche automatique mais un approche anthropocentrée . Il clr agir d' un approche dans laquelle le expert faire partie intégrant du processus en jouer le rôle d' heuristique évolutif . ce article présenter le résultat de son démarche de recherche . 	Recherche de règles d'association hiérarchiques par une approche anthropocentrée	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Réduction d'un jeu de règles d'association par des méta-règles issues de la logique du 		Martine Cadot, Joseph Di Martino, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001058	http://editions-rnti.fr/render_pdf.php?p=1001058	1741	fr		@loria.fr	Réduction d'un jeu de règles d'association par des méta-règles issues de la logique du  	Réduction d'un jeu de règles d'association par des méta-règles issues de la logique du 	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Réduction du coût d'évaluation d'une règle relationnelle	De nombreuses tâches en Fouille de Données visent à extraire des connaissances exprimées sous la forme d'un ensemble de règles. Les algorithmes dédiés à ces tâches engendrent des règles dont l'adéquation aux données doit être évaluée. On se place dans le cadre où cette évaluation est réalisée directement en lançant des requêtes de dénombrement sur la base de données, et où cette base est relationnelle. Les requêtes comptent les données qui s'apparient avec la règle, calcul qui peut être extrêmement coûteux. Dans cet article, nous étudions l'impact d'une approche d'échantillonnage visant à réduire le coût de l'évaluation des règles relationnelles en tenant compte des spécificités structurelles des requêtes induites.	Agnès Braud, Teddy Turmeaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001039	http://editions-rnti.fr/render_pdf.php?p=1001039	1742	fr	fr	@fct.unl.pt, @lifo.univ	réduction du coût d' évaluation d' un règle relationnelle  un nombreux tâche en fouille de Données viser à extraire un connaissance exprimer sous le forme d' un ensemble de règle . le algorithme dédier à ce tâche engendrer un règle dont le adéquation aux donnée devoir être évaluer . On clr placer dans le cadre où ce évaluation être réaliser directement en lancer un requête de dénombrement sur le base de donnée , et où ce base être relationnel . le requête compter le donnée qui clr apparier avec le règle , calcul qui pouvoir être extrêmement coûteux . Dans ce article , nous étudier le impact d' un approche d' échantillonnage viser à réduire le coût de le évaluation des règle relationnel en tenir compte des spécificité structurel des requête induire . 	Réduction du coût d'évaluation d'une règle relationnelle	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Règles d'identification et méthodes de visualisation d'objets architecturaux	Dans l'étude du patrimoine bâti, la gestion d'informations pose aujourd'hui des problèmes d'interfaçage non triviaux, notamment par la masse, la diversité, la complexité et le caractère hétérogène des contenus. La représentation tridimensionnelle du tissu urbain à différentes échelles (de la ville au corpus architectural), parce qu'elle localise spatialement l'information à délivrer et l'attache à la morphologie de l'édifice, apparaît comme une des réponses possibles. Cette réponse semble par ailleurs bien adaptée aux problématiques spécifiques de l'analyse architecturale du patrimoine que sont par exemple la restitution d'édifices disparus (et les notions d'incertitude qui s'y attachent) ou le réemploi d'éléments de corpus. Pourtant, la représentation tridimensionnelle dans notre champ d'application est aujourd'hui loin de remplir ce rôle. Notre contribution vise à discuter quelques uns des pré-requis qui nous semblent s'imposer à la lumière de nos expériences pour faire de la maquette 3D un outil d'investigation des connaissances sur l'édifice.	Iwona Dudek, Jean-Yves Blaise	http://editions-rnti.fr/render_pdf.php?p1&p=1001157	http://editions-rnti.fr/render_pdf.php?p=1001157	1743	fr	fr	@gamsau.map.archi.fr	règle d' identification et méthode de visualisation d' objet architecturaux  Dans le étude du patrimoine bâti , le gestion d' information poser aujourd' hui un problème d' interfaçage non trivial , notamment par le masse , le diversité , le complexité et le caractère hétérogène des contenu . le représentation tridimensionnel du tissu urbain à différent échelle ( de le ville au corpus architectural ) , parce qu' elle localiser spatialement le information à délivrer et le attache à le morphologie de le édifice , apparaître comme un des réponse possible . ce réponse sembler par ailleurs bien adapter aux problématique spécifique de le analyse architectural du patrimoine que être par exemple le restitution d' édifice disparaître ( et le notion d' incertitude qui clr y attacher ) ou le réemploi d' élément de corpus . pourtant , le représentation tridimensionnel dans son champ d' application être aujourd' hui loin de remplir ce rôle . son contribution viser à discuter quelque un des pré-requis qui nous sembler clr imposer à le lumière de son expérience pour faire de le maquette 3D un outil d' investigation des connaissance sur le édifice . 	Règles d'identification et méthodes de visualisation d'objets architecturaux	4
Revue des Nouvelles Technologies de l'Information	EGC	2004	Régression linéaire symbolique avec variables taxonomiques	Le présent papier concerne l'extension des méthodes classiques de régression linéaire aux cas des données symboliques et fait suite à de précédents travaux de Billard et Diday sur la régression linéaire avec variables intervalles et histogrammes. Dans ce papier, nous présentons des méthodes de régression avec variables taxonomiques. Les variables taxonomiques sont des variables organisées en arbre exprimant plusieurs niveaux de généralité (les villes sont regroupées en régions qui sont elles-mêmes regroupées en pays). La méthode proposée sera testée sur données simulées. Finalement, nous observerons que ces méthodes nous permettent d'utiliser la régression linéaire pour étudier des concepts et pour réduire le nombre de données afin d'améliorer les résultats obtenus par rapport à une régression classique.	Filipe Afonso, Lynne Billard, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001000	http://editions-rnti.fr/render_pdf.php?p=1001000	1744	fr	fr	@ceremade.dauphine.fr, @ceremade.dauphine.fr, @stat.uga.edu	régression linéaire symbolique avec variable taxonomiques  le présent papier concerner le extension des méthode classique de régression linéaire aux cas des donnée symbolique et faire suite à un précédent travail de Billard et Diday sur le régression linéaire avec variable intervalle et histogramme . Dans ce papier , nous présenter un méthode de régression avec variable taxonomiques . le variable taxonomique être un variable organiser en arbre exprimer plusieurs niveau de généralité ( le ville être regrouper en région qui être lui-même regrouper en pays ) . le méthode proposer être tester sur donnée simuler . finalement , nous observer que ce méthode nous permettre d' utiliser le régression linéaire pour étudier un concept et pour réduire le nombre de donnée afin d' améliorer le résultat obtenir par rapport à un régression classique . 	Régression linéaire symbolique avec variables taxonomiques	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Relations entre gènes impliqués dans les cancers de la thyroïde	Des relations entre gènes et protéines impliqués dans les cancers de la thyroïde ont été mises en évidence par l'analyse d'un important corpus de résumés de  la base de données bibliographique Medline. Une approche pluridisciplinaire (biologistes, cliniciens, linguistes et chercheurs en sciences de l'information) a permis l'indexation automatique et l'analyse de ce corpus. L'indexation contrôlée, structurée en classes sémantiques, à partir de vastes ressources hétérogènes (les bases biomédicales et génétiques UMLS et LocusLink), prend en compte la spécificité des termes : nomenclatures biochimiques, acronymes de gènes, aberrations chromosomiques ou encore variantes linguistiques de termes. Les deux méthodes de classification complémentaires appliquées révèlent un réseau lexical dense de gènes concurrents autour de trois principales pathologies de la thyroïde : les cancers médullaires, papillaires et des dysfonctionnements du système immunitaire. Les développements apportés aux outils de visualisation interactifs du serveur VISA de l'INIST facilitent lecture et navigation au sein des documents.	Jean Royauté, Claire François, Alain Zasadzinski, Dominique Besagni, Philippe Dessen, Sylvaine Le Minor, Marie-Thérèse Maunoury	http://editions-rnti.fr/render_pdf.php?p1&p=1001119	http://editions-rnti.fr/render_pdf.php?p=1001119	1745	fr	fr	@lif.univ-mrs.fr, @inist.fr, @igr.fr	relation entre gène impliquer dans le cancer de le thyroïde  un relation entre gène et protéine impliquer dans le cancer de le thyroïde avoir être mettre en évidence par le analyse d' un important corpus de résumé de le base de donnée bibliographique Medline . un approche pluridisciplinaire ( biologiste , clinicien , linguiste et chercheur en science de le information ) avoir permettre le indexation automatique et le analyse de ce corpus . le indexation contrôler , structurer en classe sémantique , à partir un vaste ressource hétérogène ( le base biomédical et génétique UMLS et LocusLink ) , prendre en compte le spécificité des terme : nomenclature biochimique , acronyme de gène , aberration chromosomique ou encore variante linguistique de terme . le deux méthode de classification complémentaire appliquer révéler un réseau lexical dense de gène concurrent autour de trois principal pathologie de le thyroïde : le cancer médullaire , papillaires et des dysfonctionnement du système immunitaire . le développement apporter aux outil de visualisation interactif du serveur VISA de le INIST faciliter lecture et navigation au sein des document . 	Relations entre gènes impliqués dans les cancers de la thyroïde	4
Revue des Nouvelles Technologies de l'Information	EGC	2004	Représentation condensée de motifs émergents	Les motifs émergents sont des associations de caractéristiques fortement présentes dans une classe et rares dans les autres. Ils font ressortir les distinctions entre classes et se révèlent particulièrement efficaces pour construire des classifieurs et apporter une aide au diagnostic. À cause de la forte combinatoire du problème, la recherche et la représentation des motifs émergents restent des tâches complexes pour de grandes bases de données. Nous proposons ici une représentation condensée exacte des motifs émergents (i.e., les motifs et leurs taux de croissance sont directement obtenus depuis la représentation condensée). L'idée principale est de s'appuyer sur les récents résultats relatifs aux représentations condensées de motifs fermés fréquents. À partir de cette représentation, nous donnons aussi une méthode aisée à mettre en oeuvre pour obtenir les motifs émergents ayant les meilleurs taux de croissance. Ces motifs, appelés motifs émergents forts, ont été exploités avec succès dans une collaboration avec la société Philips.	Arnaud Soulet, Bruno Crémilleux, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001022	http://editions-rnti.fr/render_pdf.php?p=1001022	1746	fr	fr	@unicaen.fr	représentation condenser de motif émergents  le motif émergent être un association de caractéristique fortement présent dans un classe et rare dans le autre . Ils faire ressortir le distinction entre classe et clr révéler particulièrement efficace pour construire un classifieur et apporter un aide au diagnostic . À cause de le fort combinatoire du problème , le recherche et le représentation des motif émergent rester un tâche complexe pour un grand base de donnée . Nous proposer ici un représentation condenser exact des motif émergent ( id est , le motif et son taux de croissance être directement obtenir depuis le représentation condenser ) . le idée principal être de clr appuyer sur le récent résultat relatif aux représentation condenser de motif fermer fréquent . À partir de ce représentation , nous donner aussi un méthode aisé à mettre en oeuvre pour obtenir le motif émergent avoir le meilleur taux de croissance . ce motif , appeler motif émergent fort , avoir être exploiter avec succès dans un collaboration avec le société Philips . 	Représentation condensée de motifs émergents	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Représentation de graphes par ACP granulaire	"L'extraction d'information de grands graphes repose le plus souvent sur leur représentation dans des espaces de dimension réduite et on utilise généralement des méthodes factorielles appliquées à des mesures de dissimilarités calculées à partir des matrices associée du graphe ou l'analyse spectrale de leur Laplacien discret. Efficaces pour dégager les structures globales, ces représentations sont parfois peu exploitables dès lors que l'on s'intéresse à une perspective du graphe à partir de certains sommets privilégiés. Or l'information recherchée a souvent un caractère ""local"". Pour représenter le graphe du point de vue d'un ou plusieurs sommets sélectionnés, nous proposons une méthode d'Analyse en Composantes Principales ""Granulaire"" consistant à appliquer une A.C.P. ""filtrée"" à un tableau de proximités. La visualisation d'un graphe de dictionnaire dont la mesure de proximité est obtenue à partir d'un algorithme original illustre notre propos."	Bruno Gaume, Louis Ferré	http://editions-rnti.fr/render_pdf.php?p1&p=1001083	http://editions-rnti.fr/render_pdf.php?p=1001083	1747	fr	fr		représentation de graphe par ACP granulaire  " le extraction d' information de grand graphe reposer le plus souvent sur son représentation dans un espace de dimension réduire et on utiliser généralement un méthode factoriel appliquer à un mesure de dissimilarités calculer à partir un matrice associer du graphe ou le analyse spectral de son Laplacien discret . . Efficaces pour dégager le structure global , ce représentation être parfois peu exploitable dès lors que le on clr intéresser à un perspective du graphe à partir de certain sommet privilégier . . Or le information rechercher avoir souvent un caractère " " local " " . . Pour représenter le graphe du point de vue d' un ou plusieurs sommet sélectionner , nous proposer un méthode d' Analyse en Composantes Principales " " granulaire " " consister à appliquer un A.C.P. " " filtrer " " à un tableau de proximité . . le visualisation d' un graphe de dictionnaire dont le mesure de proximité être obtenir à partir d' un algorithme original illustrer son propos . " 	Représentation de graphes par ACP granulaire	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Résumé de cubes de données multidimensionnelles à l'aide de règles floues	Dans le contexte des entrepôts de données, et des magasins de données multidimensionnelles, les outils OLAP fournissent des moyens aux utilisateurs de naviguer dans leur données afin d'y découvrir des informations pertinentes. cependant, les données à traiter sons souvent très volumineuses et ne permettent pas une exploration systématique et exhaustive. Il s'agit donc de développer des traitements automatisés facilitant la visualisation et la navigation dans les données. Dans cet article, nous étudions une méthode originale permettant de construire et d'identifier de manière automatique et efficace des blocs de données similaires présents dans les cubes de données pouvant être exprimés sous la forme de règles. Cette méthode est fondée sur l'utilisation combinée d'un algorithme par niveaux (de type Apriori) et de la théorie des sous-ensembles flous. Cette théorie nous permet en effet de pallier les problèmes posés par le fait que les blocs de données calculés par notre algorithme peuvent se recouvrir.	Yeow Wei Choong, Anne Laurent, Dominique Laurent, Pierre Maussion	http://editions-rnti.fr/render_pdf.php?p1&p=1000904	http://editions-rnti.fr/render_pdf.php?p=1000904	1748	fr	fr	@help.edu.my, @help.edu.my, @lirmm.fr, @dept-info.u	résumer de cube de donnée multidimensionnel à le aide de règle floues  Dans le contexte des entrepôt de donnée , et des magasin de donnée multidimensionnel , le outil OLAP fournir un moyen aux utilisateur de naviguer dans son donnée afin d' y découvrir un information pertinent . cependant , le donnée à traiter son souvent très volumineux et ne permettre pas un exploration systématique et exhaustif . Il clr agir donc de développer un traitement automatiser faciliter le visualisation et le navigation dans le donnée . Dans ce article , nous étudier un méthode original permettre de construire et d' identifier de manière automatique et efficace des bloc de donnée similaire présent dans le cube de donnée pouvoir être exprimer sous le forme de règle . ce méthode être fonder sur le utilisation combiner d' un algorithme par niveau ( de type Apriori ) et de le théorie des sous-ensemble flou . ce théorie nous permettre en effet de pallier le problème poser par le fait que le bloc de donnée calculer par son algorithme pouvoir clr recouvrir . 	Résumé de cubes de données multidimensionnelles à l'aide de règles floues	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Sélection d'attributs et classification d'objets complexes		Alexandre Blansché, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000994	http://editions-rnti.fr/render_pdf.php?p=1000994	1749	fr		@lsiit.u-strasbg.fr	Sélection d'attributs et classification d'objets complexes 	Sélection d'attributs et classification d'objets complexes	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Sélection rapide en apprentissage supervisé	La sélection de variables (SdV) permet de réduire l'espace de représentation des données. Ce processus est de plus en plus critique en raison de l'augmentation de la taille des bases de données. Traditionnellement, les méthodes de SdV nécessitent plusieurs accès au jeu de données, ce qui peut représenter une part relativement importante du temps d'exécution de ces algorithmes. Nous proposons une nouvelle méthode efficiente et rapide (ne nécessitant qu'un unique accès aux données). Cette méthode utilise les algorithmes génétiques ainsi que des mesures de validité de classification non supervisée (cns).	Nicolas Nicoloyannis, Gaëlle Legrand, Pierre-Emmanuel Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1000953	http://editions-rnti.fr/render_pdf.php?p=1000953	1750	fr	fr	@univ-lyon2.fr, @univ-lyon2.fr	sélection rapide en apprentissage supervisé  le sélection de variable ( SdV ) permettre de réduire le espace de représentation des donnée . ce processus être de plus en plus critique en raison de le augmentation de le taille des base de donnée . traditionnellement , le méthode de SdV nécessiter plusieurs accès au jeu de donnée , ce qui pouvoir représenter un part relativement important du temps d' exécution de ce algorithme . Nous proposer un nouveau méthode efficient et rapide ( ne nécessiter qu' un unique accès aux donnée ) . ce méthode utiliser le algorithme génétique ainsi que un mesure de validité de classification non superviser ( cns ) . 	Sélection rapide en apprentissage supervisé	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Sous-ensembles flous définis sur une ontologie	"Les sous-ensembles flous peuvent être utilisés pour représenter des valeurs imprécises, comme un intervalle aux limites mal définies. Ils peuvent également servir à l'expression de préférences dans les critères de sélection de requêtes en bases de données. En représentation des connaissances, l'utilisation de hiérarchies de types est largement répandue afin de modéliser les relations existant entre les types d'objets d'un domaine donné. Nous nous intéressons aux sous-ensembles flous dont le domaine de définition est une hiérarchie d'éléments partiellement ordonnés par la relation ""sorte de"", que nous appelons ontologie. Nous introduisons la notion de sous-ensemble flou défini sur une partie de l'ontologie, puis sa forme développée définie sur l'ensemble de l'ontologie, que nous appelons extension du sous-ensemble flou. Des classes d'équivalence de sous-ensembles flous définis sur une ontologie peuvent être caractérisées par un représentant unique que nous appelons sous-ensemble flou minimal. Nous concluons par un exemple d'application dans un système d'information relatif à la prévention du risque micro-biologique en sécurité alimentaire."	Rallou Thomopoulos, Patrice Buche, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1000914	http://editions-rnti.fr/render_pdf.php?p=1000914	1751	fr	fr	@inapg.fr	sous-ensemble flou définir sur un ontologie  " le sous-ensemble flou pouvoir être utiliser pour représenter un valeur imprécis , comme un intervalle aux limite mal définir . . Ils pouvoir également servir à le expression de préférence dans le critère de sélection de requête en base de donnée . . En représentation des connaissance , le utilisation de hiérarchie de type être largement répandre afin de modéliser le relation existant entre le type d' objet d' un domaine donner . . Nous nous intéresser aux sous-ensemble flou dont le domaine de définition être un hiérarchie d' élément partiellement ordonner par le relation " " sorte de " " , que nous appeler ontologie . . Nous introduire le notion de sous-ensemble flou définir sur un partie de le ontologie , puis son forme développer définir sur le ensemble de le ontologie , que nous appeler extension du sous-ensemble flou . . un classe d' équivalence de sous-ensemble flou définir sur un ontologie pouvoir être caractériser par un représentant unique que nous appeler sous-ensemble flou minimal . . Nous conclure par un exemple d' application dans un système d' information relatif à le prévention du risque micro- biologique en sécurité alimentaire . " 	Sous-ensembles flous définis sur une ontologie	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Uitliation de connaissances pour l'aide à la recherche documentaire fondée sur le contenu		Amedeo Napoli, Rim Al Hulou	http://editions-rnti.fr/render_pdf.php?p1&p=1001125	http://editions-rnti.fr/render_pdf.php?p=1001125	1752	fr		@loria.fr, @kr2002	Uitliation de connaissances pour l'aide à la recherche documentaire fondée sur le contenu 	Uitliation de connaissances pour l'aide à la recherche documentaire fondée sur le contenu	
Revue des Nouvelles Technologies de l'Information	EGC	2004	Un algorithme de génération des itemsets fermés pour la fouille de données	Le traitement de grand volume de données est un problème pour l'extraction de connaissances. La fouille de données nécessite des méthodes de résolution efficaces. Le treillis de concepts (treillis de Galois) est un outil utile pour l'analyse de données. Des travaux en classification et sur les règles d'association ont permis d'accroître son intérêt. Plusieurs algorithmes de génération on été proposés, parmi lesquels NextClosure est l'un des meilleurs pour traiter des données de grande taille.Mais la complexité de NextClosure reste malgré tout très élevé. Aussi nous proposons un nouvel algorithme efficace nommé ScalingNextClosure, et basé sur une méthode de partitionnement de données pour générer de manière indépendante les itemsets fermés de chaque partition. Les résultats expérimentaux montrer que cette technique de partitionnement améliore efficacement NextClosure.	Engelbert Mephu Nguifo, Huaiguo Fu	http://editions-rnti.fr/render_pdf.php?p1&p=1001062	http://editions-rnti.fr/render_pdf.php?p=1001062	1753	fr	fr	@univ-artois.fr	un algorithme de génération des itemsets fermer pour le fouille de données  le traitement de grand volume de donnée être un problème pour le extraction de connaissance . le fouille de donnée nécessiter un méthode de résolution efficace . le treillis de concept ( treillis de Galois ) être un outil utile pour le analyse de donnée . un travail en classification et sur le règle d' association avoir permettre d' accroître son intérêt . plusieurs algorithme de génération on être proposer , parmi lesquels NextClosure être le un des meilleur pour traiter un donnée de grand taille . Mais le complexité de NextClosure rester malgré tout très élever . aussi nous proposer un nouveau algorithme efficace nommer ScalingNextClosure , et baser sur un méthode de partitionnement de donnée pour générer de manière indépendant le itemsets fermer de chaque partition . le résultat expérimental montrer que ce technique de partitionnement améliorer efficacement NextClosure . 	Un algorithme de génération des itemsets fermés pour la fouille de données	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Une approche probabiliste pour le classement d'objets incomplets dans un arbre de décision		Lamis Hawarah, Ana Simonet, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1001029	http://editions-rnti.fr/render_pdf.php?p=1001029	1754	fr		@imag.fr	Une approche probabiliste pour le classement d'objets incomplets dans un arbre de décision 	Une approche probabiliste pour le classement d'objets incomplets dans un arbre de décision	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Une étude d'algorithmes de classification supervisée basée sur les treillis de Galois		Huaiyu Fu, Huaiguo Fu, Patrick Njiwoua, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001025	http://editions-rnti.fr/render_pdf.php?p=1001025	1755	fr		@univ-artois.fr	Une étude d'algorithmes de classification supervisée basée sur les treillis de Galois 	Une étude d'algorithmes de classification supervisée basée sur les treillis de Galois	0
Revue des Nouvelles Technologies de l'Information	EGC	2004	Une méthode pour l'appropriation de savoir-faire, capitalisé avec MASK	La gestion explicite des savoirs et savoir-faire occupe une place de plus en plus importante dans les organisations. La construction de mémoires d'entreprise dans un but de préservation et de partage est devenu une pratique assez courante. Cependant, on oublie trop suivent que l'efficacité de ces activités est étroitement liée aux capacités d'appropriation et d'apprentissage des acteurs de l'organisation.Dans cet article, nous proposons des démarches générales d'accompagnement permettant de faciliter le processus d'appropriation des mémoires d'entreprise construits avec la méthode MASK, en exploitant des techniques d'ingénierie pédagogique.	Oswaldo Castillo, Nada Matta, Jean-Louis Ermine	http://editions-rnti.fr/render_pdf.php?p1&p=1000911	http://editions-rnti.fr/render_pdf.php?p=1000911	1756	fr	fr	@utt.fr, @int-evry.fr	un méthode pour le appropriation de savoir-faire , capitaliser avec MASK  le gestion expliciter un savoir et savoir-faire occuper un place de plus en plus important dans le organisation . le construction de mémoire d' entreprise dans un but de préservation et de partage être devenir un pratique assez courant . cependant , on oublier trop suivre que le efficacité de ce activité être étroitement lier aux capacité d' appropriation et d' apprentissage des acteur de le organisation . Dans ce article , nous proposer un démarche général d' accompagnement permettre de faciliter le processus d' appropriation des mémoire d' entreprise construire avec le méthode MASK , en exploiter un technique d' ingénierie pédagogique . 	Une méthode pour l'appropriation de savoir-faire, capitalisé avec MASK	1
Revue des Nouvelles Technologies de l'Information	EGC	2004	Utilisation des graphes de proximité dans le cadre de l'apprentissage basé sur les voisins	"La classification suivant les plus proches voisins est une règle simple et attractive, basée sur une définition paramétrique du voisinage. Les graphes des proximité, quand à eux, induisent des notions plus souples de voisinage. Il s'agit ici d'effectuer la substitution.Les variantes obtenues, peu testées dans la bibliographie, ont été soumises à une expérimentation intensive, sur bases de données de l'UCI et de France Télécom. On a ainsi considéré divers types de prétraitement des données et plusieurs catégories de graphes. De plus, on a caractérisé les effets du ""piège de la dimension"" sur le comportement théorique de tous les graphes présentés, une quantification empirique du phénomène ayant été réalisée.Il ressort de notre étude que l'utilisation du voisinage de Gabriel provoque une amélioration en moyenne et que le prétraitement basé sur la statistique de rang est le plus adéquate. Quoiqu'il arrive, des précautions doivent être prises en grande dimension."	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001061	http://editions-rnti.fr/render_pdf.php?p=1001061	1757	fr	fr	@francetelecom.com, @francetelecom.com	utilisation des graphe de proximité dans le cadre de le apprentissage baser sur le voisins  " le classification suivre le plus proche voisin être un règle simple et attractif , baser sur un définition paramétrique du voisinage . . le graphe des proximité , quand à lui , induire un notion plus souple de voisinage . . Il clr agir ici d' effectuer le substitution . . le variante obtenir , peu tester dans le bibliographie , avoir être soumettre à un expérimentation intensif , sur base de donnée de le UCI et de France télécommunications . . On avoir ainsi considérer divers type de prétraitement un donnée et plusieurs catégorie de graphe . . De plus , on avoir caractériser le effet du " " piège de le dimension " " sur le comportement théorique de tout le graphe présenter , un quantification empirique du phénomène avoir être réaliser . . Il ressortir de son étude que le utilisation du voisinage de Gabriel provoquer un amélioration en moyenne et que le prétraitement baser sur le statistique de rang être le plus adéquat . . Quoiqu' il arriver , un précaution devoir être prendre en grand dimension . " 	Utilisation des graphes de proximité dans le cadre de l'apprentissage basé sur les voisins	2
Revue des Nouvelles Technologies de l'Information	EGC	2004	Validation de graphes conceptuels	"Les travaux menés en validation des connaissances visent à améliorer la qualité des bases de connaissances. Le modèle des graphes conceptuels est un modèle de représentation des connaissances de la famille des réseaux sémantiques, fondé sur la théorie des graphes et sur la logique du premier ordre. Nous proposons une solution pour valider sémantiquement une base de connaissances composée de graphes conceptuels. La validation sémantique d'une base de connaissance consiste à confronter ses connaissances à des contraintes certifiées fiables. Nous proposons d'utiliser des contraintes descriptives, exprimées sous forme de graphes conceptuels, qui permettent de poser des conditions sur la représentation de certaines connaissance dans la base. Ces contraintes introduisent une notion de cardinalités, et sont soit minimales, soit maximales. Elles permettent respectivement d'exprimer ""si A, alors au moins ou au plus n fois B"". La satisfaction de ces contraintes par une base de connaissances repose sur l'utilisation de l'opération de base du modèle des graphes conceptuels : la projection."	Juliette Dibie-Barthélemy, Ollivier Haemmerlé, Eric Salvat	http://editions-rnti.fr/render_pdf.php?p1&p=1000913	http://editions-rnti.fr/render_pdf.php?p=1000913	1758	fr	fr		validation de graphe conceptuels  " le travail mener en validation des connaissance viser à améliorer le qualité des base de connaissance . . le modèle des graphe conceptuel être un modèle de représentation des connaissance de le famille des réseau sémantique , fonder sur le théorie des graphe et sur le logique du premier ordre . . Nous proposer un solution pour valider sémantiquement un base de connaissance composer de graphe conceptuel . . le validation sémantique d' un base de connaissance consister à confronter son connaissance à un contrainte certifier fiable . . Nous proposer d' utiliser un contrainte descriptif , exprimer sous forme de graphe conceptuel , qui permettre de poser un condition sur le représentation de certain connaissance dans le base . . ce contrainte introduire un notion de cardinalités , et être soit minimal , soit maximal . . Elles permettre respectivement d' exprimer " " si A , alors au moins ou au plus n foi B " " . . le satisfaction de ce contrainte par un base de connaissance reposer sur le utilisation de le opération de base du modèle des graphe conceptuel : le projection . " 	Validation de graphes conceptuels	3
Revue des Nouvelles Technologies de l'Information	EGC	2004	Veille technologique assistée par la Fouille de Textes	Le domaine de la veille technologique vise à récolter, traiter, et analyser des informations scientifiques et techniques utiles aux acteurs économiques. Dans cet article nous proposons d'utiliser des techniques de fouille de textes pour automatiser le processus de traitement des données issues de bases de textes scientifiques. Toutefois, la veille introduit une difficulté inhabituelle par rapport aux domaines d'application classiques des techniques de fouille de textes, puisqu'au lieu de rechercher de la connaissances fréquente cachée dans les données, il faut rechercher de la connaissance inattendue. Les mesures usuelles d'extraction de la connaissance à partir de textes doivent de ce fait être revues. Pour ce faire, nous avons développé le système UnexpectedMiner dans lequel de nouvelles mesures permettent d'estimer le caractère inattendu d'un document. Notre système est évalué sur une base d'articles dans le domaine de l'apprentissage automatique.	François Jacquenet, Christine Largeron, Stéphanie Chapaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001097	http://editions-rnti.fr/render_pdf.php?p=1001097	1759	fr	fr		veille technologique assister par le fouille de Textes  le domaine de le veille technologique viser à récolter , traiter , et analyser un information scientifique et technique utile aux acteur économique . Dans ce article nous proposer d' utiliser un technique de fouille de texte pour automatiser le processus de traitement des donnée issu de base de texte scientifique . toutefois , le veille introduire un difficulté inhabituel par rapport aux domaine d' application classique des technique de fouille de texte , puisqu' au lieu de rechercher de le connaissance fréquent cacher dans le donnée , il faillir rechercher de le connaissance inattendu . le mesure usuel d' extraction de le connaissance à partir de texte devoir de ce fait être revoir . Pour ce faire , nous avoir développer le système UnexpectedMiner dans lequel de nouveau mesure permettre d' estimer le caractère inattendu d' un document . son système être évaluer sur un base d' article dans le domaine de le apprentissage automatique . 	Veille technologique assistée par la Fouille de Textes	4
Revue des Nouvelles Technologies de l'Information	EGC	2004	Vers un entrepôt de données pour la gestion des risques naturels	Les entrepôts de données sont l'un des plus importants développements dans le domaine des systèmes d'informations. Ils permettent d'intégrer des données de plusieurs sources, souvent très volumineux, distribuées et hétérogènes. Dans cet article, nous examinons la possibilité d'utiliser la technique d'entrepôt de données dans la gestion des risques naturels. Nous présentons un modèle conceptuel pour l'entrepôt proposé, avec la présence de formats et types variés de données tel que des données géographiques et multimédia. Nous proposons également des opérations OLAP pour la navigation des informations stockées dans le cube de données.	Hicham Hajji, Nourdine Badji, Jean-Pierre Asté	http://editions-rnti.fr/render_pdf.php?p1&p=1001163	http://editions-rnti.fr/render_pdf.php?p=1001163	1760	fr	fr	@univ-lyon1.fr, @gipea.fr	Vers un entrepôt de donnée pour le gestion des risque naturels  le entrepôt de donnée être le un des plus important développement dans le domaine des système d' information . Ils permettre d' intégrer un donnée de plusieurs source , souvent très volumineux , distribuer et hétérogène . Dans ce article , nous examiner le possibilité d' utiliser le technique d' entrepôt de donnée dans le gestion des risque naturel . Nous présenter un modèle conceptuel pour le entrepôt proposer , avec le présence de format et type varier de donnée tel que un donnée géographique et multimédia . Nous proposer également un opération OLAP pour le navigation des information stocker dans le cube de donnée . 	Vers un entrepôt de données pour la gestion des risques naturels	0
