series	booktitle	year	title	abstract	authors	pdf1page	pdfarticle
Revue des Nouvelles Technologies de l'Information	EDA	2015	An Approach for Alert Raising in Real-Time Data Warehouses	This work proposes an approach for alert raising within a real-timedata warehouse environment. It is based on the calculation of confidence intervalsfor measures from historical facts. As new facts arrive to the data warehouseon a real-time basis, they are systematically compared with their appropriateconfidence intervals and alerts are raised when anomalies are detected. The interestof this approach is illustrated using the particular real world use case oftechnical analysis of stock data.	Maximiliano Ariel López, Sergi Nadal, Mahfoud Djedaini, Patrick Marcel, Veronika Peralta, Pedro Furtado	http://editions-rnti.fr/render_pdf.php?p1&p=1002132	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Analysis Framework for Reduced Data Warehouse	Our aim is to define a framework supporting analysis in MDW withreductions. Firstly, we describe a modeling solution for reducedMDW. A schemaof reduced MDW is composed of states. Each state is defined as a star schemacomposed of one fact and its related dimensions valid for a certain period oftime. Secondly, we present a multi-state analysis framework. Extensions of classicaldrilldown and rollup operators are defined to support multi-states analyses.Finally we present a prototype of our framework aiming to prove the feasibilityof concept. By implementing our extended operators, the prototype automaticallygenerates appropriate SQL queries over metadata and reduced data.	Franck Ravat, Jiefu Song, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1002128	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Analysis of Interbank Messages for the Enforcement of Financial Regulations	In the context of the recent policies concerning anti-money launderingand counter terrorist financing defined by the Financial Action Task ForceRecommendation 16, it is the responsibility of the financial institution to monitorthe quality of the information present in wire transfers. To that end, we presentin this paper an approach to automate the monitoring and the validation of theinformation contained in interbank transfer messages. The approach is implementedin a solution built around an event-driven architecture where the datais processed as a stream and transformed at each stage. This architecture is inline with the latest research in data warehouses with stream data processing. Weshow that our approach is suitable to the requirements and the standards in thebanking industry	Nam-Luc Tran	http://editions-rnti.fr/render_pdf.php?p1&p=1002136	
Revue des Nouvelles Technologies de l'Information	EDA	2015	BI2 : Un profil UML pour les Indicateurs Décisionnels	Aujourd'hui de plus en plus de données sont disponibles pour uneanalyse décisionnelle et reposent sur des indicateurs décisionnels. Bien que différentestechnologies décisionnelles aient été développées, nous constatons lemanque d'un cadre conceptuel pour la définition et l'implémentation de ces indicateurs.Dans ce papier, nous présentons une première classification de cesindicateurs. De plus, motivés par le besoin d'un formalisme pour la définition deces indicateurs à un niveau conceptuel, nous présentons un profil UML BI2 quipermet de représenter des indicateurs OLAP, OLTP et stream. Nous présentonségalement leur implémentation dans les outils industriels existants.	Sandro Bimonte	http://editions-rnti.fr/render_pdf.php?p1&p=1002123	
Revue des Nouvelles Technologies de l'Information	EDA	2015	D-WorM : Middleware pour le traitement des requêtes massives dans les entrepôts de données massives	Avec l'accroissement massif des données, plusieurs problèmes connusdes entrepôts de données sont à repenser à l'échelle du Big Data et des environnementsdistribués. Dans cette article nous proposons D-WorM (Data warehouseWorkload Manager), une solution pour résoudre la problématique posée par letraitement des requêtes massives. Notre Middleware a pour objectif d'améliorerla gestion des montées en charge.	Rado Ratsimbazafy, Omar Boussaid, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1002135	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Entrepôts de données multidimensionnelles NoSQL	Les données des systèmes d'analyse en ligne (OLAP, On-Line AnalyticalProcessing) sont traditionnellement gérées par des bases de données relationnelles.Malheureusement, il devient difficile de gérer des mégadonnées (degros volumes de données, « Big Data »). Dans un tel contexte, comme alternative,les environnements « Not-Only SQL » (NoSQL) peuvent fournir un passageà l'échelle tout en gardant une certaine flexibilité pour un système OLAP. Nousdéfinissons ainsi des règles pour convertir un schéma en étoile, ainsi que son optimisation,le treillis d'agrégats pré-calculés, en deux modèles logiques NoSQL :orienté-colonnes ou orienté-documents. En utilisant ces règles, nous implémentonset analysons deux systèmes décisionnels, un par modèle, avec MongoDB etHBase. Nous comparons ces derniers sur les phases de chargement des données(générées avec le benchmark TPC-DS), de calcul d'un treillis et d'interrogation.	Max Chevalier, Mohammed El Malki, Arlind Kopliku, Olivier Teste, Ronan Tournier	http://editions-rnti.fr/render_pdf.php?p1&p=1002133	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Experimental Evaluation of a Dynamic Cubing System: Workflow, Metrics, and Prototype	This article originates in the efforts we made in the prototyping stageof a project conducted in the field of new agile BI applications. In this earlierwork, we defined a model and algorithms for handling dynamic cubes that areupdated as single fact comes in. The proposal includes the definition of a treestructure, called DyTree, used to store these cubes. A comprehensive experimentalevaluation was conducted to observe the behavior of the proposal undersome varying circumstances or settings, to help in understanding its workingand to provide feedback to improve the solution. In this paper, we describe theworkflow which has been used for this experimental evaluation. Performancemetrics and behavioral metrics are defined as output of the experimental evaluationworkflow. Different types of data sets and parameters for algorithms arethe input used to configure the experiments. Using this environment and the prototype,some examples of experimental studies are presented. This shows howdifferent scenarios can be simply constructed and how the experimental resultscan be used to understand the behavior of our proposal and to evaluate performance.	 Anne Tchounikine, Maryvonne Miquel, Usman Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1002134	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Extending the Multidimensional Model for Linking Cubes	Data warehouses structure data in multidimensional cubes, where dimensionsspecify different ways in which measures in facts may be viewed,aggregated, and sorted. It is essential for data analysts to combine data from heterogeneousmultidimensional cubes to enhance their analysis capabilities. Forthis, users are restricted in using only shared dimensions for navigating relatedmultidimensional cubes. In this paper, we show that this limits the analysis possibilitiesand introduce an explicit link that relates two multidimensional cubes,indicating that they represent different aspects of the same reality, and hencethey may be connected. We argue that the standard drill-across operator is notsuited to perform such operation, and we extend it by proposing a new operatorcalled drill-across-link.	Alberto Sabaini, Esteban Zimányi, Carlo Combi	http://editions-rnti.fr/render_pdf.php?p1&p=1002124	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Huawei Open Data Analytics Platform	New frameworks such as Spark, Tez, Flink, or Hive offer new possibilities,but using more than one framework is generally not easy. We buildan abstraction layer that allows users to define their big data operations in adeclarative way. This abstraction layer is backed by a platform that optimizesthe workflow by carefully profiling each operation and running them, transparently,on the most appropriate framework given the description of the job andthe currently available resources.	Gary Verhaegen, Charles Bonneau, Antonios Tsaltas	http://editions-rnti.fr/render_pdf.php?p1&p=1002137	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Intégration Holistique des Graphes basée sur la Programmation Linéaire pour l'Entreposage des Open Data	Dans cet article, nous proposons une approche holistique pour l'intégrationdes graphes d'Open Data. Ces graphes représentent une classificationhiérarchique des concepts extraits des Open Data. Nous nous focalisons sur laconservation de hiérarchies strictes lors de l'intégration afin de pouvoir définirun schéma multidimensionnel à partir de ces hiérarchies et entreposer par la suiteces sources de données. Notre approche est basée sur un programme linéairequi résout automatiquement la tâche de matching des graphes tout en maximisantglobalement la somme des similarités entre les concepts. Ce programme estcomposé de contraintes sur la cardinalité du matching et de contraintes sur lastructure des graphes. A notre connaissance, notre approche est la première àfournir une solution optimale globale pour le matching holistique des graphesavec un temps de résolution raisonnable. Nous comparons également la qualitédes résultats de notre approche par rapport à d'autres approches de la littérature.	Alain Berro, Imen Megdiche, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1002130	
Revue des Nouvelles Technologies de l'Information	EDA	2015	On-Line Analytical Processing on Graphs Generated from Social Network Data	Social Network services have quickly become a powerful means bywhich people share real-time messages. Typically, social networks are modeledas large underlying graphs. Responding to this emerging trend, it becomescritically important to interactively view and analyze this massive amount ofdata from different perspectives and with multiple granularities. While Onlineanalytical processing (OLAP) is a powerful primitive for structured dataanalysis, it faces major challenges in manipulating this complex interconnectingdata. In this paper, we suggest a new data warehousing model, namelySocial Graph Cube to support OLAP technologies on multidimensional socialnetworks. Based on the proposed model we represent data as heterogeneous informationgraphs for more comprehensive illustration than the traditional OLAPtechnology. Going beyond traditional OLAP operations, Social Graph Cubeproposes a new method that combines data mining area and OLAP operators tonavigate through dimension hierarchies. Experimental results show the effectivenessof Social Graph Cube for decision-making.	Lilia Hannachi, Omar Boussaid, Nadjia Benblidia, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1002129	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Requirements Engineering for Data Warehouses	Data Warehouses (DWs) aim at supporting the decision-making processof an organization. In the Requirements Engineering (RE) domain, severalmethods were proposed for the development of DWs, most of them basedon the Goal-Oriented Requirements Engineering (GORE) approach. However,there is not yet a comprehensive and unified perspective of the various methodsproposed. In this paper, a coherent view of the GORE approach for the developmentof DWs is presented, by classifying existing methods according to thedecision-making process, integrating them together, and linking modeling andanalysis techniques throughout the overall process. The result of our study is anintegrated GORE-based method for the development of DWs. We illustrate themethod with a concrete example from the health care sector.	Azadeh Nasiri, Esteban Zimányi, Robert Wrembel	http://editions-rnti.fr/render_pdf.php?p1&p=1002126	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Temporal DataWarehouses: Logical Models and Querying	Data warehouses (DWs) integrate data from multiple and heterogeneousdata sources. Most of the DW design methods assume that the contentsof the dimensions in a DW will not change, but this is not the case in reality.Therefore, DWs must reflect these changes in the real-world in order to enableusers to ask various types of temporal queries. Since temporal queries are complexand costly, it is necessary to know which modeling approach is better forsuch queries. In this paper, we discuss two possible approaches to implement aDW capable of maintaining the history of the changes in dimension members.We also present a classification of temporal queries that can be used to evaluatethe two approaches.	Waqas Ahmed, Esteban Zimányi, Robert Wrembel	http://editions-rnti.fr/render_pdf.php?p1&p=1002125	
Revue des Nouvelles Technologies de l'Information	EDA	2015	TLabel: Nouvel opérateur d'agrégation par catégorisation dans les cubes de textes	L'analyse en ligne (OLAP) dans les cubes de textes nécessite la définitionde nouveaux types d'opérateurs d'analyse appropriés aux données textuelles.En effet, les opérateurs d'agrégation classiques ont montré leur efficacitépour l'analyse en ligne des données numériques, mais ils sont inadaptés pourl'analyse des données textuelles. Dans cet article, nous proposons un nouvel opérateurd'agrégation par catégorisation nommé TLabel (Text Label) permettantd'agréger les données textuelles en plusieurs classes de documents. A chaqueclasse sera associée une étiquette (Label) qui représente le contenu sémantiquedes données textuelles de la classe grâce à une adaptation des techniques defouille de textes à l'OLAP. Nous avons effectué une étude expérimentale surnotre opérateur TLabel. Les résultats préliminaires montrent l'intérêt de notreapproche pour l'analyse en ligne des données textuelles.	Lamia Oukid, Omar Boussaid, Nadjia Benblidia, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1002131	
Revue des Nouvelles Technologies de l'Information	EDA	2015	Une nouvelle approche mixte d'enrichissement de dimensions dans un schéma multidimensionnel en constellation	Les entrepôts de données (DW) et les systèmes OLAP sont des technologiesd'analyse en ligne pour de grands volumes de données, basés sur les besoinsdes utilisateurs. Leur succès dépend essentiellement de la phase de conceptionoù les exigences fonctionnelles sont confrontées aux sources de données(méthodologie de conception mixte). Cependant, les méthodes de conceptionexistantes semblent parfois inefficaces, lorsque les décideurs définissent des exigencesfonctionnelles qui ne peuvent être déduites à partir des sources de données(approche centrée sur les données), ou lorsque le décideur n'a pas intégrétous ces besoins durant la phase de conception (approche centrée sur l'utilisateur).Cet article propose une nouvelle méthodologie mixte d'enrichissement deschémas en constellation, où l'approche classique de conception est amélioréegrâce à la fouille de données dans le but de créer de nouvelles hiérarchies au seind'une dimension. Un prototype associé est également présenté.	Lucile Sautot, Sandro Bimonte, Ludovic Journaux, Bruno Faivre	http://editions-rnti.fr/render_pdf.php?p1&p=1002127	
Revue des Nouvelles Technologies de l'Information	EGC	2015	A Clustering Based Approach for Type Discovery in RDF Data Sources	RDF(S)/OWL data sources are not organized according to a predefined schema, as they are structureless by nature. This lack of schema limits their use to express queries or to understand their content. Our work is a contribution towards the inference of the structure of RDF(S)/OWL data sources. We present an approach relying on density-based clustering to discover the types describing the entities of possibly incomplete and noisy data sets.	Kenza Kellou-Menouer, Zoubida Kedad	http://editions-rnti.fr/render_pdf.php?p1&p=1002113	
Revue des Nouvelles Technologies de l'Information	EGC	2015	A Framework for Mesh Segmentation and Annotation using Ontologies	La segmentation et annotation de maillages utilisant la sémantique a été l'objet d'un intérêt grandissant avec la démocratisation des techniques de reconstruction 3D. Une approche classique consiste à réaliser cette tâche en deux étapes, tout d'abord en segmentant le maillage, puis en l'annotant. Cependant, cette approche ne permet pas à chaque étape de profiter de l'autre. En traitement d'images, quelques méthodes combinent la segmentation et l'annotation, mais ces approches ne sont pas génériques, et nécessitent des ajustements d'implémentation ou des réécritures pour chaque modification des connaissances expertes. Dans ce travail, nous décrivons un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre d'étapes de segmentation, et nous présentons des résultats préliminaires qui montrent la faisabilité de l'approche.Notre système fournit une ontologie générique qui décrit sous forme de concepts les propriétés d'un objet (géométrie, topologie, etc.), ainsi que des algorithmes permettant de détecter ces concepts. Cette ontologie peut être étendue par un expert pour décrire formellement un domaine spécifique. La description formelle du domaine est alors utilisée pour réaliser automatiquement l'assemblage de la segmentation et de l'annotation d'objets et de leurs propriétés, en sélectionnant à chaque étape l'algorithme le plus pertinent, étant données les information sémantiques déjà détectées. Cette approche originale comporte plusieurs avantages. Tout d'abord, elle permet de segmenter et d'annoter des objets sans aucune connaissance en traitement d'images ou de maillages, en décrivant uniquement les propriétés de l'objet en terme de concepts ontologiques. De plus, ce cadre de fontionnement peut facilement être réutilisé et appliqué à différents contextes, dès lors qu'une ontologie de domaine a été définie. Finalement, la réalisation conjointe de la segmentation et de l'annotation permet d'utiliser d'une manière efficace la connaissance experte, en réduisant les erreurs de segmentation et le temps de calcul, en lançant toujours l'algorithme le plus pertinent.	Thomas Dietenbeck, Ahlem Othmani, Marco Attene, Jean-Marie Favreau	http://editions-rnti.fr/render_pdf.php?p1&p=1002088	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse des paramètres de recherche d'information: Etude de l'influence des paramètres sur les résultats	Cet article présente une analyse détaillée d'un ensemble de 2 millions de résultats de recherche d'information obtenus par différents paramétrages de systèmes de recherche d'information. Plus spécifiquement, nous avons utilisé la plateforme Terrier et l'interface RunGeneration pour créer différentes exécutions (run en anglais) en modifiant les modèles d'indexation et de recherche. Nous avons ensuite évalué chacun des résultats obtenus selon différentes mesures de performance de recherche d'information. Une analyse systématique a été menée sur ces données afin de déterminer d'une part quels étaient les paramètres qui ont le plus d'influence, d'autre part quels étaient les valeurs de ces paramètres les plus susceptibles de conduire à de bonnes performances du système.	Josiane Mothe, Marion Moulinou	http://editions-rnti.fr/render_pdf.php?p1&p=1002059	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse et visualisation d'opinions dans un cadre de veille sur leWeb	L'analyse d'opinions est une tâche qui consiste en l'identification et la classification de textes subjectifs. Dans ce travail, nous nous intéressons au problème d'analyse d'opinions dans un contexte de veille sur le Web. Nous proposons une approche pour visualiser les résultats d'analyse d'opinions, basée sur l'utilisation de termes clés. Nous décrivons également la plateforme de veille sur leWeb AMIEI, au sein de laquelle notre approche a été implémentée. La démonstration consistera en une expérimentation de la plateforme de veille AMIEI et du module d'analyse d'opinions sur un corpus de tweets politiques.	Mohamed Dermouche, Leila Khouas, Sabine Loudcher, Julien Velcin, Eric Fourboul	http://editions-rnti.fr/render_pdf.php?p1&p=1002110	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse OLAP sur des tweets et des blogs : un retour d'expérience	Le projet ANR IMAGIWEB dans lequel s'inscrit ce travail s'est donné pour mission d'étudier les images véhiculées sur Internet en se basant sur la détection d'opinions. Deux cas d'étude ont été définis : (1) le premier vise à répondre aux besoins d'analyse de chercheurs en science politique grâce à des données issues de Twitter durant la campagne présidentielle de 2012 ; (2) le second doit permettre à l'entreprise française EDF d'évaluer l'opinion du public en matière de sécurité, d'emploi et de prix à partir de billets de blogs. Dans cet article, nous présentons un retour d'expérience sur l'usage de l'analyse en ligne OLAP (OnLine Analytical Processing) pour des données textuelles, mettant en avant l'intérêt de ce type d'analyse pour les membres du projet.	Brice Olivier, Cécile Favre, Sabine Loudcher	http://editions-rnti.fr/render_pdf.php?p1&p=1002106	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse visuelle pour la détection des intrusions	La démocratisation d'Internet, couplée à l'effet de la mondialisation, a pour résultat d'interconnecter les personnes, les états et les entreprises. Le côté déplaisant de cette interconnexion mondiale des systèmes d'information réside dans un phénomène appelé "Cybercriminalité". Nous proposons une méthode de visualisation de grands "graphes" et l'exploitation d'analyses statiques des flux permettant de détecter les comportements anormaux et dangereux afin d'appréhender les risques d'une façon compréhensible par tous les acteurs.	David Pierrot, Nouria Harbi	http://editions-rnti.fr/render_pdf.php?p1&p=1002079	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Approche d'extraction de classes interlangues à partir de documents multilingues à base de Concepts Fermés	In this article, we highlight the interest and usefulness of Formal Concept Analysis (FCA) in multilingual document clustering. We propose a statistical approach for clustering multilingual documents based on Closed Concepts and vector model partition the documents of one or more collections.An experimental evaluation was conducted on the collection of bilingual documents French-English of CLEF' 2 2003 and showed the merits of this method and the interesting degree of comparability of the obtained bilingual classes.	Mohamed Chebel, Chiraz Latiri	http://editions-rnti.fr/render_pdf.php?p1&p=1002119	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Approche relationnelle de l'apprentissage de séquences	We observe an increasing amount of sequential data, for instance open data sources provide real-time information. In order to apply classical learning algorithms, sequential data are often modelled in an attribute-value setting using a sliding window. In this paper, we propose a relational approach. A first advantage is to let the relational algorithm choose the length of the window. A second advantage is to allow to consider conditions based on the existential quantifier and aggregates. A third advantage is to be able to consider several granularities at the same time.	Clément Charnay, Nicolas Lachiche, Agnès Braud	http://editions-rnti.fr/render_pdf.php?p1&p=1002118	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Big Data and the Dawn of Algorithms in Everything	The mainstream adoption of the internet as a source for knowledge and interaction for the past decades has given rise to new data sources that are characterized by large sizes and rapid creation. In addition, sensory data from mobile devices and machinery are on the rise with similar characteristics. All these sources have the commonality that they will tell us something new or something more detailed than before. From a business standpoint these data sources holds the opportunity to create more customized services and improved products in practically anything, however, they also present a challenge since they are big and typically residing outside the traditional server structure of organizations. This talk will explore the challenges of integrating these new, so-called Big Data, in decision processes. Specifically, we will explore the paradigm shifts when external data become equally or more important than internal data. We will also explore the emerging shift in decision making becoming algorithmic as opposed to human discovery driven.	Morten Middelfart	http://editions-rnti.fr/render_pdf.php?p1&p=1002057	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Big Data is all about data that we don't have	Big Data is now becoming a buzz word in information technology industry and research. Is Big Data only about large volume of data?, and if it is yes, why is it suddenly becoming a trend. Hasn't the growth of data volume been gigantic in the last decade? From a research point of view, it is not surprising to see researchers from all walks of computer science are trying to align their research to Big Data for the sake of being trendy. The question remains whether it tackles the real Big Data problems. In this talk, I will describe the misconceptions of Big Data, present motivating cases, and discuss the unavoidable challenges faced by industry and research.	David Taniar	http://editions-rnti.fr/render_pdf.php?p1&p=1002055	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Challenges and Opportunities in HCI, Visual Analytics and Knowledge Management for the development of Sustainable Cities	While overtly exposed in the media, the challenges faced by our societies to transition towards sustainable energy use are quite formidable. A simple visual refresher of the cold hard facts should amply reveal the importance of visualization to assess the situation. Private companies, such as IBM, and public research centers are joining forces and investing to design and evaluate novel approaches to build and manage Cities, defined as the rational organisation of dense human habitat. Information and Communication technologies are certainly part of the answers, in particular in areas related to knowledge management, data mining, HCI and social computing. Illustrated with telltaling examples of research work carried at IBM, the CSTB and the Efficacity Institute, I will argue that Interactive Information Technologies can help managing the energy transition of cities in 3 key aspects:   -- to support the city design process, notably computer supported tooling and information infrastructure that help taming the complexity of the intertwinning actors and interests at play,   -- to help understand better the city's dynamics, identifiy inefficiencies and reveal optimization opportunities, where knowledge management and extraction is crucial,   -- and foremost, to ease the necessary changes that will have to happen in our mobility and housing habits with novel tools and services that alleviate our energy needs.	Thomas Baudel	http://editions-rnti.fr/render_pdf.php?p1&p=1002058	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Choix d'une mesure de proximité discriminante dans un contexte topologique	Les résultats de toute opération de classification ou de classement d'objets dépendent fortement de la mesure de proximité choisie. L'utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d'équivalence topologique fait appel à la structure de voisinage local.Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la "meilleure" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la "meilleure" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d'apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique.Le principe de l'approche proposée est illustré à partir d'un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d'évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la "meilleur" mesure de proximité discriminante.	Fatima-Zahra Aazi, Rafik Abdesselam	http://editions-rnti.fr/render_pdf.php?p1&p=1002069	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Classification évidentielle avec contraintes d'étiquettes	Ce papier propose une version améliorée de l'algorithme de classification automatique évidentielle semi-supervisée SECM. Celui-ci bénéficie de l'introduction de données étiquetées pour améliorer la pertinence de ses résultats et utilise la théorie des fonctions de croyance afin de produire une partition crédale qui généralise notamment les concepts de partitions dures et floues. Le pendant de ce gain d'expressivité est une complexité qui est exponentielle avec le nombre de classes, ce qui impose en retour l'utilisation de schémas efficaces pour optimiser la fonction objectif. Nous proposons dans cet article une heuristique qui relâche la contrainte classique de positivité liée aux masses de croyances des méthodes évidentielles. Nous montrons sur un ensemble de jeux de données de test que notre méthode d'optimisation permet d'accélérer sensiblement l'algorithme SECM avec un schéma d'optimisation classique, tout en améliorant également la qualité de la fonction objectif.	Violaine Antoine, Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1002071	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Classification multi-label par raisonnement logique pour l'indexation sémantique de documents	Cet article présente une solution centrée sur les ontologies pour la classification multi-label automatique d'information nécessaire à un système de recommandation d'informations économiques.	David Werner, Christophe Cruz, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1002114	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Clustering topologique pour le flux de données	Actuellement, le clustering de flux de données devient le moyen le plus efficace pour partitionner un très grand ensemble de données. Dans cet article, nous présentons une nouvelle approche topologique, appelée G-Stream, pour le clustering de flux de données évolutives. La méthode proposée est une extension de l'algorithme GNG (Growing Neural Gas) pour gérer le flux de données. G-Stream permet de découvrir de manière incrémentale des clusters de formes arbitraires en ne faisant qu'une seule passe sur les données. Les performances de l'algorithme proposé sont évaluées à la fois sur des données synthétiques et réelles.	Mohammed Ghesmoune, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1002072	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Cohérence des données de bases RDF en évolution constante	Le maintien de la qualité et de la fiabilité de bases de connaissances RDF du Web Sémantique est un problème courant. De nombreuses propositions pour l'intégration de « bonnes » données ont été faites, se basant soit sur les ontologies de ces bases, soit sur des méta-données additionnelles. Dans cet article, nous proposons une approche originale, basée exclusivement sur l'étude des données de la base. Le principe est de déterminer si les modifications apportées par la mise à jour candidate rendent la partie ciblée de la base plus similaire - selon certains critères - à d'autres parties existantes dans la base. La mise à jour est considérée cohérente avec cette base et peut être appliquée.	Pierre Maillot, Thomas Raimbault, David Genest	http://editions-rnti.fr/render_pdf.php?p1&p=1002085	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Comparison of linear modularization criteria using the relational formalism, an approach to easily identify resolution limit	La modularisation de grands graphes ou recherche de communautés est abordée comme l'optimisation d'un critère de qualité, l'un des plus utilisés étant la modularité de Newman-Girvan. D'autres critères, ayant d'autres propriétés, aboutissent à des solutions différentes. Dans cet article, nous présentons une réécriture relationnelle de six critères linéaires: Zahn-Condorcet, Owsi´nski- Zadro&#729;zny, l'Ecart à l'Uniformité, l'Ecart à l'Indétermination et la Modularité Equilibrée. Nous utilisons une version générique de l'algorithme d'optimisation de Louvain pour approcher la partition optimale pour chaque critère sur des réseaux réels de différentes tailles. Les partitions obtenues présentent des caractéristiques différentes, concernant notamment le nombre de classes. Le formalisme relationnel nous permet de justifier ces différences d'un point de vue théorique. En outre, cette notation permet d'identifier facilement les critères ayant une limite de résolution (phénomène qui empêche en pratique la détection de petites communautés sur de grands graphes). Une étude de la qualité des partitions trouvées dans les graphes synthétiques LFR permet de confirmer ces résultats.	Patricia Conde-Céspedes, Jean-François Marcotorchino, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1002080	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Compromis précision-rappel dans l'évaluation des performances 	Dans de nombreux problèmes d'apprentissage automatique la performance des algorithmes est évaluée à l'aide des mesures précision et rappel. Or ces deux mesures peuvent avoir une importance très différente en fonction du contexte. Dans cet article nous étudions le comportement des principaux indices de performance en fonction du couple précision-rappel. Nous proposons un nouvel outil de visualisation de performances et définissons l'espace de compromis qui représente les différents indices en fonction du compromis précision-rappel. Nous analysons les propriétés de ce nouvel espace et mettons en évidence ses avantages par rapport à l'espace précision-rappel.	Blaise Hanczar, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002070	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Contribution au calcul du skyline par réduction de l'espace candidat	L'opérateur skyline est devenu un paradigme dans les bases de données. Il consiste à localiser Sky l'ensemble des points d'un espace vectoriel qui ne sont pas dominés. Cet opérateur est utile lorsqu'on n'arrive pas à se décider dans les situations conflictuelles. Le calcul des requêtes skyline est pénalisé par le nombre de points que peuvent contenir les bases de données. Dans ce papier, nous présentons une solution analytique pour la réduction de l'espace candidat et nous proposons une méthode efficace pour le calcul de ce type de requêtes	Lougmiri Zekri, Hadjer Belaicha	http://editions-rnti.fr/render_pdf.php?p1&p=1002082	
Revue des Nouvelles Technologies de l'Information	EGC	2015	D113 : une plateforme open-source dédiée à l'analyse des flux et à la détection des intrusions	Ce travail se situe dans le domaine de la "Cybersécurité", le projet "D113" permet de visualiser en temps réel les flux transitant sur des équipements de filtrage sans avoir recours au traitement manuel des journaux d'événements. Nous centrerons notre démonstration sur la visualisation de grands "graphes" et l'exploitation d'analyses statiques des flux.	David Pierrot, Nouria Harbi	http://editions-rnti.fr/render_pdf.php?p1&p=1002108	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Découverte de proportions analogiques dans les bases de données : une première approche	Cet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue unCet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue une première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».e première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».	William Correa Beltran, Hélène Jaudoin, Olivier Pivert	http://editions-rnti.fr/render_pdf.php?p1&p=1002075	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Détection automatique de reformulations - Correspondance de concepts appliquée à la détection du plagiat	Dans le cadre de la détection du plagiat, la phase de comparaison de deux documents est souvent réduite à une comparaison mot à mot, une recherche de « copier/coller ». Dans cet article, nous proposons une approche naïve de comparaison de deux documents dans le but de détecter automatiquement aussi bien les phrases copiées de l'un des textes dans l'autre que les paraphrases et reformulations, ceci en se focalisant sur l'existence des mots porteurs de sens, ainsi que sur leurs mots de substitution possibles. Nous comparons trois algorithmes utilisant cette approche afin de déterminer la plus efficace pour ensuite l'évaluer face à des méthodes existantes. L'objectif est de permettre la détection des similitudes entre deux textes en utilisant uniquement des mots clefs. L'approche proposée permet de détecter des reformulations non paraphrastiques impossibles à détecter avec des approches conventionnelles faisant appel à une phase d'alignement.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002089	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Détection et regroupement automatique de style d'écriture dans un texte	La détection de plagiat extrinsèque devient vite inefficace lorsque l'on n'a pas accès aux documents potentiellement sources du plagiat ou lorsque l'on se confronte à un espace aussi vaste que leWeb, ce qui est souvent le cas dans les logiciels anti-plagiat actuels. Dès lors la détection intrinsèque devient nettement plus efficace. Dans cet article, nous traitons justement de la détection automatique d'auteurs qui permet de savoir si un passage d'un texte n'appartient pas au même auteur que le reste du texte et donc en théorie de repérer les passages plagiés d'un document. Nous expliquons notre contribution aux procédures déjà existantes et évaluons les limites de notre approche. L'objectif est de permettre la détection et le regroupement de passages d'un document par auteur.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002060	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Deux approches pour catégoriser le risque	Le risque chimique ou alimentaire couvre les situations où les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l'environnement. Les experts qui assurent le contrôle et la gestion de ces substances se retrouvent face à de gros volumes de littérature scientifique, qui doit être analysée pour appuyer la prise de décisions. Nous proposons une aide automatique pour l'analyse de cette littérature. Nous abordons la tâche comme une problématique de catégorisation: il s'agit de catégoriser les phrases des textes dans les classes du risque lié aux substances. Nous utilisons deux approches: par apprentissage supervisé et la recherche d'information. Les résultats obtenus avec l'apprentissage supervisé (toute classe confondue, F-mesure autour de 0,8 pour le risque alimentaire, entre 0,61 et 0,64 pour le risque chimique) sont meilleurs que ceux obtenus avec par recherche d'information (toute classe confondue, F-mesure entre 0,18 et 0,226 pour le risque alimentaire, entre 0,20 et 0,32 pour le risque chimique). Le rappel est compétitif avec les deux approches.	Natalia Grabar, Niña Kerry	http://editions-rnti.fr/render_pdf.php?p1&p=1002067	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Échantillonnage de flux de données sémantiques : Une approche orientée graphe	Nowadays, processing online massive data streams with special techniques like load shedding is an unavoidable alternative to optimize system resources use. In this paper, we propose a graph-oriented approach for load shedding semantic data streams. Our approach, unlike the RDF triple based one, preserves the semantic level of the data streams, which improves the responses quality of the RDF data stream processing systems.	Fethi Belghaouti, Amel Bouzeghoub, Zakia Kazi-aoul, Raja Chiky	http://editions-rnti.fr/render_pdf.php?p1&p=1002120	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Etude de La Pertinence lors de La Sélection de Collections dans les Systèmes Distribués	This paper presents a new function of collection selection. Our function is free of any extracollection parameter and is based on the documents relevance. The ranking of a collection is proportional to its number of relevant documents.	Kheira Mechach, Lougmiri Zekri, Mustapha Kamel Abdi	http://editions-rnti.fr/render_pdf.php?p1&p=1002122	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Extraction complète efficace de chemins pondérés dans un a-DAG	Un nouveau domaine de motifs appelé chemins pondérés condensés a été introduit en 2013 lors de la conférence IJCAI. Le contexte de fouille est alors un graphe acyclique orienté (DAG) dont les sommets sont étiquetés par des attributs. Nous avons travaillé à une implémentation efficace de ce type de motifs et nous montrons que l'algorithme proposé était juste mais incomplet. Nous établissons ce résultat d'incomplétude et nous l'expliquons avant de trouver une solution pour réaliser une extraction complète. Nous avons ensuite développé des structures complémentaires pour calculer efficacement tous les chemins pondérés condensés. L'algorithme est amélioré en performance de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l'appliquons à des données réelles pour motiver qualitativement l'usage des chemins pondérés.	Nazha Selmaoui-Folcher, Frédéric Flouvat, Chengcheng Mu, Jérémy Sanhes, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002077	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Extraction de l'intérêt implicite des utilisateurs dans les attributs des items pour améliorer les systèmes de recommandations	Les systèmes de recommandation ont pour objectif de sélectionner et présenter d'abord les informations susceptibles d'intéresser les utilisateurs. Ce travail expose un système de recommandation qui s'appuie sur deux concepts: des relations sémantiques sur les données et une technique de filtrage collaboratif distribué basée sur la factorisation des matrices (MF). D'une part, les techniques sémantiques peuvent extraire des relations entre les données, et par conséquent, améliorer la précision des recommandations. D'autre part, MF donne des prévisions très précises avec un algorithme facilement parralélisable. Notre proposition utilise cette technique en ajoutant des relations sémantiques au processus. En effet, nous analysons en profondeur les intérêts cachés des utilisateurs dans les attributs des items à recommander. Nous utilisons dans nos expérimentations le jeu de données MovieLens enrichi par la base de données IMDb. Nous comparons notre travail à une technique MF classique. Les résultats montrent une précision dans les recommandations, tout en préservant un niveau élevé d'abstraction du domaine. En outre, nous améliorons le passage à l'échelle du système en utilisant des techniques parallélisables.	Manuel Pozo, Raja Chiky, Elisabeth Métais	http://editions-rnti.fr/render_pdf.php?p1&p=1002093	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Feedback - Study and Improvement of the Random Forest of the Mahout library in the context of marketing data of Orange	L'apprentissage automatique a fait son apparition dans l'écosystème Hadoop créant, de par la puissance promise, une opportunité sans précédent pour ce domaine. Dans cet écosystème, Apache Mahout est une réponse à la question du temps de calcul et/ou de la volumétrie: il consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous portés afin de s'exécuter sur Map/Reduce. Ce rapport se concentre sur le portage et l'utilisation de l'algorithme des Random Forest dans Mahout. Il montre à travers notre retour d'expérience les difficultés qui peuvent être rencontrées tant pratiques que théoriques et suggère une piste d'amélioration.	C. Thao, Nicolas Voisine, Vincent Lemaire, R. Trinquart	http://editions-rnti.fr/render_pdf.php?p1&p=1002104	
Revue des Nouvelles Technologies de l'Information	EGC	2015	gapIT : Un outil visuel pour l'imputation de valeurs manquantes en hydrologie	Les données manquantes sont problématiques en hydrologie, car elles gênent le calcul de statistiques interannuelles et sur de longues périodes, ainsi que l'analyse et l'interprétation de la variabilité des données. Dans cet article, nous présentons gapIT, une plateforme d'analyse de données permettant d'inspecter visuellement les données manquantes et ensuite de choisir la méthode de correction adéquate. Nous avons utilisé l'outil pour estimer les données manquantes dans des séries temporelles correspondant aux débits mesurés par des stations hydrométriques du Luxembourg.	Olivier Parisot, Laura Giustarini, Olivier Faber, Renaud Hostache, Ivonne Trebs, Mohammad Ghoniem	http://editions-rnti.fr/render_pdf.php?p1&p=1002107	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Gestion de l'incertitude dans le cadre d'une extraction des connaissances à partir de texte	The knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.	Fadhela Kerdjoudj, Olivier Curé	http://editions-rnti.fr/render_pdf.php?p1&p=1002116	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Heuristiques pour l'adaptation des mappings entre ontologies dynamiques	Les correspondances sémantiques entre ontologies (mappings) jouent un rôle essentiel dans les systèmes d'information. Cependant, en vertu de l'évolution des connaissances, les éléments ontologiques sont sujets à modification invalidant potentiellement les alignements préalablement établis. Des techniques de maintenance sont donc nécessaires pour maintenir la validité des mappings. Dans cet article, nous présentons un ensemble d'heuristiques guidant leur adaptation. Notre approche s'appuie sur l'explication des mappings existants, les informations provenant de l'évolution des ontologies ainsi que les adaptations possibles applicables aux mappings. Nous proposons une validation expérimentale à partir d'ontologies du domaine médical et des mappings qui leur sont associés.	Julio Cesar Dos Reis, Cédric Pruski, Chantal Reynaud-Delaître	http://editions-rnti.fr/render_pdf.php?p1&p=1002084	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Identification des utilisateurs atypiques dans les systèmes de recommandation sociale	Malgré des performances très satisfaisantes, l'approche sociale de la recommandation ne fournit pas de bonnes recommandations à un sous-ensemble des utilisateurs. Nous supposons ici que certains de ces utilisateurs ont des préférences différentes de celles des autres, nous les qualifions d'atypiques. Nous nous intéressons à leur identification, en amont de la tâche de recommandation, et proposons plusieurs mesures représentant l'atypicité des préférences d'un utilisateur. L'évaluation de ces mesures sur un corpus de l'état de l'art montre qu'elles permettent d'identifier de façon fiable des utilisateurs recevant de mauvaises recommandations.	Benjamin Gras, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1002097	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Identification d'auteurs par apprentissage automatique	Etant donné un ensemble de documents rédigés par un même auteur, le problème d'authentification d'auteurs consiste à décider si un nouveau texte a été rédigé ou non par cet auteur. Pour résoudre ce problème, nous avons proposé et implémenté différentes approches : comptage de similarité, techniques de vote et apprentissage supervisé qui exploitent différents modèles de représentation des documents. Les expérimentations réalisées à partir des collections de la compétition PAN-CLEF 2013 et 2014 ont confirmé l'intérêt de nos approches et leur performance en termes de temps de traitement.	Jordan Frery, Christine Largeron, Mihaela Juganaru-Mathieu	http://editions-rnti.fr/render_pdf.php?p1&p=1002062	
Revue des Nouvelles Technologies de l'Information	EGC	2015	LeveragingWeb 2.0 for Informed Real-Estate Services	The perception about real estate properties, both for individuals and agents, is not formed exclusively by their intrinsic characteristics, such as surface and age, but also from property externalities, such as pollution, traffic congestion, criminality rates, proximity to playgrounds, schools and stimulating social interactions that are equally important. In this paper, we present the Real-Estate 2.0 System that in contrary to existing Real-Estate e-services and applications, takes also into account important externalities. By leveraging Web 2.0 (content from Social Networks, POI listings) applications and Open Data enables the thorough analysis of the current physical and social context of the property, the context-based objective valuation of RE properties, along with an advanced property search and selection experience that unveils otherwise "hidden" property features and significantly reduces user effort and time spent in their RE quest. The system encompasses the above to provide services which assist individuals and agents in making more informed and sound RE decisions.	Papantoniou Katerina, Athanasiadis Marios - Lazaros, Fundulaki Irini, Georgis Christos, Stavrakas Yannis, Troullinos Michalis, Tsitsanis Anastasios	http://editions-rnti.fr/render_pdf.php?p1&p=1002105	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Linked Data Annotation and Fusion driven by Data Quality Evaluation	Dans cet article nous présentons une approche de fusion de données fondée sur l'utilisation d'informations sur la qualité des données pour résoudre les éventuels conflits entre valeurs.	Ioanna Giannopoulou, Fatiha Saïs, Rallou Thomopoulos	http://editions-rnti.fr/render_pdf.php?p1&p=1002086	
Revue des Nouvelles Technologies de l'Information	EGC	2015	L'apport d'une approche symbolique pour le repérage des entités nommées en langue amazighe	Le repérage des Entités Nommées (REN) en langue amazighe est un prétraitement éventuellement essentiel pour de nombreuses applications du traitement automatique des langues (TAL), en particulier pour la traduction automatique. Dans cet article, nous présentons une chaîne de repérage des entités nommées en amazighe fondée sur une étude synthétique des spécificités de la langue et des entités nommées en amazighe. L'article met l'accent sur les choix méthodologiques à résoudre les ambiguïtés dues à la langue, en exploitant les technologies existantes pour d'autres langues.	Meryem Talha, Siham Boulaknadel, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1002061	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Managing Big Multidimensional Data	Multidimensional database concepts such as cubes, dimensions with hierarchies, and measures have been a cornerstone of analytical business intelligence tools for decades. However, the standard data models and system implementations (OLAP) for multidimensional databases cannot handle "Big Multidimensional Data", very large amounts of complex and highly dynamic multidimensional data that occur in a number of emerging domains such as energy, transport, logistics, as well as science. This talk will discuss similarities and differences between traditional Business Intelligence (BI) and Big Data, present examples of Big Multidimensional data with the characteristics of large volume, high velocity (fast data), and/or high variety (complex data) and discuss how to manage Big Multidimensional Data, including modeling, algorithmic, implementation, as well as practical, issues.	Torben Bach Pedersen	http://editions-rnti.fr/render_pdf.php?p1&p=1002056	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Mesure d'influence via les indicateurs de centralité dans les réseaux sociaux	For social network analysis, existing centrality measures emphasize the importance of an actor considering only the structural position in the network regardless of a priori information on this actors such as popularity, accessibility or behavior. In this study new variants of centrality measures are proposed operating both the network structure and the specific attributes of an actor. Experiments have validated the contribution of valuations especially for the detection of broadcasters in social networks.	Oualid Benyahia, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1002112	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Méthode alternative à la détection de « copier/coller » : intersection de textes et construction de séquences maximales communes 	La détection du plagiat passe le plus souvent par la phase de recherche de similitudes la plus naïve, la détection de « copier/coller ». Dans cet article, nous proposons une méthode alternative à l'approche standard de comparaison mot à mot. Le principe étant d'effectuer une intersection des deux textes à comparer, récupérant ainsi un tableau des mots qu'ils ont en commun et de ne conserver que les séquences maximales des mots se suivant dans l'un des textes et existant également dans l'autre. Nous montrons que cette méthode est plus rapide et moins coûteuse en ressources que les méthodes de parcours de textes habituellement utilisées. L'objectif étant de détecter les passages identiques entre deux textes plus rapidement que les méthodes de comparaison mot à mot, tout en étant plus efficace que les méthodes n-grammes.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002065	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Mining Classes by Multi-label Classification	We propose a new approach to mine potential classes in news documents by examining close relationship between new classes and probability vectors of multiple labeling of the documents. Using EM algorithm to obtain the distribution over linear mixture models, we make clustering and mine classes.	Yuichiro Kase, Takao Miura	http://editions-rnti.fr/render_pdf.php?p1&p=1002066	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Modèle de Biclustering dans un paradigme "Mapreduce"	Biclustering is a main task in a variety of areas of machine learning providing simultaneous observations and features clustering. Biclustering approches are more complex compared to the traditional clustering particularly those requiring large dataset and Mapreduce platforms. We propose a new approach of biclustering based on popular self-organizing maps for cluster analysis of large dataset. We have designed scalable implementations of the new biclustering algorithm using MapReduce with the Spark platform. We report the experiments and demonstrated the performance public dataset using different cores. Using practical examples, we demonstrate that our algorithm works well in practice. The experimental results show scalable performance with near linear speedups across different data and 120 cores.	Tugdual Sarazin, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1002111	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Nouvelle approche de contextualisation de tweets basée sur les règles d'association inter-termes	Tweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet's vocabulary by a set of thematically related words using mining association rules between terms.	Meriem Amina Zingla, Mohamed Ettaleb, Chiraz Latiri, Yahia Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1002121	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Pour une meilleure exploitation de la classification croisée dans les systèmes de filtrage collaboratif	Pour la prédiction automatique des items préférés par des utilisateurs sur le Web, différents systèmes de filtrage collaboratif ont été proposés. La plupart d'entre eux sont basés sur la factorisation matricielle et les approches de type k plus proches voisins. Malheureusement ces deux approches requièrent un temps de calcul important. Une partie de ces problèmes a pu être surmontée par la classification croisée ou co-clustering qui s'avère pertinente du fait qu'elle permet par nature une gestion simultanée des ensembles correspondant aux utilisateurs et aux items. Cependant, des travaux doivent encore être menés pour une meilleure prise en compte des données manquantes. Dans ce travail, nous proposons donc une gestion efficace des données non observées permettant une meilleure exploitation du potentiel de la classification croisée dans le domaine des systèmes de recommandation. Nous montrons de plus qu'elle permet d'obtenir des représentations à base de graphes bipartis facilitant l'interprétation interactive des affinités entre des groupes d'utilisateurs et des groupe d'items.	Aghiles Salah, Nicoleta Rogovschi, François Role, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002095	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Proposition d'outil de clustering visuel et interactif	Cet article présente un nouvel outil visuel de clustering interactif. Il utilise une technique de réduction de dimensionnalité pour permettre une représentation 2D des données et des classes associées, initialement établies de manière non-supervisée. L'originalité de l'outil consiste à autoriser des modifications itératives à la fois du clustering et de la projection 2D. Grâce à des contrôles adaptés, l'utilisateur peut ainsi injecter ses préférences, et observer le changement induit en temps réel. La méthode de projection utilisée suit une métaphore physique, qui facilite le suivi des changements par l'utilisateur. Nous montrons un exemple illustrant l'intérêt pratique de l'outil.	Pierrick Bruneau, Philippe Pinheiro, Bertjan Broeksema, Benoît Otjacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002073	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Qualité et complexité en évaluation des mesures d'intérêt	Remplacer des hypothèses sur le modèle de données par des informations mesurées sur les données réelles est l'une des forces de la fouille de données. Cet article étudie cet ajustement entre les données et les méthodes de découverte de motifs pour en évaluer la qualité et la complexité. Nous formalisons ce lien entre données et mesures d'intérêt en identifiant les motifs liés qui sont ceux nécessaires pour l'évaluation d'une mesure ou d'une contrainte. Nous formulons alors trois axiomes que devraient satisfaire ces motifs liés pour qu'une méthode d'extraction se comporte bien. En outre, nous définissons la complexité en évaluation qui quantifie finement l'interrelation entre les motifs au sein d'une méthode d'extraction. A la lumière de ces axiomes et de cette complexité en évaluation, nous dressons une typologie de multiples méthodes de découverte de motifs impliquant la fréquence.	Bruno Crémilleux, Arnaud Giacometti, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002094	
Revue des Nouvelles Technologies de l'Information	EGC	2015	RankMerging: Apprentissage supervisé de classements pour la prédiction de liens dans les grands réseaux sociaux	Trouver les liens manquants dans un grand réseau social est une tâche difficile, car ces réseaux sont peu denses, et les liens peuvent correspondre à des environnements structurels variés. Dans cet article, nous décrivons RankMerging, une méthode d'apprentissage supervisé simple pour combiner l'information obtenue par différentes méthodes de classement. Afin d'illustrer son intérêt, nous l'appliquons à un réseau d'utilisateurs de téléphones portables, pour montrer comment un opérateur peut détecter des liens entre les clients de ses concurrents. Nous montrons que RankMerging surpasse les méthodes à disposition pour prédire un nombre variable de liens dans un grand graphe épars.	Lionel Tabourier, Anne-Sophie Libert, Renaud Lambiotte	http://editions-rnti.fr/render_pdf.php?p1&p=1002102	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Réduction de la complexité spatiale et temporelle du Compact Prediction Tree pour la prédiction de séquences	La prédiction de séquences de symboles est une tâche ayant de multiples applications. Plusieurs modèles de prédiction ont été proposés tels que DG, All-k-order markov et PPM. Récemment, il a été montré qu'un nouveau modèle nommé Compact Prediction Tree (CPT) utilisant une structure en arbre et un algorithme de prédiction plus complexe, offre des prédictions plus exactes que plusieurs approches de la littérature. Néanmoins, une limite importante de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ce problème en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact et est 4.5 fois plus rapide que CPT, tout en conservant une exactitude très élevée par rapport à All-K-order Markov, DG, Lz78, PPM et TDAG.	Ted Gueniche, Philippe Fournier-Viger	http://editions-rnti.fr/render_pdf.php?p1&p=1002064	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Regroupement d'attributs par règles d'association dans les systèmes d'inférence floue	Dans les systèmes d'apprentissage supervisé par construction de règles de classification floues, un nombre élevé d'attributs descriptifs conduit à une explosion du nombre de règles générées et peut affecter la précision des algorithmes d'apprentissage. Afin de remédier à ce problème, une solution est de traiter séparément des sous-groupes d'attributs. Cela permet de décomposer le problème d'apprentissage en des sous-problèmes de complexité inférieure, et d'obtenir des règles plus intelligibles car de taille réduite. Nous proposons une nouvelle méthode de regroupement des attributs qui se base sur le concept des règles d'association. Ces règles découvrent des relations intéressantes entre des intervalles de valeurs des attributs. Ces liaisons locales sont ensuite agrégées au niveau des attributs mêmes en fonction du nombre de liaisons trouvées et de leur importance. Notre approche, testée sur différentes bases d'apprentissage et comparée à l'approche classique, permet d'améliorer la précision tout en garantissant une réduction du nombre de règles.	Ilef Ben Slima, Amel Borgi	http://editions-rnti.fr/render_pdf.php?p1&p=1002092	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Régularisation de noyaux temporellement élastiques et analyse en composantes principales non-linéaire pour la fouille de séries temporelles 	Dans le domaine de la fouille de séries temporelles, plusieurs travaux récents exploitent des noyaux construits à partir de distances élastiques de type Dynamic Time Warping (DTW) au sein d'approches à base de noyaux. Pourtant les matrices, apparentées aux matrices de Gram, construites à partir de ces noyaux n'ont pas toujours les propriétés requises ce qui peut les rendre in fine impropres à une telle exploitation. Des approches émergeantes de régularisation de noyaux élastiques peuvent être mises à profit pour répondre à cette insuffisance. Nous présentons l'une de ces méthodes, KDTW, pour le noyau DTW, puis, autour d'une analyse en composantes principales non-linéaire (K-PCA), nous évaluons la capacité de quelques noyaux concurrents (élastiques v.s non élastiques, définis v.s. non définis) à séparer les catégories des données analysées tout en proposant une réduction dimensionnelle importante. Cette étude montre expérimentalement l'intérêt d'une régularisation de type KDTW.	Pierre-François Marteau	http://editions-rnti.fr/render_pdf.php?p1&p=1002063	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Requêtes Skyline en présence des données évidentielles	Dans cet article, nous nous intéressons à la recherche des points les plus intéressants au sens de l'ordre de Pareto, dans les bases de données évidentielles. Nous présentons le modèle skyline évidentiel qui est adapté à la nature des données incertaines. Ensuite, nous présentons une évaluation expérimentale de notre approche.	Sayda Elmi, Karim Benouaret, Allel HadjAli, Mohamed Anis Bach Tobji, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1002081	
Revue des Nouvelles Technologies de l'Information	EGC	2015	To initiate a corporate memory with a knowledge compendium: ten years of learning from experience with the Ardans method	Ardans method ArdansSas (2006b) and technology ArdansSas (2006a) of knowledge capitalization and structuration are used with different industries (automotive, aerospace, energy, defence, steel, health, etc.) for more than a decade in France and Europe.The proposed solutions in knowledge management and especially in expertise capitalisation have set a lot of feedback over time. With a view toward ongoing improvement, what are the impacts of these feedbacks on the method nowadays? Put into practice into the industry, the return of investment of a capitalization campaign is inferred from the quality of the knowledge base delivered at the end of the campaign. Therefore, the method and the technology are intrinsically connected. How IT tools can assist with the quality diagnosis of the knowledge base?A comparative study was conducted on the basis of the method Mariot et al. (2007) exposed at EGC'2007. This article sets out the results of the changes and improvements of the method, in conjunction with the latest technical and scientific development on the one hand, and the change of the industry needs on the other hand.	Vincent Besson, Alain Berger	http://editions-rnti.fr/render_pdf.php?p1&p=1002103	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Towards Linked Data Extraction From Tweets	Millions of Twitter users post messages every day to communicate with other users in real time information about events that occur in their environment. Most of the studies on the content of tweets have focused on the detection of emerging topics. However, to the best of our knowledge, no approach has been proposed to create a knowledge base and enrich it automatically with information coming from tweets. The solution that we propose is composed of four main phases: topic identification, tweets classification, automatic summarization and creation of an RDF triplestore. The proposed approach is implemented in a system covering the entire sequence of processing steps from the collection of tweets written in English language (based on both trusted and crowd sources) to the creation of an RDF dataset anchored in DBpedia's namespace.	Manel Achichi, Zohra Bellahsene, Dino Ienco, Konstantin Todorov	http://editions-rnti.fr/render_pdf.php?p1&p=1002100	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Ultrametricity of Dissimilarity Spaces and Its Significance for Data Mining	Nous introduisons une mesure d'ultramétricité pour les dissimilaritées et examinons les transformations des dissimilaritées et leurs impact sur cette mesure. Ensuite, nous étudions l'influence de l'ultramétricité sur la comportement de deux classes d'algorithmes d'exploration de données (le kNN algorithme de classification et l'algorithme de regroupement PAM) appliqués sur les espaces de dissimilarité. On montre qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour les clusters, une augmentation d'ultramétricité genere regroupements avec une meilleure séparation. Une diminution de la ultramétricité produit groupes plus compacts.	Dan Simovici, Rosanne Vetro, Kaixun Hua	http://editions-rnti.fr/render_pdf.php?p1&p=1002068	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Un algorithme EM pour une version parcimonieuse de l'analyse en composantes principales probabiliste	Nous considérons une version parcimonieuse de l'analyse en composantes principales probabiliste. La pénalité `1 imposée sur les composantes principales rend leur interprétation plus aisée en ne faisant dépendre ces dernières que d'un nombre restreint de variables initiales. Un algorithme EM, simple de mise en oeuvre, est proposé pour l'estimation des paramètres du modèle. La méthode de l'heuristique de pente est finalement utilisée pour choisir le coefficient de pénalisation.	Charles Bouveyron, Julien Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002074	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Un algorithme ICM basé sur la compacité pour la segmentation des images satellites à très haute résolution	Dans cet article nous proposons une modification pour l'algorithme "Iterated Conditional Modes" (ICM) appliqué à la segmentation d'images à très haute résolution. Pour ce faire, nous introduisons un nouveau critère de convergence basé sur la compacité des clusters et qui repose sur une fonction d'énergie adaptée aux modèles de voisinages irréguliers de ce type d'images. Grâce à cette méthode, nos premières expériences ont montré que nous obtenons des résultats plus fiables en terme de convergence et de meilleure qualité qu'en utilisant l'énergie globale comme critère d'arrêt.	Jérémie Sublime, Younès Bennani, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1002078	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Un langage d'interrogation à la SPARQL pour les graphes conceptuels	Cet article propose un langage générique d'interrogation pour le modèle des graphes conceptuels. D'abord, nous introduisons les graphes d'interrogation. Un graphe d'interrogation est utilisé pour exprimer un « ou » entre deux sous-graphes, ainsi qu'une « option » sur un sous-graphe optionnel. Ensuite, nous proposons quatre types de requêtes (interrogation, sélection, description et construction) en utilisant les graphes d'interrogation. Enfin, les réponses à ces requêtes sont calculées à partir d'une opération basée sur l'homomorphisme de graphe.	Marc Legeay, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1002083	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une approche centrée graine pour la détection de communautés dans les réseaux multiplexes	Nous nous intéressons dans ce travail au problème de détection de communautés dans les réseaux multiplexes. Le modèle de réseau multiplexe a été récemment introduit afin de faciliter la modélisation des réseaux multirelationnels, des réseaux dynamiques et/ou des réseaux attribués. Les approches existantes pour la détection de communautés dans ce genre de graphes sont, pour la plupart, basées sur des schémas d'agrégation de couches ou d'agrégation de partitions. Nous proposons ici une nouvelle approche centrée graine qui permet de prendre en compte directement la nature multi-couche d'un réseau multiplexe. Des expérimentations effectuées sur différents réseaux multiplexes montrent que notre approche surpasse les approches de l'état de l'art en termes de qualité des communautés identifiées.	Issam Falih, Manel Hmimida, Rushed Kanawati	http://editions-rnti.fr/render_pdf.php?p1&p=1002099	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une approche de visualisation analytique pour comparer les modèles de propagation dans les réseaux sociaux	Les modèles de propagation d'informations, d'influence et d'actions dans les réseaux sociaux sont nombreux et diversifiés rendant le choix de celui approprié à une situation donnée potentiellement difficile. La sélection d'un modèle pertinent pour une situation exige de pouvoir les comparer. Cette comparaison n'est possible qu'au prix d'une traduction des modèles dans un formalisme commun et indépendant de ceux-ci. Nous proposons l'utilisation de la réécriture de graphes afin d'exprimer les mécanismes de propagation sous la forme d'un ensemble de règles de transformation locales appliquées selon une stratégie donnée. Cette démarche prend tout son sens lorsque les modèles ainsi traduits sont étudiés et simulés à partir d'une plate-forme de visualisation analytique dédiée à la réécriture de graphe. Après avoir décrit les modèles et effectué différentes simulations, nous exhibons comment la plate-forme permet d'interagir avec ces formalismes, et comparer interactivement les traces d'exécution de chaque modèle grâce à diverses mesures soulignant leurs différences.	Jason Vallet, Bruno Pinaud, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1002098	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une nouvelle formalisation des changements ontologiques composés et complexes	L'évolution d'une ontologie est un processus indispensable dans son cycle de vie. Elle est exprimée et définie par des changements ontologiques de différents types : élémentaires, composés et complexes. Les changements complexes et composés sont très utiles dans le sens où ils aident l'utilisateur à adapter son ontologie sans se perdre dans les détails des changements élémentaires. Cependant, ils cachent derrière une formalisation sophistiquée puisqu'ils affectent, à la fois, plusieurs entités ontologiques et peuvent causer des inconsistances à l'ontologie évoluée. Pour adresser cette problématique, cet article présente une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés. Cette formalisation s'appuie sur l'approche algébrique Simple Pushout (SPO) de transformation de graphes et possède deux principaux avantages : (1) fournir une nouvelle formalisation permettant de contrôler les transformations de graphes et éviter les incohérences d'une manière a priori, (2) simplifier la définition des changements composés et complexes en réduisant le nombre de changements élémentaires nécessaires à leur application.	Mariem Mahfoudh, Laurent Thiry, Germain Forestier, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1002087	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une nouvelle méthode de Web Usage Mining basée sur une analyse sémiotique du comportement de navigation	L'objectif de nos travaux est de proposer une méthode d'analyse automatique du comportement des utilisateurs à des fins de prédiction de leur propension à réaliser une action suggérée. Nous proposons dans cet article une nouvelle méthode de Web Usage Mining basée sur une étude sémiotique des styles perceptifs, considérant l'expérience de l'utilisateur comme élément déterminant de sa réaction à une sollicitation. L'étude de ces styles nous a amené à définir de nouveaux indicateurs (des descripteurs sémiotiques) introduisant un niveau supplémentaire à l'approche sémantique d'annotation des sites. Nous proposons ensuite un modèle neuronal adapté au traitement de ces nouveaux indicateurs. Nous expliquerons en quoi le modèle proposé est le plus pertinent pour traiter ces informations.	Sandra Mellot, Tony Bourdier, Moez Baccouche	http://editions-rnti.fr/render_pdf.php?p1&p=1002090	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Une Plateforme ETL parallèle et distribuée pour l'intégration de données massives	Nous nous intéressons, dans ce papier, à l'impact des données massives dans un environnement décisionnel et plus particulièrement sur la phase d'intégration des données. Dans ce contexte, nous avons développé une plateforme, baptisée P-ETL (Parallel-ETL), destinée à l'entreposage de données massives selon le paradigme MapReduce. P-ETL permet le paramétrage de processus ETL (workflow) et un paramétrage avancé relatif à l'environnement parallèle et distribué. Ce papier décrit la plateforme P-ETL en vue d'une démonstration. Face à des jeux de données allant de 244 * 106 à 7, 317 * 109 tuples, les expérimentations menées ont montré l'amélioration significative des performances de P-ETL lorsque la taille du cluster et le nombre des tâches parallèles augmentent.	Mahfoud Bala, Oussama Mokeddem, Omar Boussaid, Zaia Alimazighi	http://editions-rnti.fr/render_pdf.php?p1&p=1002109	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Using Social Conversational Context For Detecting Users Interactions on Microblogging Sites	Dans ce travail, nous proposons une nouvelle méthode de détection des conversations sur les sites des réseaux sociaux. Cette méthode est basée sur l'analyse et l'enrichissement de contenu dans le but de présenter un résultat informatif basé sur les interactions des utilisateurs. Nous avons évalué notre méthode sur corpus recueillis de réseau social lié à des sujets spécifiques, et nous avons obtenu des bons résultats.	Rami BELKAROUI, Rim Faiz, Aymen Elkhlifi	http://editions-rnti.fr/render_pdf.php?p1&p=1002101	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Utilisation des pyramides pour visualiser la contamination des manuscrits	In this paper we present a new codicum stemma visualization method. Don Quentin's modeling is usec to classify the textual tradition.We supplement the genealogical editor's information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets	Marc Le Pouliquen	http://editions-rnti.fr/render_pdf.php?p1&p=1002115	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Vers la découverte de modèles exceptionnels locaux : des règles descriptives liant les molécules à leurs odeurs	Issue d'un phénomène complexe partant d'une molécule odorante jusqu'à la perception dans le cerveau, l'olfaction reste le sens le plus difficile à appréhender par les neuroscientifiques. L'enjeu principal est d'établir des règles sur les propriétés physicochimiques des molécules (poids, nombre d'atomes, etc.) afin de caractériser spécifiquement un sous-ensemble de qualités olfactives (fruité, boisé, etc.). On peut trouver de telles règles descriptives grâce à la découverte de sous-groupes ("subgroup discovery"). Cependant les méthodes existantes permettent de caractériser soit une seule qualité olfactive ; soit toutes les qualités olfactives à la fois ("exceptional model mining") mais pas un sousensemble. Nous proposons alors une approche de découverte de sous-groupes caractéristiques de seulement certains labels, par une nouvelle technique d'énumération, issue de la fouille de redescriptions. Nous avons expérimenté notre méthode sur une base de données d'olfaction fournie par des neuroscientifiques et pu exhiber des premiers sous-groupes intelligibles et réalistes.	Guillaume Bosc, Mehdi Kaytoue, Marc Plantevit, Fabien De Marchi, Moustafa Bensafi, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002091	
Revue des Nouvelles Technologies de l'Information	EGC	2015	Visualizing Shooting Spots using Geo-tagged Photographs from Social Media Sites	Hotspots, à laquelle de nombreuses photographies ont été prises, pourraient être des lieux intéressants pour beaucoup de gens faire du tourisme. Visualisation des hotspots révèle les intérêts des utilisateurs, ce qui est important pour les industries telles que la recherche et du marketing touristiques. Bien que plusieurs techniques basées sociaux-pour hotspots extraction indépendamment ont été proposés, un hotspot a une relation à d'autres hotspots dans certains cas. Pour organiser ces hotspots, nous proposons une méthode pour détecter et de visualiser les relations entre les hotspots. Notre méthode proposée détecte et évalue les relations de taches de tir et sujets photographiques. Notre approche extrait les relations à l'aide de sous-hotspots, qui sont fendus d'un hotspot qui comprend des photographies de différents types.	Masaharu Hirota, Masaki Endo, Shohei Yokoyama, Hiroshi Ishikawa	http://editions-rnti.fr/render_pdf.php?p1&p=1002076	
Revue des Nouvelles Technologies de l'Information	EGC	2015	XEWGraph : Outil de Visualisation et Analyse des Hypergraphes pour un Système d'Intelligence Economique	The Competitive Intelligence System Xplor EveryWhere helps searching, visualizing, and sharing useful data. In this paper, we will intorduce Xplor EveryWhere and its newest feature called XEWGraph, which is dedicated to the analysis of massive data and visualization of hypergraphs.	Zakaria Boulouard, Amine El Haddadi, Anass El Haddadi, Lahcen Koutti, Abdelhadi Fennan	http://editions-rnti.fr/render_pdf.php?p1&p=1002117	
Revue des Nouvelles Technologies de l'Information	AAFD	2014	Apprentissage et Factorisation pour la Recommandation	De nombreux sites de l'internet proposent aujourd'hui des conseils personnalisés élaborés par un système de recommandation, un ensemble de logiciels et de méthodes permettant de suggérer automatiquement des articles pouvant être utiles à un utilisateur. L'arrivée des réseaux sociaux en ligne a ajouté une nouvelle dimension à la recommandation, dans laquelle la structure du graphe social peut être utilisée comme source d'informations. Cet article constitue un état de l'art de méthodes de recommandation orientées modèle, que sont les méthodes de factorisation dont l'intérêt a été démontré lors du challenge Netflix. Il présente également un travail montrant une approche qui prend en compte le réseau social et qui réalise de la prédiction sur des données d'appréciation implicite (implicit feedback). Nous avons testé ce modèle sur des données réelles volumineuses et observé l'amélioration des performances obtenues par d'autres méthodes de l'état de l'art.	Julien Delporte, Stéphane Canu, Alexandros Karatzoglou	http://editions-rnti.fr/render_pdf.php?p1&p=1001992	
Revue des Nouvelles Technologies de l'Information	AAFD	2014	Étude comparative d'extraction de règles d'association positives et négatives et optimisations	La littérature s'est beaucoup intéressée à l'extraction de règles d'association positives et peu à l'extraction de règles négatives en raison essentiellement du coût de calculs et du nombre prohibitif de règles extraites qui sont pour la plupart redondantes et inintéressantes. Dans cet article, nous nous sommes intéressés aux algorithmes d'extraction de RAPN (Règles d'Association Positives et Négatives) reposant sur l'algorithme fondateur Apriori. Nous avons fait une étude de ceux-ci en mettant en évidence leurs avantages et leurs inconvénients. A l'issue de cette étude, nous avons proposé un nouvel algorithme qui améliore cette extraction au niveau du nombre et de la qualité des règles extraites (recherche de motifs raisonnablement fréquents et utilisation d'une mesure d'intérêt supplémentaire) et au niveau du parcours de recherche des règles (étude de la moitié des règles négatives potentiellement valides et proposition de règles d'élagage). L'étude s'est terminée par une évaluation de cet algorithme sur deux bases de données.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001993	
Revue des Nouvelles Technologies de l'Information	AAFD	2014	Modularisation et Recherche de Communautés dans les réseaux complexes par Unification Relationnelle	Un graphe étant un ensemble d'objets liés par une certaine relation typée, le problème de "modularisation" des grands graphes (qui revient à leur partitionnement en classes) peut, alors, être modélisé mathématiquement via l'Analyse Relationnelle. Cette modélisation permet de comparer sur les mêmes bases un certain nombre de critères de découpage de graphe c'est-à-dire de modularisation. Nous proposons une réécriture Relationnelle des critères de modularisation connus tels le critère de Newman-Girvan, le critère de Mancoridis-Gansner, le critère de Zahn-Condorcet, etc. Cette approche permet de faciliter leur compréhension, et d'interpréter plus clairement leurs finalités en y associant la preuve de leur utilité dans certains contextes pratiques.	Patricia Conde-Céspedes, François Marcotorchino	http://editions-rnti.fr/render_pdf.php?p1&p=1001995	
Revue des Nouvelles Technologies de l'Information	AAFD	2014	Prévision de liens dans les graphes bipartites avec attributs	Les réseaux sociaux se modélisent fréquemment par des graphes exprimant les relations explicites ou implicites entre les entités considérées. Ces graphes sont très dynamiques: de nouveaux liens, de nouvelles entités apparaissent et disparaissent rapidement. Ce travail porte sur la prévision de liens dans les graphes bipartites dynamiques, en particulier dans le cas où des données (attributs) sont associées aux entités. Ce cas est important en pratique, notamment pour les systèmes de recommandation: la prévision de liens dans un réseau Clients/Produits revient en effet à prévoir les produits qu'un client est susceptible d'acquérir dans un avenir proche.Le problème de prévision de liens peut s'aborder en considérant les propriétés structurelles du graphes (approches topologiques) ou via les systèmes de recommandation (eg filtrage collaboratif). Nous proposons des méthodes qui prennent en compte simultanément les attributs des noeuds et des liens. Nous illustrons ces méthodes sur le cas bien connu des graphes de collaborations scientifiques, où nos modèles utilisent à la fois les relations de co-publications et les résumés des articles.Pour les graphes bipartites, une proposons approche basée sur les règles d'association liées au voisinage des noeuds. L'évaluation sur 4 sections d'arXiv montre que ces méthodes permettent d'obtenir, par rapport aux approches topologiques et le filtrage collaboratif, une amélioration d'AUC située entre 6% et 16%.	Vanessa Kamga, Maurice Tchuente, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001994	
Revue des Nouvelles Technologies de l'Information	AAFD	2014	Utilisation des réseaux sociaux dans la lutte contre la fraude à la carte bancaire sur Internet	Du fait de sa croissance très rapide, le commerce électronique devient une cible majeure pour les fraudeurs. La fraude à la carte bancaire sur Internet est le fait de réseaux internationaux du crime organisé. La lutte contre cette fraude est donc un objectif majeur pour assurer la sécurité des moyens de paiement. Les systèmes classiques de détection de la fraude sont basés depuis les années 80 sur des techniques de data mining. Aujourd'hui, les techniques d'analyse des réseaux sociaux sont très largement décrites dans la littérature. Nous présentons ici une méthodologie permettant d'utiliser ces techniques dans la lutte contre la fraude, à la fois pour la détection et pour l'investigation. Nous présentons les résultats obtenus dans le cadre du projet collaboratif eFraudBox, mené avec le GIE Cartes Bancaires.	Françoise Fogelman-Soulié, Amine Mekki, Savaneary Sean, Philippe Stepniewski	http://editions-rnti.fr/render_pdf.php?p1&p=1001996	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	An appropriate variance reduction of an adjusted premium estimator in simulated insurance data		Kmar Fersi, Kamal Boukhetala, Samir Ben Ammou	http://editions-rnti.fr/render_pdf.php?p1&p=1002007	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	Aternatives au modèle de Cox Application en assurance automobile	Ce travail traite de méthodologie, pour l'étude de durée de vie de contrats d'assurance VAM. L'approche développée est celle de modèles de durée, selon les profils de description. Les données consistent en une grande base, correspondant au portefeuille automobile d'une grande compagnie d'assurance européenne. On teste, par différentes approches, l'hypothèse de proportionnalité des hasards instantanés, à la base du modèle de Cox s'avérant inadéquat. On montre que pour ces données, le choix se porte sur un modèle faisant évoluer les paramètres en fonction du temps : le modèle de Aalen.	Farid Beninel, Jean-Marie Marion	http://editions-rnti.fr/render_pdf.php?p1&p=1002009	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	Characteristics of the demand for private long-term care insurance in France : a step by step estimation algorithm in an analytical CRM context		Sébastien Nouet, Maximilien Nayaradou, Manuel Plisson	http://editions-rnti.fr/render_pdf.php?p1&p=1002004	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	Les statistiques au service de la lutte contre la fraude	Suivant les problématiques étudiées, les comportements de fraudeurs sont très différents les uns des autres : fraude à l'assurance, fraude au prestation de santé, fraude à la carte bancaire, fraude sur internet, ...Toutefois, un point commun rassemble ces différentes typologies : la volonté pour le fraudeur de tirer un bénéfice important de son acte illégal. Nous verrons dans cet article, comment les méthodes d'analyse statistique ont pleinement leur place au sein des différentes stratégies mises en oeuvre pour lutter contre la fraude.	Yves Péchiné	http://editions-rnti.fr/render_pdf.php?p1&p=1002006	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	Modélisation stochastique du risque de dégradation par processus de diffusion	Ce travail a pour but d'étudier le modèle de dégradation structurelle, qui représente l'évolution temporelle de la taille d'un défaut de fissuration dans des structures physiques. La description détaillée du phénomène et la définition de ses paramètres physiques sont représentées. Des solutions analytiques du processus de dégradation sont examinées. La loi de probabilité du premier instant de franchissement par la taille de la fissure, d'un seuil de dégradation indésirable, est estimée.	Kamal Boukhetala, Nawel Khellouf	http://editions-rnti.fr/render_pdf.php?p1&p=1002008	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	Optimisation du ciblage des opérations de fidélisation	Cet article présente les fondements d'une méthode de sélection commerciale radicalement nouvelle, basée sur le second principe de ciblage, pour l'optimisation des campagnes anti-attrition, la maximisation des ventes additionnelles des campagnes de ventes croisées et la minimisation du nombre de personnes sélectionnées pour les opérations commerciales.	Jean-Blaise Diebold	http://editions-rnti.fr/render_pdf.php?p1&p=1002005	
Revue des Nouvelles Technologies de l'Information	ASSU	2014	Transfer Learning Using Logistic Regression in Credit Scoring		Waad Bouaguel, Farid Beninel, Ghazi Bel Mufti	http://editions-rnti.fr/render_pdf.php?p1&p=1002003	
Revue des Nouvelles Technologies de l'Information	CAL	2014	A Practical Approach to the Measurement of Similarity between WSDL-basedWeb Services	Similarity measurement between web services is a key solution tobenefit from the reuse of the large number of web services freely available inthe internet. This paper presents a practical approach that enables an effectivemeasurement of web service similarity based on their interfaces descibed withWSDL. The approach relies on the use of multiple matching techniques and differentsemantic and structural similarity metrics. The measurement of similaritydetermines the best substitute for a failed web service. So, it serves as a goodindicator of the substitutability relation and thus of the capacity for reuse. A supporttool, implementing the approach, is also presented with some experimentalresults conducted on real-world web services.	Okba Tibermacine, Chouki Tibermacine, Foudil Cherif	http://editions-rnti.fr/render_pdf.php?p1&p=1002037	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Apports des architectures logicielles pour l'Internet des objets	Les architectures logicielles supportent désormais non seulement laconception mais aussi l'exécution de systèmes logiciels complexes. Dans cetteprésentation, nous présentons les résultats obtenus par notre équipe dans le domainedu développement de plateformes intergicielles supportant l'exécutiond'architectures logicielles orientées services réparties sur l'Internet. Nous montronsensuite comment les principes adoptés à l'échelle de l'Internet des servicespeut être transposés dans l'Internet des objets. L'adoption d'architectureslogicielles dans le développement de systèmes contraints tels que des capteurscontribue non seulement à améliorer l'empreinte énergétique de ces systèmesmais il met aussi en évidence la nécessité pour les architectures logicielles d'évoluerpour proposer des notations plus canoniques et réutilisables. La dernièrepartie de cette présentation couvre donc des travaux plus récents sur le prototypagerapide d'architectures en utilisant un langage dédié à la programmation desarchitectures logicielles.	Romain Rouvoy	http://editions-rnti.fr/render_pdf.php?p1&p=1002036	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Conception de profils UML pour la Gestion de Données de Référence	Dans cet article, nous proposons un formalisme permettant à des utilisateursd'être impliqués dans les phases de conception de modèles de Gestion deDonnées de Référence (dite MDM), tout en faisant abstraction des spécificitéstechniques de la plateforme cible. Pour cela, des profils sont définis pour étendrela sémantique d'UML, trop générique, à celle du MDM par l'intermédiaire d'unprofil XML Schema. L'intégration de ces profils se fait dans le contexte de laplateforme EBX5 qui est un XMLWare.	Myriam Lamolle, Chan Le Duc, Ludovic Menet	http://editions-rnti.fr/render_pdf.php?p1&p=1002038	
Revue des Nouvelles Technologies de l'Information	CAL	2014	IMOCA : une architecture à base de modes de fonctionnement pour les systèmes de contrôle de processus	Les systèmes de contrôle de processus s'appuient sur plusieurs modesde fonctionnement dépendants du contexte. Dans un environnement naturel fortementperturbé, il est alors nécessaire de répondre aux trois questions :1. Quels sont les modes pertinents ?2. Comment enchaîner plusieurs modes ?3. Comment paramétrer les modes ?Pour répondre à ces questions, IMOCA propose une architecture logicielle etune méthodologie épaulées par des outils de simulation et de mise au point. Lechoix des modes et leur enchaînement reposent sur une expertise métier. L'architectureassure la mise en relation des modes et des contextes (données), lagestion de l'enchaînement des modes par un automate et la mise au point du paramétragedes modes. L'architecture, indépendante des technologies de capteurset d'actionneurs, s'appuie sur trois contrôleurs dits réactif, expert et adaptatiffonctionnant de manière parallèle et asynchrone.IMOCA est appliquée au problème du pilotage automatique d'un voilier.	Goulven Guillou, Jean-Philippe Babau	http://editions-rnti.fr/render_pdf.php?p1&p=1002045	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Kalimucho : Plateforme de supervision d'applications sensibles au contexte	Le développement d'applications ubiquitaires est particulièrementcomplexe. Au-delà de l'aspect dynamique de telles applications, l'évolutionde l'informatique vers la multiplication des terminaux d'accès mobiles nefacilite pas les choses. Une solution pour simplifier le développement etl'exploitation de telles applications est d'utiliser des plateformes logiciellesdédiées au déploiement et à l'adaptation des applications et gérantl'hétérogénéité des périphériques. Elles permettent aux concepteurs de sefocaliser sur les aspects métiers et facilitent la réutilisation. C'est dans cetteoptique qu'a été conçue et développée la plateforme Kalimucho. Elle permetl'exécution et la supervision d'applications à base de composants logiciels etoffre aux applications un accès à leur contexte d'exécution.	Keling Da, Marc Dalmau, Philippe Roose	http://editions-rnti.fr/render_pdf.php?p1&p=1002046	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Patrons transactionnels dynamiques pour des services composés fiables et flexibles	Dans cet article, nous proposons une solution pour spécifier une compositiondynamique fiable et flexible. Pour y parvenir, nous introduisons unnouveau concept appelé les patrons transactionnels dynamiques. Ce nouveauconcept est une convergence entre les patrons workflows dynamiques et les modèlestransactionnels avancés. Il peut être considéré comme une coordinationdynamique et comme une transaction structurelle. Nous intégrons l'expressivitéet la puissance des patrons de workflow dynamique et la fiabilité des modèlestransactionnels avancés. Nous utilisons un ensemble de règles pour assurer unecomposition dynamique fiable et flexible de services Web transactionnels.	Imed Abbassi, Mohamed Graiet, Eric Cariou, Zied Jaoua	http://editions-rnti.fr/render_pdf.php?p1&p=1002039	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Réseaux FIFO Colorés Stricts pour la formalisation des applications de visualisation scientifique interactives	La programmation par composants est devenue une approche essentielleet très utilisée en génie logiciel. En particulier, dans le cadre des applicationsde visualisation scientifique interactives, cette approche offre une architectureclaire aux développeurs permettant de bien séparer les différentes partiesfonctionnelles de l'application comme l'interaction, la simulation et la visualisation.La modélisation d'applications de visualisation scientifique interactive doitpermettre la description des comportements de chaque composant et de l'assemblagede ces composants en une application. Cet assemblage s'exprime par unschéma de communications qui peut être très complexe avec la possibilité deperdre des messages pour gagner en performance. Nous proposons un modèlepar composants spécifique pour ces applications, associé à une formalisation pardes réseaux FIFO colorés. Le modèle a pour objectifs de décrire les différentscomportements des composants et du réseau de communications en les formulantafin d'offrir des outils de vérifications.	Abderrahim Ait Wakrime, Sébastien Limet, Sophie Robert	http://editions-rnti.fr/render_pdf.php?p1&p=1002043	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Squash, un modèle d'évaluation de la qualité des systèmes d'informations	A partir de l'étude des normes ISO 9126 et SQuaRE notamment, nousavons formalisé un modèle de qualité nommé Squash, prenant en compte lesparticularités et les exigences du domaine industriel. Nous avons tout d'abordentrepris de poser une définition de la qualité et de définir les propriétés quenous voulions voir respecter par notre modèle en nous basant à la fois sur lesmodèles existants mais également sur les exigences des entreprises, partenairesde nos travaux de recherche. Puis nous avons déterminé la liste des propriétésindispensables à une bonne évaluation de la qualité que nous avons appliqués ànos différents modes de calcul : évaluation à partir de métriques ou de donnéesnon issues de métriques. A partir de ces propriétés nous avons construit un modèlede qualité appelé Squash, qui permet d'adapter l'évaluation de la qualitélogicielle en fonction des systèmes et des exigences liées au système.	Karine Mordal	http://editions-rnti.fr/render_pdf.php?p1&p=1002042	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Test de conformité basé sur l'architecture logicielle	Au cours des deux dernières décennies, l'architecture logiciellea joué un rôle central dans le développement des systèmes logiciels. Il fournitune description de haut niveau pour les systèmes complexes de grandetaille en utilisant des abstractions appropriées pour les composants dusystème et pour leurs interactions. Dans notre travail, l'architecture logicielleest décrite en utilisant un langage de description architecturale(Architecture Description Language ou ADL) formel appelé -ADL-C&C.L'un des objectifs de cet ADL est de permettre la validation formelled'un système implémenté, par rapport à son modèle architectural. Dansl'article, nous proposons une approche fondée sur le test de conformitépour valider l'implémentation du système par rapport à son architecture.Les tests architecturaux sont dérivés à partir d'un système de transitions,représentant la structure de l'architecture d'un système et de ses comportements,et sont exécutés sur le système sous test. Pour illustrer notreapproche, nous utilisons l'exemple de la machine à café.	Elena Leroux, Flavio Oquendo	http://editions-rnti.fr/render_pdf.php?p1&p=1002040	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Vers une approche d'analyse et de diagnostic pour les architectures M2M autonomes et sensibles au contexte	L'évolution incontournable des systèmes ubiquitaires et en particulierles systèmes machine-to-machine (M2M) provoque la complexité de la gestionde ce type de système. La conception d'un tel système autonome et sensible aucontexte basé sur une architecture d'auto-adaptabilité représente une solutionprometteuse. Cette architecture modifie le comportement du système en fonctiondes besoins des utilisateurs, des ressources disponibles et de l'environnementde l'application. Dans cet article, nous proposons une approche d'analyseet de diagnostic des systèmes M2M en se basant sur des méthodes statistiquesqui visent, en premier lieu, à détecter les changements de contexte (phase d'analyse).En deuxième lieu, le module de diagnostic permet d'identifier les causesdes changements de contexte détectés durant la phase d'analyse et fournir à laphase de planification les informations nécessaires afin d'exécuter les actionsd'adaptation appropriées.	Imene Lahyani, Lamia Ben Amor, Ismael Bouassida Rodriguez, Mohamed Jmaiel	http://editions-rnti.fr/render_pdf.php?p1&p=1002041	
Revue des Nouvelles Technologies de l'Information	CAL	2014	Vers une architecture d'auto-réparation sur le Cloud Computing	Avec le Cloud Computing les organisations, institutions et entreprisesn'ont plus besoin d'investir lourdement dans des ressources informatiques, nécessairementlimitées, et exigeant une gestion lourde et coûteuse. Aujourd'hui,elles ont le choix de migrer vers un modèle Cloud Computing où elles peuventlouer des ressources en ligne. Ce modèle leur épargne les coûts de gestion interne,puisque les ressources informatiques sont administrées au niveau du fournisseurdu Cloud. Le monitoring devient primordial vu qu'il jouera un rôle importantdans les opérations des entreprises fournissant des services tels que lesinfrastructures (IaaS), plateformes (PaaS) et logiciels (SaaS). Le choix d'une solutionstable et performante nécessite une étude approfondie des différents outilsde monitoring existants pour guider le fournisseur Cloud afin de choisir l'outil leplus adéquat. Dans ce travail, nous procédons à une enquête sur des différentessolutions de monitoring existantes, à savoir l'open sources et les payantes afinde guider l'administrateur à choisir la solution la plus adéquate à son besoin.Ces outils garantissent les phases de monitoring et d'analyse pour détecter ladégradation de performance d'une infrastructure de Cloud. Mais malheureusement,ces deux phases ne suffisent pas pour que le système réagit et résout lesproblèmes liés à cette dégradation. Pour celà, nous envisageons d'appliquer unearchitecture d'auto-réparation sur l'OpenStack comme étant notre Cloud privé	Faten Khemakhem, Riadh Ben Halima, Ahmed Hadj Kacem	http://editions-rnti.fr/render_pdf.php?p1&p=1002044	
Revue des Nouvelles Technologies de l'Information	EDA	2014	BI4people : le décisionnel pour tous	Cette démonstration a pour objet un système décisionnel en mode Software as a Service destiné aux très petites entreprises, associations et individus. L'objectif est de permettre une prise en main simplifiée du processus décisionnel, en masquant les phases d'intégration de données et de conception d'un entrepôt et en permettant une navigation simple dans les données étudiées.	Oksana Grabova, Somayeh Sobati Moghadam, Samaneh Chagheri, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1002022	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Entrepôts de données multidimensionnelles réduites : principes et expérimentations	Notre objectif est de proposer une solution pour la réduction de données d'un Entrepôt de Données Multidimensionnelles (EDM) afin d'obtenir des schémas agrégés sur différentes périodes et de ne retenir que les informations pertinentes pour les prises de décision. Dans un premier temps, nous proposons une solution pour la modélisation des EDM réduits basée sur des états contenant des schémas en étoile et des opérateurs de réduction pour définir les schémas réduits. Dans un second temps, nous décrivons nos expérimentations et les résultats obtenus dans différents contextes : BD R-OLAP sans réduction et BD R-OLAP réduite. Nous montrons que, quelques soit le type d'analyse, les exécutions dans un contexte réduit sont plus performantes.	Faten Atigui, Franck Ravat, Jiefu Song, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1002018	
Revue des Nouvelles Technologies de l'Information	EDA	2014	L'opérateur CUBE pour les entrepôts de données NoSQL orientés colonnes	L'émergence de grands volumes de données, imposée par les grands acteurs du web, nécessite de nouveaux modèles de gestion de données et des nouvelles architectures de stockage et de traitement capables de trouver rapidement une information dans une volumétrie considérable de données. Les bases de données NoSQL (Not Only SQL) orientées colonnes offrent pour les big data, un modèle approprié aux entrepôts de données et à une structuration multidimensionnelles sous forme de cube OLAP(On-Line Analytical Processing). Cependant, en l'absence d'opérateur de calcul de cube OLAP, nous proposons dans cet article, un nouvel opérateur d'agrégation, baptisé CN-CUBE (Columnar NoSQL CUBE), qui permet de calculer des cubes de données à partir d'entrepôts de données stockés dans un système de gestion de base de données NoSQL orientées colonnes. Nous avons implémenté l'opérateur CN-CUBE sous l'interface SQL (Phoenix 1) du SGBD orienté colonnes Hbase 2, et réalisé des expérimentations sur un entrepôt de données publiques dans un environnement distribué réalisé avec Hadoop 3. Nous avons pu montrer ainsi que notre opérateur CN-CUBE présente des temps de calcul de cubes OLAP intéressants pour les entrepôts de big data.	Khaled Dehdouh, Fadila Bentayeb, Nadia Kabachi, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1002012	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Mediation-basedWeb Services fed Data Warehouse		John Samuel, Christophe Rey, Franck Martin, Lionel Peyron	http://editions-rnti.fr/render_pdf.php?p1&p=1002023	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Modélisation des transformations pour l'évolution de modèles multidimensionnels	La modélisation et l'entreposage des données ont constitué, depuis plus d'une décennie, une problématique de recherche pour laquelle différentes approches ont été proposées. Ces approches se focalisent sur des aspects statiques de l'entrepôt de données. Or, l'évolution du système d'information qui alimente un entrepôt peut avoir un impact sur ce dernier et peut conduire, par conséquent, à l'évolution de son modèle multidimensionnel. Dans ce contexte évolutif, nous proposons une démarche dirigée par les modèles pour automatiser la propagation de l'évolution du modèle de la source de données relationnelle vers l'entrepôt. Cette démarche est fondée sur deux modèles d'évolution ainsi qu'un ensemble de règles de transformation formalisées en Query/View/Transformation. Nous développons un prototype logiciel nommé DWE (« Data Warehouse Evolution ») qui supporte cette démarche.	Saïd Taktak, Jamel Feki, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1002016	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Modélisation d'un entrepôt de documents XML	Le format XML est aujourd'hui omniprésent dans les organisations et sur le Web. Il facilite le transport et l'échange de données complexes et hétérogènes représentant une information précieuse très peu, voire pas du tout exploitée. Les technologies OLAP et les systèmes d'entrepôts de données actuels permettent l'analyse et le stockage des données transactionnelles issues des bases données relationnelles. Cependant, ces outils ne sont pas adaptés à l'analyse des documents XML du fait de leur structure hiérarchique, ou de leur contenu étant souvent textuel. Nous proposons dans cet article une approche permettant de construire un entrepôt de document XML « centré document » dont le schéma conceptuel est modélisé en utilisant le formalisme UML ; nous présentons aussi une architecture pour l'intégration physique de ces documents dans un environnement XML natif.	Fatma Abdelhédi, Landry Ntsama, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1002011	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Personnalisation de l'exploitation d'un entrepôt de données dirigée par des ontologies : Application au management hospitalier	L'Entrepôt de Données (ED) est au coeur du Système d'Information Décisionnel (SID). Dans de grandes organisations, les ressources d'exploitation de tels ED sont partagées par des utilisateurs de profils voire de cultures très différents. Pour aider l'utilisateur à bien appréhender ces ressources et choisir les plus pertinentes pour son besoin, diverses connaissances relatives à ces ressources, à l'ED lui-même et au domaine d'application sont nécessaires. Dans cet article nous proposons un système de personnalisation des ressources d'exploitation d'un ED dirigée par des ontologies. Ce système est composé d'une BC et d'un moteur de personnalisation qui l'exploite. La base de connaissances (BC) est composée de trois ontologies spécifiques et connexes : ontologie de domaine "OD", l'ontologie de l'ED "OED" et l'ontologie des ressources d'exploitation existantes "OR". On s'intéressera dans cet article plus particulièrement à la BC.	Lama El Sarraj, Bernard Espinasse, Thérèse Libourel	http://editions-rnti.fr/render_pdf.php?p1&p=1002017	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Prédiction des valeurs manquantes dans un entrepôt de données par combinaison de la programmation par contraintes et des KPPV	La présence de données manquantes dans les grandes bases de données scientifiques et statistiques est très courante. Dans cet article, nous proposons un modèle permettant la reconstruction des données manquantes dans le contexte des entrepôts de données. Notre approche de reconstruction de données manquantes consiste à combiner la programmation par contraintes et une technique d'apprentissage automatique, à savoir l'algorithme des k-plus proches voisins. La programmation par contraintes permet d'inférer les données manquantes à partir des contraintes sommaires définies sur les données de base. La méthode des k-plus proches voisins permet d'augmenter la précision de cette inférence. Outre l'application aux entrepôts de données classiques, nous avons étendu notre approche à des entrepôts de données dits qualitatifs correspondant à notre cas d'étude, à savoir l'évaluation des gênes des chantiers urbains.	Fatiha Amanzougarene, Karine Zeitouni, Mohamed Chachoua	http://editions-rnti.fr/render_pdf.php?p1&p=1002021	
Revue des Nouvelles Technologies de l'Information	EDA	2014	ProtOLAP : un système de prototypage rapide pour les entrepôts de données	Les approches disponibles pour concevoir un entrepôt de données, y compris celles adoptant les pratiques agiles, sont basées sur l'hypothèse que les données sources sont connues et disponibles à l'avance. Cette hypothèse n'est pas toujours vraie dans certains contextes. Pour pallier ces limites, nous proposons ProtOLAP, une méthodologie assistée par un outil de prototypage rapide.	Hassan Nazih, Myoung-Ah Kang, Sandro Bimonte	http://editions-rnti.fr/render_pdf.php?p1&p=1002025	
Revue des Nouvelles Technologies de l'Information	EDA	2014	RecoOLAP : Un système de recommandation de requêtes OLAP	Une session d'analyse OLAP peut être définie comme une session interactive durant laquelle un utilisateur lance des requêtes pour naviguer dans un cube. Très souvent, choisir quelle partie du cube va être naviguée par la suite, et, de ce fait, concevoir la prochaine requête, est une tâche difficile. Dans cette démonstration, nous proposons un système de recommandation qui utilise ce que tous les utilisateurs du système OLAP ont fait pendant leurs précédentes explorations du cube afin de recommander des requêtes MDX à l'utilisateur.	Elsa Negre	http://editions-rnti.fr/render_pdf.php?p1&p=1002026	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Règles d'Association Triadiques pour la recommandation et l'enrichissement de requêtes décisionnelles	Cet article décrit un nouveau processus de personnalisation de requêtes décisionnelles à travers une nouvelle approche d'extraction de règles d'association triadiques. Ce processus exploite les fichiers log des utilisateurs et comporte cinq étapes : (1) génération d'un contexte triadique à partir des fichiers log de requêtes d'un serveur d'analyse OLAP ; (2) passage d'un contexte triadique vers un contexte dyadique ; (3) production de règles d'association dyadiques conventionnelles ; (4) génération d'un ensemble de règles d'association triadiques par factorisation des règles dyadiques. L'avantage de ces règles est qu'elles sont moins nombreuses et plus compactes que les règles dyadiques et qu'elles véhiculent une sémantique plus riche. Et enfin, (5) exploitation de ces règles triadiques pour la personnalisation des analyses OLAP. Nous avons développé un prototype logiciel P-TRIAR ( OLAP Personalization based on TRIadic Association Rules) permettant d'extraire deux types de règles à partir de fichiers log de requêtes. Le premier type de règles servira à la recommandation de requêtes par la prise en compte de l'aspect collaboratif des utilisateurs lors de leurs interrogations de l'entrepôt de données alors que le deuxième type de règles permettra l'enrichissement des requêtes utilisateurs.	Sid Ali Selmane, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1002010	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Requêtes Skyline Hiérarchiques	Les requêtes skyline constituent un puissant outil d'analyse de données multidimensionnelles et d'aide à la décision. Lorsque les dimensions sont conflictuelles, les requêtes skyline retournent les meilleurs compromis associés à ces dimensions. De nombreux travaux se sont intéressés à l'extraction de points skyline dans le contexte de bases de données multidimensionnelles, mais, à notre connaissance, aucun de ces derniers n'a traité la problématique des skyline associés à des données agrégées lorsque les dimensions sont multiples et hiérarchiques. Cet article présente une méthode qui étend la recherche des meilleurs compromis aux dimensions hiérarchiques. Notre proposition HSky (Hierarchical Skyline Queries) se focalise sur la définition de la sémantique et du calcul efficace de points skyline sur des dimensions présentant plusieurs niveaux hiérarchiques.	Tassadit Bouadi, Marie-Odile Cordier, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1002014	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Roaring bitmap : nouveau modèle de compression bitmap	Les index bitmap sont très utilisés dans les entrepôts de données et moteurs de recherche. Leur capacité à exécuter efficacement des opérations binaires entre bitmaps améliore significativement les temps de réponse des requêtes. Cependant, sur des attributs de hautes cardinalités, ils consomment un espace mémoire important. Ainsi, plusieurs techniques de compression bitmap ont été introduites pour réduire l'espace mémoire occupé par ces index, et accélérer leurs temps de traitement. Ce papier introduit un nouveau modèle de compression bitmap, appelé Roaring bitmap. Une comparaison expérimentale, sur des données réelles et synthétiques, avec deux autres solutions de compression bitmap connues dans la littérature : WAH (Word Aligned Hybrid compression scheme) et Concise (Compressed 'n' Composable integer Set), a montré que Roaring bitmap n'utilise que  25% d'espace mémoire comparé à WAH et  50% par rapport à Concise, tout en accélérant significativement les temps de calcul des opérations logiques entre bitmaps (jusqu'à 1100 fois pour les intersections).	Samy Chambi, Daniel Lemire, Robert Godin, Owen Kaser	http://editions-rnti.fr/render_pdf.php?p1&p=1002013	
Revue des Nouvelles Technologies de l'Information	EDA	2014	SimOLAP: A System for the semi-automatic implementation of Simulation Data Warehouses		Sandro Bimonte, Nicolas Dumoulin	http://editions-rnti.fr/render_pdf.php?p1&p=1002024	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Une approche spatio-multidimensionnelle pour l'analyse des aléas environnementaux	Les entrepôts de données spatiales et les systèmes SOLAP permettent l'analyse en ligne de gros volumes de données géo référencées. Dans un contexte de gestion des risques naturels, ces systèmes ont été utilisés avec succès sur des données officielles dont la collecte suit un processus bien établi. Le développement récent d'outils collaboratifs pour la saisie de données géographiques, y compris par des utilisateurs non-experts a engendré un nouveau concept : celui de l'Information Géographique Volontaires (VGI), et a créé un besoin et une opportunité pour prendre en compte ces données dans un processus décisionnel. Dans cet article, nous décrivons un modèle et un système spatiomultidimensionnel permettant l'utilisation conjointe des données VGI avec des systèmes SOLAP pour l'analyse des aléas environnementaux.	Sandro Bimonte, Omar Boucelma, Olivier Machabert, Sana Sellami	http://editions-rnti.fr/render_pdf.php?p1&p=1002015	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Une étude sur l'efficacité des méthodes de conception et d'implémentation pour les Entrepôts de Données par une méthodologie « requirement-based »: Cas d'étude de la consommation d'énergie en agriculture	Data Warehouses and OLAP systems allow decision-makers exploring and analyzing huge volumes of data modeled according the multidimensional model, and extracted from heterogeneous data sources. Usually, DW design is a complex, and time and resources consuming task. Then, DW experts are necessary during design and implementation phases. In this paper, we present a new methodology and a tool allowing modelers (DW unskilled users) to design and implement DWs for analyzing simulation results data by themselves, without any intervention of DW experts.	Sandro Bimonte, Jean-Pierre Chanet, Jacques Capdeville, Aurélie Tailleur, Marc Luciano	http://editions-rnti.fr/render_pdf.php?p1&p=1002019	
Revue des Nouvelles Technologies de l'Information	EDA	2014	Une nouvelle approche d'estimation pour les entrepôts de données multi-granulaires incomplètes	Les entrepôts de données spatiales (EDS) sont caractérisés par une forte corrélation des données. De ce fait, les méthodes d'interpolation spatiales et temporelles sont très utilisées pour estimer les faits manquants. Ces méthodes ignorent souvent la présence éventuelle des mesures agrégées. Ce qui entraîne un biais sur l'agrégation. Nous proposons une approche qui adapte les fonctions d'estimation existantes pour la prise en compte des des mesures agrégées connues.	Nestor Koueya, Sandro Bimonte, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1002020	
Revue des Nouvelles Technologies de l'Information	EGC	2014	1d-SAX : une nouvelle représentation symbolique pour les séries temporelles	SAX (Symbolic Aggregate approXimation) est une des techniquesmajeures de symbolisation des séries temporelles. La non prise en compte destendances dans la symbolisation est une limitation bien connue de SAX. Cet articleprésente 1d-SAX, une méthode pour représenter une série temporelle parune séquence de symboles contenant des informations sur la moyenne et la tendancedes fenêtres successives de la série segmentée. Nous comparons l'efficacitéde 1d-SAX vs SAX dans une tâche de classification de séries temporellesd'images satellites. Les résultats montrent que 1d-SAX améliore les taux de classificationpour une quantité d'information identique utilisée.	Simon Malinowski, Thomas Guyet, Rene Quiniou, Romain Tavenard	http://editions-rnti.fr/render_pdf.php?p1&p=1001930	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Agrégation de sac-de-sacs-de-mots pour la recherche d'information par modèles vectoriels	Cet article étudie l'intérêt de représenter les documents textuels nonplus comme des sacs-de-mots, mais comme des sacs-de-sacs-de-mots. Au coeurde l'utilisation de cette représentation, le calcul de similarité entre deux objetsnécessite alors d'agréger toutes les similarités entre sacs de chacun des objets.Nous évaluons cette représentation dans un cadre de recherche d'information,et étudions les propriétés attendues de ces fonctions d'agrégation. Les expériencesrapportées montrent l'intérêt de cette représentation lorsque les opérateursd'agrégation respectent certaines propriétés, avec des gains très importantspar rapport aux représentations standard.	Vincent Claveau	http://editions-rnti.fr/render_pdf.php?p1&p=1001925	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Alignement d'ontologies : exploitation des ontologies liées sur le web de données	Nous proposons dans cet article une méthode d'alignement d'une ontologiesource avec des ontologies cibles déjà publiées et liées sur le web dedonnées. Nous présentons ensuite un retour d'expérience sur l'alignement d'uneontologie dans le domaine des sciences du vivant et de l'environnement avecAGROVOC et NALT.	Thomas Hecht, Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu, Cássia Trojahn dos Santos	http://editions-rnti.fr/render_pdf.php?p1&p=1001911	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Annotation sémantique de documents administratifs	La numérisation de documents administratifs est un enjeu économiqueet écologique prioritaire dans le contexte sociétal actuel. La dématérialisationmassive de document n'est pas sans conséquence et soulève les problèmes d'organisation,de stockage et d'accès à l'information. Le défi n'est donc plus la numérisationdu document, mais l'extraction des informations qu'ils contiennent.Les documents sont produits par l'Homme et pour l'Homme. Cette propriétépermet de localiser des informations dans les zones saillantes du document (logos).La saillance et la reconnaissance sont deux éléments essentiels pour laclassification rapide de documents. A l'opposé, la recherche d'un document oud'un ensemble de documents repose presque toujours sur le texte brut, il estdonc nécessaire de faire une correspondance entre une requête textuelle et ledocument. Cet article présente une nouvelle approche d'annotation automatiquede documents administratifs qui utilise une approche visuel et une approche defouille de texte.	Benjamin Duthil, Mickaël Coustaty, Vincent Courboulay, Jean-Marc Ogier	http://editions-rnti.fr/render_pdf.php?p1&p=1001913	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Application du paradigme MapReduce aux données ouvertes Cas : Accessibilité des personnes à mobilité réduite aux musées	Le modèle MapReduce est aujourd'hui l'un des modèles de programmationparallèle les plus utilisés. Définissant une architecture Maître-Esclave,il permet le traitement parallèle de grandes masses de données. Dans ce papier,nous proposons un algorithme basé sur MapReduce qui permet, à partir des donnéespubliques du Ministère Français de la Communication et de la Culture, dedéfinir un classement des galeries et musées nationaux selon leurs degré d'accessibilitéaux personnes handicapées. Tout en profitant de la puissance et de laflexibilité du paradigme MapReduce, les décideurs pourront mettre en place desstratégies efficaces à moindre coût et avoir ainsi une vision plus précise sur lesétablissements culturels et leurs limites relatives à cette catégorie de personnes.L'algorithme que nous proposons peut être exploité et appliqué à d'autres casd'études avec des jeux de données plus volumineux.	Billel Arres, Nadia Kabachi, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001963	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage de fonctions de tri pour la prédiction d'interactions protéine-ARN	Les fonctions biologiques dans la cellule mettent en jeu des interactions3D entre protéines et ARN. Les avancées des techniques exérimentalesrestent insuffisantes pour de nombreuse applications. Il faut alors pouvoir prédirein silico les interactions protéine-ARN. Dans ce contexte, nos travaux sontfocalisés sur la construction de fonctions de score permettant d'ordonner les solutionsgénérées par le programme d'amarrage protéine-ARN RosettaDock. Laméthodologie d'évaluation utilisée par RosettaDock impose de trouver une fonctionde score s'exprimant comme une combinaison linéaire de mesures physicochimiques.Avec une approche d'apprentissage supervisé par algorithme génétique,nous avons appris différentes fonctions de score en imposant descontraintes sur la nature des poids recherchés. Les résultats obtenus montrentl'importance de la signification des poids à apprendre et de l'espace de rechercheassocié.	Adrien Guilhot-Gaudeffroy, Jérôme Azé, Julie Bernauer, Christine Froidevaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001960	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage incrémental anytime d'un classifieur Bayésien naïf pondéré	Nous considérons le problème de classification supervisée pour desflux de données présentant éventuellement un très grand nombre de variablesexplicatives. Le classifieur Bayésien naïf se révèle alors simple à calculer etrelativement performant tant que l'hypothèse restrictive d'indépendance des variablesconditionnellement à la classe est respectée. La sélection de variables etle moyennage de modèles sont deux voies connues d'amélioration qui reviennentà déployer un prédicteur Bayésien naïf intégrant une pondération des variablesexplicatives. Dans cet article, nous nous intéressons à l'estimation directe d'untel modèle Bayésien naïf pondéré. Nous proposons une régularisation parcimonieusede la log-vraisemblance du modèle prenant en compte l'informativité dechaque variable. La log-vraisemblance régularisée obtenue étant non convexe,nous proposons un algorithme de gradient en ligne qui post-optimise la solutionobtenue afin de déjouer les minima locaux. Les expérimentations menéess'intéressent d'une part à la qualité de l'optimisation obtenue et d'autre part auxperformances du classifieur en fonction du paramétrage de la régularisation.	Carine Hue, Marc Boullé, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001939	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage non supervisé de dépendances syntaxiques à partir de texte étiqueté, plusieurs variantes de PCFG légères	L'apprentissage de dépendances est une tâche consistant à établir, àpartir des phrases d'un texte, un modèle de construction d'arbres traduisant unehiérarchie syntaxique entre les mots. Nous proposons un modèle intermédiaireentre l'analyse syntaxique complète de la phrase et les sacs de mots. Il est basésur une grammaire stochastique hors-contexte se traduisant par des relations dedépendance entre les catégories grammaticales d'une phrase. Les résultats expérimentauxobtenus sur des benchmarks attestés dépassent pour cinq langues surdix les scores de l'algorithme de référence DMV, et pour la première fois desscores sont obtenus pour le français. La très grande simplicité de la grammairepermet un apprentissage très rapide, et une analyse presque instantanée.	Marie Arcadias, Guillaume Cleuziou, Edmond Lassalle, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001924	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Approche formelle de fusion d'ontologies à l'aide des grammaires de graphes typés	L'article propose une approche formelle de fusion d'ontologies se reposantsur les grammaires de graphes typés. Elle se décompose en trois étapes :1) la recherche de similarités entre concepts ; 2) la fusion des ontologies parl'approche algébrique SPO (Simple Push Out) ; 3) l'adaptation d'une ontologieglobale par le biais de règles de réécriture de graphes. Contrairement aux solutionsexistantes, cette méthode offre une représentation formelle de la fusiond'ontologies ainsi qu'une implémentation fonctionnelle basée sur l'outil AGG.	Mariem Mahfoudh, Germain Forestier, Laurent Thiry, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1001980	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Approche par motifs pour l'analyse de données multi-résolution	Dans cet article nous nous intéressons aux approches pour l'analysede graphes pouvant évoluer dans le temps et tel qu'un sommet à un temps donnépeut correspondre à plusieurs sommets au temps suivant et où les sommets sontassociés à un ensemble d'attributs catégoriels. Dans ce type de données, nousproposons une nouvelle classe de motifs basée sur des contraintes permettant dedécrire l'évolution de structures homogènes. Ce type d'approche est particulièrementadaptée pour l'analyse d'images multi-résolution sans perte d'information.Nous présentons un résultat qualitatif dans ce domaine.	Pierre-Nicolas Mougel, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001952	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Automatic correction of SVM for drifted data classification	Concept drift is an important feature of real-world data streams thatcan make usual machine learning techniques rapidly become unsuitable. Thispaper addresses the problem of sudden concept drift in classification problemsfor which standard techniques may fail. To this end, support vector machines(SVMs) are automatically corrected to cope with a new suddenly drifted dataset.Results on real-world datasets with several types of sudden drift indicate that themethod is able to correct the SVM in order to better classify the new data afterthe concept drift, using a correction based on the difference between the initialdataset and the new drifted dataset, even when the new dataset is small.	Alexandra Degeest, Benoît Frénay, Michel Verleysen	http://editions-rnti.fr/render_pdf.php?p1&p=1001942	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Broad Data: What happens when the Web of Data becomes real?	"Big Data" is used to refer to the very large datasets generated by scientists, to the manypetabytes of data held by companies like Facebook and Google, and to analyzing real-time dataassets like the stream of twitter messages emerging from events around the world. Key areasof interest include technologies to manage much larger datasets (cf. NoSQL), technologies for the visualization and analysis of databases, cloud-based data management and dataminingalgorithms.Recently, however, we have begun to see the emergence of another, and equally compellingdata challenge - that of the "Broad data" that emerges from millions and millions of rawdatasets available on the World Wide Web. For broad data the new challenges that emerge includeWeb-scale data search and discovery, rapid and potentially ad hoc integration of datasets,visualization and analysis of only-partially modeled datasets, and issues relating to the policiesfor data use, reuse and combination. In this talk, we present the broad data challenge anddiscuss potential starting points for solutions. We illustrate these approaches using data froma "meta-catalog" of over 1,000,000 open datasets that have been collected from about twohundred governments from around the world.	Jim Hendler	http://editions-rnti.fr/render_pdf.php?p1&p=1001905	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Classification des actions humaines basée sur les descripteurs spatio-temporels	Dans cet article, nous proposons un nouveau descripteurspatio-temporel appelé ST-SURF pour l'analyse et la reconnaissance d'actionsdans des flux vidéo. L'idée principale est d'enrichir le descripteur Speed UpRobust Feature (SURF) en intégrant l'information de mouvement issue du flotoptique. Seuls les points d'intérêts qui ont subi un déplacement sont pris encompte pour générer un dictionnaire de mots visuels (DMV) robuste basé surl'algorithme des k-moyennes (K-means). Le dictionnaire est utilisé lors du processusd'apprentissage et de reconnaissance d'actions basé sur la méthode desmachines à vecteurs supports (SVM). Les résultats obtenus confirment l'intérêtdu descripteur proposé ST-SURF pour l'analyse de scènes et en particulierpour la reconnaissance d'actions. La méthode atteind une précision de reconnaissancede l'ordre de 80.7%, équivalente aux performances des descripteursspatio-temporels de l'état de l'art.	Sameh Megrhi, Azeddine Beghdadi, Wided Souidène	http://editions-rnti.fr/render_pdf.php?p1&p=1001945	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Classification et prédiction du flux solaire	La prédiction du rayonnement solaire horaire dans une journée estun enjeu primordial pour la production d'énergie de type photovoltaïque. Nousprésentons deux stratégies de classification des jours selon leurs rayonnementssolaires puis une méthode de prédiction du flux solaire cohérente avec la classification.	Henri Ralambondrainy, Yves Lechevallier, Jean Daniel Lan-Sun-Luk, Jean-Pierre Chabriat	http://editions-rnti.fr/render_pdf.php?p1&p=1001962	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Classifieur naïf de Bayes pondéré pour flux de données	Un classifieur naïf de Bayes est un classifieur probabiliste basé surl'application du théorème de Bayes avec l'hypothèse naïve, c'est-à-dire que lesvariables explicatives (Xi) sont supposées indépendantes conditionnellement àla variable cible (C). Malgré cette hypothèse forte, ce classifieur s'est avéré trèsefficace sur de nombreuses applications réelles et est souvent utilisé sur les fluxde données pour la classification supervisée. Le classifieur naïf de Bayes nécessitesimplement en entrée l'estimation des probabilités conditionnelles parvariable P(Xi|C) et les probabilités a priori P(C). Pour une utilisation sur lesflux de données, cette estimation peut être fournie à l'aide d'un « résumé superviséen-ligne de quantiles ». L'état de l'art montre que le classifieur naïf de Bayespeut être amélioré en utilisant une méthode de sélection ou de pondération desvariables explicatives. La plupart de ces méthodes ne peuvent fonctionner quehors-ligne car elles nécessitent de stocker toutes les données en mémoire et/oude lire plus d'une fois chaque exemple. Par conséquent, elles ne peuvent être utiliséessur les flux de données. Cet article présente une nouvelle méthode baséesur un modèle graphique qui calcule les poids des variables d'entrée en utilisantune estimation stochastique. La méthode est incrémentale et produit un classifieurNaïf de Bayes Pondéré pour flux de données. Cette méthode est comparéeau classique classifieur naïf de Bayes sur les données utilisées lors du challenge« Large Scale Learning ».	Christophe Salperwyck, Vincent Lemaire, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1001938	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Clustering de données relationnelles pour la structuration de flux télévisuels	Les approches existantes pour structurer automatiquement un flux detélévision (i.e. reconstituer un guide de programme exact et complet), sont supervisées.Elles requièrent de grandes quantités de données annotées manuellement,et aussi de définir a priori les types d'émissions (publicités, bandes annonces,programmes, sponsors...). Pour éviter ces deux contraintes, nous proposonsune classification non supervisée. La nature multi-relationnelle de nosdonnées proscrit l'utilisation des techniques de clustering habituelles reposantsur des représentations sous forme attributs-valeurs. Nous proposons et validonsexpérimentalement une technique de clustering capable de manipuler ces donnéesen détournant la programmation logique inductive (PLI) pour fonctionnerdans ce cadre non supervisé.	Vincent Claveau, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1001934	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Clustering de séquences d'évènements temporels	Nous proposons une nouvelle méthode de clustering et d'analyse deséquences temporelles basée sur les modèles en grille à trois dimensions. Lesséquences sont partitionnées en clusters, la dimension temporelle est discrétiséeen intervalles et la dimension évènement est partitionnée en groupes. La grille decellules 3D forme ainsi un estimateur non-paramétrique constant par morceauxde densité jointe des séquences et des dimensions des évènements temporels.Les séquences d'un cluster sont ainsi groupés car elles suivent une distributionsimilaire d'évènements au cours du temps. Nous proposons aussi une méthoded'exploitation du clustering par simplification de la grille ainsi que des indicateurspermettant d'interpréter les clusters et de caractériser les séquences quiles composent. Les expériences sur des données artificielles ainsi que sur desdonnées réelles issues de DBLP démontrent le bien-fondé de notre approche.	Romain Guigourès, Dominique Gay, Marc Boullé, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1001929	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Clusters dans les réseaux sociaux : intersections entre liens conceptuels fréquents et communautés	La recherche de liens conceptuels fréquents (FCL) est une nouvelleapproche de clustering de réseaux, qui exploite à la fois la structure et les attributsdes noeuds. Bien que les travaux récents se soient déjà intéressés à l'optimisationdes algorithmes de recherche des FCL, peu de travaux sont aujourd'huimenés sur la complémentarité qui existe entre les liens conceptuels et l'approcheclassique de clustering qui consiste en l'extraction de communautés. Ainsi dansce papier, nous nous intéressons à ces deux approches. Notre objectif est d'évaluerles relations potentiellement existantes entre les communautés et les FCLpour comprendre la façon dont les motifs obtenus par chacune des méthodespeuvent correspondre ou s'intersecter ainsi que la connaissance utile résultantde la prise en compte de ces deux types de connaissance. Nous proposons pourcela un ensemble de mesures originales, basées sur la notion d'homogénéité, visantà évaluer le niveau d'intersection des FCL et des communautés lorsqu'ilssont extraits d'un même jeu de données. Notre approche est appliquée à deuxréseaux et démontre l'importance de considérer simultanément plusieurs typesde connaissance et leur intersection.	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001920	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Comment Devenir Cybercondriaque ?	Nous avons tous déjà eu l'occasion d'effectuer des recherches d'ordremédical sur Internet. Si certains sites spécialisés se refusent à tout diagnosticen ligne, préférant le renvoi vers des professionnels de santé, d'autres en revancheconduisent souvent à des déclarations alarmistes faisant état de situationshumaines difficiles. Dans ce travail, nous étudions l'ampleur de ce phénomèneet montrons que quel que soit le syndrome recherché, les résultats obtenusconduisent toujours à l'énoncé des mots "cancer" ou "tumeur".	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001986	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Comparaison de bornes théoriques pour l'accélération du clustering incrémental en une passe	Le clustering incrémental en une passe repose sur l'affectation efficacede chaque nouveau point aux clusters existants. Dans le cas général, où lesclusters ne peuvent être représentés par une moyenne, la détermination exhaustivedu cluster le plus proche possède une complexité quadratique avec le nombrede données. Nous proposons dans ce papier une nouvelle méthode d'affectationstochastique à chaque cluster qui minimise le nombre de comparaisons à effectuerentre la donnée et chaque cluster pour garantir, étant donné un taux d'erreuracceptable, l'affectation au cluster le plus proche. Plusieurs bornes théoriques(Bernstein, Hoeffding et Student) sont comparées dans ce papier. Les résultatssur des données artificielles et réelles montrent que la borne de Bernstein donneglobalement les meilleurs résultats (notamment lorsqu'elle est réduite) car ellepermet une accélération forte du processus de clustering, tout en conservant unnombre très faible d'erreurs.	Nicolas Labroche, Marcin Detyniecki, Thomas Baerecke	http://editions-rnti.fr/render_pdf.php?p1&p=1001959	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Comparaison des chemins de Hilbert adaptatif et des graphes de voisinage pour la caractérisation d'un parcellaire agricole	Cet article compare deux représentations de données spatiales, lesgraphes de voisinages et les chemins de Hilbert-Peano, utilisées par des algorithmesde fouille. Cette comparaison s'appuie sur la mise en oeuvre d'une méthoded'énumération de « sacs de noeuds », qui permet d'obtenir des caractérisationshomogènes à partir des deux représentations. La méthode est appliquée àla caractérisation de parcellaires agricoles et les résultats tendent à montrer quela linéarisation de l'espace capte la majorité de l'information, à l'exception deséléments rares, sur cet exemple particulier.	Thomas Guyet, Sébastien Da Silva, Claire Lavigne, Florence Le Ber	http://editions-rnti.fr/render_pdf.php?p1&p=1001974	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Compréhension de recettes de cuisine utilisateurs par extraction de connaissances intrinsèques	Sur les sites Web communautaires, les utilisateurs échangent des connaissances,en étant à la fois auteurs et lecteurs. Nous présentons une méthodepour construire notre propre compréhension de la sémantique de la communauté,sans recours à une base de connaissances externe. Nous effectuons une extractionde la connaissance présente dans les contributions analysées. Nous proposonsune évaluation de la confiance imputable à cette compréhension déduite,afin d'évaluer la qualité du contenu, avec application à un site Web de partagede recettes de cuisine.	Damien Leprovost, Thierry Despeyroux, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001984	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Construction de cube OLAP à partir d'un entrepôt de données orienté colonnes	L'optimisation de la construction de cubes OLAP 1 a été jusqu'à présentaxée sur le développement d'algorithmes de calcul performants. Ces derniersopèrent sur des données extraites de l'entrepôt de données qui est généralementimplémenté selon le modèle relationnel qui adopte l'architecture orientéelignes. Or, pour les requêtes décisionnelles, l'architecture orientée colonnes offrede meilleures performances. Cependant, les SGBDR 2 selon cette architecture nedisposent pas d'opérateurs appropriés pour le calcul de cube OLAP. Nous proposonsdans cet article une nouvelle méthode de calcul de cube OLAP. Les résultatsobtenus à partir des expérimentations que nous avons menées démontrentque notre approche optimise considérablement le temps de construction de cubeOLAP et réduit le temps de réponse relatif à l'exploitation du cube comparé àl'approche orientée lignes.	Khaled Dehdou, Fadila Bentayeb, Nadia Kabachi, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001970	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Construction de profils de préférences contextuelles basée sur l'extraction de motifs séquentiels	L'utilisation de préférences suscite un intérêt croissant pour personnaliserdes réponses et effectuer des recommandations. En amont, l'étape essentielleest l'élicitation des préférences qui consiste à construire un profil depréférences en sollicitant le moins possible l'utilisateur. Dans cet article, nousprésentons une méthode basée sur l'extraction de motifs séquentiels afin de générerdes règles de préférences contextuelles à partir d'une base de paires detransactions. À partir de ces règles générées, qui ont une expressivité plus richeque celle des approches existantes, nous montrons comment construire et utiliserun profil modélisant les préférences de l'utilisateur. De plus, notre approchea l'avantage de bénéficier des nombreux algorithmes efficaces d'extraction deséquences fréquentes. L'évaluation de notre méthode sur des données réellesmontre que les modèles de préférences construits permettent d'effectuer des recommandationsjustes à un utilisateur.	Arnaud Giacometti, Dominique Haoyuan Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001954	
Revue des Nouvelles Technologies de l'Information	EGC	2014	De l'ombre à la lumière : plus de visibilité sur l'Eclipse	L'extraction de connaissances à partir de données issues du génie logicielest un domaine qui s'est beaucoup développé ces dix dernières années, avecnotamment la fouille de référentiels logiciels (Mining Software Repositories) etl'application de méthodes statistiques (partitionnement, détection d'outliers) àdes thématiques du processus de développement logiciel. Cet article présente ladémarche de fouille de données mise en oeuvre dans le cadre de Polarsys, ungroupe de travail de la fondation Eclipse, de la définition des exigences à la propositiond'un modèle de qualité dédié et à son implémentation sur un prototype.Les principaux concepts adoptés et les leçons tirées sont également passés enrevue.	Boris Baldassari, Flavien Huynh, Philippe Preux	http://editions-rnti.fr/render_pdf.php?p1&p=1001967	
Revue des Nouvelles Technologies de l'Information	EGC	2014	De nouvelles pondérations adaptées à la classification de petits volumes de données textuelles	Un des défis actuels dans le domaine de la classification supervisée dedocuments est de pouvoir produire un modèle fiable à partir d'un faible volumede données. Avec un volume conséquent de données, les classifieurs fournissentdes résultats satisfaisants mais les performances sont dégradées lorsque celui-cidiminue. Nous proposons, dans cet article, de nouvelles méthodes de pondérationsrésistant à une diminution du volume de données. Leur efficacité, évaluéeen utilisant des algorithmes de classification supervisés existants (Naive Bayeset Class-Feature-Centroid) sur deux corpus différents, est supérieure à celle desautres algorithmes lorsque le nombre de descripteurs diminue. Nous avons étudiéen parallèle les paramètres influençant les différentes approches telles que lenombre de classes, de documents ou de descripteurs.	Flavien Bouillot, Pascal Poncelet, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001922	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Des humanités au numérique : interdisciplinarité et réciprocité	Les Humanités Numériques, aussi contestable et critiquable que soit le terme, font maintenantpartie du paysage de la recherche en sciences humaines, institutionnalisées par la TrèsGrande Infrastructure de Recherche Huma-Num du CNRS. Elles sont généralement définiescomme la convergence de disciplines autour d'un matériau numérique, matériau inévitablementaccompagné d'un outillage tout aussi numérique. Ce matériau, suivant la discipline quil'observe pourra être considéré comme un objet éditorial, un objet analysable ou un objetcalculable. Nous tenterons de montrer que ce matériau peut aussi être perçu, voire construit,comme un dépôt voire un entrepôt de connaissances. Notre présentation s'appuiera sur diversprojets de recherche en humanités numériques auxquels nous contribuons afin de mettre enexergue le lien qui peut être fait entre extraction et gestion de connaissances d'une part ethumanités numériques d'autre part : le premier peut trouver un terrain expérimental dans lesecond tandis que le second peut tirer profit des méthodes et outils développés par le premier.Nous égrainerons par ailleurs d'autres problématiques inhérentes aux Humanités numériques :de la constitution à l'analyse du corpus en passant par la formalisation et la normalisationdes données. Enfin, nous tenterons de montrer par l'exemple que les questions posées par leshumanités numériques ne sont pas sans rappeler celles des industries de la connaissance.	Thomas Lebarbé	http://editions-rnti.fr/render_pdf.php?p1&p=1001907	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection de changements dans des flots de données qualitatives	Pour mieux analyser et extraire de la connaissance de flots de données,des approches spécifiques ont été proposées ces dernières années. L'un deschallenges auquel elles doivent faire face est la détection de changement dansles données. Alors que de plus en plus de données qualitatives sont générées,peu de travaux de recherche se sont intéressés à la détection de changement dansce contexte et les travaux existants se sont principalement focalisés sur la qualitéd'un modèle appris plutôt qu'au réel changement dans les données. Danscet article nous proposons une nouvelle méthode de détection de changementnon supervisée, appelée CDCStream (Change Detection in Categorical DataStreams), adaptée aux flux de données qualitatives.	Dino Ienco, Albert Bifet, Bernhard Pfahringer, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001968	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection de situations à risque basée sur des détecteurs de mouvement à domicile pour les personnes dépendantes	Avec le vieillissement de la population dans les décennies à venir, laprise en charge de la dépendance est devenu un enjeu majeur. Les nouvellestechnologies permettent d'améliorer le confort et la sécurité des personnes dépendantesà domicile. Dans cet article nous proposons une méthode de détectionde situations à risques basée sur le seuillage automatique des intervalles d'inactivitédes capteurs de mouvement de type infrarouge passif. Notre contributionconsiste à apprendre de façon automatique la durée maximale d'inactivité, parpièce et par plage horaire. La méthode est évaluée sur des données réelles provenantde l'activité d'une personne réelle dans un appartement équipé de capteursdomotiques. Notre approche permet de réduire le temps d'appel des secours.	Alban Meffre, Nicolas Lachiche, Pierre Gançarski, Christophe Collet	http://editions-rnti.fr/render_pdf.php?p1&p=1001983	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection d'opinions dans des tweets	Twitter est à l'heure actuelle un des réseau sociaux les plus utilisé aumonde et analyser les opinions qui y sont contenues permet de fournir de précieusesinformations notamment aux entreprises commerciales. Dans cet article,nous décrivons une méthode permettant de déterminer l'opinion d'un tweet endétectant dans un premier temps sa subjectivité, puis sa polarité.	Caroline Collet, Alexandre Pauchet, Laurent Vercouter, Khaled Khelif	http://editions-rnti.fr/render_pdf.php?p1&p=1001964	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Du texte à la base de données géographiques	Avec la prolifération des données géographiques, il y a un fort besoinde concevoir des outils automatiques pour l'exploitation des connaissances géographiquesincarnées dans les documents textuels. C'est dans ce contexte, quenous proposons une approche permettant de générer une base de données géographiques(BDG) à partir de textes. Notre approche s'articule autour de deuxgrandes phases : la génération du schéma de la BDG et la détermination desdonnées qui serviront au remplissage de cette base. L'implémentation de notreapproche a donné naissance à un outil que nous avons baptisé GDB Generatoret que nous avons intégré dans le SIG : OpenJUMP.	Nesrine Hassini, Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001972	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Dynamique des communautés par prédiction d'interactions dans les réseaux sociaux	Dans cet article, nous proposons une approche générale de prédictiondes communautés basée sur un modèle d'apprentissage automatique pour la prédictiondes interactions. En effet, nous pensons que, si on peut prédire avec précisionla structure du réseau, alors on a juste à rechercher les communautés surle réseau prédit. Des expérimentations sur des jeux de données réels montrent lafaisabilité de cette approche.	Blaise Ngonmang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001977	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Evaluation de la pertinence dans un système de recommandation sémantique de nouvelles économiques	De nos jours dans les secteurs commerciaux et financiers la veille estcruciale et complexe, car la charge d'informations est importante. Pour répondreà cette problématique, nous proposons un système novateur de recommandationd'articles basé sur une modélisation ontologique des connaissances. Nous présentonségalement une nouvelle méthode d'évaluation de la pertinence utilisantle modèle vectoriel intrinsèquement efficace et adapté afin de pallier la confusionnative de ces modèles entre les notions de similarité et de pertinence.	David Werner, Christophe Cruz, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001976	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Exploration d'une collection de chansons à partir d'une interface de visualisation basée sur une analyse des paroles	Dans cet article, nous présentons une approche de fouille de textesainsi qu'une interface de visualisation afin d'explorer une large collection dechansons frana¸ises à partir des paroles. Dans un premier temps, nous collectonsparoles et métadonnées de différentes sources sur leWeb. Nous utilisons une approchecombinant clustering et analyse sémantique latente afin d'identifier différentesthématiques et de déterminer différents descripteurs significatifs. Noustransformons par la suite le modèle afin d'obtenir une visualisation interactivepermettant d'explorer la collection de chansons	Rémy Kessler, Audrey Laplante, Dominic Forest	http://editions-rnti.fr/render_pdf.php?p1&p=1001946	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extension de l'étiquetage géographique des pixels d'une image par fouille de données	Les techniques de classification modernes permettent d'étiqueter leszones non couvertes des bases de données cartographiques, mais souffrent d'unmanque de robustesse important. Dans cet article, nous proposons une méthoderobuste d'extension d'étiquetage sur l'emprise d'une image satellite, par analysehiérarchique des données existantes. Notre approche est fondée sur une sélectiond'attributs par thème de la base de données, une sélection des pixels d'apprentissageet des classifications par objet de chaque thème. La décision finaled'étiquetage est prise après fusion des classifications par thème. Notre méthodeest appliquée avec succès et comparée à plusieurs méthodes de classification,couplant données d'occupation du sol et imagerie spatiale très haute résolution.	Adrien Gressin, Nicole Vincent, Clément Mallet, Nicolas Paparoditis	http://editions-rnti.fr/render_pdf.php?p1&p=1001966	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraction de motifs dans des graphes orientés attribués en présence d'automorphisme	Les graphes orientés attribués sont des graphes orientés dans lesquelsles noeuds sont associés à un ensemble d'attributs. De nombreuses données, issuesdu monde réel, peuvent être représentées par ce type de structure, maisencore peu d'algorithmes sont capables de les traiter directement. La fouille desgraphes attribués est difficile, car elle nécessite de combiner l'exploration de lastructure du graphe avec l'identification d'itemsets fréquents. De plus, du fait del'explosion combinatoire des itemsets, les isomorphismes de sous-graphes, dontla présence impacte énormément les performances des algorithmes de fouille,sont beaucoup plus nombreux que dans les graphes étiquetés.Dans cet article, nous présentons une nouvelle méthode de fouille de donnéesqui permet d'extraire des motifs fréquents à partir d'un ou de plusieurs graphesorientés attribués. Nous montrons comment réduire l'explosion combinatoireprovoquée par les isomorphismes de sous-graphes en traitant de manière particulièreles motifs automorphes.	Claude Pasquier, Frédéric Flouvat, Jérémy Sanhes, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001949	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraction de règles d'épisodes minimales dans des séquences complexes	Les messages déposés quotidiennement sur les réseaux sociaux et lesblogs sont très nombreux et constituent une source d'informations précieuse.Leur fouille peut être utilisée dans un but de prédiction d'informations. Notreobjectif dans cet article est de proposer un algorithme permettant la prédictiond'informations au plus tôt et de façon fiable, par le biais de l'identification derègles d'épisodes.	Lina Fahed, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001975	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraire les motifs minimaux efficacement et en profondeur	Les représentations condensées ont fait l'objet de nombreux travauxdepuis 15 ans. Tandis que les motifs maximaux des classes d'équivalence ontreçu beaucoup d'attention, les motifs minimaux sont restés dans l'ombre notammentà cause de la difficulté de leur extraction. Dans ce papier, nous présentonsun cadre générique concernant l'extraction de motifs minimaux en introduisantla notion de système minimisable d'ensembles. Il permet de considérer des langagesvariés comme les motifs ensemblistes ou les chaînes de caractères, maisaussi différentes métriques dont la fréquence. Ensuite, pour n'importe quel systèmeminimisable d'ensembles, nous introduisons un test de minimalité rapidepermettant d'extraire en profondeur les motifs minimaux. Nous démontrons quel'algorithme proposé est polynomial-delay et polynomial-space. Des expérimentationssur les benchmarks traditionnels complètent notre étude.	Arnaud Soulet, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001950	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Fouille de données par programmation visuelle structurée avec KD-Ariane	Nous présentons ici la plate-forme KD-Ariane, un déploiement d'outilspour la fouille de données dans l'environnement de programmation visuelleAriane. Ce déploiement facilite la conception de chaînes structurées de traitementspour l'extraction de connaissance dans les données	Régis Clouard, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001989	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Fouille de motifs séquentiels pour l'élicitation de stratégies à partir de traces d'interactions entre agents en compétition	Pour atteindre un but, tout agent en compétition élabore inévitablementdes stratégies. Lorsque l'on dispose d'une certaine quantité de traces d'interactionsentre agents, il est naturel d'utiliser la fouille de motifs séquentielspour découvrir de manière automatique ces stratégies. Dans cet article, nous proposonsune méthodologie qui permet l'élicitation de stratégies et leur capacité àdiscriminer une réussite ou un échec. La méthodologie s'articule en trois étapes :(i) les traces brutes sont transformées en une base de séquences selon des choixqui permettent, (ii) l'extraction de stratégies fréquentes, (iii) lesquelles sont muniesd'une mesure originale d'émergence. C'est donc une méthodologie de découvertede connaissances que nous proposons. Nous montrons l'intérêt des motifsextraits et la faisabilité de l'approche à travers des expérimentations quantitativeset qualitatives sur des données réelles issues du domaine émergent dusport électronique.	Guillaume Bosc, Mehdi Kaytoue-Uberall, Chedy Raïssi, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001948	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Généralisation des k-moyennes pour produire des recouvrements ajustables	La recherche de groupes non-disjoints à partir de données non-étiquetéesest une problématique importante en classification non-supervisée. Laclassification recouvrante (Overlapping clustering) contribue à la résolution deplusieurs problèmes réels qui nécessitent la détermination de groupes qui se chevauchent.Cependant, bien que les recouvrements entre groupes soient tolérésvoire encouragés dans ces applications, il convient de contrôler leur importance.Nous proposons dans ce papier des généralisations de k-moyennes offrant lecontrôle et le paramétrage des recouvrements. Deux principes de régulation sontmis en place, ils visent à contrôler les recouvrements relativement à leur tailleet à la dispersion des classes. Les expérimentations réalisées sur des jeux dedonnées réelles, montrent l'intérêt des principes proposés.	Chiheb-Eddine Ben N'Cir, Guillaume Cleuziou, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001932	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Génération d'un extrait textuel à partir de bases de données	Dans ce papier, nous présentons une approche dédiée à la transformationd'une base de données en un extrait textuel. L'idée sous-jacente à notreproposition est d'apporter plus de sémantique aux données de la base. Cet objectifest atteint moyennant l'utilisation des ontologies comme ressources sémantiques.Notre approche prend comme input un ensemble de bases de donnéeset associe à chacune une ontologie. Une ontologie globale est générée, à partirde laquelle des règles d'association sont proposées pour mieux expliciter sasémantique. Enfin, la génération d'un extrait textuel prend lieu.	Ghada Landoulsi, Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001971	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Granularité des motifs de co-variations dans des graphes attribués dynamiques	Découvrir des connaissances dans des graphes qui sont dynamiqueset dont les sommets sont attribués est de plus en plus étudié, par exemple dansle contexte de l'analyse d'interactions sociales. Il est souvent possible d'expliciterdes hiérarchies sur les attributs permettant de formaliser des connaissancesa priori sur les descriptions des sommets. Nous proposons d'étendre destechniques de fouille sous contraintes récemment proposées pour l'analyse degraphes attribués dynamiques lorsque l'on exploite de telles hiérarchies et doncle potentiel de généralisation/spécialisation qu'elles permettent. Nous décrivonsun algorithme qui calcule des motifs de co-évolution multi-niveaux, c'est-à-diredes ensembles de sommets qui satisfont une contrainte topologique et qui évoluentde la même façon selon un ensemble de tendances et de pas de temps. Nosexpérimentations montrent que l'utilisation d'une hiérarchie permet d'extrairedes collections de motifs plus concises sans perdre d'information.	Elise Desmier, Marc Plantevit, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001955	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Identification de classes non-disjointes ayants des densités différentes	La classification recouvrante correspond à un enjeu important en classificationnon-supervisée en permettant à une observation d'appartenir à plusieursclusters. Plusieurs méthodes ont été proposées pour faire face à cetteproblématique en utilisant plusieurs approches usuelles de classification. Cependant,malgré l'efficacité de ces méthodes à déterminer des groupes non-disjoints,elles échouent lorsque les données comportent des groupes de densités différentescar elles ignorent la densité locale de chaque groupe et ne considèrentque la distance Euclidienne entres les observations. Afin de détecter des groupesnon-disjoints de densités différentes, nous proposons deux méthodes de classificationintégrant la variation de densité des différentes classes dans le processusde classification. Des expériences réalisées sur des ensembles de données artificiellesmontrent que les méthodes proposées permettent d'obtenir de meilleuresperformances lorsque les données contiennent des groupes de densités différentes.	Hela Masmoudi, Chiheb-Eddine Ben N&#146;Cir, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001936	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Identification de rôles communautaires dans des réseaux orientés appliquée à Twitter	La notion de structure de communautés est particulièrement utile pourétudier les réseaux complexes, car elle amène un niveau d'analyse intermédiaire,par opposition aux plus classiques niveaux local (voisinage des noeuds) et global(réseau entier). Le concept de rôle communautaire permet de décrire le positionnementd'un noeud en fonction de sa connectivité communautaire. Cependant,les approches existantes sont restreintes aux réseaux non-orientés, utilisentdes mesures topologiques ne considérant pas tous les aspects de la connectivitécommunautaire, et des méthodes d'identification des rôles non-généralisables àtous les réseaux. Nous proposons de résoudre ces problèmes en généralisant lesmesures existantes, et en utilisant une méthode non-supervisée pour déterminerles rôles. Nous illustrons l'intérêt de notre méthode en l'appliquant au réseaude Twitter. Nous montrons que nos modifications mettent en évidence les rôlesspécifiques d'utilisateurs particuliers du réseau, nommés capitalistes sociaux.	Nicolas Dugué, Vincent Labatut, Anthony Perez	http://editions-rnti.fr/render_pdf.php?p1&p=1001921	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Incremental learning with latent factor models for attribute prediction in social-attribute networks	Dans ce travail, nous nous intéressons au problème de la prédiction d'attributs sur lesnoeuds dans un réseau social. La plupart des techniques sont hors ligne et ne sont pas adaptéesà des situations où les données arrivent massivement en flux comme dans le cas des médiassociaux. Dans ce travail, nous utilisons les modèles de variables latentes pour prédire les attributsinconnus des noeuds dans un réseau social et proposer une méthode pour mettre à jourincrémentalement le modèle avec des nouvelles données. Des expérimentations sur un jeu dedonnées issues des médias sociaux montrent que notre méthode est moins coûteuse en tempsde calcul et peut garantir des performances acceptables en comparaison avec les techniquesnon-incrémentales de l'état de l'art.	Duc Kinh Le Tran, Cécile Bothorel, Pascal Cheung Mon Chan	http://editions-rnti.fr/render_pdf.php?p1&p=1001916	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Intégration de plusieurs formes de représentations spatiales dans un modèle de simulation	In this paper, we focus on modeling expert knowledge for simulating complex landscapespatial dynamics. One modeling tool to do that is the Ocelet modeling language that usesinteraction graphs to describe spatial dynamics. Most present approaches impose an a priorichoice of spatial format between: (i) a vector format representing the shapes of the entities, or(ii) a gridding of space into regular elements (raster). In this paper we show how Ocelet wasextended to support the interaction semantics between these two spatial formats (vector andraster). As case study, we present a runoff model in a tropical insular environment.	Mathieu Castets, Pascal Degenne, Danny Lo Seen, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001988	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Intégration et visualisation de données liées thématiques sur un référentiel géographique	De nombreuses ressources publiées sur le Web des données sont décritespar une composante qui désigne d'une manière directe ou indirecte unelocalisation géographique. Comme toute autre propriété, cette information delocalisation peut être mise à profit pour permettre l'interconnexion des donnéesavec d'autres sources. Elle permet en outre leur représentation cartographique.Cependant, les informations de localisation utilisées dans les sources de donnéeslinked data peuvent parfois s'avérer imprécises ou hétérogènes d'une source àl'autre. Ceci rend donc leur exploitation pour réaliser une interconnexion difficile,voire impossible. Dans cet article, nous proposons de pallier ces difficultésen ancrant les données linked data thématiques aux objets d'un référentielgéographique. Nous mettons à profit le référentiel géographique afin de mettreen correspondance des données thématiques dotées d'indications de localisationhétérogènes. Nous exploitons enfin les relations de correspondance créées entredonnées thématiques et référentiel géographique dans une application de visualisationcartographique des données.	Abdelfettah Feliachi, Nathalie Abadie, Fayçal Hamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1001912	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Investigation visuelle d'événements dans un grand flot de liens	Nous présentons une nouvelle méthode d'analyse exploratoirede grands flots de liens que nous appliquons à la détection d'événementssignificatifs dans plus de 2 millions d'interactions (pendant 4 mois) entreutilisateurs du réseau social en ligne Github. Nous combinons une méthodestatistique de détection automatique d'événements dans une série temporelle,Outskewer, avec un système de visualisation de graphes. Outskewer identifiedes instants de l'évolution du graphe d'interactions méritant d'être étudiés, etun analyste peut valider et interpréter ces événements par la visualisation demotifs anormaux dans les sous-graphes correspondants. Nous montrons par demultiples exemples que cette approche 1) permet de détecter des événementspertinents et de rejeter ceux qui ne le sont pas, 2) est adaptée à une démarcheexploratoire car elle ne nécessite pas de connaissance a priori sur les données.	Sébastien Heymann, Bénédicte Le Grand	http://editions-rnti.fr/render_pdf.php?p1&p=1001918	
Revue des Nouvelles Technologies de l'Information	EGC	2014	La subjectivité dans le discours médical : sur les traces de l'incertitude et des émotions	Les acteurs et usagers du domaine médical (médecins, infirmiers, patients,internes, pharmaciens, etc.) ne sont pas issus de la même catégorie socioprofessionnelleet ne présentent pas le même niveau de maîtrise du domaine.Leurs écrits en témoignent et véhiculent, de plus, la subjectivité qui leur estpropre. Nous nous intéressons à l'étude automatisée de la subjectivité dans lediscours médical dans des textes en langue française. Nous confrontons le discoursdes médecins (articles scientifiques, rapports cliniques) à celui des patients(messages de forums de santé) en analysant contrastivement les différencesd'emploi des descripteurs tels que les marqueurs d'incertitude et de polarité,les marques émotives non lexicales (smileys, ponctuations répétées, etc.)et lexicales, et les termes médicaux relatifs aux pathologies, traitements et procédures.Nous effectuons une annotation et catégorisation automatiques des documentsafin de mieux observer les spécificités que présentent les discours médicauxciblés.	Pierre Chauveau Thoumelin, Natalia Grabar	http://editions-rnti.fr/render_pdf.php?p1&p=1001958	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Les nouvelles théories de l'incertain	La notion d'incertitude a été longtemps un sujet de controverses. En particulier la prééminencede la théorie des probabilités dans les sciences tend à gommer les différences présentesdans les premières tentatives de formalisation, remontant au 17ème siècle, entre l'incertitudedue à la variabilité des phénomènes répétables et l'incertitude due au manque d'information(dite épistémique). L'école Bayésienne affirme que quelle que soit l'origine de l'incertitude,celle-ci peut être modélisée par une distribution de probabilité unique. Cette affirmation a étébeaucoup remise en cause dans les trente dernières années. En effet l'emploi systématiqued'une distribution unique en cas d'information partielle mène à des utilisations paradoxales dela théorie des probabilités.Dans de nombreux domaines, il est crucial de distinguer entre l'incertitude due à la variabilitéd'observations et l'incertitude due à l'ignorance partielle. Cette dernière peut être réduitepar l'obtention de nouvelles informations, mais pas la première, dont on ne se prémunit quepar des actions concrètes. Dans le cas des bases de données, il est souvent supposé qu'ellessont précises, et l'incertitude correspondante est souvent négligée. Quant elle est abordée onreste souvent dans une approche probabiliste orthodoxe.Néanmoins, les statisticiens ont développé des outils qui ne relèvent pas de la théorie deKolmogorov pour pallier le manque de données (intervalles de confiance, principe de maximumde vraisemblance...).De nouvelles théories de l'incertain ont émergé, qui offrent la possibilité de représenter lesincertitudes épistémiques et aléatoires de façon distincte, notamment l'incertitude épistémique,en remplaçant la distribution de probabilité unique par une famille de distributions possibles,cette famille étant d'autant plus grande que l'information est absente. Cette représentationcomplexe possède des cas particuliers plus simples à utiliser en pratique, comme les ensemblesaléatoires (théorie des fonctions de croyance), les distributions de possibilité (représentant desensembles flous de valeurs possibles) et les p-boxes, notamment.Le but de cet exposé est de susciter l'intérêt pour ces nouvelles théories de l'incertain,d'en donner les bases formelles, d'en discuter la philosophie sous-jacente, de faire le lien aveccertaines notions en statistique, et de les illustrer sur des exemples.	Didier Dubois	http://editions-rnti.fr/render_pdf.php?p1&p=1001906	
Revue des Nouvelles Technologies de l'Information	EGC	2014	LOCAL-GENERATOR : "diviser pour régner" pour l'extraction des traverses minimales d'un hypergraphe	Du fait qu'elles apportent des solutions dans de nombreuses applications,les traverses minimales des hypergraphes ne cessent de susciter l'intérêt dela communauté scientifique et le développement d'algorithmes pour les calculer.Dans cet article, nous présentons une nouvelle approche pour l'optimisation del'extraction des traverses minimales basée sur les notions d'hypergraphe partielet de traverses minimales locales selon une stratégie diviser pour régner. Nousintroduisons aussi un nouvel algorithme, appelé LOCAL-GENERATOR pour lecalcul des traverses minimales. Les expérimentations effectuées sur divers jeuxde données ont montré l'intérêt de notre approche, notamment sur les hypergraphesayant un nombre de transversalité élevé et renfermant un nombre trèsimportant de traverses minimales.	M. Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001935	
Revue des Nouvelles Technologies de l'Information	EGC	2014	L'utilisation des entités nommées pour l'expansion sémantique des requêtes Web	Les entités nommées sont des éléments intéressants pour les applicationsfondées sur le Traitement du Langage Naturel. Dans le cas de la recherched'information, les entités nommées sont largement employées par les utilisateursdu web dans les requêtes de recherche, soit pour définir un concept debase, soit pour décrire un autre concept dans la requête. Du côté du modèlede recherche, les entités nommées sont des éléments riches en information quiaident à mieux cibler les documents pertinents. Dans cet article, nous étudionsl'avantage d'étendre les entités nommées dans la requête. L'idée est d'utiliserune technique d'expansion sémantique sur une ontologie générale (Yago) pourdésambiguïser les entités nommées et pour trouver leurs différentes appellationsque l'on intègre dans la requête en utilisant 3 approches : sac de mots, dépendanceséquentielle, et concept clé. Nous mesurons l'efficacité de ces expériencesen termes de précision et rappel, et nous étudions l'effet du rôle des entités nomméessur l'expansion. Nous concluons que l'expansion des entités nommées estune méthode simple qui améliore significativement la qualité de la recherchequand elle est comparée à un modèle de référence sans expansion. De plus, cetteméthode est assez compétitive par rapport à l'approche pseudo retour de pertinencesouvent utilisée pour l'expansion de la requête.	Bissan Audeh, Philippe Beaune, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1001910	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Méthodologie 3-way d'extraction d'un modèle articulatoire de la parole à partir des données d'un locuteur	Pour parler, le locuteur met en mouvement un ensemble complexed'articulateurs : la mâchoire qu'il ouvre plus ou moins, la langue à laquelle ilfait prendre de nombreuses formes et positions, les lèvres qui lui permettent delaisser l'air s'échapper plus ou moins brutalement, etc. Le modèle articulatoirele plus connu est celui de Maeda (1990), obtenu à partir d'Analyses en ComposantesPrincipales faites sur les tableaux de coordonnées des points des articulateursd'un locuteur en train de parler. Nous proposons ici une analyse 3-way dumême type de données, après leur transformation en tableaux de distances. Nousvalidons notre modèle par la prédiction des sons prononcés, qui s'avère presqueaussi bonne que celle du modèle acoustique, et même meilleure quand on prenden compte la co-articulation.	Martine Cadot, Yves Laprie	http://editions-rnti.fr/render_pdf.php?p1&p=1001985	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Mining the Crowd	Harnessing a crowd of Web users for data collection has recently become a wide-spreadphenomenon. A key challenge is that the human knowledge forms an open world and it is thusdifficult to know what kind of information we should be looking for. Classic databases haveaddressed this problem by data mining techniques that identify interesting data patterns. Thesetechniques, however, are not suitable for the crowd. This is mainly due to properties of thehuman memory, such as the tendency to remember simple trends and summaries rather thanexact details. Following these observations, we develop here a novel model for crowd mining.We will consider in the talk the logical, algorithmic, and methodological foundations neededfor such a mining process, as well as the applications that can benefit from the knowledgemined from crowd.	Tova Milo	http://editions-rnti.fr/render_pdf.php?p1&p=1001908	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Modélisation de trajectoires cible/caméra : requêtes spatio-temporelles dans le cadre de la videosurveillance	Le nombre de caméras de vidéosurveillance installées dans le monde augmente chaquejour. En France, le système de la RATP déployé sur Paris comprend 9000 caméras fixes et19000 mobiles. Lors de faits particuliers (e.g., agressions, vols), les opérateurs de vidéo surveillancese basent sur les indications spatiales et temporelles de la victime et sur leur connaissancede la localisation des caméras pour sélectionner les contenus intéressants pour l'enquête.Deux grands problèmes peuvent alors survenir : (1) le temps de réponse est long (jusqu'à plusieursjours de traitement) et (2) un risque important de perte de résultats à cause d'une mauvaiseconnaissance du terrain (appel à des opérateurs extérieurs). Le but de notre recherche estde définir des outils d'assistance aux opérateurs qui puissent, à partir d'une trajectoire donnée,sélectionner de façon automatique les caméras pertinentes par rapport à la requête.	Dana Codreanu, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1001982	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Motifs récursifs : extraction ascendante hiérarchique d'ensembles d'items ou d'évènements pour le résumé de données transactionnelles ou séquentielles	Nous proposons une méthode originale pour extraire un résumé compact,représentatif et intelligible des motifs fréquents dans des données transactionnellesou séquentielles. Notre approche consiste à extraire un nouveau typede motifs que nous appelons motifs récursifs, i.e. des motifs de motifs, à l'aided'un algorithme hiérarchique agglomératif nommé RepaMiner. Nous généronsnon pas un simple ensemble de motifs mais une véritable structure dérivée dedendrogrammes, le RPgraph.	Julien Blanchard	http://editions-rnti.fr/render_pdf.php?p1&p=1001956	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Passage aux noyaux en classification recouvrante	La classification recouvrante correspond à un domaine d'étude très actifces dernières années et dont l'objectif est d'organiser un ensemble de donnéesen groupes d'individus similaires avec la particularité d'autoriser des chevauchementsentre les groupes. Parmi les approches étudiées nous nous intéressonsaux extensions recouvrantes des modèles de type moindres carrés et constatonsles difficultés théoriques et pratiques liées à leur adaptation aux noyaux. Nousformulons alors une nouvelle définition ensembliste pour caractériser un recouvrementde plusieurs classes, nous montrons que cette modélisation permet lerecours aux noyaux et nous proposons une solution algorithmique efficace pourrépondre au problème de la classification recouvrante à noyaux.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001931	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Pondération de blocs de variables en bi-partitionnement topologique	Dans cet article, nous proposons une nouvelle approche permettantà la fois le bi-partitionnement topologique (bi-clustering) et la pondération deblocs variables. Le modèle que nous proposons FBR-BiTM (Feature Block Relevanceusing BiTM) permet de découvrir un espace topologique d'un ensembled'observations et de variables en associant un nouveau score de pondération àchaque sous ensemble de variables. L'estimation des coefficients de pondérationest réalisée dans le même processus d'apprentissage que le bi-partitionnement.Ces pondérations sont locales et associées à chaque prototype. Elles reflètentl'importance locale de chaque bloc de variables pour le bi-partitionnement. L'évaluationmontre que l'approche proposée, comparée	Amine Chaibi, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001943	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Prédiction de valeurs manquantes dans les bases de données-- Une première approche fondée sur la notion de proportion analogique	Cet article présente une méthode originale de prédiction de valeursmanquantes dans les bases de données relationnelles, fondée sur la notion deproportion analogique. Nous montrons en particulier comment un algorithmeproposé dans le cadre de la classification automatique peut être adapté à cette fin.Deux cas sont considérés : celui d'une base de données transactionnelle (attributsbooléens), et celui où les valeurs manquantes peuvent être de type numérique.	William Correa Beltran, Hélène Jaudoin, Olivier Pivert	http://editions-rnti.fr/render_pdf.php?p1&p=1001961	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Que ressentent les patients ?	Les forums de santé en ligne sont des espaces d'échanges où les patientspartagent leurs sentiments à propos de leurs maladies, traitements, etc.Sous couvert d'anonymat, ils expriment très librement leurs expériences personnelles.Ces forums sont donc une source d'informations très utile pour les professionnelsde santé afin de mieux identifier et comprendre les problèmes, lescomportements et les sentiments de leurs patients. Dans cet article, nous proposonsd'exploiter les messages des forums via des techniques de fouille de textespour extraire des traces d'émotions (e.g. joie, colère, surprise , etc.).	Soumia Melzi, Amine Abdaoui, Jérôme Azé, Sandra Bringay, Pascal Poncelet, Florence Galtier	http://editions-rnti.fr/render_pdf.php?p1&p=1001957	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Réconciliation des profils dans les réseaux sociaux	It is not uncommon that individuals create multiple profiles across several SNSs, eachcontaining partially overlapping sets of personal information. As a result, the creation of aglobal profile that gives an holistic view of the information of an individual requires methodsthat automatically match, or reconciliates, profiles across SNSs. In this paper, we focus on theproblem of identifying, or matching, the profiles of any individual across social networks.	Nacéra Bennacer, Coriane Nana Jipmo, Antonio Penta, Gianluca Quercini	http://editions-rnti.fr/render_pdf.php?p1&p=1001915	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Reconstruction et analyse sémantique de chronologies cybercriminelles	La reconstruction de chronologies d'évènements cybercriminels (oureconstruction d'évènements) est une étape primordiale dans une investigationnumérique. Cette phase permet aux enquêteurs d'avoir une vue des évènementssurvenus durant un incident. La reconstruction d'évènements requiert l'étuded'importants volumes de données en raison de l'omniprésence des nouvellestechnologies dans notre quotidien. De plus, les conclusions produites se doiventde respecter les critères fixés par la justice. Afin de répondre à ces challenges,nous proposons une nouvelle méthodologie basée sur une ontologie permettantd'assister les enquêteurs tout au long du processus d'enquête.	Yoan Chabot, Aurélie Bertaux, Tahar Kechadi, Christophe Nicolle	http://editions-rnti.fr/render_pdf.php?p1&p=1001969	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Règles d'association inter-langues au service de la recherche d'information multilingue	Dans cet article, nous proposons de montrer l'intérêt et l'utilité de déploiementdes règles d'association inter-langues (RAILs) dans le domaine de laRecherche d'Information Multilingue (RIM). Ces règles sont des connaissancesadditionnelles résultantes d'un processus de fouille de grands corpus parallèlesalignés au niveau de la phrase. En effet, leurs conclusions exprimées dans unelangue cible représentent des traductions potentielles de leurs prémisses, expriméesdans une langue source. Nous illus trons l'utilisation des RAILs dans lecontexte de la RIM à travers deux propositions, à savoir : (i) la traduction desrequêtes et (ii) la traduction des termes de l'index. L'évaluation expérimentale aété menée sur la collection de documents MUCHMORE. Les résultats ont montréune amélioration significative de la pertinence système.	Belhaj Rhouma Sourour, Asma Ben Achour, Malek Hajjem, Chiraz Latiri	http://editions-rnti.fr/render_pdf.php?p1&p=1001927	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Representative training sets for classification and the variability of empirical distributions	We propose a novel approach for the estimation of the size of trainingsets that are needed for constructing valid models in machine learning and datamining. We aim to provide a good representation of the underlying populationwithout making any distributional assumptions.Our technique is based on the computation of the standard deviation of the 2-statistics of a series of samples. When successive statistics are relatively close,we assume that the samples produced represent adequately the true underlyingdistribution of the population, and the models learned from these samples willbehave almost as well as models learned on the entire population.We validate our results by experiments involving classifiers of various levels ofcomplexity and learning capabilities.	Saaid Baraty, Dan Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1001940	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Requêtes skyline en présence d'exceptions	Dans cet article, nous nous intéressons à la recherche des points lesplus intéressants au sens de l'ordre de Pareto, i.e., à l'évaluation de requêtes« skyline » , dans des jeux de données présentant des anomalies. Il n'est pas rareque les données, de petites annonces par exemple, soient peuplées d'erreurs oud'exceptions qui peuvent perturber la recherche des meilleurs points car cellescisont susceptibles de dominer les autres points. L'approche présentée vise àcalculer les requêtes skyline malgré la présence de ces exceptions, sans pourautant les écarter définitivement, et à présenter graphiquement les résultats defaçon à identifier rapidement les points d'intérêt et les anomalies potentielles.	Hélène Jaudoin, Olivier Pivert, Daniel Rocacher	http://editions-rnti.fr/render_pdf.php?p1&p=1001944	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Sélection de prototypes en vue d'une catégorisation de textes avec les K plus proches voisins : étude comparative	La technique des K plus proches voisins (KNN) est une méthoded'apprentissage à base d'instances, elle a été appliquée dans la catégorisationde textes depuis de nombreuses années. En contraste avec ses performances declassification, il est reconnu que cet algorithme est lent pendant la classificationd'un nouveau document. Les Techniques de sélection de prototypes sont apparuescomme des méthodes très compétitives pour améliorer le KNN grâce à laréduction des données. L'étude contenue dans ce papier a pour objectif d'analyserl'impact de ces méthodes sur la performance de la classification de textesavec l'algorithme KNN.	Fatiha Barigou, Baya Naouel Barigou, Baghdad Atmani, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1001926	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Sélection d'une méthode de classification multi-label pour un système interactif	L'objectif de cet article est d'évaluer la capacité de 12 algorithmesde classification multi-label à apprendre, en peu de temps, avec peu d'exemplesd'apprentissage. Les résultats expérimentaux montrent des différences importantesentre les méthodes analysées, pour les 3 mesures d'évaluation choisies:Log-Loss, Ranking-Loss et Temps d'apprentissage/prédiction, et les meilleursrésultats sont obtenus avec: multi-label k Nearest neighbours (ML-kNN), suivide Ensemble de Classifier Chains (ECC) et Ensemble de Binary Relevance (EBR).	Noureddine Yacine Nair Benrekia, Pascale Kuntz, Franck Meyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001941	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Sous échantillonnage et machine à noyaux élastiques pour la classification de données de mouvement capturé	Dans le domaine de la reconnaissance de gestes isolés, bon nombrede travaux se sont intéressés à la réduction de dimension sur l'axe spatial pourréduire à la fois la complexité algorithmique et la variabilité des réalisationsgestuelles. Il est assez étonnant de constater que peu de ces méthodes se sontexplicitement penchées sur la réduction de dimension sur l'axe temporel. Enmatière de complexité, la réduction de dimension sur cet axe est un enjeu majeurquant à l'utilisabilité de distances élastiques en complexité quadratique. Parailleurs, la prise en compte de la variabilité sur cet axe demeure une source avéréede gain de performance. Pour tenter d'apporter un éclairage en matière deréduction de dimension sur l'axe temporel, nous présentons dans cet article uneapproche basée sur un sous échantillonnage temporel associé à l'exploitationd'un apprentissage automatique à base de noyaux élastiques. Nous montronsexpérimentalement, sur deux jeux de données très référencés dans la communautéet très opposés en matière de qualité de capture de mouvement, qu'il estpossible de réduire sensiblement le nombre de postures sur les trajectoires temporellestout en conservant, grâce à des noyaux élastiques, des performances dereconnaissance au niveau de l'état de l'art du domaine. Le gain de complexitéobtenu rend une telle approche éligible pour des applications temps-réel.	Pierre-François Marteau, Sylvie Gibet, Clément Reverdy	http://editions-rnti.fr/render_pdf.php?p1&p=1001928	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Stratégies argumentatives pour la classification collaborative multicritères des connaissances cruciales	Dans cet article, nous proposons une approche argumentative visant àautomatiser la résolution des conflits entre les décideurs qui ont des préférencescontradictoires lors d'une classification multicritères collaborative des connaissancescruciales. Notre étude expérimentale a prouvé que cette approche peutrésoudre jusqu'à 81% des conflits et améliorer la qualité d'approximation dedécideurs d'un taux de 0.62 pour un récepteur et de 0.15 pour un initiateur.	Sarra Bouzayane, Inès Saad	http://editions-rnti.fr/render_pdf.php?p1&p=1001981	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Symétries et Extraction de Motifs Ensemblistes	Les symétries sont des propriétés structurelles qu'on détecte dans ungrand nombre de bases de données. Dans cet article, nous étudions l'exploitationdes symétries pour élaguer l'espace de recherche dans les problèmes d'extractionde motifs ensemblistes. Notre approche est basée sur une intégrationdynamique des symétries dans les algorithmes de type Apriori permettant de réduirel'espace des motifs candidats. En effet, pour un motif donné, les symétriesnous permettent de déduire les motifs qui lui sont symétriques et vérifiant parconséquent les mêmes propriétés. Nous détaillons notre approche en utilisantl'exemple des motifs fréquents. Ensuite, nous la généralisons au cadre unificateurde Mannila et Toivonen pour l'extraction des motifs ensemblistes. Les expériencesmenées montrent la faisabilité et l'apport de notre approche d'élagagebasé sur les symétries.	Said Jabbour, Mehdi Khiari, Lakhdar Sais, Yakoub Salhi, Karim Tabia	http://editions-rnti.fr/render_pdf.php?p1&p=1001953	
Revue des Nouvelles Technologies de l'Information	EGC	2014	The Hitchhiker's Guide to Ontology	Artificial Intelligence has long had the dream of making computers smarter. For quite sometime, this vision has remained just that: a dream. With the development of large knowledgebases, though, we now have large amounts of semantic information at our hands. This changesthe game of AI. Computers have indeed become smarter. In this talk, we present the latestdevelopments in the field: The construction of general purpose knowledge bases (includingYAGO and DBpedia, as well as NELL and TextRunner), and their applications to tasks thatwere previously out of scope, The extraction of fine-grained information from natural languagetexts, semantic query answering, and the interpretation of newspaper texts at large scale.	Fabian Suchanek	http://editions-rnti.fr/render_pdf.php?p1&p=1001909	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Un système de détection de thématiques populaires sur Twitter	With the ever-growing amount of messages exchanged via Twitter, there is an increasinginterest in filtering this information, which is delivered under the form of a stream of messages.In this paper, we present a system for detecting popular topics in Twitter. The system can beapplied to static corpora and can also handle the live Twitter stream.	Adrien Guille, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001990	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche algébrique au problème du consensus de partitions	En classification non-supervisée, le consensus de partitions a pour objectifde produire une partition unique, représentant le consensus, à partir d'unensemble de partitions où chacune est engendrée indépendamment des autres,voire avec des méthodologies différentes. En complément des techniques ayantleur qualité propre en terme de robustesse ou de passage à l'échelle, nous apportonsun point de vue original sur le consensus de partitions, c'est-à-dire, par lebiais de définitions algébriques qui permettent d'établir la nature des déductionspouvant être réalisées dans une approche systématique (p.ex. un système à basede connaissances). Nous fondons notre approche sur le treillis des partitions pourlequel nous montrons comment peuvent être adjoint des opérateurs dans le butde formuler une expression caractérisant le consensus à partir d'un ensemble departitions.	Frédéric Dumonceaux, Guillaume Raschia, Marc Gelgon	http://editions-rnti.fr/render_pdf.php?p1&p=1001937	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche basée sur STATIS pour la fusion de cartes topologiques auto-organisées	Dans le cadre des cartes topologiques, nous proposons une nouvelleapproche d'ensemble clusters basée sur la méthode STATIS. Les méthodes d'ensembleclusters visent à améliorer la qualité de la partition d'un jeu de donnéesà travers la combinaison de plusieurs partitions.Les différentes partitions peuvent être obtenues en faisant varier les paramètresd'un algorithme (choix des centres initiaux, du voisinage initial et final des cellulesdans le cas des cartes topologiques auto-organisée SOM, etc). L'approcheprésentée dans cette communication repose sur la méthode d'analyse de donnéesmulti-tableaux STATIS pour déterminer une matrice compromis représentant aumieux la similarité entre les partitions issues des cartes topologiques. La fusiondes cartes topologiques est alors obtenue à travers une classification basée surcette matrice compromis. La méthode proposée est illustrée sur des donnéesréelles issues de l'UCI et sur des données simulées.	Mory Ouattara, Ndeye Niang, Rania Gasri, Fouad Badran, Corinne Mandin	http://editions-rnti.fr/render_pdf.php?p1&p=1001947	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche PPC pour la fouille de données séquentielles	Nous proposons dans cet article une nouvelle approche croisant destechniques de programmation par contraintes et de fouille pour l'extraction demotifs séquentiels. Le modèle que nous proposons offre un cadre générique etdéclaratif pour modéliser et résoudre des contraintes de nature hétérogène	Jean-Philippe Métivier, Samir Loudni, Thierry Charnois	http://editions-rnti.fr/render_pdf.php?p1&p=1001951	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approcheWeb sémantique et combinatoire pour un système de recommandation sensible au contexte appliqué à l'apprentissage mobile	Au vu de l'émergence rapide des nouvelles technologies mobiles et lacroissance des offres et besoins d'une société en mouvement, les travaux se multiplientpour identifier de nouvelles plateformes d'apprentissage pertinentes afind'améliorer et faciliter l'apprentissage à distance. La prochaine étape de l'apprentissageà distance est naturellement le port de l'e-learning (apprentissageélectronique) vers les nouveaux systèmes mobiles. On parle de m-learning (apprentissagemobile). Nos travaux portent sur le développement d'une nouvellearchitecture pour le m-learning dont l'objectif est d'adapter et recommander desparcours de formations selon les contraintes contextuelles de l'apprenant.	Fayrouz Soualah Alila, Christophe Nicolle, Florence Mendes	http://editions-rnti.fr/render_pdf.php?p1&p=1001979	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une heuristique pour le paramétrage automatique de l'algorithme de clustering spectral	Trouver le nombre optimal de groupes dans le contexte d'un algorithmede clustering est un problème notoirement difficile. Dans cet article,nous en décrivons et évaluons une solution approchée dans le cas de l'algorithmespectral. Notre méthode présente l'avantage d'être déterministe, et peucoûteuse. Nous montrons qu'elle fonctionne de manière satisfaisante dans beaucoupde cas, même si quelques limites amènent des perspectives à ce travail.	Pierrick Bruneau, Olivier Parisot, Philippe Pinheiro	http://editions-rnti.fr/render_pdf.php?p1&p=1001933	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode hybride pour la prédiction du profil des auteurs	Dans cet article, nous nous intéressons à la détection du profil desauteurs (âge, genre) à travers leurs discussions. La méthode proposée s'appuiesur la classification automatique qui utilise certaines données extraites d'une manièrestatistique à partir de corpus source. Nous présentons une méthode hybridequi combine l'analyse de surface dans les textes avec une méthode d'apprentissageautomatique. A fin d'obtenir une meilleure gestion de ces données, nousnous sommes basés sur l'utilisation des arbres de décision. Notre méthode adonné des résultats intéressants pour la détection du genre.	Seifeddine Mechti, Maher Jaoua, Lamia Hadrich Belguith	http://editions-rnti.fr/render_pdf.php?p1&p=1001973	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode pour caractériser les communautés des réseaux dynamiques à attributs	De nombreux systèmes complexes sont étudiés via l'analyse de réseauxdits complexes ayant des propriétés topologiques typiques. Parmi cellesci,les structures de communautés sont particulièrement étudiées. De nombreusesméthodes permettent de les détecter, y compris dans des réseaux contenant desattributs nodaux, des liens orientés ou évoluant dans le temps. La détection prendla forme d'une partition de l'ensemble des noeuds, qu'il faut ensuite caractériserrelativement au système modélisé. Nous travaillons sur l'assistance à cettetâche de caractérisation. Nous proposons une représentation des réseaux sous laforme de séquences de descripteurs de noeuds, qui combinent les informationstemporelles, les mesures topologiques, et les valeurs des attributs nodaux. Lescommunautés sont caractérisées au moyen des motifs séquentiels émergents lesplus représentatifs issus de leurs noeuds. Ceci permet notamment la détectionde comportements inhabituels au sein d'une communauté. Nous décrivons uneétude empirique sur un réseau de collaboration scientifique.	Gu&#776;nce Keziban Orman, Vincent Labatut, Marc Plantevit, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001919	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode pour la détection de thématiques populaires sur Twitter	L'explosion du volume de messages échangés via Twitter entraîne unphénomène de surcharge informationnelle pour ses utilisateurs. Il est donc crucialde doter ces derniers de moyens les aidant à filtrer l'information brute, laquelleest délivrée sous la forme d'un flux de messages. Dans cette optique, nousproposons une méthode basée sur la modélisation de l'anomalie dans la fréquencede création de liens dynamiques entre utilisateurs pour détecter les picsde popularité et extraire une liste ordonnée de thématiques populaires. Les expérimentationsmenées sur des données réelles montrent que la méthode proposéeest capable d'identifier et localiser efficacement les thématiques populaires.	Adrien Guille, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001917	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Une nouvelle approche pour la sélection de variables basée sur une métrique d'estimation de la qualité	La maximisation d'étiquetage (F-max) est une métrique non biaiséed'estimation de la qualité d'une classification non supervisée (clustering) qui favoriseles clusters ayant une valeur maximale de F-mesure d'étiquetage. Danscet article, nous montrons qu'une adaptation de cette métrique dans le cadrede la classification supervisée permet de réaliser une sélection de variables etde calculer pour chacune d'elles une fonction de contraste. La méthode est expérimentéesur différents types de données textuelles. Dans ce contexte, nousmontrons que cette technique améliore les performances des méthodes de classificationde façon très significative par rapport à l'état de l'art des techniquesde sélection de variables, notamment dans le cas de la classification de donnéestextuelles déséquilibrées, fortement multidimensionnelles et bruitées.	Jean-Charles Lamirel, Pascal Cuxac, Kafil Hajlaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001923	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Utilisation de relations ontologiques pour la comparaison d'images décrites par des annotations sémantiques	Face à la complexité des nouvelles générations d'images médicales, les processus de recherche d'images basés sur leurs contenus visuels peuvent s'avérer insuffisants. Cet article propose une nouvelle approche basée sur l'annotation des images via des termes sémantiques pouvant pallier ce problème. Elle repose sur la combinaison d'une distance hiérarchique permettant de comparer les images en considérant les corrélations entre les termes utilisés pour les décrire et d'une mesure de similarité permettant d'évaluer la proximité sémantique entre des termes ontologiques. Cette approche est validée dans le cadre de la recherche d'images tomodensitométriques.	Camille Kurtz, Daniel Rubin	http://editions-rnti.fr/render_pdf.php?p1&p=1001991	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Vectorisation paramétrée des données textuelles	Automatic processing of textual data enables users to analyze semi-automatically and on alarge scale the data. This analysis is based on two successive processes: (i) representation oftexts, (ii) gathering of textual data (clustering). The software described in this paper focuses onthe first step of the process by offering expert a parameterized representation of textual data.	Célia Da Costa Pereira, Mathieu Lafourcade, Patrick Lloret, Cédric Lopez, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001987	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Vers une classification non supervisée adaptée pour obtenir des arbres de décision simplifiés	L'induction d'arbre de décision est une technique puissante et populairepour extraire de la connaissance. Néanmoins, les arbres de décision obtenusdepuis des données issues du monde réel peuvent être très complexes et donc difficilesà exploiter. Dans ce cadre, cet article présente une solution originale pouradapter le résultat d'une classification non supervisée quelconque afin d'obtenirdes arbres de décision simplifiés pour chaque cluster.	Olivier Parisot, Yoanne Didry, Pierrick Bruneau, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1001965	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Vers une modularité pour données vectorielles	La modularité, introduite par Newman pour mesurer la qualité d'unepartition des sommets d'un graphe, ne prend pas en compte d'éventuelles valeursassociées à ces sommets. Dans cet article, nous introduisons une mesure de modularitécomplémentaire, basée sur l'inertie, et adaptée pour évaluer la qualitéd'une partition d'éléments représentés dans un espace vectoriel réel. Cette mesurese veut un pendant pour la classification non supervisée de la modularitéde Newman. Nous présentons également 2Mod-Louvain, une méthode utilisantce critère de modularité basée sur l'inertie conjointement à la modularité deNewman pour détecter des communautés dans des réseaux d'information. Lesexpérimentations que nous avons menées ont montré qu'en exploitant à la foisles données relationnelles et vectorielles, 2Mod-Louvain détectait plus efficacementles communautés que des méthodes utilisant un seul type de données etqu'elle était robuste face à des dégradations des données.	David Combe, Christine Largeron, Elod Egyed-Zsigmond, Mathias Géry	http://editions-rnti.fr/render_pdf.php?p1&p=1001914	
Revue des Nouvelles Technologies de l'Information	EGC	2014	Visualisation de données de prosopographie pour la reconstruction de carrières de personnages et de réseaux socio-professionnels	Dans cet article nous présentons deux approches de visualisation développéesdans le cadre d'un projet collaboratif sur l'accès et l'exploitation desdonnées prosopographiques de la Renaissance en France. L'objectif du projetest de modéliser et réaliser un portail sémantique assurant l'accès à différentesbases de données prosopographiques existantes afin de permettre une meilleureexploration et exploitation de ces données. Dans ce cadre, nous avons proposédeux interfaces de visualisation ProsoGraph et ProsoMap qui s'appuient respectivementsur la visualisation de graphes de réseaux sociaux et la visualisationde lieux géographiques et de trajectoires spatio-temporelles. Les deux interfacescommuniquent avec le portail via une couche sémantique et lui offrent des fonctionnalitésd'interrogation supplémentaires.	Nizar Messai, Thomas Devogele	http://editions-rnti.fr/render_pdf.php?p1&p=1001978	
Revue des Nouvelles Technologies de l'Information	FDC	2014	2S-SOM : une méthode de soft-subspace clustering pour données multi-blocs basée sur les cartes topologiques auto-organisées	Nous proposons une méthode de soft subspace clustering basée sur les cartes topologiques pour la classification d'individus décrits par des variables structurées en blocs homogènes. L'algorithme nommé Soft Subspace SOM (2SSOM) consiste à optimiser la fonction de coût de SOM modifiée en introduisant des poids adaptatifs sur les blocs et sur les variables de chaque bloc. Cette double pondération permet de distinguer les blocs les plus importants prenant ainsi en compte la structuration en blocs, et d'identifier pour chaque bloc les variables les plus informatives pour les classes. La méthode permet alors de déterminer simultanément les groupes d'individus et leurs sous espaces caractéristiques optimaux. La méthode est illustrée sur des données réelles issues des bases de l'UCI repository of machine learning et sur des données simulées.	Mory Ouattara, Ndeye Niang, Fouad Badran, Corinne Mandin	http://editions-rnti.fr/render_pdf.php?p1&p=1002050	
Revue des Nouvelles Technologies de l'Information	FDC	2014	Apprentissage de règles floues pour caractériser des objets d'intérêt dans une image de télédétection	Les nouveaux capteurs satellitaires permettent l'acquisition d'images d'un très haut niveau de détail à des cadences élevés, produisant ainsi une importante masse de données. Le traitement manuel de ces données étant devenu impossible, de nouveaux outils sont nécessaires afin de les traiter automatiquement. Dans ce cadre, des algorithmes de segmentation efficaces sont nécessaires pour extraire des objets d'intérêt de ces images. Cependant, les segments produits ne correspondent généralement pas aux objets d'intérêt. Dans cet article, nous proposons de changer le niveau d'abstraction afin d'interpréter les objets d'intérêt comme des objets composés par des segments. Pour cela, nous avons mis en place un processus d'apprentissage multi-niveaux, basé sur des connaissances expertes, dans le but d'apprendre des règles de compositions définissant des objets d'intérêt. Pour gérer l'imprécision relative à l'analyse d'images de télédétection nous proposons d'utiliser la logique floue afin de modéliser les règles de composition. La méthode proposée est validée sur des données de synthèse ainsi que sur des données réelles.	Bruno Belarte, Cédric Wemmert, Germain Forestier, Christiane Weber, Manuel Grizonnet	http://editions-rnti.fr/render_pdf.php?p1&p=1002048	
Revue des Nouvelles Technologies de l'Information	FDC	2014	Approche Fouille de Texte pour la détection précoce de tendances économiques	Cet article présente un retour d'expérience sur de la fouille de données complexes dans un processus d'extraction des connaissances dans un contexte industriel. Á partir de données volumineuses non structurées issues de dépêches d'actualités économiques et selon certains traitements linguistiques et économétriques, notre objectif est de prédire des tendances économiques dans des séquences d'évènements d'actualités. Pour cela, trois étapes sont primordiales : (i) l'extraction d'indicateurs économiques par des techniques linguistiques (comme les indices boursiers, les taux de change, les noms des monnaies ou encore les cours des matières premières. . .), (ii) l'annotation, par le recours à des terminologies externes, de ces indicateurs économiques : les données extraites portent alors des étiquettes permettant de les identifier, (iii) leur superposition à des modèles statistiques. Á la suite de ce traitement, nous pouvons vérifier si il existe une corrélation entre des indicateurs économiques relevés par l'étude linguistique pour un secteur d'activité donné et sur un territoire donné (la production d'un élément A sur le prix d'un élément B par exemple). L'intérêt de cette méthode est d'apporter des outils linguistiques en complément des méthodes statistiques utilisées habituellement pour faire émerger des données cointégrées. L'article décrit ensuite les expérimentations effectuées et tire les premières conclusions sur divers aspects de cette méthode.	Marilyne Latour, Antoine Sigwalt	http://editions-rnti.fr/render_pdf.php?p1&p=1002052	
Revue des Nouvelles Technologies de l'Information	FDC	2014	Considérant la dépendance dans la théorie des fonctions de croyance	La fusion d'informations issues de plusieurs sources cherche à améliorer la prise de décision. La théorie des fonctions de croyance, pour réaliser cette fusion, utilise des règles de combinaison faisant bien souvent l'hypothèse forte de l'indépendance des sources. Cette hypothèse d'indépendance n'est cependant pas formalisée ni vérifiée. Nous proposons dans cet article un apprentissage de l'indépendance cognitive de sources d'information permettant de mesurer la dépendance ou l'indépendance. Cette mesure exprimée par une fonction de masse est ensuite intégrée par une approche d'affaiblissement avant de réaliser la combinaison d'informations.	Mouna Chebbah, Mouloud Kharoune, Arnaud Martin, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1002049	
Revue des Nouvelles Technologies de l'Information	FDC	2014	Détection de nouveautés en utilisant un nouveau score de détection de "groupes-outliers"	Dans cet article, nous introduisons une nouvelle mesure pour qualifier "l'outlier-ness" de chaque groupe/cluster. Cette mesure, nommée GOF, est intégrée et estimée dans un processus d'apprentissage non supervisé en utilisant les cartes topologiques. Ceci permet d'apprendre la structure des données tout en fournissant un nouveau score (GOF). Ce paramètre est basé sur la densité et quantifie ainsi la particularité de chaque groupe (cluster) : plus la valeur est grande, plus le groupe est susceptible d'être un "groupe-outlier". GOF est utilisé par la suite comme classifieur pour le problème de détection de nouveautés.	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1002051	
Revue des Nouvelles Technologies de l'Information	FDC	2014	GENDESC : Vers une nouvelle représentation des données textuelles	Dans cet article, nous nous intéressons à la classification automatique de données textuelles par des algorithmes d'apprentissage supervisé. L'objectif est de montrer comment l'amélioration de la représentation des données textuelles influe sur les performances des algorithmes d'apprentissage. Partant du postulat qu'un mot n'a pas un sens bien établi sans son contexte, nous proposerons des descripteurs donnant le plus d'information possible sur le contexte des mots. Pour cela, nous avons mis au point une méthode, nommée GENDESC, qui consiste à "généraliser" les mots les moins pertinents pour la classification, c'est-à-dire, à éviter le bruit sémantique (souvent dû à la polysémie) provoqué par ces termes non ou peu pertinents. Cette généralisation s'appuie sur des informations grammaticales, telles que la catégorie et la position dans la structure. La méthode GENDESC a été évaluée et adaptée à la problématique de classification de textes selon une opinion ou une thématique.	Guillaume Tisserant, Violaine Prince, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1002053	
Revue des Nouvelles Technologies de l'Information	FDC	2014	Opérateurs d'agrégation pour les données multiples	Cet article traite du sujet des opérateurs d'agrégation issus de la logique floue. En particulier, un exemple d'opérateur d'agrégation nommé triple Pi, ainsi que certaines de ses variantes et de ses généralisation, sont présentés ; le triple Pi présente certaines potentialités intéressantes pour la fusion d'informations. Quelques applications de cet opérateur sont également présentées. De façon plus général, à travers cet exemple, on cherche à illustrer quelques-unes des nombreuses possibilités offertes par l'utilisation des opérateurs d'agrégations dans le domaine de la fouille de données complexes et massives.	Sébastien Régis, Richard Emilion, Andrei Doncescu	http://editions-rnti.fr/render_pdf.php?p1&p=1002047	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	A la croisée des langues : Annotation et fouille de corpus plurilingues	Un programme de recherche en cours sur l'étude des phénomènes decontact de langues et de leur rôle dans le changement linguistique s'attache à recueillirdes corpus plurilingues, témoignant d'une grande variété de phénomènesde contact sur un échantillon suffisamment varié de langues génétiquement et typologiquementdistinctes. Cet effort a impliqué le développement d'une chaînede traitement des corpus numériques qui tienne compte des spécificités des corpusplurilingues, pour la représentation des données linguistiques, leur stockage,leur annotation, leur visualisation, et les traitements de recherche d'information.Les normes existantes ont dû être étendues pour prendre en compte l'appartenancepotentielle d'unités à plusieurs langues dans les pratiques langagièresplurilingues. Dans cet article, nous décrivons la manière dont a été définie lastructure de ces corpus plurilingues, et la conception technique de l'unité linguistiquemultilingue qui préside à la fouille de données dans ces corpus.	Pascal Vaillant, Isabelle Léglise	http://editions-rnti.fr/render_pdf.php?p1&p=1002030	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	Fouille au Corps des Media Français : Un exemple concret de Fouille Multimodale Transmedia	Numérique, réseaux et mobiles ont bousculé irréversiblement la production,la diffusion et la consommation des media d'actualité. L'ObservatoireTransmedia est une plateforme de description, d'unification et d'analyse des actualitésdiffusées par la radio, la télévision, le web et l'AFP, sur la période mi2011/fin 2013. La collaboration des chercheurs en informatique, SHS et professionnelsde l'information a permis la conception, la validation et l'améliorationde la plateforme. La réalisation d'études -analyse de propagation, chronologiesd'évènements, ou encore analyse des taux de reprise des dépêches AFPnousa permis de répondre partiellement aux questions initiales du projet:Comment l'actualité aborde t'elle un sujet ? Qui produit l'information ? Lamultiplication des supports garantit-elle la diversité?	M.L. Viaud, A. Saulnier, D. Teyssou, N. Hervé, B. Renoust, J. Thièvre	http://editions-rnti.fr/render_pdf.php?p1&p=1002031	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	Le programme de recherche sur les archives du Fonds Alcides Giraldi : un exemple de production de données en humanités numériques1	Parmi les patrimoines écrits que les institutions publiques ou privéessont susceptibles de recevoir, les archives des écrivains contemporains représentent,par leur originalité, leur diversité et leur amplitude, un défipour celles qui acceptent d'assumer leur conservation, leur préservation et de faciliter leur consultation. Du point de vue de la recherche, au sein deshumanités numériques en particulier, ces archives constituent une importanteréserve de données à condition, bien sûr, que celles-ci soient exploitables. Cetarticle propose d'expliquer comment, depuis 2003, nous nous sommes engagésdans la constitution d'une telle réserve à travers le projet de recherches et devalorisation du fonds « Alcides Giraldi ». Ce fonds, actuellement hébergé àLille, comporte de nombreux inédits susceptibles de documenter la recherche internationalesur la circulation des idées entre l'Europe et les Amériques durantles principaux conflits du XXème siècle.	Cécile Braillon-Chantraine, André Davignon, Nicolas Laboche, Fatiha Idmhand, Cécile Martini	http://editions-rnti.fr/render_pdf.php?p1&p=1002034	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	Les Humanités Numériques : la révolution en Sciences Humaines et Sociales	Comprendre l'homme et la société et leurs interactions mutuelles avec l'environnement selon différentes dimensions - matérielle, sociale, économique,culturelle... - constitue l'une des missions premières des disciplines de SciencesHumaines et Sociales (SHS). L'avènement du Numérique peut être considérécomme la troisième révolution majeure de l'être humain après l'écriture, il y aplus de 3500 ans, et l'imprimerie, il y a plus de 500 ans. Le contenu de cet articleest volontairement construit sur le mode d'un rapport de conjoncture surle couple SHS et Numérique, que nous désignons par Humanités Numériques(HN). Nous voulons voir comment et dans quelle mesure cette attelage pourraitfaciliter cette missions des SHS. Pour ce faire, nous proposons d'identifier lesconditions de cette réussite et d'en évaluer les retombées tant scientifiques quesocio-économiques. Dans ce papier, il s'agit, très concrètement, de cartographierles contours des (HN) afin de bien cerner leur objet et leurs rapports aux SHSd'une part, et à l'informatique, d'autre part. Nous dressons un panorama des acteursvisibles sur la toile Internet, nous tentons d'appréhender leurs méthodes detravail pour entrevoir comment le Numérique pourrait être encore mieux mobilisépar les SHS afin leur donner une place centrale dans la science et dans lasociété. Nous terminons ce portrait par une liste, certes partielle, de quelquesgrands défis scientifiques et techniques qui attendent les HN. Défis qui pourraientconstituer de belles opportunités de recherche théoriques et appliquéespour une large communauté scientifique allant des SHS aux mathématiques enpassant par l'informatique.	Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1002027	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	OPILAND : identification de la perception des territoires par la fouille de texte	De nombreux travaux ont été réalisés en extraction d'informations etplus particulièrement en fouille de données d'opinions dans des contextes spécifiquestels que les critiques de films, les évaluations de produits commerciaux,les discours électoraux... Dans le cadre du projet SENTERRITOIRE, nous nousposons la question de l'adéquation de ces méthodes pour des documents associésà l'aménagement des territoires. Ces documents renferment différents typesd'informations se rapportant à des acteurs, des opinions, des informations géographiques,et tout autre aspect lié plus généralement à la notion de territoire.Cependant, il est extrêmement difficile d'identifier puis de lier les opinions àces informations. Après avoir souligné les limites des propositions actuelles etles verrous soulevés par les données textuelles associées, nous proposons la méthodesemi-automatique nommée OPILAND (OPinion mIning from LAND-useplanning documents) combinant une chaîne de Traitement Automatique du LangageNaturel et des techniques de Fouilles de Textes pour (1) détecter les entitésnommées de type lieu et organisation, (2) construire un vocabulaire d'opinionsrelatif au domaine d'application, et (3) identifier les opinions relatives aux entitésnommées traitées. Les expérimentations sont menées sur des données du bassinde Thau (France), puis appliquées sur trois corpus relatifs à d'autres domainesafin de mettre en avant la généricité de notre approche.	Eric Kergosien, Bernard Laval, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1002035	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	Sélection et détection de communautés virtuelles par structures locales cohésives	La sélection et la détection de communautés virtuelles posent encoredes problèmes méthodologiques, notamment parce que les algorithmes de partitionde graphes ne sont pas efficaces dans des réseaux partiels ou trop larges,cas typique des environnements d'hyperliens. Nous proposons ici une méthodede détection de communautés virtuelles ne nécessitant qu'une connaissance partielledu réseau. Les communautés virtuelles sont explorées par un robot d'indexationdétectant des structures cohésives locales. Nous présenterons certainsrésultats expérimentaux montrant les différences entre un robot utilisant des arcsorientés et non orientés, de même qu'une application plus générale à la détectionde communautés virtuelles dans la blogosphère politique belge francophone.	Yoann Veny	http://editions-rnti.fr/render_pdf.php?p1&p=1002028	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	TIC et expression démocratique en Afrique : mirage ou virage ? Essai d'analyse wébométrique de l'espace public numérique sénégalais	Au Sénégal, malgré un taux de pénétration d'internet assez faible lemonde politique n'est pas insensible aux opportunités qu'offrent les TIC. Àl'occasion des élections présidentielles de 2012, les partis politiques ontinvesti des moyens humains et financiers dans le but de créer une interactiviténumérique avec le public. Pendant ce temps se développe un e-engagementcitoyen qui se révèle à travers l'utilisation des médias sociaux. Par une analysewébométrique, cet article se propose de mesurer l'impact de cette « nouvelleforme de communication » en s'interrogeant sur : 1) l'audience et la popularitédes actions de communication électronique mises en place par les partispolitiques. 2) l'organisation des mouvements de la société civile à traversInternet. 3) l'e-engagement citoyen des Sénégalais pour la transparence et lerespect des règles démocratiques.	Djibril Diakhaté	http://editions-rnti.fr/render_pdf.php?p1&p=1002032	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	Vers une approche semi-automatique pour la définition de motifs d'argumentation utilisés dans les résumés de projets scientifiques du domaine de la biodiversité	Nous positionnons notre travail dans le domaine de l'analyse et de lavisualisation de données textuelles produites par les scientifiques et réunies encorpus calibré. Ce domaine est reconnu pour sa contribution à la réflexion surla composition et l'évaluation des politiques scientifiques. Le corpus que nousutilisons est une collection de tous les résumés de projets acceptés dans desguichets d'appels à projet dans le domaine de la biodiversité référencés par leréseau européen BiodivERsA. L'objectif de ce travail ancré dans la sociologiedes sciences consiste à mieux comprendre les principales caractéristiques utiliséespar les scientifiques pour présenter leur projet et convaincre de ses qualités.Pour cela nous avons utilisé une pluralité d'outils face à la difficulté de dépouillerl'information pour associer le niveau sémantique (structure del'information) au niveau pragmatique (relations entre les rédacteurs de projet).Notre contribution repose sur un nouveau type d'extraction d'information, horsentités nommées, basé sur l'extraction de motifs d'argumentation. D'une parton remarque que l'usage de ces motifs marque la présence d'arguments dansdes résumés de projets, et d'autre part croît avec le temps.	Rosa Cetro, Marc Barbier, Philippe Breucker, Hilde Eggermont, Philippe Gambette, Tita Kyriacopoulou, Xavier Le Roux, Claude Martineau, Nicolas Turenne	http://editions-rnti.fr/render_pdf.php?p1&p=1002029	
Revue des Nouvelles Technologies de l'Information	MASHS	2014	Vers une réduction du fossé sémantique dans le traitement des images de documents anciens à base d'ontologies : Application aux lettrines	Nous présentons dans cet article une approche de réduction du fossésémantique dans le traitement des documents anciens, et plus particulièrementles images de lettrines. Cette approche modélise les connaissances de deux domaines(experts historiens et experts en traitement d'images) au sein d'ontologies.Cette démarche permet d'établir des liens entre ces domaines afin d'aiderles historiens à interpréter ces images et à les situer dans le temps. Nous avonscombiné trois ontologies (thésaurus défini par les historiens, ontologie du traitementd'image et ontologie spatiale) pour l'annotation de ces images; ainsi nousavons donc défini des règles permettant d'annoter certaines régions de l'imagecomme étant la lettre, une partie du corps de personnage ou encore de caractériserune lettrine comme figurative. Des expérimentations prouvent la pertinencedu système dans l'annotation automatique d'images. Par ailleurs, la généricitéde l'approche permet d'en envisager une exploitation dans d'autres contextescombinant analyse d'image, analyse spatiale et sémantique du domaine, commepar exemple des images de bandes dessinées Tsopze et al. (2012).	Mickaël Coustaty, Norbert Tsopzé, Alain Bouju, Karell Bertet, Georges Louis	http://editions-rnti.fr/render_pdf.php?p1&p=1002033	
Revue des Nouvelles Technologies de l'Information	W	2014	CA-Manager: a middleware for mutual enrichment between information extraction systems and knowledge repositories	Knowledge enrichment aims at bridging the large gap between structured knowledge and the large volumes of unstructured text data that companies and people need to deal with daily. Alas, the process is very laborious and error-prone, even when performed semi-automatically. The two key steps in this process -semantic annotation and ontology population- still hold outstanding challenges although they are actively studied by researchers. While there exists a large number of tools, many of them lack compliance with Semantic Web standards, but more important, they lack the flexibility to customise the entire knowledge acquisition workflow. In this paper, we present the Content Augmentation Manager (CA-Manager) framework which plays a middleware role between Information Extraction (IE) tools and knowledge repositories (KR)s. CA-Manager allows us an easy plug-in of various types of components leading to create a virtuous cycle within the annotation workflow.	Hacène Cherfi, Martin Coste, Florence Amardeilh	http://editions-rnti.fr/render_pdf.php?p1&p=1001998	
Revue des Nouvelles Technologies de l'Information	W	2014	CAMERA-DREAM : Une étude duWeb de données dans le contexte d'un projet universitaire	L'enseignement dispensé dans le cadre du Master Génie Informatique et Logiciel de l'Université de Rouen inclut un projet de grande envergure qui mobilise chaque année tous les étudiants de la promotion. En 2012-2013, ce projet intitulé CAMERA-DREAM visait à constituer une base de connaissance consacrée au cinéma et publiable sur le Web en accès ouvert. Pour exploiter le contenu de cette base, une application de filtrage collaboratif devait être développée afin de permettre à un internaute de sélectionner des films répondant à ses goûts et à ses attentes. Pour atteindre ces différents objectifs, la modélisation d'une ontologie du cinéma et la définition d'un algorithme de calcul de distance sémantique constituaient des prérequis.	Patrick Giroux, Esther Nicart	http://editions-rnti.fr/render_pdf.php?p1&p=1002002	
Revue des Nouvelles Technologies de l'Information	W	2014	DOWSER : Discovery ofWeb Sources by Evaluating Relevance	The constant growth of the Web in recent years has made more difficult the discovery of new sources of interest on a given topic. In particular for intelligence analysts which are confronting the search of hard-to-find pages on specific topics with traditional Information Retrieval tools. In this paper, we describe a new Web source discovery system called DOWSER (Discovery Of Web Sources Evaluating Relevance). The goal of this system is to provide users with new relevant sources of information according to their needs without using search engines. We study the interest of exploiting a user profile to lead a focused crawling process in order to avoid collecting and indexing all accessible Web documents. The user's information needs are not specified using keywords, but using user'sWeb pages of interests represented by DBPedia resources (Bizeret al., 2009). A series of experiments were conducted on the Web and they provided an empirical evaluation. Results of these user experiments are presented in this paper.	Romain Noël, Alexandre Pauchet, Bruno Grilheres, Nicolas Malandain, Stéphan Brunessaux, Laurent Vercouter	http://editions-rnti.fr/render_pdf.php?p1&p=1001997	
Revue des Nouvelles Technologies de l'Information	W	2014	Enrichissement d'une RTO par l'ajout de termes spécialisés	Nous proposons dans cet article une méthode d'enrichissement d'une Ressource Termino-Ontologique (RTO) par l'ajout de termes extraits d'un corpus de documents textuels constitué dans le domaine d'application décrit par la RTO. Une RTO est une ressource comportant une composante conceptuelle (l'ontologie) et une composante terminologique (la terminologie), dans laquelle les termes sont distingués des concepts qu'ils dénotent. Nous nous intéressons à l'enrichissement d'une RTO permettant de modéliser des relations n-aires entre des données quantitatives expérimentales, où les arguments peuvent être des concepts symboliques ou des quantités caractérisées par des unités de mesure. La méthode proposée consiste à enrichir la terminologie associée aux concepts symboliques et la terminologie associée aux unités de mesure par de nouveaux termes extraits du corpus.	Soumia Lilia Berrahou, Ludovic Lebras, Patrice Buche, Juliette Dibie-Barthélemy, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1002000	
Revue des Nouvelles Technologies de l'Information	W	2014	Vers une utilisation automatique des Informations Personnelles pour la réalisation de e-procedures	Les utilisateurs possèdent une énorme quantité d'informations personnelles et qui ne cesse d'augmenter. Ceci pose un vrai besoin de développer un système de gestion d'informations personnelles qui permet aux utilisateurs de gérer leur données d'une façon efficace. L'une des tâches à effectuer par les utilisateurs est le remplissage de formulaires. Nous proposons une méthode pour le remplissage automatique de formulaires. Dans cet article nous proposons deux approches complémentaires afin de réaliser le remplissage automatique de formulaires. La sélection des informations personnelles pertinente au remplissage du formulaire et la recherche des informations manquantes en utilisant la composition de services.	Rania Khefifi, Pascal Poizat, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001999	
Revue des Nouvelles Technologies de l'Information	W	2014	WebLab-PROV : la gestion de la provenance dans la plateformeWebLab	Dans une démarche de gestion de la qualité au sein de la plateforme de media-mining WebLab, nous présentons un modèle de provenance permettant aux fournisseurs de services de définir les dépendances entre les données en sortie et les données en entrée. Ce modèle utilise une version étendue du standard XPath, et XQuery afin de parcourir les noeuds XML des entrées et sorties des services.Nous présentons également l'implémentation de ce modèle de provenance au sein de la plateforme WebLab, montrant tous les formats de stockage des informations, ainsi que le fonctionnement pas à pas de notre outil dans un cas d'utilisation typique.	Clément Caron, Bernd Amann, Camelia Constantin, Patrick Giroux	http://editions-rnti.fr/render_pdf.php?p1&p=1002001	
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2013	La méthode des équations structurelles : Principes fondamentaux et applications en marketing	Le recours aux modèles des équations structurelles en sciences de gestion et plus particulièrement en marketing, représente un axe méthodologique et empirique prometteur, et une orientation innovante en matière de développement de la théorie, grâce à un ensemble de démarches et de techniques avancées. Par conséquent, le présent article s'attache essentiellement à expliciter l'utilité et l'intérêt portés à ces méthodes de deuxième génération dans la validation des mesures et des modèles de causalité, et dans la spécification des construits théoriques ainsi que les relations étudiées simultanément. Après avoir présenté un aperçu relatif aux fondements conceptuels et à la procédure de la réalisation d'un modèle d'équations structurelles, le deuxième volet de cet article tente d'exposer la pratique couramment adoptée de ces méthodes par les chercheurs en marketing.Empiriquement, il parait important de proposer un exemple illustratif concret traitant l'étude de la relation entre la qualité de services, la satisfaction et la fidélité des clients envers leurs prestataires de services téléphoniques. A cet effet, une enquête a été opérée auprès de 223 répondants dans le but de valider un modèle causal dans le domaine des services.	Hechmi Najjar, Chaker Najar	http://editions-rnti.fr/render_pdf.php?p1&p=1001904	http://editions-rnti.fr/render_pdf.php?p=1001904
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2013	Multiple and Joint Correspondence Analysis: Testing the True Dimension of a Study	On discute le problème de la dimension d'une Analyse des Correspondances Multiples,basé soit sur la ré-evaluation de l'inertie expliquée sensu Benzécri (1979) et Greenacre (2006) et le test proposé par Ben Ammou and Saporta (1998). Ceci conduit à la considération d'une meilleure reconstruction des sous-tableaux hors-diagonale du tableau de Burt qui croise les charactères nominaux considérés. On introduit donc l'Analyse des Correspondances Conjointe (JCA, Greenacre, 1988) et on montre les résultats obtenu dans deux applications. On compare aussi la qualité de la reconstruction obtenue par les solutions MCA et JCA avec les résultats de l'Analyse des Correspondances Simples sur les tableaux à deux voies. l résulte que la reconstruction de dimensione réduite de la JCA est fort meilleure de celle de la MCA, qui s'avère fort biaisée et non-monotone.	Sergio Camiz, Gastão Coelho Gomes	http://editions-rnti.fr/render_pdf.php?p1&p=1001903	http://editions-rnti.fr/render_pdf.php?p=1001903
Revue des Nouvelles Technologies de l'Information	EDA	2013	Cube de textes et opérateur d'agrégation basé sur un modèle vectoriel adapté	Les technologies d'entreposage de données et d'analyse en ligne (On-Line Analytical Processing OLAP) ont largement fait leurs preuves pour l'analysede données structurées, mais elles sont inadaptées pour l'analyse des donnéestextuelles, faute d'outils et de méthodes adaptés. Nous proposons dans cetarticle, un modèle de cube textuel nommé TCube, qui comporte plusieurs dimensionssémantiques, pour une meilleure prise en charge de la sémantique desdonnées textuelles. Les attributs de chaque dimension sémantique sont regroupésdans une hiérarchie de concepts, extraite à partir d'une ontologie de domaineutilisée comme une ressource externe. Notre cube de textes comprend une mesured'analyse textuelle qui s'appuie à la fois sur un modèle vectoriel adapté àl'analyse OLAP et sur une technique de propagation de pertinence. Il est égalementassocié à un nouvel opérateur d'agrégation appelé ORank(OLAP-Rank)permettant d'agréger les données textuelles dans un environnement OLAP. Lesrésultats préliminaires de notre étude expérimentale montrent l'intérêt de notreapproche.	Lamia Oukid, Ounas Asfari, Fadila Bentayeb, Nadjia Benblidia, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001897	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Gathering Real OLAP Analysis Sessions: A Feedback	The use of OLAP sessions, conducted by professional analysts, seemsto be the best way to assess the relevance of OLAP solutions based on formerqueries (in particular with user-centric approaches, like recommendation or personalizationof queries). However, for scholar research teams, obtaining suchlogs is often difficult. Moreover, the complexity of the queries produced in theselogs can lead to an important treatment of them, denaturing the performed analysis.In this paper, we propose a feedback from real OLAP sessions performedby graduate students in Business Intelligence. This feedback reports the designof questionnaires and the use of an original user interface to easily conduct realOLAP sessions.	Julien Aligon	http://editions-rnti.fr/render_pdf.php?p1&p=1001902	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Information Marketplaces for Big Data Analytics	The analysis of freely available Big Data is an increasing market segment. Currently, multipledata vendors utilize the cloud-computing paradigm for trading data, associated analyticaldata services, and analytic results as a commodity good on information marketplaces. In thefirst part of this talk we present insights from interviews with established vendors about typicalqueries and key challenges with regard to pricing strategies in different market situations.In the second part of the talk I introduce to the technical infrastructure of the MIA marketplace.This large research project will provide an infrastructure which ensures the sustainableoperation of a reliable and trusted platform for the production, provision and use of the data ofthe .DE-Web and other free information. This infrastructure enables completely new businessmodels with information and analysis as electronically tradable goods. The collective storage,analysis and utilization of data from the .DE-Web offers many cost savings and high innovationcapabilities. Thus, especially for small and medium enterprises, significant market entrybarriers and impediments to innovation are eliminated. The talk concludes with interestingresearch problems for the business intelligence, database and text mining communities.	Alexander Löser	http://editions-rnti.fr/render_pdf.php?p1&p=1001889	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Intégration des données champs continus incomplets dans l'OLAP : de la modélisation conceptuelle à l'implémentation	Les champs continus sont des types de représentations utilisés pourmodéliser des phénomènes tels que la température, la pollution ou l'élévation.Différents travaux se sont intéressés à l'intégration de ce type de représentationsdans un système SOLAP. Ainsi, dans ce papier, nous proposons une modélisationainsi qu'une implémentation des dimensions spatiales représentant deschamps continus incomplets sous forme de grilles régulières à des résolutionsdifférentes, à fin de les intégrer dans un système SOLAP.	Mehdi Zaamoune, Sandro Bimonte, François Pinet, Philippe Beaune	http://editions-rnti.fr/render_pdf.php?p1&p=1001893	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Jenga and the Art of Data-Intensive Ecosystems Maintenance	Software maintenance amounts up to 60% of the resources spent on building and operatinga software system. Data-intensive ecosystems that include several software applications tightlycoupled to underlying data repositories cannot escape the above rule. In such environments, theimpact of evolution is three-fold: (a) syntactical (meaning, that the evolution of either the datarepositories or the software that implements the operational processes of the data warehousecan lead to operational failures and crashes due to some form of syntactic incorrectness), (b)semantic (meaning that changes in a view or a data transformation module can lead to differentsemantics for the propagated data), and (c) performance-oriented, as different configurationsof the ecosystem's components (be it data or software) lead to different performance for itsoperations.As in all data-intensive ecosystems, the evolution of the software constructs of a data warehouseenvironment can severely affect its operations. In this talk, we focus on two aspects ofthe management of data warehouse evolution. On the one hand, we are interested in predictingthe maintenance effort of ETL workflows. To this end, we present the findings of a case studyon how a set of graph-theoretic metrics can be used for the prediction of evolution vulnerabilityfor the components of ETL scenarios. On the other hand, we are interested in supportingthe graceful evolution of the ecosystem's components and we present a method for the adaptationof ecosystems that assess the potential impact of a change and rewrite the ecosystem'scomponents in order to adapt to the change.	Panos Vassiliadis	http://editions-rnti.fr/render_pdf.php?p1&p=1001890	
Revue des Nouvelles Technologies de l'Information	EDA	2013	La Fragmentation Horizontale Revisitée: Prise en Compte de l'Interaction de Requêtes	La principale caractéristique des requêtes définies sur un entrepôt dedonnées relationnel (EDR) est le fait que leurs jointures passent systématiquementpar la table des faits. Cette situation favorise l'interaction entre les requêtes.Cette interaction a été largement exploitée par les algorithmes d'optimisation derequêtes dans les bases de données traditionnelles, connue sous le nomd'optimisationmulti-requêtes. Dans les EDR, cette interaction a été utilisée pour définirdes méthodes de sélection des vues matérialisées. Dans cet article, nous revisitonsle problème de sélection de schéma de fragmentation horizontale. Aprèsun état de l'art, où nous soulignons le fait que les algorithmes existants ignorentl'interaction entre les requêtes, nous proposons un nouvel algorithme basé surcette interaction. Sa principale caractéristique est l'utilisation d'une structure dedonnées incrémentale considérant l'interaction. Il utilise le principe d'électiondes requêtes pour aiguiller le processus de fragmentation et propager le gain aumieux sur l'ensemble des requêtes de la charge. Finalement, une étude expérimentaleest conduite pour prouver l'efficacité de notre approche.	Amira Kerkad, Ladjel Bellatreche, Dominique Geniet	http://editions-rnti.fr/render_pdf.php?p1&p=1001900	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Modèles de Coût pour la Sélection de Vues Matérialisées dans le Nuage, Application aux Services Amazon EC2 et S3	La performance des entrepôts de données est classiquement assuréegrâce à des structures comme les index ou les vues matérialisées. Dans ce contexte,des modèles de coût permettent de sélectionner un ensemble pertinent dece type de structures. Toutefois, cette sélection devient plus complexe dans lesnuages informatiques, car en plus des temps de réponse, il faut simultanémentoptimiser le coût monétaire. Nous proposons dans cet article de nouveaux modèlesde coût intégrant le paiement à la demande en vigueur dans les nuages. Surla base de ces modèles, nous définissons un problème d'optimisation consistantà sélectionner, parmi des vues candidates, celles à matérialiser pour minimiser lecoût d'interrogation et de maintenance, ainsi que le temps de réponse pour unecharge de requêtes donnée. Dans un premier temps, nous optimisons les deuxcritères séparément : le temps est optimisé sous contrainte de coût et vice versa.Notre proposition est ensuite validée de manière expérimentale.	Romain Perriot, Jérémy Pfeifer, Laurent d&#146;Orazio, Bruno Bachelet, Sandro Bimonte, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1001895	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Omniscience dans la Conception des Entrepôts de Données Parallèles sur un Cluster	Généralement, le processus de conception d'un entrepôt de donnéesparallèle passe principalement par deux étapes : (1) la fragmentation des donnéeset (2) l'allocation des fragments générés sur les différents noeuds de traitement.Le principal inconvénient d'une telle approche de conception est le coûtélevé de communication pour équilibrer la charge entre les noeuds de traitement,ainsi le noeud coordinateur peut devenir un goulot d'étranglement dans le système.Pour remédier à ces problèmes, la réplication de données (RD) est utilisée.Fréquemment, la fragmentation des données, l'allocation des fragments etla réplication de données sont effectuées de manière isolée. En effet, l'interactionentre ces phases est ignorée. Dans cet article, nous proposons une nouvelleapproche de conception d'un entrepôt de données parallèle qui traite conjointementla fragmentation, l'allocation et la réplication. Un algorithme d'allocationredondant basé sur l'algorithme de classification floue "Fuzzy k-means" est proposé.Nous avons également formalisé le problème du traitement parallèle desrequêtes comme un Dual Bin Packing, un algorithme glouton est proposé pourla résolution du problème. Enfin, une validation de nos propositions en utilisantle banc d'essai "Star Schema Benchmark" (SSB) est proposée.	Soumia Benkrid, Ladjel Bellatreche, Alfredo Cuzzocrea	http://editions-rnti.fr/render_pdf.php?p1&p=1001894	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Opérateurs OLAP dans les bases de données multidimensionnelles multifonctions	Notre modèle conceptuel des bases de données multidimensionnellespermet d'associer à chaque mesure plusieurs fonctions d'agrégation en fonctiondes dimensions, des hiérarchies et des niveaux de granularité. Cet article étudieles impacts de ce modèle sur la table multidimensionnelle (TM) et l'algèbreOLAP. A cause des caractéristiques de ce modèle, il y a des changements quise produisent sur l'algorithme interne de certains opérateurs OLAP. Les changementspeuvent affecter la TM même. Ils peuvent nécessiter d'adapter la TM àprésenter plusieurs fonctions d'agrégation.	Ali Hassan, Franck Ravat, Olivier Teste, Ronan Tournier, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001896	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Problèmes d'additivité dus à la présence de hiérarchies complexes dans les modèles multidimensionnels : définitions, solutions et travaux futurs	De nos jours, les entrepôts de données et les outils d'analyse OLAPsont très utilisés dans les entreprises qui ont besoin de systèmes décisionnelsqui s'adaptent à toutes les situations particulières du monde réel, pour éviterles erreurs d'analyse (plus connues dans la littérature sous le nom de problèmesd'additivité ou summarizability issues en Anglais). Dans cet article, nous présentonsun état de l'art des travaux sur les problèmes d'additivité dus à la présencede hiérarchies complexes dans les modèles multidimensionnels et discutons destravaux restant à mener dans ce domaine.	Marouane Hachicha, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1001891	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Schéma multidimensionnel dédié pour l'OLAP des Tweets	Les tweets permettent l'échange de faits et d'opinions entre les utilisateursdu réseau social "Twitter". Le nombre de tweets échangés ne cesse d'augmenteret constitue ainsi une nouvelle source importante d'informations. L'applicationdes techniques OLAP "On-Line analytical Processing" sur ces gros volumesde tweets permet d'extraire de nouvelles informations et/ou connaissancesconcernant par exemple le comportement des usagers ou les sujets émergents.Cet article propose un schéma multidimensionnel générique dédié à l'OLAP desdonnées dynamiques (tweets).	Maha Ben Kraiem, Kaïs Khrouf, Jamel Feki, Franck Ravat, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1001899	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Sharing-based Privacy and Availability of Cloud DataWarehouses	Cloud computing can help reduce costs, increase business agility anddeploy applications with a high return on investment such as data warehouses.However, storing and managing data in the cloud may not be fully trustworthy.In this article, we focus on both data security (data privacy, availability andintegrity) and data analysis in the cloud. To solve the data security issue, we proposea new (m, n, t) multi secret sharing scheme based on block cryptography,secret sharing and hash functions. Moreover, we apply this solution onto a clouddata warehouse such that data security and data analysis are addressed. An extensivesecurity and performance analysis shows that the proposed schemes canprevent most attacks, guarantee data availability and integrity, and allow analyzingdata at low costs (data storage, data transfer and time computation) in thepay-as-you-go economic model in the cloud.	Varunya Attasena, Nouria Harbi, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1001892	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Talking to Your Inner Liberal Arts Major	One of the key challenges of Business Intelligence is to offer business users the opportunityof asking business questions in an intuitive manner, and receiving answers they can understandand trust. The problem is not about computing the right answers for a given query: it's tointerpret--and if necessary guide--the user's actions in order to deduce their intent and makesure we answer the question they intended to ask in the first place. As engineers, scientists anddevelopers, we tend to adopt authoring metaphors which we find intellectually elegant but willpuzzle most users. Subqueries, for instance, are simply out of reach of most users, and canbe replaced by incremental construction of query elements. Moreover, the lack of semanticdepth in many tools -- Excel to start with -- can encourage users to perform mathematicallyexpressible yet generally meaningless computations, such as an average of percentages. Moregenerally, the subtlety of business questions is largely ignored, and BI tools tend to make apriori, undocumented decisions about the intended semantics of business questions--leadingnot to wrong, but to misleading answers.Through the exploration of a number of real-life examples, this presentation aims at identifyinghow we can at last adapt our query metaphors to a general audience.	Yannick Cras	http://editions-rnti.fr/render_pdf.php?p1&p=1001888	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Vers l'intégration multidimensionnelle d'Open Data dans les entrepôts de données	L'émergence de nombreuses sources d'Open Data poussent plusieurscommunautés de recherche ainsi que des entreprises à développer des outils permettantleur exploitation. En particulier, les données statistiques présentes dansles Open Data peuvent constituer des informations utiles aux analyses décisionnelles.Toutefois les Open Data très hétérogènes et disséminés en plusieurs morceauxde données sur le web, rendent difficile leur intégration au sein d'un entrepôtde données. Les travaux actuels sur l'intégration des Open Data proposentdes processus d'intégration basés sur des Linked Open Data, dont la mise enplace n'est pas automatisée. Dans cet article, nous proposons un processus visantà automatiser l'entreposage multidimensionnel des Open Data. Notre démarcherepose sur la transformation des Open Data en un graphe générique etenrichi favorisant leur intégration. Ce graphe sert de support pour la définitionsemi-automatique et incrémentale du schéma multidimensionnel d'entreposage.	Alain Berro, Imen Megdiche, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1001898	
Revue des Nouvelles Technologies de l'Information	EDA	2013	Votre Plan d'Exécution de Requêtes est un Circuit Intégré : Changer de Métier	Dans la première génération des bases de données, les optimiseursétaient conçus pour optimiser des requêtes individuelles. Après identificationdes interactions entre les requêtes, des optimiseurs étaient proposés pour offrirune optimisation multiple. La difficulté de cette optimisation est l'identificationdes expressions communes entre les requêtes. Ce problème est connu commeNP-difficileSellis et Ghosh (1990). Pour résoudre ce problème, des solutions baséessur la fusion des arbres algébriques des requêtes ont été proposées. Ellessouffrent du problème de passage à l'échelle. Dans cet article, nous proposonsl'utilisation de la théorie de graphes fortement utilisée dans le domaine de laconception des circuits intégrés pour la résolution de ce problème. Premièrement,nous définissons l'analogie entre notre problème et celui de la conceptiondes circuits électroniques. Des algorithmes issus de la théorie des graphes sontensuite proposés. Finalement, une évaluation de nos propositions est effectuée àl'aide de l'outil HMETIS.	Ahcène Boukorca, Ladjel Bellatreche, Sid-Ahmed Benali Senouci, Zoé Faget	http://editions-rnti.fr/render_pdf.php?p1&p=1001901	
Revue des Nouvelles Technologies de l'Information	EGC	2013	20 ans de découverte de motifs : une étude bibliographique quantitative	Depuis deux décennies, la découverte de motifs a été l'un des champs de recherche les plus actifs de l'exploration de données. Cet article en établit une étude bibliographique quantitative en nous appuyant sur 1030 publications issues de 5 conférences internationales majeures : KDD, PKDD, PAKDD, ICDM et SDM. Nous avons d'abord mesuré depuis 2005 un sévère ralentissement de l'activité de recherche dédiée à la découverte de motifs. Puis, nous avons quantifié les principales contributions en terme de langages, de contraintes et de représentations condensées de sorte à comprendre ce ralentissement et à esquisser les directions actuelles.	Arnaud Giacometti, Dominique Haoyuan Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001830	
Revue des Nouvelles Technologies de l'Information	EGC	2013	3D : de nouvelles perspectives en fouille exploratoire avec la stéréoscopie	Si la 3D est un sujet de débat dans la communauté, les expériences sur lesquelles s'appuient les discussions concernent le plus souvent des restitutions visuelles basées sur une projection classique en perspective linéaire. L'objectif de cette communication est de renouveler le cadre expérimental en étudiant l'impact de l'ajout de la disparité binoculaire. Nous nous focalisons ici sur une tâche importante en analyse de réseaux : l'identification de communautés. Et nous comparons la 3D monoscopique et la 3D stéréoscopique à la fois pour la performance de résolution de la tâche et pour le comportement exploratoire à travers l'analyse du mouvement du pointeur de la souris et de la dynamique des modifications de points de vue sur les graphes. Nos résultats expérimentaux mettent en évidence des performances significativement meilleures pour la 3D stéréoscopique et des différences comportementales dans l'exploration avec un centrage plus important sur des zones restreintes en stéréoscopie.	Nicolas Greffard, Fabien Picarougne, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1001831	
Revue des Nouvelles Technologies de l'Information	EGC	2013	A POS Tagger analysed in collaboration environments and literary texts	Part-of-speech (POS) tagging is often used in other modules of natural language processing and therefore the results of this process should be as precise as possible. Many different types of taggers have been developed to improve the accuracy of the results in the field of literature or newspapers. Nowadays when the internet is widespread, the environments for online collaboration as chats, forums, blogs, wikis have become important means of communication. The purpose of this research is to analyse the results of tagging the words obtained from the labelling of the words from the online collaboration environments and literary texts with the corresponding parts of speech. In the case of POS tagging, the ambiguities arise due to the fact that a word may have multiple morphological values depending on context.	Dumitru-Clementin Cercel, Stefan Trausan-Matu	http://editions-rnti.fr/render_pdf.php?p1&p=1001847	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Accélération de la méthode des K plus proches voisins pour la catégorisation de textes	Parmi la panoplie de classificateurs utilisés dans la catégorisation de textes, nous nous intéressons à l'algorithme des k-voisins les plus proches. Ces performances le situent parmi les meilleures méthodes de catégorisation de textes. Toutefois, il présente certaines limites: (i) coût mémoire car il faut stocker l'ensemble d'apprentissage en entier et (ii) coût élevé de calcul car il doit explorer l'ensemble d'apprentissage pour classer un nouveau document. Dans ce papier, nous proposons une nouvelle démarche pour réduire ce temps de classification sans dégrader les performances de classification.	Fatiha Barigou, Baghdad Atmani, Youcef Bouziane, Naouel Barigou	http://editions-rnti.fr/render_pdf.php?p1&p=1001841	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse conceptuelle de données de simulation de systèmes complexes pour l'aide à la décision : Application à la conception d'une cabine d'avion	Dans cet article nous présentons une approche conceptuelle d'aide à la décision dans la conception de systèmes complexes. Cette approche s'appuie sur le formalisme de l'analyse de concepts formels par similarité (ACFS) pour la classification, la visualisation et l'exploration de données de simulation afin d'aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents. L'approche est illustrée sur un cas test de conception de cabine d'un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin d'identifier celles qui assurent un confort convenable pour les passagers la cabine. La classification des données de simulation avec leurs scores de confort en utilisant l'ACFS permet d'identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers. Les résultats obtenus ont été confirmés et validés par de nouvelles simulations.	Nizar Messai, Cassio Melo, Mohamed Hamdaoui, Dung Bui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001835	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse de réseaux sociaux par l'analyse formelle de concepts	L'analyse formelle de concepts (AFC) est un formalisme de représentation et d'extraction de connaissance fondé sur les notions de concepts et de treillis de concepts (Galois).L'AFC a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel, les bases et entrepôts de données, l'extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine, la psychologie, la linguistique et la sociologie.Dans cette présentation, nous allons explorer le potentiel de l'AFC et de quelques extensions de cette théorie (ex. analyse triadique de concepts) dans l'analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex. détection de communautés et d'individus influents à partir d'un réseau d'amis) ou même de réseaux hétérogènes (ex. extraction de règles d'association d'un réseau bibliographique).	Rokia Missaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001814	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse des réclamations d'allocataires de la CAF : un cas d'étude en fouille de données	La gestion des réclamations est un élément fondamental dans la relation client. C'est le cas en particulier pour la Caisse Nationale des Allocations Familiales qui veut mettre en place une politique nationale pour faciliter cette gestion. Dans cet article, nous décrivons la démarche que nous avons adoptée afin de traiter automatiquement les réclamations provenant d'allocataires de la CAF du Rhône. Les données brutes mises à notre disposition nécessitent une série importante de prétraitements pour les rendre utilisables. Une fois ces données correctement nettoyées, des techniques issues de l'analyse des données et de l'apprentissage non supervisé nous permettent d'extraire à la fois une typologie des réclamations basée sur leur contenu textuel mais aussi une typologie des allocataires réclamants. Après avoir présenté ces deux typologies, nous les mettons en correspondance afin de voir comment les allocataires se distribuent selon les différents types de réclamation.	Sabine Loudcher, Julien Velcin, Vincent Forissier, Cyril Broilliard, Philippe Simonnot	http://editions-rnti.fr/render_pdf.php?p1&p=1001866	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse Relationnelle de Concepts pour l'exploration de données relationnelles	L'Analyse Relationnelle de Concepts (ARC) est une extension de l'Analyse Formelle de Concepts (AFC), une méthode de classification non supervisée d'objets sous forme de treillis de concepts. L'ARC supporte en plus la gestion de relations entre objets de différents contextes ce qui permet d'établir des liens entre les concepts des différents treillis. Cette particularité lui permet d'être plus intuitive à utiliser pour extraire des connaissances à partir de données relationnelles et de donner des résultats plus riches. Malheureusement lorsque les jeux de données présentent de nombreuses relations, les résultats obtenus sont difficilement exploitables et des problèmes de passages à l'échelle se posent. Nous proposons dans cet article une adaptation possible de l'ARC pour explorer les relations de manière supervisée pour augmenter la pertinence des résultats obtenus et réduire le temps de calcul. Nous prenons pour exemple des données hydrobiologiques ayant trait à la qualité des milieux aquatiques.	Xavier Dolques, Florence Le Ber, Marianne Huchard, Clémentine Nebut	http://editions-rnti.fr/render_pdf.php?p1&p=1001829	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Approche orientée objet sémantique et coopérative pour la classification des images de zones urbaines à très haute résolution	La classification orientée objet (COO) prend de plus en plus de dimension dans les travaux de télédétection grâce à sa capacité d'intégrer des connaissances de haut niveau telles que la taille, la forme et les informations de voisinage. Cependant, les approches existantes restent tributaires de l'étape de construction des objets à cause de l'absence d'interaction entre celle-ci et celle de leur identification. Dans cet article, nous proposons une approche sémantique, hiérarchique et collaborative entre les algorithmes de croissances de régions et une classification orientée objet supervisée, permettant une coopération entre l'extraction et l'identification des objets de l'image. Les expériences menées sur une image de très haute résolution de la région de Strasbourg ont confirmé l'intérêt de l'approche introduite.	Aymen Sellaouti, Atef Hamouda, Aline Deruyver, Cédric Wemmert	http://editions-rnti.fr/render_pdf.php?p1&p=1001827	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Classification multi-étiquettes pour l'alignement multiple de séquences protéiques	Cet article présente une application de classification multi-étiquettes permettant de déterminer le programme à utiliser pour construire un alignement multiple d'un ensemble de séquences protéiques donné. Dans un premier temps, nous avons réussi à améliorer le système existant, Alexsys en ajoutant des attributs. Dans un second temps, nous déterminons pour un ensemble de séquences protéiques donné le ou les aligneurs capable de produire les alignements de meilleur score, à epsilon près. Les mesures de performances propres à la classification multi-étiquette nous permettent d'analyser l'influence de epsilon et de choisir une valeur assez petite pour distinguer les meilleurs aligneurs des autres.	Lina Fahed, Gabriel Frey, Julie Dawn Thompson, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1001864	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Classifications croisées de données de trajectoires contraintes par un réseau routier	Le clustering (ou classification non supervisée) de trajectoires a fait l'objet d'un nombre considérable de travaux de recherche. La majorité de ces travaux s'est intéressée au cas où les objets mobiles engendrant ces trajectoires se déplacent librement dans un espace euclidien et ne prennent pas en compte les contraintes liées à la structure sous-jacente du réseau qu'ils parcourent (ex. réseau routier). Dans le présent article, nous proposons au contraire la prise en compte explicite de ces contraintes. Nous représenterons les relations entre trajectoires et segments routiers par un graphe biparti et nous étudierons la classification de ses sommets. Nous illustrerons, sur un jeu de données synthétiques, l'utilité d'une telle étude pour comprendre la dynamique du mouvement dans le réseau routier et analyser le comportement des véhicules qui l'empruntent.	Mohamed K. El Mahrsi, Romain Guigourès, Fabrice Rossi, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001854	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Comprendre et interpréter les données : enjeux et implantations d'un système de codage dans des gisements de données historiques	L'accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd'hui deux grands types de difficultés aux historiens.Le premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante. C'est par exemple le cas pour l'intégration d'un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au Lamop, ou même dans le cadre d'un projet dont le seul lien est une problématique spatiale et temporelle (projet ANR-DFG, Euroscientia).Le deuxième tient en la nature des données introduites dans ces différents systèmes : elles sont souvent hétérogènes, ambiguës, floues. Pour que le chercheur puisse se les approprier, les données doivent faire l'objet d'un véritable travail, afin de comprendre comment elles ont été obtenues, structurées. L'historien doit donc les évaluer et les valider s'il souhaite les mettre en relation. Cette évaluation nécessitant, elle-même de pouvoir être commentée, partagée et critiquée par d'autres chercheurs.Dans les deux cas, il est nécessaire de développer des outils d'appropriation, qui permettent d'entrer dans le réel historique contenu dans les stocks de données. C'est là la fonction du projet Histobase, un système permettant d'entrer dans la structuration des gisements, d'en évaluer l'information, d'ajouter des couches d'interprétation (qualification de l'information historique) de les évaluer et de partager les données « obtenues ». Chacune des analyses individuelles et collectives fait l'objet d'une mémorisation. Il faut pour cela laisser une place importante aux historiens en tant qu'expert en prêtant une attention particulière aux processus métiers qu'ils mettent en oeuvre.	Stéphane Lamassé, Julien Alerini	http://editions-rnti.fr/render_pdf.php?p1&p=1001816	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Construction de descripteurs à partir du coclustering pour la classification supervisée de séries temporelles	Nous présentons un processus de construction de descripteurs pour la classification supervisée de séries temporelles. Ce processus est libre de tout paramétrage utilisateur et se décompose en trois étapes : (i) à partir des données originales, nous générons de multiples nouvelles représentations simples ; (ii) sur chacune de ces représentations, nous appliquons un algorithme de coclustering ; (iii) à partir des résultats de co-clustering, nous construisons de nouveaux descripteurs pour les séries temporelles. Nous obtenons une nouvelle base de données objets-attributs dont les objets (identifiant les séries temporelles) sont décrits par des attributs issus des diverses représentations générées. Nous utilisons un classifieur Bayésien sur cette nouvelle base de données. Nous montrons expérimentalement que ce processus offre de très bonnes performances prédictives comparées à l'état de l'art.	Dominique Gay, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001855	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Découverte des soft-skypatterns avec une approche PPC	Les skypatterns sont des motifs traduisant des préférences de l'utilisateur selon une relation de dominance. Dans cet article, nous introduisons la notion de souplesse dans la problématique des skypatterns et nous montrons comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement. Nous proposons une méthode efficace d'extraction de skypatterns ainsi que de soft-skypatterns, méthode fondée sur la programmation par contraintes. La pertinence de notre approche est illustrée à travers une étude de cas en chémoinformatique pour la découverte de toxicophores.	Willy Ugarte, Patrice Boizumault, Samir Loudni, Bruno Crémilleux, Alban Lepailleur	http://editions-rnti.fr/render_pdf.php?p1&p=1001838	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Detecting Academic Plagiarism with Graphs	In this paper, we tackle the problem of detecting academic plagiarism, which is considered as a severe problem owing to the convenience of online publishing. Typical information retrieval methods, stopword-based methods and ngerprinting methods, are commonly used to detect plagiarism by using the sequence of words as they appear in the article. As such, they fail to detect plagiarism when an author reconstructs a source article by re-ordering and recombining phrases. Because graph structure ts for representing relationships between entities, we propose a novel plagiarism detection method, in which we use graphs to represent documents by modeling grammatical relationships between words. Experimental results show that our proposed method outperforms two n-gram methods and increases recall values by 10 to 20%.	Bin-Hui Chou, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001848	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Détection efficace des traverses minimales d'un hypergraphe par élimination de la redondance	L'extraction des traverses minimales d'un hypergraphe est une problématique réputée comme particulièrement difficile et qui a fait l'objet de plusieurs travaux dans la littérature. Dans cet article, nous établissons un lien entre les concepts de la fouille de données et ceux de la théorie des hypergraphes, proposant ainsi un cadre méthodologique pour le calcul des traverses minimales. Le nombre de ces traverses minimales étant, souvent, exponentiel même pour des hypergraphes simples, nous proposons d'en représenter l'ensemble de manière concise et exacte. Pour ce faire, nous introduisons la notion de traverses minimales irrédondantes, à partir desquelles nous pouvons retrouver l'ensemble global de toutes les traverses minimales, à l'aide de l'algorithme IMT-EXTRACTOR. Une étude expérimentale de ce nouvel algorithme a confirmé l'intérêt de l'approche introduite.	Mohamed Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001833	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Détection précoce de tendances produits dans le cadre des activités commerciales de la grande distribution	Dans ce papier, nous présentons une nouvelle approche qui permet la détection précoce de tendances "produits" dans le cadre des activités commerciales de la grande distribution. S'agissant d'un domaine où la concurrence est très vive entre les différentes enseignes avec des enjeux financiers colossaux, les stratégies commerciales ont pour principal objectif de fidéliser la clientèle pour limiter leur défection. C'est là qu'intervient la détection des changements de tendances produits, qui va permettre d'anticiper l'attrition de la clientèle. Déceler des tendances suffisamment tôt permettra aux décideurs de mettre en place des stratégies préventives efficaces à moindre coût. Notre objectif est donc d'analyser et de modéliser clairement les changements de tendances et leurs impacts potentiels globaux sur les achats des clients. Nous illustrerons notre approche sur des données réelles d'achats de clients d'une grande enseigne.	Gaël Bardury, Jean-Emile Symphor	http://editions-rnti.fr/render_pdf.php?p1&p=1001840	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Enrichissement d'ontologies grâce à l'annotation sémantique de pages web	Nous présentons une approche pour enrichir automatiquement une ontologie à partir d'un ensemble de pages web structurées. Cette approche s'appuie sur un noyau d'ontologie initial. Son originalité est d'exploiter conjointement la structure des documents et des annotations sémantiques produites à l'aide du noyau d'ontologie pour identifier de nouveaux concepts et des spécialisations de relations qui enrichissent l'ontologie. Nous avons implémenté et évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage.	Nathalie Aussenac-Gilles, Davide Buscaldi, Catherine Comparot, Mouna Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1001839	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Étude des corrélations spatio-temporelles des appels mobiles en France	Nous proposons dans cet article de présenter une application d'analyse d'une base de données de grande taille issue du secteur des télécommunications. Le problème consiste à segmenter un territoire et caractériser les zones ainsi définies grâce au comportement des habitants en terme de téléphonie mobile. Nous disposons pour cela d'un réseau d'appels inter-antennes construit pendant une période de cinq mois sur l'ensemble de la France. Nous proposons une analyse en deux phases. La première couple les antennes émettrices dont les appels sont similairement distribués sur les antennes réceptrices et vice versa. Une projection de ces groupes d'antennes sur une carte de France permet une visualisation des corrélations entre la géographie du territoire et le comportement de ses habitants en terme de téléphonie. La seconde phase découpe l'année en périodes entre lesquelles on observe un changement de distributions d'appels sortant des groupes d'antennes. On peut ainsi caractériser l'évolution temporelle du comportement des usagers de mobiles dans chacune des zones du pays.	Romain Guigourès, Marc Boullé, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1001865	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Étude des techniques d'oubli dans les moindres carrés récursifs pour l'apprentissage incrémental de systèmes d'inférence floue évolutifs : application à la reconnaissance de formes	Cet article étudie les possibilités d'utilisation d'oubli dans l'apprentissage incrémental en-ligne de classifieurs évolutifs basés sur des systèmes d'inférence floue. Pour cela, nous étudions différentes possibilités, existant dans la littérature dédiée au contrôle, pour introduire de l'oubli dans l'algorithme des moindres carrés récursifs. Nous présentons l'impact de ces différentes techniques dans le contexte de l'apprentissage incrémental de classifieurs évolutifs en environnement non stationnaire. Ces approches sont évaluées, pour l'optimisation des systèmes d'inférence floue, sur la problématique de la reconnaissance de gestes manuscrits sur surface tactile.	Manuel Bouillon, Eric Anquetil, Abdullah Almaksour	http://editions-rnti.fr/render_pdf.php?p1&p=1001818	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Évolution d'une ontologie dédiée à la représentation de relations n-aires	Nous nous intéressons dans cet article à la problématique d'évolution d'une ontologie permettant de représenter des relations n-aires. Nous présentons la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence structurelle. Nous illustrerons nos propos sur une ontologie dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives.	Rim Touhami, Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu	http://editions-rnti.fr/render_pdf.php?p1&p=1001862	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction de motifs condensés dans un unique graphe orienté acyclique attribué	Les graphes orientés acycliques attribués peuvent être utilisés dans beaucoup de domaines applicatif. Dans ce papier, nous étudions un nouveau domaine de motif pour permettre leur analyse : les chemins pondérés fréquents. Nous proposons en conséquence des contraintes primitives permettant d'évaluer leur pertinence (par exemple, les contraintes de fréquence et de compacité), et un algorithme extrayant ces solutions. Nous aboutissons à une représentation condensée dont l'efficacité et le passage à l'échelle sont étudiés empiriquement.	Jérémy Sanhes, Frédéric Flouvat, Claude Pasquier, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001837	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction de motifs fréquents dans des arbres attribués	L'extraction de motifs fréquents est une tâche importante en fouille de données. Initialement centrés sur la découverte d'ensembles d'items fréquents, les premiers travaux ont été étendus pour extraire des motifs structurels comme des séquences, des arbres ou des graphes. Dans cet article, nous proposons une nouvelle méthode de fouille de données qui consiste à extraire de nouveaux types de motifs à partir d'une collection d'arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des ensembles d'attributs. L'extraction de ces motifs (appelés sous-arbres attribués) combine une recherche d'ensembles d'items fréquents à une recherche de sous-arbres et nécessite d'explorer un immense espace de recherche. Nous présentons plusieurs nouveaux algorithmes d'extraction d'arbres attribués et montrons que leurs implémentations peuvent efficacement extraire des motifs fréquents à partir de grands jeux de données.	Claude Pasquier, Jérémy Sanhes, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001836	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction des nombres de Betti avec un modèle génératif	L'analyse exploratoire de données multidimensionnelles est un problème complexe. Nous proposons d'extraire certains invariants topologiques appelés nombre de Betti, pour synthétiser la topologie de la structure sous-jacente aux données. Nous définissons un modèle génératif basé sur le complexe simplicial de Delaunay dont nous estimons les paramètres par l'optimisation du critère d'information Bayésien (BIC). Ce Complexe Simplicial Génératif nous permet d'extraire les nombres de Betti de données jouets et d'images d'objets en rotation. Comparé à la technique géométrique des Witness Complex, le CSG apparait plus robuste aux données bruitées.	Maxime Maillot, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001826	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction et filtrage de syntagmes nominaux pour la Recherche d'Information	Nous proposons dans cet article un Système de Recherche d'Information (SRI) qui se base sur des techniques d'indexation de textes en langue naturelle. Nous présentons une méthode d'indexation de documents qui repose sur une approche hybride pour la sélection de descripteurs textuels. Cette approche emploie des traitements du langage naturel pour l'extraction des syntagmes nominaux et sur un filtrage statistique basé sur l'information mutuelle pour sélectionner les syntagmes nominaux les plus informatifs pour le processus d'indexation. Nous effectuons des expérimentations en utilisant le corpus Le Monde 94 de la collection CLEF 2001 et sur le SRI Lemur pour évaluer l'approche proposée.	Chedi Bechikh Ali, Hatem Haddad	http://editions-rnti.fr/render_pdf.php?p1&p=1001842	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction optimisée de Règles d'Association Positives et Négatives (RAPN)	La littérature s'est beaucoup intéressée à l'extraction de règles d'association positives et peu à l'extraction de règles négatives en raison essentiellement du coût de calculs et du nombre prohibitif de règles extraites qui sont pour la plupart redondantes et inintéressantes. Dans cet article, nous nous sommes intéressés aux algorithmes d'extraction de RAPN (Règles d'Association Positives et Négatives) reposant sur l'algorithme fondateur Apriori. Nous avons fait une étude de ceux-ci en mettant en évidence leurs avantages et leurs inconvénients. A l'issue de cette étude, nous avons proposé un nouvel algorithme qui améliore cette extraction au niveau du nombre et de la qualité des règles extraites et au niveau du parcours de recherche des règles. L'étude s'est terminée par une évaluation de cet algorithme sur plusieurs bases de données.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001832	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Grille bivariée pour la détection de changement dans un flux étiqueté	Nous présentons une méthode en-ligne de détection de changement de concept dans un flux étiqueté. Notre méthode de détection est basée sur un critère supervisé bivarié qui permet d'identifier si les données de deux fenêtres proviennent ou non de la même distribution. Notre méthode a l'intérêt de n'avoir aucun a priori sur la distribution des données, ni sur le type de changement et est capable de détecter des changements de différentes natures (changement dans la moyenne, dans la variance...). Les expérimentations montrent que notre méthode est plus performante et robuste que les méthodes de l'état de l'art testées. De plus, à part la taille des fenêtres, elle ne requiert aucun paramètre utilisateur.	Christophe Salperwyck, Marc Boullé, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001859	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Identification de compatibilités entre descripteurs de lieux et apprentissage automatique	Les travaux présentés dans cet article s'inscrivent dans le paradigme des recherches visant à acquérir des relations sémantiques à partir de folksonomies (ensemble de tags attribués à des ressources par des utilisateurs). Nous expérimentons plusieurs approches issues de l'état de l'art ainsi que l'apport de l'apprentissage automatique pour l'identification de relations entre tags. Nous obtenons dans le meilleur des cas un taux d'erreur de 23,7 % (relations non reconnues ou fausses), ce qui est encourageant au vu de la difficulté de la tâche (les annotateurs humains ont un taux de désaccord de 12%).	Estelle Delpech, Laurent Candillier, Léa Laporte, Samuel Phan	http://editions-rnti.fr/render_pdf.php?p1&p=1001850	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Identification de complexes protéine-protéine par combinaison de classifieurs. Application à Escherichia Coli	Nous proposons une approche permettant de prédire des complexes impliquant trois protéines (appelés trimères) à partir de combinaison de classifieurs appris sur des complexes n'impliquant que deux protéines (dimères). La prédiction de ces trimères repose sur deux hypothèses biologiques : (i) deux protéines orthologues présentent des caractéristiques fonctionnelles similaires; (ii) deux protéines interagissant sous la forme d'un complexe sous-tendent une fonction biologique essentielle à l'espèce concernée. Ces deux hypothèses sont exploitées pour décrire chaque paire de protéines par l'ensemble des espèces pour lesquelles elles possèdent un orthologue. Un ensemble de mesures de qualité classiquement utilisées pour évaluer l'intérêt des règles d'association est utilisé pour évaluer la force du lien entre les deux protéines. L'organisme modèle Escherichia Coli a été utilisé pour évaluer notre approche.	Thomas Bourquard, Damien M. de Vienne, Jérôme Azé	http://editions-rnti.fr/render_pdf.php?p1&p=1001863	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Inférence de réseaux biologiques : un défi pour la fouille de données structurées	La réponse cellulaire d'un organisme vivant à un signal donné, hormone, stress ou médicament, met en jeu des mécanismes complexes d'interaction et de régulation entre les gènes, les ARN messagers, les protéines et d'autres éléments tels que les micro-ARNs. On parle de réseau d'interaction pour décrire l'ensemble des interactions possibles entre protéines et de réseau de régulation génique pour représenter un ensemble de régulations entre gènes. Identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet d'envisager de mieux soigner par le biais du ciblage thérapeutique. Puisque les techniques expérimentales de mesure à grande échelle, récemment développées, fournissent des données d'observation de ces réseaux, ce problème d'identification de réseau, généralement appelé inférence de réseau en biologie des systèmes, s'inscrit dans le cadre général de la fouille de données et plus particulièrement de l'apprentissage artificiel. Voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes d'étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.En partant des deux problématiques distinctes que sont l'inférence de réseau d'interaction et l'inférence de réseau de régulation, je montrerai que ces deux tâches d'apprentissage posent, chacune de manière différente, la problématique de la prédiction de sorties structurées. L'inférence de réseau d'interaction entre protéines, vue comme un problème transductif de prédiction de liens, peut être résolue comme un problème d'apprentissage d'un noyau de sortie à partir d'un noyau d'entrée. L'inférence de réseau de régulation, impliquant la modélisation d'un système dynamique, peut être abordée par l'approximation parcimonieuse et structurée de fonctions à valeurs vectorielles. Je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de Hilbert, fondés sur des noyaux à valeur opérateur, qui fournissent d'excellents résultats en inférence de réseaux biologiques. Des expériences in silico sur des données artificielles, chez la levure du boulanger ou chez l'homme illustreront mes propos. En fin d'exposé, je tracerai quelques perspectives concernant les " nouveaux " défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées.	Florence D'Alche-Buc	http://editions-rnti.fr/render_pdf.php?p1&p=1001815	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Les capitalistes sociaux sur Twitter : détection via des mesures de similarité	Les réseaux sociaux tels que Twitter font partie du phénomène de Déluge des données, expression utilisée pour décrire l'apparition de données de plus en plus volumineuses et complexes. Pour représenter ces réseaux, des graphes orientés sont souvent utilisés. Dans cet article, nous nous focalisons sur deux aspects de l'analyse du réseau social de Twitter. En premier lieu, notre but est de trouver une méthode efficace et haut niveau pour stocker et manipuler le graphe du réseau social en utilisant des ressources informatiques raisonnables. Cet axe de recherche constitue un enjeu majeur puisqu'il est ainsi possible de traiter des graphes à échelle réelle sur des machines potentiellement accessibles par tous. Ensuite, nous étudions les capitalistes sociaux, un type particulier d'utilisateurs de Twitter observé par Ghosh et al. (2012). Nous proposons une méthode pour détecter et classifier efficacement ces utilisateurs.	Nicolas Dugué, Anthony Perez	http://editions-rnti.fr/render_pdf.php?p1&p=1001852	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Modèle de Recherche d'Information Sociale Centré Utilisateur	L'émergence des réseaux sociaux a révolutionné leWeb en permettant notamment aux individus de prolonger leur connexion virtuelle en une relation plus réelle et de partager leurs connaissances. Ce nouveau contexte de diffusion de l'information sur le Web peut constituer un moyen efficace pour cerner les besoins en information des utilisateurs du Web, et permettre à la recherche d'information (RI) de mieux répondre à ces besoins en adaptant les modèles d'indexation et d'interrogation. L'exploitation des réseaux sociaux confronte la RI à plusieurs défis dont les plus importants concernent la représentation de l'information dans ce modèle social de RI et son évaluation, en l'absence de collections de test et de compétitions dédiées. Dans cet article, nous présentons un modèle de RI sociale dans lequel nous proposons de modéliser et d'exploiter le contexte social de l'utilisateur. Nous avons évalué notre modèle à l'aide d'une collection de test de RI sociale construite à partir des annotations du réseau social de bookmarking collaboratif Delicious.	Chahrazed Bouhini, Mathias Géry, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001846	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Non-disjoint grouping of text documents based Word Sequence Kernel	This paper deals with two issues in text clustering which are the detection of non disjoint groups and the representation of textual data. In fact, a text document can discuss several themes and then, it must belong to several groups. The learning algorithm must be able to produce non disjoint clusters and assigns documents to several clusters. The second issue concerns the data representation. Textual data are often represented as a bag of features such as terms, phrases or concepts. This representation of text avoids correlation between terms and doesn't give importance to the order of words in the text. We propose a non supervised learning method able to detect overlapping groups in text document by considering text as a sequence of words and using the Word Sequence Kernel as similarity measure. The experiments show that the proposed method outperforms existing overlapping methods using the bag of word representation in terms of clustering accuracy and detect more relevant groups in textual documents.	Chiheb-Eddine Ben N'Cir, Afef Zenned, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001843	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Nouvelle approche de bi-partitionnement topologique	Dans ce papier, nous proposons une nouvelle approche topologique de bi-partitionnement (bi-clustering) appelée BiTM en utilisant les cartes autoorganisatrices. L'idée principale de l'approche est d'utiliser une seule carte pour le partitionnement simultané des lignes (observations) et des colonnes (variables). Contrairement aux approches utilisant les cartes topologiques, notre modèle ne nécessite pas de pré-traitement de la base de données. Ainsi, une nouvelle fonction de coût est proposée. De plus, BiTM fournit une visualisation topologique des blocs ou bi-clusters facilement interprétable. Les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique.	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001820	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Paramétrage intelligent de l'alignement d'ontologies par l'intégrale de Choquet	Le nombre croissant d'ontologies rend le processus d'alignement une composante essentielle du Web sémantique. Plusieurs outils ont été conçus dans le but de produire des alignements. La qualité des alignements fournis par ces outils est étroitement liée à certains paramètres qui régissent leurs traitements. Dans ce papier, nous proposons une nouvelle approche permettant l'adaptation automatique des paramètres d'alignement d'ontologies par l'utilisation de l'intégrale de Choquet, comme un opérateur d'agrégation. Les expérimentations montrent une nette amélioration des résultats par rapport à un paramétrage statique et figé.	Marouen Kachroudi, Sami Zghal, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001857	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Processus itératif d'extraction de classes en non supervisée	Nous proposons dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif. L'approche utilise une méthode d'extraction de classes basée sur la détection de limite de classe, chaque classe étant définie par son centre. Nous avons également défini des critères d'évaluation adaptés à la méthode proposée. Plusieurs expérimentations ont montré l'intérêt de l'approche dans divers problèmes.	Alexandre Blansché, Lydia Boudjeloud	http://editions-rnti.fr/render_pdf.php?p1&p=1001817	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Ré-écriture de requêtes dans un système d'intégration sémantique	Nous décrivons la deuxième phase de réalisation d'un système d'intégration qui minimise l'intervention humaine habituellement nécessaire. Après la phase de construction semi-automatique du schéma (ontologie) global décrite dans de précédents articles, nous présentons ici le processus de ré-écriture de requêtes globales en des requêtes adressées aux sources.	Cheikh Niang, Béatrice Bouchou, Moussa Lo, Yacine Sam	http://editions-rnti.fr/render_pdf.php?p1&p=1001858	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Recherche de documents similaires sur le web par segmentations hiérarchiques et extraction de mots-clés	La recherche de documents similaires est un processus qui consiste à trouver les documents présentant des similitudes, comme la copie ou la reformulation, sur des bases documentaires ou sur internet. Elle est utilisée notamment pour protéger la propriété intellectuelle de productions issues de l'enseignement, de la recherche ou de l'industrie. Dans cet article, nous définissons une approche automatique pour permettant d'extraire des mots-clés d'un document en effectuant un bouclage sur une succession de découpage de plus en plus petit. Cette approche permet d'obtenir des mots-clés impossibles à obtenir par une approche globale notamment quand la thématique, le style ou le contenu d'un document varient dans le document. L'objectif est de permettre la détection des documents présentant des similitudes en utilisant uniquement des mots-clés.	Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1001860	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Réutiliser les connaissances d'expert pour assister l'analyse de l'activité sur simulateur pleine échelle de conduite de centrale nucléaire - Approche à base de M-Trace	Notre travail porte sur l'aide à l'observation de l'activité dans les simulateurs pleine échelle de centrale nucléaire pour assister les formateurs pendant les simulations. Notre approche consiste à représenter l'activité sous la forme de trace modélisée et à les transformer afin d'extraire et de visualiser des informations de haut niveau permettant aux formateurs de mieux retracer et analyser les simulations. Afin de valider notre approche, nous avons conçu le prototype D3KODE que nous avons évalué avec des experts formateurs d'EDF.	Olivier Champalle, Karim Sehaba	http://editions-rnti.fr/render_pdf.php?p1&p=1001828	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Sélection de variables non supervisée sous contraintes hiérarchiques	La sélection des variables a un rôle très important dans la fouille de données lorsqu'un grand nombre de variables est disponible. Ainsi, certaines variables peuvent être peu significatives, corrélées ou non pertinentes. Une méthode de sélection a pour objectif de mesurer la pertinence d'un ensemble utilisant principalement un critère d'évaluation. Nous présentons dans cet article un critère non supervisé permettant de mesurer la pertinence d'un sous-ensemble de variables. Ce dernier repose sur l'utilisation du score Laplacien auquel nous avons ajouté des contraintes hiérarchiques. Travailler dans le cadre non supervisé est un vrai challenge dans ce domaine dû à l'absence des étiquettes de classes. Les résultats obtenus sur plusieurs bases de tests sont très encourageants et prometteurs.	Nhat-Quang Doan, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001823	
Revue des Nouvelles Technologies de l'Information	EGC	2013	SNOW, un algorithme exploratoire pour le subspace clustering	Cet article propose un nouvel algorithme pour le problème de subspace clustering dénommé SNOW. Contrairement aux approches descendantes classiques, il ne repose pas sur l'hypothèse de localité et permet l'affectation d'une donnée à plusieurs clusters dans des sous-espaces différents. Les expérimentations préliminaires montrent que notre approche obtient de meilleurs résultats que l'algorithme COPAC sur une base de référence et a été appliquée sur une base de données réelles.	Sylvain Dormieu, Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1001868	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Technique de factorisation multi-biais pour des recommandations dynamiques	La factorisation de matrices offre une grande qualité de prédiction pour les systèmes de recommandation. Mais sa nature statique empêche de tenir compte des nouvelles notes que les utilisateurs produisent en continu. Ainsi, la qualité des prédictions décroît entre deux factorisations lorsque de nombreuses notes ne sont pas prises en compte. La quantité de notes écartées est d'autant plus grande que la période entre deux factorisation est longue, ce qui accentue la baisse de qualité.Nos travaux visent à améliorer la qualité des recommandations. Nous proposons une factorisation de matrices utilisant des groupes de produits et intégrant en ligne les nouvelles notes des utilisateurs. Nous attribuons à chaque utilisateur un biais pour chaque groupe de produits similaires que nous mettons à jour. Ainsi, nous améliorons significativement les prédictions entre deux factorisations. Nos expérimentations sur des jeux de données réels montrent l'efficacité de notre approche.	Modou Gueye, Talel Abdesssalem, Hubert Naacke	http://editions-rnti.fr/render_pdf.php?p1&p=1001856	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Text2Geo : des données textuelles aux informations géospatiales	Dans cet article, nous nous intéressons aux méthodes d'extraction d'informations spatiales dans des documents textuels. Nous présentons la méthode hybride Text2Geo qui combine une approche d'extraction d'informations, fondée sur des patrons avec une approche de classification supervisée permettant d'explorer le contexte associé. Nous discutons des résultats expérimentaux obtenus sur le jeu de données de l'étang de Thau.	Sabiha Tahrat, Eric Kergosien, Sandra Bringay, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001861	
Revue des Nouvelles Technologies de l'Information	EGC	2013	ToTeM: une méthode de détection de communautés adaptées aux réseaux d'information	Alors que les réseaux sociaux s'attachaient à représenter des entités et les relations qui existaient entre elles, les réseaux d'information intègrent également des attributs décrivant ces entités ; ce qui conduit à revisiter les méthodes d'analyse et de fouille de ces réseaux. Dans cet article, nous proposons une méthode de classification des sommets d'un graphe qui exploite d'une part leurs relations et d'autre part les attributs les caractérisant. Cette méthode reprend le principe de la méthode de Louvain en l'étendant de façon à permettre la manipulation d'attributs continus d'une manière symétrique à ce qui existe pour les relations.	David Combe, Christine Largeron, Elod Egyed-Zsigmond, Mathias Géry	http://editions-rnti.fr/render_pdf.php?p1&p=1001849	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Towards a New Science of Big Data Analytics, based on the Geometry and the Topology of Complex, Hierarchic Systems	My work is concerned with pattern recognition, knowledge discovery, computer learning and statistics. I address how geometry and topology can uncover and empower the semantics of data. In addition to the semantics of data that can be explored using Correspondence Analysis and related multivariate data analyses, hierarchy is a fundamental concept in this work. I address not only low dimensional projection for display purposes, but carry out search and pattern recognition, whenever useful, in very high dimensional spaces. High dimensional spaces present very different characteristics from low dimensions, I have shown that in a particular sense very high dimensional space becomes, as dimensionality increases, hierarchical. I have also shown how in hierarchy, and hence in an ultrametric topological mapping of information space, we track change or anomaly or rupture.In this presentation, the first theme discussed is that of linear time hierarchical clustering with application to sky survey data in astronomy, and to chemo-informatics. The second theme discussed is computational text analysis. It is interesting to note that J.P. Benzécri's original motivation was in language and linguistics. In my text analysis work, I have taken the dictum of McKee (Story : Substance, Structure, Style and the Principles of Screenwriting, Methuen, 1999) that "text is the sensory surface of a work of art" and show just how this insight can be rendered in computational terms. This leads to demarcating, tracking, statistical modelling, visualizing, and pattern recognition of narrative. In an application to collaborative writing, I developed an interactive framework for critiquing, and assessing fit and appropriateness of content, on the basis of semantics, leading to books that were published as e-books, having been written by school children in a few days of collaborative class work. In many aspects of this work, hierarchy expresses both continuity and change in the textual narrative or in the narrative of chronological events.	Fionn Murtagh	http://editions-rnti.fr/render_pdf.php?p1&p=1001813	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Un Critère d'évaluation pour la construction de variables à base d'itemsets pour l'apprentissage supervisé multi-tables	Dans le contexte de la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement liés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. Dans cet article, nous proposons un Framework basé sur des itemsets pour la construction de variables à partir des tables secondaires. L'informativité de ces nouvelles variables est évaluée dans le cadre de la classification supervisée au moyen d'un critère régularisé qui vise à éviter le surapprentissage. Pour ce faire, nous introduisons un espace de modèles basés sur des itemsets dans la table secondaire ainsi qu'une estimation de la densité conditionnelle des variables construites correspondantes. Une distribution a priori est définie sur cet espace de modèles, pour obtenir ainsi un critère sans paramètres permettant d'évaluer la pertinence des variables construites. Des expérimentations préliminaires montrent la pertinence de l'approche.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1001824	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Un système hybride de recherche d'information intégrant le raisonnement à partir de cas et la composition d'ontologies	La croissance des informations disponibles sur le web nécessite des outils de recherche de plus en plus performants permettant de répondre efficacement aux besoins des utilisateurs. Dans ce contexte, l'utilisation des ontologies présente des atouts importants. Cependant, la construction manuelle d'ontologies est très coûteuse, ceci a poussé à proposer des approches permettant d'automatiser cette construction. Cet article présente un système de recherche d'information hybride basée sur le Raisonnement à Partir de Cas (RàPC) et la composition d'ontologies. Ce système vise à combiner la construction automatique d'ontologies modulaires et le RàPC, qui a pour but d'améliorer les résultats de recherche d'information (RI). Des expérimentations ont été menées et les résultats obtenus montrent une amélioration de la précision dans le cas d'une recherche d'information sur le Web.	Ghada Besbes, Hajer Baazaoui-Zghal, Henda Ben Ghezela	http://editions-rnti.fr/render_pdf.php?p1&p=1001845	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Une approche en programmation par contraintes pour la classification non supervisée	Dans cet article, nous abordons le problème de classification non supervisée sous contraintes fondé sur la programmation par contraintes (PPC). Nous considérons comme critère d'optimisation la minimisation du diamètre maximal des clusters. Nous proposons un modèle pour cette tâche en PPC et nous montrons aussi l'importance des stratégies de recherche pour améliorer son efficacité. Notre modèle basé sur la distance entre les objets permet de traiter des données qualitatives et quantitatives. Des contraintes supplémentaires sur les clusters et les instances peuvent directement être ajoutées. Des expériences sur des ensembles de données classiques montrent l'intérêt de notre approche.	Thi-Bich-Hanh Dao, Khanh-Chuong Duong, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001822	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Une nouvelle mesure pour l'évaluation des méthodes d'extraction de thématiques : la Vraisemblance Généralisée	Les méthodes dédiées à l'extraction automatique de thématiques sont issues de domaines variés : linguistique computationnelle, TAL, algèbre linéaire, statistique, etc. A ces méthodes spécifiques, peuvent s'ajouter des méthodes adaptées d'autres domaines, notamment de l'apprentissage automatique non supervisé. Les résultats produits par l'ensemble de ces méthodes prennent des formes hétérogènes : partitions de documents, distributions de probabilités sur les mots, matrices. Cela pose clairement un problème pour les comparer de manière uniforme. Dans cet article, nous proposons une nouvelle mesure de qualité, intitulée Vraisemblance Généralisée, pour permettre une évaluation et ainsi la comparaison de différentes méthodes d'extraction de thématiques. Les résultats, obtenus sur un corpus de documents Web autour des élections présidentielles françaises de 2012, ainsi que sur le corpus Associated Press, montrent la pertinence de la mesure proposée.	Mohamed Dermouche, Julien Velcin, Sabine Loudcher, Leila Khouas	http://editions-rnti.fr/render_pdf.php?p1&p=1001851	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Unsupervised Video Tag Correction System	We present a new system for video auto tagging which aims at correcting and completing the tags provided by users for videos uploaded on the Internet. Unlike most existing systems, we do not learn any tag classifiers or use the questionable textual information to compare our videos. We propose to compare directly the visual content of the videos described by different sets of features such as Bag-of-visual-Words or frequent patterns built from them. Then, we propagate tags between visually similar videos according to the frequency of these tags in a given video neighborhood. We also propose a controlled experimental set up to evaluate such a system. Experiments show that with suitable features, we are able to correct a reasonable amount of tags in Web videos.	Hoang-Tung Tran, Elisa Fromont, François Jacquenet, Baptiste Jeudy, Adrien Martins	http://editions-rnti.fr/render_pdf.php?p1&p=1001867	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Validation d'une carte cognitive	Les cartes cognitives sont un modèle graphique représentant des influences entre des concepts. Malgré le fait qu'une carte cognitive soit relativement simple à construire, certaines influences peuvent se contredire l'une l'autre. Cet article propose différents critères pour valider une carte cognitive, c'est-àdire indiquer si la carte contient ou non des contradictions. Nous distinguons deux types de critères : les critères de vérification qui valident une carte cognitive en déterminant sa cohérence interne et les critères de test qui valident une carte à partir d'un ensemble de contraintes choisies par le concepteur.	Aymeric Le Dorze, Laurent Garcia, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001825	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers un cadre évolutif de classification non supervisée	La classification non supervisée (clustering) évolutive surpasse généralement par celle statique en produisant des groupes de données (clusters) qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme. Dans ce travail, nous présentons un cadre différent pour le clustering évolutif d'une manière incrémentale par un suivi précis des variables de proximité temporelles entre les objets suivis par un clustering statique ordinaire.	Mohamed Charouel, Minyar Sassi-Hidri, Mohamed Ali Zoghlami	http://editions-rnti.fr/render_pdf.php?p1&p=1001821	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une architecture multicouche d'ontologies dédiée à la résolution mixte de problèmes	Dans cet article, nous nous intéressons à la gestion d'expériences générées au sein des processus de résolution mixte (individuelle et/ou collective) de problèmes afin d'assister la capitalisation et le partage des connaissances dans les environnements collaboratifs. Dans ce contexte, nous proposons un cadre ontologique générique par rapport au domaine dédié à la modélisation formelle et consensuelle de ces expériences en adoptant une architecture multicouche basée sur quatre strates. La première strate est basée sur la spécialisation d'ontologies fondationnelles. La deuxième strate est basée sur la conception de trois patrons conceptuels ontologiques (PCO) noyaux (le PCO organisationnel, le PCO téléologique et le PCO argumentatif modélisant respectivement les acteurs, le problème et les solutions proposées). La troisième strate est basée sur la spécialisation des PCO noyaux dans un domaine particulier et la dernière strate est basée sur l'instanciation du modèle ontologique de domaine pour la représentation d'une situation du monde réel.	Nesrine Ben Yahia, Narjès Bellamine Ben Saoud, Henda Hajjami Ben Ghezala	http://editions-rnti.fr/render_pdf.php?p1&p=1001844	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une Automatisation de la Construction de Variables pour la Classification Supervisée	Dans cet article, nous proposons un cadre visant à automatiser la construction de variables pour l'apprentissage supervisé, en particulier dans le cadre multi-tables. La connaissance du domaine est spécifiée d'une part en structurant les données en variables, tables et liens entre tables, d'autre part en choisissant des règles de construction de variables. L'espace de construction de variables ainsi défini est potentiellement infini, ce qui pose des problèmes d'exploration combinatoire et de sur-apprentissage. Nous introduisons une distribution de probabilité a priori sur l'espace des variables constructibles, ainsi qu'un algorithme performant de tirage d'échantillons dans cette distribution. Des expérimentations intensives montrent que l'approche est robuste et performante.	Marc Boullé, Dhafer Lahbib	http://editions-rnti.fr/render_pdf.php?p1&p=1001819	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une mesure de similarité pour les séquences complexes	Le calcul de similarité entre les séquences est d'une extrême importance dans de nombreuses approches d'explorations de données. Il existe une multitude de mesures de similarités de séquences dans la littérature. Or, la plupart de ces mesures sont conçues pour des séquences simples, dites séquences d'items. Dans ce travail, nous étudions d'un point de vue purement combinatoire le problème de similarité entre des séquences complexes (i.e., des séquences d'ensembles ou itemsets). Nous présentons de nouveaux résultats afin de compter efficacement toutes les sous-séquences communes à deux séquences. Ces résultats théoriques sont la base d'une mesure de similarité calculée efficacement grâce à une approche de programmation dynamique.	Elias Egho, Chedy Raïssi, Toon Calders, Thomas Bourquard, Nicolas Jay, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001853	
Revue des Nouvelles Technologies de l'Information	EGC	2013	Visualisation radiale : approche parallèle entre CPU et GPU	Dans cet article, nous proposons une parallélisation sur CPU et GPU d'une méthode de visualisation radiale à base de points d'intérêt. Nous montrons que cette approche peut visualiser avec des temps très courts des millions de données sur des dizaines de dimensions, et nous étudions l'efficacité de la parallélisation dans différentes configurations.	Tianyang Liu, Fatma Bouali, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001834	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	A hybrid recommender system to predict online job offer performance	With the expansion of internet to advertise, the number of potentialchannels is increasing every day. In the Human Resource domain, recruiters haveto choose between hundreds of job search web sites when they post a job offeron the internet. In order to save costs, assessing job board expected performancehas become necessary. In this paper, three recommender systems providing jobboard performance estimation for a given job posting are introduced. This workrefers principally to the new item problem, which is still a challenging topic inthe literature. The first system (PLS-R) is a content-based approach, while othersare hybrid recommendation approaches. Estimation is made on item neighborhoodaccording to a "naive" similarity or a supervised similarity measure. Thesepredictive algorithms are compared through experiments on a real dataset. In thisapplication, supervised similarity-based system overcomes the lacks of other approachesand outperforms them.	Julie Séguéla, Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1001887	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Adaptive Dynamic Clustering Algorithm for Interval-valued Data based on Squared-Wasserstein Distance	Wide applications of interval-valued data in various domains havetriggered the call for more powerful analytical tools. In light of this, this paperhas presented an adaptive dynamic clustering algorithm for interval-valued data,using squared-Wasserstein distance. Experiments on both synthetic data and realdata have unveiled the merits of the proposed algorithm.	Rong Guan, Yves Lechevallier, Huiwen Wang	http://editions-rnti.fr/render_pdf.php?p1&p=1001877	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Analysis of m sets of symbolic interval variables.	This work presents a new approach to analyze a series of m n  ptables X(1); : : : ;X(m) of symbolic interval variables. In this new approach, wefirstly define a space of intervals with laws of composition ; 1; . Thisallows extending this reasoning to the matrices of intervals. Then, we define an  p compromise matrix X = &#1048576;Xiji=1;:::;n; j=1;:::;p; of type intervals, a measureof covariance between interval variables, a new measure of correlation between interval variables and the product operator 2 between a matrix n  pof intervals and one p vector u. This way, we achieve a symbolic PCA of compromise.To express the variability of tables X(1); : : : ;X(m), they are projectedon the principal axes of PCA of intervals of compromise. For the interpretationof factorial map, a new measure of correlation  will be used.	Sun Makosso-Kallyth	http://editions-rnti.fr/render_pdf.php?p1&p=1001882	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	General overview of methods of analysis of multi-group datasets	Methods of analysis of a dataset where the individuals are partitionedinto groups are discussed. These methods encompass known strategies of analysisand a new method called dual generalized Procrustes analysis. The emphasisis put on how the methods used in the context of multi-block data analysis canbe adapted to the present context of multi-group setting. The similarities andthe differences between the various approaches of analysis are highlighted andillustrated on the basis of three datasets.	Aida Eslami, Achim Kohler, El Mostafa Qannari, Stéphanie Bougeard	http://editions-rnti.fr/render_pdf.php?p1&p=1001883	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Graph Aggregation : Application to Social Networks	In the enterprise context, people need to exploit and mainly visualizedifferent types of interactions between heterogeneous objects. Graph modelseems to be the most appropriate way to represent those interactions. However,the extracted graphs have in general a huge size which makes it difficult to analyzeand visualize. An aggregation step is needed to have more understandablegraphs in order to allow users discovering underlying information and hiddenrelationships between entities. In this work, we propose new measures to evaluatethe quality of summaries based on an existing algorithm named k-SNAP thatproduces a summarized graph according to user-selected node attributes and relationships.	Amine Louati, Marie-Aude Aufaure, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001886	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Hierarchical Mixed Topological Maps	We address the problem of clustering individuals described with severalmixed variables divided in homogeneous blocks. We propose a hierarchicalmethod with two levels to partition the individuals. The method is based on twosuccessive steps using mixed topological maps combined with agglomerative hierarchicalclustering. The proposed approach allows to take into account simultaneouslyqualitative and quantitative variables as well as the variable blocking.A real example on indoor air quality illustrates the proposed method.	Ndeye Niang, Mory Ouattara	http://editions-rnti.fr/render_pdf.php?p1&p=1001884	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Multiple Linear Regression for Histogram Data using Least Squares of Quantile Functions: a Two-components model.	Histograms are commonly used for representing summaries of observeddata and they can be considered non parametric estimates of probabilitydistributions. Symbolic Data Analysis formalized the concept of histogramsymbolic variable, as a variable which allows to describe statistical units by histogramsinstead of single values. In this paper we present a linear regressionmodel for multivariate histogram variables. We use a Least Square estimationmethod where the sum of squared errors is defined according to the `2 Wassersteinmetric between the observed and the predicted histogram data. Consistentlywith the l2 Wasserstein metric, we solve the Least Square computational problemby introducing a suitable inner product between two vectors of histogramdata. Finally, measures of goodness of fit are discussed and an application onreal data shows some interpretative advantages of the proposed method.	Rosanna Verde, Antonio Irpino	http://editions-rnti.fr/render_pdf.php?p1&p=1001881	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Normalizing Constrained Symbolic Data for Clustering	Clustering is one of the most common operation in data analysiswhile constrained is not so common. We present here a clustering method in theframework of Symbolic Data Analysis (S.D.A) which allows to cluster SymbolicData. Such data can be constrained relations between the variables, expressed byrules which express the domain knowledge. But such rules can induce a combinatorialincrease of the computation time according to the number of rules. Wepresent in this paper a way to cluster such data in a quadratic time. This methodis based first on the decomposition of the data according to the rules, then wecan apply to the data a clustering algorithm based on dissimilarities.	Marc Csernel, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001880	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Principal Component Analysis of Functional Data based on Constant Numerical Characteristics	A new approach to principal component analysis (PCA) is proposedfor functional data. In prevailing methods of functional principal componentanalysis (FPCA), the definition of a mean is in the form of a function. However,data centralisation based on this kind of mean actually obtains a residualfunction. The result of FPCA, given its matrix of residual functions, may thusfail to present the essential variation of the original data. Besides, applications inFPCA are mainly for types of one sample problems. Numerical characteristics offunctional data are defined as real constants. Centralisation in terms of constantnumerical characteristics implies the relocation of the entire matrix of functionalvariances in order to obtain original curves whose centres of gravity are settledon the origin. Furthermore, based on the covariance matrix obtained from constantnumerical characteristics, functional principal components for multivariatesample problems are proposed. Conclusions are validated by simulation in a realsituation.	Meiling Chen, Huiwen Wang	http://editions-rnti.fr/render_pdf.php?p1&p=1001885	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	Symbolic Principal Components for Interval-Valued Data	The centers method (Cazes et al., 1997, Chouakria, 1998) was thefirst principal component analysis for interval-valued data (where it is implicitlyassumed that values within an interval are uniformly distributed across thatinterval). Many other methods have since been proposed. All fail in variousways to capture fully all the information contained in the data. Here, we setthese in context against a new method which calculates the covariance matrixexactly. This new method also includes a new visualization of the projection ofthe observations onto the principal component space.	Lynne Billard, J. Le-Rademacher	http://editions-rnti.fr/render_pdf.php?p1&p=1001878	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	The style characteristic of China's stock market: an application to PCA for interval symbolic data	By applying the symbolic principal component analysis (SPCA) onthe empirical data of the CITIC style indices in six years (2005-2010), we stud-ied the characteristics of Chinese stock market from multiple perspectives. Twocomponents are extracted from five variables-P/E ratio, NMC, turnover rate,return rate, and volatility-and are defined as the market performance factor andthe size factor. Further, drawing the run track of the six stock style portfolios andcombining with the zoom-star plots of symbolic data, we find that the Chinesestock market is excessive speculated and bounded rational.	Dingmu Cao, Wen Long	http://editions-rnti.fr/render_pdf.php?p1&p=1001879	
Revue des Nouvelles Technologies de l'Information	HDSDA	2013	The symbolic data analysis paradigm, discriminant discretization and financial application	The variability inside classes of individuals, categories (defined by acategorical variable) or concepts (defined by an intent and an extent, like speciesfor example), is expressed by the use of intervals, histograms, distributions, sequencesof weighted values and the like. In this way we obtain new kinds of datacalled "symbolic". The aim of "Symbolic Data Analysis" (SDA) is to study andextract new knowledge from these new kinds of data by an extension of Statisticsand Data Mining to symbolic data.We show that SDA is a new paradigm openedto a vast field of research and applications. Then, we give a way for obtainingdiscriminate symbolic descriptions by an original discretisation method, whichis illustrated by a financial application.	Edwin Diday, Filipe Afonso, Raja Haddad	http://editions-rnti.fr/render_pdf.php?p1&p=1001876	
Revue des Nouvelles Technologies de l'Information	SM	2013	A Passive Interoperability Testing Approach Applied to the Constrained Application Protocol (CoAP)	Constrained Application Protocol (CoAP) is an application protocol designed for the Internet of Things, where smart devices cooperate to provide machine-to-machine Web services. In this context, a high level of interoperability is crucial. This paper addresses the interoperability testing of CoAP applications. It proposes a methodology based on passive testing, which is a technique to test a running system by only observing its behavior without introducing any test input. The methodology (the proposed method and a corresponding testing tool) was put into operation during the CoAP interoperability testing event (Plugtest) organized by ETSI in Paris in March 2012, where a number of CoAP applications were successfully tested, showing the validity and efficiency of this approach.	Nanxing Chen, César Viho	http://editions-rnti.fr/render_pdf.php?p1&p=1001869	
Revue des Nouvelles Technologies de l'Information	SM	2013	Architecture Multi Domiciliée dans les Réseaux Mobiles : Diminution de l'Impact de la Mobilité sur les Protocoles de Transport	Les routeurs mobiles fournissent une connexion fiable avec une mobilitétransparente à tous les noeuds connectés à leur réseau. Les protocoles deTransport sont affectés par cette transparence car l'état du réseau d'accès changesans avertissement, rendant l'évaluation du réseau difficile. Notre architecturepropose d'informer les protocoles de Transport lorsqu'un changement de réseauest effectué afin de diminuer l'impact sur les communications. Cette solution estbasée sur la définition de plusieurs interfaces réseaux au niveau des noeuds duréseau mobile et l'utilisation du protocole de Transport multi domicilié SCTP.	Lionel Bertaux, Pascal Berthou, Thierry Gayraud	http://editions-rnti.fr/render_pdf.php?p1&p=1001871	
Revue des Nouvelles Technologies de l'Information	SM	2013	PIGA-Cloud : une protection obligatoire des environnements d'informatique en nuage	La garantie de propriétés de sécurité nécessite la mise en place d'uncontrôle d'accès obligatoire (Mandatory Access Control). Les Clouds supportentmal ces mécanismes sans offrir une protection pour tous les niveaux (hôte, invité,application, réseau). PIGA-Cloud garantit des propriétés avancées pour des fluxindirects et des combinaisons variées de flux et protège en profondeur en contrôlantles flux entre les systèmes d'exploitation invités et l'hôte, les flux internes àun invité mais aussi les flux entre objets Java et les flux réseau. L'article montrecomment PIGAiser des environnements aussi divers que des machines Unix, desapplications Java et des Clouds. Il étend les politiques d'accès directs SELinuxet sVirt à une machine virtuelle Java pour au final protéger de façon avancée desClouds de type IaaS, PaaS ou SaaS. L'approche simplifie l'administration despolitiques directes en empêchant les millions de vulnérabilités résiduelles. Cetravail est partiellement supporté par le projet européen Seed4C.	Zaina Afoulki, Aline Bousquet, Jérémy Briffaut, Laurent Clévy, Jonathan Rouzaud-Cornabas, Christian Toinard, Benjamin Venelle	http://editions-rnti.fr/render_pdf.php?p1&p=1001872	
Revue des Nouvelles Technologies de l'Information	SM	2013	Un algorithme distribué de contrôle des feux de circulation sur plusieurs intersections par un réseau de capteurs sans fil	Dans cet article, nous étudions le scénario consistant à utiliser un réseaude capteurs sans fil afin de contrôler les feux de circulation d'un réseau detransport urbain et d'optimiser son fonctionnement en réduisant le temps moyend'attente des usagers. Le faible coût des capteurs autorisant un nombre de pointsde mesure important, il est possible de résoudre les problèmes de congestionlocalement, sans passer par un processus de décision centralisé généralementcoûteux. Nous nous intéressons ici à un réseau couvrant plusieurs intersectionset nous proposons un algorithme distribué permettant aux capteurs de coopéreret d'adapter constamment la politique de feux aux conditions de trafic. Nossimulations montrent que notre algorithme, appliqué à plusieurs intersections,permet d'améliorer considérablement le temps moyen d'attente des usagers ens'adaptant mieux aux variations de trafic, en comparaison à des méthodes plustraditionnelles.	Sébastien Faye, Claude Chaudet, Isabelle Demeure	http://editions-rnti.fr/render_pdf.php?p1&p=1001875	
Revue des Nouvelles Technologies de l'Information	SM	2013	Un système de prédiction spectrale pour une connectivité permanente dans le cadre du suivi en temps réel de patients	Le suivi régulier des signes vitaux garantit un traitement préventifdes pathologies courantes chez une personne, lui assurant ainsi unmeilleur état de santé. La majorité des solutions proposées dans ce contexte,repose sur un ensemble de capteurs sans fil hétérogènes équipantle patient et son environnement. L'urgence des transmissions de donnéesmédicales générées par ces capteurs, appelle à garantir une connectivitépermanente à moindre coût pour les noeuds relais. Pour répondre à cetteproblématique, nous avons besoin de définir une architecture de communicationrobuste, capable d'exploiter différentes technologies et standards,permettant aux équipements (noeuds) de disposer de bandes de fréquencesgarantissant les transmissions. La radio cognitive, proposant une occupationopportuniste par détection des bandes libres, bien que propice, resteassujettie aux contraintes de mobilité du patient et aux changements defréquence induits. La solution que nous proposons à cet effet, est un modèlede prédiction de l'état du canal à sonder. Le modèle associe les techniquesd'apprentissage artificiel au système Grey Model afin d'allier faiblecoût algorithmique et célérité dont l'objectif réside en l'assurance d'uneconnectivité permanente, indispensable au suivi en temps réel de patients.	Dramane Ouattara, Francine Krief, Mohamed Aymen Chalouf, Omessaad Hamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1001874	
Revue des Nouvelles Technologies de l'Information	SM	2013	Virtualisation distribuée de réseaux dynamiques et mobiles avec NEmu	L'expérimentation est généralement la dernière phase dans l'élaborationd'une application réseau avant sa diffusion. Malheureusement, il est assezdifficile, voir impossible, de posséder une infrastructure physique suffisammentpuissante, contrôlée et dynamique de réseau fixe ou mobile afin de tester et validerl'application. La virtualisation est par conséquent une technique fiable etéconome pour jouer le rôle de plate-forme de test. Nous proposons un outil appeléNEmu ayant pour but de générer des réseaux virtuels statiques, dynamiquesou mobiles à la demande afin de tester et de valider des prototypes d'applicationsréseaux avec un contrôle complet de la topologie, de la mobilité des noeuds ainsique des propriétés des liens. NEmu permet la création et la gestion de réseauxvirtuels avec des ressources matérielles limitées et sans aucun droit d'accès particulier.Nous proposons également une illustration de l'utilisation de NEmu afinde tester l'efficacité d'une application de distribution de fichiers reposant sur unarbre de connexions TCP. Nous mesurons ainsi l'impact d'un tel chaînage surles débits et les délais en fonction du nombre de noeuds dans l'arbre, de la tailledes paquets et de la bande passante globale des liens.	Vincent Autefage, Damien Magoni	http://editions-rnti.fr/render_pdf.php?p1&p=1001870	
Revue des Nouvelles Technologies de l'Information	SM	2013	WACA: Politique de répartition de charge des services web dans une architecture de type Cloud	De plus en plus d'applications font appel à des services web déployéssur des architectures de type cloud. Ces services peuvent traiter des donnéesmassives et des fichiers en grandes quantités pour répondre à un nombre croissantde demandes (requêtes) des utilisateurs. Ces données peuvent être de différentstypes allant des données multimédia aux données numériques provenantde capteurs. En plus de la volumétrie des données, les applications web doiventégalement gérer la satisfaction des utilisateurs ce qui impose une garantie de laqualité du service fourni.	Sylvain Lefebvre, Sathya Prabhu Kumar, Raja Chiky	http://editions-rnti.fr/render_pdf.php?p1&p=1001873	
Revue des Nouvelles Technologies de l'Information	EGC	2012	Antipattern Detection inWeb Ontologies: an Experiment using SPARQL Queries	Ontology antipatterns are structures that reflect ontology modelling problems because they lead to inconsistencies, bad reasoning performance or bad formalisation of domain knowledge. We propose four methods for the detection of antipatterns using SPARQL queries. We conduct some experiments to detect antipattern in a corpus of OWL ontologies.	Catherine Roussey, Oscar Corcho, Ond&#711;rej ?váb-Zamazal, François Scharffe, Stephan Bernard	http://editions-rnti.fr/render_pdf.php?p1&p=1001177	http://editions-rnti.fr/render_pdf.php?p=1001177
Revue des Nouvelles Technologies de l'Information	EGC	2012	Apprentissage d'ensemble d'opérateurs de projection orthogonale pour la détection de nouveauté	Dans ce papier, nous proposons une approche de détection de nouveautéfondée sur les opérateurs de projection orthogonale et l'idée de doublebootstrap (bi- bootstrap). Notre approche appelée Random Subspace NoveltyDetection Filter (RS-NDF), combine une technique de rééchantillonnage etl'idée d'apprentissage d'ensemble. RS-NDF est un ensemble de filtres NDF(Novelty Detection Filter), induits à partir d'échantillons bootstrap des donnéesd'apprentissage, en utilisant une sélection aléatoire des variables pour l'apprentissagedes filtres. RS-NDF utilise donc un double bootstrap, c'est à dire unrééchantillonnage avec remise sur les observations et un rééchantillonnage sansremise sur les variables. La prédiction est faite par l'agrégation des prédictionsde l'ensemble des filtres. RS-NDF présente généralement une importante améliorationdes performances par rapport au modèle de base NDF unique. Grâce àson algorithme d'apprentissage en ligne, l'approche RS-NDF est également enmesure de suivre les changements dans les données au fil du temps. Plusieursmétriques de performance montrent que l'approche proposée est plus efficace,robuste et offre de meilleures performances pour la détection de nouveauté comparéeaux autres techniques existantes.	Fatma Hamdi, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001156	http://editions-rnti.fr/render_pdf.php?p=1001156
Revue des Nouvelles Technologies de l'Information	EGC	2012	Apprentissage par analyse linéaire discriminante des paramètres de fusion pour la recherche d'information multimédia texte-image	Avec le développement du numérique, des quantités très importantesde documents composés de texte et d'images sont échangés, ce qui nécessite ledéveloppement demodèles permettant d'exploiter efficacement ces informationsmultimédias. Dans le contexte de la recherche d'information, unmodèle possibleconsiste à représenter séparément les informations textuelles et visuelles et àcombiner linéairement les scores issus de chaque représentation. Cette approchenécessite le paramétrage de poids afin d'équilibrer la contribution de chaquemodalité. Le but de cet article est de présenter une nouvelle méthode permettantd'apprendre ces poids, basée sur l'analyse linéaire discriminante de Fisher(ALD). Des expérimentations réalisées sur la collection ImageCLEF montrentque l'apprentissage des poids grâce à l'ALD est pertinent et que la combinaisondes scores correspondante améliore significativement les résultats par rapport àl'utilisation d'une seule modalité.	Christophe Moulin, Christine Largeron, Cécile Barat, Mathias Géry, Christophe Ducottet	http://editions-rnti.fr/render_pdf.php?p1&p=1001201	http://editions-rnti.fr/render_pdf.php?p=1001201
Revue des Nouvelles Technologies de l'Information	EGC	2012	Biological event extraction using SVM and composite kernel function	With an overwhelming of experimental and computational results inmolecular biology, there is an increasing interest to provide tools that will automaticallyextract structured biological information recorded in freely availabletext. Extraction of named entities such as protein, gene or disease names andof simple relations of these entities, such as statements of protein-protein interactionshas gained certain success, and now the new focus research has beenmoving to higher level of information extraction such as co-reference resolutionand event extraction. It is precisely the last of these tasks which will be focusedin this paper. The biological event template allows detailed representations ofcomplex natural language statements, which is specified by a trigger and argumentslabeled by semantic roles.In this paper, we have developed a biological event extraction approach whichuses Support Vector Machines (SVM) and a suitable composite kernel functionto identify triggers and to assign the corresponding arguments. Also, we makeuse of a number of features based on both syntactic and contextual informationwhich where automatically learned from the training data.We implemented our event extraction system using the state-of-the-art of NLPtools. We achieved competitive results compared to the BioNLP'09 Shared taskbenchmark.	Maha Amami, Aymen Elkhlifi, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001150	http://editions-rnti.fr/render_pdf.php?p=1001150
Revue des Nouvelles Technologies de l'Information	EGC	2012	Caractérisation et extraction de biclusters de valeurs similaires avec l'analyse de concepts triadiques	Le biclustering de données numériques est devenu depuis le début desannées 2000 une tâche importante d'analyse de données, particulièrement pourl'étude de données biologiques d'expression de gènes. Un bicluster représenteune association forte entre un ensemble d'objets et un ensemble d'attributs dansune table de données numériques. Les biclusters de valeurs similaires peuventêtre vus comme des sous-tables maximales de valeurs proches. Seules quelquesméthodes se sont penchées sur une extraction complète (i.e. non heuristique),exacte et non redondante de tels motifs, qui reste toujours un problème difficile,tandis qu'aucun cadre théorique fort ne permet leur caractérisation. Dans le présentarticle, nous introduisons des liens importants avec l'analyse formelle deconcepts. Plus particulièrement, nous montrons de manière originale que l'analysede concepts triadiques (TCA) propose un cadre mathématique intéressant etpuissant pour le biclustering de données numériques. De cette manière, les algorithmesexistants de la TCA, qui s'appliquent habituellement à des données binaires,peuvent être utilisés (directement ou après quelques modifications) aprèsun prétraitement des données pour l'extraction désirée.	Mehdi Kaytoue-Uberall, Sergei O. Kuznetsov, Amedeo Napoli, Juraj Macko, Wagner Meira Jr	http://editions-rnti.fr/render_pdf.php?p1&p=1001166	http://editions-rnti.fr/render_pdf.php?p=1001166
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification Conceptuelle avec Généralisation par Intervalles	Nous nous intéressons aux méthodes de classification hiérarchique oupyramidale, où chaque classe formée correspond à un concept, i.e. une paire (extension,intension), considérant des données décrites par des variables quantitativesà valeurs réelles ou intervalles, ordinales et/ou prenant la forme de distributionde probabilités/fréquences sur un ensemble de catégories. Les concepts sontobtenus par une correspondance de Galois avec généralisation par intervalles, cequi permet de traiter les données de différents types dans un cadre commun. Unemesure de la généralité d'un concept est alors calculée sous une forme communepour les différents types de variables. Un exemple illustre la méthode proposée.	Géraldine Polaillon, Paula Brito	http://editions-rnti.fr/render_pdf.php?p1&p=1001141	http://editions-rnti.fr/render_pdf.php?p=1001141
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification de données EEG par algorithme évolutionnaire pour l'étude d'états de vigilance	L'objectif de ce travail est de prédire l'état de vigilance d'un individuà partir de l'étude de son activité cérébrale (signaux d'électro-encéphalographieEEG). La variable à prédire est binaire (état de vigilance "normal" ou "relaxé").Des EEG de 44 participants dans les deux états (88 enregistrements), ont étérecueillis via un casque à 58 électrodes. Après une étape de prétraitement et devalidation des données, un critère nommé "critère des pentes" a été choisi. Desméthodes de classification supervisée usuelles (k plus proches voisins, arbresbinaires de décision (CART), forêts aléatoires, PLS et sparse PLS discriminante)ont été appliquées afin de fournir des prédictions de l'état des participants. Lecritère utilisé a ensuite été raffiné grâce à un algorithme génétique, ce qui apermis de construire un modèle fiable (taux de bon classement moyen par CARTégal à 86.68 ± 1.87%) et de sélectionner une électrode parmi les 58 initiales.	Laurent Vezard, Pierrick Legrand, Marie Chavent, Frédérique Faïta-Aïnseba, Julien Clauzel	http://editions-rnti.fr/render_pdf.php?p1&p=1001198	http://editions-rnti.fr/render_pdf.php?p=1001198
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification des données catégorielles via la maximisation spectrale de la modularité	Ce papier présente un algorithme spectrale pour maximiser le critèrede la modularité étendu à la classification des données catégorielles. Il met enevidence la connexion formelle entre la maximisation de la modularité et la classificationspectrale, il présente en particulier le problème de maximisation de lamodularité sous forme d'un problème algèbrique de maximisation de la trace.Nous développons ensuite un algorithme efficace pour trouver la partition optimalemaximisant le critère de modularité. Les résultats expérimentaux montrentl'efficacité de notre approche	Lazhar Labiod, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001194	http://editions-rnti.fr/render_pdf.php?p=1001194
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification probabiliste non supervisée et visualisation des données séquentielles	Nous proposons dans ce papier un nouvel algorithme de classificationnon supervisée à base de modèle de mélange topologique pour des donnéesnon i.i.d (non independently and identically distributed). Ce nouveau paradigmeprobabiliste, plonge les cartes topologiques probabilistes dans une formulationsous forme de chaînes de Markov cachées. Dans cette formulation, la générationd'une observation à un instant donné du temps est conditionnée par les étatsvoisins au même instant du temps. Ainsi, une grande proximité impliquera unegrande probabilité pour la contribution à la génération. L'approche proposée estévaluée en utilisant des données séquentielles réelles issues des bases de donnéesde l'Institut Nationale de l'Audiovisuel (INA). Les résultats obtenus sonttrès encourageants et prometteurs.	Rakia Jaziri, Mustapha Lebbah, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001187	http://editions-rnti.fr/render_pdf.php?p=1001187
Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification topologique probabiliste pour des données catégorielles	Cet article présente une carte auto-organisatrice probabiliste pour l'analyseet la classification topologique des données catégorielles. En considérant unmodèle de mélanges parcimonieux nous introduisons une nouvelle carte autoorganisatrice(SOM) probabiliste. L'estimation des paramètres de notre modèleest réalisée à l'aide de l'algorithme EM classique. Contrairement à SOM, l'algorithmed'apprentissage proposé optimise une fonction objective. Ces performancesont été évaluées sur des données réelles et les résultats obtenus sontencourageants et prometteurs à la fois pour la classification et pour la modélisation.	Nicoleta Rogovschi, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1001188	http://editions-rnti.fr/render_pdf.php?p=1001188
Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering de séquences d'activités pour l'étude de procédures neurochirurgicales	L'utilisation de modèles de procédure chirurgicale (Surgical ProcessModel, SPM) a récemment émergé dans le domaine de la conception d'outilsd'intervention chirurgicale assistée par ordinateur. Ces modèles, qui sont utiliséspour analyser et évaluer les interventions, représentent des procédures chirurgicales(Surgical Process, SP) qui sont formalisées comme des structures symboliquesdécrivant une chirurgie à un niveau de granularité donné. Un enjeu importantréside dans la définition de métriques permettant la comparaison et l'évaluationde ces procédures. Ainsi, les relations entre ces métriques et des donnéespré-opératoires permettent de classer les chirurgies pour mettre en lumière desinformations sur la procédure elle-même, mais également sur le comportementdu chirurgien. Dans ce papier, nous étudions la classification automatique d'unensemble de procédures chirurgicales en utilisant l'algorithme Dynamic TimeWarping (DTW) pour calculer une mesure de similarité entre procédures chirurgicales.L'utilisation de DTW permet de se concentrer sur les différents typesd'activité effectués pendant la procédure, ainsi que sur leur séquencement touten réduisant les différences temporelles. Des expériences ont été menées sur 24procédures chirurgicales d'hernie discale lombaire dans le but de discriminer leniveau d'expertise des chirurgiens à partir d'une classification connue. A l'aided'un algorithme de clustering hiérarchique utilisant DTW nous avons retrouvédeux groupes de chirurgiens présentant des niveaux d'expertise différents (junioret senior).	Germain Forestier, Florent Lalys, Laurent Riffaud, Brivael Trelhu, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1001153	http://editions-rnti.fr/render_pdf.php?p=1001153
Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering hiérarchique non paramétrique de données fonctionnelles	Dans cet article, il est question de clustering de courbes. Nous proposonsune méthode non paramétrique qui segmente les courbes en clusters etdiscrétise en intervalles les variables continues décrivant les points de la courbe.Le produit cartésien de ces partitions forme une grille de données qui est inféréeen utilisant une approche Bayésienne de sélection de modèle ne faisant aucunehypothèse concernant les courbes. Enfin, une technique de post-traitement, visantà réduire le nombre de clusters dans le but d'améliorer l'interprétabilitédes clusters, est proposée. Elle consiste à fusionner successivement et de façonoptimale les clusters, ce qui revient à réaliser une classification hiérarchique ascendantedont la mesure de dissimilarité correspond à la variation du critère.De manière intéressante, cette mesure est en fait une somme pondérée de divergencesde Kullback-Leibler entre les distributions des clusters avant et aprèsfusions. L'intérêt de l'approche dans le cadre de l'analyse exploratoire de donnéesfonctionnelles est illustré par un jeu de données artificiel et réel.	Marc Boullé, Romain Guigourès, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1001137	http://editions-rnti.fr/render_pdf.php?p=1001137
Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering multi-niveaux de graphes : hiérarchique et topologique		Nhat-Quang Doan, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001205	http://editions-rnti.fr/render_pdf.php?p=1001205
Revue des Nouvelles Technologies de l'Information	EGC	2012	Combinaison de classificateurs simples pour une sélection rapide de caractéristiques	La sélection de caractéristiques est une technique permettant de choisirles caractéristiques les plus pertinentes, celles adaptées à la résolution d'unproblème particulier. Les méthodes classiques présentent certains inconvénients.Par exemple, elles peuvent être trop complexes, elles peuvent faire dépendreles caractéristiques sélectionnées du classificateur utilisé, elles risquent de sélectionnerdes caractéristiques redondantes. Dans le but de limiter ces inconvénients,nous proposons dans cet article une nouvelle méthode rapide de sélectionde caractéristiques basée sur la construction et la sélection de classificateurssimples associés à chacune des caractéristiques. Une optimisation par unalgorithme génétique est proposée afin de trouver la meilleure combinaison desclassificateurs. Différentes méthodes de combinaison sont considérées et adaptéesà notre problème. Cette méthode a été appliquée sur différents ensemblesde caractéristiques de tailles variées et construite à partir de la base de chiffresmanuscrits MNIST. Les résultats obtenus montrent la robustesse de l'approcheainsi que l'efficacité de la méthode. En moyenne, le nombre de caractéristiquessélectionnées a diminué de 69,9% tout en conservant le taux de reconnaissance.	Hassan Chouaib, Florence Cloppet, Salvatore-Antoine Tabbone, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001196	http://editions-rnti.fr/render_pdf.php?p=1001196
Revue des Nouvelles Technologies de l'Information	EGC	2012	Combinaison de classification supervisée et non-supervisée par la théorie des fonctions de croyance	Nous proposons dans cet article une nouvelle approche de classificationfondée sur la théorie des fonctions de croyance. Cette méthode repose surla fusion entre la classification supervisée et la classification non supervisée. Eneffet, nous sommes face à un problème de manque de données d'apprentissagepour des applications dont les résultats de classification supervisée et non superviséesont très variables selon les classificateurs employés. Les résultats ainsiobtenus sont par conséquent considérés comme incertains.Notre approche se propose de combiner les résultats des deux types de classificationen exploitant leur complémentarité via la théorie des fonctions de croyance.Celle-ci permet de tenir compte de l'aspect d'incertitude et d'imprécision. Aprèsavoir dresser les différentes étapes de notre nouveau schéma de classification,nous détaillons la fusion de classificateurs. Cette nouvelle approche est appliquéesur des données génériques, issues d'une vingtaine de bases de données.Les résultats obtenus ont montré l'efficacité de l'approche proposée.	Fatma Karem, Mounir Dhibi, Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001143	http://editions-rnti.fr/render_pdf.php?p=1001143
Revue des Nouvelles Technologies de l'Information	EGC	2012	Community Detection in Social Networks with Attribute and Relationship Data		The Anh Dang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001204	http://editions-rnti.fr/render_pdf.php?p=1001204
Revue des Nouvelles Technologies de l'Information	EGC	2012	Découverte de règles d'association pour l'aide à la prévision des accidents maritimes	Les systèmes de surveillance maritime permettent la récupération et lafusion des informations sur les navires (position, vitesse, etc.) à des fins de suividu trafic maritime sur un dispositif d'affichage. Aujourd'hui, l'identification desrisques à partir de ces systèmes est difficilement automatisable compte-tenu del'expertise à formaliser, du nombre important de navires et de la multiplicité desrisques (collision, échouement, etc). De plus, le remplacement périodique desopérateurs de surveillance complique la reconnaissance d'événements anormauxqui sont éparses et parcellaires dans le temps et l'espace. Dans l'objectif de faireévoluer ces systèmes de surveillance maritime, nous proposons dans cet article,une approche originale fondée sur le data mining pour l'extraction de motifsfréquents. Cette approche se focalise sur des règles de prévision et de ciblagepour l'identification automatique des situations induisant ou constituant le cadredes accidents maritimes.	Bilal Idiri, Aldo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001192	http://editions-rnti.fr/render_pdf.php?p=1001192
Revue des Nouvelles Technologies de l'Information	EGC	2012	Détection de groupes outliers en classification non supervisée	Nous proposons dans ce papier une nouvelle méthode de détection degroupes outliers. Notre mesure nommée GOF (Group Outlier Factor) est estiméepar l'apprentissage non-supervisé. Nous l'avons intégré dans l'apprentissage descartes topologiques. Notre approche est basée sur la densité relative de chaquegroupe de données, et fournit simultanément un partitionnement des donnéeset un indicateur quantitatif (GOF) sur "la particularité" de chaque cluster ougroupe. Les résultats obtenus sont très encourageants et prometteurs pour continuerdans cette optique.	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001184	http://editions-rnti.fr/render_pdf.php?p=1001184
Revue des Nouvelles Technologies de l'Information	EGC	2012	Détection non supervisée d'une sous-population par méthode d'ensemble et changement de représentation itératif	L'apprentissage non supervisé a classiquement pour objectif la détectionde sous-populations homogènes (classes) considérées de manière équivalentesans information a priori sur celles-ci. Le problème étudié dans cet articleest quelque peu distinct. On se focalise ici uniquement sur une sous-populationd'intérêt que l'on cherche à identifier avec un rappel et une précision optimales.Nous proposons, pour cela, une méthode s'appuyant sur les principes suivants :(1) travailler dans l'espace de représentation fourni par des experts faibles pourcette tâche, (2) confronter ces experts pour détecter des seuils de sélection pluspertinents, et (3) les combiner itérativement afin de converger vers l'expert idéal.Cette méthode est éprouvée et comparée sur des données synthétiques.	Christine Martin, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1001195	http://editions-rnti.fr/render_pdf.php?p=1001195
Revue des Nouvelles Technologies de l'Information	EGC	2012	Development of a distributed recommender system using the Hadoop Framework	Producing high quality recommendations has become a challenge inthe recent years. Indeed, the growth in the quantity of data involved in the recommendationprocess pose some scalability and effectiveness problems. Theseissues have encouraged the research of new technologies. Instead of developinga new recommender system we improve an already existing method. A distributedframework was considered based on the known quality and simplicity ofthe MapReduce project. The Hadoop Open Source project played a fundamentalrole in this research. It undoubtedly encouraged and facilitated the constructionof our application, supplying all tools needed. Our main goal in this research wasto prove that building a distributed recommender system was not only possible,but simple and productive.	Raja Chiky, Renata Ghisloti, Zakia Kazi Aoul	http://editions-rnti.fr/render_pdf.php?p1&p=1001179	http://editions-rnti.fr/render_pdf.php?p=1001179
Revue des Nouvelles Technologies de l'Information	EGC	2012	Evaluating Bayesian Networks by Sampling with Simplified Assumptions	The most common fitness evaluation for Bayesian networks in the presence of data is the Cooper-Herskovitz criterion. This technique involves massive amounts of data and, therefore, expansive computations. We propose a cheaper alternative evaluation method using simplified ssumptions which produces evaluations that are strongly correlated with the Cooper-Herskovitz criterion.	Saaid Baraty, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1001135	http://editions-rnti.fr/render_pdf.php?p=1001135
Revue des Nouvelles Technologies de l'Information	EGC	2012	Evaluation rapide du diamètre d'un graphe	Lors de l'analyse de graphes, il est important de connaître leurs propriétésafin de pouvoir par exemple identifier leur structure et les comparer.Une des caractérisations importante de ces graphes repose sur le fait de déterminers'il s'agit ou non d'un "petit monde". Pour ce faire, la valeur du diamètredu graphe est essentielle. Or la mesure du diamètre est pour un très grandgraphe, une opération extrêmement longue. Nous proposons un algorithme endeux phases qui permet d'obtenir rapidement une estimation du diamètre d'ungraphe avec une proportion d'erreur faible. En réduisant cet algorithme à uneseule phase et en acceptant une marge d'erreur plus élevée, nous obtenons uneestimation très rapide du diamètre. Nous testons cet algorithme sur deux grandsgraphes de terrain (plus d'un million de noeuds) et comparons ses performancesavec celles d'un algorithme de référence BFS (Breadth-First Search). Les résultatsobtenus sont décrits et commentés.	Christian Belbeze, Max Chevalier, Chantal Soulé-Dupuy	http://editions-rnti.fr/render_pdf.php?p1&p=1001183	http://editions-rnti.fr/render_pdf.php?p=1001183
Revue des Nouvelles Technologies de l'Information	EGC	2012	Exploitation de l'asymétrie entre termes pour l'extraction automatique de taxonomies à partir de textes	Nous présentons dans cet article une nouvelle approche pour la générationautomatique de structures lexicales (ou taxonomies) à partir de textes.Cette tâche est fondée sur l'hypothèse forte selon laquelle l'accumulation defaits statistiques simples sur les usages en corpus permet d'approximer des informationsde niveau sémantique sur le lexique. Nous utilisons la prétopologiecomme cadre de travail afin de formaliser et de combiner plusieurs hypothèsessur les usages terminologiques et enfin de structurer le lexique sous la formed'une taxonomie. Nous considérons également le problème de l'évaluation destaxonomies résultantes et proposons un nouvel indice afin de les comparer et depositionner notre approche par rapport à la littérature.	Davide Buscaldi, Guillaume Cleuziou, Gaël Dias, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1001200	http://editions-rnti.fr/render_pdf.php?p=1001200
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de co-variations entre des propriétés de sommets et leur position topologique dans un graphe attribué	L'analyse de grands réseaux est très étudiée en fouille de données.Toutefois, les approches existantes proposent une analyse soit à un niveau macroscopique(étude des propriétés globales comme la distribution des degrés),soit à un niveau microscopique (extraction de sous-graphes fréquents ou denses).Nous proposons une nouvelle méthode qui effectue une analyse intermédiairepermettant de découvrir des motifs regroupant des propriétés microscopiques etmacroscopiques du réseau. Ces motifs capturent des co-variations entre des propriétésnumériques relatives aux sommets. Par exemple, un motif mésoscopiquedans un réseau de co-auteurs peut être plus le nombre de publications à EGC estimportant, plus la centralité des sommets correspondants dans le réseau l'estégalement. Notre contribution est multiple. D'abord, ce travail est le premierà exploiter conjointement des propriétés locales et des propriétés topologiques.De plus, nous produisons de nouvelles avancées dans le domaine de l'extractionde co-variations en revisitant les motifs émergents dans ce contexte. Enfin, nousrapportons une analyse d'un réseau bibliographique réel issu de DBLP.	Adriana Prado, Marc Plantevit, Celine Robardet, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001171	http://editions-rnti.fr/render_pdf.php?p=1001171
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de Dépendances Fonctionnelles Approximatives	La découverte de dépendances fonctionnelles (DF) à partir d'une relationexistante est une technique importante pour l'analyse de Bases de Données.L'ensemble des DF exactes ou approximatives extraites par les algorithmes existantsest valide tant que la relation n'est pas modifiée. Ceci est insuffisant pourdes situations réelles où les relations sont constamment mises à jour.Nous proposons une approche incrémentale qui maintiens à jour l'ensemble desDF valides, exactes ou approximatives selon une erreur donnée, quand des tuplessont insérés et supprimés. Les résultats expérimentaux indiquent que lors de l'extractionde DF à partir d'une relation continuellement modifiée, les algorithmesexistants sont sensiblement dépassés par notre stratégie incrémentale.	Noel Novelli, Ekaterina Simonenko	http://editions-rnti.fr/render_pdf.php?p1&p=1001503	http://editions-rnti.fr/render_pdf.php?p=1001503
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de Liens Fréquents dans les Réseaux Sociaux	Cet article présente FLMin, une nouvelle méthode d'extraction de motifsfréquents dans les réseaux sociaux. Contrairement aux méthodes traditionnellesqui s'intéressent uniquement aux régularités structurelles, l'originalité denotre approche réside dans sa capacité à exploiter la structure et les attributs desnoeuds pour extraire des régularités, que nous appelons "liens fréquents", dansles liens entre des noeuds partageant des caractéristiques communes.	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001174	http://editions-rnti.fr/render_pdf.php?p=1001174
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de séquences fréquentes avec intervalles d'incertitude	Lors de l'extraction des séquences, la granularité temporelle est plusou moins importante selon les besoins des utilisateurs et les contraintes du domained'application. Nous proposons un algorithme d'extraction de séquencesfréquentes par intervalles à partir de séquences à estampilles temporelles discrètes.Nous intégrons une relaxation des contraintes temporelles en introduisantla définition de "séquences temporelles par intervalles" (STI). Ces intervalles reflètentune incertitude sur les occurrences précises des évènements. Nous formalisonsce nouveau concept en exhibant certaines de ses propriétés et nous menonsquelques expériences afin de comparer (qualitativement) nos résultats avec uneautre proposition assez proche de la nôtre	Asma Ben Zakour, Sofian Maabout, Mohamed Mosbah, Marc Sistiaga	http://editions-rnti.fr/render_pdf.php?p1&p=1001160	http://editions-rnti.fr/render_pdf.php?p=1001160
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de sous-parties ciblées d'une ontologie généraliste pour enrichir une ontologie particulière	Différentes ressources ontologiques généralistes de très grande tailleont été développées de façon collective et sont aujourd'hui disponibles sur leweb. Ainsi l'ontologie YAGO est une énorme base de connaissances décrivantplus de 2 millions d'entités. Afin de tirer parti de ce gigantesque travail collectif,nous montrons comment en extraire des sous-parties thématiquement focaliséespour enrichir une autre ontologie, dite cible, de taille plus limitée mais de domainecentré sur une application particulière 1.	Fayçal Hamdi, Brigitte Safar, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001176	http://editions-rnti.fr/render_pdf.php?p=1001176
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction d'opinions appliquée à des critères	Les technologies de l'information et le succès des services associés(e.g., blogs, forums,...) ont ouvert la voie à un mode d'expression massive d'opinionssur les sujets les plus variés. Récemment, de nouvelles techniques de détectionautomatique d'opinions (opinion mining) ont fait leur apparition et viades analyses statistiques des avis exprimés, tendent à dégager une tendance globaledes opinions exprimées par les internautes. Néanmoins une analyse plusfine de celle-ci montre que les arguments avancés par les internautes relèvent decritères de jugement distincts. Ici, un film sera décrié pour un scénario décousu,là il sera encensé pour une bande son époustouflante. Dans cet article, nous proposons,après avoir caractérisé automatiquement des critères dans un document,d'en extraire l'opinion relative. A partir d'un ensemble restreint de mots clésd'opinions, notre approche construit automatiquement une base d'apprentissagede documents issus du web et en déduit un lexique de mots ou d'expressionsd'opinions spécifiques au domaine d'application. Des expériences menées surdes jeux de données réelles illustrent l'efficacité de l'approche.	Benjamin Duthil, François Trousset, Gérard Dray, Pascal Poncelet, Jacky Montmain	http://editions-rnti.fr/render_pdf.php?p1&p=1001197	http://editions-rnti.fr/render_pdf.php?p=1001197
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction et gestion d'informations pour la construction d'une base vidéo d'apprentissage	Indexer une vidéo consiste à rattacher un ou plusieurs concepts à dessegments de cette vidéo, un concept étant défini comme une représentation intellectuelled'une idée abstraite. L'indexation automatique se base sur l'extractionautomatique de caractéristiques fournies par un système de traitement d'images.Cependant, il est nécessaire de définir les index ou concepts. Pour cela il fautdéfinir le lien qui existe entre ces caractéristiques et ces concepts. Ce qui sépareles caractéristiques extraites sur lesquelles se base l'indexation automatique etles concepts est appelé fossé sémantique qui est le manque de concordance entreles informations que les machines peuvent extraire depuis les documents numériqueset les interprétations que les humaines en font. La définition d'un conceptpeut être faite automatiquement si l'on dispose d'une base d'apprentissage liéeau concept. Dans ce cas, il est possible "d'apprendre" le concept de manièrestatistique. Mais la construction de cette base d'apprentissage nécessite de faireintervenir un utilisateur ou un expert applicatif. En fait, il s'agit de s'appuyer surses connaissances pour extraire des segments vidéo représentatifs du conceptque l'on souhaite définir. On peut lui demander d'indexer manuellement la based'apprentissage, mais cette opération est longue et fastidieuse. Dans cet article,nous proposons une méthode qui permet d'extraire l'expertise pour que l'implicationde l'expert soit la plus simple et la plus limitée possible.	Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1001190	http://editions-rnti.fr/render_pdf.php?p=1001190
Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction incrémentale de séquences fréquentes dans un flux d'itemsets		Thomas Guyet, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1001206	http://editions-rnti.fr/render_pdf.php?p=1001206
Revue des Nouvelles Technologies de l'Information	EGC	2012	Human Detection by a Small Autonomous Mobile Robot	Nous proposons une méthode utilisant les histogrammes de gradientorienté (HOG) et les séparateurs à vaste marge (SVM) pour la détection de personnesà partir d'images prises depuis un petit robot mobile autonome. Les travauxantérieurs réalisés dans le domaine de la détection d'êtres humains à partird'images ne peuvent pas être employés pour ce type d'application car ils supposentque les images sont prises à partir d'une position élevée (au moins lahauteur d'un petit enfant) alors que la taille de notre robot n'est que de 15cm.Nous employons à la fois les HOG et les SVM car cette combinaison de méthodesest reconnue comme étant celle ayant le plus de succès pour la détectionde personnes. Pour traiter une grande variété de formes humaines, principalementen raison de la distance existant entre les personnes et le robot, nous avonsdéveloppé une nouvelleméthode de prédiction à deux étapes utilisant deux typesde classificateurs SVM qui reposent sur une estimation de la distance. L'estimationest basée sur une proportion de pixels de couleur de peau dans l'image, cequi nous permet de clairement séparer notre problème de la détection de corpsentier et de celle de corps partiel. Les essais réalisés dans un bureau ont montrédes résultats prometteurs de notre méthode avec une valeur de F de 0,93.	Kouhei Takemoto, Shigeru Takano, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001172	http://editions-rnti.fr/render_pdf.php?p=1001172
Revue des Nouvelles Technologies de l'Information	EGC	2012	Identification et caractérisation de différents types de boycott par des méthodes d'Analyse de Données		Henri Ralambondrainy, Marinette Amirault-Thébault	http://editions-rnti.fr/render_pdf.php?p1&p=1001207	http://editions-rnti.fr/render_pdf.php?p=1001207
Revue des Nouvelles Technologies de l'Information	EGC	2012	K-moyennes contraintes par un classifieur Application à la personnalisation de scores de campagnes	Lorsqu'on désire contacter un client pour lui proposer un produit oncalcule au préalable la probabilité qu'il achètera ce produit. Cette probabilitéest calculée à l'aide d'un modèle prédictif pour un ensemble de clients. Le servicemarketing contacte ensuite ceux ayant la plus forte probabilité d'acheter leproduit. En parallèle, et avant le contact commercial, il peut être intéressant deréaliser une typologie des clients qui seront contactés. L'idée étant de proposerdes campagnes différenciées par groupe de clients. Cet article montre commentil est possible de contraindre la typologie, réalisée à l'aide des k-moyennes, àrespecter la proximité des clients vis-à-vis de leur score d'appétence.	Vincent Lemaire, Nicolas Creff, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1001155	http://editions-rnti.fr/render_pdf.php?p=1001155
Revue des Nouvelles Technologies de l'Information	EGC	2012	L'extraction de règles de dépendance bien définies entre ensembles de variables multivaluées	Cet article étudie la faisabilité et l'intérêt de l'extraction de règles dedépendance entre ensembles de variables multivaluées en comparaison du problèmebien connu de l'extraction des règles d'association fréquentes. Une règlede dépendance correspond à une dépendance fonctionnelle approximative caractériséeprincipalement par l'entropie conditionnelle associée. L'article montrecomment établir une analogie formelle entre les deux familles de règles et commentadapter à l'aide de cette analogie l'algorithme « Eclat » afin d'extraire d'unjeu de données les règles de dépendance dites bien définies. Une étude expérimentaleconclut sur les forces et inconvénients des règles de dépendance biendéfinies vis-à-vis des règles d'association fréquentes	Frédéric Pennerath	http://editions-rnti.fr/render_pdf.php?p1&p=1001168	http://editions-rnti.fr/render_pdf.php?p=1001168
Revue des Nouvelles Technologies de l'Information	EGC	2012	Méta-règles pour la génération de règles négatives	La littérature s'est beaucoup intéressée à l'extraction de règles classiques(ou positives) et peu à l'extraction des règles négatives en raison essentiellementd'une part, du coût de calculs et d'autre part, du nombre prohibitif derègles redondantes et inintéressantes extraites. La démarche que nous avons retenueest de dégager les règles négatives lors de l'extraction des règles positives,et pour cela, nous recherchons les règles négatives que l'on peut inférer ou pas àpartir de la pertinence d'une règle positive. Ces différentes inférences vont êtreformalisées par un ensemble de méta-règles.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001162	http://editions-rnti.fr/render_pdf.php?p=1001162
Revue des Nouvelles Technologies de l'Information	EGC	2012	Mining Genetic Interactions in Genome-Wide Association Study	Advanced biotechnologies have rendered feasible high-throughput data collecting in human and other model organisms. The availability of such data holds promise for dissecting complex biological processes. Making sense of the flood of biological data poses great statistical and computational challenges. I will discuss the problem of mining gene-gene interactions in high-throughput genetic data. Finding genetic interactions is an important biological problem since many common diseases are caused by joint effects of genes. Previously, it was considered intractable to find genetic interactions in the whole-genome scale due to the enormous search space. The problem was commonly addressed using heuristics which do not guarantee the optimality of the solution. I will show that by utilizing the upper bound of the test statistic and effectively indexing the data, we can dramatically prune the search space and reduce computational burden. Moreover, our algorithms guarantee to find the optimal solution. In addition to handling specific statistical tests, our algorithms can be applied to a wide range of study types by utilizing convexity, a common property of many commonly used statistics.	Wei Wang    	http://editions-rnti.fr/render_pdf.php?p1&p=1001128	http://editions-rnti.fr/render_pdf.php?p=1001128
Revue des Nouvelles Technologies de l'Information	EGC	2012	Modèle de supervision d'interactions non-intrusif basé sur les ontologies	L'automatisation et la supervision des systèmes pervasifs est à l'heureactuelle principalement basée sur l'utilisation massive de capteurs distribuésdans l'environnement. Dans cet article, nous proposons un modèle de supervisiond'interactions basé sur l'analyse sémantique des logs domotiques (commandesémises par l'utilisateur), visant à limiter l'utilisation de ces capteurs :le principe est d'utiliser des outils d'inférences avancés, afin de déduire les informationshabituellement captées. Pour cela, une ontologie, automatiquementdérivée d'un processus dirigé par les modèles, définit les interactions utilisateursystème.L'utilisation d'un système de règles permet ensuite d'inférer des informationssur la localisation et l'intention de l'utilisateur, dans le but de réaliserdu monitoring et de proposer des services domotiques adaptés.	Willy Allègre, Thomas Burger, Pascal Berruet, Jean-Yves Antoine	http://editions-rnti.fr/render_pdf.php?p1&p=1001148	http://editions-rnti.fr/render_pdf.php?p=1001148
Revue des Nouvelles Technologies de l'Information	EGC	2012	PLS path modeling and regularized generalized canonical correlation analysis for multi-block data analysis	Regularized generalized canonical correlation analysis (RGCCA) is a generalization of regularizedcanonical correlation analysis to three or more sets of variables. It constitutes a generalframework for many multi-block data analysis methods. It combines the power of multi-blockdata analysis methods (maximization of well identified criteria) and the flexibility of PLS pathmodeling (the researcher decides which blocks are connected and which are not). Searchingfor a fixed point of the stationary equations related to RGCCA, a new monotone convergentalgorithm, very similar to the PLS algorithm proposed by Herman Wold, is obtained. Finally,a practical example is discussed.	Michel Tenenhaus	http://editions-rnti.fr/render_pdf.php?p1&p=1001133	http://editions-rnti.fr/render_pdf.php?p=1001133
Revue des Nouvelles Technologies de l'Information	EGC	2012	Prétraitement Supervisé des Variables Numériques pour la Fouille de Données Multi-Tables	Le prétraitement des variables numériques dans le contexte de lafouille de données multi-tables diffère de celui des données classiques individuvariable.La difficulté vient principalement des relations un-à-plusieurs où lesindividus de la table cible sont potentiellement associés à plusieurs enregistrementsdans des tables secondaires. Dans cet article, nous décrivons une méthodede discrétisation des variables numériques situées dans des tables secondaires.Nous proposons un critère qui évalue les discrétisations candidates pour ce typede variables. Nous décrivons un algorithme d'optimisation simple qui permetd'obtenir la meilleure discrétisation en intervalles de fréquence égale pour lecritère proposé. L'idée est de projeter dans la table cible l'information contenuedans chaque variable secondaire à l'aide d'un vecteur d'attributs (un attributpar intervalle de discrétisation). Chaque attribut représente le nombre de valeursde la variable secondaire appartenant à l'intervalle correspondant. Ces attributsd'effectifs sont conjointement partitionnés à l'aide de modèles en grille de donnéesafin d'obtenir une meilleure séparation des valeurs de la classe. Des expérimentationssur des jeux de données réelles et artificielles révèlent que l'approchede discrétisation permet de découvrir des variables secondaires pertinentes.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1001191	http://editions-rnti.fr/render_pdf.php?p=1001191
Revue des Nouvelles Technologies de l'Information	EGC	2012	Raisonner sur une ontologie cartographique pour concevoir des légendes de cartes	Concevoir une carte géographique, plus particulièrement sa légende,exige des compétences spécifiques. L'objectif de ce papier est de présenter unebase de connaissances destinée à aider tout utilisateur à concevoir une ou plusieurslégendes adaptées à son besoin et conformes aux règles de cartographie.La base de connaissances est formée d'une ontologie de la cartographie nomméeOntoCarto, d'un corpus de règles : OntoCartoRules et d'un moteur de raisonnement: Corese. Dans ce papier, chaque demande de conception de légende estvue comme une instanciation particulière de l'ontologie, associée à une sélectionde règles pertinentes dans le corpus de règles, sur laquelle Corese va raisonnerpour construire des légendes adaptées à la configuration spécifique traitée. Laconception de la légende s'appuie sur la définition de deux hiérarchies d'objetsgéographiques et cartographiques. Les principes de fonctionnement de Coresesont présentés. Un prototype a été implémenté et des extraits des résultats sontmontrés.	Catherine Dominguès, Olivier Corby, Fayrouz Soualah-Alila	http://editions-rnti.fr/render_pdf.php?p1&p=1001178	http://editions-rnti.fr/render_pdf.php?p=1001178
Revue des Nouvelles Technologies de l'Information	EGC	2012	Recherche d'Information Agrégée dans des documents XML basée sur les Réseaux Bayésiens	Dans cet article, nous nous intéressons à la recherche agrégée dansdes documents XML. Pour cela, nous proposons un modèle basé sur les réseauxbayésiens. Les relations de dépendances entre requête-termes d'indexation ettermes d'indexation-éléments sont quantifiées par des mesures de probabilité.Dans ce modèle, la requête de l'utilisateur déclenche un processus de propagationpour trouver des éléments. Ainsi, au lieu de récupérer une liste des élémentsqui sont susceptibles de répondre à la requête, notre objectif est d'agréger dansun agrégat des éléments pertinents, non-redondants et complémentaires. Nousavons évalué notre approche dans le cadre de la compagne d'évaluation INEX2009 et avons présenté quelques résultats expérimentaux mettant en évidencel'impact de l'agrégation de tels éléments.	Najeh Naffakhi, Mohand Boughanem, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001202	http://editions-rnti.fr/render_pdf.php?p=1001202
Revue des Nouvelles Technologies de l'Information	EGC	2012	Relational Learning from Spatial Data: Retrospect and Prospect	Learning from spatial data is characterized by two main features. First, spatial objects have a locational property which implicitly defines several spatial relationships (topological, directional, distancebased) between objects. Second, attributes of spatially related units tend to be statistically correlated. These two features argue against the assumption of the independent generation of data samples (i.i.d. assumption) underlying classic machine learning algorithms, and motivate the application of relational learning algorithms, whose inferences are based on both instance properties and relations between data. This relational learning approach to spatial domains has already been investigated in the last decade, and important accomplishments in this direction have already been performed. In this talk, we retrospectively survey major achievements on relational learning from spatial data and we report open problems which still challenges researchers and prospectively suggest important topics for incorporation into a research agenda.	Donato Malerba	http://editions-rnti.fr/render_pdf.php?p1&p=1001131	http://editions-rnti.fr/render_pdf.php?p=1001131
Revue des Nouvelles Technologies de l'Information	EGC	2012	Réorganisation hiérarchique de visualisations dans OLAP	Dans cet article nous proposons un nouvel algorithme pour la réorganisationhiérarchique des cubes OLAP (On-Line Analytical Processing) ayantpour objectif d'améliorer leur visualisation. Cet algorithme se caractérise par lefait qu'il peut traiter des dimensions organisées hiérarchiquement et optimiserconjointement les dimensions du cube, contrairement aux autres approches. Ilutilise un algorithme génétique qui réorganise des arbres n-aires quelconques. Ila été intégré dans une interface OLAP puis testé en comparaison avec d'autresapproches de réorganisation, et fournit des résultats très positifs. A ce titre,nous avons également généralisé l'algorithme heuristique classique BEA ("bondenergy algorithm") au cas de hiérarchies OLAP. Enfin, notre approche a été évaluéepar des utilisateurs et les résultats soulignent l'intérêt de la réorganisationdans des exemples de tâches à résoudre pour OLAP.	Sébastien Lafon, Fatma Bouali, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001181	http://editions-rnti.fr/render_pdf.php?p=1001181
Revue des Nouvelles Technologies de l'Information	EGC	2012	Représentations de services web : impact sur la découverte et la recommandation		Mustapha Aznag, Mohamed Quafafou, Nicolas Durand, Zahi Jarir	http://editions-rnti.fr/render_pdf.php?p1&p=1001203	http://editions-rnti.fr/render_pdf.php?p=1001203
Revue des Nouvelles Technologies de l'Information	EGC	2012	RICSH : Recherche d'information contextuelle par segmentation thématique de documents	Le but principal des systèmes de recherche d'informations (SRI) classiquesest de retrouver dans un corpus de documents l'information considéréecomme pertinente pour une requête utilisateur. Cette pertinence est souvent liéeà la fréquence d'apparition des termes dans le texte par rapport au corpus sanstenir compte du contexte de la recherche. Partant de ce constat, nous proposonsdans cet article une approche pour la recherche d'information contextuelle parsegmentation thématique de documents (RICSH). Cette approche s'appuie surla méthode de pondération tf-idf que nous avons adaptée dans notre cas pourindexer le corpus. Cette adaptation se situe au niveau de l'importance du termeet de son pouvoir de discrimination par rapport aux fragments de textes et nonau corpus. Ces fragments sont obtenus grâce à un processus d'identification desunités thématiques les plus pertinentes pour chaque document.	Fadila Bentayeb, Omar Boussaid, Rachid Aknouche	http://editions-rnti.fr/render_pdf.php?p1&p=1001199	http://editions-rnti.fr/render_pdf.php?p=1001199
Revue des Nouvelles Technologies de l'Information	EGC	2012	Sélection Bayésienne de Modèles avec Prior Dépendant des Données	Cet article analyse la consistance asymptotique des modèles en grilleappliqués à l'estimation de densité jointe de deux variables catégorielles. Lesmodèles en grille considèrent un partitionnement des valeurs de chacune des variables,le produit Cartésien des partitions formant une grille dont les cellulespermettent de résumer la table de contingence des deux variables. Le meilleurmodèle de co-partitionnement est recherché au moyen d'une approche MAP(maximum a posteriori), présentant la particularité peu orthodoxe d'exploiterune famille de modèles et une distribution a priori de ces modèles qui dépendentdes données. Ces modèles sont par nature des modèles de l'échantillon d'apprentissage,et non de la distribution sous-jacente. Nous démontrons la consistancede l'approche, qui se comporte comme un estimateur universel de densité jointeconvergeant asymptotiquement vers la vraie distribution jointe.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001136	http://editions-rnti.fr/render_pdf.php?p=1001136
Revue des Nouvelles Technologies de l'Information	EGC	2012	Solving Problems with Visual Analytics: Challenges and Applications	Never before in history data is generated and collected at such high volumes as it is today. As the volumes of data available to business people, scientists, and the public increase,their effective use becomes more challenging. Keeping up to date with the flood of data,using standard tools for data analysis and exploration, is fraught with difficulty. The field ofvisual analytics seeks to provide people with better and more effective ways to understandand analyze large datasets, while also enabling them to act upon their findings immediately. Visual analytics integrates the analytic capabilities of the computer and the abilities of the human analyst, allowing novel discoveries and empowering individuals to take control of the analytical process. Visual analytics enables unexpected and hidden insights, which may lead to beneficial and profitable innovation. The talk presents the challenges of visual analytics and exemplifies them with application examples, illustrating the exiting potential of current visual analysis techniques.	Daniel Keim	http://editions-rnti.fr/render_pdf.php?p1&p=1001130	http://editions-rnti.fr/render_pdf.php?p=1001130
Revue des Nouvelles Technologies de l'Information	EGC	2012	Structuration des décisions de jurisprudence basée sur une ontologie juridique en langue arabe	L'informatique juridique, est un domaine en évolution constante. Lecontexte général de notre travail est l'élaboration d'un système de recherchede jurisprudence tunisienne en langue arabe. L'objectif opérationnel de ce systèmeest de fournir une aide aux juristes pour résoudre une situation juridiquedonnée en mettant à leur disposition une collection de situations similaires cequi améliorera leur raisonnement futur. Une ontologie du domaine juridiqueconstruite à partir des documents des décisions juridiques est nécessaire dansnotre contexte.Cette ontologie a pour but : (i) la structuration des décisions, (ii)la formulation des requêtes d'interrogation de la base des décisions, et (iii) larecherche des décisions. Dans cet article, nous présentons l'architecture de notresystème de recherche de jurisprudence. Nous nous focalisons sur l'ontologie dudomaine de jurisprudence que nous avons élaborée, aisni que sur le module destructuration des décisions.	Karima Dhouib, Sylvie Desprès, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001175	http://editions-rnti.fr/render_pdf.php?p=1001175
Revue des Nouvelles Technologies de l'Information	EGC	2012	SweetDeki : le wiki sémantique couteau suisse du réseau social ISICIL	Le projet ANR ISICIL 1 mixe les nouvelles applications virales duweb avec des représentations formelles et des processus d'entreprise pour les intégrerdans les pratiques de veille en entreprise. Les outils développés s'appuientsur les interfaces avancées des applications du web 2.0 (blog, wiki, social bookmarking,extensions de navigateurs) pour les interactions et sur les technologiesdu web sémantique pour l'interopérabilité et le traitement de l'information. Leprésent article décrit plus précisément le wiki sémantique développé dans lecadre de ce projet et son intégration au coeur du framework ISICIL	Michel Buffa, Guillaume Husson, Nicolas Delaforge	http://editions-rnti.fr/render_pdf.php?p1&p=1001151	http://editions-rnti.fr/render_pdf.php?p=1001151
Revue des Nouvelles Technologies de l'Information	EGC	2012	TMD-MINER : Une nouvelle approche pour la détection des diffuseurs dans un système communautaire	Plusieurs méthodes ont été développées ces dernières années pour détecter,dans un réseau social, les membres qualifiés, selon les auteurs, d'influenceurs,de médiateurs, d'ambassadeurs ou encore d'experts. Dans cet article, nousproposons un nouveau cadre méthodologique permettant d'identifier des diffuseursdans le contexte où seule l'information sur l'appartenance des membres duréseau à des communautés est disponible. Ce cadre, basé sur une représentationdu réseau sous forme d'hypergraphe, nous a permis de formaliser la notion dediffuseur et d'introduire l'algorithme TMD-MINER, dédié à la détection des diffuseurset basé sur les itemsets essentiels.	Mohamed Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001193	http://editions-rnti.fr/render_pdf.php?p=1001193
Revue des Nouvelles Technologies de l'Information	EGC	2012	Topological Decomposition and Heuristics for High Speed Clustering of Complex Networks	With the exponential growth in the size of data and networks, developmentof new and fast techniques to analyze and explore these networks isbecoming a necessity. Moreover the emergence of scale free and small worldproperties in real world networks has stimulated lots of activity in the field ofnetwork analysis and data mining. Clustering remains a fundamental techniqueto explore and organize these networks. A challenging problem is to find a clusteringalgorithm that works well in terms of clustering quality and is efficient interms of time complexity.In this paper, we propose a fast clustering algorithm which combines someheuristics with a Topological Decomposition to obtain a clustering. The algorithmwhich we call Topological Decomposition and Heuristics for Clustering(TDHC) is highly efficient in terms of asymptotic time complexity as comparedto other existing algorithms in the literature. We also introduce a number ofHeuristics to complement the clustering algorithm which increases the speed ofthe clustering process maintaining the high quality of clustering. We show theeffectiveness of the proposed clustering method on different real world data setsand compare its results with well known clustering algorithms.	Faraz Zaidi, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001147	http://editions-rnti.fr/render_pdf.php?p=1001147
Revue des Nouvelles Technologies de l'Information	EGC	2012	Transformation de l'espace de description pour l'apprentissage par transfert	Dans ce papier, nous proposons une étude sur l'utilisation de l'apprentissagetopologique pondéré et les méthodes de factorisation matricielle pourtransformer l'espace de représentation d'un jeu de données "sparse" afin d'augmenterla qualité de l'apprentissage, et de l'adapter au cas de l'apprentissagepar transfert. La factorisation matricielle nous permet de trouver des variableslatentes et l'apprentissage topologique pondéré est utilisé pour détecter les pluspertinentes parmi celles-ci. La représentation de nouvelles données est basée surleurs projections sur le modèle topologique pondéré.Pour l'apprentissage par transfert, nous proposons une nouvelle méthode où lareprésentation des données est faite de la même manière que dans la premièrephase, mais en utilisant un modèle topologique élagué.Les expérimentations sont présentées dans le cadre d'un Challenge Internationaloù nous avons obtenu des résultats prometteurs (5ieme rang de la compétitioninternationale).1 Introduction	Nistor Grozavu, Younès Bennani, Lazhar Labiod	http://editions-rnti.fr/render_pdf.php?p1&p=1001185	http://editions-rnti.fr/render_pdf.php?p=1001185
Revue des Nouvelles Technologies de l'Information	EGC	2012	Un algorithme de classification automatique pour des données relationnelles multi-vues	classification automatique (De Carvalho et al., 2012) capable de partitionnerdes objets en prenant en compte de manière simultanée plusieurs matricesde dissimilarité qui les décrivent. Ces matrices peuvent avoir été généréesen utilisant différents ensembles de variables et de fonctions de dissimilarité.Cette méthode, basée sur l'algorithme de nuées dynamiques est conçu pour fournirune partition et un prototype pour chaque classe tout en découvrant une pondérationpertinante pour chaque matrice de dissimilarité en optimisant un critèred'adéquation entre les classes et leurs représentants. Ces pondérations changentà chaque itération de l'algorithme et sont différentes pour chacune des classes.Nous présentons aussi plusieurs outils d'aide à l'interprétation des groupes et dela partition fournie par cette nouvelle méthode. Deux exemples illustrent l'interêtde la méthode. Le premier utilise des données concernant des chiffres manuscrits(0 à 9) numérisés en images binaires provenant de l'UCI. Le second utilise unensemble de rapports dont nous connaissons une classification experte donnée àpriori.	Francisco de Assis Tenório de Carvalho, Filipe M. de Melo, Yves Lechevallier, Thierry Despeyroux	http://editions-rnti.fr/render_pdf.php?p1&p=1001142	http://editions-rnti.fr/render_pdf.php?p=1001142
Revue des Nouvelles Technologies de l'Information	EGC	2012	Un assistant utilisateur pour le choix et le paramétrage des méthodes de fouille visuelle de données	Nous nous intéressons dans cet article au problème de l'automatisation du processus de choix et de paramétrage des visualisations en fouille visuelle de données. Pour résoudre ce problème, nous avons développé un assistant utilisateur qui effectue deux étapes : à partir des objectifs annoncés par l'utilisateur et des caractéristiques de ses données, le système commence par proposer à l'utilisateur différents appariements entre la base de données à visualiser et les visualisations qu'il gère. Ces appariements sont générés par une heuristique utilisant une base de connaissances sur les visualisations et la perception visuelle. Ensuite, afin d'affiner les différents paramétrages suggérés par le système, nous utilisons un algorithme génétique interactif qui permet aux utilisateurs d'évaluer et d'ajuster visuellement ces paramétrages. Nous présentons une évaluation utilisateur qui montre l'intérêt de notre système pour deux tâches.	Abdelheq Et-tahir Guettala, Fatma Bouali, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001180	http://editions-rnti.fr/render_pdf.php?p=1001180
Revue des Nouvelles Technologies de l'Information	EGC	2012	Un environnement efficace pour la classification d'images à grande échelle	La plupart des processus de classification d'images comportent troisprincipales étapes : l'extraction de descripteurs de bas niveaux, la création d'unvocabulaire visuel par quantification et l'apprentissage à l'aide d'un algorithmede classification (eg.SVM). De nombreux problèmes se posent pour le passageà l'échelle comme avec l'ensemble de données ImageNet contenant 14 millionsd'images et 21,841 classes. La complexité concerne le temps d'exécution dechaque tâche et les besoins en mémoire et disque (eg. le stockage des SIFTs nécessite11To). Nous présentons une version parallèle de LibSVM pour traiter degrands ensembles de données dans un temps raisonnable. De plus, il y a beaucoupde perte d'information lors de la phase de quantification et les mots visuelsobtenus ne sont pas assez discriminants pour de grands ensembles d'images.Nous proposons d'utiliser plusieurs descripteurs simultanément pour améliorerla précision de la classification sur de grands ensembles d'images. Nous présentonsnos premiers résultats sur les 10 plus grandes classes (24,817 images)d'ImageNet.	Thanh-Nghi Doan, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001189	http://editions-rnti.fr/render_pdf.php?p=1001189
Revue des Nouvelles Technologies de l'Information	EGC	2012	Une approche multidimensionnelle basée sur les comportements individuels pour la prédiction de la diffusion de l'information sur Twitter	Aujourd'hui, les réseaux sociaux en ligne sont devenus des outils trèspuissants de propagation de l'information. Ils favorisent la diffusion rapide àgrande échelle de contenu et les conséquences d'une information inexacte voirefausse peuvent alors prendre une ampleur considérable. Par conséquent il devientindispensable de proposer des moyens d'analyser le phénomène de diffusionde l'information dans ces réseaux. De nombreuses études récentes ont traitéde la modélisation du processus de diffusion de l'information, essentiellementd'un point de vue topologique et dans une perspective théorique, mais les facteursimpliqués sont encore méconnus. Nous proposons ici une solution pratiquedont l'objectif est de prédire la dynamique temporelle de la diffusion au sein deTwitter, basée sur des techniques d'apprentissage automatique. Notre approcherepose sur l'inférence de probabilités de diffusion tirées d'une analyse multidimensionnelledes comportements individuels. Les expérimentations menéesmontrent l'intérêt de la modélisation proposée.	Adrien Guille, Hakim Hacid, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001173	http://editions-rnti.fr/render_pdf.php?p=1001173
Revue des Nouvelles Technologies de l'Information	EGC	2012	Une distance hiérarchique basée sur la sémantique pour la comparaison d'histogrammes nominaux	La plupart des distances entre histogrammes sont définies pour comparerdes histogrammes ordonnés (dont les entités représentées sont totalementordonnées) ou des histogrammes nominaux (dont les entités représentées nepeuvent pas être comparées). Cependant, il n'existe aucune distance qui permettede comparer des histogrammes nominaux dans lesquels il est possible dequantifier des valeurs de proximité sémantique entre les entités considérées. Cetarticle propose une nouvelle distance permettant de pallier ce problème. Dans unpremier temps, une hiérarchie d'histogrammes, obtenue par le biais d'une fusionprogressive des entités considérées (prenant en compte leurs proximités sémantiques),est construite. Pour chaque étage de cette hiérarchie, une distance standardde comparaison d'histogrammes nominaux est calculée. Finalement, pourobtenir la distance proposée, ces différentes distances sont fusionnées en prenanten compte la cohérence sémantique associée aux niveaux de chaque étage de lahiérarchie. Cette distance a été validée dans le cadre de la classification de donnéesgéographiques. Les résultats obtenus sont encourageants et montrent ainsil'intérêt et l'utilité de cette dernière pour des processus de fouille de données.	Camille Kurtz	http://editions-rnti.fr/render_pdf.php?p1&p=1001144	http://editions-rnti.fr/render_pdf.php?p=1001144
Revue des Nouvelles Technologies de l'Information	EGC	2012	User Evaluation: Why?	Research in information visualisation has changed significantly in the past two decades.Once it was sufficient to simply design and implement an impressive visualisation system.Today editors and reviewers expect papers to present not only a novel system, but empiricalevidence of its worth. Why has this change come about, and what impact has it had on thoseworking in this area? This talk will discuss how a field dominated by algorithms and toolsbecame infected by human participants, and why this is a positive development in a maturingresearch discipline.	Helen Purchase	http://editions-rnti.fr/render_pdf.php?p1&p=1001132	http://editions-rnti.fr/render_pdf.php?p=1001132
Revue des Nouvelles Technologies de l'Information	EGC	2012	Utilisation d'invariants pour une médiation inter-domaines de modèles utilisateurs : ressources invariantes et invariants sémantiques	Les services de personnalisation du Web 2.0 reposent sur l'exploitationde modèles utilisateurs. Schématiquement, plus la quantité d'informationssur les utilisateurs est grande, meilleures sont la modélisation et la qualité du service.En pratique, nombre de services rencontrent un problème de manque d'informationssur les utilisateurs. Dans cet article, nous y répondons par médiationinter-domaines de modèles utilisateurs, c'est-à-dire la complétion de modèles enexploitant des données d'un autre domaine. La médiation que nous proposonsrepose sur un transfert d'informations inter-domaines. Ce transfert consiste enl'utilisation de couples invariants ou très corrélés pouvant être des couples deressources ou de descripteurs sémantiques, identifiés après enrichissement sémantiquedes modèles. Nous montrons que le transfert sous forme de couple deressources permet une complétion de qualité et que l'exploitation de descripteurssémantiques augmente la couverture à qualité égale. Enrichir sémantiquementest donc bénéfique pour le transfert inter-domaines.	Emilien Perrin, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001170	http://editions-rnti.fr/render_pdf.php?p=1001170
Revue des Nouvelles Technologies de l'Information	EGC	2012	Validation et optimisation d'une décomposition hiérarchique de graphes	De nombreux algorithmes de fragmentation de graphes fonctionnentpar agrégations ou divisions successives de sous-graphes menant à une décompositionhiérarchique du réseau étudié. Une question importante dans ce domaineest de savoir si cette hiérarchie reflète la structure du réseau ou si elle n'estqu'un artifice lié au déroulement de la procédure. Nous proposons un moyen devalider et, au besoin, d'optimiser la décomposition multi-échelle produite parce type de méthode. On applique notre approche sur l'algorithme proposé parBlondel et al. (2008) basé sur la maximisation de la modularité. Dans ce cadre,une généralisation de cette mesure de qualité au cas multi-niveaux est introduite.Nous testons notre méthode sur des graphes aléatoires ainsi que sur des exemplesréels issus de divers domaines.	Francois Queyroi	http://editions-rnti.fr/render_pdf.php?p1&p=1001182	http://editions-rnti.fr/render_pdf.php?p=1001182
Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers la construction d'un observatoire des pratiques agricoles : gestion et propagation de l'imprécision des données agronomiques	L'un des objectifs d'Observox est de traiter et gérer l'imprécisiondes données agronomiques tant spatialement (parcelles agricoles) et quantitativement(quantités de produits disséminées) et de toujours associer une évaluationde la qualité aux données. Aussi, nous avons choisi le cadre théorique desensembles flous. A partir d'un modèle conceptuel gérant l'imperfection, nousconstruisons une base de données gérant des entités spatiotemporelles imprécisesappelées « entités agronomiques floues ». Cependant, ce choix de représentationrend possible le chevauchement des composantes spatiales entre entités.Dans ce cas, nous propageons l'imprécision du spatial vers le quantitatif àl'aide d'un opérateur de caractère additif qui prend en compte à la fois l'informationspatiale et quantitative, et qui fournit une information quantitative localeet floue. Le système ainsi construit nous permet d'obtenir une représentationfloue des quantités de produits phytosanitaires disséminés à chaque endroit duterritoire étudié.	Asma Zoghlami, Karima Zayrit, Cyril de Runz, Eric Desjardin, Herman Akdag	http://editions-rnti.fr/render_pdf.php?p1&p=1001158	http://editions-rnti.fr/render_pdf.php?p=1001158
Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers une approche efficace d'extraction de motifs spatio-séquentiels	Ces dernières années, l'augmentation de la quantité d'informationsspatio-temporelles stockées dans les bases de données a fait naître de nouveauxbesoins, notamment en matière de gestion des risques naturels, sanitaires ou anthropiques(p. ex. compréhension de la dynamique d'une épidémie de Dengue).Dans cet article, nous définissons un cadre théorique pour l'extraction de motifsspatio-séquentiels, séquences de motifs spatiaux représentant l'évolution dansle temps d'une localisation et de son voisinage. Nous proposons un algorithmed'extraction efficace qui effectue un parcours en profondeur en s'appuyant surdes projections successives de la base de données. Nous introduisons égalementune mesure d'intérêt adaptée aux aspects spatio-temporels de ces motifs. Les expérimentationsréalisées sur des jeux de données réels soulignent la pertinencede l'approche proposée par rapport aux méthodes de la littérature.	Hugo Alatrista Salas, Sandra Bringay, Frédéric Flouvat, Nazha Selmaoui-Folcher, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001159	http://editions-rnti.fr/render_pdf.php?p=1001159
Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers une méthode automatique de construction de hiérarchies contextuelles	Dans de nombreux domaines (e.g., fouille de données, entrepôts dedonnées), l'existence de hiérarchies sur certains attributs peut être extrêmementutile dans le processus analytique. Toutefois, cette connaissance n'est pas toujoursdisponible ou adaptée. Il est alors nécessaire de disposer d'un processusde découverte automatique pour palier ce problème. Dans cet article, nous combinonset adaptons des techniques issues de la théorie de l'information et duclustering pour proposer une technique orientée données de construction automatiquede taxonomies. Les deux principaux avantages d'une telle approchesont son caractère totalement non-supervisé et l'absence de paramètre utilisateurà spécifier. Afin de valider notre approche, nous l'avons appliquée sur desdonnées réelles et avons conduit plusieurs types d'expérimentation. D'abord,les hiérarchies obtenues ont été expertisées pour en examiner le pouvoir informatif.Ensuite, nous avons évalué l'apport de ces taxonomies comme support àdes tâches de fouille de données nécessitant une définition hiérarchique des valeursd'attributs : l'extraction de séquences fréquentes multidimensionnelles etmulti-niveaux ainsi que la construction de résumés de tables relationnelles. Lesrésultats obtenus permettent de conclure quant à l'intérêt de notre approche	Dino Ienco, Yoann Pitarch, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001186	http://editions-rnti.fr/render_pdf.php?p=1001186
Revue des Nouvelles Technologies de l'Information	EGC	2012	Webmarks : Le marquage d'intérêt sur le Web de données	Depuis son apparition au sein du W3C, la définition de la ressourceWeb n'a cessé d'évoluer au delà du simple document. Lieu, service, conceptd'ontologie, représentation d'un objet réel ou non, la ressource web est complexeet il nous a semblé que les outils à disposition des internautes pour sa manipulation,comme les bookmarks par exemple, n'exploitaient pas pleinementces nouvelles dimensions. Dans cet article, nous présenterons le modèle Webmarksqui permet de préciser l'objet du marquage, la ressource, mais égalementl'intérêt de l'auteur de la marque. L'implémentation de ce modèle au sein duprojet ISICIL sera également présentée et nous discuterons de son apport encomparaison des technologies existantes	Nicolas Delaforge, Fabien Gandon	http://editions-rnti.fr/render_pdf.php?p1&p=1001152	http://editions-rnti.fr/render_pdf.php?p=1001152
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Approche préventive de la qualité des données d'importation dans le contexte de la protéomique clinique	Dans le domaine biomédical, la protéomique est confrontée à dessources de données de plus en plus nombreuses et à des volumes de donnéestrès importants du fait de la multiplication des technologies dites à haut débit.L'hétérogénéité de la provenance des données implique de fait une hétérogénéitédans la représentation et le contenu de ces données. Les données peuventaussi se révéler incorrectes ce qui engendre des erreurs sur les conclusions desexpériences protéomiques. Notre approche a pour objectif de garantir la qualitéinitiale des données lors de leur importation dans un système d'informationdédié à la protéomique. Elle est basée sur le couplage entre des modèles représentantles sources et le système protéomique, et des ontologies utilisées commemédiatrices entre les modèles. Les différents contrôles que nous proposons demettre en place garantissent la validité des domaines de valeurs, la sémantiqueet la cohérence des données lors de l'importation.	Pierre Naubourg, Marinette Savonnet, Eric Leclercq, Kokou Yétongnon	http://editions-rnti.fr/render_pdf.php?p1&p=1001118	http://editions-rnti.fr/render_pdf.php?p=1001118
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Catégorisation des mesures d'intérêt pour l'extraction des connaissances	La recherche de règles d'association intéressantes est un domaine derecherche important et actif en fouille de données. Les algorithmes de lafamille Apriori reposent sur deux mesures pour extraire les règles, le support etla confiance. Bien que ces deux mesures possèdent des vertus algorithmiquesaccélératrices, elles génèrent un nombre prohibitif de règles dont la plupartsont redondantes et sans intérêt. Il est donc nécessaire de disposer d'autresmesures filtrant les règles inintéressantes. Des travaux ont été réalisés pourdégager les "bonnes" propriétés des mesures d'extraction des règles et cespropriétés ont été évaluées sur 61 mesures. L'objectif de cet article est dedégager des catégories de mesures afin de répondre à une préoccupation desutilisateurs : le choix d'une ou plusieurs mesures lors d'un processusd'extraction des connaissances dans le but d'éliminer les règles valides nonpertinentes extraites par le couple (support, confiance). L'évaluation despropriétés sur les 61 mesures a permis de dégager 7 classes de mesures, classesobtenues grâce à deux techniques : une méthode de la classification ascendantehiérarchique et une version de la méthode de classification non-hiérarchiquedes k-moyennes.	Sylvie Guillaume, Dhouha Grissa, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001115	http://editions-rnti.fr/render_pdf.php?p=1001115
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Degrés d'équivalence de mesures de comparaison pour données binaires et pour données numériques	Afin d'aider au choix d'une mesure pour comparer des données, problèmeau coeur de la conception de systèmes dans les domaines de la fouillede données, l'apprentissage automatique ou la recherche d'information, nouscomparons les mesures les plus courantes selon l'ordre qu'elles induisent surles données et nous quantifions leur accord par des degrés d'équivalence. Nousproposons une étude systématique des mesures de comparaison appliquées auxdonnées binaires et aux données numériques, en examinant les principales mesuresde similarité, distance et produits scalaires. Nous établissons leurs degrésd'équivalence, en considérant des bases de données artificielles et réelles et identifionsdes mesures équivalentes et quasi-équivalentes, qui peuvent être considéréescomme redondantes dans un cadre de recherche d'information.	Marie-Jeanne Lesot, Maria Rifqi	http://editions-rnti.fr/render_pdf.php?p1&p=1001116	http://editions-rnti.fr/render_pdf.php?p=1001116
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Évaluation de la qualité de données biométriques	L'évaluation de la qualité des données biométriques est un facteur primordialdans le processus biométrique. Dans cet article, nous proposons uneméthode générique pour évaluer la qualité des données biométriques morphologiques.Elle est basée sur l'utilisation conjointe de deux types d'informations: 1)la qualité de l'image, et 2) la qualité des paramètres extraits en utilisant le descripteurScale Invariant Feature Transformation (SIFT). Cinq bases de données(quatre de visages et une d'empreintes digitales), et un système d'authentificationbiométrique ont été utilisés pour quantifier les performances de la méthodeproposée. Les résultats expérimentaux montrent l'intérêt de la méthode proposéepour détecter plusieurs types d'altérations réelles des données, qui ont unimpact majeur sur la performance globale des systèmes biométriques. Les résultatsexpérimentaux montrent également que la méthode proposée est plus efficaceque la méthode NIST Fingerprint Image Quality (NFIQ) pour prédire lesperformances du système biométrique testé.	Mohamad El-Abed, Baptiste Hemery, Christophe Charrier, Christophe Rosenberger	http://editions-rnti.fr/render_pdf.php?p1&p=1001109	http://editions-rnti.fr/render_pdf.php?p=1001109
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Évaluation d'un résultat d'interprétation d'images	Les algorithmes de traitement d'images regroupent un ensemble deméthodes qui vont traiter l'image depuis son acquisition jusqu'à l'extraction del'information utile pour une application donnée. Parmi ceux-ci, les algorithmesd'interprétation ont pour but de détecter, localiser et/ou reconnaître un ou plusieursobjets dans une image. Le problème traité réside dans l'évaluation derésultats d'interprétation d'une image ou une vidéo lorsque l'on dispose de lavérité terrain associée. Les enjeux sont multiples comme la comparaison d'algorithmes,l'évaluation d'algorithmes en cours de développement ou leur paramétrageoptimal. Cet article présente la méthode d'évaluation de la qualité d'unrésultat d'interprétation d'image que nous avons développée. Cette méthode permetde prendre en compte la qualité de la localisation, de la reconnaissance ainsique de la détection des objets. Paramétrable, cette méthode peut être adaptéepour une application particulière. Son comportement a été testé sur une largebase et présente des résultats intéressants.	Baptiste Hemery, Hélène Laurent, Bruno Emile, Christophe Rosenberger	http://editions-rnti.fr/render_pdf.php?p1&p=1001112	http://editions-rnti.fr/render_pdf.php?p=1001112
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Mesure formelle de la robustesse des règles d'association	Nous proposons dans cet article une définition formelle de la robustessepour les règles d'association, s'appuyant sur une modélisation que nous avonsprécédemment définie. Ce concept est à notre avis central dans l'évaluation desrègles et n'a à ce jour été que très peu étudié de façon satisfaisante. Il est crucialcar malgré une très bonne évaluation par une mesure de qualité, une règlepeut être très fragile par rapport à des variations légères des données. La mesurede robustesse que nous proposons dépend de la mesure de qualité utilisée pourévaluer les règles et du seuil d'acceptation minimal. Il est alors possible à partirde ces deux seuls éléments et de la valeur prise par la règle sur la mesure d'évaluersa robustesse. Nous présentons plusieurs propriétés de cette robustesse,montrons sa mise en oeuvre et illustrons celle-ci par les résultats d'expériencessur plusieurs bases de données pour quelques mesures. Nous donnons ainsi unnouveau regard sur la qualification des règles.	Yannick Le Bras, Patrick Meyer, Philippe Lenca, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001113	http://editions-rnti.fr/render_pdf.php?p=1001113
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Motifs corrélés rares : caractérisation et nouvelles représentations concises exactes	Dans la littérature, les travaux se sont principalement focalisés surl'extraction des motifs fréquents. Toutefois, récemment, la fouille des motifsrares s'est avérée intéressante puisque ces motifs permettent de véhiculer desconnaissances concernant des événements rares, inattendus. Ils ont ainsi prouvéleur grande utilité dans plusieurs domaines d'application. Cependant, un constatimportant associé à l'extraction des motifs rares est d'une part leur nombre trèsélevé et d'autre part la qualité faible de plusieurs motifs extraits. Ces dernierspeuvent en effet ne pas présenter des corrélations fortes entre les items les constituant.Afin de pallier ces inconvénients, nous proposons dans cet article d'intégrerla mesure de corrélation bond afin d'extraire seulement l'ensemble des motifsrares vérifiant cette mesure. Une caractérisation de l'ensemble résultant, desmotifs corrélés rares, est alors réalisée en se basant sur l'étude des contraintesde nature différentes induite par la rareté et la corrélation. En outre, en se basantsur les classes d'équivalence associées à un opérateur de fermeture dédié à lamesure bond, nous proposons des représentations concises exactes des motifscorrélés rares.1 Introduction	Souad Bouasker, Tarek Hamrouni, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001114	http://editions-rnti.fr/render_pdf.php?p=1001114
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Recherche de séquences spatio-temporelles peu contredites dans des données hydrologiques	Dans cet article, nous présentons un projet de découverte de connaissancesdans des données hydrologiques. Pour cela, nous appliquons un algorithmed'extraction de motifs séquentiels sur les données relevées au niveau destations réparties le long de plusieurs rivières. Les données sont pré-traitées afinde considérer différentes proximités spatiales et l'analyse du nombre de motifsobtenus souligne l'influence des relations ainsi définies. Nous proposonset détaillons une mesure objective d'évaluation, appelée la mesure de moindrecontradiction temporelle, afin d'aider l'expert dans la découverte de nouveautés.Ces éléments posent les premières bases de travaux plus ambitieux permettantde proposer des indicateurs spatialisés pour l'aide à l'interprétation des donnéesde suivi écologique des cours d'eau et des données de pression.	Hugo Alatrista Salas, Jérôme Azé, Sandra Bringay, Flavie Cernesson, Frédéric Flouvat, Nazha Selmaoui-Folcher, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001117	http://editions-rnti.fr/render_pdf.php?p=1001117
Revue des Nouvelles Technologies de l'Information	MQDC	2012	Un protocole d'évaluation applicative des terminologies bilingues destinées à la traduction spécialisée	Cet article argumente en faveur d'une évaluation applicative des terminologiesbilingues et propose un protocole d'évaluation pour ce type de terminologies. Le protocole est appliqué aux terminologies issues de corpus comparableset l'application envisagée est la traduction humaine spécialisée. Le protocoleconsiste à faire traduire des textes spécialisés dans différentes situations detraduction : sans ressource spécialisée, avec une terminologie issue d'un corpuscomparable, à l'aide d'Internet. La qualité des éléments traduits via ces ressourcesest ensuite comparée, ce qui permet de déterminer la valeur ajoutée desterminologies bilingues dans le cadre d'une tâche de traduction spécialisée.	Estelle Delpech	http://editions-rnti.fr/render_pdf.php?p1&p=1001110	http://editions-rnti.fr/render_pdf.php?p=1001110
Revue des Nouvelles Technologies de l'Information	CAL	2012	A Distributed Test Architecture for Adaptable and Distributed Real-Time Systems	This work focuses on testing the consistency of distributed real-timesystems when their configurations evolve dynamically, called also adaptable systems.In this context, runtime testing which is carried out on the final executionenvironment is emerging as a new solution for the validation of these systems.To reduce testing effort, cost and time, we apply the dependency analysis techniquein order to identify affected parts of the system under test due to runtimereconfiguration. In addition, we propose a flexible and evolvable distributed testarchitecture made of two kinds of testers: Single Component Testers and ComponentComposition Testers. These testers execute unit tests (respectively integrationtests) on the affected components (respectively component compositions) assoon as reconfiguration actions occur. An illustrative example describing interactionsbetween the proposed testers when two reconfiguration scenarios happenis given.	Mariam Lahami, Moez Krichen, Mohamed Jmaiel	http://editions-rnti.fr/render_pdf.php?p1&p=1001804	
Revue des Nouvelles Technologies de l'Information	CAL	2012	Construction automatique d'applications de visualisation scientifique interactive fortement cohérentes	Cet article traite de l'usage de la programmation par composants pourla conception d'applications de visualisation scientifique interactive temps-réel.Notre but est d'automatiser cette conception grâce à une méthode pour construireun réseau de connexion entre les différents composants qui constituent une telleapplication à partir des contraintes fournies par l'utilisateur. Ce type d'applicationsdoit fonctionner le plus rapidement possible tout en préservant la précisionde ses résultats. Ces deux aspects sont souvent en conflit, par exemple lorsqu'ils'agit d'autoriser la perte de messages ou pas. Notre approche vise à trouverautomatiquement le meilleur compromis entre ces deux critères au moment deconstruire l'application.	Sébastien Limet, Sophie Robert, Ahmed Turki	http://editions-rnti.fr/render_pdf.php?p1&p=1001803	
Revue des Nouvelles Technologies de l'Information	CAL	2012	Une approche architecturale à base de composants pour l'implémentation des Systèmes Multi-Agents	Motivés par le développement des Systèmes Multi-Agents (SMA),nous explorons dans cet article la production de supports de développementorientés agent spécialisés en utilisant des architectures logicielles à composants.L'objectif de ce travail est de faciliter le passage de la conception du SMA, entermes de types d'agents et d'interactions, à son implémentation, à l'aide de ceque nous nommons une micro-architecture. Celle-ci est un moyen de prendreen compte les exigences que la conception orientée agent ne considère pas.À l'aide d'un exemple réel, nous mettons en évidence les spécificités des applicationsSMA et les implications architecturales de celles-ci. La principalecontribution de cet article réside dans la définition du modèle de composantsSPEAD (Species-based Architectural Design) qui introduit un type spécifiquede composants, le transverse, qui permet de réaliser l'interconnexion entre lesagents du système et leur plateforme d'exécution. Cette abstraction est complétéepar deux autres, l'espèce et l'écosystème, qui supportent au niveau de lamicro-architecture la réalisation des concepts manipulés dans les SMA. Nousprésentons une implémentation de SPEAD sous forme d'un langage de descriptiond'architectures, utilisable en conjonction avec JAVA. Ce langage est utilisédans notre équipe pour supporter le développement dans le cadre de projets derecherche.	Victor Noël, Jean-Paul Arcangeli, Marie-Pierre Gleizes	http://editions-rnti.fr/render_pdf.php?p1&p=1001801	
Revue des Nouvelles Technologies de l'Information	CAL	2012	Vérification des propriétés structurelles et non fonctionnelles d'assemblages de composants UML2.0	L'approche par composant vise la réutilisation par un assemblage aiséet cohérent des composants. Un assemblage cohérent de composants exige la vérificationdes propriétés liées à la cohérence d'interface, sémantique, de synchronisationet non fonctionnelle. Nous visons la vérification des propriétés structurelleset non fonctionnelles sur un assemblage de composants UML2.0 dotés despropriétés non fonctionnelles (performance, fiabilité, sécurité, sûreté de fonctionnement,etc.) décrites dans un langage de type CQML. Notre approche baséesur des contrats d'assemblage établis entre les composants serveurs et les composantsclients, plaide en faveur de l'utilisation de l'ADL Acme/Armani commemachine de vérification d'un assemblage de composants UML2.0/CQML.	Mourad Kmimech, Mohamed Tahar Bhiri, Mohamed Graiet, Philippe Aniorté	http://editions-rnti.fr/render_pdf.php?p1&p=1001805	
Revue des Nouvelles Technologies de l'Information	CAL	2012	Vers une démarche pour le développement de modèles à base de Composants Multivue	VUML (View based UML) est un langage de modélisation objet qui a introduit les concepts de Classe Multivue et de Composant Multivue dans UML.Cependant, tout comme UML, VUML ne propose pas de méthode pour élaborer le diagramme de Composants Multivue. Pour combler ce manque, nous nous sommes appuyés sur les méthodes UP (Unified Process) et CUP (Component Unified Process) pour proposer une démarche pour le développement d'un PIM (Platform Independent Model) à base de Composants Multivue. Dans cet article, nous décrivons les étapes de cette démarche permettant d'aboutir au diagramme de Composants Multivue.	Mustapha Hain, Abdelaziz Marzak, Bernard Coulette, Mahmoud Nassar	http://editions-rnti.fr/render_pdf.php?p1&p=1001802	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Archivage d'entrpôts de données multidimensionnelles	Les données d'un entrepôt sont rafraîchies périodiquement et conservées de manière permanente. Cependant, les décideurs portent généralement un intérêt moindre pour les données anciennes. Dans cet article, nous proposons un mécanisme permettant de synthétiser les données les plus anciennes. Nous définissons un modèle conceptuel d'archivage de données multidimensionnelles. Nous présentons, ensuite, le modèle logique correspondant et les principes permettant d'interroger des schémas multidimensionnels archivés.	Faten Atigui, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001226	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Cubes de données convexes non-dérivables fermés	De nombreuses approches sont proposées pour pré-calculer des cubes de données afin de répondre efficacement aux requêtes OLAP. La notion de cube de données a été déclinée sous différentes appellations: cubes icebergs, cubes différentiels ou encore cubes émergents. Les cubes convexes permettent de focaliser l'attention de l'utilisateur sur un ensemble particulier de tuples intéressants. Dans cet article, nous étudions les représentations concises des cubes convexes. À cet effet, nous introduisons une nouvelle structure d'un cube de données: le Cube Convexe Non-Dérivable Fermé (CCND-Cube). Ce dernier permet de capturer tous les tuples d'un cube de données satisfaisant une combinaison de contraintes monotones et/ou antimonotones. Les expériences montrent que notre proposition fournit la représentation la plus compacte d'un cube de données de manière à optimiser à la fois le temps de calcul ainsi que l'espace de stockage nécessaire.	Hanen Brahmi, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001223	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Exploitation de l'interaction des requêtes OLAP pour la gestion de cache et l'ordonnancement de traitements	Le cache est l'une des composantes principales d'un système de gestion de bases de données (SGBD). Les SGBD manipulant des bases de données volumineuses comme les entrepôts de données stockent souvent les données sur le disque. En conséquence, l'interrogation nécessite un transfert des données du disque vers la mémoire centrale via le tampon. Un nombre important de travaux sur la gestion de tampon ont été proposés. Malheureusement, ils supposent que les requêtes soient ordonnées. Dans le contexte des entrepôts de données, les requêtes interagissent du fait qu'elles utilisent la table des faits. Cette interaction pourrait impacter la gestion de cache et offrir un bon ordonnancement de requêtes. Dans cet article, nous proposons d'étudier conjointement le problème de gestion de tampon (BMP) et le problème d'ordonnancement de requêtes (QSP). Trois algorithmes sont proposés pour résoudre le problème conjoint. Finalement, un simulateur et une validation sous Oracle11G sont proposés.	Amira Kerkad, Ladjel Bellatreche, Dominique Geniet	http://editions-rnti.fr/render_pdf.php?p1&p=1001228	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Integrating Database Systems and Data Mining Algorithms		Carlos Ordonez	http://editions-rnti.fr/render_pdf.php?p1&p=1001210	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Matérialisation partielle et interrogation d'un hypercube de données dynamiques	Les entrepôts de données ont généralement une stratégie de chargement des données par bloc et hors ligne ce qui les rendent peu compatibles avec des applications où les performances en temps sont critiques. Dans cet article, nous présentons un modèle multidimensionnel pour entreposer en temps réel les données d'un espace multidimensionnel hiérarchique. Nous proposons une matérialisation partielle de l'hypercube de données dans une structure d'arbre qui regroupe les données multidimensionnelles dans des partitions non ordonnées appelées Minimum Bounding Spaces (MBS). Nous présentons le principe des algorithmes d'insertion d'un nouveau fait et de requêtage. Nous évaluons la performance de notre solution en utilisant le Star Schema Benchmark. L'étude expérimentale montre que notre proposition est particulièrement performante à la fois en temps d'insertion et pour le traitement des requêtes.	Usman Ahmed,  Anne Tchounikine, Maryvonne Miquel	http://editions-rnti.fr/render_pdf.php?p1&p=1001212	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Modélisation des bases de données multidimensionnelles à agrégations multiples et différentiées	De nombreux modèles ont été proposés pour représenter les données multidimensionnelles. Ces propositions considèrent généralement une même fonction d'agrégation pour déterminer les valeurs d'une mesure aux différents niveaux de granularité de l'espace multidimensionnel. Nous proposons un nouveau modèle conceptuel plus flexible supportant des agrégations multiples différentiées. L'agrégation multiple permet d'associer à une même mesure, une fonction d'agrégation différente pour chaque dimension. L'agrégation différentiée autorise des agrégations spécifiques à chaque paramètre. Notre modèle repose sur un double formalisme graphique suffisamment expressif pour contrôler la validité des fonctions d'agrégation. Nous étudions également les conséquences de cette modélisation conceptuelle pour la construction efficace des treillis de pré-agrégats dans le contexte R-OLAP.	Ali Hassan, Franck Ravat, Olivier Teste, Ronan Tournier, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001215	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Multidimensional skylines		Chedy Raïssi	http://editions-rnti.fr/render_pdf.php?p1&p=1001217	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Natural language interfaces for data warehouses	Data warehouses process a huge mass of data, that are often modeled in a way humans hardly understand. In order to make the search paradigm more accessible to end users, some efforts have been made in the field of Business Intelligence. However, expressing information need as a structured query is still an artificial task. This explains why "natural" approaches are preferred nowadays. In this paper, we present a Question Answering (Q&A) system for structured data in a context of BI. Some benefits of our proposal are described through an iPhone/iPad application and through an HTML prototype.	Nicolas Kuchmann-Beauger, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001218	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Nouvelle approche de corrélation d'alertes basée sur la fouille multidimensionnelle	En réponse aux problèmes posés par la complexité croissante des réseaux et des attaques, les Systèmes de Détection d'Intrusions (SDIs) constituent une bonne alternative pour mieux sécuriser un système informatique. Cependant, les SDIs existants présentent des lacunes en terme de génération excessive d'alertes. Réellement, la majorité de ces alertes ne correspondent pas à des attaques (fausses alertes, alertes redondantes, etc.). Ainsi, la corrélation d'alertes est un processus d'analyse appliqué à des journaux d'alertes. Dans cet article, nous proposons une nouvelle approche pour la corrélation d'alertes basée sur le couplage entre la fouille de données et les outils OLAP (On Line Analytical Processing). L'idée intuitive derrière cette approche est de profiter des avantages de la fouille de données multidimensionnelles afin de rehausser l'analyse des alertes et introduire une solution puissante pour faire face aux défauts des SDIs. Les expérimentations, que nous avons menées, montrent l'efficacité de notre nouvelle méthode de corrélation d'alertes.	Hanen Brahmi, Imen Brahmi, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001222	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Problématique de la gesion des grandes masses de données dans le domaine de la maintenance aéronautique		Franck Duluc	http://editions-rnti.fr/render_pdf.php?p1&p=1001225	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Qualité de données dans les entrepôts de données : élimination des similaires	Ce papier aborde la problématique de l'élimination des similaires (doublons non stricts) dans un entrepôt de données. En effet, la notion de la qualité de données présente un très grand enjeu pour une bonne gouvernance des données afin d'améliorer les interactions entre les différents collaborateurs d'une ou plusieurs organisations concernées. La présence de données en double ou similaires engendre des préoccupations importantes autour de la qualité des données. Un panorama des méthodes de calcul de distance de similarité entre les données ainsi que des algorithmes d'élimination des similaires sont exposés et comparés.	Faouzi Boufarès, Aïcha Ben Salem, Sebastiao Correia	http://editions-rnti.fr/render_pdf.php?p1&p=1001213	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Sélection incrémentale d'un schéma de fragmentation horizontale d'un entrepôt de données relationnel	La fragmentation horizontale permet de réduire la complexité des requêtes décisionnelles exécutées sur un entrepôt de données relationnel. Elle se base sur le principe de réorganisation des données qui ne nécessite pas un espace de stockage supplémentaire. Cependant, sélectionner un schéma de fragmentation horizontale d'un entrepôt n'est guère une tâche facile, vu l'espace de recherche très complexe à exploiter. Les algorithmes existants sélectionnent un schéma de fragmentation lors de la phase de conception physique d'un entrepôt, afin d'optimiser une charge de requêtes préalablement connue. Ces algorithmes ne prendre pas en considération les changements au niveau des requêtes. Dans cet article, nous proposons d'effectuer une sélection d'un schéma de fragmentation dite incrémentale basée sur les algorithmes génétiques. Notre approche permet l'optimisation de l'exécution de la charge de requêtes décisionnelles et l'adaptation du schéma de fragmentation aux changements de la charge. Nous réalisons une étude expérimentale qui montre l'intérêt de la sélection incrémentale d'un schéma de fragmentation.	Rima Bouchakri, Ladjel Bellatreche	http://editions-rnti.fr/render_pdf.php?p1&p=1001211	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Summarizing former sessions for user-centric OLAP	We propose a framework for summarizing former analyses to assist the user exploring a data cube. In this framework, simple operators are used for automatically summarizing log files consisted of sequences of unevaluated OLAP queries. We provide a simple implementation of the framework for summarizing logs of OLAP queries, and we test it with respect to a query personalization technique based on mining a query log.	Julien Aligon, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1001227	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Un cadre conceptuel basé sur UML et Spatial OCL pour la définition des contraintes d'intégrité dans les systèmes SOLAP	Les entrepôts de données spatiales et les systèmes Spatial OLAP sont des technologies d'aide à la décision permettant l'analyse en ligne des gros jeux de données géo-référencées. Dans un tel type de systèmes, la qualité de l'analyse dépend : de la qualité des données entreposées, de comment les agrégations sont effectuées, et de comment les données entreposées sont explorées. Dans cet article, nous proposons une méthode pour garantir la qualité de ces trois facteurs, fondée sur un profil UML et sur des contraintes d'intégrité définies avec Spatial OCL. Afin de valider notre proposition, nous proposons également une implémentation automatique dans une architecture ROLAP.	Sandro Bimonte, François Pinet, Kamal Boulil	http://editions-rnti.fr/render_pdf.php?p1&p=1001214	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Un entrepôt de données pour l'analyse de la recharge des véhicules electriques : un retour d'expérience	La technologie de l'entreposage de données a contribué largement au développement des entreprises de grande distribution (comme Walmart, Ohlinger (2006)). Dans cet article, nous présentons un retour d'expérience de développement d'entrepôt stockant des données issues des expérimentations véhicules électriques au sein de l'entreprise Electricité de France (EDF). Cet entrepôt constitue le coeur de la plateforme EDF-MINERVE 1 c chargée de collecter, stocker et d'analyser les données des expérimentations. Premièrement, nous présentons la chaîne d'acquisition des données et sa validation, puis nous détaillons nos attentes concernant l'exploitation des données. Finalement, nous exposons les raisons qui nous ont amenés à choisir la solution des entrepôts de données ainsi que sa mise en oeuvre pour répondre à la problématique d'analyse temporelle multi-critères des données de charge.	Kevin Royer, Ladjel Bellatreche, Anne Le-Mouel, Gilbert Schmitt	http://editions-rnti.fr/render_pdf.php?p1&p=1001224	
Revue des Nouvelles Technologies de l'Information	EDA	2012	Une approche connexionniste pour l'extension de l'OLAP à des capacités de prédiction	Les outils de l'analyse en ligne (OLAP) permettent à l'utilisateur de réaliser des tâches exploratoires dans les cubes de données. Cependant, ils n'offrent aucun moyen pour la prédiction ou l'explication des faits. En vue de renforcer le processus de l'aide à la décision, plusieurs travaux ont proposé l'extension de l'analyse en ligne à des capacités plus avancées. Dans cet article, nous proposons une nouvelle approche d'extension de l'analyse en ligne à des capacités de prédiction à deux phases. La première est une phase de réduction des dimensions des cubes de données, qui repose sur l'analyse en composantes principales (ACP). La deuxième est une phase de prédiction dans laquelle nous introduisons une nouvelle architecture de percéptrons multicouches (PMC). Notre étude expérimentale a montré une capacité de prédiction prometteuse, ainsi qu'une bonne robustesse dans le cas d'un cube épars.	Wiem Abdelbaki, Sadok Ben Yahia, Riadh Ben Messaoud	http://editions-rnti.fr/render_pdf.php?p1&p=1001216	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Cartographie de la chronique d'Henri de Livonie	La chronique d'Henri de Livonie est la principale et la plus ancienne source écrite à notre disposition en ce qui concerne l'évangélisation des actuels Pays Baltes. Nous nous proposons d'étudier par des méthodes simples issues des statistiques ou de la théorie des graphes la cartographie de la Livonie que cette chronique nous dessine, dans l'espoir d'en tirer quelques intuitions tant sur la rédaction du texte que sur les évènements qu'il relate.	Nicolas Bourgeois	http://editions-rnti.fr/render_pdf.php?p1&p=1001810	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Découplage de système lent/rapide appliqué en Economie et Econophysique	La compréhension des phénomènes économiques nécessite de prendre en compte plusieurs échelles de temps simultanément. Nous étudions le cas d'un modèle simple d'épargne, où plusieurs échelles de temps caractéristiques coexistent. Nous montrons qu'il est possible de séparer les contributions lentes et rapides confondues dans une même variable observée en nous appuyant d'une part sur une linéarisation de la dynamique (stochastique et nonlinéaire) autour d'un point d'équilibre, et d'autre part sur un découplage via la transformation de Chang, issue de la théorie de la commande.Cette approche est originale car elle s'appuie non pas sur les méthodes des séries temporelles (décomposition en tendance, saisons et résidu) mais sur une étude analytique.Dans un second temps, nous abordons le problème de l'agrégation du comportement d'un grand nombre d'agents, sous l'angle du découplage des composantes lentes/rapides. Nous rendons compte des critiques adressées dans la littérature aux modèles DSGE et examinons la possibilité de découpler un système multi-agents, puis un modèle éconophysique de condensation de la richesse.	Aurélien Hazan	http://editions-rnti.fr/render_pdf.php?p1&p=1001806	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Evolving informal risk-sharing cooperatives and other-regarding preferences	In this paper we present a model of formation and destruction of informal cooperatives in a population of agents who perform a risky activity and who are heterogeneous in terms of success in their actions. Although some agents have high-risk and others low-risk, our model displays a dynamics with cooperatives in which agents share equally their income with a certain stability.We are interested in studying at the same time the existence of cooperatives, their ability to integrate a large proportion of agents and the degree of segregation of these cooperatives. Three factors can explain the existence, stability and lack of segregation. First, we show that the classical explanation in economics holds within the framework of our model: when agents are risk averse, high success agents can share with low success agents so that to stabilize the value of their income - the higher the risk aversion, the more stable the cooperatives and the lower the segregation. Learning can explain in a small proportion the existence of cooperatives: we designed agents so that they have to learn whether they are high or low-risk, and while they are learning, they tend to create cooperatives that can last. Eventually we worked on the integration of other-regarding preferences in the model, with two different definitions. As expected, the influence of other-regarding preferences is to increase stability and decrease segregation, and the two models of rationality react differently to the type of network in which the agents are immersed. This paper, mainly exploratory, presents our model and shows the influence of the definition of network as well as all other factors presented before. In that sense, although we have mainly done a rough exploration of its relevant parameters for the moment, it exposes different insights that can be gained by its study.	Renaud Bourlès, Juliette Rouchier	http://editions-rnti.fr/render_pdf.php?p1&p=1001808	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Intérêt de la simulation centrée interactions pour les sciences humaines et sociales	La simulation informatique a permis aux disciplines dépourvues de la possibilité de réaliser des expériences de se doter d'outils leur permettant d'évaluer leurs hypothèses et leurs modèles. Elle reste toutefois cantonnée à l'intégration numérique de modèles mathématiques ou stochastiques, alors que ses méthodes actuelles, à travers la simulation centrée individu, permettent une représentation réaliste des entités d'un système et de leurs comportements, afin de comprendre les mécanismes qui concourent à la production d'un phénomène donné. Nous montrons ici ce que peut apporter l'agentification des modèles de simulation aux sciences humaines et sociales. En outre nous en présentons une spécialisation, les simulations centrées interactions, qui visent à faciliter l'expression des connaissances, la construction incrémentale des modèles et leur révision, tout en réduisant le risque de biais d'implémentation. Cette méthode permet également d'explorer des variantes d'un même modèle et de décomposer des systèmes complexes multi-échelles.	Philippe Mathieu, Sébastien Picault	http://editions-rnti.fr/render_pdf.php?p1&p=1001807	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Les réseaux sociaux : un regard critique	L'expression « réseaux sociaux » est essentiellement utilisée par des personnes extérieures au champ de la sociologie; elle induit par ailleurs une dimension spatiale dans la lecture des échanges entre personnes, ce qui en fait une notion complexe. Les fondateurs de la sociologie (Simmel, Durkheim) avaient déjà conscience de cette difficulté et tous deux s'accordaient sur le pouvoir coercitif des faits sociaux. Pourtant, les utilisateurs de la notion de réseau social négligent autant ces déterminismes sociologique et politique que la réflexion sur le concept de territoire qui, lui aussi, articule le social et le spatial.Cette tendance s'explique par l'entrée de deux types d'acteurs : les professionnels de l'écriture actuelle (physiciens et informaticiens) et les publicitaires. Les premiers transforment l'objet « réseau social » du fait de leurs compétences techniques (scribales) et de leurs représentations en matière de sciences sociales. Les seconds favorisent une démarche utilitariste (repérage de clients) et promeuvent des discours enchanteurs qui censurent toute description des rapports de force économiques et politiques.Le monde de la recherche subit désormais ce type d'influence. Parfois, il accompagne de tels discours d'escorte et adopte les représentations de la notion de communauté issues du marketing. Le travail scientifique s'en trouve menacé.	Éric Guichard	http://editions-rnti.fr/render_pdf.php?p1&p=1001812	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Setting bounds in a homogeneous corpus: a methodological study applied to medieval literature	The authors present here an exploratory and unspecific method that does not necessitate any a priori on the data - or any heavy transformation such as lemmatisation- that would have to be understood as a first step in the apprehension of a corpus. After a first phase of calibration, based on a control sample, the authors will introduce a method which heuristic value is to bring out, at different levels, internal divisions of different kinds (diachronic, diatopic, related to authorship or scribes, ... ), that can then be analysed specifically. The authors illustrate this method by applying it to a corpus of Occitan medieval texts, the vidas. The corpus's authors and origins are in good part unknown.	Jean-Baptiste Camps, Florian Cafiero	http://editions-rnti.fr/render_pdf.php?p1&p=1001809	
Revue des Nouvelles Technologies de l'Information	MASHS	2012	Spatial correlation in bipartite networks: the impact of the geographical distances on the relations in a corpus of medieval transactions	In this article, the influence between a spatial information and interactions between individuals is addressed. This issue is illustrated through the analysis of a corpus of notarial acts established during the Middle Ages. In this corpus, the persons interact in common transactions that are geolocalized. The present work tries to quantify the impact of this spatial information on the relations between people. As the spatial information as well as the relations between individuals are derived from the same source (the transactions in which the individuals have been involved), a standard Mantel test (Mantel, 1967) is not suited to address this issue. A similar methodology, based on the adaptation of the original permutation test, is thus proposed and illustrated in that context.	Nathalie Villa-Vialaneix, Bertrand Jouve, Fabrice Rossi, Florent Hautefeuille	http://editions-rnti.fr/render_pdf.php?p1&p=1001811	
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2011	Méthodes de traitement statistique des données issues d'une épreuve de tri libre.	En évaluation sensorielle, l'épreuve de catégorisation dite 'tri libre' connaît une popularité grandissante du fait de sa facilité de mise en oeuvre et sa capacité de procurer une représentation pertinente des différences perceptives entre des produits. Plusieurs méthodes de traitement statistique des données issues d'une épreuve de catégorisation ont été proposées. L'objectif est de faire une étude bibliographique des méthodes utilisées et de les comparer sur la base d'un exemple afin d'orienter l'utilisateur dans son choix d'une stratégie d'analyse.	Pauline Faye, Philippe Courcoux, El Mostafa Qannari, Agnès Giboreau	http://editions-rnti.fr/render_pdf.php?p1&p=1001783	http://editions-rnti.fr/render_pdf.php?p=1001783
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2011	Richesse et complexité des données fonctionnelles.	Les progrés récents en matière de stockage et de traitement des données se traduisent de plus en plus fréquemment dans de nombreux domaines scientifiques par la présence de données de type fonctionnel (courbes, images, ...). Les défis proposés aux statisticiens pour appréhender ce type de données ont abouti depuis quelques années à la construction de nombreuses méthodes statistiques. Il se trouve que la complexité de ce type de données amène une richesse d'information qu'une méthode statistique (aussi sophistiquée soit elle) arrive difficilement à capter, tandis que des techniques de boosting capables d'utiliser les complémentarités de différentes méthodes se révèlent souvent plus performantes. L'objectif de ce travail est d'illustrer ce point de vue au travers d'un problème couramment rencontré en pratique: celui de la prévision d'une variable réponse réelle à partir d'une variable explicative fonctionnelle. Un rapide tour d'horizon des méthodes habituellement utilisées sera effectué, et leur complémentarité sera mise en évidence au travers d'un jeu de données issu d'un problème de chimie quantitative.	Frédéric Ferraty, Philippe Vieu	http://editions-rnti.fr/render_pdf.php?p1&p=1001784	http://editions-rnti.fr/render_pdf.php?p=1001784
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2011	Structure fractale d'un dendrogramme.	La structure fractale présentée par la polygonale formée par les unions des points moyens des sommets, des noeuds ou des crêtes des classes terminales est présentée. Son fractal générateur, les particularités de sa construction et la façon de mesurer ses segments sont définis. La propriété d'échelle inverse qu'il possède, le type de maillage, sa propriété de symétrie axiale et un théorème sur la transformation d'affinité linéaire sont posés. Enfin, deux applications avec des données réelles servent d'exemple.	Francisco Casanova del Angel	http://editions-rnti.fr/render_pdf.php?p1&p=1001785	http://editions-rnti.fr/render_pdf.php?p=1001785
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Analyse de réseaux sociaux et Recommandation de contenus non populaires	Nous présentons une méthode adressant le problème de la recommandation dans la Longue Traîne, et plus généralement celui du démarrage à froid. Les contenus non populaires, peu annotés, sont difficiles à recommander. Notre originalité est duale et repose sur le fait de capturer la richesse d'annotations du Web Social d'une part, et d'autre part, d'exploiter le fait que les internautes, via un réseau social, sélectionnent eux-mêmes leurs prescripteurs de contenus. La méthode Social Popularity détecte des communautés dans un réseau social de fans de cinémas, puis calcule, contextuellement à ces communautés, des similarités entre films rares. La méthode montre des résultats préliminaires intéressants, elle permet d'augmenter notablement le taux de rappel en retrouvant d'avantage de films sélectionnés par les utilisateurs (les vrais positifs). La précision reste globalement faible comme les autres méthodes testées, ce qui montre qu'il est très difficile de diminuer le nombre de prédictions fausses.	Cécile Bothorel	http://editions-rnti.fr/render_pdf.php?p1&p=1001099	http://editions-rnti.fr/render_pdf.php?p=1001099
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Bipartitionnement d'un tableau de contingence	La recherche simultanée de partitions sur l'ensemble de lignes et l'ensemble de colonnes d'un tableau de données a donné naissance à des méthodes de classification simultanée ou bipartitionnement. On parle aussi de la classification croisée ou la classification par blocs. Plusieurs algorithmes de bipartitionnement ont été proposés dans la littérature selon le type de tableau des données. Nous nous intéressons dans ce papier à l'algorithme Croki2 de classification croisée des tableaux de contingence. Nous proposons dans ce papier une variante plus rapide de cet algorithme que nous comparons à la version originale à travers des expérimentations sur des données présentant une structure de biclasses générées artificiellement selon une méthodologie que nous détaillons.	Malika Charrad, Yves Lechevallier, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1001103	http://editions-rnti.fr/render_pdf.php?p=1001103
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Classification incrémentale supervisée : un panel introductif	Les dix dernières années ont été témoin du grand progrès réalisé dans le domaine de l'apprentissage statistique et de la fouille de données. Il est possible à présent de trouver des algorithmes d'apprentissage efficaces et automatiques. Historiquement les méthodes d'apprentissage faisaient l'hypothèse que toutes les données étaient disponibles et pouvaient être chargées en mémoire pour réaliser l'apprentissage. Mais de nouveaux domaines d'application de la fouille de données émergent telles que : la gestion de réseaux de télécommunications, la modélisation des utilisateurs au sein d'un réseau social, le web mining... La volumétrie des données explose et il est nécessaire d'utiliser des algorithmes d'apprentissage incrémentaux. Cet article a pour but de présenter les principales approches de classification supervisée incrémentale recensées dans la littérature. Il a pour vocation de donner à un lecteur débutant des indications de lecture sur ce sujet; sujet qui connaît déjà des applications industrielles.	Christophe Salperwyck, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001104	http://editions-rnti.fr/render_pdf.php?p=1001104
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Construction Incrémentale de Graphes de Voisinage avec Accès Réduit aux Disques	La recherche efficace de voisinage dans un espace multidimensionnel a été étudiée dans plusieurs domaines tels que la reconnaissance de formes et l'exploration de données. Les graphes de voisinage sont des structures géométriques basées sur le concept de proximité des éléments de données, et ils peuvent être utilisés pour déterminer les plus proches voisins de données multidimensionnelles. Le problème est que, outre la construction couteuse de ces graphes, leur mise à jour est aussi difficile en raison de l'insertion ou la suppression d'un élément. Dans cet article nous proposons d'adapter les graphes de voisinage pour l'indexation de plus grandes quantités de données multidimensionnelles. Nous proposons une modification d'une méthode existante de mise à jour locale de graphes de voisinage de telle sorte à ce qu'elle puisse considérer des accès réduits aux disques afin de prendre en compte l'évolution des bases de données. Cette proposition est étendue pour faire de la construction incrémentale de graphes de voisinage. Des évaluations de l'approche ont été effectuées et les résultats montrent que notre approche est prometteuse.	Hakim Hacid, Tetsuya Yoshida	http://editions-rnti.fr/render_pdf.php?p1&p=1001102	http://editions-rnti.fr/render_pdf.php?p=1001102
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Détection de communautés dans les graphes bipartites	La recherche de communautés est un problème important pour de nombreux problèmes d'analyse des réseaux sociaux. Nous nous intéressons dans ce travail à la détection de communautés en utilisant uniquement le graphe des relations, dans la lignée des travaux de Newman. Le problème se formule donc comme la recherche de la partition du graphe maximisant un critère de qualité, comme la modularité. Le traitement des graphes bipartites est important pour de nombreuses applications (clients achetant des produits, objets associés à des étiquettes, etc.). La modularité proposée par Newman ne peut pas s'appliquer au cas des graphes bipartites, aussi plusieurs variantes ont été récemment proposées. Dans cet article, nous présentons deux formulations de critères et un algorithme d'optimisation heuristique similaire à celui de Louvain. Des résultats sur des graphes synthétiques et naturels sont présentés et discutés.	The Anh Dang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001101	http://editions-rnti.fr/render_pdf.php?p=1001101
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Formulation Condorcéenne du critère de la modularité	La mesure de modularité a été utilisée récemment pour la classification de graphes (Newman et Girvan, 2004), (Agarwal et Kempe, 2008). Dans ce papier, nous montrons que la mesure de modularité peut être formellement étendue pour la classification non supervisée des données catégorielles. Nous établissons également des connexions entre le critère de modularité et celui de l'analyse relationnelle qui est basé sur le critère de Condorcet. Nous développons ensuite un algorithme efficace inspiré de l'heuristique de l'analyse relationnelle pour trouver la partition optimale maximisant le critère de modularité. Les résultats expérimentaux montrent l'efficacité de notre approche.	Lazhar Labiod, Nistor Grozavu, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001106	http://editions-rnti.fr/render_pdf.php?p=1001106
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Prévision des trajectoires d'avions par les méthodes d'apprentissage automatique : Approche par CART et forêts aléatoires	La forte croissance du trafic aérien dans l'espace européen (5% par an) indique que le contrôle aérien à l'avenir devra faire face à un nombre d'avions de plus en plus important. La prévision de l'incertitude sur les trajectoires des avions s'impose alors comme une nécessité opérationnelle. Cet article vise un double objectif : (i) A partir des données réelles du trafic aérien, nous montrons que les méthodes d'apprentissage automatique CART et les forêts aléatoires permettent de réaliser les prévisions efficaces des instants de passage des avions en des points de leur trajectoire. Nous quantifions le pouvoir prédictif du modèle construit par la méthode CART et de celui des forêts aléatoires. Ces modèles sont évalués sur les données test. (ii) Nous montrons que sous certaines conditions, le modèle des forêts aléatoires présente un risque de surapprentissage.	Norbert Fouemkeu, Nour-Eddin El Faouzi, Jacques Sau, Rémy Fondacci	http://editions-rnti.fr/render_pdf.php?p1&p=1001105	http://editions-rnti.fr/render_pdf.php?p=1001105
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Un Modèle de Diffusion de l'Information dans les Réseaux Sociaux	Les réseaux sociaux sont un outil que les gens utilisent de plus en plus pour communiquer et partager de l'information. Un certain nombre d'études ont été effectuées, sur les réseaux sociaux, la propagation de l'innovation et les maladies afin de comprendre et de modéliser la diffusion dans des graphes d'utilisateurs. Dans un premier temps, nous présentons ici un modèle de diffusion de l'information dans les graphes de contenu alliant aussi bien l'influence des voisins que celle de la proximité avec le contenu, avant d'illustrer notre modèle par des exemples de diffusion sur des réseaux générés manuellement et des réseaux réels. Puis, dans une seconde partie, nous introduisons une dynamique de groupe afin de considérer ensemble les utilisateurs similaires au sein du réseau social.	Cédric Lagnier, Éric Gaussier	http://editions-rnti.fr/render_pdf.php?p1&p=1001100	http://editions-rnti.fr/render_pdf.php?p=1001100
Revue des Nouvelles Technologies de l'Information	AAFD	2011	Une approche de co-classification automatique à base des cartes topologiques	Nous présentons dans ce papier une nouvelle approche de co-classification automatique sur les tableaux de données continues. Cette approcheest basée sur les cartes topologiques de Kohonen que nous appelons Bi-SOM(Bi-clustering based on one Self-Organizing Map). En outre de la question principalede la co-classification automatique liée au traitement simultané des ligneset des colonnes d'une matrice de données, nous proposons dans cette approchede répondre à plusieurs problématiques liées à cette tâche, à savoir: (1) la visualisationtopologiques de bi-clusters avec une notion de voisinage, (2) l'optimisationde ces dits bi-clusters dans des macro-blocs et (3) la réduction de dimensionpar élimination itérative de blocs de "bruit". Enfin, nous présentons des résultatsissus des expérimentations faites sur plusieurs bases de données réelles etd'autres synthétiques pour valider notre approche en comparaison avec d'autresméthodes de co-classification automatique.	Kais Allab, Khalid Benabdeslem, Alexandre Aussem	http://editions-rnti.fr/render_pdf.php?p1&p=1001098	http://editions-rnti.fr/render_pdf.php?p=1001098
Revue des Nouvelles Technologies de l'Information	EDA	2011	A survey of query recommendation techniques for datawarehouse exploration	Lots of data are gathered in datawarehouses that are navigated andexplored for analytical purposes. Only recently has the problem of recommendinga datawarehouse query to a datawarehouse user attracted attention. In thispaper, we propose a simple formal framework for expressing datawarehousequery recommendations. We propose to see the problem of recommending adatawarehouse query for exploration purposes as a function computing a setof queries and associated ratings given a query log, a session, a user profile, adatawarehouse instance, and an expectation function. The rating computed indicatesthe usefulness of each query for a session. With this viewpoint, we reviewand categorize the few techniques that, to the best of our knowledge, have beenproposed and we illustrate them with a case study	Elsa Negre, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1001088	http://editions-rnti.fr/render_pdf.php?p=1001088
Revue des Nouvelles Technologies de l'Information	EDA	2011	Analyse de gazouillis en ligne	Les tweets échangés sur Internet constituent une source d'information importante même si leurs caractéristiques les rendent difficiles à analyser (140 caractères au maximum, notations abrégées, ...). Dans cet article, nous définissons un modèle d'entrepôt de données permettant de valoriser et d'analyser de gros volumes de tweets en proposant des mesures pertinentes dans un contexte de découverte de connaissances. L'utilisation des entrepôts de données comme outil de stockage et d'analyse de documents textuels n'est pas nouvelle mais les mesures ne sont pas adaptées aux spécificités des données manipulées. Les résultats des expérimentations sur des données réelles soulignent la pertinence de notre proposition.	Sandra Bringay, Nicolas Béchet, Flavien Bouillot, Pascal Poncelet, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001085	http://editions-rnti.fr/render_pdf.php?p=1001085
Revue des Nouvelles Technologies de l'Information	EDA	2011	Dépendances fonctionnelles et matérialisation partielle des cubes de données	La sélection de vues à matérialiser dans des entrepôts de données deplus en plus volumineux est une nécessité. Dans cet article, nous montrons qu'ilexiste un lien très étroit entre recherche des cuboïdes à matérialiser dans un cubede données afin d'optimiser les traitements et les dépendances fonctionnelles surcelui-ci. La contrainte que nous imposons sur les vues que l'on matérialise neporte pas sur une borne d'espace de stockage à ne pas dépasser comme c'estle cas dans la plupart des travaux relatifs, mais elle porte sur le facteur de performancef que celles-ci vérifient. Nous tentons cependant d'utiliser le moinsd'espace mémoire pour atteindre cet objectif. Nous caractérisons formellementtoute solution optimale (en terme d'espace mémoire) répondant à ce critère. Onprouve que ce problème est NP-difficile et on démontre l'efficacité de nos algorithmesgloutons pour répondre à ce problème en respectant la contrainte deperformance fixée par l'utilisateur.	Eve Garnaud, Sofian Maabout, Mohamed Mosbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001090	http://editions-rnti.fr/render_pdf.php?p=1001090
Revue des Nouvelles Technologies de l'Information	EDA	2011	L'analyse OLAP par la cartographie... d'hier à aujourd'hui...		Marie-Josée Proulx	http://editions-rnti.fr/render_pdf.php?p1&p=1001076	http://editions-rnti.fr/render_pdf.php?p=1001076
Revue des Nouvelles Technologies de l'Information	EDA	2011	Métadonnées de personnalisation dans le système SelfStar	Pour concevoir une base décisionnelle, un décideur fait généralementappel à un spécialiste : concepteur de bases de données multidimensionnelles,cogniticien ou informaticien. Celui-ci élabore un schéma multidimensionnel entermes de faits à analyser et d'axes d'analyse selon une démarche ascendante,descendante ou mixte. Ce processus, qui s'appuie sur les sources de données àanalyser et le cas échéant sur les besoins des décideurs, s'avère complexe etsouvent approximatif. Le projet SelfStar vise à définir des mécanismes et desoutils permettant à un décideur d'élaborer lui-même un schémamultidimensionnel. Pour assister et guider le décideur, SelfStar alimente puisutilise des métadonnées personnalisées. Dans le processus d'élaboration d'unebase décisionnelle, ces métadonnées vont aider l'utilisateur en guidant seschoix. Cet article présente les principes et les algorithmes qui génèrent lesmétadonnées associées à chaque décideur et les implantées dans un prototype.	Fatma Abdelhédi, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001089	http://editions-rnti.fr/render_pdf.php?p=1001089
Revue des Nouvelles Technologies de l'Information	EDA	2011	Modèle unifié pour la transformation des schémas en constellation	Au cours de ces dernières années, plusieurs approches ont abordé la modélisation et le développement des Entrepôts de Données (ED). La plupart de ces approches fournit des solutions partielles qui traitent soit la modélisation multidimensionnelles, soit la modélisation des processus d'Extraction-Transformation-Loading (ETL). Toutefois, peu de travaux ont visé à unifier ces deux problématiques dans un cadre structuré ou à automatiser le processus d'entreposage. Afin de pallier ces limites, nous proposons dans ce papier une démarche unifiée et semi-automatique qui intègre la modélisation des ED et des processus ETL. Cette démarche est définie dans le cadre d'une Architecture Dirigée par les Modèles (MDA). Elle permet (i) de formaliser les besoins des décideurs, ensuite (ii) de générer les modèles conceptuel, logique et physique de l'ED et des processus ETL conjoints (iii) ainsi que les codes de création et d'alimentation (ETL) des structures multidimensionnelles. Les règles de transformations entre modèles sont formalisées en Query/View/Transformation.	Faten Atigui, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001077	http://editions-rnti.fr/render_pdf.php?p=1001077
Revue des Nouvelles Technologies de l'Information	EDA	2011	Modélisation multidimensionnelle d'entrepôts de documents XML répartis	De nos jours, et avec l'ouverture des organisations sur Internet, les documents constituent une source intéressante pour les analyses décision-nelles; ils aident les décideurs à mieux comprendre l'évolution des processus métier de leur organisation. Généralement, ces documents existent sous format XML, sont géographiquement répartis et décrits par des structures différentes. Cet article propose une méthode de construction d'entrepôts de documents dis-tribués comportant deux étapes : i) Unification des structures des documents XML, et ii) Modélisation multidimensionnelle des arbres unifiés. Plus précisé-ment, il focalise sur l'étape de modélisation multidimensionnelle.	Gilles Zurfluh, Jamel Feki, Ines Ben Messaoud	http://editions-rnti.fr/render_pdf.php?p1&p=1001081	http://editions-rnti.fr/render_pdf.php?p=1001081
Revue des Nouvelles Technologies de l'Information	EDA	2011	NoSQL: The Death of the Star		Alberto Abelló	http://editions-rnti.fr/render_pdf.php?p1&p=1001075	http://editions-rnti.fr/render_pdf.php?p=1001075
Revue des Nouvelles Technologies de l'Information	EDA	2011	Osons tout Persister dans un Entrepôt: Données et Modèles	La structure de stockage actuelle des entrepôts de données matérialisedes données conformément à une modèle logique défini par les concepteurs. Récemment,plusieurs travaux ont montré l'intérêt de la modélisation conceptuelledans le contexte des entrepôts de données du fait qu'elle offre une abstraction dudomaine étudié. Sa disponibilité facilite l'interrogation de l'entrepôt de données,car il est plus riche que le modèle logique. Les modèles conceptuel et logique del'entrepôt sont définis à partir de deux composantes : les sources de données etles besoins des utilisateurs. L'utilisation des besoins utilisateurs est actuellementreconnue comme une étape indispensable pour la réussite des projets d'entreposage.L'utilisation des besoins se fait sentir à différentes phases du cycle devie de l'entrepôt (optimisation, personnalisation, recommandations, traçabilité,etc.). Nous remarquons cependant qu'aucune trace du modèle conceptuel ni dumodèle des besoins n'est sauvegardée dans l'entrepôt final. Pour remédier à ceslimites, nous proposons dans cet article une piste de réflexion sur la propositiond'une nouvelle structure de stockage d'entrepôt permettant de représenterd'une manière persistante ces trois modèles : le modèle conceptuel, le modèledes besoins et le modèle logique.	Selma Khouri, Ladjel Bellatreche	http://editions-rnti.fr/render_pdf.php?p1&p=1001080	http://editions-rnti.fr/render_pdf.php?p=1001080
Revue des Nouvelles Technologies de l'Information	EDA	2011	Représentation Graphique des Hiérarchies Contextuelles : Modèle avec Satellites	Les modèles d'entrepôts dits classiques (étoile, etc.) ont émergé etconnu un vif succès au sein des entreprises de part leur présentation graphiquefacile à lire. Ceci est nécessaire dans un contexte où la modélisation multidimensionnelledoit être confrontée à l'avis des décideurs. Dans des travaux antérieurs,nous avons mis en avant un certain manque d'expressivité de ces modèles. Parexemple, dans le cas d'un entrepôt de données médicales, il n'était pas possiblede modéliser le fait que la tension artérielle d'un patient soit « faible », « normale» ou « élevée » (hiérarchisation de la mesure) dépend de son âge et dufait qu'il fume ou non. Ayant développé une formalisation de hiérarchies dites« contextuelles » pour pallier à ce problème, nous proposons dans ce papier unmodèle graphique, pour faciliter la lisibilité du modèle auprès des décideurs, quenous baptisons le « modèle avec satellites ».	Cécile Favre, Anne Laurent, Yoann Pitarch, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001079	http://editions-rnti.fr/render_pdf.php?p=1001079
Revue des Nouvelles Technologies de l'Information	EDA	2011	Sélection Statique et Incrémentale des Index de Jointure Binaires Multiples	Les index de jointure binaires ont montré leur intérêt dans la réductiondes coûts d'exécution des requêtes décisionnelles définies sur un schémarelationnel en étoile. Leur sélection reste cependant difficile vu le vaste et complexeespace de recherche à explorer. Peu d'algorithmes de sélection des indexde jointure existent, contrairement à la sélection des index définis sur une seuletable qui a connu un intérêt particulier auprès de la communauté des bases dedonnées traditionnelles. La principale particularité de ces algorithmes est qu'ilssont statiques et supposent la connaissance préalable des requêtes. Dans cetarticle, nous présentons une démarche de sélection des index de jointures binairesdéfinis sur plusieurs attributs appartenant à des tables de dimension enutilisant des algorithmes génétiques. Ces derniers sont utilisés dans le cadre statiqueet incrémental qui prévoit l'adaptation des index sélectionnés à l'arrivéede nouvelles requêtes. Nous concluons nos travaux par une étude expérimentaledémontrant l'intérêt de la sélection des index de jointure binaires multiple, del'élagage de l'espace de recherche et de l'efficacité des algorithmes génétiquesdans les cas statique ou incrémentale.	Ladjel Bellatreche, Rima Bouchakri	http://editions-rnti.fr/render_pdf.php?p1&p=1001091	http://editions-rnti.fr/render_pdf.php?p=1001091
Revue des Nouvelles Technologies de l'Information	EDA	2011	Système décisionnel et référentiel des territoires au CG34	Le Conseil général de l'Hérault s'est doté depuis dix ans d'unsystème d'information pour l'aide à la décision (SIAD34) structuré de façonoriginale. Celui-ci permet d'établir le lien entre les systèmes d'informationclassiquement qualifiés de décisionnels (infocentres et entrepôts) et le systèmed'information à références spatiales (SIRS ou SIG), deux mondes qui tropsouvent se méconnaissent, voir s'ignorent, dans les collectivités locales. Cettemise en relation est rendue possible par la mise en place de référentiels,dispositifs pivot qui assurent la cohérence et donc le lien entre les systèmesd'information métier, les infocentres métier et les différents modes dereprésentations des indicateurs, dont la cartographie. L'article présente lesdifférents composants du SIAD34 puis expose comment le référentiel desterritoires, constituant original du système, atteint aujourd'hui ses limites tantsur le plan conceptuel que technique. Il interpelle sur la possibilité d'intégrerles récentes avancées de la recherche dans ce domaine.	Corinne Rouzet, Marie Terrier	http://editions-rnti.fr/render_pdf.php?p1&p=1001084	http://editions-rnti.fr/render_pdf.php?p=1001084
Revue des Nouvelles Technologies de l'Information	EDA	2011	Usage Des Mesures Pour La Génération Des Règles d'Associations Cycliques	L'analyse en ligne (OLAP) fournit aux utilisateurs une navigation interactivedes données multidimensionnelles. Cependant, aucun moyen pour expliquerles corrélations existantes entre les données n'est offert. Ainsi, le couplagede l'OLAP et de la fouille de données, plus particulièrement les règlesd'association a efficacement apporté une solution satisfaisante à ce problème.Dans ce cadre, on s'intéresse à une classe particulière de règles d'associationqui est les règles d'association cycliques. Ces règles visent la découverte de modèlesqui se reproduisent périodiquement à des intervalles réguliers définis parl'utilisateur. Généralement, les motifs générés ne prennent pas en considérationles spécificités du contexte multidimensionnel à savoir, l'existence des mesureset leurs agrégations. Dans cet article, nous proposons une méthode d'extractionde règles d'association cycliques à partir de mesures et nous redéfinissonsles métriques d'évaluation de la qualité de ces motifs en s'inspirant de l'additivitétemporelle des mesures à travers l'intégration des fonctions d'agrégationappropriées. Pour prouver l'utilité de notre approche, nous menons une étudeempirique sur un entrepôt de données réel.	Eya Ben Ahmed, Ahlem Nabli, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001087	http://editions-rnti.fr/render_pdf.php?p=1001087
Revue des Nouvelles Technologies de l'Information	EGC	2011	@KRex : une méthode de construction des connaissances pour la maîtrise des activités à risques - application au domaine de la sécurité nucléaire	Dans les industries à risque, comme le nucléaire, les connaissances liées au savoir et à l'expérience participent à la maîtrise des activités. Elles sont explicites, formalisables dans des documents, ou tacites, expression du savoir faire moins souvent prise en compte. AREVA développe la méthode @KRex pour valoriser le retour d'expérience existant, créer une dynamique d'extraction et de capitalisation des connaissances, faciliter leur partage et leur enrichissement. Cette communication décrit le protocole expérimental de construction des connaissances explicites et tacites du métier sécurité nucléaire.	Julien Giudici, Hervé Janiaut, Rémy Gautier	http://editions-rnti.fr/render_pdf.php?p1&p=1001010	http://editions-rnti.fr/render_pdf.php?p=1001010
Revue des Nouvelles Technologies de l'Information	EGC	2011	A la recherche des tweets porteurs d'informations journalistiques		Benjamin Rosoor, Laurent Sebag, Sandra Bringay, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000961	http://editions-rnti.fr/render_pdf.php?p=1000961
Revue des Nouvelles Technologies de l'Information	EGC	2011	Acquisition de structures lexico-sémantiques à partir de textes : un nouveau cadre de travail fondé sur une structuration prétopologique	Les structures lexico-sémantiques jouent un rôle essentiel dans les processus de fouille de textes. En codant les relations sémantiques entre concepts du discours elles apportent une connaissance stratégiques pour enrichir les capacités de raisonnement. Le développement de telles structures étant fortement limité du fait des efforts nécessaires à leur construction, nous proposons un nouveau formalisme d'acquisition automatique d'ontologies terminologiques à partir de textes. Nous utilisons pour cela une formalisation prétopologique de l'espace des termes sur laquelle s'appuie un modèle générique de structuration. Nous présentons une étude empirique préliminaire rendant compte du potentiel de ce modèle en terme d'extraction de connaissances.	Guillaume Cleuziou, Gaël Dias, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1000936	http://editions-rnti.fr/render_pdf.php?p=1000936
Revue des Nouvelles Technologies de l'Information	EGC	2011	Adaptation de l'algorithme CART pour la tarification des risques en assurance non-vie	Les développements récents en tarification de l'assurance non-vie se concentrent majoritairement sur la maîtrise et l'amélioration des Modèles Linéaires Généralisés. Performants, ces modèles imposent cependant à la fois des contraintes sur la structure du risque modélisé et sur les interactions entre variables explicatives du risque. Ces restrictions peuvent conduire, dans certaines sous-populations d'assurés, à une estimation biaisée de la prime d'assurance. Les arbres de régression permettent de s'affranchir de ces contraintes et, de plus, augmentent la lisibilité des résultats de la tarification. Nous présentons une modification de l'algorithme CART pour prendre en compte les spécificités des données d'assurance non-vie. Nous comparons alors notre proposition aux modèles linéaires généralisés sur un portefeuille réel de véhicules. Notre proposition réduit les mesures d'erreur entre le risque mesuré et le risque modélisé, et permet ainsi une meilleure tarification.	Antoine Paglia, Martial Phélippé-Guinvarc'h, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1001028	http://editions-rnti.fr/render_pdf.php?p=1001028
Revue des Nouvelles Technologies de l'Information	EGC	2011	Agrégation robuste de données massives à la volée : application aux compteurs électriques communicants	Dans les années à venir, plusieurs millions de compteurs électriques communicants seront déployés sur l'ensemble du territoire français. Afin d'assurer la fiabilité d'un réseau de cette envergure nous proposons une topologie de communication multi-chemins qui repose sur la duplication des données transmises. Toute exploitation des données collectées doit alors tenir compte de la présence d'éléments dupliqués. Dans cet article, nous proposons une nouvelle méthode permettant de calculer en ligne des consommations électriques agrégées (agrégation spatiale). L'idée est d'adapter l'algorithme probabiliste Summation sketch de Considine et al. au contexte des compteurs communicants. Cette approche a l'avantage d'être insensible à la duplication et permet de profiter de la structure massivement distribuée du réseau de communication des futurs compteurs électriques. L'expérimentation de cette méthode sur des données réelles montre qu'elle donne une bonne précision sur l'estimation des consommations agrégées. Cette approche est aussi complétée par une méthode basée sur la théorie des sondages : On obtient une meilleure réactivité de l'estimateur avec rapidement et donc sur des données significativement partielles une erreur inférieure à 2.5%	Yousra Chabchoub, Benoît Grossin	http://editions-rnti.fr/render_pdf.php?p1&p=1000943	http://editions-rnti.fr/render_pdf.php?p=1000943
Revue des Nouvelles Technologies de l'Information	EGC	2011	Aide à l'Analyse Visuelle de Réseaux Sociaux pour la Détection de Comportements Suspects	Cet article traite de l'analyse visuelle de réseaux sociaux pour la détection de comportements suspects à partir de données de communications fournies à des enquêteurs suivant deux procédures : l'interception légale et la rétention de données. Nous proposons les contributions suivantes : (i) un modèle de données et un ensemble d'opérateurs pour interroger ces données dans le but d'extraire des comportements suspects et (ii) une représentation visuelle conviviale pour une navigation simplifiée dans les données de communication accompagnée avec une implémentation.	Amyn Bennamane, Hakim Hacid, Arnaud Ansiaux, Alain Cagnati	http://editions-rnti.fr/render_pdf.php?p1&p=1000948	http://editions-rnti.fr/render_pdf.php?p=1000948
Revue des Nouvelles Technologies de l'Information	EGC	2011	Algorithmes de recherche exhaustif et guidé pour la recommandation d'un expert dans un réseau professionnel		Maria Malek	http://editions-rnti.fr/render_pdf.php?p1&p=1000970	http://editions-rnti.fr/render_pdf.php?p=1000970
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse comparative de méthodologies et d'outils de construction automatique d'ontologies à partir de ressources textuelles	Plusieurs méthodologies et outils de construction automatique des ontologies à partir de ressources textuelles ont été proposés ces dernières années. Dans cet article nous analysons quatre approches en les comparant à une approche de référence - Methontology. Dans leur sélection nous avons privilégié celles qui couvrent l'ensemble des étapes du processus de construction d'ontologies. Puis nous analysons et comparons la portée, les limites et les performances des implémentations logicielles associées aux approches analysées. Ces outils ont été testés sur un corpus de ressources textuelles, et nous avons comparé leurs résultats à ceux obtenus manuellement.	Toader Gherasim, Mounira Harzallah, Giuseppe Berio, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000989	http://editions-rnti.fr/render_pdf.php?p=1000989
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse du comportement limite d'indices probabilistes pour une sélection discriminante	Nous étudions ici le comportement de deux types d'indices probabilistes discriminants en présence de données dont le volume va en croissant. A cet égard, un modèle spécifique de croissance de la taille des données et de liaison entre variables est mis en oeuvre et celui-ci va permettre de déterminer le comportement limite des différents indices quel que soit le niveau de liaison entre la prémisse et la conclusion de la règle donnée. La clarté des résultats obtenus nous conduit à en chercher l'explication formelle. L'expérimentation a été effectuée avec la base de données UCI Wages.	Sylvie Guillaume, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001036	http://editions-rnti.fr/render_pdf.php?p=1001036
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse factorielle des correspondances hiérarchique pour la fouille d'images	Nous proposons un outil graphique interactif qui permet de visualiser et d'extraire des connaissances à partir des résultats de l'Analyse Factorielle des Correspondances (AFC) sur les images. L'AFC est une technique descriptive développée pour analyser des tableaux de contingence. L'AFC est originellement utilisée dans l'Analyse des Données Textuelles (ADT) où le corpus est représenté par un tableau de contingence croisant des documents et des mots. Dans la fouille d'images, nous définissons d'abord les « mots visuels » dans les images (analogues aux mots textuels). Ces mots visuels sont construits à partir des descripteurs locaux SIFT (Scale Invariant Feature Transform) dans l'image. Ensuite, nous appliquons l'AFC sur le tableau de contingence obtenu. Notre outil (appelé HCAViz) analyse ce tableau de contingence de façon récursive et aide l'utilisateur à interpréter et interagir avec les résultats de l'AFC. D'abord, les résultats de la première AFC sur les images sont visualisés. L'utilisateur sélectionne ensuite un groupe d'images et fait une deuxième AFC sur le nouveau tableau de contingence. Ce processus peut continuer jusqu'à ce qu'un thème « pur » se dévoile. Ceci permet de découvrir une arborescence des thèmes dans une collection d'images. Une application sur la base Caltech-4 illustre l'intérêt de HCAViz dans la fouille d'images.	Nguyen-Khang Pham, Annie Morin, François Poulet, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1000941	http://editions-rnti.fr/render_pdf.php?p=1000941
Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse spatiotemporelle des vecteurs de mouvement : application au comptage des personnes	Cet article présente une nouvelle approche qui permet de compter le nombre d'individus franchissant une ligne de comptage. L'approche proposée accumule dans le temps les vecteurs de mouvement pour chaque point de la ligne de comptage formant une carte spatiotemporelle. Une procédure de détection en ligne des blobs est ensuite utilisée afin de déterminer les régions de la carte spatiotemporelle qui correspondent à des personnes franchissant cette ligne. Le nombre d'individus associé à chaque blob est estimé grâce à un modèle de régression linéaire appliqué aux caractéristiques du blob. L'approche proposée est validée sur la base de plusieurs ensembles de données enregistrées à l'aide d'une caméra verticale ou d'une caméra oblique.	Yassine Benabbas, Tarek Yahiaoui, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000942	http://editions-rnti.fr/render_pdf.php?p=1000942
Revue des Nouvelles Technologies de l'Information	EGC	2011	Annotation d'entités nommées par extraction de règles de transduction	La reconnaissance d'entités nommées est une problématique majoritairement traitée par des modèles spécifiés à l'aide de règles ou par apprentissage numérique. Les premiers ont le désavantage d'être coûteux à développer pour obtenir une couverture satisfaisante, les seconds sont souvent difficiles à interpréter par des experts (linguistes). Dans cet article, nous présentons une approche, dont l'objectif est d'extraire des règles symboliques discriminantes qu'un humain puisse consulter. A partir d'un corpus de référence, nous extrayons des règles de transduction, dont seules les plus informatives sont retenues. Elles sont ensuite appliquées pour effectuer une annotation : à cet effet, un algorithme recherche parmi les annotations possibles celles de meilleure qualité en termes de couverture et de probabilité. Nous présentons les résultats expérimentaux et discutons de l'intérêt et des perspectives de notre approche.	Arnaud Soulet, Damien Nouvel	http://editions-rnti.fr/render_pdf.php?p1&p=1000937	http://editions-rnti.fr/render_pdf.php?p=1000937
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apport de la catégorisation iconique pour la gestion coopérative des connaissances1		Xiaoyue Ma, Jean-Pierre Cahier, L'Hédi Zaher	http://editions-rnti.fr/render_pdf.php?p1&p=1000978	http://editions-rnti.fr/render_pdf.php?p=1000978
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apport des données thématiques dans les systèmes de recommandation : hybridation et démarrage à froid	Des travaux récents (Pilaszy et al., 2009) suggèrent que les métadonnées sont quasiment inutiles pour les systèmes de recommandation, y compris en situation de cold-start : les données de logs de notation sont beaucoup plus informatives. Nous étudions, sur une base de référence de logs d'usages pour la recommandation automatique de DVD (Netflix), les performances de systèmes de recommandation basés sur des sources de données collaboratives, thématiques et hybrides en situation de démarrage à froid (cold-start). Nous exhibons des cas expérimentaux où les métadonnées apportent plus que les données de logs d'usage (collaboratives) pour la performance prédictive. Pour gérer le cold-start d'un système de recommandation, nous montrons que des approches "en cascade", thématiques puis hybrides, puis collaboratives, seraient plus appropriées.	Frank Meyer, Éric Gaussier, Fabrice Clérot, Julien Schluth	http://editions-rnti.fr/render_pdf.php?p1&p=1000947	http://editions-rnti.fr/render_pdf.php?p=1000947
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apprendre les contraintes topologiques dans les cartes auto-organisatrices	La Carte Auto-Organisatrice (SOM : Self-Organizing Map) est une méthode populaire pour l'analyse de la structure d'un ensemble de données. Cependant, certaines contraintes topologiques de la SOM sont fixées avant l'apprentissage et peuvent ne pas être pertinentes pour la représentation de la structure des données. Dans cet article nous nous proposons d'améliorer les performances des SOM avec un nouvel algorithme qui apprend les contraintes topologiques de la carte à partir des données. Des expériences sur des bases de données artificielles et réelles montrent que l'algorithme proposé produit de meilleurs résultats que SOM classique. Ce n'est pas le cas avec une relaxation triviale des contraintes topologiques, qui résulte en une forte augmentation de l'erreur topologique de la carte.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000939	http://editions-rnti.fr/render_pdf.php?p=1000939
Revue des Nouvelles Technologies de l'Information	EGC	2011	Apprentissage génératif de la structure de réseaux logiques de Markov à partir d'un graphe des prédicats	Les Réseaux Logiques de Markov (MLNs) combinent l'apport statistique des Réseaux de Markov à la logique du premier ordre. Dans cette approche, chaque clause logique se voit affectée d'un poids, l'instanciation des clauses permettant alors de produire un Réseau deMarkov. L'apprentissage d'un MLN consiste à apprendre d'une part sa structure (la liste de clauses logiques) et d'autre part les poids de celles-ci. Nous proposons ici une méthode d'apprentissage génératif de Réseau Logique de Markov. Cette méthode repose sur l'utilisation d'un graphe des prédicats, produit à partir d'un ensemble de prédicats et d'une base d'apprentissage. Une méthode heuristique de variabilisation est mise en oeuvre afin de produire le jeu de clauses candidates. Les résultats présentés montrent l'intérêt de notre approche au regard de l'état de l'art.	Quang-Thang Dinh, Matthieu Exbrayat, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000993	http://editions-rnti.fr/render_pdf.php?p=1000993
Revue des Nouvelles Technologies de l'Information	EGC	2011	Cartes cognitives : une exploitation à base d'échelle, vue et profil	Une carte cognitive est un réseau d'influences entre différents concepts. Le modèle des cartes cognitives permet à un utilisateur de calculer l'influence entre deux concepts. Les cartes cognitives contenant un grand nombre de concepts et d'influences sont difficiles à comprendre. Cet article introduit la notion de carte cognitive ontologique qui associe une ontologie à une carte cognitive classique pour en organiser les concepts. Afin de faciliter la compréhension d'une carte, l'utilisateur peut obtenir une vue de cette carte la simplifiant selon une échelle qu'il aura choisie. Un profil peut être créé pour construire des vues correspondant aux objectifs d'un type d'utilisateur. Si une carte est manipulée par différents utilisateurs, leurs profils combinés permettent de construire une vue partagée.	Lionel Chauvin, David Genest, Aymeric Le Dorze, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001008	http://editions-rnti.fr/render_pdf.php?p=1001008
Revue des Nouvelles Technologies de l'Information	EGC	2011	Catégorisation des mesures d'intérêt pour l'extraction des connaissances	La recherche de règles d'association intéressantes est un domaine de recherche important et actif en fouille de données. Les algorithmes de la famille Apriori reposent sur deux mesures pour extraire les règles, le support et la confiance. Bien que ces deux mesures possèdent des vertus algorithmiques accélératrices, elles génèrent un nombre prohibitif de règles dont la plupart sont redondantes et sans intérêt. Il est donc nécessaire de disposer d'autres mesures filtrant les règles inintéressantes. Des travaux ont été réalisés pour dégager les "bonnes" propriétés des mesures d'extraction des règles et ces propriétés ont été évaluées sur 61 mesures. L'objectif de cet article est de dégager des catégories de mesures afin de répondre à une préoccupation des utilisateurs : le choix d'une ou plusieurs mesures lors d'un processus d'extraction des connaissances dans le but d'éliminer les règles valides non pertinentes extraites par le couple (support, confiance). L'évaluation des propriétés sur les 61 mesures a permis de dégager 9 classes de mesures, classes obtenues grâce à deux techniques : une méthode de la classification ascendante hiérarchique et une version de la méthode de classification non-hiérarchique des k-moyennes.	Sylvie Guillaume, Dhouha Grissa, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001014	http://editions-rnti.fr/render_pdf.php?p=1001014
Revue des Nouvelles Technologies de l'Information	EGC	2011	Classificateurs aléatoires Topologiques à base de graphes de voisinage	En apprentissage supervisé, les Méthodes Ensemble (ME) ont montré leurs qualités. L'une des méthodes de référence dans ce domaine est les Forêts Aléatoires (FA). Cette dernière repose sur des partitionnements de l'espace de représentation selon des frontières parallèles aux axes ou obliques. Les conséquences de cette façon de partitionner l'espace de représentation peuvent affecter la qualité de chaque prédicteur. Il nous a semblé que cette approche pouvait être améliorée si on se libérait de cette contrainte de manière à mieux coller à la structure topologique de l'ensemble d'apprentissage. Dans cet article, nous proposons une nouvelle ME basée sur des graphes de voisinage dont les performances, sur nos premières expérimentations, sont aussi bonnes que celles des FA.	Fabien Rico, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000933	http://editions-rnti.fr/render_pdf.php?p=1000933
Revue des Nouvelles Technologies de l'Information	EGC	2011	Classification des aéronefs par estimation de la pose	Dans le présent travail, nous proposons un outil d'aide à la reconnaissance de cibles radar basé sur la signature de forme et de la pose de la cible. La tâche principale dans le cadre de cet article consiste à établir la fonction de recherche d'images ISAR par l'exemple en exploitant l'information de pose estimée depuis les images ISAR. L'objectif est d'introduire l'information de pose dans l'indexation des images, notamment dans la phase de sélection des images candidates. Nous proposons une nouvelle méthode d'estimation de la pose basée sur l'axe le plus symétrique de la cible. La méthode proposée est ensuite comparée avec d'autres techniques connues telles que la transformée de Hough et la transformée en ondelette. Enfin, la tâche de classification est réalisée en utilisant les k-plus proches voisins incluant l'information de la pose.	Mohamed Nabil Saidi, Abdelmalek Toumi, Ali Khenchaf, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1001012	http://editions-rnti.fr/render_pdf.php?p=1001012
Revue des Nouvelles Technologies de l'Information	EGC	2011	Closed-set-based Discovery of Representative Association Rules Revisited	The output of an association rule miner is often huge in practice. This is why several concise lossless representations have been proposed, such as the "essential" or "representative" rules. We revisit the algorithm given by Kryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS 2189, 350-359) for mining representative rules. We show that its output is sometimes incomplete, due to an oversight in its mathematical validation, and we propose an alternative complete generator that works within only slightly larger running times.	José L Balcazar , Cristina Tîrnauca	http://editions-rnti.fr/render_pdf.php?p1&p=1001032	http://editions-rnti.fr/render_pdf.php?p=1001032
Revue des Nouvelles Technologies de l'Information	EGC	2011	Comparaison entre deux indices pour l'évaluation probabiliste discriminante des règles d'association	L'élaboration d'une échelle de probabilité discriminante pour la comparaison mutuelle entre plusieurs attributs observés sur un échantillon d'objets de "grosse" taille, nécessite une normalisation préalable. L'objet de cet article est l'analyse comparée entre deux approches. La première dérive de l' "Analyse de la Vraisemblance des Liens Relationnels Normalisée". La seconde est fondée sur la notion de "Valeur Test" sur un échantillon virtuel de taille 100, synthétisant l'échantillon initial.	Israël-César Lerman, Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1001034	http://editions-rnti.fr/render_pdf.php?p=1001034
Revue des Nouvelles Technologies de l'Information	EGC	2011	Complex Information Processing	It is commonplace nowadays to claim that information is everywhere and that, as a result, finding the right information (mathematically : according to a set of criteria optimizing a specific goal) is very difficult. Defence applications have to cope with similar problems : communication networks, surveillance and information systems transmit and generate significant amounts of complex information which cannot be processed with low level algorithms. The challenge is to build high-level processing units (which demand a lot of computing power) so as process video streams and communication packets with little possibility of a false alarm as automatically as possible. Methods for processing, aligning, merging low-level and high-level information (from syntactic to semantic information) extracted from still images, videos, speech, text and the Internet are being considered. The framework includes theoretical approaches, algorithms as well as evaluation methods. Topics of interest are data fusion, learning techniques, data mining, HCI, even Artificial Intelligence. Defence applications are numerous, from scene understanding to weak signal detection.	Jacques Blanc-Talon	http://editions-rnti.fr/render_pdf.php?p1&p=1000922	http://editions-rnti.fr/render_pdf.php?p=1000922
Revue des Nouvelles Technologies de l'Information	EGC	2011	Conception et implémentation d'une nouvelle technique cellulaire de discrétisation : intégration dans TANAGRA		Mohamed Benamina, Baghdad Atmani	http://editions-rnti.fr/render_pdf.php?p1&p=1000976	http://editions-rnti.fr/render_pdf.php?p=1000976
Revue des Nouvelles Technologies de l'Information	EGC	2011	Construction d'une Ontologie d'aide au renforcement de la sécurité des systèmes de transport automatisés.		Lassaâd Mejri, Ahmed Maalel, Habib Hadj Mabrouk, Henda Ben Ghezela Hadjami	http://editions-rnti.fr/render_pdf.php?p1&p=1000967	http://editions-rnti.fr/render_pdf.php?p=1000967
Revue des Nouvelles Technologies de l'Information	EGC	2011	Construction ontologique à partir de séquences d'expression de champignons		Houda Fyad, Karim Bouamrane, Baghdad Atmani, Claire Toffano-Nioche	http://editions-rnti.fr/render_pdf.php?p1&p=1000968	http://editions-rnti.fr/render_pdf.php?p=1000968
Revue des Nouvelles Technologies de l'Information	EGC	2011	Data stream summarization by on-line histograms clustering		Antonio Balzanella, Lidia Rivoli, Rosanna Verde	http://editions-rnti.fr/render_pdf.php?p1&p=1000977	http://editions-rnti.fr/render_pdf.php?p=1000977
Revue des Nouvelles Technologies de l'Information	EGC	2011	Découverte de motifs d'évolution significatifs dans les séries temporelles d'images satellites	Les séries temporelles d'images satellites (ou Satellite Image Time Series - SITS) sont d'importantes sources d'informations sur l'évolution du territoire. Étudier ces images permet de comprendre les changements sur des zones précises mais aussi de découvrir des schémas d'évolution à grande échelle. Toutefois, découvrir ces phénomènes impose de répondre à plusieurs défis qui sont liés aux caractéristiques des SITS et à leurs contraintes. Premièrement, chaque pixel d'une image satellite est décrit par plusieurs valeurs (les niveaux radiométriques sur différentes longueurs d'ondes). Deuxièmement, ces motifs d'évolution portent sur des périodes très longues et ne sont pas forcément synchrones selon les régions. Troisièmement, les régions qui ne sont pas concernées par des évolutions significatives sont majoritaires et leur domination rend difficile l'extraction des motifs d'évolution. Dans cet article, nous proposons une méthode qui répond à ces difficultés et nous la validons sur une série d'images satellites acquises sur une période de 20 ans.	François Petitjean, Florent Masseglia, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1001037	http://editions-rnti.fr/render_pdf.php?p=1001037
Revue des Nouvelles Technologies de l'Information	EGC	2011	Des graphes de documents aux réseaux sociaux		Michel Plantié, Michel Crampes	http://editions-rnti.fr/render_pdf.php?p1&p=1000972	http://editions-rnti.fr/render_pdf.php?p=1000972
Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection de changements de distribution dans un flux de données : une approche supervisée	L'analyse de flux de données traite des données massives grâce à des algorithmes en ligne qui évitent le stockage exhaustif des données. La détection de changements dans la distribution d'un flux est une question importante dont les applications potentielles sont nombreuses. Dans cet article, la détection de changement est transposée en un problème d'apprentissage supervisé. Nous avons choisi d'utiliser la méthode de discrétisation supervisée MODL car celle-ci présente des propriétés intéressantes. Notre approche est comparée favorablement à une méthode de l'état-de-l'art sur des flux de données artificiels.	Marc Boullé, Alexis Bondu	http://editions-rnti.fr/render_pdf.php?p1&p=1000944	http://editions-rnti.fr/render_pdf.php?p=1000944
Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection de redondances dans les tableaux guidée par une ontologie	Nous nous intéressons dans cet article à la réconciliation d'annotations floues associées à des tableaux de données par une méthode d'annotation sémantique, qui est guidée par une ontologie de domaine. Etant donnés deux tableaux, la méthode consiste à détecter leurs instances de relation redondantes. Elle s'appuie sur les connaissances déclarées dans l'ontologie, ainsi que sur des scores de similarité entre les annotations floues représentées par des sous-ensembles flous numériques ou par des sous-ensembles flous symboliques	Rania Khefifi, Patrice Buche, Juliette Dibie-Barthélemy, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001016	http://editions-rnti.fr/render_pdf.php?p=1001016
Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection des profils à long terme et à court terme dans les réseaux sociaux	La conception des profils et contextes utilisateurs se situe au coeur de l'étude et de la mise en oeuvre des mécanismes de personnalisation ou d'adaptation de contenus (recherche d'information, systèmes de recommandation, etc.). Plusieurs modèles et dimensions de profils et contextes sont décrits dans la littérature. Dans la vie réelle tout comme dans les systèmes d'information, le comportement de l'utilisateur est très souvent influencé par son environnement social. Cependant, la dimension sociale des profils et contextes utilisateurs reste très peu étudiée et évaluée. Dans cet article, nous présentons une méthode de visualisation des profils utilisateurs permettant d'évaluer la pertinence du réseau social de l'utilisateur dans l'évolution de son profil. L'expérimentation de la méthode à partir de Facebook permet d'identifier d'une part, les centres d'intérêts à court-terme et à long-terme des profils utilisateurs, et d'autre part, l'influence réelle à court-terme et à long-terme du réseau social de chaque utilisateur. Ces résultats démontrent l'intérêt de modéliser et d'intégrer une dimension sociale dans les profils et contextes utilisateurs, afin de tenter d'améliorer les mécanismes de personnalisation ou d'adaptation de contenus.	Dieudonné Tchuente, Marie-Françoise Canut, Nadine Baptiste-Jessel	http://editions-rnti.fr/render_pdf.php?p1&p=1000987	http://editions-rnti.fr/render_pdf.php?p=1000987
Revue des Nouvelles Technologies de l'Information	EGC	2011	Early Classification on Temporal Sequences	Early classification of temporal sequences has applications in, for example, health informatics, intrusion detection, anomaly detection, and scientific and engineering sequence data monitoring. In early classification, instead of optimizing accuracy, our goal is to produce classification as early as possible provided that the accuracy meets some expectation. In this talk, I will advocate early classification as an exciting and challenging research problem, which has not been systematically studied in the literature. I will discuss several interesting formulations of the problem, which provide complimentary features possibly desirable in different application scenarios. I will also review some of our recent progress on this aspect.	Jian Pei	http://editions-rnti.fr/render_pdf.php?p1&p=1000918	http://editions-rnti.fr/render_pdf.php?p=1000918
Revue des Nouvelles Technologies de l'Information	EGC	2011	Entropic-Genetic Clustering	This paper addresses the clustering problem given the similarity matrix of a dataset. We define two distinct criteria with the aim of simultaneously minimizing the cut size and obtaining balanced clusters. The first criterion minimizes the similarity between objects belonging to different clusters and is an objective generally met in clustering. The second criterion is formulated with the aid of generalized entropy. The trade-off between these two objectives is explored using a multi-objective genetic algorithm with enhanced operators	Mihaela Breaban, Henri Luchian, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1000931	http://editions-rnti.fr/render_pdf.php?p=1000931
Revue des Nouvelles Technologies de l'Information	EGC	2011	Equilibrer l'analyse des motifs fréquents	Cet article propose une méthode originale d'évaluation de la qualité des motifs en anticipant la manière qui sera utilisée pour les analyser. Nous commençons par introduire le modèle de l'analyse aléatoire d'un ensemble de motifs selon une mesure d'intérêt. Avec ce modèle, nous constatons que l'étude des motifs fréquents avec le support conduit à une analyse déséquilibrée du jeu de données. Afin que chaque transaction reçoive la même attention, nous définissons le support équilibré qui corrige le support classique en pondérant les transactions. Nous proposons alors un algorithme qui calcule ces poids et nous validons expérimentalement son efficacité.	Arnaud Giacometti, Arnaud Soulet, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000927	http://editions-rnti.fr/render_pdf.php?p=1000927
Revue des Nouvelles Technologies de l'Information	EGC	2011	Equivalence topologique entre mesures de proximité	Le choix d'une mesure de proximité entre objets a un impact direct sur les résultats de toute opération de classification, de comparaison, d'évaluation ou de structuration d'un ensemble d'objets. Pour un problème donné, l'utilisateur est amené à choisir une parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence choisie, comme celle basée sur les préordonnances, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche pour comparer les mesures de proximité. Celle-ci est basée sur l'équivalence topologique. A cet effet, nous introduisons un nouveau concept baptisé équivalence topologique. Ce dernier fait appel à la structure de voisinage local. Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure. Nous établissons ensuite des liens formels avec l'équivalence en préordonnance. Les deux approches sont comparées sur le plan théorique et sur le plan empirique. Nous illustrons le principe de cette comparaison sur un exemple simple pour une quinzaine de mesures de proximités de la littérature.	Djamel Abdelkader Zighed, Rafik Abdesselam, Ahmed Bounekkar	http://editions-rnti.fr/render_pdf.php?p1&p=1000928	http://editions-rnti.fr/render_pdf.php?p=1000928
Revue des Nouvelles Technologies de l'Information	EGC	2011	Estimation de la densité d'arcs dans les graphes de grande taille: une alternative à la détection de clusters	La recherche de structures dans les graphes est un sujet étudié depuis longtemps, qui a bénéficié d'un regain d'intérêt avec la mise à disposition de graphes de grande taille sur le web, tels les réseaux sociaux. De nombreuses méthodes de recherche de clusters "naturels" dans les graphes ont été proposées, fondées notamment sur la modularité de Newman. On introduit dans cet article une nouvelle façon de résumer la structure des graphes de grande taille, en utilisant des estimateurs de densité des arcs exploitant des modèles en grille, basés sur un co-partitionnent des noeuds source et cible des arcs. Les structures identifiées par cette méthode vont au delà de la "classique" détection de clusters dans les graphes, et permettent d'estimer asymptotiquement la densité des arcs. Les expérimentations confirment le potentiel de l'approche, qui permet d'identifier des structures fortement informatives dans les graphes, sans faire l'hypothèse d'une décomposition en clusters denses.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000986	http://editions-rnti.fr/render_pdf.php?p=1000986
Revue des Nouvelles Technologies de l'Information	EGC	2011	Être ou ne pas être usager d'internet telle est la question ?		Abdoulaye Sarr, Philippe Lenca, Annabelle Boutet, Jocelyne Tremenbert	http://editions-rnti.fr/render_pdf.php?p1&p=1000963	http://editions-rnti.fr/render_pdf.php?p=1000963
Revue des Nouvelles Technologies de l'Information	EGC	2011	Evaluation des outils d'extraction terminologique Quezao et Acabit	L'article décrit l'évaluation de deux outils d'extraction terminologique Acabit et Quezao. Si acabit est plus connu car librement disponible, Quezao est issu des travaux d'Orange Labs sur la recherche d'informations. Après une comparaison sur les approches théoriques des deux systèmes, une évaluation concrète va porter sur un corpus d'actualité (2424Actu) pour l'aspect qualitatif et sur un corpus de presse pour l'aspect quantitatif	Edmond Lassalle, Prem Kumar Casimir, Emilie Guimier De Neef	http://editions-rnti.fr/render_pdf.php?p1&p=1000938	http://editions-rnti.fr/render_pdf.php?p=1000938
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction de motifs séquentiels contextuels	Les motifs séquentiels traditionnels ne tiennent généralement pas compte des informations contextuelles fréquemment associées aux données séquentielles. Dans le cas des séquences d'achats de clients dans un magasin, l'extraction classique de motifs se focalise sur les achats des clients sans considérer leur catégorie socio-professionnelle, leur sexe, leur âge. Or, en considérant le fait qu'un motif séquentiel est spécifique à un contexte donné, un expert pourra adapter sa stratégie au type du client et prendre les décisions adéquates. Dans cet article, nous proposons d'extraire des motifs de la forme «l'achat des produits A et B suivi de l'achat du produit C est spécifique aux jeunes clients». En mettant en valeur les propriétés formelles de tels contextes, nous développons un algorithme efficace d'extraction de motifs séquentiels contextuels. Les expérimentations effectuées sur un jeu de données réelles montrent les apports et l'efficacité de l'approche proposée.	Julien Rabatel, Sandra Bringay	http://editions-rnti.fr/render_pdf.php?p1&p=1000924	http://editions-rnti.fr/render_pdf.php?p=1000924
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction de motifs temporels à partir de séquences d'événements avec intervalles temporels	La fouille de base de données séquentielles a pour objet l'extraction de motifs séquentiels représentatifs. La plupart des méthodes concernent des motifs composés d'événements liés par des relations temporelles basées sur la précédence des instants. Pourtant, dans de nombreuses situations réelles une information quantitative sur la durée des événements ou le délai inter-événements est nécessaire pour discriminer les phénomènes. Nous proposons deux algorithmes, QTIAPriori et QTIPrefixSpan, pour extraire des motifs temporels composés d'événements associés à des intervalles décrivant leur position dans le temps et leur durée. Chacun d'eux ajoute aux algorithmes GSP et PrefixSpan une étape de catégorisation d'intervalles multi-dimensionnels pour extraire les intervalles temporelles représentatifs. Les expérimentations sur des données simulées montrent la capacité des algorithmes à extraire des motifs précis en présence de bruit et montrent l'amélioration des performances en temps de calcul.	Rene Quiniou, Thomas Guyet	http://editions-rnti.fr/render_pdf.php?p1&p=1000925	http://editions-rnti.fr/render_pdf.php?p=1000925
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction et Analyse de réseaux sociaux issus de Bases de Données Relationnelles	Dans un contexte d'entreprise, beaucoup d'informations importantes restent stockées dans des bases de données relationnelles, constituant une source riche pour construire des réseaux sociaux. Le réseau, ainsi extrait, a souvent une taille importante ce qui rend son analyse et sa visualisation difficiles. Dans ce travail, nous proposons une étape d'extraction suivie d'une étape d'agrégation des réseaux sociaux à partir des bases de données relationnelles. L'étape d'extraction ou de construction transforme une base de données relationnelle en base de données graphe, puis le réseau social est extrait. L'étape d'agrégation, qui est basée sur l'algorithme k-SNAP, produit un graphe résumé.	Rania Soussi, Amine Louati, Marie-Aude Aufaure, Hajer Baazaoui Zghal, Yves Lechevallier, Henda Ben Ghezela Hadjami	http://editions-rnti.fr/render_pdf.php?p1&p=1000988	http://editions-rnti.fr/render_pdf.php?p=1000988
Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction sous contraintes d'ensembles de cliques homogènes	Nous proposons une méthode de fouille de données sur des graphes ayant un ensemble d'étiquettes associé à chaque sommet. Une application est, par exemple, d'analyser un réseau social de chercheurs co-auteurs lorsque des étiquettes précisent les conférences dans lesquelles ils publient.Nous définissons l'extraction sous contraintes d'ensembles de cliques tel que chaque sommet des cliques impliquées partage suffisamment d'étiquettes. Nous proposons une méthode pour calculer tous les Ensembles Maximaux de Cliques dits Homogènes qui satisfont une conjonction de contraintes fixée par l'analyste et concernant le nombre de cliques séparées, la taille des cliques ainsi que le nombre d'étiquettes partagées. Les expérimentations montrent que l'approche fonctionne sur de grands graphes construits à partir de données réelles et permet la mise en évidence de structures intéressantes	Pierre-Nicolas Mougel, Marc Plantevit, Christophe Rigotti, Olivier Gandrillon, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000999	http://editions-rnti.fr/render_pdf.php?p=1000999
Revue des Nouvelles Technologies de l'Information	EGC	2011	Heuristique pour l'extraction de motifs ensemblistes bruités	La recherche de motifs ensemblistes dans des matrices de données booléennes est une problématique importante dans un processus d'extraction de connaissances. Elle consiste à rechercher tous les rectangles de 1 dans une matrice de données à valeurs dans {0,1} dans lesquelles l'ordre des lignes et colonnes n'est pas important. Plusieurs algorithmes ont été développés pour répondre à ce problème, mais s'adaptent difficilement à des données réelles susceptibles de contenir du bruit. Un des effets du bruit est de pulvériser un motif pertinent en un ensemble de sous-motifs recouvrants et peu pertinents, entraînant une explosion du nombre de motifs résultats. Dans le cadre de ce travail, nous proposons une nouvelle approche heuristique basée sur les algorithmes de graphes pour la recherche de motifs ensemblistes dans des contextes binaires bruités. Pour évaluer notre approche, différents tests ont été réalisés sur des données synthétiques et des données réelles issues d'applications bioinformatiques.	Céline Rouveirol, Lucas Létocart, Karima Mouhoubi	http://editions-rnti.fr/render_pdf.php?p1&p=1001002	http://editions-rnti.fr/render_pdf.php?p=1001002
Revue des Nouvelles Technologies de l'Information	EGC	2011	Import automatique et interactif de données dans les systèmes de visualisations	La première étape du processus de visualisation d'information consiste à transformer les données d'un format brut vers une structure de données utilisable par les différents composants de visualisation. Dans les applications réelles, cette première étape représente une barrière empêchant l'accès des utilisateurs novices à une riche variété de techniques de visualisation. Par exemple, il peut être techniquement impossible pour un utilisateur lambda de transformer des données arborescentes en un modèle de graphe pouvant utiliser une représentation à base de TreeMap. Une autre barrière est aussi la multitude de transformations possible des données brutes. Il faut pouvoir explorer cet ensemble de combinaisons. Basé sur nos retours d'expériences avec des utilisateurs finaux, dans cet article, nous considérons que le format brut est sous forme tabulaire. Ce format est le plus couramment utilisé et est facilement accessible par nos utilisateurs. Nous proposons une méthode novatrice permettant de générer automatiquement des graphes valués à partir de n'importe quelle table. En analysant le contenu de chaque dimension nous identifions les interconnexions entre celles-ci. Puis nous caractérisons les entités, les attributs et les relations possibles au sein des tables. Finalement, nous intégrons l'utilisateur dans le processus de transformation en lui proposant un ensemble de transformations valides.	David Auber, Frédéric Gilbert	http://editions-rnti.fr/render_pdf.php?p1&p=1001006	http://editions-rnti.fr/render_pdf.php?p=1001006
Revue des Nouvelles Technologies de l'Information	EGC	2011	Intégration de données haptiques brutes dans des systèmes experts de diagnostic des connaissances	Cet article a pour cadre un environnement informatique pour l'apprentissage humain (EIAH) dédié à la chirurgie orthopédique, et plus précisément sur le diagnostic des connaissances des apprenants. Pour ce faire, un réseau bayésien infère à partir d'exercices que les étudiants réalisent sur un simulateur avec bras articulé. Ce réseau résulte d'une approche centrée expert du domaine, comme très souvent dans les EIAH. Pourtant, dans un domaine comme la chirurgie où les connaissances sont tacites, le geste de l'apprenant semble intéressant à considérer. Le but de nos travaux est donc d'adopter une démarche plus centrée sur les données en incorporant au réseau bayésien les données haptiques continues issues du simulateur. Divers problèmes se posent néanmoins, d'une part sur le besoin d'étudier la nature des données pour conserver la généricité du système, et d'autre part pour trouver des méthodes de validation pertinentes concernant leur traitement	Sébastien Lallé, Vanda Luengo	http://editions-rnti.fr/render_pdf.php?p1&p=1001026	http://editions-rnti.fr/render_pdf.php?p=1001026
Revue des Nouvelles Technologies de l'Information	EGC	2011	Interprétation graphique de la courbe ROC		François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1000969	http://editions-rnti.fr/render_pdf.php?p=1000969
Revue des Nouvelles Technologies de l'Information	EGC	2011	Interprétation spectrale de la classification relationnelle	Ce papier présente une vue spectrale sur l'approche de l'analyse relationnelle pour la classification des données catégorielles. Il établit d'abord le lien théorique entre l'approche de l'analyse relationnelle et le problème de classification spectrale. En particulier, le problème de classification relationnelle est présenté comme un problème de maximisation de trace, ce problème est donc transformé par la relaxation spectrale en un problème d'optimisation sous contraintes qui peut être résolu par des multiplicateurs de Lagrange, la solution est donnée par un problème de valeurs propres.	Lazhar Labiod, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000997	http://editions-rnti.fr/render_pdf.php?p=1000997
Revue des Nouvelles Technologies de l'Information	EGC	2011	Introduction de l'ingénierie ontologique dans la méthodologie de développement des progiciels de gestion des collectivités territoriales		Wilfried Despagne, Thomas Burger	http://editions-rnti.fr/render_pdf.php?p1&p=1000975	http://editions-rnti.fr/render_pdf.php?p=1000975
Revue des Nouvelles Technologies de l'Information	EGC	2011	ISICIL : Intégration Sémantique d'Informations à travers des Communautés d'Intelligence en Ligne		Pavel Arapov, Sébastien Comos, Olivier Corby, Nicolas Delaforge, Guillaume Erétéo, Catherine Faron-Zucker, Michel Buffa, Fabien Gandon, Guillaume Husson, Freddy Limpens	http://editions-rnti.fr/render_pdf.php?p1&p=1000960	http://editions-rnti.fr/render_pdf.php?p=1000960
Revue des Nouvelles Technologies de l'Information	EGC	2011	Les moteurs de wikis sémantiques : un état de l'art	Cet article est un état de l'art sur les moteurs de wiki sémantique, en particulier sur leur utilisation des technologies du Web sémantique. Les principales notions liées aux wikis sémantiques sont d'abord présentées. Ensuite, plusieurs projets actifs de moteurs de wiki sont comparés selon différents points de vue. Finalement, des recommandations sont données pour le choix d'un moteur de wiki. En conclusion, les auteurs s'interrogent sur les perspectives des wikis sémantiques telles que la faible interopérabilité de certains moteurs.	Thomas Meilender, Nicolas Jay, Jean Lieber, Fabien Palomares	http://editions-rnti.fr/render_pdf.php?p1&p=1001020	http://editions-rnti.fr/render_pdf.php?p=1001020
Revue des Nouvelles Technologies de l'Information	EGC	2011	M3A : Une plateforme d'ingénierie de maintenance assistée par apprentissage automatique		Abdelkader Benameur, Baghdad Atmani	http://editions-rnti.fr/render_pdf.php?p1&p=1000954	http://editions-rnti.fr/render_pdf.php?p=1000954
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mesure de concordance pour les bases de données évidentielles	Dans cet article, nous proposons une mesure de concordance d'une source avec les autres sources. Cette mesure pourra servir à réduire l'importance de ses fonctions de masse avant de les combiner afin de trouver un compromis et donc réduire le conflit. Cette mesure sera illustrée par des données réelles.	Mouna Chebbah, Arnaud Martin, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1000945	http://editions-rnti.fr/render_pdf.php?p=1000945
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mesures d'hétérogénéité sémantique des systèmes P2P non-structurés	L'autonomie des participants dans les systèmes P2P pour le partage de données peut conduire à une situation d'hétérogénéité sémantique dans le cas où les participants utilisent leurs propres ontologies pour représenter leurs données. Dans cet article nous commençons par définir des mesures de disparité entre participants en considérant leurs contextes sémantiques. En considérant la topologie du système et les disparités entre participants, nous proposons des mesures d'hétérogénéité sémantique d'un système P2P non-structuré.	Thomas Cerqueus, Sylvie Cazalens, Philippe Lamarre	http://editions-rnti.fr/render_pdf.php?p1&p=1000984	http://editions-rnti.fr/render_pdf.php?p=1000984
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mixer les moyens pour extraire les gloses	Nous proposons d'extraire des connaissances lexicales en exploitant les « gloses » de mot, ces descriptions spontanées de sens, repérables par des marqueurs lexicaux et des configurations morpho-syntaxiques spécifiques. Ainsi dans l'extrait suivant, le mot testing est suivi d'une glose en c'est-à dire : « 10 % de ces embauches vont porter sur un métier qui monte : le «testing», c'est-à-dire la maîtrise des méthodologies rigoureuses de test des logiciels». Cette approche ouvre des perspectives pour l'acquisition lexicale et terminologique, fondamentale pour de nombreuses tâches. Dans cet article, nous comparons deux façons d'extraire les unités en relation de glose : patrons et statistiques d'associations d'unités sur le web, en les évaluant sur des données réelles.	Augusta Mela, Mathieu Roche, Mohamed el Amine Bekhtaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000935	http://editions-rnti.fr/render_pdf.php?p=1000935
Revue des Nouvelles Technologies de l'Information	EGC	2011	Mobility, Data Mining and Privacy: Mining Human Movement Patterns from Trajectory Data	The technologies of mobile communications and ubiquitous computing pervade our society, and wireless networks sense the movement of people and vehicles, generating large volumes of mobility data, such as mobile phone call records and GPS tracks. This is a scenario of great opportunities and risks : on one side, mining this data can produce useful knowledge, supporting sustainable mobility and intelligent transportation systems ; on the other side, individual privacy is at risk, as the mobility data contain sensitive personal information. A new multidisciplinary research area is emerging at this crossroads of mobility, data mining, and privacy. The talk assesses this research frontier from a data mining perspective, and illustrates the results of a European-wide research project called GeoPKDD, Geographic Privacy-Aware Knowledge Discovery and Delivery. GeoPKDD has created an integrated platform named MATLAS for complex analysis of mobility data, which combines spatio-temporal querying capabilities with data mining, visual analytics and semantic technologies, thus providing a full support for the Mobility Knowledge Discovery process. In this talk, we focus on the key data mining models : trajectory patterns and trajectory clustering, and illustrate the analytical power of our system in unvealing the complexity of urban mobility in a large metropolitan area by means of a large scale experiment, based on a massive real life GPS dataset, obtained from 17,000 vehicles with on-board GPS receivers, tracked during one week of ordinary mobile activity in the urban area of the city of Milan, Italy.	Fosca Giannotti	http://editions-rnti.fr/render_pdf.php?p1&p=1000920	http://editions-rnti.fr/render_pdf.php?p=1000920
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modèle pour une analyse du phénomène de linéarité de catégories sémantiques dans les énoncés en français		Bernard Decobert	http://editions-rnti.fr/render_pdf.php?p1&p=1000982	http://editions-rnti.fr/render_pdf.php?p=1000982
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation de la dynamique de phénomènes spatio-temporels par des séquences de motifs	Dans ce papier, nous proposons un nouveau cadre théorique permettant de modéliser la dynamique de phénomènes spatio-temporels. Nous définissons le concept de séquences spatio-temporelles de motifs afin de capturer les interactions entre des ensembles de propriétés et un phénomène à observer. Un algorithme incrémental est proposé pour extraire des séquences spatiotemporelles de motifs sous contraintes, et une nouvelle structure de données est mise en place afin d'améliorer ses performances. Un prototype a été développé et testé sur des données réelles.	Loïc Mabit, Nazha Selmaoui-Folcher, Frédéric Flouvat	http://editions-rnti.fr/render_pdf.php?p1&p=1001001	http://editions-rnti.fr/render_pdf.php?p=1001001
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation de la propagation de l'information sur le Web : de l'extraction des données à la simulation	Nous proposons un modèle de la propagation de l'information dans un réseau, en détaillant toutes les étapes de sa réalisation et de son utilisation dans un cadre de simulation. A partir de données réelles extraites du Web, nous identifions parmi les sources des catégories de comportements de publication distincts. Nous proposons ensuite une extension d'un modèle de diffusion de l'information existant, afin d'augmenter son pouvoir d'expression, en particulier pour reproduire ces comportements de publication, puis nous le validons sur un exemple de simulation.	François Nel, Marie-Jeanne Lesot, Philippe Capet, Thomas Delavallade	http://editions-rnti.fr/render_pdf.php?p1&p=1001023	http://editions-rnti.fr/render_pdf.php?p=1001023
Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation d'une ressource termino-ontologique de domaine pour l'annotation sémantique de tableaux	Nous proposons dans cet article une modélisation d'une ressource termino-ontologique (RTO) de domaine, guidée par la tâche d'annotation sémantique de tableaux. L'annotation d'un tableau consiste à annoter ses cellules, pour pouvoir ensuite identifier les concepts représentés par ses colonnes et enfin identifier la ou les relations n-aires qu'il représente. La RTO proposée permet d'une part de modéliser dans sa composante lexicale les termes utilisés pour l'annotation des cellules en intégrant la gestion des synonymes et du multilingue, et, d'autre part, de modéliser dans sa composante conceptuelle les concepts symboliques, les concepts numériques et les relations n-aires, qui sont propres au domaine étudié.	Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu, Abir Saïd	http://editions-rnti.fr/render_pdf.php?p1&p=1001021	http://editions-rnti.fr/render_pdf.php?p=1001021
Revue des Nouvelles Technologies de l'Information	EGC	2011	Moteur de questions réponses à partir de données du web sémantique		Michel Plu	http://editions-rnti.fr/render_pdf.php?p1&p=1000958	http://editions-rnti.fr/render_pdf.php?p=1000958
Revue des Nouvelles Technologies de l'Information	EGC	2011	Moteur de questions-réponses d'une base de connaissances	Cet article présente comment la gestion et l'exploitation de connaissances issues du site web Wikipedia ont permis de développer une telle fonction qui a été intégrée depuis février 2010 dans un moteur de recherche internet français pour le grand public. Aujourd'hui cette fonction est capable de répondre à des questions formulées en langage naturelle sur environs 170 000 lieux ou personnes. La formalisation des données extraites de wikipedia en connaissances au format OWL ou RDFS a permis de déduire de nouvelles informations manquantes, de typer les entités nommées trouvées et de traiter de nouvelles formes de questions qui étaient non traitées.	Michel Plu, Johannes Heinecke	http://editions-rnti.fr/render_pdf.php?p1&p=1001024	http://editions-rnti.fr/render_pdf.php?p=1001024
Revue des Nouvelles Technologies de l'Information	EGC	2011	Motifs Séquentiels delta-Libres	Bien que largement étudiée, l'extraction de motifs séquentiels reste une tâche très difficile et pose aussi le défi du grand nombre de motifs produits. Dans cet article, nous proposons une nouvelle approche extrayant les motifs séquentiels les plus généraux à fréquence similaire. Nous montrons en quoi l'extension de cette notion, déjà connue pour les motifs ensemblistes, est un problème particulièrement difficile pour les séquences. Les motifs delta-libres ainsi produits sont en nombre réduit et facilitent les usages d'un processus de fouille et nous montrons leur apport comme descripteurs dans un contexte de classification de séquences.	Marc Plantevit, Chedy Raïssi, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1000926	http://editions-rnti.fr/render_pdf.php?p=1000926
Revue des Nouvelles Technologies de l'Information	EGC	2011	MuMIe: Une Approche Automatique pour l'Interopérabilité des Métadonnées	Avec l'explosion du multimedia, l'utilisation des métadonnées est devenue cruciale pour assurer une bonne gestion des contenus. Cependant, il est nécessaire d assurer un accès uniforme aux métadonnées. Plusieurs techniques ont ainsi été développées afin de réaliser cette interopérabilité. La plupart d'entre elles sont spécifiques à un seul langage de description. Les systèmes de matching existants présentent certaines limites, en particulier dans le traitement des informations structurelles. Nous présentons dans cet article un nouveau système d'intégration qui supporte des schémas provenant de langages descriptifs différents. De plus, la méthode de matching proposée a recours à plusieurs types d'information de façon à augmenter la précision de matching	Samir Amir, Ioan Marius Bilasco, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000985	http://editions-rnti.fr/render_pdf.php?p=1000985
Revue des Nouvelles Technologies de l'Information	EGC	2011	Nomao : la recherche géolocalisée personnalisée		Laurent Candillier	http://editions-rnti.fr/render_pdf.php?p1&p=1000955	http://editions-rnti.fr/render_pdf.php?p=1000955
Revue des Nouvelles Technologies de l'Information	EGC	2011	Nouvelle approche de fouille de graphes AC-réduits fréquents	La fouille de graphes est devenue une piste de recherche intéressante et un défi réel en matière de fouille de données. Parmi les différentes familles de motifs de graphes, les graphes fréquents permettent une caractérisation intéressante des groupes de graphes, ainsi qu'une discrimination des différents graphes lors de la classification ou de la segmentation. A cause de la NP-complétude du test d'isomorphisme de sous-graphes et de l'immensité de l'espace de recherche, les algorithmes de fouille de graphes sont exponentiels en temps d'exécution et/ou occupation mémoire. Dans cet article, nous étudions un nouvel opérateur de projection polynomial nommé AC-projection basé sur une propriété clé du domaine de la programmation par contraintes, à savoir l'arc consistance. Cet opérateur est censé remplacer l'utilisation de l'isomorphisme de sous-graphes en établissant un biais sur la projection. Cette étude est suivie d'une évaluation expérimentale du pouvoir discriminant des patterns AC-réduits découverts.	Brahim Douar, Michel Liquiere, Cherif Chiraz Latiri, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001004	http://editions-rnti.fr/render_pdf.php?p=1001004
Revue des Nouvelles Technologies de l'Information	EGC	2011	Optimisation de l'extraction de l'alignement des ontologies avec la contrainte de différence	Dans ce papier, nous proposons une approche basée sur la programmation par contraintes pour aborder efficacement le problème de l'alignement des ontologies, et plus particulièrement l'extraction des correspondances à partir des mesures de similarités. La complexité de ce problème est accentuée dans les applications à caractère dynamique où l'aspect performance est capital. Plus précisément, nous exploitons la contrainte globale de différence développée dans le domaine de la programmation par contraintes pour extraire un alignement total et injectif. Nous montrons que cette approche est efficace et se prête à une mise en oeuvre à la fois interactive et automatique.	Moussa Benaissa, Yahia Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000991	http://editions-rnti.fr/render_pdf.php?p=1000991
Revue des Nouvelles Technologies de l'Information	EGC	2011	Optimisation directe des poids de modèles dans un prédicteur Bayésien naïf moyenné	Le classifieur Bayésien naïf est un outil de classification efficace en pratique pour de nombreux problèmes réels, en dépit de l'hypothèse restrictive d'indépendance des variables conditionnellement à la classe. Récemment, de nouvelles méthodes permettant d'améliorer la performance de ce classifieur ont vu le jour, sur la base à la fois de sélection de variables et de moyennage de modèles. Dans cet article, nous proposons une extension de la sélection de variables pour le classifieur Bayésien naïf, en considérant un modèle de pondération des variables utilisées et des algorithmes d'optimisation directe de ces poids. Les expérimentations confirment la pertinence de notre approche, en permettant une diminution significative du nombre de variables utilisées, sans perte de performance prédictive.	Marc Boullé, Romain Guigourès	http://editions-rnti.fr/render_pdf.php?p1&p=1000932	http://editions-rnti.fr/render_pdf.php?p=1000932
Revue des Nouvelles Technologies de l'Information	EGC	2011	Parameter-free association rule mining with yacaree		José L Balcazar 	http://editions-rnti.fr/render_pdf.php?p1&p=1000952	http://editions-rnti.fr/render_pdf.php?p=1000952
Revue des Nouvelles Technologies de l'Information	EGC	2011	Point of View Based Clustering of Socio-Semantic Networks		Juan David Cruz, Cécile Bothorel, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000973	http://editions-rnti.fr/render_pdf.php?p=1000973
Revue des Nouvelles Technologies de l'Information	EGC	2011	Pondération et classification simultanée de données binaires et continues	Dans cet article, nous proposons une nouvelle approche de classification topologique et de pondération des variables mixtes (qualitatives et quantitatives codées en binaire) durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage est combiné à un mécanisme de pondération des différentes variables sous forme de poids d'influence sur la pertinence des variables. L'apprentissage des pondérations et des prototypes est réalisé d'une manière simultanée en favorisant une classification optimisée des données. L'approche proposée a été validée sur des données qualitatives codées en binaire et plusieurs bases de données mixtes.	Nicoleta Rogovschi, Mustapha Lebbah, Nistor Grozavu	http://editions-rnti.fr/render_pdf.php?p1&p=1000930	http://editions-rnti.fr/render_pdf.php?p=1000930
Revue des Nouvelles Technologies de l'Information	EGC	2011	PPMI : étude formelle d'une variante à valeurs positives de la PMI		Mohamed Nadif, François Role	http://editions-rnti.fr/render_pdf.php?p1&p=1000965	http://editions-rnti.fr/render_pdf.php?p=1000965
Revue des Nouvelles Technologies de l'Information	EGC	2011	Prévision de trajectoires de cyclones à l'aide de forêts aléatoires avec arbres de régression	Nous présentons une étude pour la prédiction des trajectoires de cyclones dans l'océan Atlantique Nord à partir de données issues d'images satellites. On y extrait des mesures de vitesses de vent, de vorticité, d'humidité (base JRA-25) et des mesures de latitude, de longitude et de vitesse de vent instantanée des cyclones toutes les 6 heures (base IBTrACS). Les modèles de référence à ce jour ne tiennent pas compte des corrélations entre les données et les prévisions ce qui limite leur intérêt pour certains utilisateurs. Nous proposons ainsi de prédire le déplacement en latitude et le déplacement en longitude au même instant à un horizon de 120 h toutes les 6 h à l'aide de forêts aléatoires avec arbres de régression. Sur le long terme, à partir de 18 h, la méthode proposée donne de meilleurs résultats que les méthodes existantes.	Sterenn Liberge, Sileye O. Ba, Philippe Lenca, Ronan Fablet	http://editions-rnti.fr/render_pdf.php?p1&p=1001031	http://editions-rnti.fr/render_pdf.php?p=1001031
Revue des Nouvelles Technologies de l'Information	EGC	2011	Prise en compte du réseau de sources pour la fusion d'informations		Thomas Bärecke, Marie-Jeanne Lesot, Herman Akdag, Bernadette Bouchon-Meunier	http://editions-rnti.fr/render_pdf.php?p1&p=1000980	http://editions-rnti.fr/render_pdf.php?p=1000980
Revue des Nouvelles Technologies de l'Information	EGC	2011	Propositionaliser des attributs numériques sans les discrétiser, ni les agréger	La fouille de données relationnelles considère des données contenues dans au moins deux tables reliées par une association un-à-plusieurs, par exemple des clients et leurs achats, ou des molécules et leurs atomes. Une façon de fouiller ces données consiste à transformer les données en une seule table attribut-valeur. Cette transformation est appelée propositionalisation. Les approches existantes gèrent principalement les attributs catégoriels. Une première solution est donc de discrétiser les attributs numériques pour les transformer en attributs catégoriels. Les approches alternatives, qui gèrent les attributs numériques, consistent à les agréger. Nous proposons une approche duale de la discrétisation, qui inverse l'ordre de traitement du nombre d'objets et du seuil, et dont la discrétisation généralise les quartiles. Nous pouvons ainsi construire des attributs que les approches existantes de propositionalisation ne peuvent pas construire, et qui ne peuvent pas non plus être obtenus par les systèmes complets de fouille de données.	Agnès Braud, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1000998	http://editions-rnti.fr/render_pdf.php?p=1000998
Revue des Nouvelles Technologies de l'Information	EGC	2011	Reasoning about the learning process	Data Mining is faced with new challenges. In emerging applications (like financial data, traffic TCP/IP, sensor networks, etc) data continuously flow eventually at high speed. The processes generating data evolve over time, and the concepts we are learning change. In this talk we present a one-pass classification algorithm able to detect and react to changes. We present a framework that identify contexts using drift detection, characterize contexts using meta-learning, and select the most appropriate base model for the incoming data using unlabeled examples. Evolving data requires that learning algorithms must be able to monitor the learning process and the ability of predictive self-diagnosis. A significant and useful characteristic is diagnostics - not only after failure has occurred, but also predictive (before failure). These aspects require monitoring the evolution of the learning process, taking into account the available resources, and the ability of reasoning and learning about it.	João Gama	http://editions-rnti.fr/render_pdf.php?p1&p=1000921	http://editions-rnti.fr/render_pdf.php?p=1000921
Revue des Nouvelles Technologies de l'Information	EGC	2011	Reconnaissance d'Actions par Modélisation du Mouvement	Cet article propose une approche utilisant les modèles de direction et de magnitude de mouvement pour détecter les actions qui sont effectuées par des êtres humains dans des séquences vidéo. Des mélanges Gaussiens et de lois de von Mises sont estimés à partir des orientations et des magnitudes des vecteurs du flux optique calculés pour chaque bloc de la scène. Les paramètres de ces modèles sont estimés grâce à un algorithme d'apprentissage en ligne. Les actions sont reconnues grâce à une mesure qui se base sur la distance de Bhattacharyya et qui permet de comparer le modèle d'une séquence donnée avec les modèles créés à partir de séquences d'apprentissage. L'approche proposée est évaluée sur deux ensembles de vidéos contenant des actions variées exécutées aussi bien dans des environnements intérieur qu'extérieur.	Yassine Benabbas, Adel Lablack, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000940	http://editions-rnti.fr/render_pdf.php?p=1000940
Revue des Nouvelles Technologies de l'Information	EGC	2011	Résumés et interrogations de logs de requêtes OLAP	Une façon d'assister l'analyse d'entrepôt de données repose sur l'exploitation et la fouille de fichiers logs de requêtes OLAP. Mais, à notre connaissance, il n'existe pas de méthode permettant d'obtenir une représentation d'un tel log qui soit à la fois concise et exploitable. Dans ce papier, nous proposons une méthode pour résumer et interroger des logs de requêtes OLAP. L'idée de base est qu'une requête résume une autre requête et qu'un log, qui est une séquence de requêtes, résume un autre log. Notre cadre formel est composé d'une algèbre simple destinée à résumer des requêtes OLAP, et d'une mesure évaluant la qualité du résumé obtenu. Nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité, et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement. Des tests sur des logs de requêtes MDX ont montré l'intérêt de notre approche.	Julien Aligon, Elsa Negre, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000951	http://editions-rnti.fr/render_pdf.php?p=1000951
Revue des Nouvelles Technologies de l'Information	EGC	2011	Sélection des variables informatives pour l'apprentissage supervisé multi-tables	Dans la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. La plupart des approches existantes opèrent en transformant la représentation multi-tables, notamment par mise à plat. Par conséquent, on perd la représentation initiale naturellement compacte mais également on risque d'introduire des biais statistiques. Notre approche a pour objectif d'évaluer l'informativité des variables explicatives originelles par rapport à la variable cible dans le contexte des relations un-à-plusieurs. Elle consiste à résumer l'information contenue dans chaque variable par un tuple d'attributs représentant les effectifs des modalités de celle-ci. Des modèles en grilles multivariées sont alors employés pour qualifier l'information apportée conjointement par les nouveaux attributs, ce qui revient à une estimation de densité conditionnelle de la variable cible connaissant la variable explicative en relation un-à-plusieurs. Les premières expérimentations sur des bases de données artificielles et réelles montrent qu'on arrive à identifier les variables explicatives potentiellement pertinentes sur tout le domaine relationnel.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000995	http://editions-rnti.fr/render_pdf.php?p=1000995
Revue des Nouvelles Technologies de l'Information	EGC	2011	Service de recherche Web3.0 de contenus audiovisuels		François Paulus, Jérôme Royan	http://editions-rnti.fr/render_pdf.php?p1&p=1000957	http://editions-rnti.fr/render_pdf.php?p=1000957
Revue des Nouvelles Technologies de l'Information	EGC	2011	Structuration automatique des flux télévisuels par apprentissage non supervisé des répétitions		Rakia Jaziri, Mustapha Lebbah, Younès Bennani, Jean-Hugues Chenot	http://editions-rnti.fr/render_pdf.php?p1&p=1000974	http://editions-rnti.fr/render_pdf.php?p=1000974
Revue des Nouvelles Technologies de l'Information	EGC	2011	Système de recherche de musique adaptable à la perception de chaque utilisateur	Dans le cadre de nos travaux sur le portage linguistique des systèmes de gestion de contenu traitant des énoncés spontanés en langue naturelle, nous présentons ici une évaluation du portage d'IMRS (système de recherche de morceau de musique en langue naturelle) Kumamoto (2007) du japonais vers le français. Cette évaluation peut se faire au niveau des représentations internes en les comparant, ou au niveau de la tâche. Ici, nous nous intéressons à une évaluation liée à la tâche en proposant un service Web qui permet de mesurer la performance globale de la nouvelle version obtenue. Nous avons par la suite cherché à améliorer et ajouter de nouvelles fonctionnalités en proposant un service de recherche de musique adaptable à la perception de chaque utilisateur. En effet, un même morceau de musique peut être jugé calme pour un premier auditeur, très calme pour un deuxième, et assez calme pour un troisième, etc. On se demande l'impression finale que porte ce dernier morceau de musique. C'est naturel que les utilisateurs évaluent différemment un même morceau de musique car ils ont des perceptions différentes. Devant cette situation, nous proposons un service de recherche de musique basé des méthodes simples et automatisées et qui sont adaptables à la perception de chaque utilisateur.	Najeh Hajlaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000962	http://editions-rnti.fr/render_pdf.php?p=1000962
Revue des Nouvelles Technologies de l'Information	EGC	2011	Système pour la catégorisation automatique des offres d'emploi en une typologie de fonctions	Depuis les deux dernières décennies, l'augmentation du nombre de sites d'emploi sur Internet a accentué la nécessité de proposer des outils d'aide à la décision adaptés aux besoins des recruteurs. Cet article présente un système pour la catégorisation des textes d'offres d'emploi destinées à être diffusées sur Internet. Après un pré-traitement adapté des offres, les termes descripteurs sont choisis en fonction de leur pouvoir discriminant vis-à-vis des différentes classes ce qui permet de réduire leur nombre de manière significative. Les offres sont ensuite représentées par leurs coordonnées dans l'espace factoriel obtenu par analyse des correspondances et la classification réalisée dans un cadre supervisé à l'aide de SVM.	Julie Séguéla	http://editions-rnti.fr/render_pdf.php?p1&p=1001009	http://editions-rnti.fr/render_pdf.php?p=1001009
Revue des Nouvelles Technologies de l'Information	EGC	2011	Towards a DistributedWeb Search Engine	In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (230 millions at the end of 2009) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability ; in spite of network latency and scattered data. In this talk we present the main challenges behind the design of a distributed Web retrieval system and our research in all the components of a search engine : crawling, indexing, and query processing.	Ricardo Baeza-Yates	http://editions-rnti.fr/render_pdf.php?p1&p=1000919	http://editions-rnti.fr/render_pdf.php?p=1000919
Revue des Nouvelles Technologies de l'Information	EGC	2011	Treillis des concepts SKYLINES : Analyse multidimensionnelle des SKYLINES fondée sur les ensembles en accord	Le concept de SKYLINE a été introduit pour mettre en évidence les objets « les meilleurs » selon différents critères. Une généralisation multidimensionnelle du SKYLINE a été proposée à travers le SKYCUBE qui réunit tous les SKYLINES possibles selon toutes les combinaisons de critères et permet d'analyser les liens entre objets SKYLINES. Comme le data cube, le SKYCUBE s'avère extrêmement volumineux si bien que des approches de réduction sont incontournables. Dans cet article, nous définissons une approche de matérialisation partielle du SKYCUBE. L'idée sous-jacente est d'éliminer de la représentation les Skycuboïdes facilement re-calculables. Pour atteindre cet objectif de réduction, nous caractérisons un cadre formel : le treillis des concepts ACCORDS. Cette structure combine la notion d'ensemble en accord et le treillis des concepts. À partir de cette structure, nous dérivons le treillis des concepts SKYLINES qui en est une instance contrainte. Le point fort de notre approche est d'être orientée attribut ce qui permet de borner le nombre de noeuds du treillis et d'obtenir une navigation efficace à travers les Skycuboïdes.	Sébastien Nedjar, Fabien Pesci, Lotfi Lakhal, Rosine Cicchetti	http://editions-rnti.fr/render_pdf.php?p1&p=1000949	http://editions-rnti.fr/render_pdf.php?p=1000949
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un critère Bayésien pour évaluer la robustesse des règles de classification	L'utilisation de règles de classification dans les modèles prédictifs a été très étudiée ces dernières années. La forme simple et interprétable des règles en font des motifs très populaires. Les classifieurs combinant des règles de classification intéressantes (selon une mesure d'intérêt) offrent de bonnes performances de prédictions. Cependant, les performances de ces classifieurs dépendent de la mesure d'intérêt (e.g., confiance, taux d'accroissement, ... ) et du seuillage (non-trivial) de cette mesure pour déterminer les règles pertinentes. De plus, il est facile de montrer que les règles extraites ne sont pas individuellement robustes. Dans cet article, nous proposons un nouveau critère pour évaluer la robustesse des règles de classification dans les données Booléennes. Notre critère est issu d'une approche Bayésienne : nous proposons une expression analytique de la probabilité d'une règle connaissant les données. Ainsi, les règles les plus probables sont robustes. Le critère Bayésien nous permet alors d'identifier (sans paramètre) les règles robustes parmi un ensemble de règles données.	Marc Boullé, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001013	http://editions-rnti.fr/render_pdf.php?p=1001013
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un cycle de vie complet pour l'enrichissement sémantique des folksonomies	Les tags fournis par les utilisateurs des plateformes de tagging social ne sont pas explicitement liés sémantiquement, et ceci limite considérablement les possibilités d'exploitation de ces données. Nous présentons dans cet article notre approche pour l'enrichissement sémantiques des folksonomies qui intègre une combinaison de traitements automatiques ainsi que la capture des contributions de structuration des utilisateurs via une interface ergonomique. De plus, notre modèle supporte les points de vue qui divergent tout en permettant de les combiner en respectant leur cohérence locale. Cette approche s'adresse aux communautés de connaissances collaborant en ligne, et en intégrant leurs usages, nous sommes en mesure de proposer un cycle de vie complet pour le processus de structuration sémantique des folksonomies. La navigation dans les données de tagging est ainsi améliorée, et les folksonomies peuvent alors être directement intégrées dans la construction de thesauri.	Freddy Limpens, Fabien Gandon, Michel Buffa	http://editions-rnti.fr/render_pdf.php?p1&p=1000990	http://editions-rnti.fr/render_pdf.php?p=1000990
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un outil de géolocalisation et de résumé automatique pour faciliter l'accès à l'information dans des corpus d'actualité		Emilie Guimier De Neef, Aurélien Bossard, Frédéric Gavignet, Olivier Collin	http://editions-rnti.fr/render_pdf.php?p1&p=1000956	http://editions-rnti.fr/render_pdf.php?p=1000956
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un outil de navigation dans un espace sémantique		Yann Vigile Hoareau, Murat Ahat, David Medernach, Marc Bui	http://editions-rnti.fr/render_pdf.php?p1&p=1000959	http://editions-rnti.fr/render_pdf.php?p=1000959
Revue des Nouvelles Technologies de l'Information	EGC	2011	Un système cellulaire neuro-symbolique pour l'extraction et la gestion des connaissances	Le CNSS - Cellular Neuro-Symbolic System - est un système hybride ralliant conjointement le neuro-symbolique et le cellulaire. CNSS permet, à partir d'une base de cas pratique, de faire coopérer un réseau de neurones, un graphe d'induction et un automate cellulaire pour la construction d'un modèle de prédiction. En détectant et en éliminant les individus non applicables et les variables non pertinentes, le réseau de neurones optimise la base d'apprentissage. Le résultat ainsi obtenu est affiné par un processus d'apprentissage symbolique à base de graphe d'induction. Ce raffinement se fait par une modélisation booléenne qui va assister l'apprentissage symbolique à optimiser le graphe d'induction et va assurer, par la suite, la représentation et la génération des règles de classification sous forme conjonctives avant d'entamer la phase de déduction par un moteur d'inférence cellulaire. CNSS a été testé sur plusieurs applications en utilisant des problèmes académiques et réels. Les résultats montrent que le système CNSS a des performances supérieures et de nombreux avantages.	Baghdad Atmani, Mohamed Benamina, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1000934	http://editions-rnti.fr/render_pdf.php?p=1000934
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une Approche à Base de Règles Floues pour les Requêtes à Préférences Contextuelles		Olivier Pivert, Amine Mokhtari, Allel HadjAli	http://editions-rnti.fr/render_pdf.php?p1&p=1000964	http://editions-rnti.fr/render_pdf.php?p=1000964
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une mesure de distance dans l'espace des alignements entre parties potentiellement homologues de deux ontologies légères	Nous proposons dans cet article une méthode qui calcule la distance entre ontologies dans un but d'aide à la décision sur la pertinence ou non de leur fusion. Cette méthode calcule la distance entre parties homologues de deux ontologies par rapport à leurs niveaux de détail et leurs structures taxonomiques, et ce en exploitant les correspondances produites par un alignement préalablement effectué entre ces ontologies, et en adaptant la méthode de la distance d'édition entre arbres ordonnés. Nous limitons notre étude ici aux ontologies légères, c'est à dire des taxonomies représentées en langages OWL, le langage d'ontologies pour le Web. Notre méthode a été implémentée et testée sur des ontologies réelles, et les résultats obtenus semblent prometteurs.	Ammar Mechouche, Nathalie Abadie, Sébastien Mustière	http://editions-rnti.fr/render_pdf.php?p1&p=1001018	http://editions-rnti.fr/render_pdf.php?p=1001018
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une méthodologie de recommandations produits fondée sur l'actionnabilité et l'intérêt économique des clients	Dans un contexte économique difficile, la fidélisation des clients figure au premier rang des préoccupations des entreprises. En effet, selon le Gartner, fidéliser des clients existants coûterait beaucoup moins cher que prospecter de nouveaux clients. Pour y parvenir, les entreprises optimisent la marge et le cycle de vie des clients en développant une relation personnalisée aboutissant à de meilleures recommandations. Dans cet article, nous proposons une méthodologie pour les systèmes de recommandations fondée sur l'analyse des chiffres d'affaires des clients sur des familles de produits. Plus précisément, la méthodologie consiste à extraire des comportements de référence sous la forme de règles d'association et à en évaluer l'intérêt économique et l'actionnabilité. Les recommandations sont réalisées en ciblant les contre-exemples les plus actionnables sur les règles les plus rentables. Notre méthodologie est appliquée sur 12 000 clients et 100 000 produits de VMMatériaux afin d'orienter les commerciaux sur les possibilités d'accroissement de la valeur client.	Julien Blanchard, Thomas Piton, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000946	http://editions-rnti.fr/render_pdf.php?p=1000946
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une nouvelle approche pour l'extraction non supervisée de critères	Récemment de nouvelles techniques regroupées sous le vocable de détection automatique d'opinions (opinion mining) ont fait leur apparition et proposent une évaluation globale d'un document. Ainsi, elles ne permettent pas de mettre en avant le fait que les personnes expriment une opinion très positive du scénario d'un film alors qu'elles trouvent que les acteurs sont médiocres. Dans cet article, nous proposons de caractériser automatiquement les segments de textes relevant d'un critère donné sur un corpus de critiques.	Benjamin Duthil, François Trousset, Mathieu Roche, Michel Plantié, Gérard Dray, Jacky Montmain	http://editions-rnti.fr/render_pdf.php?p1&p=1000981	http://editions-rnti.fr/render_pdf.php?p=1000981
Revue des Nouvelles Technologies de l'Information	EGC	2011	Une nouvelle approche visuelle pour la classification hiérarchique et topologique	Nous proposons dans cet article une nouvelle méthode de classification hiérarchique et topologique. Notre approche consiste à construire de manière auto-organisée une partition de données représentées par un ensemble "forêt" d'arbres répartis sur une grille 2D. Chaque cellule de la grille est modélisée par un arbre dont les noeuds représentent les données. La partition globale obtenue est visualisée à l'aide d'une carte de TreeMap dans laquelle chaque TreeMap représente un arbre de données. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Des résultats numériques et visuels seront présentés et discutés.	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001038	http://editions-rnti.fr/render_pdf.php?p=1001038
Revue des Nouvelles Technologies de l'Information	EGC	2011	Utilisation d'une ontologie du domaine pour la découverte du contenu de bases de données géographiques	L'essor récent des technologies associées à la géomatique a permis la production rapide de nombreuses données géographiques. Or, pour tirer profit de ces données, il convient de pouvoir évaluer leur pertinence et leur complexité vis à vis de l'application à laquelle on les destine. Dans cet article, nous présentons une application permettant à un utilisateur de découvrir le contenu de bases de données géographiques, à savoir, quels types d'entités géographiques sont représentés au sein de chaque base et comment. Pour accéder à ces informations l'utilisateur interroge le système via une ontologie globale du domaine qui décrit les types d'entités topographiques du monde réel. Des ontologies locales ou d'application sont utilisées pour formaliser les spécifications de chaque base de données décrite. Elles sont annotées à l'aide de concepts issus de l'ontologie globale. Ce système est implémenté sous la forme d'une interface Web et inclut un affichage cartographique d'échantillons de données	Ammar Mechouche, Nathalie Abadie, Emeric Prouteau, Sébastien Mustière	http://editions-rnti.fr/render_pdf.php?p1&p=1000983	http://editions-rnti.fr/render_pdf.php?p=1000983
Revue des Nouvelles Technologies de l'Information	EGC	2011	Utilisation de la Machine Cellulaire pour la Détection des Courriels Indésirables		Fatiha Barigou, Baghdad Atmani, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1000979	http://editions-rnti.fr/render_pdf.php?p=1000979
Revue des Nouvelles Technologies de l'Information	EGC	2011	Utiliser des résultats d'alignement pour enrichir une ontologie	En établissant des relations entre des concepts issus de deux ontologies distinctes, les outils d'alignement peuvent être utilisés pour enrichir une des deux ontologies avec les concepts de l'autre. A partir d'une expérience menée dans le cadre du projet ANR GeOnto 1 dans le domaine de la topographie, cet article identifie des traitements complémentaires à l'alignement pour l'enrichissement et montre leur mise en oeuvre dans TaxoMap Framework.	Fayçal Hamdi, Brigitte Safar, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000992	http://editions-rnti.fr/render_pdf.php?p=1000992
Revue des Nouvelles Technologies de l'Information	EGC	2011	Vers la fusion d'informations hétérogènes et partielles pour l'aide au codage diagnostique		Laurent Lecornu, Clara Le Guillou, Frédéric Le Saux, Matthieu Hubert, Julien Montagner, John Puentes, Jean-Michel Cauvin	http://editions-rnti.fr/render_pdf.php?p1&p=1000971	http://editions-rnti.fr/render_pdf.php?p=1000971
Revue des Nouvelles Technologies de l'Information	EGC	2011	Visualisation de l'intra et inter structure des groupes en classification non supervisée	La croissance exponentielle des données engendre des volumétries de bases de données très importantes. Une solution couramment envisagée est l'utilisation d'une description condensée des propriétés et de la structure des données. De ce fait, il devient crucial de disposer d'outils de visualisation capables de représenter la structure des données, non pas à partir des données elles mêmes, mais à partir de ces descriptions condensées. Nous proposons une méthode de description des données à partir de prototypes enrichis puis segmentés à l'aide d'un algorithme adapté de classification non supervisée. Nous introduisons ensuite un procédé de visualisation capable de mettre en valeur la structure intra et inter-groupes des données.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001005	http://editions-rnti.fr/render_pdf.php?p=1001005
Revue des Nouvelles Technologies de l'Information	FDC	2011	Classification faiblement supervisée : arbre de décision probabiliste et apprentissage itératif	Dans le domaine de la fouille de données, il existe plusieurs types demodèles de classification qui dépendent de la complexité de l'ensemble d'apprentissage.Ce papier traite de la classification faiblement supervisée pour laquellel'ensemble d'apprentissage est constitué de données de labels inconnusmais dont les probabilités de classification a priori sont connues. Premièrement,nous proposons une méthode pour apprendre des arbres de décision à l'aide desprobabilités de classification a priori. Deuxièmement, une procédure itérativeest proposée pour modifier les labels des données d'apprentissage, le but étantque les a priori faibles convergent vers des valeurs binaires, et donc vers un apriori fort. Les méthodes proposées sont évaluées sur des jeux de données issusde la base de données UCI, puis nous proposons d'appliquer ces méthodesd'apprentissage dans le cadre de l'acoustique halieutique	Riwal Lefort, Ronan Fablet, Jean-Marc Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001245	http://editions-rnti.fr/render_pdf.php?p=1001245
Revue des Nouvelles Technologies de l'Information	FDC	2011	Classification par voisinages successifs sur des descriptions morphologiques complexes	Afin de classifier des descriptions morphologiques issues de basesde connaissances en biologie, nous proposons une méthode de fouille de donnéesincrémentale, interactive et semi-dirigée. Cette méthode est fondée sur laconstruction itérative du voisinage de la description partielle de l'objet à classer.Nous proposons différents indices de similarité adaptés à la nature complexedes données considérées (multi-valuées, incomplètes et structurées), pour sélectionnerles descriptions les plus proches. Les connaissances du domaine sontutilisées aux différentes étapes du processus de classification, notamment pourle choix de variables discriminantes. A partir de la base de connaissances sur lescoraux des Mascareignes, une application montre l'intérêt de cette approche.	David Grosser, Noël Conruyt, Henri Ralambondrainy	http://editions-rnti.fr/render_pdf.php?p1&p=1001241	http://editions-rnti.fr/render_pdf.php?p=1001241
Revue des Nouvelles Technologies de l'Information	FDC	2011	Clustering multi-vues : une approche centralisée	Nous abordons dans ce papier le problème de la classification nonsuperviséemulti-vues, i.e. où les données peuvent être décrites par plusieursensembles de variables ou par plusieurs matrices de proximités. De nombreuxdomaines d'applications sont concernés, tels la Recherche d'Information, la Biologie,la Chimie et le Marketing. L'objet de cet axe de recherche est de proposerun cadre théorique et méthodologique permettant la découverte d'une classificationréalisant un consensus entre les organisations émanant de toutes les vues.Il convient alors de combiner les informations de chacune des vues par l'intermédiaired'un processus de fusion consistant à identifier l'accord entre les vueset à réduire le conflit. Plusieurs stratégies de fusion peuvent être appliquées,en amont, en aval, ou pendant le processus de classification. Nous présentonsles différentes solutions de fusion envisageables suivant différents contextes applicatifs,puis nous nous focalisons sur des techniques dites centralisées. Nousproposons une approche de classification non supervisée floue qui généralise différentessolutions de fusion et nous présentons une extension à noyaux de cetteapproche, permettant le traitement de données hétérogènes. Nousmontrons l'apportthéorique et expérimental de cette approche sur des jeux de données benchmarkssynthétiques et réels.	Jacques-Henri Sublemontier, Guillaume Cleuziou, Matthieu Exbrayat, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001242	http://editions-rnti.fr/render_pdf.php?p=1001242
Revue des Nouvelles Technologies de l'Information	FDC	2011	Estimation de la fiabilité des sources des bases de données évidentielles	Dans cet article, nous proposons une méthode permettant l'estimationdes fiabilités 1 des sources à partir de toutes leurs fonctions de croyance stockéesdans des bases de données évidentielles. Nous proposons également d'assurer lemême niveau de fiabilité pour toutes ces fonctions de croyance. Les degrés defiabilité des sources sont utilisés pour affaiblir leurs fonctions de croyance stockéesdans des bases de données évidentielles. Cette méthode a été évaluée surdes données radar réelles et a montré une amélioration remarquable des fiabilitésaprès affaiblissement.	Mouna Chebbah, Arnaud Martin, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1001246	http://editions-rnti.fr/render_pdf.php?p=1001246
Revue des Nouvelles Technologies de l'Information	FDC	2011	Exploration visuelle de données spatiotemporelles imprécises : application en archéologie	Dans cet article, nous proposons d'exploiter une technique spécifique d'exploration visuelle d'un ensemble d'objets archéologiques dont les composantes spatiales et temporelles sont représentées par des ensembles flous convexes et normalisés. Pour cela, en nous basant sur la définition de vecteurs multidimensionnels issus de défuzzifications ou de comparaisons entre deux nombres flous, nous construisons une image couleur dans laquelle chaque pixel représente un objet. L'image couleur donne un rendu synthétique de l'information permettant à l'utilisateur de l'observer et de l'analyser.	Cyril de Runz, Frédéric Blanchard, Philippe Vautrot, Eric Desjardin, Michel Herbin	http://editions-rnti.fr/render_pdf.php?p1&p=1001249	http://editions-rnti.fr/render_pdf.php?p=1001249
Revue des Nouvelles Technologies de l'Information	FDC	2011	Extraction des itemsets fréquents à partir de données évidentielles : application à une base de données éducationnelles	Dans cet article, nous étudions le problème de l'extraction des itemsetsfréquents (EIF) à partir de données imparfaites, et plus particulièrement cequ'on appelle désormais les données évidentielles. Une base de données évidentiellestocke en effet des données dont l'imperfection est modélisée via la théoriede l'évidence. Nous introduisons une nouvelle approche d'EIF qui se base surune structure de données de type arbre. Cette structure est adaptée à la naturecomplexe des données. La technique que nous avons conçue, génère jusqu'à50% de la totalité des itemsets fréquents lors du premier parcours de l'arbre.Elle a été appliquée sur des bases de données synthétiques ainsi que sur unebase de données éducationnelles. Les expérimentations menées sur la nouvelleméthode, montrent qu'elle est plus performante en terme de temps d'exécutionen comparaison avec les méthodes existantes d'EIF.	Mohamed Anis Bach Tobji, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1001247	http://editions-rnti.fr/render_pdf.php?p=1001247
Revue des Nouvelles Technologies de l'Information	FDC	2011	Modèles de mélanges topologiques pour la classification de données catégorielles et mixtes	Cet article présente une méthode basée sur les cartes auto-organisatricesprobabilistes dédiées à la classification non supervisée et la visualisation de donnéescatégorielles et des données mixtes contenant des composantes quantitativeset binaires. Pour chacun de ces types de données, nous proposons un formalismeprobabiliste dans lequel les unités de la carte topologique sont représentéespar un modèle de mélanges de loi de Bernoulli, dans le cas des donnéesbinaires et par un modèle de mélanges de lois de Bernoulli et Gaussienne dans lecas des données mixtes. Dans cette étude, la carte topologique est vue comme unmodèle génératif et est revisitée dans un formalisme probabiliste de modèles demélanges. L'idée de base de ce travail repose sur le principe de la conservationde la structure initiale des données en utilisant le formalisme probabiliste. Lesmodèles de mélanges proposés ici vérifient ce principe et fournissent des résultatsdirectement interprétables par rapport aux données initiales, qu'elles soientsimplement binaires ou mixtes. L'apprentissage consiste alors à estimer les paramètresdu modèle en maximisant la vraisemblance des données d'apprentissage.L'algorithme d'apprentissage (PrMTM :Probabilistic Mixed Topological Map)que nous proposons est basé sur l'algorithme EM (Estimation-Maximisation).Nous avons montré que l'algorithme à base de modèles de mélanges fournitdifférentes informations pertinentes qui peuvent être utilisées dans des applicationspratiques. Nos approches ont été validées sur différentes bases de donnéesréelles et fournissent des résultats prometteurs.	Nicoleta Rogovschi, Mustapha Lebbah, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001240	http://editions-rnti.fr/render_pdf.php?p=1001240
Revue des Nouvelles Technologies de l'Information	FDC	2011	Modélisation et extraction des liens complexes entre variables. Application à des données socio-économiques	Nous nous intéressons ici à un type particulier de complexité qui estcelle des liaisons entre variables. Il existe des modèles statistiques qui ont étéconstruits pour traiter certains aspects de cette complexité. Ainsi le modèle linéairegénéral (Azaïs et Bardet 2005) permet de rendre compte d'aspects spécifiquesde la complexité comme les interactions d'ordre quelconque, les liaisonsnégatives au même titre que les positives, et les « contrastes ». Mais cesméthodes sont mal adaptées au cas d'un grand nombre de variables et ellesexigent une explicitation a priori des liaisons en jeu. Nous présentons notreméthode MIDOVA qui extrait directement des données le même type de liaisonsque le modèle linéaire général, sans nécessiter d'hypothèses contraignantes,tout en étant compatible avec un grand nombre de variables, pour l'instantqualitatives. Nous l'illustrons en l'appliquant à des données issues de l'enquêtePAPFEM, réalisée en 2001 par l'Office National de la Famille et de la Populationen Tunisie, et nous mettons au jour le lien particulièrement complexe entrela pauvreté du ménage et la situation socio-économique des deux conjoints.	Martine Cadot, Dhouha El Haj Ali	http://editions-rnti.fr/render_pdf.php?p1&p=1001239	http://editions-rnti.fr/render_pdf.php?p=1001239
Revue des Nouvelles Technologies de l'Information	FDC	2011	Organiser la fusion d'informations par l'analyse formelle de concepts	Le problème étudié dans cet article est la fusion d'informations provenantde différentes sources : il s'agit de constituer une nouvelle informationen fusionnant celles qui sont délivrées par les sources. Le problème de la fusiondevient délicat surtout quand les sources fournissent des informations contradictoires.Parfois, les résultats de la fusion globale, appliquée à l'ensemble de toutesles sources, ne peuvent pas être directement utilisés pour une décision. Dans cepapier, nous utiliserons l'analyse formelle de concepts pour organiser les résultatsdes méthodes de fusion d'informations numériques. Cette approche permetd'associer un sous-ensemble de sources avec son résultat de fusion. Quand le résultatglobal de la fusion est imprécis, la méthode permet à l'utilisateur d'identifierun sous-ensemble maximal de sources qui a un résultat de fusion plus préciset utile.Enfin, une expérience en agronomie nous sert de première validationpour l'aide à la décision de pratiques agricoles	Zainab Assaghir, Mehdi Kaytoue-Uberall, Amedeo Napoli, Henri Prade	http://editions-rnti.fr/render_pdf.php?p1&p=1001250	http://editions-rnti.fr/render_pdf.php?p=1001250
Revue des Nouvelles Technologies de l'Information	FDC	2011	Recalage et fusion d'images sonar multivues : critère de dissimilarité et ignorance	Ce papier présente une application pour le recalage et la fusion d'imagessonar classifiées. Nous adaptons ici la méthode présentée dans un précédentpapier à des données multivues. Pour la caractérisation de fond marin, nousavons besoin de fusionner des images sonar multivues afin d'améliorer les résultats.Néanmoins, avant de pouvoir fusionner ces images, il faut les recaler. Notreapproche de recalage s'appuie sur un critère de dissimilarité calculé à partir duconflit issu de la combinaison des fonctions de croyance. L'utilisation de la théoriedes fonctions de croyance offre un cadre théorique adequat qui permet unebonne modélisation des imperfections, et qui a déjà prouvé son intérêt pour lafusion de classifieurs en traitement d'images.	Cedric Rominger, Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001248	http://editions-rnti.fr/render_pdf.php?p=1001248
Revue des Nouvelles Technologies de l'Information	FDC	2011	SLEMC : Apprentissage semi-supervisé enrichi par de multiples clusterings	La tâche de classification supervisée consiste à induire un modèle deprédiction en utilisant un ensemble d'échantillons étiquetés. La précision dumodèle augmente généralement avec le nombre d'échantillons disponibles. Aucontraire, lorsque seuls quelques échantillons sont disponibles pour l'apprentissage,le modèle qui en résulte donne généralement des résultats médiocres. Malheureusement,comme la tâche d'étiquetage est souvent très coûteuse en temps,les utilisateurs fournissent principalement un très petit nombre d'objets étiquetés.Dans ce cas, de nombreux échantillons non étiquetés sont généralementdisponibles. Lorsque les deux types de données sont disponibles, les méthodesd'apprentissage semi-supervisé peuvent être utilisées pour tirer parti à la fois desdonnées étiquetées et non étiquetés. Dans cet article nous nous concentrons surle cas où le nombre d'échantillons étiquetés est très limité. Ainsi, définir des approchescapables de gérer ce type de problème représente un verrou scientifiqueimportant à lever. Dans cet article, nous passons en revue et comparons les différentsalgorithmes d'apprentissage semi-supervisé existant, et nous présentonségalement une nouvelle manière de combiner apprentissage supervisé et non superviséafin d'utiliser conjointement les données étiquetées et non étiquetées. Laméthode proposée utilise des résultats de clustering pour produire des descripteurssupplémentaires des données, qui sont utilisés alors lors d'une étape d'apprentissagesupervisé. L'efficacité de la méthode est évaluée sur différents jeuxde données de l'UCI 1 et sur une application réelle de classification d'une imagede télédétection à très haute résolution avec un nombre très faible d'échantillonsétiquetés. Les expériences donnent des indications sur l'intérêt de combiner desdonnées étiquetées et non dans le cadre de l'apprentissage automatique.	Cédric Wemmert, Germain Forestier	http://editions-rnti.fr/render_pdf.php?p1&p=1001244	http://editions-rnti.fr/render_pdf.php?p=1001244
Revue des Nouvelles Technologies de l'Information	FDC	2011	Traitement automatique d'informations textuelles complexes : connaissances linguistiques hétérogènes et à granularité variable	Dans cet article, nous présentons une méthodologie permettant le traitementet la structuration de données linguistiques complexes. Par données complexes,nous envisageons des informations textuelles présentant la particularitéd'être à la fois hétérogènes sémantiquement et à granularité variable. Pour passerd'une structure linguistique constituée d'objets complexes à une organisationdes données permettant l'application de méthodes statistiques et/ou de fouille dedonnées, nous proposons un modèle de représentation des unités du discours.Ce travail est mené dans le cadre d'un projet visant la mise en oeuvre d'un prototyped'aide à la mise à jour de documents encyclopédiques articulé autour durepérage automatique de zones textuelles contenant de l'information obsolète.	Marion Laignelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001238	http://editions-rnti.fr/render_pdf.php?p=1001238
Revue des Nouvelles Technologies de l'Information	FDC	2011	Une famille de matrices sparses pour une modélisation multi-échelle par blocs	La sériation est une technique d'analyse de données qui ordonne lesobservations directement à partir de leur tableau de valeurs afin de révéler unestructure intrinsèque à ces données. Une telle approche présente de nombreuxavantages de visualisation mais dès lors que les données sont bruitées ou que lesgroupes se superposent, la visualisation de toute structure devient difficile. Pourfaire face à ces problèmes, nous introduisons de la parcimonie dans les donnéesà travers une famille de matrices indicatrices de voisins communs. Celles-ci sontordonnées selon un algorithme de type branch and bound et la matrice révélantla meilleure structure au sens de "diagonale par blocs" est sélectionnée au moyend'un critère dérivé des problématiques de compression de données. Cet outil departitionnement identifie des sous-ensembles de données relatifs aux clusterstout en écartant celles qui sont bruitées ou extrêmes ce qui permet de visualiserla structure globale intrinsèque aux données. Cependant, une trop grande sparsitédes données amène parfois à l'éviction de données sous-représentées; nousproposons à cet effet, une approche multi-échelle combinant différents niveauxde sparsité dans une même visualisation.	Camille Brunet, Thomas Villman, Vincent Vigneron	http://editions-rnti.fr/render_pdf.php?p1&p=1001243	http://editions-rnti.fr/render_pdf.php?p=1001243
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Approche pour le suivi de l'évolution des données d'usage du Web: application sur un jeu de données en marketing	Dans la fouille des flux des données d'usage du Web, la dimension temporelle joue un rôle très important car les comportements des internautes peuvent changer au cours du temps. Dans cet article, nous présentons l'application d'une approche de classification automatique basée sur des fenêtres sautantes pour la détection et le suivi de changements sur un jeu de données en marketing. Cette approche combine les cartes auto organisatrices de Kohonen et la méthode de Ward pour la découverte automatique du nombre de clusters de comportement ainsi que deux indices de validation basés sur l'extension pour la détection des changements au cours du temps.	Alzennyr Da Silva, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001799	http://editions-rnti.fr/render_pdf.php?p=1001799
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Approches multidimensionnelles pour l'analyse de la relation toxicité-efficacité d'un traitement pour le cancer de l'estomac. Application  dans le cadre d'une méta-analyse	Les relations différentielles entre toxicité et efficacité sont étudiées pour deux chimiothérapies dans des essais cliniques randomisés, par des analyses exploratoires multidimensionnelles suivies d'une méta-analyse pour valider leurs résultats.L'analyse des correspondances multiples réalisée sur les données fusionnées pour la méta-analyse, a mis en évidence une proximité entre la toxicité et l'efficacité (réponse au traitement) pour le traitement contenant du Docetaxel (Traitement B). En effet les patients ayant répondu au traitement sont ceux qui ont le plus développé des toxicités de grades 3-4. De façon intéressante, on observe en tendance un lien plus faible toxicité-réponse pour le traitement en comparaison moins efficace. L'introduction de la baseline « coefficient de Karnofsky » comme variable illustrative a mis en évidence la proximité entre les toxicités jugées gênantes (Anémia, Trombocytopenia, et Renal impairment), mais toujours spécifique du Docetaxel, et un coefficient de Karnofsky faible indiquant un état affaibli des patients à l'entrée de l'étude, ce qui suggère une hypothèse pour expliquer la susceptibilité de cette classe de patients à développer ce genre de pathologies. Les plans factoriels sélectionnés suggèrent de classifier automatiquement les patients en trois classes. Nous avons donc construit la partition correspondante à partir d'une classification ascendante hiérarchique, et avons projetées les classes sur le premier plan factoriel sélectionné. On valide ensuite la différentiation de la position des centres de gravité des classes ce qui permet de distinguer significativement les trois groupes de patients.	Mounia Abouloussoud, Lucile Awad, Mireille Gettler-Summa, Brigitte Le Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001797	http://editions-rnti.fr/render_pdf.php?p=1001797
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Clustering models for high dimensional, temporal, and dissimilarity data		Maurizio Vichi	http://editions-rnti.fr/render_pdf.php?p1&p=1001792	http://editions-rnti.fr/render_pdf.php?p=1001792
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Determinants of the net interest margin in the banking institutions: contribution of PLS regression compared to the principal components regression	Information :Il est apparu, après publication, que la rédaction de cet article soulevait des problèmes. La Direction de la Revue a transmis ces difficultés aux auteurs de l'article dont la publication est suspendue.	Salwa Benammou, Mlayeh Chahrazed	http://editions-rnti.fr/render_pdf.php?p1&p=1001769	http://editions-rnti.fr/render_pdf.php?p=1001769
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Développements récents en analyse des correspondances multiples	Depuis une dizaine d'années, la taille des données croit plus vite que la puissance des processeurs.  Lorsque les données disponibles sont pratiquement infinies, c'est le temps de calcul qui limite les possibilités de l'apprentissage statistique. Ce document montre que ce changement d'échelle nous conduit vers un compromis qualitativement différent dont les conséquences ne sont pas évidentes. En particulier, bien que la descente de gradient stochastique soit un algorithme d'optimisation médiocre, on montrera, en théorie et en pratique, que sa performance est excellente pour l'apprentissage statistique à grande échelle.	Jean Chiche, Brigitte Le Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001795	http://editions-rnti.fr/render_pdf.php?p=1001795
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Discussion sur les prédicteurs conformes proposés par Alex Gammerman et Vladimir Vovk	Conformer predictors approach seems to be new and powerful. Its main advantage is that it is nonparametric and based only on the i.i.d. assumption. In comparison to the Bayesian approach, no prior distribution is used. The main theoretical result is the proof of validity of proposed conformal predictors. The second result is that asymptotically the relative number of cases when the real output value is within confidence interval converges to the average value of conformal predictors. The proposed technique is now applied to a large variety of practical problems. Two drawbacks of the approach are still mentioned in this discussion	Alexey Chervonenkis	http://editions-rnti.fr/render_pdf.php?p1&p=1001782	http://editions-rnti.fr/render_pdf.php?p=1001782
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Extension de l'ACPVI multibloc à la discrimination qualitative. Application en épidémiologie vétérinaire	Une nouvelle présentation de l'Analyse en Composantes Principales sur Variables Instrumentales Multibloc, dont l'objectif est de prédire un tableau Y à partir de plusieurs tableaux (X_1, ..., X_K), est proposée. Elle est basée sur la détermination, pas à pas, de composantes dans l'espace des variables Y. Chaque composante est projetée sur les espaces engendrés respectivement par les variables des tableaux X_k. L'ACPVI multibloc consiste à maximiser, en moyenne, la variance restituée par ces projections. Cette méthode multibloc est ensuite appliquée au cadre de la description et la prédiction d'une variable qualitative $y$ par un ensemble de variables qualitatives (x_1, ..., x_K), chaque variable étant codée en un tableau contenant les indicatrices de ses modalités. La discrimination est opérée sur la base de composantes globales mutuellement orthogonales résumant l'ensemble des variables explicatives. La démarche d'analyse est comparée à d'autres méthodes de discrimination qualitative et illustrée sur la base d'une étude de cas en épidémiologie vétérinaire.	Stéphanie Bougeard, El Mostafa Qannari, Christelle Fablet	http://editions-rnti.fr/render_pdf.php?p1&p=1001765	http://editions-rnti.fr/render_pdf.php?p=1001765
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	In memoriam Henry Rouanet (1931-2008)		Philippe Bonnet, Brigitte Le Roux, Frédéric Lebaron	http://editions-rnti.fr/render_pdf.php?p1&p=1001793	http://editions-rnti.fr/render_pdf.php?p=1001793
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Introduction à la Phylogénie moléculaire	Cet article est une introduction au domaine de la phylogénie moléculaire et en particulier à la robustesse des arbres phylogénétiques. Nous commençons par une brève présentation historique du domaine avant de passer en revue les méthodes de reconstruction les plus populaires. Nous nous intéressons tout particulièrement à la méthode du maximum de vraisemblance. Cette méthode nécessite de construire un modèle probabiliste d'évolution des macromolécules biologiques mais fournit en contrepartie un cadre statistique propice à quantifier la variabilité de l'arbre estimé. Nous présentons tout d'abord les modèles d'évolution couramment utilisé, puis le calcul de la vraisemblance avant de montrer que la nature discrète de l'arbre rend caducs les outils traditionnels d'étude de la variabilité.	M. Mariadassou, Bar Hen	http://editions-rnti.fr/render_pdf.php?p1&p=1001790	http://editions-rnti.fr/render_pdf.php?p=1001790
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	L'analyse géométrique des données dans un programme de recherche sociologique: Le cas de la sociologie de Bourdieu	Le premier objectif de ce texte est de fournir quelques repères sur ce plan et de mettre en avant la solution retenue par Bourdieu, du moins depuis La distinction : la modélisation géométrique des données, fondée sur l'analyse géométrique des données. Dans un deuxième temps, nous insistons sur le rôle de la notion de multidimensionnalité dans ce processus, avec l'exemple central de « L'anatomie du goût » / La distinction. Dans une troisième partie, nous montrons que la notion de champ développée par Bourdieu est constamment opérationnalisée à travers l'analyse géométrique des données, avec l'exemple du « Patronat ». Nous essayons enfin d'inférer de la pratique de Bourdieu un programme de recherche sociologique général fondé sur l'usage de l'analyse géométrique des données.	Frédéric Lebaron	http://editions-rnti.fr/render_pdf.php?p1&p=1001794	http://editions-rnti.fr/render_pdf.php?p=1001794
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	L'apprentissage statistique à grande échelle	Depuis une dizaine d'années, la taille des données croit plus vite que la puissance des processeurs.  Lorsque les données disponibles sont pratiquement infinies, c'est le temps de calcul qui limite les possibilités de l'apprentissage statistique. Ce document montre que ce changement d'échelle nous conduit vers un compromis qualitativement différent dont les conséquences ne sont pas évidentes. En particulier, bien que la descente de gradient stochastique soit un algorithme d'optimisation médiocre, on montrera, en théorie et en pratique, que sa performance est excellente pour l'apprentissage statistique à grande échelle.	Léon Bottou, Olivier Bousquet	http://editions-rnti.fr/render_pdf.php?p1&p=1001788	http://editions-rnti.fr/render_pdf.php?p=1001788
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	L'incohérence de l'aire sous la courbe ROC, que faire à ce propos ?	Différents critères sont largement utilisés pour évaluer la performance de règles de classement. L'un d'eux est l'aire sous la courbe ROC (AUC : the area under the curve). Cette mesure a l'agréable propriété de synthétiser la performance pour tous les seuils de classement possibles. Malheureusement, au coeur de l'AUC se trouve une distribution qui dépend de l'outil de classification dont la performance est évaluée, si bien que les estimations qui utilisent cette mesure sont fondamentalement incohérentes: c'est-à-dire qu'aucune comparaison ne peut être faite quand l'AUC est utilisé. Cette incohérence est examinée, ses implications sont présentées, et une alternative cohérente est décrite.	David J. Hand	http://editions-rnti.fr/render_pdf.php?p1&p=1001789	http://editions-rnti.fr/render_pdf.php?p=1001789
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	La sémantique d'un récit: état de l'art et perspectives	On s'intéresse ici à la sémantique de l'information sous deux aspects : (i) l'ensemble de toutes les relations binaires, (ii) la reconnaissance et le suivi des changements ainsi que des anomalies. Dans le premier cas, on munit l'espace des textes ou des sous textes d'un côté, et l'espace des mots de l'autre côté, d' une métrique euclidienne dans un cadre commun. Dans le deuxième cas, on modélise l'information par une métrique ultramétrique. Une façon d'aboutir à une représentation euclidienne consiste à faire une analyse des correspondances, qui s'applique par exemple à des tableaux en entrée de fréquences. A partir de la représentation euclidienne, on munit l'espace de l'information d'une ultramétrique qui permet de construire une classification hiérarchique. En l'occurrence dans ce travail, on contraindra l'ultramétrique à respecter l'ordre induit par les séquences temporelles. Après une revue de l'existant pour l'analyse de la sémantique des scripts de films, on s'intéresse à la question suivante : serait-il possible d'analyser de façon analogue la sémantique de la littérature de la recherche.	Fionn Murtagh, Adam Ganz	http://editions-rnti.fr/render_pdf.php?p1&p=1001786	http://editions-rnti.fr/render_pdf.php?p=1001786
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	L'arbre à noeuds probabilistes: Une nouvelle approche à la construction d'arbres de  prédictions	Nous montrons la connexion entre, d'une part, l'analyse de données symboliques, et, d'autre part, certains algorithmes d'apprentissage supervisé pour la prédiction d'une variable réponse qualitative ou quantitative. Dans le contexte des données symboliques, nous avions développé un algorithme de prédiction à arbre; cela nous avait permis de traiter des données imprécises et de construire des arbres de prédiction classiques à partir de ce type de données. Par la suite, nous avons repris le problème de la construction d'arbres pour des données précises (numérique), mais en permettant des noeuds probabilistes ou 'tendres', c'est-à-dire des noeuds correspondant à des décisions probabiliste du type : 'aller à gauche avec probabilité p et à droite avec probabilité 1-p'. Un tel arbre décrit la distribution prédictive conditionnelle de la variable réponse comme un mélange de distributions, tel que les coefficients des composantes du mélange dépendent des variables : ces coefficients sont en effet des produits de fonctions sigmoïdes de certaines variables de prédiction choisies par l'algorithme guidé par les données.  Nous décrivons une approche EM pour l'estimation des paramètres du modèle correspondant. La méthode a été évaluée par des simulations et des analyses de données réelles. Nous discutons, pour conclure, les avantages et les limites de ce type d'arbre en comparaison avec les arbres conventionnels.	Antonio Ciampi	http://editions-rnti.fr/render_pdf.php?p1&p=1001787	http://editions-rnti.fr/render_pdf.php?p=1001787
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Modélisation de séries temporelles multiples et multidimensionnelles	On présente ici une recherche de modélisation de séries temporelles multiples et multidimensionnelles extraites de données de sites officiels. La difficulté réside d'une part dans la construction des bases de données en raison des différents formats initiaux, des incohérences et des données manquantes, d'autre part dans le grand nombre de variables, endogènes et exogènes, et dans la multiplicité des entrées admissibles pour le problème. Les séries temporelles exogènes sont de plus munies d'une partition a priori. On présente dans cette recherche une approche pour la réduction des variables  et des solutions de modélisation de ces données complexes que l'on construit à partir d'adaptation de solutions classiques au contexte temporel multidimensionnel.	Mireille Gettler-Summa, Bernard Goldfarb, Laurent Schwartz, Jean Marc Steyaert, Frédérique Lefaudeux	http://editions-rnti.fr/render_pdf.php?p1&p=1001791	http://editions-rnti.fr/render_pdf.php?p=1001791
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Optimisation pour plans d'expériences combinés à base des règles de lecture des cartes de contrôle	Les plans d'expériences forment un outil de pré-optimisation puissant. Ils sont souvent sous -exploités en industrie non électrique ou chimique à cause du contraste qualité des résultats - coût d'expérimentation et la difficulté de modélisation. Nous montrerons que le coût d'expérimentation n'est pas nécessairement déterminé par le nombre d'expériences. Il peut être exclusivement dû au nombre de variations de niveaux subits par les entrées. Nous proposons dans ce cas, une méthode simple pour réduire le prix d'expérimentation sans passer par les aliases: Il suffit d'attribuer convenablement les facteurs aux colonnes de la matrice des expériences. De plus, nous présentons une approche pratique pour la conception du modèle mathématique à optimiser, en se référant aux règles de lecture des cartes de contrôle. Ceci, dans le cas de plusieurs plans d'expériences simultanés.	Abdellah Ait Ouahman, Aomar Ibourk, Abdelhakim Rharrassi	http://editions-rnti.fr/render_pdf.php?p1&p=1001766	http://editions-rnti.fr/render_pdf.php?p=1001766
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Prédictions contrôlées en apprentissage automatique	Les récentes avancées obtenues en apprentissage automatique rendent possible la conception d'algorithmes efficients de prédiction pour des ensembles de données à grand nombre de paramètres. Cet article décrit une nouvelle méthode pour contrôler les prédictions élaborées par de nombreux algorithmes, incluant les machines à vecteurs support, la régression ridge à noyau, les plus proches voisins par noyau et bien d'autres méthodes correspondant à l'actuel état de l'art. Les prédictions contrôlées pour les étiquettes de nouveaux objets comportent des mesures quantitatives de leur précision et de leur fiabilité. Nous prouvons que ces mesures sont valides sous hypothèse de randomisation, traditionnelle en apprentissage automatique : les objets et leurs étiquettes sont supposés indépendamment générés par la même distribution de probabilité. En particulier, il devient possible de contrôler (aux fluctuations statistiques près) le nombre de prédictions erronées en choisissant un niveau de confiance approprié. La validité étant assurée, l'objectif restant pour les prédictions contrôlées est l'efficience : prendre au mieux les caractéristiques des nouveaux objets ainsi que l'information disponible pour produire des prédictions aussi précises que possible. Ceci peut être obtenu avec succès en utilisant toute la puissance des méthodes modernes d'apprentissage automatique	Alex Gammerman, Vladimir Vovk	http://editions-rnti.fr/render_pdf.php?p1&p=1001781	http://editions-rnti.fr/render_pdf.php?p=1001781
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Processus d'extraction des règles floues pour la classification phonémique. Etude du corpus de parole TIMIT	Plusieurs techniques de classification et de décision ont été élaborées. La création des règles floues, pouvant servir à une bonne classification, constitue une difficulté majeure pour ces techniques. C'est dans cette perspective que cet article s'intéresse à proposer une nouvelle approche de processus d'extraction des règles floues pour la classification phonémique.	Dorra Ben Ayed Mezghanni, Noureddine Ellouze	http://editions-rnti.fr/render_pdf.php?p1&p=1001770	http://editions-rnti.fr/render_pdf.php?p=1001770
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Quelques notes sur Choriogenèse : la genèse de l'espace. Comment passer d'un ensemble discret à un espace continu... et non l'inverse ?		Jean-Paul Benzecri	http://editions-rnti.fr/render_pdf.php?p1&p=1001780	http://editions-rnti.fr/render_pdf.php?p=1001780
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Structure géométrique des distances hiérarchiques	La structure théorique de la classification hiérarchique nécessaire à la construction de l'arbre ou du dendrogramme est la base pour montrer la relation théorique des distances hiérarchiques géométriques pour une séquence de hiérarchies partielles où, lorsque lors de la suivante élection de classes à ajouter deux hiérarchies partielles égales existent, alors la hiérarchie partielle à ajouter dépendra des distances géométriques présentant les hiérarchies partielles se rapportant à celles de la troisième classe. Le développement théorique est illustré par le biais d'applications avec des données sur l'effet de la corrosion atmosphérique sur l'acier structurel dont est constituée l'infrastructure civile dans la Ville de Mexico, ainsi que sur l'évaluation du rendement des enseignants de troisième cycle au Mexique.	Francisco Casanova del Angel	http://editions-rnti.fr/render_pdf.php?p1&p=1001767	http://editions-rnti.fr/render_pdf.php?p=1001767
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Sur l'utilisation conjointe de la régression sur composantes principales et des ondelettes	La régression sur composantes principales (RCP) est une régression sur les facteurs d'une ACP préalablement effectuée sur des variables initialement corrélées. L'utilisation de l'ACP permet de remplacer les variables initiales, par des composantes principales qui conservent la quasi-totalité de l'information, et qui présentent l'avantage d'être non corrélées. Ces composantes, sont prises comme variables explicatives pour une régression linéaire multiple. La qualité de la modélisation par RCP reste affectée par l'existence de bruit dans les variables initiales. Nous proposons dans ce travail un débruitage des données par ondelettes (wavelets) permettant de séparer le signal du bruit sans perte d'information. Nous montrons, sur des données boursières françaises, que l'élimination du bruit sur les composantes principales par un seuillage doux à base d'ondelettes améliore la qualité d'ajustement du modèle de régression ainsi que la qualité des prévisions. Nous confirmons le résultat par simulation.	Salwa Benammou, Nabiha Haouas, Zied Kacem	http://editions-rnti.fr/render_pdf.php?p1&p=1001768	http://editions-rnti.fr/render_pdf.php?p=1001768
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Typologie des consommateurs et mesure de loyauté / fidélité	Face aux problèmes de pouvoir d'achat, les consommateurs changent leur manière d'acheter: certains se dirigent vers les marques premiers prix d'autres restent fidèles à leurs marques habituelles. Les industriels souhaitent définir des typologies de consommateurs concernant leur fidélité à une marque au cours du temps. Après une description du jeu de données concernant le suivi de consommation d'un panel de consommateurs sur deux marchés, nous avons défini une mesure de la loyauté/fidélité envers une marque sur la période considérée. Nous avons ainsi pu mettre en avant quatre groupes de consommateurs et visualiser la position des consommateurs vis-à-vis des différentes marques présentes sur le marché considéré. Dans un second temps, nous nous sommes attachés à caractériser la régularité de cette fidélité au cours du temps et avons obtenu une typologie des consommateurs. Enfin, nous avons cherché le lien éventuel entre les marques et les typologies considérées.	Stéphanie Ledauphin-Menard, Sébastien Lê	http://editions-rnti.fr/render_pdf.php?p1&p=1001798	http://editions-rnti.fr/render_pdf.php?p=1001798
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2010	Un nouvel algorithme de classification spatiale	La classification spatiale généralise la classification hiérarchique et pyramidale au cas où les objets à classer sont projetés sur une grille au lieu d'une simple droite. Nous présentons dans cet article un nouvel algorithme qui permet d'obtenir une structure classifiante spatiale en utilisant de nouvelles agrégations.	Mohamed Cherif Rahal	http://editions-rnti.fr/render_pdf.php?p1&p=1001796	http://editions-rnti.fr/render_pdf.php?p=1001796
Revue des Nouvelles Technologies de l'Information	AV	2010	Méthode visuelle et interactive de partitionnement d'un ensemble de données à l'aide de graphes de voisinage construits par des fourmis artificielles	Nous présentons dans cet article une méthode de découverte visuelle et interactive d'un partitionnement de données qui s'appuie sur la visualisation d'un graphe de voisinage obtenu à l'aide de la méthode biomimétique AntGraph. Le but est de construire ce graphe en un temps de calcul très faible grâce aux principes de l'heuristique développée, puis de laisser l'expert du domaine procéder de manière interactive à la définition d'une classification sur l'ensemble de données considéré. Nous proposons une évaluation expérimentale de notre méthode interactive sur un panel d'utilisateurs experts et non-experts et comparons les résultats obtenus en terme de qualité avec une méthode de classification interactive visuelle à base de points d'intérêts, et deux méthodes automatiques de classification que sont la Classification Ascendante Hiérarchique et AntTree. Nous montrons finalement que l'utilisation d'une technique de visualisation et d'exploration interactive par des utilisateurs experts ou non sur des graphes de données construits par AntGraph permet de découvrir une classification ayant une qualité similaire à celles obtenues avec des méthodes interactives ou automatiques	Julien Lavergne, Hanane Azzag, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001208	http://editions-rnti.fr/render_pdf.php?p=1001208
Revue des Nouvelles Technologies de l'Information	AV	2010	Prise en compte de connaissances pour la visualisation par l'intégration interactive de contraintes	La projection et la visualisation d'objets dans un espace à deux ou trois dimensions constituent une tâche courante de l'analyse de données. Cette opération induit des pertes dans le sens où certains objets peuvent se retrouver très proches alors qu'ils sont à l'origine assez éloignés. A partir de cette visualisation, il semble intéressant d'offrir à l'utilisateur la possibilité d'apporter de la connaissance sous forme de contraintes spécifiant les similarités attendues entre divers objets, lorsque ceux ci sont, dans l'espace d'observation, visuellement trop proches, ou au contraire trop éloignés. Nous proposons ici trois types de contraintes et présentons une méthode de résolution de celles-ci dérivée de l'analyse en composantes principales (ACP). Deux types d'expérimentation sont présentées, reposant respectivement sur un jeu de données synthétique et sur des jeux standards. Ces tests montrent qu'une représentation de bonne qualité peut être obtenue avec un nombre limité de contraintes ajoutées	Guillaume Cleuziou, Frédéric Moal, Lionel Martin, Matthieu Exbrayat	http://editions-rnti.fr/render_pdf.php?p1&p=1001220	http://editions-rnti.fr/render_pdf.php?p=1001220
Revue des Nouvelles Technologies de l'Information	AV	2010	Un cadre graphique pour la visualisation et la caractérisation de classes en mode non-supervisé	Dans ce papier, nous proposons un système d'extraction de connaissances à partir de données pour la visualisation et l'interprétation de profils issus d'une classification automatique en mode non supervisé. Une méthode efficace utilisée pour la classification est la carte auto-organisatrice ou Self-Organizing Map (SOM). Dans cette méthode, la détection des regroupements est en général obtenue par d'autres techniques de classification (par partitionnement ou hiérarchisation). Dans le cadre de ce travail, nous explorons une autre voie pour la segmentation de la carte par une approche basée sur la théorie des graphes (la coloration minimale). Enfin, pour caractériser les classes issues de cette segmentation, nous proposons une solution basée sur un test statistique et un arbre couvrant maximum pour la sélection de variables locales à chaque classe permettant ainsi sa caractérisation de manière automatique. Des expérimentations seront présentées sur plusieurs bases de données pour valider l'approche proposée.	Khalid Benabdeslem, Haytham Elghazel, Rakia Jaziri	http://editions-rnti.fr/render_pdf.php?p1&p=1001209	http://editions-rnti.fr/render_pdf.php?p=1001209
Revue des Nouvelles Technologies de l'Information	AV	2010	Visualisation de motifs spatiaux dans un SIG	Une des tâches classiques en fouille de données spatiales est l'extraction de co-localisations intéressantes dans des données spatiales. L'objectif est de trouver des sous-ensembles de caractéristiques booléennes apparaissant fréquemment dans des objets spatiaux voisins. Toutefois, l'interprétation des motifs extraits est difficile pour les experts du domaine. En effet, les mesures d'intérêt existantes, utilisées pour filtrer les co-localisations intéressantes, posent des problèmes d'interprétation et les résultats sont présentés aux experts sous forme textuelle. Dans ce papier, nous proposons une nouvelle mesure d'intérêt pour les co-localisations, ainsi qu'une nouvelle représentation visuelle de ces motifs. Notre mesure d'intérêt reflète mieux l'importance d'une co-localisation pour les experts, et est totalement intégrée dans le processus d'extraction. L'approche de visualisation proposée est une représentation simple, concise et intuitive des co-localisations, qui prend en considération la nature spatiale des objets sous-jacents et les pratiques des experts. Un prototype a été développé et intégré dans un SIG. Des expérimentations on été menées sur des données géologiques réelles, et les résultats validés par un expert du domaine.	Nazha Selmaoui-Folcher, Frédéric Flouvat, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001219	http://editions-rnti.fr/render_pdf.php?p=1001219
Revue des Nouvelles Technologies de l'Information	AV	2010	WinSitu, un nouveau paradigme pour l'analyse exploratoire de données basée sur des projections	Dans cet article, nous discutons des limites pratiques de l'analyse exploratoire de données basée sur les techniques de projection non linéaires continues. Nous montrons que ces méthodes de projection sont inutilisables en l'état pour permettre une inférence quelconque sur les données originelles. Nous présentons une méthode de visualisation in situ et montrons au travers de différentes expériences, qu'elle est indispensable à leur interprétation. Ce processus implémente le paradigme WinSitu d'analyse exploratoire visuelle basée sur des projections que nous introduisons pour la première fois dans ce travail. Ce changement de paradigme permet de rendre aux méthodes de projection toute leur utilité.	Michaël Aupetit	http://editions-rnti.fr/render_pdf.php?p1&p=1001221	http://editions-rnti.fr/render_pdf.php?p=1001221
Revue des Nouvelles Technologies de l'Information	CAL	2010	Assembly of components based on interface automata and UML component model	We propose an approach which combines component UML model and interface automata in order to assemble components and to verify their interoperability. We specify component based system architecture with component UML model, and component interfaces with interface automata. Interface automata is a common Input Output (I/O) automata-based formalism intended to specify the signature and the protocol level of component interfaces. We improve interface automata approach by component UML model, in order to consider system architecture, in component composition and interoperability verification methods. Therefore, we handle in interface automata, the connection between components, and the hierarchical connections between composite components and their subcomponents.	Samir Chouali, Sebti Mouelhi, Hassan Mountassir	http://editions-rnti.fr/render_pdf.php?p1&p=1000897	http://editions-rnti.fr/render_pdf.php?p=1000897
Revue des Nouvelles Technologies de l'Information	CAL	2010	Dedal : Un ADL à trois dimensions pour gérer l'évolution des architectures à base de composants	Une architecture logicielle peut être définie à différents niveaux d'abstraction, correspondants aux différentes étapes de son processus de développement : spécification, implémentation et déploiement. La cohérence entre les différentes définitions d'une architecture doit être maintenue : sa définition à un niveau d'abstraction doit être conforme à sa définition au niveau d'abstraction immédiatement supérieur. Ce principe permet de contrôler l'évolution d'une architecture, en validant les modifications réalisées à un certain niveau d'abstraction ou en motivant la création d'une nouvelle version pour propager les modifications entre niveaux d'abstraction. Malheureusement, aucun ADL ne propose un modèle de définition d'architectures séparant clairement les niveaux d'abstraction couvrant le cycle de vie d'une architecture. Cet article présente Dedal, un ADL permettant une définition séparée de la spécification, de la configuration et de l'assemblage d'une architecture afin de prévenir l'érosion ou la dérive qui surviennent lors des évolutions entre les différents niveaux de définitions des architectures.	Huaxi (Yulin) Zhang, Christelle Urtado, Sylvain Vauttier	http://editions-rnti.fr/render_pdf.php?p1&p=1000890	http://editions-rnti.fr/render_pdf.php?p=1000890
Revue des Nouvelles Technologies de l'Information	CAL	2010	Graph grammar-based transformation for context-aware architectures supporting group communication	Handling context-aware dynamically adaptable architectures contributes to the design of self-configuring software systems. This kind of problem for communicating systems is even more challenging since adaptation should address simultaneously the different levels. This is necessary for handling both changes in the low level constraints and evolutions in the high level requirements. In this paper, we address this problem by providing a model-based, rule-oriented approach that supports the adaptation process based on a run-time transformation of the system architecture. Such architecture may represent the different possible service compositions and the associated architectural configurations. We consider the multi-level models of the communicating system architecture and the intra-level architecture transformations as the elementary adaptation actions. We handle consistently the related inter-level adaptation actions by considering additional architectural relationships viewing the lower level architecture as a refinement of the upper level.We provide the algorithms characterizing the multi-level architecture-based adaptation process. We then develop a rule-oriented implementation using graph grammar and handling architectural transformations as graph transformation rules. We consider Emergency Response and Crisis Management Systems (ERCMS) as a case study from the more general group communication systems to which our results apply.	Ismael Bouassida Rodriguez, Christophe Chassot, Mohamed Jmaiel	http://editions-rnti.fr/render_pdf.php?p1&p=1000892	http://editions-rnti.fr/render_pdf.php?p=1000892
Revue des Nouvelles Technologies de l'Information	CAL	2010	MODA : une architecture multimédia dirigée par les ontologies pour des systèmes multimédia en réseau	Les environnements multimédia actuels étant très hétérogènes, l'interopérabilité des systèmes multimédia mis en réseau et le déploiement automatique de tels réseaux multimédia sont très difficiles. En effet, la diversité des langages, des protocoles et des plateformes génèrent des problèmes d'incompatibilité très importants. De plus, l'instanciation et la configuration des systèmes multimédia guidées par les préférences et/ou les exigences des utilisateurs de façon dynamique n'est guère possible dans ce contexte. Cette étude introduit un cadre de travail appelé MODA fondé sur des ontologies pour le déploiement et la configuration automatiques de systèmes multimédia en réseau. S'appuyant sur des standards bien connus, MODA intègre les concepts du multimédia, des systèmes distribués et des plateformes logicielles. Nous présentons une étude de cas montrant comment un système multimédia en réseau peut être déployé dynamiquement afin d'illustrer les avantages inhérents à MODA.	Myriam Lamolle, Jorge Gomez, Ernesto Exposito	http://editions-rnti.fr/render_pdf.php?p1&p=1000901	http://editions-rnti.fr/render_pdf.php?p=1000901
Revue des Nouvelles Technologies de l'Information	CAL	2010	Modélisation et alignement sémantique des intentions des clients avec les offres des fournisseurs	La mise en place de contrats de qualités de service entre les clients et les fournisseurs reste une tâche assez complexe. En effet, ces deux parties n'ont pas le même degré de connaissances et peuvent ne pas partager le même langage. Dans le but de palier à ce problème, nous avons commencé par définir des modèles sémantiques basés sur les ontologies pour exprimer les intentions des clients et les offres des fournisseurs. Ensuite, nous avons élaboré une approche sémantique qui se base sur des techniques d'inférence pour (1) détecter les correspondances linguistiques et sémantiques entre ces modèles, (2) raffiner les correspondances générées et (3) évaluer ces correspondances pour vérifier si le fournisseur pourrait répondre positivement aux attentes du client. Pour valider notre approche, nous avons implémenté un prototype qui permet de faciliter et d'automatiser la tâche de traitement des intentions des clients pour les offres des fournisseurs. Ce prototype peut être facilement étendu pour aider les clients à choisir les fournisseurs les plus proches de leurs intentions et leurs exigences.	Kaouthar Fakhfakh, Tarak Chaari, Said Tazi, Mohamed Jmaiel, Ikbel Guidara	http://editions-rnti.fr/render_pdf.php?p1&p=1000900	http://editions-rnti.fr/render_pdf.php?p=1000900
Revue des Nouvelles Technologies de l'Information	CAL	2010	Preuve de cohérence de composants Kmelia à l'aide de la méthode B	Pour répondre aux objectifs de qualité dans la construction de systèmes à base de composants logiciels et améliorer la confiance dans les composants et leur assemblage, il est nécessaire de disposer d'un support de correction. Kmelia est un modèle à composants multi-services dans lequel les composants sont abstraits et formels de façon à pouvoir y exprimer des propriétés et les vérifier. Dans cet article nous étudions l'automatisation de la vérification de cohérence des composants Kmelia et de leur assemblage en nous servant de la méthode B. Pour ce faire, des machines B sont extraites des composants Kmelia et vérifiées en utilisant l'Atelier B, établissant de fait les propriétés au niveau de Kmelia. La démarche est illustrée par un exemple de gestion de stock.	Mohamed Messabihi, Pascal André, Christian Attiogbé	http://editions-rnti.fr/render_pdf.php?p1&p=1000902	http://editions-rnti.fr/render_pdf.php?p=1000902
Revue des Nouvelles Technologies de l'Information	CAL	2010	Towards Architecture-based Autonomic Software Performance Engineering	Autonomic systems can be self-adaptive and have the potential to achieve high performance through run-time configuration changes. This paper describes an architecture-centric self-adaptive approach and presents a simple application in a distributed system where it can be advantageous to switch architectures based on the workload being presented to the system. The self-adaptive framework is built on top of a generative system which comprises three software architectural alternatives, namely Single Thread (ST), Half-Sync/Half-Async (HS/HA) and Leaders-Followers (LFs). A software performance analysis tool called the Layered Queuing Network Solver (LQNS) is integrated into the framework to support the architecture selection process. A comparison of the performance of the three different software architecture alternatives is also presented. The results from this analysis are used to support the construction of a performance knowledge base and analysis policies for the self-adaptive system.	Xu Zhang, Chung-Horng Lung, Greg Franks	http://editions-rnti.fr/render_pdf.php?p1&p=1000909	http://editions-rnti.fr/render_pdf.php?p=1000909
Revue des Nouvelles Technologies de l'Information	CAL	2010	Un modèle de composant pour la reconfiguration dynamique de réseaux de capteurs sans fil	Dans ce papier, nous proposons une approche permettant la reconfiguration dynamique des réseaux de capteurs sans fil (WSNs). Cette proposition repose sur une architecture logicielle bâtie autour d'un nouveau système d'exploitation et d'un modèle de composant basé sur Fractal. Ce système d'exploitation, baptisé Valentine, permet de conserver à l'exécution la construction par composant, en opposition à une construction monolithique caractéristique des principales solutions existantes dans les WSNs. De plus, étant dédié aux WSNs, il est tout naturellement orienté événement. Il a été généré à partir du canevas logiciel Think. Le modèle de composant permet de réutiliser la flexibilité du modèle abstrait Fractal et d'implémenter le mécanisme de reconfiguration dynamique.	Natacha Hoang, Nicolas Belloir, Xavier Detant, Congduc Pham	http://editions-rnti.fr/render_pdf.php?p1&p=1000894	http://editions-rnti.fr/render_pdf.php?p=1000894
Revue des Nouvelles Technologies de l'Information	CAL	2010	Une approche hybride pour la spécification de système reconfigurable	La mobilité d'action au sein d'un logiciel est une voie d'évolution pour obtenir un logiciel plus réactif en particulier à son contexte d'exécution. Les travaux de recherche présentés dans ce document explorent cette notion suivant trois aspects. Une première facette est la spécification de la mobilité au travers de langages formels tel que le pi calcul d'ordre supérieur. Les spécifications obtenues représentent des supports d'analyse essentiels. Elles offrent des possibilités de génération d'informations importantes telles que des tests ou du code exécutable. Ainsi, la réalisation ou la mise en oeuvre de la mobilité est la deuxième facette de ce travail, où il est davantage question de construction à partir de spécifications formelles. Les implémentations fournies ont alors un objectif essentiel de validation de propriétés. Enfin le dernier aspect abordé porte sur l'architecture des applications à base d'agents mobiles. La définition d'une structure logicielle commune exprime la volonté de réutiliser l'expérience acquise dans des domaines d'intérêt différents.	Mâamoun Bernichi, Fabrice Mourlin	http://editions-rnti.fr/render_pdf.php?p1&p=1000903	http://editions-rnti.fr/render_pdf.php?p=1000903
Revue des Nouvelles Technologies de l'Information	CAL	2010	Vers un Modèle de Déploiement à base de Bigraphes	Le déploiement constitue une phase importante dans le cycle de vie d'un logiciel, souvent construite de façon ad-hoc. Les préoccupations des architectes aujourd'hui sont communes, et s'articulent autour de la définition d'un processus de déploiement générique permettant d'assembler et de distribuer correctement des applications logicielles quelle que soit leur technologie d'implémentation. Cet article propose un cadre formel, à base des bigraphes, permettant la modélisation de l'opération de déploiement d'une application dans un environnement cible. D'une part, un méta modèle à base des systèmes réactifs bigraphiques (BRS) est utilisé pour définir formellement un style architectural regroupant une famille d'architectures ayant un ensemble de propriétés communes. D'autre part, la description formelle de la plateforme d'exécution chargée d'accueillir les composants de l'application après son déploiement est réalisée aussi au moyen d'un BRS particulier. Enfin, entre ces deux éléments se situe le processus de déploiement, défini formellement par une opération de composition des deux bigraphes précédents.	Faiza Belala, Chafia Bouanaka, Malika Benammar	http://editions-rnti.fr/render_pdf.php?p1&p=1000906	http://editions-rnti.fr/render_pdf.php?p=1000906
Revue des Nouvelles Technologies de l'Information	CAL	2010	Vers une architecture d'adaptation automatique des applications reparties basées composants	Les systèmes informatiques d'aujourd'hui sont de plus en plus pervasifs, composés de composants hétérogènes fournissant des fonctionnalités avec des interactions complexes. Les recherches existantes sur le développement à base de composants ont surtout porté sur la structure des composants, les interfaces et les fonctionnalités de ces derniers. Le domaine de l'architecture logicielle traite, entre autres, de l'importance significative des interactions des composants, y compris la notion de connecteurs logiciels. Si les travaux sur l'assemblage des composants ne manquent pas, peu d'approches considèrent l'hétérogénéité des interactions en matière de types et de formats des données manipulés permettant ainsi d'assurer la compatibilité technique et sémantique des échanges. Dans ce papier, nous proposons une architecture basée sur les données de type multimédia pour l'adaptation des composants hétérogènes. Nous proposons dans un premier temps un typage des interactions des composants afin de pouvoir présenter les déférents formats de média (image, texte, son, vidéo). Nous développons ensuite un service d'adaptation permettant de détecter et de résoudre le problème de l'hétérogénéité entre les composants incompatibles. Nous proposons de voir l'adaptation comme une propriété non fonctionnelle assurée par un connecteur appelé « connecteur d'adaptation ». Mots clés : Architecture Logicielle, Connecteur, Service, Multimédia, Hétérogénéité.	Makhlouf Derdour, Philippe Roose, Marc Dalmau, Nacéra Ghoualmi Zine, Adel Alti	http://editions-rnti.fr/render_pdf.php?p1&p=1000888	http://editions-rnti.fr/render_pdf.php?p=1000888
Revue des Nouvelles Technologies de l'Information	CAL	2010	Vers une meilleure compréhension de la Composition de Services par Méta Modélisation d'un Service Composite	La composition de services est un des enjeux principaux des Architectures Orientées Services (AOS). Elle a pour vocation la maximisation des réutilisations en permettant les combinaisons de ressources existantes. Ces ressources, encapsulées sous la notion de service, collaborent afin de réaliser une tâche complexe. Quantité de travaux se focalisent sur la composition de services et la résolution de ses nombreux problèmes. Cependant, la multitude des approches et leur caractère souvent spécialisé ne permettent pas d'avoir une vision globale de la composition de services qui soit indépendante de toutes technologies ou tous domaines d'application. Notre article s'inscrit dans cette logique d'explicitation de la composition de services. Il propose un métamodèle de service composite qui réifie d'un seul tenant l'ensemble des caractéristiques d'une composition de services. Il définit leurs interdépendances et assure la capacité de réutilisation de cette composition. De plus, nous définissons un mécanisme d'auto composition qui permet des modifications dynamiques de l'architecture du composite et des logiques de compositions associées.	Mourad Oussalah, Anthony Hock-koon	http://editions-rnti.fr/render_pdf.php?p1&p=1000896	http://editions-rnti.fr/render_pdf.php?p=1000896
Revue des Nouvelles Technologies de l'Information	EDA	2010	Analyse flexible dans les entrepôts de données : quand les contextes s'en mêlent	En autorisant l'observation des données à plusieurs niveaux de précision,les hiérarchies occupent une place importante dans les analyses d'entrepôtsde données. Malheureusement, les modèles d'entrepôts existants ne considèrentqu'un sous-ensemble restreint des types possibles de hiérarchie. Par exemple, iln'est pas possible de modéliser le fait que le caractère "faible", "normal" ou"élevé" de la tension artérielle d'un patient (qui constitue une hiérarchisation dela mesure) dépend de son âge (élément lié à la dimension). Ces hiérarchies, ditescontextuelles, ont récemment été introduites dans des travaux précédents. Danscet article, nous proposons la première approche pour les modéliser. Une baseexperte représentant la connaissance du domaine est créée. Ensuite, un algorithmede réécriture de requêtes est proposé pour permettre une analyse flexible,efficace et adéquate d'un entrepôt possédant de telles hiérarchies. Par exemple,il est désormais possible de répondre à la requête "Quels patients ont eu unetension faible au cours de la nuit?" en prenant en compte de manière adéquateles contextes associés au caractère "faible" de la mesure tension.	Yoann Pitarch, Cécile Favre, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001528	http://editions-rnti.fr/render_pdf.php?p=1001528
Revue des Nouvelles Technologies de l'Information	EDA	2010	F-CheX : Une approche de fouille dans les documents XML	Nous présentons dans cet article une approche de fouille dans les documentsXML qui prend en compte la structure et le contenu. Notre approcheconsiste à effectuer un clustering sur les documents XML. Ces derniers sontreprésentés par des ensembles de chemins conservant la structure arborescentedes éléments. Les ensembles de chemins sont mappés dans une matrice sur laquelleune méthode de clustering est appliquée. L'approche proposée utilise unthésaurus créé au préalable pour gérer l'aspect sémantique des mots. Une évaluationde notre approche est effectuée à travers une étude expérimentale surdeux collections de documents XML.	Amina Madani, Omar Boussaid, Hafida Abed	http://editions-rnti.fr/render_pdf.php?p1&p=1001530	http://editions-rnti.fr/render_pdf.php?p=1001530
Revue des Nouvelles Technologies de l'Information	EDA	2010	Index de Jointure Binaires: Stratégies de Sélection & Étude de Performances	La conception physique des entrepôts de données relationnels est baséeessentiellement sur la sélection d'un ensemble d'index afin de réduire le coûtd'exécution des requêtes OLAP complexes. Ces entrepôts sont généralementmodélisés par un schéma en étoile caractérisé par une table de faits volumineuseet un ensemble de tables de dimension liées à la table des faits par leurs clésétrangères. Les requêtes définies sur ce schéma (appelées requêtes de jointureen étoile) comportent plusieurs jointures entre la tables des faits et les tables dedimension ce qui rend leur coût d'exécution considérable. Les index de jointurebinaires sont très adaptés pour réduire le coût d'exécution de ces jointures. Ilssont défini sur la table de faits en utilisant un ou plusieurs attributs de dimension.Sélectionner une configuration d'index pour réduire le coût d'exécutiond'un ensemble de requêtes est reconnu comme un problème NP-Complet. Dansce papier, nous présentons d'abord le problème de sélection des index de jointurebinaires et les principaux travaux effectués dans ce domaine. Nous présentonspar la suite notre approche de sélection et les algorithmes que nous proposons.Nous effectuons des expériences pour comparer les différentes stratégies de sélection.Enfin, nous effectuons une validation réelle des différents algorithmessous Oracle en utilisant les données issues du banc d'essai APB1.	Boukhalfa Kamel, Benameur Ziani, Ladjel Bellatreche	http://editions-rnti.fr/render_pdf.php?p1&p=1001527	http://editions-rnti.fr/render_pdf.php?p=1001527
Revue des Nouvelles Technologies de l'Information	EDA	2010	Intégration des Tableaux Multidimensionnels en Pig pour l'Entreposage de Données sur les Nuages	Les entrepôts de données et les systèmes OLAP correspondent à destechnologies d'aide à la décision. Ils permettent d'analyser à la volée de gros volumesde données représentés en fonction d'un modèlemultidimensionnel. L'informatiquedans les nuages, sous l'impulsion des grandes compagnies telles queGoogle, Microsoft ou encore Amazon, a récemment suscité une attention particulière.Considérer l'interrogation OLAP et les entrepôts de données au seinde telles infrastructures devient alors un enjeu majeur. Les problèmes devantêtre considérés sont ceux classiques des systèmes largement distribués (interrogationde gros volumes de données, hétérogénéité sémantique et structurelleou encore variabilité), mais d'un nouveau point de vue devant considérer lesspécificités de ces architectures (facturation à l'utilisation, élasticité et facilitéd'utilisation). Dans ce papier nous abordons dans un premier temps les règlesde facturation à l'utilisation pour le stockage des entrepôts de données. Nousproposons d'utiliser des techniques de stockage pour nuages à base de tableauxmultidimensionnels. De premières expérimentations montrent l'intérêt de notreproposition. Ensuite, nous listons des perspectives de recherche.	Sandro Bimonte, Laurent d'Orazio	http://editions-rnti.fr/render_pdf.php?p1&p=1001507	http://editions-rnti.fr/render_pdf.php?p=1001507
Revue des Nouvelles Technologies de l'Information	EDA	2010	Modèles d'arbre pour XOLAP	Avec l'avènement de XML comme standard de représentation de donnéesdécisionnelles, les entrepôts de données XML trouvent leur place dans ledéveloppement de solutions décisionnelles. Dans ce contexte, il devient nécessairede permettre des analyses OLAP sur des cubes de données XML. Afin decontribuer à ces recherches, de définir un cadre formel et de permettre l'optimisationindispensable des requêtes décisionnelles exprimées en XQuery, noustravaillons à définir une algèbre XML-OLAP (ou XOLAP). En premier lieu,nous avons exprimé avec l'algèbre XML TAX (Tree Algebra for XML) les opérateursOLAP usuels. Il s'agit maintenant de prendre en compte les structurescomplexes permises par XML. Comme premier pas, nous proposons dans cetarticle un opérateur rollup basé sur un modèle d'arbre qui prend en compte desdonnées XML multidimensionnelles organisées en hiérarchies complexes.Mots clés : XML, Entrepôts de données, XOLAP, Modèles d'arbre, Hiérarchiescomplexes, Algèbre, Opérateur rollup	Jérôme Darmont, Marouane Hachicha	http://editions-rnti.fr/render_pdf.php?p1&p=1001512	http://editions-rnti.fr/render_pdf.php?p=1001512
Revue des Nouvelles Technologies de l'Information	EDA	2010	Motifs Séquentiels pour la Sélection des Webviews à Matérialiser	Dans cet article nous proposons une approche pour la recherched'indicateurs renforçant le choix des webviews à matérialiser. Un webviewmatérialisé est une instance statique, d'une page web dynamique, stockée auniveau du serveur web à un point de temps. Il sert à réduire le coût degénération répétitive des données sources des requêtes. Notre contributionconsiste à analyser l'historique du site web, moyennant la technique de webusage mining, pour calculer un poids de matérialisation pour chaque webview.Ce poids servira par la suite pour l'algorithme de sélection des webviews àmatérialiser afin de décider si un webview est recommandé pour lamatérialisation ou non. Plus précisément, il sert pour estimer le profit (entermes de temps de réponse) pouvant être produit par la matérialisation d'unwebview. Nos résultats d'expérimentation ont montré que notre approchepermet de réduire le risque de matérialisation des webviews, dont le profitn'est certain au moment de leur sélection, de plus de 20%.	Ali Ben Ammar, Mouna Badis, Abdelaziz Abdellatif	http://editions-rnti.fr/render_pdf.php?p1&p=1001516	http://editions-rnti.fr/render_pdf.php?p=1001516
Revue des Nouvelles Technologies de l'Information	EDA	2010	Opérateurs OLAP pour des cubes d'objets complexes: construction, visualisation et analyse	La modélisation multidimensionnelle est aujourd'hui reconnuecomme reflétant le mieux la vision des décideurs sur les données à analyser.Cependant, les modèles multidimensionnels classiques ont été pensés pourtraiter des données numériques ou symboliques mais échouent dès lors qu'ils'agit de données complexes. Les opérateurs d'analyse en ligne (OLAP) classiquessont alors à redéfinir dans le cadre de données complexes, voire d'autressont à créer. Dans ce papier, nous proposons deux familles d'opérateurs OLAPpour manipuler un modèle multidimensionnel d'objets complexes que nousavons proposé. La première famille d'opérateurs permet la construction denouveaux cubes complexes à partir du schéma multidimensionnel de l'entrepôtou à partir de cubes existants. La deuxième famille d'opérateurs permet de visualiseret d'analyser les données des cubes complexes.	Doulkifli Boukraâ, Omar Boussaid, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1001509	http://editions-rnti.fr/render_pdf.php?p=1001509
Revue des Nouvelles Technologies de l'Information	EDA	2010	Organisation de log de requêtes OLAP sous forme de site web	Vu comme une simple collection de requêtes, un log de requêtes d'unserveur OLAP est une structure peu exploitable. Dans cet article, nous proposonsd'organiser un log de requêtes d'un serveur OLAP sous la forme d'un site web.Cela a plusieurs avantages, comme la compréhension rapide de ce qui a été fait lorsdes sessions d'analyse précédentes ou comme l'aide aux futures sessions d'analyse.La technique utilisée a fait ses preuves pour la réorganisation de sites web.Le log est transformé en un graphe pondéré de requêtes, les poids reflétant unesimilarité entre requêtes. Cette similarité est calculée en tenant compte à la foisdes faits interrogés et de la présence ou non des deux requêtes au sein de la mêmesession. Ces poids sont ensuite ajustés par une heuristique utilisant des fourmisartificielles pour trouver un équilibre entre ces deux facteurs. Enfin le log est présentésous la forme d'un site web dont la structure est l'arbre couvrant minimal dece graphe. Cette technique est implémentée en java pour le traitement de logs derequêtes MDX. Des expériences ont été conduites sur des logs synthétiques pourtester la qualité des solutions obtenues selon des critères utilisés dans le domainedu web et des interfaces homme-machine	Elsa Negre, Patrick Marcel, Sonia Colas	http://editions-rnti.fr/render_pdf.php?p1&p=1001511	http://editions-rnti.fr/render_pdf.php?p=1001511
Revue des Nouvelles Technologies de l'Information	EDA	2010	Personalization of OLAP queries		Matteo Golfarelli	http://editions-rnti.fr/render_pdf.php?p1&p=1001504	http://editions-rnti.fr/render_pdf.php?p=1001504
Revue des Nouvelles Technologies de l'Information	EDA	2010	Personnalisation du contenu des bases de données multidimensionnelles	Les systèmes OLAP se basent généralement sur des Bases de DonnéesMultidimensionnelles (BDM) qui représentent des extractions del'entrepôt, dédiées à des groupes de décideurs. Les utilisateurs d'un mêmegroupe ont souvent différentes perceptions du contenu d'une BDM. Nous proposonsun cadre de personnalisation pour les systèmes de gestion des BDMsbasé sur des profils utilisateurs. Ces profils sont constitués de préférences contextuellesqui permettent d'adapter le contenu de la BDM à la perception dechaque utilisateur, formant un contenu personnalisé. Durant l'exécution d'unerequête, le système reformule la requête en tenant compte des préférences del'utilisateur afin de simuler son exécution sur un contenu individuel.	Houssem Jerbi, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001506	http://editions-rnti.fr/render_pdf.php?p=1001506
Revue des Nouvelles Technologies de l'Information	EDA	2010	Sécurisation des entrepôts de données contre les inférences en utilisant les réseaux Bayésiens	Les entrepôts de données permettent aux analyste-décideurs d'établirdes prévisions et de prendre des décisions stratégiques. La sécurisation de cesentrepôts est, par conséquent, importante afin de protéger les informationssensibles. Par ailleurs, cette sécurisation ne doit pas constituer une entrave àl'exploitation efficace et rapide de l'entrepôt, ni être trop souple induisantl'inférence des données interdites (i.e., données personnelles, confidentielles).Dans cet article, nous examinons la sécurisation des entrepôts de données àtravers une approche basée sur les réseaux Bayésiens. Elle comporte deuxavantages : d'une part, elle ne nécessite pas un traitement supplémentaire aprèschaque phase d'alimentation de l'entrepôt et, d'autre part, elle n'entraine pasl'altération des données originales.	Hanene Ben-Abdallah, Jamel Feki, Nouria Harbi, Salah Triki	http://editions-rnti.fr/render_pdf.php?p1&p=1001508	http://editions-rnti.fr/render_pdf.php?p=1001508
Revue des Nouvelles Technologies de l'Information	EDA	2010	Toward Active XML DataWarehousing	Warehousing data is not a trivial task, particularly when dealing withhuge amounts of distributed and heterogeneous data. Moreover, traditional decisionsupport systems do not feature intelligent capabilities for integrating suchcomplex data. Therefore, we propose an approach for intelligent decision supportbased on active XML warehousing. We exploit XML as a pivot languagein order to unify, model and store complex data. Furthermore, Web servicestackle the distribution and interoperability problems of data sources. They areemployed for complex data integration and react with active rules for realizingintelligent ETL. In this paper, we focus on the integration phase and propose anarchitecture for integrating complex data into a repository of Active XML documents,based onWeb services and event-driven rules. We have finally developeda software prototype to validate this approach.	Rashed Salem, Jérôme Darmont, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001510	http://editions-rnti.fr/render_pdf.php?p=1001510
Revue des Nouvelles Technologies de l'Information	EDA	2010	Un formalisme pour l'intégration de données hétérogènes	Dans ce papier nous proposons, un formalisme d'intégration de données hétérogènes. Nous définissons, d'une manière générale, une source de données comme un ensemble de composants muni des relations et fonctions qui relient ces composants, et un environnement d'intégration comme un ensemble de sources associé à un ensemble de "liens d'intégration" entre ces dernières. L'approche générale d'intégration que nous proposons s'inscrit dans le cadre de la construction d'entrepôts de données hétérogènes basés sur des sources de catégories différentes, structurées, semi-structurées et non structurées telles que relationnelles, objet-relationnelles et XML. Le processus d'intégration est composé de trois étapes : le filtrage de l'ensemble des composants de l'entrepôt, la génération de son schéma global et la construction des vues qui le composent.	Sana Hamdoun, Faouzi Boufarès	http://editions-rnti.fr/render_pdf.php?p1&p=1001513	http://editions-rnti.fr/render_pdf.php?p=1001513
Revue des Nouvelles Technologies de l'Information	EDA	2010	Un Modèle Multidimensionnel pour l'Analyse en Ligne des Champs Continus	L'intégration des données spatiales dans les modèles multidimensionnelsconduit au concept d'OLAP Spatial (SOLAP). Les modèles SOLAPexistants exploitent les données spatiales vectorielles. Peu de travaux intègrentles données spatiales continues (champs continus) en dimension et en mesure.Dans ce papier, nous proposons un modèle multidimensionnel qui utilise leschamps continus comme mesures et dimensions indépendamment de leur implantationdans les SGBDs Spatiaux.	Sandro Bimonte, Myoung-Ah Kang	http://editions-rnti.fr/render_pdf.php?p1&p=1001517	http://editions-rnti.fr/render_pdf.php?p=1001517
Revue des Nouvelles Technologies de l'Information	EDA	2010	Une Sélection Multiple des Structures d'Optimisation Dirigée par la Méthode de Classification K-means	Le volume d'information contenu dans un entrepôt de données s'accroîtsans cesse, augmentant de ce fait le temps d'exécution des requêtes décisionnelles.Pour y remédier, l'administrateur doit, durant la phase de conceptionphysique de l'entrepôt, effectuer une sélection de structures d'optimisation(index, vues matérialisées ou fragmentation), puis assurer leur gestion et maintenance.Pour optimiser un nombre maximum de requêtes, il est indispensabled'opter pour une sélection multiple de structures ayant une forte similarité. Dansla littérature, deux principales similarités entre les structures d'optimisation ontété identifiées : une entre les vues et les index et l'autre entre la fragmentationhorizontale dérivée et les index de jointure binaire. Dans ce travail, nousproposons une approche de sélection multiple des index de jointure binaire etde fragmentation. Vue la complexité de la sélection multiple, nous proposonsune nouvelle approche permettant d'abord de partager l'ensemble des attributsextraits des requêtes entre les deux structures, ensuite sélectionner chaque structureavec un algorithme. Pour réaliser ce partage, nous proposons d'utiliser laméthode K-means. Une étude expérimentale et des tests comparatifs sur un entrepôtde données réel sous le SGBD Oracle 11g sont proposés illustrant l'intérêtde notre approche.	Rym Bouchakri, Ladjel Bellatreche, Boukhalfa Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1001529	http://editions-rnti.fr/render_pdf.php?p=1001529
Revue des Nouvelles Technologies de l'Information	EDA	2010	Une structure logicielle distribuée pour la découverte et la visualisation des règles d'association spatiales	Dans ce travail nous présentons l'architecture et les fonctionnalités d'un système de data mining spatial exploitant les transactions spatiales. Le système proposé met en oeuvre des techniques de découverte des règles d'association spatiales basées sur la fermeture de Galois parallèle, distribuée et déployée sur un bus CORBA. La spécification de l'architecture du système ainsi que les diverses composantes logicielles seront développées. Le module d'extraction des transactions spatiales constitue la pièce maîtresse du système. Il fait recours à des éléments structurants de voisinage de type grille, buffer et polygone de voronoÏ. La visualisation des règles d'association spatiales issues du processus de découverte de connaissances spatiales est fondée sur la technologie web mapping, qui permet de déployer la visualisation pour plusieurs acteurs décideurs. La navigation est rendue flexible grâce à un mécanisme de structuration basée sur la décomposition d'une similarité floue. Le prototype actuel est validé auprès d'un opérateur de télécommunication pour une application en géomarketing.	Azedine Boulmakoul	http://editions-rnti.fr/render_pdf.php?p1&p=1001505	http://editions-rnti.fr/render_pdf.php?p=1001505
Revue des Nouvelles Technologies de l'Information	EDA	2010	Validation Formelle de Schéma Multidimensionnel vis à vis de	Comme tout autre modèle, les modèles multidimensionnels doiventadhérer à un ensemble de contraintes de bonne formation structurelle etsémantique afin de garantir l'exactitude des analyses. Une partie de cescontraintes régissent la relation d'un modèle multidimensionnel avec sa sourcede données. Dans ce papier, nous proposons un cadre formel pour laspécification, la vérification syntaxique et la validation d'un schémamultidimensionnel par rapport à celui de sa source de données.	Ali Salem, Faiza Ghozzi, Hanene Ben-Abdallah	http://editions-rnti.fr/render_pdf.php?p1&p=1001515	http://editions-rnti.fr/render_pdf.php?p=1001515
Revue des Nouvelles Technologies de l'Information	EDA	2010	Vers la définition des contraintes d'intégrité d'entrepôts de données spatiales avec OCL	Les Entrepôts de Données Spatiales (EDS) et les systèmes SOLAPreprésentent une solution efficace pour l'analyse spatiale de phénomènes géographiques.Cependant, la qualité de cette analyse dépend fortement de la qualitédes données stockées. Dans ce contexte, quelques travaux se sont intéressésà la définition et la spécification des contraintes d'intégrité spécifiques auxEDS. Dans cet article, motivé par le manque d'implémentation basée sur uneapproche MDA (Model Driven Approach), nous proposons deux classificationsde contraintes d'EDS ainsi que leurs spécifications et implémentations enutilisant le standard OMG OCL (Object Constraint Language).	François Pinet, Hadj Maboubi, Sandro Bimonte, Kamal Boulil	http://editions-rnti.fr/render_pdf.php?p1&p=1001514	http://editions-rnti.fr/render_pdf.php?p=1001514
Revue des Nouvelles Technologies de l'Information	EGC	2010	AbsTop-K &#945;: un algorithme d'extraction de paires abstraites hautement corrélées pour mieux recommander dans la "longue traine	De nombreux systèmes de recommandation se focalisent sur les articles(que nous appellerons "items") les plus "populaires" et ignorent souventla "longue traîne" des produits qui le sont moins. Nous proposons l'algorithmeAbsTop-k&#945; qui améliore les recommandations en se basant sur la combinaison(pondérée par &#945;) de paires hautement corrélées entre des abstractions d'items etentre des paires d'items concrets classiquement recherchées.	Minh Thu Tran Nguyen, François Sempé, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001439	http://editions-rnti.fr/render_pdf.php?p=1001439
Revue des Nouvelles Technologies de l'Information	EGC	2010	Action Rules and Meta-actions		Zbigniew W. Ras	http://editions-rnti.fr/render_pdf.php?p1&p=1001261	http://editions-rnti.fr/render_pdf.php?p=1001261
Revue des Nouvelles Technologies de l'Information	EGC	2010	Affichage de publicités sur des portails web	Nous nous intéressons au problème de l'affichage de publicités surle web. De plus en plus d'annonceurs souhaitent maintenant payer uniquementlorsque quelqu'un clique sur leurs publicités. Dans ce modèle, l'opérateur duportail a intérêt à identifier les publicités les plus cliquées, selon ses catégoriesde visiteurs. Comme les probabilités de clic sont inconnues a priori, il s'agit d'undilemme exploration/exploitation. Ce problème a souvent été traité en ne tenantpas compte de contraintes provenant du monde réel : les campagnes de publicitésont une durée de vie et possèdent un nombre de clics à assurer et ne pas dépasser.Pour cela, nous introduisons une approche hybride (MAB+LP) entre la programmationlinéaire et les bandits. Nos algorithmes sont testés sur des modèles créésavec un important acteur du web commercial. Ces expériences montrent que cesapproches atteignent une performance très proche de l'optimum et mettent enévidence des aspects clés du problème.	Victor Gabillon, Jérémie Mary, Philippe Preux	http://editions-rnti.fr/render_pdf.php?p1&p=1001268	http://editions-rnti.fr/render_pdf.php?p=1001268
Revue des Nouvelles Technologies de l'Information	EGC	2010	Agrégation de systèmes de fermetures: cas des hiérarchies, topologies et géométries convexes		Florent Domenach	http://editions-rnti.fr/render_pdf.php?p1&p=1001469	http://editions-rnti.fr/render_pdf.php?p=1001469
Revue des Nouvelles Technologies de l'Information	EGC	2010	Aide à la décision pour la maintenance ferroviaire préventive	La maintenance de trains est un problème particulièrement délicat liéà de nombreux enjeux à la fois financiers, sécuritaires et énergétiques. Nous nousintéressons à la mise en place d'une maintenance préventive basée sur la détectionet la correction de tout comportement anormal susceptible de provoquer unproblème majeur dans un futur proche. Nous proposons ainsi un outil d'aide à ladécision afin de (i) dégager des connaissances utiles sur l'historique des trains,et (ii) détecter et étudier les anomalies comportementales, dans le but de prendredes décisions optimales en termes de maintenance ferroviaire	Julien Rabatel, Sandra Bringay, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001319	http://editions-rnti.fr/render_pdf.php?p=1001319
Revue des Nouvelles Technologies de l'Information	EGC	2010	Allier CSPs et motifs locaux pour la découverte de motifs sous contraintes n-aires	Dans cet article, nous étudions la relation entre la découverte de motifssous contraintes et les CSPs (Constraint Satisfaction Problems) afin de définirdes contraintes de plus haut niveau qui sont précieuses pour mener à bien destâches de fouille de données. Pour cela, nous proposons une approche de modélisationet d'extraction de motifs sous contraintes n-aires exploitant les motifslocaux. L'utilisateur définit un ensemble de contraintes n-aires et un solveur deCSP génère l'ensemble des solutions. Notre approche profite des progrès récentssur l'extraction de motifs locaux et permet de modéliser de manière concise etélégante tout ensemble de contraintes combinant plusieurs motifs locaux, permettantainsi la découverte de motifs répondant mieux aux buts finaux de l'utilisateur.Les expériences menées montrent la faisabilité de notre approche.	Mehdi Khiari, Patrice Boizumault, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1001292	http://editions-rnti.fr/render_pdf.php?p=1001292
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse de documents pédagogiques en vue de leur annotation	L'utilisation des documents pédagogiques, disponibles sur le web,devient de plus en plus large tant pour l'enseignant qui a besoin de préparerson support de cours que pour l'étudiant qui désire, par exemple, s'autoformer.La description d'un document pédagogique, en l'alimentant par desmétadonnées, s'avère une solution qui confère une valeur ajoutée au documentafin d'expliciter des informations placées dans ce document. Dans cetteoptique, nous proposons une méthode d'annotation de documentspédagogiques selon différents points de vue, qui est basée sur l'analysesémantique des éléments discursifs du texte	Boutheina Smine, Rim Faiz, Jean-Pierre Desclés	http://editions-rnti.fr/render_pdf.php?p1&p=1001502	http://editions-rnti.fr/render_pdf.php?p=1001502
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse de séquences d'événements avec TraMineR		Nicolas S. Müller, Matthias Studer, Alexis Gabadinho, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1001397	http://editions-rnti.fr/render_pdf.php?p=1001397
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse en ligne d'objets complexes avec l'analyse factorielle	Les entrepôts de données et l'analyse en ligne OLAP (On-line AnalysisProcessing) présentent des solutions reconnues et efficaces pour le processusd'aide à la décision. Notamment l'analyse en ligne, grâce aux opérateurs OLAP,permet de naviguer et de visualiser des données représentées dans un cube multidimensionnel.Mais lorsque les données ou les objets à analyser sont complexes,il est nécessaire de redéfinir et d'enrichir ces opérateurs OLAP. Dans cet article,nous proposons de combiner l'analyse OLAP et la fouille de données (data mining)afin de créer un nouvel opérateur de visualisation d'objets complexes. Cetopérateur utilise l'analyse factorielle des correspondances.	Loïc Mabit, Sabine Loudcher, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001321	http://editions-rnti.fr/render_pdf.php?p=1001321
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse globale du flux optique pour la détection d'évènements dans une scène de foule	Les systèmes de vidéo-surveillance sont de plus en plus autonomesdans la détection des événements anormaux. Cet article présente une méthode dedétection des flux majeurs et des évènements qui surviennent dans une scène defoule. Ces détections sont effectuées en utilisant un modèle directionnel construità partir d'un mélange de lois de von Mises appliqué à l'orientation des vecteursde mouvement. Les flux majeurs sont alors calculés en récupérant les orientationsles plus importantes des mélanges. Divers évènements se produisant dansune foule sont aussi détectés en utilisant en plus du modèle d'orientation, unmodèle probabiliste de magnitude des vecteurs de mouvement. Les résultats del'expérimentation sur un échantillon de vidéos d'événements sont présentés.	Yassine Benabbas, Nacim Ihaddadene, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001315	http://editions-rnti.fr/render_pdf.php?p=1001315
Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse incrémentale des usages pour le routage des requêtes dans les systèmes pairs à pairs		Taoufik Yeferny, Khedija Arour	http://editions-rnti.fr/render_pdf.php?p1&p=1001427	http://editions-rnti.fr/render_pdf.php?p=1001427
Revue des Nouvelles Technologies de l'Information	EGC	2010	Applying Markov Logic to Document Annotation and Citation Deduplication	Structured learning approaches are able to take into account the relationalstructure of data, thus promising an enhancement over non-relationalapproaches. In this paper we explore two document-related tasks in relationaldomains setting, the annotation of semi-structured documents and the citationdeduplication. For both tasks, we report results of comparing relational learningapproach namely Markov logic, to non-relational one namely Support VectorMachines (SVM). We discover that increased complexity due to the relationalsetting is difficult to manage in large scale cases, where non-relational modelsmight perform better. Moreover, our experiments show that in Markov logic,the contribution of its probabilistic component decreases in large scale domains,and it tends to act like First-order logic (FOL).	Jean Baptiste Faddoul, Boris Chidlovskii	http://editions-rnti.fr/render_pdf.php?p1&p=1001329	http://editions-rnti.fr/render_pdf.php?p=1001329
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apport de la technique de fouille de données spatiales dans la prédiction des risques engendrés par les changements climatiques		Hana Alouaoui, Sami Yassine Turki, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001451	http://editions-rnti.fr/render_pdf.php?p=1001451
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage de patrons lexico-syntaxiques à partir de textes	Ce papier présente une approche d'apprentissage de patrons lexicosyntaxiquesà partir de textes annotés. Les patrons lexico-syntaxiques sont utiliséspour identifier des relations lexicales dans les corpus textuels. Leur constructionmanuelle est une tâche fastidieuse et des solutions permettant l'apprentissagesont souhaitables. Nous proposons une approche d'apprentissage qui reposesur l'utilisation des chemins de dépendance pour représenter les patrons et l'implémentationd'un algorithme de classification. L'approche a été appliquée dansle domaine biomédical pour identifier des patrons lexico-syntaxiques exprimantdes relations fonctionnelles.	Valentina Dragos, Marie-Christine Jaulent	http://editions-rnti.fr/render_pdf.php?p1&p=1001371	http://editions-rnti.fr/render_pdf.php?p=1001371
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage de spécifications de CSP		Matthieu Lopez, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001465	http://editions-rnti.fr/render_pdf.php?p=1001465
Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage supervisé adaptatif de Concepts Formels à partir des données nominales		Mondher Maddouri, Nida Meddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001372	http://editions-rnti.fr/render_pdf.php?p=1001372
Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche biomimétique coopérative pour la visualisation de grands graphes multidimensionels	Face à la quantité sans cesse grandissante de données stockées, les algorithmes de fouille etde visualisation de données doivent pouvoir être capable de traiter de grandes quantités de données.Une des solutions est d'effectuer un prétraitement des données permettant la réductionde la dimension des données sans perte significative d'informations. L'idée est donc de réduirel'ensemble de descripteurs avant de faire appel à la méthode de visualisation sous forme d'ungraphe.	Lydia Boudjeloud, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001412	http://editions-rnti.fr/render_pdf.php?p=1001412
Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche complexe de l'analyse de documents anciens	Cet article présente une méthode complexe pour la caractérisation etl'indexation d'images graphiques de documents anciens. A partir d'un bref étatde l'art, une méthode pour décrire ces images en tenant compte de leur complexitéest proposée. Trois étapes principales de ce traitement sont détailléesdont une méthode novatrice d'analyse, de segmentation et de description destraits. Les résultats sont issus de travaux en cours et sont encourageants	Mickaël Coustaty, Giap NGuyen, Vincent Courboulay, Jean-Marc Ogier	http://editions-rnti.fr/render_pdf.php?p1&p=1001368	http://editions-rnti.fr/render_pdf.php?p=1001368
Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche Sémantique pour la Préservation de la Vie Privée dans les Médias Sociaux		Hakim Hacid, Johann Stan, Johann Daigremont	http://editions-rnti.fr/render_pdf.php?p1&p=1001446	http://editions-rnti.fr/render_pdf.php?p=1001446
Revue des Nouvelles Technologies de l'Information	EGC	2010	Bien cube, les données textuelles peuvent s'agréger !	La masse des données aujourd'hui disponibles engendre des besoinscroissants de méthodes décisionnelles adaptées aux données traitées. Ainsi, récemmentde nouvelles approches fondées sur des cubes de textes sont apparuespour pouvoir analyser et extraire de la connaissance à partir de documents. L'originalitéde ces cubes est d'étendre les approches traditionnelles des entrepôts etdes technologies OLAP à des contenus textuels. Dans cet article, nous nous intéressonsà deux nouvelles fonctions d'agrégation. La première propose une nouvellemesure de TF-IDF adaptative permettant de tenir compte des hiérarchiesassociées aux dimensions. La seconde est une agrégation dynamique permettantde faire émerger des groupements correspondant à une situation réelle. Lesexpériences menées sur des données issues du serveur HAL d'une universitéconfirment l'intérêt de nos propositions.	Sandra Bringay, Anne Laurent, Pascal Poncelet, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001367	http://editions-rnti.fr/render_pdf.php?p=1001367
Revue des Nouvelles Technologies de l'Information	EGC	2010	Caractériser la terminologie des usagers de santé dans le domaine du cancer du sein	Internet est devenu une source importante d'informations médicalespour les patients et leurs proches : recherche d'informations sur leurs maladieset les dernières recherches cliniques, ainsi que pour y constituer des communautés"numériques" de dialogue et de partage. Cependant, accès à Internet nesignifie pas nécessairement accès à l'information. Le manque de familiarité avecle langage médical constitue un problème majeur pour les usagers de santé dansl'accès à l'information et son interprétation. Ce papier s'inscrit dans la problématiqued'étude et de caractérisation de la terminologie des usagers de santépour pouvoir proposer des services adaptés à leur langage et à leur niveau deconnaissances. Le travail réalisé est une ontologie dans le domaine du cancerdu sein orientée vers les usagers de santé. Cette ontologie est construite à partird'un ensemble de corpus de textes représentant deux catégories : les médiateurset les usagers de santé. Les éléments de cette ontologie ont été analysés en utilisantdes méthodes quantitatives et qualitatives sur plusieurs niveaux : termes,concepts et relations.	Radja Messai, Michel Simonet, Nathalie Bricon-Souf, Mireille Mousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001326	http://editions-rnti.fr/render_pdf.php?p=1001326
Revue des Nouvelles Technologies de l'Information	EGC	2010	CARTOCEL : Un outil de cartographie des connaissances guidée par la machine cellulaire CASI	Nous présentons, dans ce papier, l'outil CARTOCEL (CARTOgraphiesCELlulaires) permettant une visualisation automatique et dynamique desdomaines de connaissances. Le fonctionnement de CARTOCEL est basé surune approche originale de modélisation booléenne de la cartographie des domainesde connaissances métiers/stratégiques inspirée du principe de la machinecellulaire CASI (Cellular Automata for Symbolic Induction). Le but,après une modélisation booléenne de la cartographie des domaines de connaissances,est double : d'une part affiner la cartographie par une fouille de donnéeorchestrée par CASI, et d'autre part réduire la complexité de stockage, ainsique le temps de calcul	Menaouer Brahami, Baghdad Atmani, Mostéfa Mokaddem	http://editions-rnti.fr/render_pdf.php?p1&p=1001375	http://editions-rnti.fr/render_pdf.php?p=1001375
Revue des Nouvelles Technologies de l'Information	EGC	2010	ChorML : Résumés visuels de bases des données géographiques		Ibtissem Cherni, Karla Lopez, Robert Laurini, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001443	http://editions-rnti.fr/render_pdf.php?p=1001443
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classer, discriminer et visualiser des séquences d'événements	Cet article 1 présente un ensemble d'outils destiné à analyser des séquencesd'événements en sciences sociales et à visualiser les résultats obtenus.Nous commençons par formaliser la notion de séquence d'événements avant dedéfinir une mesure de dissimilarité entre ces séquences afin de construire destypologies et de tester les liens entre ces séquences et d'autres variables d'intérêts.Initialement définie par Moen (2000), cette mesure se base sur la notion dedistance d'édition entre séquences et permet d'identifier les différences d'ordonnancementet de temporalité des événements. Nous proposons une extension decelle-ci afin de pouvoir prendre en compte la simultanéité des événements ainsiqu'une méthode de normalisation qui garantit le respect de l'inégalité triangulaire.Dans un deuxième temps, nous présentons un ensemble d'outils destinésà interpréter les résultats. Nous proposons ainsi deux méthodes de visualisationd'un ensemble de séquences et nous introduisons la notion de sous-séquencediscriminante qui permet d'identifier les différences d'ordonnancement des événementsles plus significatives entre groupes. L'ensemble des outils présentés estdisponible au sein de la librairie R TraMineR.	Matthias Studer, Nicolas S. Müller, Gilbert Ritschard, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1001264	http://editions-rnti.fr/render_pdf.php?p=1001264
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification de documents : calcul d'une distance structurelle	La classification des documents numériques garantit un accès rapideet ciblé à l'information. Si nous considérons qu'un document est représenté parsa ou ses structures, définir des classes de documents revient à définir desclasses de structures. Une classe structurelle représente donc des structures« proches ». Ainsi, associer la structure d'un document à sa classe structurellerevient à calculer une distance dite « structurelle ». Elle tiendra compte à lafois de l'organisation des éléments (position des noeuds, chemin), du coûtd'adaptation des représentants des classes ainsi que de la représentativité dessous-graphes. Sur un corpus de documents représentant des notices de livresissus de la bibliothèque de l'université, nous discuterons de la construction decette distance, de l'intérêt de chacun des trois paramètres utilisés	Karim Djemal, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau	http://editions-rnti.fr/render_pdf.php?p1&p=1001369	http://editions-rnti.fr/render_pdf.php?p=1001369
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification et Selection de caracteristique basees sur les concepts semantiques pour la recherche d'information multimedia	Le besoin récent de nombreuses applications multimédia basées sur le contenu a engendré une demande croissante de technologies dans le domaine de la recherche d'information multimédia. Basée sur l'état de l'art des techniques existantes, nous proposons dans cet article une approche de recherche d'information multimédia qui prend en compte les informations de scène et exploite un modèle de sélection de caractéristiques. Les principaux avantages de notre modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de sélection de caractéristiques; (iii) un index multidimensionnel. Notre framework propose un bon compromis entre précision et rapidité de la recherche	Thierry Urruty, Ismail Elsayad, Adel Lablack, Yue Feng, Jose M. Joemon	http://editions-rnti.fr/render_pdf.php?p1&p=1001271	http://editions-rnti.fr/render_pdf.php?p=1001271
Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification supervisée pour de grands nombres de classes à prédire : une approche par co-partitionnement des variables explicatives et à expliquer	Dans la phase de préparation des données du data mining, les méthodesde discrétisation et de groupement de valeurs supervisé possèdent denombreuses applications : interprétation, estimation de densité conditionnelle,sélection de type filtre des variables, recodage des variables en amont des classifieurs.Ces méthodes supposent habituellement un faible nombre de valeur àexpliquer (classes), typiquement moins d'une dizaine, et trouvent leur limitequand leur nombre augmente. Dans cet article, nous introduisons une extensiondes méthodes de discrétisation et groupement de valeurs, consistant à partitionnerd'une part la variable explicative, d'autre part la variable à expliquer.Le meilleur co-partitionnement est recherché au moyen d'une approche Bayesiennede la sélection de modèle. Nous présentons ensuite comment utiliser cetteméthode de prétraitement en préparation pour le classifieur Bayesien naïf. Desexpérimentations intensives démontrent l'apport de la méthode dans le cas decentaines de classes.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001352	http://editions-rnti.fr/render_pdf.php?p=1001352
Revue des Nouvelles Technologies de l'Information	EGC	2010	CND-Cube : Nouvelle représentation concise sans perte d'information d'un cube de données	Le calcul des cubes de données est excessivement coûteux aussi bienen temps d'exécution qu'en mémoire et son stockage sur disque peut s'avérerprohibitif. Plusieurs efforts ont été consacrés à ce problème à travers les cubesfermés, où les cellules préservant la sémantique d'agrégation sont réduites à unecellule, sans perte d'information. Dans cet article, nous introduisons le conceptdu cube de données non-dérivable fermé, nommé CND-Cube, qui généralisela notion des modèles non-dérivables fermés fréquents bidimensionnels à uncontexte multidimensionnel. Nous proposons un nouvel algorithme pour extrairele CND-Cube à partir des bases de données multidimensionnelles en se basantsur trois contraintes anti-monotones, à savoir "être fréquent", "être non dérivable"et "être un générateur minimal". Les expériences montrent que notreproposition fournit la représentation la plus concise d'un cube de données et elleest ainsi la plus efficace pour réduire l'espace de stockage	Hanen Brahmi, Tarek Hamrouni, Riadh Ben Messaoud, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001303	http://editions-rnti.fr/render_pdf.php?p=1001303
Revue des Nouvelles Technologies de l'Information	EGC	2010	Codage et classification non supervisée d'un corpus maya : extraire des contextes pour situer l'inconnu par rapport au connu	L'écriture logosyllabique des anciens Mayas comprend plus de 500signes et est en bonne partie déchiffrée, avec des degrés de certitude divers.Nous avons appliqué au codex de Dresde, l'un des trois seuls manuscrits quinous soient parvenus, codé sous LATEXavec le systèmemayaTEX, notre méthodede représentation graduée, par apprentissage non supervisé hybride entre clusteringet analyse factorielle oblique, sous la métrique de Hellinger, afin d'obtenirune image nuancée des thèmes traités : les individus statistiques sont les 212segments de folio du codex, et leurs attributs sont les 1687 bigrammes de signesextraits. Pour comparaison, nous avons introduit dans cette approche endogèneun élément exogène, la décomposition en éléments des signes composites, pourpréciser plus finement les contenus. La rétro-visualisation dans le texte originaldes résultats et expressions dégagées éclaire la signification de certains glyphespeu compris, en les situant dans des contextes clairement interprétables.	Mohamed Hallab, Bruno Delprat, Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1001366	http://editions-rnti.fr/render_pdf.php?p=1001366
Revue des Nouvelles Technologies de l'Information	EGC	2010	Combiner approche logique et numérique pour la réconciliation de données et l'alignement d'ontologies		Marie-Christine Rousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001262	http://editions-rnti.fr/render_pdf.php?p=1001262
Revue des Nouvelles Technologies de l'Information	EGC	2010	CombinerWeb 2.0 et Web Sémantique pour réduire les disparités d'expertise au sein de blogs d'entreprise	Avec l'avènement d'applications sociales en entreprise (blogs, wikis,etc.), il est fréquent que des individus aux niveaux d'expertise relativement distantsse réunissent au sein de communautés en ligne. Ces disparités d'expertisese traduisent entre autres par des comportements différents dans la manière detagguer les contenus créés, notamment en ce qui concerne les termes utilisés,rendant ainsi complexe la découverte d'informations pourtant publiées. Dans cetarticle, nous mettons en avant la possibilité offerte par les technologies du WebSémantique, combinées avec les paradigmes du Web Social, de résoudre cetteproblématique. Nous proposons ainsi une chaine de traitement combinant ontologies,wikis sémantiques et indexation de contenus permettant la production degraphes sémantiques interconnectés et facilitant de cette manière la découvertede contenus créés au sein de tels systèmes	Alexandre Passant, Philippe Laublet	http://editions-rnti.fr/render_pdf.php?p1&p=1001270	http://editions-rnti.fr/render_pdf.php?p=1001270
Revue des Nouvelles Technologies de l'Information	EGC	2010	Comparaison de critères de pureté pour l'intégration de connaissances en clustering semi-supervisé	L'utilisation de connaissances pour améliorer les processus de fouillede données a mobilisé un important effort de recherche ces dernières années. Ilest cependant souvent difficile de formaliser ce type de connaissances, commecelles-ci sont souvent dépendantes du domaine. Dans cet article, nous nous intéressonsà l'intégration de connaissances sous la forme d'objets étiquetés dansles algorithmes de clustering. Plusieurs critères permettant d'évaluer la puretédes clusters sont présentés et leur comportement est comparé sur des jeux dedonnées artificiels. Les avantages et les inconvénients de chaque critère sontanalysés pour aider l'utilisateur à faire un choix.	Germain Forestier, Cédric Wemmert, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1001278	http://editions-rnti.fr/render_pdf.php?p=1001278
Revue des Nouvelles Technologies de l'Information	EGC	2010	Comparaisons structurelles de grandes bases de données par apprentissage non-supervisé	Dans le domaine de la fouille de données, mesurer les similitudesentre différents sous-ensembles est une question importante qui a été peu étudiéejusqu'à présent. Dans cet article, nous proposons une nouvelle méthodebasée sur l'apprentissage non-supervisé. Les différents sous-ensembles à comparersont caractérisés au moyen d'un modèle à base de prototypes. Ensuite, lesdifférences entre les modèles sont détectées en utilisant une mesure de similarité	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001276	http://editions-rnti.fr/render_pdf.php?p=1001276
Revue des Nouvelles Technologies de l'Information	EGC	2010	Composition de ServicesWeb Basée sur les Réseau Sociaux	Nous proposons dans cet article une première approche qui consisteà exploiter les réseaux sociaux afin de faciliter la composition de services parles utilisateurs finaux. Nous introduisons un Framework, nommé Social Composer(SoCo), qui implémente cette approche. SoCo fournit à l'utilisateur desrecommandations dynamiques de services basées entre autre sur le réseau socialde l'utilisateur qui est construit implicitement à partir des interactions entre lesutilisateurs, les services, les différentes compositions opérées par les membresdu réseau social, ainsi que le réseau social global.	Abderrahmane Maaradji, Hakim Hacid, Johann Daigremont, Noël Crespi	http://editions-rnti.fr/render_pdf.php?p1&p=1001290	http://editions-rnti.fr/render_pdf.php?p=1001290
Revue des Nouvelles Technologies de l'Information	EGC	2010	Construction de noyaux pour l'apprentissage supervisé à partir d'arbres aléatoires	Nous montrons qu'un ensemble d'arbres de décision avec une composantealéatoire permet de construire un noyau efficace destiné à l'apprentissagesupervisé. Nous étudions théoriquement les propriétés d'un tel noyau et montronsque sous des conditions très souvent rencontrées en pratique, il existe uneséparabilité linéaire entre exemples de classes distinctes dans l'espace induit parcelui-ci. Parallèlement, nous observons également que le classique vote à la majoritéd'un ensemble d'arbres est un hyperplan (sans garantie d'optimalité) dansl'espace induit par le noyau. Enfin, comme le montrent nos expérimentations,l'utilisation conjointe d'un ensemble d'arbres et d'un séparateur à vaste marge(SVM) aboutit à des résultats extrêmement encourageants.	Vincent Pisetta, Pierre-Emmanuel Jouve, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001351	http://editions-rnti.fr/render_pdf.php?p=1001351
Revue des Nouvelles Technologies de l'Information	EGC	2010	Cubes Fermés / Quotients Émergents	Le concept de Cube Émergent a été introduit afin de comparer deuxdata cubes. Dans cet article, nous introduisons deux nouvelles représentationsréduites du Cube Émergent sans perte des mesures : le Cube Fermé Émergent etle Cube Quotient Émergent. La première représentation est basée sur le conceptde fermeture cubique. C'est la plus petite représentation possible du cube dedonnées émergent. À partir du Cube Fermé Émergent et donc en stockant le minimumd'informations, il est possible de répondre efficacement aux requêtes quipeuvent être exécutées sur le Cube Émergent lui-même. La seconde représentations'appuie sur la structure du Cube Quotient qui a été proposé pour résumer uncube de données. Le Cube Quotient est revisité afin de le doter d'une sémantiquebasée sur la fermeture cubique et donc adapté au contexte du Cube Émergent. LeCube Quotient Émergent résultant est moins réduit que le Cube Fermé Émergentmais il préserve la propriété de " spécialisation/généralisation " du data cube quipermet la navigation au sein du Cube Émergent. Nous établissons également lelien entre les deux représentations introduites et celle basée sur les bordures classiquesen fouille de données. Des expérimentations effectuées sur divers jeux dedonnées visent à comparer la taille des différentes représentations.	Sébastien Nedjar, Alain Casali, Rosine Cicchetti, Lotfi Lakhal	http://editions-rnti.fr/render_pdf.php?p1&p=1001306	http://editions-rnti.fr/render_pdf.php?p=1001306
Revue des Nouvelles Technologies de l'Information	EGC	2010	DaFOE: une plateforme pour construire des ontologies à partir de textes et de thésaurus		Jean Charlet, Sylvie Szulman, Nathalie Aussenac-Gilles, Adeline Nazarenko, Nathalie Hernandez, Nadia Nadah, Éric Sardet, Jean Delahousse, Valery Teguiak, Audrey Baneyx	http://editions-rnti.fr/render_pdf.php?p1&p=1001386	http://editions-rnti.fr/render_pdf.php?p=1001386
Revue des Nouvelles Technologies de l'Information	EGC	2010	Découverte des dépendances fonctionnelles conditionnelles fréquentes	Les Dépendances Fonctionnelles Conditionnelles (DFC) ont été introduitesen 2007 pour le nettoyage des données. Elles peuvent être considéréescomme une unification de Dépendances Fonctionnelles (DF) classiques et deRègles d'Association (RA) puisqu'elles permettent de spécifier des dépendancesmixant des attributs et des couples de la forme attribut/valeur.Dans cet article, nous traitons le problème de la découverte des DFC, i.e. déterminerune couverture de l'ensemble des DFC satisfaites par une relation r. Nousmontrons comment une technique connue pour la découverte des DF (exacteset approximatives) peut être étendue aux DFC. Cette technique a été implémentéeet des expériences ont été menées pour montrer la faisabilité et le passage àl'échelle de notre proposition.	Thierno Diallo, Noel Novelli	http://editions-rnti.fr/render_pdf.php?p1&p=1001311	http://editions-rnti.fr/render_pdf.php?p=1001311
Revue des Nouvelles Technologies de l'Information	EGC	2010	Découverte d'itemsets fréquents fermés sur architectures multicoeurs	Dans ce papier nous proposons PLCM, un algorithme parallèle dedécouverte d'itemsets fréquents fermés basé sur l'algorithme LCM, reconnucomme l'algorithme séquentiel le plus efficace pour cette tâche. Nous présentonsaussi une interface de parallélisme à la fois simple et puissante basée sur lanotion de Tuple Space, qui permet d'avoir une bonne répartition dynamique dutravail.Grâce à une étude expérimentale détaillée, nous montrons que PLCM est le seulalgorithme qui soit suffisamment générique pour calculer efficacement des itemsetsfréquents fermés à la fois sur des bases creuses et sur des bases denses,améliorant ainsi l'état de l'art.	Benjamin Négrevergne, Alexandre Termier, Jean-François Méhaut, Takeaki Uno	http://editions-rnti.fr/render_pdf.php?p1&p=1001339	http://editions-rnti.fr/render_pdf.php?p=1001339
Revue des Nouvelles Technologies de l'Information	EGC	2010	Density estimation on data streams : an application to Change Detection	In recent years, the amount of data to process has increased in manyapplication areas such as network monitoring, web click and sensor data analysis. Data stream mining answers to the challenge of massive data processing, this paradigm allows for treating pieces of data on the fly and overcoming data storage. The detection of changes in a data stream distribution is an important issue. This article proposes a new schema of change detection :i) the summarization of the input data stream by a set of micro-clusters;ii) the estimate of the data stream distribution exploiting micro-clusters;iii) the estimate of the divergence between the current estimated distribution and a reference distribution;iv) diagnostic step through the contribution of each predictive variable to the overall divergence between both distributions.Our schema of change detection is applied and evaluated on artificial data streams.	Marie-Luce Picard, Benoît Grossin, Alexis Bondu	http://editions-rnti.fr/render_pdf.php?p1&p=1001297	http://editions-rnti.fr/render_pdf.php?p=1001297
Revue des Nouvelles Technologies de l'Information	EGC	2010	Detecting Anomalies in Data Streams using Statecharts	The environment around us is progressively equipped withvarious sensors, producing data continuously. The applications usingthese data face many challenges, such as data stream integration over anattribute (such as time) and knowledge extraction from raw data. In thispaper we propose one approach to face those two challenges. First, datastreams integration is performed using statecharts which represents aresume of data produced by the corresponding data producer. Second,we detect anomalous events over temporal relations among statecharts.We describe our approach in a demonstration scenario, that is using avisual tool called Patternator	Vasile-Marian Scuturici, Dan-Mircea Suciu, Romain Vuillemot, Aris Ouksel, Lionel Brunie	http://editions-rnti.fr/render_pdf.php?p1&p=1001394	http://editions-rnti.fr/render_pdf.php?p=1001394
Revue des Nouvelles Technologies de l'Information	EGC	2010	Détection des mouvements anormaux dans des vidéos		Md. Haidar Sharif, Husam Alustwani, Ioan Marius Bilasco, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001448	http://editions-rnti.fr/render_pdf.php?p=1001448
Revue des Nouvelles Technologies de l'Information	EGC	2010	Développement de méthodes de classification basées sur l'Analyse de Concepts Formels sous la plateforme WEKA		Besma Khalfi, Rahma Cherif, Nida Meddouri, Mondher Maddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001373	http://editions-rnti.fr/render_pdf.php?p=1001373
Revue des Nouvelles Technologies de l'Information	EGC	2010	Differentes variantes GMM-SMOs pour l'identification du locuteur	Dans cet article, nous présentons différentes variantes GMM-SMOs pour l'identification du locuteur en mode indépendant du texte. Pour mettre en oeuvre les différents systèmes, nous avons opté une représentation multi-gaussienne de l'espace des caractéristiques basées sur l'algorithme Expectation Maximisation (EM). Ces nouvelles représentations constituent les vecteurs d'entrés pour entraîner les supports vecteurs machines (SVMs) par l'algorithme de type Optimisation par Minimisation Séquentielle (SMO).	Siwar Zribi Boujelbene, Dorra Ben Ayed Mezghanni, Noureddine Ellouze	http://editions-rnti.fr/render_pdf.php?p1&p=1001421	http://editions-rnti.fr/render_pdf.php?p=1001421
Revue des Nouvelles Technologies de l'Information	EGC	2010	Etude comparative des langages de requêtes sémantiques pour l'extraction des liens complexes dans une base de connaissances		Thabet Slimani, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1001445	http://editions-rnti.fr/render_pdf.php?p=1001445
Revue des Nouvelles Technologies de l'Information	EGC	2010	Etude de stabilité de méthodes de sélection de motifs à partir des séquences protéiques		Rabie Saidi, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001454	http://editions-rnti.fr/render_pdf.php?p=1001454
Revue des Nouvelles Technologies de l'Information	EGC	2010	Expansion de requêtes SQL par une ontologie de domaine	Cet article traite un problème dans le domaine de la gestion des basesde données classiques. Il s'agit d'exploiter une ontologie de domaine pour aiderl'utilisateur d'une base de données relationnelle dans sa recherche et de luipermettre une interrogation transparente de la base de données. Pour cela, nousproposons une approche d'expansion automatique de requêtes SQL lorsquecelles-ci n'ont pas de réponses. Notre approche est décrite par un algorithmedéfini de manière générique afin d'être utilisé pour une base de données quelconque.	Ines Fayech, Habib Ounalli	http://editions-rnti.fr/render_pdf.php?p1&p=1001345	http://editions-rnti.fr/render_pdf.php?p=1001345
Revue des Nouvelles Technologies de l'Information	EGC	2010	Explication de décisions de réconciliation : approche fondée sur les réseaux de Petri colorés	L'objectif des systèmes d'intégration de données est de faciliter l'exploitationet l'interprétation d'informations hétérogènes provenant de différentessources. Lorsque l'on doit intégrer de grands volumes de données, le recours àun expert n'est pas envisageable mais l'exploitation de processus d'intégrationautomatiques peut introduire des approximations ou des erreurs. Nous nous focalisonssur les résultats fournis par les méthodes de réconciliation de données.Ces dernières comparent les données entre elles et détectent celles qui réfèrent àla même entité du monde réel. Pour renforcer la confiance des utilisateurs dansles résultats retournés par ces méthodes, nous proposons dans cet article une approched'explication graphique fondée sur les réseaux de Petri colorés qui estparticulièrement adaptée aux approches de réconciliation globales, numériqueset guidées par une ontologie.	Souhir Gahbiche, Nathalie Pernelle, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001313	http://editions-rnti.fr/render_pdf.php?p=1001313
Revue des Nouvelles Technologies de l'Information	EGC	2010	Exploration de dépendances fonctionnelles et de règles d'association avec OLAP		Pierre Allard, Sébastien Ferré	http://editions-rnti.fr/render_pdf.php?p1&p=1001408	http://editions-rnti.fr/render_pdf.php?p=1001408
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de la région d'intérêt d'une personne sur un obstacle		Adel Lablack, Thierry Urruty, Yassine Benabbas, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001430	http://editions-rnti.fr/render_pdf.php?p=1001430
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de motifs graduels clos	La découverte automatique de règles et motifs graduels ("plus l'âged'une personne est élevé, plus son salaire est élevé") trouve de très nombreusesapplications sur des bases de données réelles (e.g. biologie, flots de données decapteurs). Si des algorithmes de plus en plus efficaces sont proposés dans desarticles récents, il n'en reste pas moins que ces méthodes génèrent un nombrede motifs tellement important que les experts peinent à les exploiter. Dans cetarticle, nous proposons donc une représentation condensée des motifs graduelsen introduisant les concepts théoriques associés aux opérateurs de fermeture surde tels motifs.	Sarra Ayouni, Sadok Ben Yahia, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001293	http://editions-rnti.fr/render_pdf.php?p=1001293
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de règles d'association séquentielle à l'aide de modèles semi-paramétriques à risques proportionnels	La recherche de liens entre objets fréquents a été popularisée par lesméthodes d'extraction de règles d'association. Dans le cas de séquences d'événements,les méthodes de fouille permettent d'extraire des sous-séquences quipeuvent ensuite être exprimées sous la forme de règles d'association séquentielleentre événements. Cette utilisation de la fouille de séquences pour la recherchede liens entre des événements pose deux problèmes. Premièrement, lecritère principal utilisé pour sélectionner les sous-séquences d'événements estla fréquence, or les occurrences de certains événements peuvent être fortementliées entre elles même lorsqu'elles sont peu fréquentes. Deuxièmement, les mesuresactuelles utilisées pour caractériser les règles d'association ne tiennent pascompte du caractère temporel des données, comme l'importance du timing desévénements ou le problème des données censurées. Dans cet article, nous proposonsune méthode pour rechercher des liens significatifs entre des événementsà l'aide de modèles de durée. Les règles d'association sont construites à partirdes motifs séquentiels observés dans un ensemble de séquences. L'influence surle risque que l'événement « conclusion » se produise après le ou les événements« prémisse » est estimée à l'aide d'un modèle semi-paramétrique à risques proportionnels.Outre la présentation de la méthode, l'article propose une comparaisonavec d'autres mesures d'association	Nicolas S. Müller, Matthias Studer, Gilbert Ritschard, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1001263	http://editions-rnti.fr/render_pdf.php?p=1001263
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction des séquences fermées fréquentes à partir de corpus parallèles : Application à la traduction automatique	Dans cet article, nous abordons la problématique d'extraction de séquencesfréquentes à partir de corpus de textes parallèles en prenant en comptel'ordre d'apparition des mots dans une phrase. Notre finalité est d'exploiter cesséquences dans la traduction automatique (TA). Nous introduisons ainsi la notionde règles associatives inter-langues (RAIL) et nous définissons notre modèlede traduction à base de ces associations. Nous décrivons également les différentesexpérimentations conduites sur le corpus EUROPARL afin de construire àpartir des RAIL une table de traduction bilingue qui est intégrée par la suite dansun processus complet de TA.	Cherif Chiraz Latiri, Cyrine Nasri, Kamel Smaïli, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001266	http://editions-rnti.fr/render_pdf.php?p=1001266
Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction d'itemsets distinctifs dans les flux de données	L'extraction d'itemsets distinctifs est un sujet de recherche récent quiconnait plusieurs algorithmes pour les données statiques (Knobbe et Ho, 2006;Heikinheimo et al., 2007). Ces solutions ne sont toutefois pas conçues pour lecas des flux de données, pour lesquels les temps de réponse doivent être aussifaibles que possible. Nous considérons le problème de l'extraction d'itemsetsdistinctifs dans les flux, qui peut avoir de nombreuses applications dans la sélectionde variables, la classification ou encore la recherche d'information. Nousproposons l'heuristique IDkF (Itemsets Distinctifs dans les Flux) et des résultatsd'expérimentations en comparaison d'une technique de la littérature.	Chongsheng Zhang, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1001291	http://editions-rnti.fr/render_pdf.php?p=1001291
Revue des Nouvelles Technologies de l'Information	EGC	2010	Fouille visuelle de données en 3D et réalité virtuelle : état de l'art	La fouille visuelle de données (ou Visual Data Mining, VDM) a pourobjectif de faciliter l'interprétation des résultats issus d'une fouille de données,grâce à l'usage de représentations graphiques. Au cours de la dernière décennie,un grand nombre de techniques de visualisation d'information ont été mises aupoint, permettant la visualisation de données multidimensionnelles dans des environnementsvirtuels. Lors des travaux antérieurs, les chercheurs ont proposédes taxonomies pour classer les techniques de VDM (Chi (2000), Herman et al.(2000)). Toutefois, ces taxonomies ne prennent en compte que partiellement lestechniques récentes relatives à l'utilisation de la 3D et de la réalité virtuelle. Lebut de cet article est de faire un état de l'art récent et spécifique à ces techniques.Celles-ci sont détaillées, classées et comparées selon différents critères : les applications,l'encodage graphique, les techniques d'interaction, les avantages etles inconvénients de chaque approche. Ces techniques sont présentées dans destableaux accompagnées d'illustrations graphiques	Zohra Ben Said, Fabrice Guillet, Paul Richard	http://editions-rnti.fr/render_pdf.php?p1&p=1001282	http://editions-rnti.fr/render_pdf.php?p=1001282
Revue des Nouvelles Technologies de l'Information	EGC	2010	Gestion sémantique des droits d'accès au contenu: l'ontologie AMO	Dans cet article nous proposons une approche de la gestion des droitsd'accès pour les systèmes de gestion de contenu qui reposent sur les modèles ettechniques du web sémantique. Nous présentons l'ontologie AMO qui consiste(1) en un ensemble de classes et propriétés permettant d'annoter les ressourcesdont il s'agit de contrôler l'accès et (2) en une base de règles d'inférence modélisantla stratégie de gestion des droits à mettre en oeuvre. Appliquées sur la based'annotations des ressources, ces règles permettent de gérer les ressources selonune stratégie donnée. Cette modélisation garantit ainsi l'adaptabilité de l'ontologieà différentes stratégies de gestion des droits d'accès. Nous illustrons l'utilisationde l'ontologie AMO sur les documents du projet ANR ISICIL produitspar le wiki sémantique SweetWiki. Nous montrons comment les documents sontannotés avec AMO, quelles règles sont mises en oeuvre et quelles requêtes permettentle contrôle de l'accès aux documents.	Michel Buffa, Catherine Faron-Zucker, Anna Kolomoyskaya	http://editions-rnti.fr/render_pdf.php?p1&p=1001340	http://editions-rnti.fr/render_pdf.php?p=1001340
Revue des Nouvelles Technologies de l'Information	EGC	2010	Identifying the Presence of Communities in Complex Networks Through Topological Decomposition and Component Densities	The exponential growth of data in various fields such as Social Networksand Internet has stimulated lots of activity in the field of network analysisand data mining. Identifying Communities remains a fundamental technique toexplore and organize these networks. Few metrics are widely used to discoverthe presence of communities in a network. We argue that these metrics do nottruly reflect the presence of communities by presenting counter examples. Thisis because these metrics concentrate on local cohesiveness among nodes wherethe goal is to judge whether two nodes belong to the same community or viseversa. Thus loosing the overall perspective of the presence of communities in theentire network. In this paper, we propose a new metric to identify the presenceof communities in real world networks. This metric is based on the topologicaldecomposition of networks taking into account two important ingredients of realworld networks, the degree distribution and the density of nodes. We show theeffectiveness of the proposed metric by testing it on various real world data sets	Faraz Zaidi, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001286	http://editions-rnti.fr/render_pdf.php?p=1001286
Revue des Nouvelles Technologies de l'Information	EGC	2010	IncFDs: un nouvel algorithme d'inférence incrémentale des dépendances fonctionnelles	L'inférence des dépendances fonctionnelles est l'une des problématiquesles plus étudiées en bases de données. Elle a fait l'objet de plusieurstravaux qui ont proposé des algorithmes afin d'inférer, efficacement, les dépendancesfonctionnelles pour les utiliser dans différents domaines : administrationde bases de données, ré-ingénierie, optimisation des requêtes,etc. Toutefois,pour les application réelles, les bases de données sont évolutives et les relationssont fréquemment augmentées ou diminuées de tuples. Par conséquent, afin des'adapter à ce cadre dynamique, une solution consiste à appliquer l'un des algorithmes,disponibles dans la littérature, pour inférer les dépendances fonctionnelles,après chaque mise à jour. Cette solution étant coûteuse, nous proposons,dans cet article, d'inférer les dépendances fonctionnelles d'une manière incrémentale.À cet effet, nous introduisons un nouvel algorithme, appelé INCFDS, etnous évaluons ses performances par rapport à l'approche classique d'inférencedes dépendances fonctionnelles à partir d'une relation dynamique.	Ghada Gasmi	http://editions-rnti.fr/render_pdf.php?p1&p=1001310	http://editions-rnti.fr/render_pdf.php?p=1001310
Revue des Nouvelles Technologies de l'Information	EGC	2010	Indexation et recherche d'images à très grande échelle avec une AFC incrémentale et parallèle sur GPU	Nous présentons un nouvel algorithme incrémental et parallèled'analyse factorielle des correspondances (AFC) pour la recherche d'images àgrande échelle en utilisant le processeur de la carte graphique (GPU). L'AFCest adaptée à la recherche d'images par le contenu en utilisant des descripteurslocaux des images (SIFT). L'AFC permet de réduire le nombre de dimensionset de découvrir des thèmes qui permettent de diminuer le nombre d'images àparcourir et donc le temps de réponse d'une requête. Pour traiter de trèsgrandes bases d'images, nous présentons une version incrémentale et parallèled'AFC, puis nous utilisons ses indicateurs pour construire des fichiers inverséspour retrouver les images contenant les mêmes thèmes que l'image requête.Cette étape est elle aussi parallélisée sur GPU pour obtenir des réponsesrapides. Les résultats numériques sur la base de données d'images Nistér-Stewénius plongée dans 1 million d'images de FlickR montrent que notrealgorithme incrémental et parallèle est très significativement plus rapide que saversion standard	Nguyen-Khang Pham, François Poulet, Annie Morin, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1001281	http://editions-rnti.fr/render_pdf.php?p=1001281
Revue des Nouvelles Technologies de l'Information	EGC	2010	Indice de complexité pour le tri et la comparaison de séquences catégorielles	Cet article1 propose un nouvel indice de la complexité de séquencescatégorielles. Bien que conçu pour des séquences représentant des trajectoiresbiographiques telles que celles rencontrées dans les sciences sociales, il s'appliqueà tous types de listes ordonnées d'états. L'indice prend en compte deuxaspects distincts, soit la complexité induite par l'ordonnancement des états successifsqui est mesurée par le nombre de transitions (changements d'état) et lacomplexité liée à la distribution des états dont rend compte l'entropie	Alexis Gabadinho, Gilbert Ritschard, Matthias Studer, Nicolas S. Müller	http://editions-rnti.fr/render_pdf.php?p1&p=1001267	http://editions-rnti.fr/render_pdf.php?p=1001267
Revue des Nouvelles Technologies de l'Information	EGC	2010	Inférence Bayesienne du Maximum d'Entropie pour le Diagnostic du Cancer		Fadi Dornaika, Fadi Chakik	http://editions-rnti.fr/render_pdf.php?p1&p=1001423	http://editions-rnti.fr/render_pdf.php?p=1001423
Revue des Nouvelles Technologies de l'Information	EGC	2010	Intégration de Connaissances a Priori dans le Principe du Maximum d'Entropie	Cet article montre que si l'on dispose d'une connaissance a priori surle problème en main, l'intégration de cette dernière dans le processus d'apprentissaged'une machine intelligente pour des tâches de classification peut améliorerla performance de cette machine. Nous étudions l'effet de l'intégration de laconnaissance a priori de convexité sur le processus d'apprentissage du principedu Maximum d'Entropie (MaxEnt) en utilisant des exemples virtuels. Nous testonsles idées proposées sur un problème benchmark bien connu dans la littératuredes machines d'apprentissage, le problème de formes d'ondes de Breiman.Nous avons abouti à un taux d'erreur de généralisation de 15.57% qui est trèsproche du taux d'erreur théorique estimé par Breiman (14%).	Fadi Chakik, Fadi Dornaika	http://editions-rnti.fr/render_pdf.php?p1&p=1001356	http://editions-rnti.fr/render_pdf.php?p=1001356
Revue des Nouvelles Technologies de l'Information	EGC	2010	Intégration interactive de contraintes pour la réduction de dimensions et la visualisation	Il existe aujourd'hui de nombreuses méthodes de réduction de dimensions,que ce soit dans un cadre supervisé ou non supervisé. L'un des intérêts deces méthodes est de pouvoir visualiser les données, avec pour objectif que lesobjets qui apparaissent "visuellement" proches soient similaires, dans un sensqui correspond aux connaissances d'un expert du domaine ou qui soit conformeaux informations de supervision. Nous nous plaçons ici dans un contexte semisuperviséoù des connaissances sont ajoutées de façon interactive : ces informationsseront apportées sous forme de contraintes exprimant les écarts entrela représentation observée et les connaissances d'un expert. Nous pourrons parexemple spécifier que deux objets proches dans l'espace d'observation sont enfait peu similaires, ou inversement. La méthode utilisée ici dérive de l'analyseen composantes principales (ACP), à laquelle nous proposons d'intégrer deuxtypes de contraintes. Nous présentons une méthode de résolution qui a été implémentéedans un logiciel offrant une représentation 3D des données et grâceauquel l'utilisateur peut ajouter des contraintes de manière interactive, puis visualiserles modifications induites par ces contraintes. Deux types d'expérimentationsont présentés, reposant respectivement sur un jeu de données synthétiqueet sur des jeux standards : ces tests montrent qu'une représentation de bonnequalité peut être obtenue avec un nombre limité de contraintes ajoutées.	Guillaume Cleuziou, Frédéric Moal, Lionel Martin, Matthieu Exbrayat	http://editions-rnti.fr/render_pdf.php?p1&p=1001320	http://editions-rnti.fr/render_pdf.php?p=1001320
Revue des Nouvelles Technologies de l'Information	EGC	2010	Interrogation des résumés de flux de données	Les systèmes de gestion de flux de données (SGFD) ont été conçusafin de traiter une masse importante de données produites en ligne de façoncontinue. Etant donné que les ressources matérielles ne permettent pas de conservertoute cette volumétrie, seule la partie récente du flux est mémorisée dans lamémoire du SGFD. Ainsi, les requêtes évaluées par ces systèmes ne peuvent porterque sur les données les plus récentes du flux. Par conséquent, les SGFD actuelsne peuvent pas traiter des requêtes qui portent sur des périodes très longues.Nous proposons dans cet article, une approche permettant d'évaluer des requêtesqui portent sur une période plus longue que la mémoire du SGFD. Ces fenêtresfont appels à des données récentes et des données historisées. Nous présentonsle niveau logique de cette approche ainsi que son implantation sous le SGFD Esper.Une technique d'échantillonnage associée à une technique de fenêtre pointde repère est appliquée pour conserver une représentation compacte des donnéesdu flux.	Nesrine Gabsi, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001300	http://editions-rnti.fr/render_pdf.php?p=1001300
Revue des Nouvelles Technologies de l'Information	EGC	2010	K-WORDS LAB : un outil d'analyse des mots clés permettant d'explorer les dynamiques d'un domaine scientifique.		Audrey Baneyx, Philippe Breucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001376	http://editions-rnti.fr/render_pdf.php?p=1001376
Revue des Nouvelles Technologies de l'Information	EGC	2010	KGRAM: une machine abstraite de graphes de connaissance	Cet article présente la machine abstraite de graphes de connaissanceKGRAM qui unifie les notions d'homomorphisme de graphe et de calcul de requêtestelles que celles du langage SPARQL sur des données RDF. KGRAMimplémente un ensemble extensible d'expressions qui définissent une famille delangages abstraits d'interrogation de graphes, GRAAL. Nous décrivons la sémantiquedynamique de GRAAL en Sémantique Naturelle et nous présentons lamachine abstraite KGRAM conçue comme l'interprète de GRAAL, qui implémenteles règles de sémantique naturelle du langage.	Olivier Corby, Catherine Faron-Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001328	http://editions-rnti.fr/render_pdf.php?p=1001328
Revue des Nouvelles Technologies de l'Information	EGC	2010	Le conflit dans la théorie des fonctions de croyance	Le conflit apparaît naturellement lorsque plusieurs sources d'informationsimparfaites sont en jeu. La théorie des fonctions de croyance offre unformalisme adapté à la fusion d'informations dans lequel la considération duconflit est centrale. Ce travail propose de revenir sur les différentes définitionsdu conflit dans cette théorie, tentant de les synthétiser et de montrer commentsupprimer ce conflit, ou bien comment en tenir compte lors de la combinaisondes informations.	Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001411	http://editions-rnti.fr/render_pdf.php?p=1001411
Revue des Nouvelles Technologies de l'Information	EGC	2010	Modèle de Langue à base de Concepts pour la Recherche d'Information	La majorité des modèles de langue appliqués à la recherched'information repose sur l'hypothèse d'indépendance des mots.Plus précisément, ces modèles sont estimés à partir des mots simplesapparaissant dans les documents sans considérer les éventuelles relationssémantiques et conceptuelles. Pour pallier ce problème, deux grandesapproches ont été explorées : la première intègre des dépendances d'ordresurfacique entre les mots, et la seconde repose sur l'utilisation des ressourcessémantiques pour capturer les dépendances entre les mots. Le modèle delangue que nous présentons dans cet article s'inscrit dans la seconde approche.Nous proposons d'intégrer les dépendances entre les mots en représentant lesdocuments et les requêtes par les concepts.	Lynda Said L'Hadj, Mohand Boughanem	http://editions-rnti.fr/render_pdf.php?p1&p=1001501	http://editions-rnti.fr/render_pdf.php?p=1001501
Revue des Nouvelles Technologies de l'Information	EGC	2010	Modélisation et interrogation de données XML multidimensionnelles	XML étant devenu omniprésent et ses techniques de stockage et d'interrogationde plus en plus efficaces, le nombre de cas d'utilisations de ces technologiesaugmente tous les jours. Un sujet prometteur est l'intégration d'XML etdes entrepôts de données, dans laquelle une base de données XML native stockeles données multidimensionnelles et exécute des requêtes OLAP écrites à l'aidedu langage d'interrogation XML XQuery. Ce papier explore les questions quipeuvent survenir lors de l'implémentation d'un tel entrepôt de données XML.	Boris Verhaegen, Esteban Zimányi, Serge Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001327	http://editions-rnti.fr/render_pdf.php?p=1001327
Revue des Nouvelles Technologies de l'Information	EGC	2010	Mysins : Make Your Semantic INformation System		Anthony Ventresque, Thomas Cerqueus, Louis-Alexandre Celton, Gaëtan Hervouet, Damien Levin, Philippe Lamarre, Sylvie Cazalens	http://editions-rnti.fr/render_pdf.php?p1&p=1001382	http://editions-rnti.fr/render_pdf.php?p=1001382
Revue des Nouvelles Technologies de l'Information	EGC	2010	Objective Novelty of Association Rules: Measuring the Confidence Boost1	On sait bien que la confiance des régles d'association n'est pas vraimentsatisfaisant comme mésure d'interêt. Nous proposons, au lieu de la substituerpar des autres mésures (soit, en l'employant de façon conjointe a desautres mésures), évaluer la nouveauté de chaque régle par comparaison de saconfiance par rapport á des régles plus fortes qu'on trouve au même ensemblede données. C'est á dire, on considère un seuil "relative" de confiance au lieu duseuil absolute habituel. Cette idée se précise avec la magnitude du "confidenceboost", mésurant l'increment rélative de confiance prés des régles plus fortes.Nous prouvons que nôtre proposte peut remplacer la "confidence width" et leblockage de régles employés a des publications précedentes.	José L Balcazar 	http://editions-rnti.fr/render_pdf.php?p1&p=1001308	http://editions-rnti.fr/render_pdf.php?p=1001308
Revue des Nouvelles Technologies de l'Information	EGC	2010	OSOM : un algorithme de construction de cartes topologiques recouvrantes	Les modèles de classification recouvrante ont montré leur capacité àgénérer une organisation plus fidèle aux données tout en conservant la simplificationattendue par une structuration en classes strictes. Par ailleurs les modèlesneuronaux non-supervisés sont plébiscités lorsqu'il s'agit de visualiser la structurede classes.Nous proposons dans cette étude d'étendre les cartes auto-organisatrices traditionnellesaux cartes auto-organisatrices recouvrantes. Nous montrons que cettenouvelle structure apporte des solutions à certaines problématiques spécifiquesen classification recouvrante (nombre de classes, complexité, cohérence des recouvrements).L'algorithme OSOM s'inspire de la version recouvrante des nuées dynamiqueset de l'approche de Kohonen pour générer de telles cartes recouvrantes. Nousdiscutons du modèle proposé d'un point de vue théorique (fonction d'énergieassociée, complexité, ...). Enfin nous présentons un cadre d'évaluation généraleque nous utilisons pour valider les résultats obtenus sur des données réelles.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001272	http://editions-rnti.fr/render_pdf.php?p=1001272
Revue des Nouvelles Technologies de l'Information	EGC	2010	Pattern Mining: The Past, Present, and Future	Pattern mining is one of the fundamental techniques in data mining. As one increases thecomplexity of the pattern types, from subsets, to subsequences, subtrees, and subgraphs, onediscovers potentially more informative patterns. In this talk I will offer a tour of the past andthe present research landscape in this area, and I'll conclude with some thoughts on directionsfor the future	Mohammed Zaki	http://editions-rnti.fr/render_pdf.php?p1&p=1001258	http://editions-rnti.fr/render_pdf.php?p=1001258
Revue des Nouvelles Technologies de l'Information	EGC	2010	PCAR : Nouvelle Approche de Génération de Règles d'Association Cycliques	Les règles d'association cycliques vise la découverte de nouvelles relationsentre des produits qui varient d'une façon régulièrement cyclique dans letemps. Dans ce cadre, nous introduisons, un nouvel algorithme nommé PCARcaractérisé par sa performance et son aspect incrémental. L'étude empirique quenous avons menée montre la robustesse et l'efficacité de notre algorithme proposévs. ceux de la littérature	Mohamed Salah Gouider, Eya Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1001415	http://editions-rnti.fr/render_pdf.php?p=1001415
Revue des Nouvelles Technologies de l'Information	EGC	2010	PGP-mc : extraction parallèle efficace de motifs graduels	Initialement utilisés pour les systèmes de commande, les règles et motifsgraduels (de la forme "plus une personne est âgée, plus son salaire est élevé")trouvent de très nombreuses applications, par exemple dans les domainesde la biologie, des données en flots (e.g. issues de réseaux de capteurs), etc. Trèsrécemment, des algorithmes ont été proposés pour extraire automatiquementde tels motifs. Cependant, même si certains d'entre eux ont permis des gainsde performance importants, les algorithmes restent coûteux et ne permettentpas de traiter efficacement les bases de données réelles souvent très volumineuses(en nombre de lignes et/ou nombre d'attributs). Nous proposons doncdans cet article une méthode originale de recherche de ces motifs utilisant lemulti-threading pour exploiter au mieux les multiples coeurs présents dans laplupart des ordinateurs et serveurs actuels. L'efficacité de cette approche est validéepar une étude expérimentale.	Anne Laurent, Benjamin Négrevergne, Nicolas Sicard, Alexandre Termier	http://editions-rnti.fr/render_pdf.php?p1&p=1001336	http://editions-rnti.fr/render_pdf.php?p=1001336
Revue des Nouvelles Technologies de l'Information	EGC	2010	Prédiction de séries temporelles et applications à l'analyse de séquences vidéo		Rémi Auguste, Ahmed El Ghini, Ioan Marius Bilasco, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001468	http://editions-rnti.fr/render_pdf.php?p=1001468
Revue des Nouvelles Technologies de l'Information	EGC	2010	PretopoLib: la librairie JAVA de la Prétopologie	PretopoLib est une librairie JAVA implémentant les concepts de laprétopologie. Son intérêt réside dans la représentation de structures de donnéespermettant la manipulation des données par des opérations ensemblistes.Celle-ci offre un cadre de développement d'algorithmes efficaces pour la fouillede données, l'apprentissage topologique et la modélisation des systèmes complexes.	Sofiane Ben Amor, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1001405	http://editions-rnti.fr/render_pdf.php?p=1001405
Revue des Nouvelles Technologies de l'Information	EGC	2010	Proposition d'opérateurs OLAP pour un modèle multidimensionnel à base d'objets complexes		Doulkifli Boukraâ, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001407	http://editions-rnti.fr/render_pdf.php?p=1001407
Revue des Nouvelles Technologies de l'Information	EGC	2010	Proposition d'une méthode de classification associative adaptative	La classification associative est une méthode de prédiction à base derègles issue de la fouille de règles d'association. Cette méthode est particulièrementintéressante car elle recherche de façon exhaustive les règles d'associationpertinentes qu'elle filtre pour ne garder que les règles d'association de classe(celles admettant pour conséquent une modalité de classe), qui sont utiliséescomme classifieur. Les connaissances produites sont ainsi directement interprétables.Des études antérieures montrent les inconvénients de cette approche,qu'il s'agisse de la génération massive de règles non utilisées ou de la mauvaiseprédiction de la classe minoritaire lorsque les classes sont déséquilibrées.Nous proposons une approche originale du type boosting de règles d'associationde classes qui utilise comme classifieur faible une base de règles significativesconstruites par un algorithme de génération d'itemsets fréquents qui se limiteà l'extraction des seules règles de classe significatives et qui prend en comptele déséquilibre des données. Des comparaisons avec d'autres méthodes de classificationassociative montrent que notre approche améliore la précision et lerappel.	Emna Bahri, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001346	http://editions-rnti.fr/render_pdf.php?p=1001346
Revue des Nouvelles Technologies de l'Information	EGC	2010	Protein Graph Repository	Protein Graph Repository (PGR) est i, outil bioinformatique sur le web permettant d'obtenir une nouvelle representation de protéines sous la forme de graphes d'acides aminés, une représentation plus simple et plus facile à étudier par les moyens informatiques et statistiques dédiés aux graphes. La génération des graphes est faite à partir d'un parseur appliqué sur des fichiers des protéines PDB extraits de la base Protein Data Bank et en precisant les parametres et la methode a utiliser. Les graphes generes sont ensuite enregistres dans un entrepot doté de moyens de recherche, de filtrage et de telechargement. PGR peut etre provisoirement consulte à l'adresse http://www.enode-edition.com/pgr/, il est spécialement dédié aux recherches intéressées à l'étude de données protéiques sous la forme de graphes et permettra donc de fournir des échantillons pour des travaux expérimentaux.	Wajdi Dhifli, Rabie Saidi	http://editions-rnti.fr/render_pdf.php?p1&p=1001403	http://editions-rnti.fr/render_pdf.php?p=1001403
Revue des Nouvelles Technologies de l'Information	EGC	2010	Recent Advances in Partitioning Clustering Algorithms for Interval-Valued Data		Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001260	http://editions-rnti.fr/render_pdf.php?p=1001260
Revue des Nouvelles Technologies de l'Information	EGC	2010	Recherche sémantique sur le Web basée sur l'ontologie Modulaire et le raisonnement à base de cas	Dans ce papier, nous présentons une approche de recherche sémantiquebasée sur les ontologies modulaires et le raisonnement à base de cas(RaPC). Un cas représente l'ensemble des requêtes similaires associées à leursrésultats pertinents. Les ontologies modulaires sont utilisées pour représenteret indexer les cas qui sont construits sur la base des requêtes antérieures et lesrésultats pertinents sélectionnés par les utilisateurs. La similarité à based'ontologies est utilisée pour retrouver les cas similaires à la requête utilisateuret pour fournir à celui-ci des propositions de reformulation de requêtes correspondantsà son besoin. La principale contribution de ce travail réside dans l'utilisationd'un mécanisme de RaPC et une représentation ontologique à deuxfins: l'amélioration de la recherche sémantique et l'enrichissement d'ontologiesà partir de cas. L'expérimentation de l'approche proposée montre que la précisionet le rappel des résultats se sont nettement améliorés.	Nesrine Ben Mustapha, Hajer Baazaoui Zghal, Marie-Aude Aufaure, Henda Ben Ghézala	http://editions-rnti.fr/render_pdf.php?p1&p=1001457	http://editions-rnti.fr/render_pdf.php?p=1001457
Revue des Nouvelles Technologies de l'Information	EGC	2010	Reconnaissance de concepts basée sur l'apprentissage		Wahiba Ben Abdessalem Karaa, Bilel Bouchamia	http://editions-rnti.fr/render_pdf.php?p1&p=1001410	http://editions-rnti.fr/render_pdf.php?p=1001410
Revue des Nouvelles Technologies de l'Information	EGC	2010	Réduction bi-directionnelle d'images - Vers une méthode d'extraction de caractéristiques multi-niveaux	Inspiré des performances du cerveau humain à identifier les élémentspar la vue, le problème de la réduction de la dimension dans le domaine de laperception visuelle consiste à extraire une quantité réduite des caractéristiquesd'un ensemble d'images afin de les identifier.Ce papier présente une approche innovante bi-directionnelle d'extraction de caractéristiquesd'images fondée sur l'utilisation partielle d'une méthode spatiotemporelle.Les expériences numériques appliquées sur 70000 images représentantdes chiffres écrits à la main ainsi que sur 698 images illustrant un visagesous différentes postures démontrent l'efficacité de notre approche à fortementréduire la dimension tout en conservant les relations intelligibles entre les objetsdes données, permettant même d'obtenir une meilleure classification à partir desversions réduites des images qu'à partir des versions originales	Marc Joliveau	http://editions-rnti.fr/render_pdf.php?p1&p=1001279	http://editions-rnti.fr/render_pdf.php?p=1001279
Revue des Nouvelles Technologies de l'Information	EGC	2010	REGLO : une nouvelle stratégie pour résumer un flux de séries temporelles	Les flux de séries temporelles sont aujourd'hui produits dans de nombreuxdomaines comme la finance (Zhu et Shasha (2002)), la surveillance deréseaux (Borgne et al. (2007); Airoldi et Faloutsos (2004)), la gestion de l'historiquedes usages fréquents (Giannella et al. (2003); Teng et al. (2003)), etc.Résumer de tels flux est devenu un domaine important qui permet de surveilleret d'enregistrer des informations fiables sur les séries observées. À ce jour, lamajorité des algorithmes de ce domaine s'est concentrée sur des résumés séparéset indépendants (Giannella et al. (2003); Zhu et Shasha (2002); Chen et al.(2002)), en accordant à chaque série le même espace en mémoire. Toutefois, lagestion de cet espace mémoire est un sujet important pour les flux de donnéeset une stratégie accordant la même quantité de mémoire à chaque série n'est pasforcément appropriée. Dans cet article, nous considérons que les séries doiventêtre en compétition vis à vis de l'espace mémoire, selon leur besoin de précision.Ainsi, nous proposons : (1) une stratégie de gestion de l'espace mémoireoptimisée et (2) une nouvelle méthode de résumé des séries temporelles par approximation.Dans ce but, nous observons à la fois l'erreur globale et les erreurslocales. La répartition de la mémoire suit les étapes suivantes : (1) recherchede la séquence la mieux représentée et (2) recherche de la partie à compresseren minimisant l'erreur. Nos expérimentations sur des données réelles montrentl'efficacité et la pertinence de notre approche.	Florent Masseglia, Alice Marascu, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001294	http://editions-rnti.fr/render_pdf.php?p=1001294
Revue des Nouvelles Technologies de l'Information	EGC	2010	Regrouper les données textuelles et nommer les groupes à l'aide de classes recouvrantes	Organiser les données textuelles et en tirer du sens est un défi majeuraujourd'hui. Ainsi, lorsque l'on souhaite analyser un débat en ligne ou unforum de discussion, on voudrait pouvoir rapidement voir quels sont les principauxthèmes abordés et la manière dont la discussion se structure autour d'eux.Pour cela, et parce que un même texte peut être associé à plusieurs thèmes, nousproposons une méthode originale pour regrouper les données textuelles en autorisantles chevauchements et pour nommer chaque groupe de manière lisible.La contribution principale de cet article est une méthode globale qui permet deréaliser toute la chaîne, partant des données textuelles brutes jusqu'à la caractérisationdes groupes à un niveau sémantique qui dépasse le simple ensemble demots.	Marian-Andrei Rizoiu, Julien Velcin, Jean-Hugues Chauchat	http://editions-rnti.fr/render_pdf.php?p1&p=1001361	http://editions-rnti.fr/render_pdf.php?p=1001361
Revue des Nouvelles Technologies de l'Information	EGC	2010	Requêtes skyline avec prise en compte des préférences utilisateurs pour des données volumineuses	Appréhender, parcourir des données ou des connaissances reste unetâche difficile en particulier lorsque les utilisateurs sont confrontés à de gros volumesde données. De nombreux travaux se sont intéressés à extraire des points"skylines" comme outil de restitution. La prise en compte des préférences a retenul'attention des travaux les plus récents mais les solutions existantes restenttrès consommatrices en terme de stockage d'informations additionnelles afind'obtenir des délais raisonnables de réponse aux requêtes. Notre proposition,EC2Sky (Efficient computation of compromises), se focalise sur deux points :(1) comment répondre efficacement à des requêtes de type skyline en présencede préférences utilisateurs malgré de gros volumes de données (aussi bien enterme de dimensions que de préférences) ; (2) comment restituer les connaissancesles plus pertinentes en soulignant les compromis associés aux préférencesspécifiées.	Tassadit Bouadi, Sandra Bringay, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001325	http://editions-rnti.fr/render_pdf.php?p=1001325
Revue des Nouvelles Technologies de l'Information	EGC	2010	Résumé généraliste de flux de données	Lorsque le volume des données est trop important pour qu'elles soient stockéesdans une base de données, ou lorsque leur fréquence de production est élevée, les Systèmesde Gestion de Flux de Données (SGFD) permettent de capturer des flux d'enregistrementsstructurés et de les interroger à la volée par des requêtes permanentes (exécutées de façoncontinue). Mais les SGFD ne conservent pas l'historique des flux qui est perdu à jamais.Cette communication propose une définition formelle de ce que devrait être un résumé généralistede flux de données. La notion de résumé généraliste est liée à la capacité de répondreà des requêtes variées et de réaliser des tâches variées de fouille de données, en utilisant lerésumé à la place du flux d'origine. Une revue de plusieurs approches de résumés est ensuiteréalisée dans le cadre de cette définition.	Christine Potier, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001301	http://editions-rnti.fr/render_pdf.php?p=1001301
Revue des Nouvelles Technologies de l'Information	EGC	2010	RSS Merger		Fekade Getahun, Richard Chbeir	http://editions-rnti.fr/render_pdf.php?p1&p=1001396	http://editions-rnti.fr/render_pdf.php?p=1001396
Revue des Nouvelles Technologies de l'Information	EGC	2010	SALINES : un automate au service de l'extraction de motifs séquentiels multidimensionnels	Les entrepôts de données occupent aujourd'hui une place centrale dans le processus décisionnel.Outre leur consultation, une des finalités des entrepôts est de servir de socle aux techniquesde fouilles de données. Malheureusement, les approches existantes exploitent peu les particularitésdes entrepôts (multidimensionnalité, hiérarchies et données historiques). Parmi ces méthodes, l'extractionde motifs séquentiels multidimensionnels a récemment été étudiée. Nous montrons dans cetarticle que ces dernières ne tirent pas pleinement profit des hiérarchies et ne découvrent par conséquentqu'une partie seulement des motifs qualitativement intéressants. Nous proposons alors uneméthode d'extraction de motifs séquentiels multidimensionnels basée sur un automate et extrayantde nouveaux motifs. Les différentes expérimentations menées sur des jeux de données synthétiquesattestent des bonnes performances de notre proposition.	Yoann Pitarch, Lionel Vinceslas, Anne Laurent, Pascal Poncelet, Jean-Emile Symphor	http://editions-rnti.fr/render_pdf.php?p1&p=1001265	http://editions-rnti.fr/render_pdf.php?p=1001265
Revue des Nouvelles Technologies de l'Information	EGC	2010	Sélection par entropie de descripteurs textuels pour la catégorisation de documents		Christophe Moulin, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001406	http://editions-rnti.fr/render_pdf.php?p=1001406
Revue des Nouvelles Technologies de l'Information	EGC	2010	Self-Clustering for Identification of Customer Purchase Behaviours	La segmentation d'une base client peut avoir différents objectifs etplusieurs segmentation peuvent être utiles pour décrire les clients ou pour s'adapteravec les stratégies commerciales d'une entreprise. Dans ce papier, nous présentonsun schéma expérimental visant à proposer un ensemble de segmentationsalternatives. Ces segmentations sont produites sur des données réelles par latransformation des données initiales, la génération et la sélection de différentessegmentations.	Guillem Lefait, Gilles Goncalves, M. Tahar Kechadi	http://editions-rnti.fr/render_pdf.php?p1&p=1001273	http://editions-rnti.fr/render_pdf.php?p=1001273
Revue des Nouvelles Technologies de l'Information	EGC	2010	SequencesViewer : comment rendre accessible des motifs séquentiels de gènes trop nombreux	Les techniques d'extraction de connaissances appliquées aux gros volumesde données, issus de l'analyse de puces ADN, permettent de découvrirdes connaissances jusqu'alors inconnues. Or, ces techniques produisent de trèsnombreux résultats, difficilement exploitables par les experts. Nous proposonsun outil dédié à l'accompagnement de ces experts dans l'appropriation et l'exploitationde ces résultats. Cet outil est basé sur trois techniques de visualisation(nuages, systèmes solaire et treemap) qui permettent aux biologistes d'appréhenderde grandes quantités de motifs séquentiels (séquences ordonnées de gènes).	Arnaud Sallaberry, Nicolas Pecheur, Sandra Bringay, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001322	http://editions-rnti.fr/render_pdf.php?p=1001322
Revue des Nouvelles Technologies de l'Information	EGC	2010	SIAM: Système d'Indexation des Articles Médicaux		Jihen Majdoubi, Mohamed Tmar, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001447	http://editions-rnti.fr/render_pdf.php?p=1001447
Revue des Nouvelles Technologies de l'Information	EGC	2010	Simplification de données de vol pour un stockage optimal et une visualisation accélérée	Le projet RECORDS (collaboration entre industriels et université) apour objectif de développer une infrastructure de service sécurisée pour assurerle suivi et l'analyse des conditions d'utilisation d'aéronefs. Chaque aéronefest muni de capteurs. Au cours de chaque mission (vol) les données mesuréessont enregistrées localement. Ces dernières sont par la suite transférées dansune base de données centralisée à des fins d'analyse. Le problème rencontré estla grande quantité de données ainsi enregistrées, ce qui en rend l'exploitationdifficile. Dans cet article, nous proposons des techniques de compression et desimplification de données avec un taux de perte contrôlé. Nos expérimentationsmontrent des gains drastiques en volumétrie avec de très faibles pertes d'informations.Ceci représente une première étape avant d'appliquer des techniquesd'extraction de connaissances.	Ibrahim Chahid, Loic Martin, Sofian Maabout, Mohamed Mosbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001324	http://editions-rnti.fr/render_pdf.php?p=1001324
Revue des Nouvelles Technologies de l'Information	EGC	2010	SimTole	La plateforme SimTOLE est dédiee a l'evaluation d'algorithmes d'alignement d'ontologies heterogenes et reparties a travers un reseau pair a pair (P2P). Cette plateforme permet de simuler un réseau P2P dans lequel chaque pair dispose de sa propre ontologie ainsi que des outils permettant l'alignement entre l'ontologie locale et une ontologie stockée sur un pair distant. Le developpement de cette plateforme s'inscrit dans le cadre de travaux de recherche étudiant l'impact de la topologie du réseau P2P dans le processus d'inférence de correspondances sémantiques. Durant cette démonstration, la plateforme simTole est présentée puis testée pour illustrer des scénarii montrant comment affiner le processus d'alignement d'ontologies dans un réseau P2P.	Nicolas Lumineau, Lionel Médini	http://editions-rnti.fr/render_pdf.php?p1&p=1001390	http://editions-rnti.fr/render_pdf.php?p=1001390
Revue des Nouvelles Technologies de l'Information	EGC	2010	SoTree : Auto-organisation topologique et hiérarchique des données	Nous proposons dans cet article d'introduire une nouvelle approche pour la classification non supervisée hiérarchique. Notre méthode nommée So-Tree consiste à construire, d'une manière autonome et simultanée, une partition topologique et hiérarchique des données. Chaque "cluster" de la partition est associé à une cellule d'une grille 2D et est modélisé par un arbre, dont chaque noeud représente une donnée. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Les résultats préliminaires obtenus sont encourageants et prometteurs pour continuer dans cette direction.	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001358	http://editions-rnti.fr/render_pdf.php?p=1001358
Revue des Nouvelles Technologies de l'Information	EGC	2010	Sous-échantillonnage topographique par apprentissage semi-supervisé	Plusieurs aspects pourraient influencer les systèmes d'apprentissage existants.Un de ces aspects est lié au déséquilibre des classes dans lequel le nombre d'observationsappartenant à une classe, dépasse fortement celui des observations dans les autresclasses. Dans ce type de cas assez fréquent, le système d'apprentissage a des difficultésau cours de la phase d'entraînement liées au déséquilibre inter-classe. Nous proposonsune méthode de sous-échantillonnage adaptatif pour traiter ce type de bases déséquilibrées.Le processus procède par le sous-échantillonnage des données majoritaires, guidépar les données minoritaires tout au long de la phase d'un apprentissage semi-supervisée.Nous utilisons comme modèle d'apprentissage les cartes auto-organisatrices. L'approcheproposée a été validée sur plusieurs bases de données en utilisant les arbres de décisioncomme classificateur avec une validation croisée. Les résultats expérimentaux ont montrédes performances très prometteuses.	Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001277	http://editions-rnti.fr/render_pdf.php?p=1001277
Revue des Nouvelles Technologies de l'Information	EGC	2010	Suivi d'Automobiles par Classification Hiérarchique Ascendante		Abdelmalek Toumi, Christophe Osswald, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001429	http://editions-rnti.fr/render_pdf.php?p=1001429
Revue des Nouvelles Technologies de l'Information	EGC	2010	Système d'extraction des connaissances à partir des données temporelles basé sur les Réseaux Bayésiens Dynamiques	Un grand nombre d'informations qui ont une structure complexeproviennent de diverses sources. Ces informations contiennent des connaissancestrès utiles pour l'aide à la décision. L'Extraction des Connaissances àpartir des Données (ECD), permet d'acquérir des informations pertinentes pourles systèmes interactifs d'aide à la décision (SIAD). Mais, dans plusieurs domaines,les données évoluent d'une manière dynamique et finissent par dépendrede plusieurs dimensions. Les Réseaux Bayésiens dynamiques (RBD)sont des modèles représentant des connaissances incertaines sur des phénomènescomplexes de processus dynamiques. Notre objectif revient à fixer lesmeilleures modèles de connaissances extraites par les RBD et à les utiliserpour la prise de décision dynamique. Ainsi, Nous proposons dans cet articleune démarche pour la mise en place d'un processus d'extraction des connaissancesà partir des données multidimensionnelles et temporelles.	Ghada Trabelsi, Mounir Ben Ayed, Adel M. Alimi	http://editions-rnti.fr/render_pdf.php?p1&p=1001299	http://editions-rnti.fr/render_pdf.php?p=1001299
Revue des Nouvelles Technologies de l'Information	EGC	2010	Tulip: a Scalable Graph Visualization Framework	The Graph Visualization Framework Tulip now enjoys 10 years ofuser experience, and has matured its architecture and development cycle. Originallydesigned to interactively navigate large graphs, the framework integratesstate-of-the-art software engineering concepts and good practices. It offers alarge panel of graphical representations (traditional graph drawing as well asalternate representations). Tulip is most useful in a data mining and knowledgediscovery context, allowing users to easily add their own data analysis and computingroutines through its plug-in architecture.	David Auber, Patrick Mary, Morgan Mathiaut, Jonathan Dubois, Antoine Lambert, Dan Archambault, Romain Bourqui, Bruno Pinaud, Maylis Delest, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001374	http://editions-rnti.fr/render_pdf.php?p=1001374
Revue des Nouvelles Technologies de l'Information	EGC	2010	Un modèle d'extraction de masses de croyance à partir de probabilités a posteriori pour une amélioration des performances en classification supervisée	L'objectif de cet article est de montrer que l'utilisation de la règle dedécision du maximum de masse de croyance en lieu et place de celle du maximumde probabilité a posteriori peut permettre de réduire le taux d'erreur en classificationsupervisée. Nous proposons une technique efficace pour extraire, à partird'un vecteur de probabilités a posteriori, un vecteur de masses de croyance surlequel baser la décision par le maximum de masse de croyance. L'applicationde notre méthode dans le domaine de la classification automatique en stades desommeil montre une amélioration des performances pouvant atteindre 80% deréduction du taux d'erreur de classification.	Teh Amouh, Monique Noirhomme-Fraiture, Benoît Macq	http://editions-rnti.fr/render_pdf.php?p1&p=1001349	http://editions-rnti.fr/render_pdf.php?p=1001349
Revue des Nouvelles Technologies de l'Information	EGC	2010	Un système d'aide à l'extraction de relations sémantiques pour la construction d'ontologies à partir de textes	Cet article présente une méthode d'extraction de relations sémantiquespour la construction d'ontologies à partir de corpus de textes. Notre objectif estde proposer une méthode générique, qui soit indépendante du domaine et de lalangue. Elle repose sur une analyse distributionnelle des unités sémantiques ducorpus pour faire émerger des relations sémantiques candidates. Cette méthodene fait aucune hypothèse sur les types de relations recherchées ni sur leur formelinguistique. Il s'agit de regrouper les associations de termes dans des classesqui représentent des relations sémantiques candidates. L'hypothèse sous-jacenteest que les occurrences de ces associations réunies sur la base des éléments decontexte qu'elles partagent ont des chances de relever d'une même relation sémantiqueet que les relations candidates ainsi proposées peuvent aider le travailde conceptualisation de l'ontologue	Rim Bentebibel, Adeline Nazarenko, Sylvie Szulman	http://editions-rnti.fr/render_pdf.php?p1&p=1001343	http://editions-rnti.fr/render_pdf.php?p=1001343
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une approche fondée sur la corrélation entre prédicats pour le traitement des réponses pléthoriques	L'interrogation de bases de données, dont les dimensions ne cessentde croître, se heurte fréquemment au problème de la gestion des réponses pléthoriques.Une des approches envisageables pour réduire l'ensemble des résultatsretournés et le rendre exploitable est de contraindre la requête initiale parl'ajout de nouvelles conditions. L'approche présentée dans cet article s'appuiesur l'identification de liens de corrélation entre prédicats associés aux attributsde la relation concernée. La requête initiale peut ainsi être intensifiée automatiquementou par validation de l'utilisateur à travers l'ajout de prédicats prochessémantiquement de ceux spécifiés.	Patrick Bosc, Allel HadjAli, Olivier Pivert, Grégory Smits	http://editions-rnti.fr/render_pdf.php?p1&p=1001305	http://editions-rnti.fr/render_pdf.php?p=1001305
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une approche probabiliste pour l'identification de structures de communautés	Dans cet article, nous valorisons et défendons l'idée que les modèles génératifs sont une approche prometteuse pour l'identification de structure de communautés (ISC). Nous proposons un nouveau modèle probabiliste pour l'idenditification de structures de communautés qui utilise le lissage afin de pallier le petit nombre de liens entre les noeuds. Notre modèle étant très sensible aux paramètres de lissage, nous proposons également une méthode basée sur la modularité pour leur estimation. Les résultats expérimentaux obtenus sur 3 jeux de données montrent que notre modèle SPCE est largement meilleur que le modèle PHITS	Nacim Fateh Chikhi, Bernard Rothenburger, Nathalie Aussenac-Gilles	http://editions-rnti.fr/render_pdf.php?p1&p=1001289	http://editions-rnti.fr/render_pdf.php?p=1001289
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une méthode d'aide au management de connaissances pour améliorer le processus de suivi et d'évaluation de la prise en charge précoce des enfants IMC : application de l'ASHMS		Mohamed Turki, Inès Saad, Gilles Kassel, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001431	http://editions-rnti.fr/render_pdf.php?p=1001431
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une nouvelle approche de découverte des correspondances complexes entre ontologies	Les correspondances complexes ont été étudiées à plusieurs reprisesdans le domaine d'alignement de schémas de bases de données. Par contre,dans le domaine d'alignement des ontologies, elles ont été peu étudiées. Nousproposons, dans ce papier, une nouvelle approche de découverte de correspondancescomplexes entre deux ontologies. L'approche proposée est extensionnelle,terminologique et implicative. Dans cette approche, nous utilisons le modèledes règles d'association afin de découvrir des correspondances de typex &#8658; y1 &#8743; ... &#8743; yn entre deux ontologies.	Fatma Kaâbi, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001414	http://editions-rnti.fr/render_pdf.php?p=1001414
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une nouvelle stratégie d'Apprentissage Bayésienne	Dans cet article, une nouvelle stratégie d'apprentissage actif est proposée. Cette stratégie est fondée sur une méthode de discrétisation Bayésienne semi-supervisée. Des expériences comparatives sont menées sur des données unidimensionnelles, l'objectif étant d'estimer la position d'un échelon à partir de données bruitées.	Alexis Bondu, Vincent Lemaire, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001463	http://editions-rnti.fr/render_pdf.php?p=1001463
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une Ontologie pour l'Acquisition et l'Exploitation des Connaissances en Conception Inventive	L'acquisition des connaissances en vue de résoudre des problèmesconcernant l'évolution des artefacts, comme elle se doit d'être pratiquée enconception inventive, a des caractéristiques spécifiques. Elle nécessite lasélection de certaines des connaissances qui peuvent induire des évolutions,elle amène à reformuler le problème initial afin de construire un modèleabstrait de l'artefact concerné. La méthode de conception inventive induite parla théorie de la Résolution des Problèmes Inventifs (aussi connue sousl'acronyme TRIZ) n'a pas encore fait l'objet d'une véritable formalisation.Nous proposons ici une ontologie des notions principales des concepts liés àl'acquisition des connaissances dans ce cadre. Cette ontologie, outre laclarification des notions en jeu, est utilisée comme support d'un environnementinformatique d'aide à la mise en oeuvre d'une méthode pour acquérir lesconnaissances et formuler les problèmes.	François Rousselot, Cecilia Zanni, Denis Cavallucci	http://editions-rnti.fr/render_pdf.php?p1&p=1001470	http://editions-rnti.fr/render_pdf.php?p=1001470
Revue des Nouvelles Technologies de l'Information	EGC	2010	Une structure basée sur les hiérarchies pour synthétiser les itemsets fréquents extraits dans des fenêtres temporelles	Le paradigme des flots de données rend impossible la conservation de l'intégralitéde l'historique d'un flot qu'il faut alors résumer. L'extraction d'itemsets fréquentssur des fenêtres temporelles semble tout à fait adaptée mais l'amoncellement des résultatsindépendants rend impossible l'exploitation de ces résultats. Nous proposons une structurebasée sur les hiérarchies des données afin d'unifiant ces résultats. De plus, puisque laplupart des données d'un flot présentent un caractère multidimensionnel, nous intégronsla prise en compte d'itemsets multidimensionnels. Enfin, nous pallions une faiblesse majeuredes Tilted TimeWindows (TTW) en prenant en compte la distribution des données.	Yoann Pitarch, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001467	http://editions-rnti.fr/render_pdf.php?p=1001467
Revue des Nouvelles Technologies de l'Information	EGC	2010	Utilisation de graphes sémantiques pour l'extraction et la traduction des idées essentielles d'un texte		Romain André-Lovichi, Kamel Smaïli, David Langlois	http://editions-rnti.fr/render_pdf.php?p1&p=1001435	http://editions-rnti.fr/render_pdf.php?p=1001435
Revue des Nouvelles Technologies de l'Information	EGC	2010	Vers une extraction et une visualisation des co-localisations adaptées aux experts	Une des tâches classiques en fouille de données spatiales est l'extractionde co-localisations intéressantes dans des données géo-référencées. L'objectifest de trouver des sous-ensembles de caractéristiques booléennes apparaissantfréquemment dans des objets spatiaux voisins. Toutefois, les relations découvertespeuvent ne pas être pertinentes pour les experts, et leur interprétation sousforme textuelle peut être difficile. Nous proposons, dans ce contexte, une nouvelleapproche pour intégrer la connaissance des experts dans la découverte desco-localisations, ainsi qu'une nouvelle représentation visuelle de ces motifs. Unprototype a été développé et intégré dans un SIG. Des expérimentations on étémenées sur des données géologiques réelles, et les résultats validés par un expertdu domaine.	Frédéric Flouvat, Nazha Selmaoui-Folcher, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001335	http://editions-rnti.fr/render_pdf.php?p=1001335
Revue des Nouvelles Technologies de l'Information	EGC	2010	Visual Sentence-Phrase-Based Document Representation for Effective and Efficient Content-Based Image Retrieval	Having effective and efficient methods to get access to desired imagesis essential nowadays with the huge amount of digital images. This paperpresents an analogy between content-based image retrieval and text retrieval.We make this analogy from pixels to letters, patches to words, sets of patchesto phrases, and groups of sets of patches to sentences. To achieve a more accuratedocument matching, more informative features including phrases and sentencesare needed to improve these scenarios. The proposed approach is basedfirst on constructing different visual words using local patch extraction and description.After that, we study different association rules between frequent visualwords in the context of local regions in the image to construct visual phrases,which will be grouped to different sentences.	Ismail Elsayad, Jean Martinet, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001284	http://editions-rnti.fr/render_pdf.php?p=1001284
Revue des Nouvelles Technologies de l'Information	EGC	2010	Visualisation de mesures agrégées pour l'estimation de la qualité des articlesWikipedia	Wikipedia, devenue l'une des bases de connaissances les plus populaires,pose le problème de la fiabilité de l'information qu'elle dissémine. Nousproposons WikipediaViz, un ensemble de visualisations basé sur un mecanismede collecte et d'agrégation de données d'édition Wikipedia pour aider le lecteurà appréhender la maturité d'un article. Nous listons cinq métriques importantes,déterminées lors de sessions de conception participative avec des experts Wikipediapour juger de la qualité, que nous présentons au lecteur sous forme devisualisations compactes et expressives, dépeignant le profil d'évolution d'un article.Nos études utilisateur ont montré queWikipediaViz réduisait significativementle temps requis pour évaluer la qualité en maintenant une bonne précision	Fanny Chevalier, Stéphane Huot, Jean-Daniel Fekete	http://editions-rnti.fr/render_pdf.php?p1&p=1001316	http://editions-rnti.fr/render_pdf.php?p=1001316
Revue des Nouvelles Technologies de l'Information	EGC	2010	WCUM pour l'analyse d'un site web	Dans ce papier, nous proposons une approche WCUM (Web Contentand Usage based Approach) permettant de relier l'analyse du contenu d'un siteWeb à l'analyse de l'usage afin de mieux comprendre les comportements de navigationsur le site. L'apport de ce travail réside d'une part dans la propositiond'une approche reliant l'analyse du contenu à l'analyse de l'usage et d'autre partdans l'extension de l'application des méthodes de block clustering, appliquéesgénéralement en bioinformatique, au contexte Web mining afin de profiter deleur pouvoir classificatoire dans la découverte de biclasses homogènes à partird'une partition des instances et une partition des attributs recherchées simultanément.	Malika Charrad, Yves Lechevallier, Mohamed Ben Ahmed, Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1001413	http://editions-rnti.fr/render_pdf.php?p=1001413
Revue des Nouvelles Technologies de l'Information	FDC	2010	Aide au diagnostic de pannes guidée par l'extraction de motifs séquentiels	La maintenance de systèmes complexes pose problème dans de nombreuxdomaines industriels. Le diagnostic de pannes à partir de données issues decapteurs fournissant de nombreuses informations complémentaires est un véritabledéfi. Nous nous intéressons ici à la caractérisation des comportements normauxde ces systèmes par des méthodes d'extraction de connaissances. Il s'agitd'un problème difficile. Les données contiennent diverses erreurs et proviennentde nombreuses sources pouvant correspondre à différents types d'informationspropres aux systèmes étudiés. Nous étudions et proposons plusieurs solutionsafin de traiter efficacement ce type de données et d'offrir des connaissancesutiles et suffisamment complètes pour répondre aux exigences de la maintenance.Nous proposons donc une méthodologie complète, allant de l'acquisitiondes données brutes issues des capteurs, jusqu'à l'extraction des connaissancessouhaitées. Ainsi, en premier lieu, nous nous intéressons au problème dela représentation de telles données pour dégager l'information contenue dans lesdonnées brutes. Puis, afin de fournir des connaissances utiles et valides, nousétudions les méthodes de fouille de données existantes pour les adapter à notreproblématique. De plus, nous proposons une méthode de détection de tendancesqui tient compte de l'évolution des comportements normaux au fil du temps due,par exemple, à l'usure du matériel. L'applicabilité des propositions développéesest évaluée sur un jeu de données réel.	Julien Rabatel, Sandra Bringay, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001254	http://editions-rnti.fr/render_pdf.php?p=1001254
Revue des Nouvelles Technologies de l'Information	FDC	2010	Association pour le suivi d'objets dans le cadre des fonctions de croyance, appliquée aux véhicules intelligents	Le problème traité dans cet article concerne le suivi d'obstacles dansle cadre de l'aide à la conduite automobile, et plus particulièrement l'étape d'association.Cette étape consiste à mettre en relation des mesures sur des objetsdétectées à un instant donné, avec des pistes correspondant aux trajectoires suiviesà des instants antérieurs. Généralement les mesures sur ces objets sontissues de plusieurs capteurs, et sont entachées d'erreurs. Une étape de fusionmulti-capteurs permet alors d'obtenir un consensus de meilleure qualité. Introduitespar Dempster et Shafer, les fonctions de croyance constituent un cadrebien adapté pour la représentation et la manipulation d'informations imparfaites.Ainsi, celles-ci ont servies de base à l'implémentation d'un algorithme d'associationintroduit par Rombaut puis développé par Gruyer. Dans cet article, unemodélisation de ce problème, dans le cadre du modèle des croyances transférables,est introduite. Elle diffère des approches précédentes notamment sur lechoix de la méthode de combinaison, et sur la construction de la décision. Cetteapproche est validée par des tests sur des données réelles.	David Mercier, Eric Lefevre, Daniel Jolly	http://editions-rnti.fr/render_pdf.php?p1&p=1001252	http://editions-rnti.fr/render_pdf.php?p=1001252
Revue des Nouvelles Technologies de l'Information	FDC	2010	Classification automatique de documents bruités à faible contenu textuel	La classification de documents numériques est une tâche complexedans un flux numérique de gestion électronique de documents. Cependant, laquantité des documents issus de la retro-conversion d'OCR (ReconnaissanceOptique de Caractères) constitue une problématique qui ne facilite pas la tâchede classification. Après l'étude et l'évaluation des descripteurs les mieux adaptésaux documents issus d'OCR, nous proposons une nouvelle approche de représentationdes données textuelles : l'approche HYBRED (HYBrid REpresentationof Documents). Elle permet de combiner l'utilisation de différents descripteursd'un texte afin d'obtenir une représentation plus pertinente de celui-ci. Les expérimentationsmenées sur des données réelles ont montré l'intérêt de notre approche	Sami Laroum, Nicolas Béchet, Hatem Hamza, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001257	http://editions-rnti.fr/render_pdf.php?p=1001257
Revue des Nouvelles Technologies de l'Information	FDC	2010	Extraction de comportements inattendus dans le cadre du Web Usage Mining	Au cours de ces dernières années, la fouille de données d'usage duWeb s'est de plus en plus concentrée sur l'extraction des comportements desutilisateurs à partir de fichiers logs. Bien que l'extraction de motifs séquentielspermette de trouver des comportements fréquents, les décideurs sont de plus enplus intéressés par des comportements inattendus qui contredisent les croyancessur des connaissances existantes. Dans cet article, nous présentons une nouvelleapproche, WebUser, pour découvrir des comportements inattendus par rapportaux croyances du domaine. Les expérimentations menées, avec des bases decroyances générées à partir des comportements connus, montrent que notre approchepermet d'extraire des comportements inattendus qui peuvent être utiliséspour, par exemple, améliorer la structure des sites Web ou repérer des usagesparticuliers.	Dong (Haoyuan) Li, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001255	http://editions-rnti.fr/render_pdf.php?p=1001255
Revue des Nouvelles Technologies de l'Information	FDC	2010	L'Analyse Formelle de Concepts au service de la construction et l'enrichissement d'une ontologie	Dans cet article, nous proposons une méthodologie appelée PACTOLE«Property And Class Caracterisation from Text to OntoLogy Enrichment» quipermet de construire une ontologie dans un domaine spécifique et pour une applicationdonnée. PACTOLE fusionne et combine différentes ressources à l'aidede l'Analyse Formelle de Concepts (AFC) et de son extension l'Analyse Relationnellede Concepts (ARC). Les expressions produites par AFC/ARC sont représentéesen expressions d'une Logique de Descriptions LD (ici FLE) puisimplémentées en OWL. Il est ensuite possible de raisonner sur ces expressions.Cette méthodologie est appliquée au domaine de l'astronomie. Nous montronsaussi comment nous avons formalisé et répondu à certaines questions que seposent les astronomes.	Yannick Toussaint, Rokia Bendaoud, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001256	http://editions-rnti.fr/render_pdf.php?p=1001256
Revue des Nouvelles Technologies de l'Information	FDC	2010	Segmentation de données de type intervalle, diagramme et taxonomique	L'objectif de cet article consiste à étendre le critère de découpagebinaire de Kolmogorov-Smirnov aux données de type intervalle, diagramme ettaxonomique. Ce critère nécessite un ordre des valeurs prises par les variablesexplicatives. Nous utilisons différentes méthodes pour ordonner ce type dedonnées. Nous présentons le format des questions binaires et la description desnoeuds terminaux pour chacun de ces types de données. Nous étudionségalement la précision de ce critère et nous le comparons aux critères del'entropie et de Gini.	Chérif Mballo	http://editions-rnti.fr/render_pdf.php?p1&p=1001251	http://editions-rnti.fr/render_pdf.php?p=1001251
Revue des Nouvelles Technologies de l'Information	FDC	2010	Une approche basée sur la qualité pour faciliter l'intégration de modèles de cubes de données spatiales	L'intégration de cubes de données spatiales permet de faciliterl'accès et la réutilisation des données qui proviennent de différents cubes afinde répondre à des besoins d'analyse stratégique. Cette intégration fait face àdes problèmes complexes d'hétérogénéité des cubes de données spatiales.Malgré des travaux intéressants dans le domaine de l'intégration des données,ces problèmes particuliers aux cubes de données spatiales n'ont pas été traités.Cet article explique l'intérêt de l'intégration des cubes de données spatiales,présente une catégorisation des problèmes d'hétérogénéité liés aux modèlesdes cubes de données spatiales, et propose une approche pour aider à prendreles décisions appropriées concernant l'intégration des cubes de donnéesspatiales. L'approche consiste en un cadre général qui se base sur une structuredescendante et une méthode qui propose et évalue un ensemble d'indicateursde qualité des modèles de cubes à intégrer. L'approche est illustrée par unexemple d'application.	Tarek Sboui, Mehrdad Salehi, Yvan Bédard, Sonia Rivest	http://editions-rnti.fr/render_pdf.php?p1&p=1001253	http://editions-rnti.fr/render_pdf.php?p=1001253
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Classification supervisée et non supervisée en grande dimension	Cet article est consacré à la classification des données de grande dimension. Supposant que de telles données vivent dans des sous-espaces de dimensions intrinsèques inférieures à la dimension de l'espace original, nous proposons une re-paramétrisation du modèle de mélange gaussien. En forçant certains paramètres à être communs dans une même classe ou entre les classes, nous exhibons une famille de modèles adaptés aux données de grande dimension, allant du modèle le plus général au plus parcimonieux. Ces modèles gaussiens sont ensuite utilisés pour la classification supervisée ou non-supervisée. La nature de notre re-paramétrisation permet aux méthodes ainsi construites de ne pas être perturbées par le mauvais conditionnement ou la singularité des matrices de covariance empiriques des classes et d'être efficaces en terme de temps de calcul.	Stéphane Girard, Charles Bouveyron	http://editions-rnti.fr/render_pdf.php?p1&p=1001776	http://editions-rnti.fr/render_pdf.php?p=1001776
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Détection d'hétérogénéité au sein de mesures de qualité de l'environnement	L'accréditation des laboratoires d'analyses de l'environnement nécessite un contrôle qualité externe des mesures qu'ils effectuent. Pour ce faire, AGLAE réalise des campagnes d'essais consistant à envoyer des échantillons à mesurer par les laboratoires. Sous l'hypothèse que les mesures retournées par les laboratoires sont gaussiennes, un intervalle d'acceptation est construit sur les résultats de mesure de sorte à étudier la performance analytique de chaque laboratoire. Il arrive toutefois que l'hypothèse de normalité ne soit pas satisfaite, ce qu'il est indispensable de prendre en compte lors de la construction de l'intervalle d'acceptation. L'intérêt de ce travail consiste à détecter une hétérogénéité au sein des mesures des laboratoires, que nous caractérisons par la présence de plusieurs sous-populations gaussiennes au sein des mesures. Le logiciel MIXMOD est alors utilisé pour identifier la présence de ces sous-populations et, le cas échéant, estimer leurs paramètres.	Julien Jacques, Carole Langlois-Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1001773	http://editions-rnti.fr/render_pdf.php?p=1001773
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Le Graphe Génératif Gaussien.	Un nuage de points est plus qu'un ensemble de points isolés. La distribution des points peut être gouvernée par une structure topologique cachée, et du point de vue de la fouille de données, modéliser et extraire cette structure est au moins aussi important que d'estimer la seule densité de probabilité du nuage. Dans cet article, nous proposons un modèle génératif basé sur le graphe de Delaunay d'un ensemble de prototypes représentant le nuage de points, et supposant un bruit gaussien. Nous dérivons un algorithme pour la maximisation de la vraisemblance des paramètres, et nous utilisons le critère BIC pour sélectionner la complexité du modèle. Ce travail a pour objectif de poser les premières pierres d'un cadre théorique basé sur les modèles génératifs statistiques, permettant la construction automatique de modèles topologiques d'un nuage de points.	Pierre Gaillard, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001777	http://editions-rnti.fr/render_pdf.php?p=1001777
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Le logiciel SpaCEM3 pour la classification de données complexes.	Le logiciel SPACEM3 (Spatial Clustering with EM and Markov Models) propose une variété d'algorithmes pour la classification, supervisée ou non supervisée, de données uni ou multi-dimensionnelles en interaction, certaines de ces données pouvant être manquantes. Les structures de dépendances prises en compte sont celles pouvant être décrites par un graphe fini quelconque. Elles incluent le cas particulier des grilles régulières utilisées notamment en segmentation d'images. L'approche principale se fonde sur l'utilisation de l'algorithme EM pour une classification floue et sur les modèles de champs de Markov pour la modélisation des dépendances. L'estimation est basée sur des développements récents mettant en oeuvre des techniques d'approximations variationnelles de type champ moyen.	Juliette Blanchet, Florence Forbes, Sophie Chopart, Lamiae Azizi	http://editions-rnti.fr/render_pdf.php?p1&p=1001779	http://editions-rnti.fr/render_pdf.php?p=1001779
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Les modèles de mélange, un outil utile pour la classification semi-supervisée.	En classification supervisée, la règle de classement est apprise à partir d'un échantillon d'apprentissage généralement constitué de données classées. Dans la plupart des cas l'obtention de la classe est plus coûteuse que l'obtention de covariables associées à la classe d'où l'intérêt d'apprendre une règle de prédiction de la classe à partir de ces covariables. Ainsi dans de nombreuses situations beaucoup de données non classées, obtenues à un coût relativement faible, sont disponibles en plus des données classées. Au cours des dernières années la classification semi-supervisée, qui fait usage des données non classées pour améliorer la précision de la règle de classement apprise, a connu un essor important, ceci notamment dans la communauté du Machine Learning. Les modèles génératifs, qui modélisent la distribution jointe de la classe et des covariables, permettent de prendre naturellement en compte l'information apportée par les données non classées dans l'apprentissage de la règle de classement. Dans cet article nous dressons un panorama de la classification semi-supervisée et nous détaillons sa mise en oeuvre dans le cadre des modèles génératifs.	Vincent Vandewalle	http://editions-rnti.fr/render_pdf.php?p1&p=1001778	http://editions-rnti.fr/render_pdf.php?p=1001778
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Mélanges gaussiens bidimensionnels pour la comparaison de deux échantillons de chromatine immunoprécipitée. 	L'immunoprécipitation de la chromatine (ChIP) permet d'étudier les interactions entre les protéines et l'ADN ainsi que différents états chromatiniens. Le ChIP-chip est une technique combinant l'immunoprécipitation de la chromatine avec le principe des puces à ADN, ce qui permet une étude à l'échelle du génome. Nous nous intéressons ici à l'analyse des différences entre deux échantillons d'ADN immunoprécipité. Biologiquement, on s'attend à distinguer quatre groupes différents : un groupe d'ADN non-immunoprécipité, un groupe d'ADN immunoprécipité identiquement dans les deux échantillons et deux groupes dans lesquels l'ADN est immunoprécipité en quantités différentes. Nous modélisons ces données par un mélange de gaussiennes bidimensionnelles à quatre composants. Les matrices de variance sont contraintes afin d'intégrer des connaissances biologiques. Les paramètres sont estimés par l'algorithme EM. Nous appliquons cette méthode pour étudier la différence de méthylation d'une histone entre l'écotype sauvage de la plante modèle \textit{Arabidopsis thaliana} et un mutant.	Caroline Bérard, Marie-Laure Martin-Magniette, Alexandra To, Francçis Roudier, Vincent Colot, Stéphane Robin	http://editions-rnti.fr/render_pdf.php?p1&p=1001774	http://editions-rnti.fr/render_pdf.php?p=1001774
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	MIXMOD : un logiciel de classification supervisée et non supervisée pour données quantitatives et qualitatives.	MIXMOD, logiciel de classification supervisée et non supervisée, traite des données multidimensionnelles en utilisant la richesse des modèles de mélanges. Il propose de nombreuses fonctionnalités (large palette de modèles gaussiens et multinomiaux, d'algorithmes, de critères de sélection, ...) et repose sur une architecture particulièrement adaptée pour atteindre un haut niveau de performance (temps de calcul et robustesse). MIXMOD est téléchargé environ 250 fois par mois, et il est utilisé dans des situations très diverses à la fois dans le milieu de la recherche et, pour une part croissante, par des utilisateurs novices. Des évolutions sont régulièrement proposées pour répondre aux demandes des utilisateurs et intégrer les résultats de la recherche.	Florent Langrognet	http://editions-rnti.fr/render_pdf.php?p1&p=1001772	http://editions-rnti.fr/render_pdf.php?p=1001772
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Pourquoi les modèles de mélange pour la classification ? 	Les modèles de mélange apportent une réponse rigoureuse, flexible et interprétable pour les multiples besoins de la classification : classification supervisée ou non, nature des données, choix du nombre de groupes, etc. Les domaines d'applications sont de plus en plus nombreux, aidés en cela par le développement de solutions logicielles adaptées	Christophe Biernacki	http://editions-rnti.fr/render_pdf.php?p1&p=1001771	http://editions-rnti.fr/render_pdf.php?p=1001771
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2009	Sélection de variables pour la classification par mélanges gaussiens pour prédire la fonction des gènes orphelins	Les biologistes s'attachent actuellement à prédire la fonction des gènes d'organismes de génome séquencé à partir de données transcriptomes, issues de l'utilisation des puces à ADN. Le développement de cette technologie permet de tester l'expression de l'ensemble du génome dans de nombreuses conditions expérimentales. Cette quantité d'information peut alors sembler être un atout pour la classification des gènes. Pourtant il est courant que seul un sous-ensemble contienne l'information pertinente pour la classification. Les procédures de sélection des variables en classification non supervisée par mélanges gaussiens supposent généralement que les variables non informatives pour la classification sont soit toutes indépendantes, soit liées à des variables informatives. Nous proposons une nouvelle modélisation du rôle des variables plus polyvalente : les variables sont soit informatives pour la classification, soit redondantes, soit totalement indépendantes. Nous proposons un critère de sélection des variables et un algorithme pour cette nouvelle modélisation. L'intérêt de cette nouvelle modélisation pour la prédiction de la fonction des gènes orphelins est illustrée sur un ensemble de données transcriptomes obtenues chez Arabidopsis thaliana.	Cathy Maugis, Marie-Laure Martin-Magniette, Jean-Philippe Tamby, Jean-Pierre Renou, Alain Lecharny, Sébastien Aubourg, Gilles Celeux	http://editions-rnti.fr/render_pdf.php?p1&p=1001775	http://editions-rnti.fr/render_pdf.php?p=1001775
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Affectation pondérée par le critère de Kolmogorov-Smirnov sur des données de type intervalle et diagramme	Le critère de découpage binaire de Kolmogorov-Smirnov a été introduitpar (Friedman, 1977) pour une partition binaire à expliquer sur des variablescontinues. Nos travaux antérieurs nous ont permis de l'étendre dans le casoù les objets destinés à être classés par un arbre de décision sont décrits par desvariables de type intervalle et diagramme ((Mballo et Diday, 2004), (Mballo etal., 2004)) en adoptant une affectation pure. Dans cet article, nous proposonsune méthode permettant d'affecter une donnée à la fois aux deux noeuds filsgénérés par le partitionnement d'un noeud non terminal. Cette approched'affectation est basée sur des poids et tient compte de la position de la donnéeà classer par rapport à celle seuil de coupure.	Chérif Mballo	http://editions-rnti.fr/render_pdf.php?p1&p=1000825	http://editions-rnti.fr/render_pdf.php?p=1000825
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Analyse de la Vraisemblance des Liens Relationnels : Une méthodologie d'analyse classificatoire des données	La méthodologie de classification des données par l'Analyse de laVraisemblance des Liens Relationnels a pris naissance vers la fin des annéessoixante. Elle s'est très largement développée. De nombreux chercheurs et praticiensont pris part à son développement. De nombreuses applications d'envergureprovenant des domaines les plus divers (Bioinformatique, Informatique,Sciences sociales, Traitement d'images, Traitement de langues naturelles, ...) ontvalidé cette méthodologie. Elle s'adresse à n'importe quel type mathématicologiquedu tableau de description des données. L'objet de notre article est deprésenter de façon illustrée les principes fondamentaux de cette approche. Cesprincipes se situent d'une part au niveau de la représentation de la description etd'autre part, au niveau de l'évaluation quantifiée des ressemblances entre lesstructures mathématiques à comparer. Dans notre cas la représentation de ladescription sera ensembliste et relationnelle et la ressemblance sera évaluée aumoyen d'une similarité qui se réfère à une échelle de probabilité établie par rapportà une hypothèse statistique d'absence de liaison. Le texte ci-dessous se veutle reflet de ma présentation orale aux "3-èmes Journées Thématiques ApprentissageArtificiel et Fouille des Données", 8-9 avril 2008.	Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1000832	http://editions-rnti.fr/render_pdf.php?p=1000832
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Classification sous contraintes probabilistes par les cartes topologiques	La classification automatique est un processus non supervisé qui vise à regrouperdes données en un ensemble de classes hétérogènes. En outre, Différents travaux ontmontré que l'intégration de contraintes peut augmenter le taux de ce processus de classificationtout en diminuant le temps d'exécution. Cette nouvelle démarche a connu, ces dernièresannées, un travail bien étendu. La forme la plus répandue de ces dites contraintes est de type« Must-Link » dont le nom indique l'obligation d'avoir les données dans une même classe, etles contraintes « Cannot-Link » dont le nom indique l'interdiction d'avoir les données dansune même classe. Le travail présenté dans cet article décrit une nouvelle version des cartestopologiques que nous appelons « PrTM » (Probabilistic constrained Topological Map) pourintégrer des contraintes probabilistes. PrTM représente une variante d'un algorithme populairedes cartes topologiques probabilistes GTM (Generative Topographic Mapping). Pourvalider notre approche, des comparaisons entre notre proposition « PrTM » et d'autres algorithmesde classification sous contraintes, sont présentées sur différentes bases de donnéesissues de la littérature.	Jihène Snoussi, Khalid Benabdeslem	http://editions-rnti.fr/render_pdf.php?p1&p=1000826	http://editions-rnti.fr/render_pdf.php?p=1000826
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Essai de Typologie Structurelle des Indices de Similarité Vectoriels par Unification Relationnelle	Cet article a pour but de proposer un regard nouveau et unificateurà la problématique des Indices de Similarité et des Critères de structuration oude partitionnement. Une catégorisation des indices, des propriétés non connuesainsi qu'une présentation dans différents axes de structuration serontsuggérées. La recherche des significations et des filiations associées seradonnée comme résultat dérivé de ce travail	François Marcotorchino	http://editions-rnti.fr/render_pdf.php?p1&p=1001500	http://editions-rnti.fr/render_pdf.php?p=1001500
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Etude comparée des performances de SVM multi-classes en prédiction de la structure secondaire des protéines	Les SVM bi-classes, introduites en bioinformatique à la fin des années90, font aujourd'hui référence pour de nombreux problèmes de traitementde séquences biologiques. Les SVM multi-classes, de conception plus récente,sont progressivement appliquées à ces problèmes, singulièrement en biologiestructurale prédictive. Dans cet article, nous proposons une étude comparée desperformances de trois SVM multi-classes en prédiction de la structure secondairedes protéines. Les modèles impliqués sont celui de Weston et Watkins,celui de Lee et co-auteurs ainsi qu'une nouvelle machine nommée M-SVM2.Cette étude se conçoit comme une étape dans la mise au point d'une méthode deprédiction hybride, intégrant systèmes discriminants et génératifs et s'appuyantsur une approche hiérarchique du problème.	Yann Guerneur	http://editions-rnti.fr/render_pdf.php?p1&p=1000827	http://editions-rnti.fr/render_pdf.php?p=1000827
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Modélisation probabiliste de collections textuelles et distributions de mots	Nous examinons dans cet article les liens entre modèles probabilistesde documents textuels et observations empiriques sur la distribution des motsau sein d'une collection. Nous proposons une caractérisation formelle de cesobservations, et introduisons la distribution beta négative binomiale. Cette distribution(connue sous diverses dénominations mais dont la dérivation que nousproposons est nouvelle) permet de rendre compte des observations empiriqueset fournit un modèle non paramétrique dont le bon comportement est validé encatégorisation de textes.	Stéphane Clinchant, Éric Gaussier	http://editions-rnti.fr/render_pdf.php?p1&p=1000828	http://editions-rnti.fr/render_pdf.php?p=1000828
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Panorama de quelques approches récentes pour la classification non supervisée de graphes	Les avancées technologiques récentes ont permis d'acquérir dans denombreux domaines des corpus de graphes. Une problématique en plein essorconsiste à classer ces données complexes pour établir des typologies. Différentesapproches développées en fouille de données sont présentées dans cet article: lavisualisation de graphes dans une perspective exploratoire, la caractérisation desgraphes par des descripteurs structurels et fonctionnels, par des sous-structureset par des décompositions spectrales, et les méthodes à noyaux	Pascale Kuntz, Fabien Picarougne	http://editions-rnti.fr/render_pdf.php?p1&p=1000830	http://editions-rnti.fr/render_pdf.php?p=1000830
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Recherche de communautés dans les grands réseaux sociaux	Cet article décrit quelques méthodes récentes pour la recherche decommunautés dans les grands réseaux sociaux (dizaines voire centaines de millionsde noeuds). Après avoir rappelé quelques notions de base sur ce sujet, nousdécrivons quelques approches récentes pour l'extraction de micro-communautéset de communautés globales, et montrons quelques résultats prouvant que cesméthodes sont parfaitement utilisables pour la fouille d'ensembles de donnéesparmi les plus grands rencontrés aujourd'hui dans les applications industrielles.	Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1000831	http://editions-rnti.fr/render_pdf.php?p=1000831
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Représentation des données par un comité de cartes auto-organisatrices : une application aux données bruitées.	Grâce aux approches ensemblistes, les performances en apprentissagesupervisé sont devenues excellentes sans pour autant être trop coûteuses entemps. Cependant, ces méthodes ne permettent que la prédiction des données.Or, le couplage entre la prédiction et une méthode de représentation ajoute unevaleur qualitative. La représentation permet de redonner la main à l'utilisateur,que ce soit avant la prédiction pour visualiser les données et juger de la qualitédu modèle ou après en permettant l'exploration des exemples qui ont conduit à laprédiction. En outre, une représentation des données - obtenue indépendammentde la variable de classe - est robuste au bruit sur la variable de classe puisquecelle-ci n'est pas intégrée dans l'apprentissage. Les données en grandes dimensionsposent toutefois le problème d'obtenir une représentation de qualité en untemps raisonnable. Dans ce contexte, nous proposons le recours à un comité decartes auto-organisatrices dont l'apprentissage est synthétisé par une carte supplémentaire,apprise grâce à un stacking de la position des neurones. Le comitétire parti du concept de diversité pour assurer une prédiction de qualité alors quele stacking géographique offre une représentation synthétique facilement manipulablepar l'utilisateur final. Les expérimentations montrent que cette stratégieest compétitive par rapport aux approches spécialisées dans la prédiction touten permettant une représentation des données. Enfin, elle permet de gérer desniveaux de bruit importants.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000829	http://editions-rnti.fr/render_pdf.php?p=1000829
Revue des Nouvelles Technologies de l'Information	AAFD	2009	Un modèle génératif pour l'Apprentissage de la Topologie	Un nuage de points est plus qu'un ensemble de points isolés. La distributiondes points peut être gouvernée par une structure topologique cachée, etdu point de vue de la fouille de données, modéliser et extraire cette structure estau moins aussi important que d'estimer la seule densité de probabilité du nuage.Dans cet article, nous proposons un modèle génératif basé sur le graphe de Delaunayd'un ensemble de prototypes représentant le nuage de points, et supposantun bruit gaussien. Nous dérivons les équations de l'algorithme Expectation-Maximisation de maximisation de la vraisemblance, et nous utilisons le critèred'information bayésien (BIC) pour sélectionner le modèle de complexité optimale.Ce modèle ne nécessite aucun réglage manuel arbitraire de paramètres.Les expériences que nous menons sur des données jouets et des bases d'imagesmontrent que la connexité du graphe reproduit correctement celle du nuage depoints. Nous montrons aussi que ce modèle peut être utilisé en tant qu'outil deprétraitement en classification supervisée de caractères manuscrits. Ce travail apour objectif de poser les premières pierres d'un cadre théorique basé sur les modèlesgénératifs statistiques, permettant la construction automatique de modèlestopologiques d'un nuage de points.	Michaël Aupetit, Pierre Gaillard, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1000824	http://editions-rnti.fr/render_pdf.php?p=1000824
Revue des Nouvelles Technologies de l'Information	ASI	2009	Analyse Implicative Séquentielle	La découverte de motifs fréquents dans des séquences (généralement des séquences temporelles d'évènements) est l'une des tâches majeures de la fouille de données. Dans cet article, nous nous intéressons à l'évaluation de la qualité des règles séquentielles. Nous proposons une mesure inédite nommée SII qui évalue la significativité des règles au regard d'un modèle probabiliste. Les simulations numériques montrent que SII que a des caractéristiques uniques en comparaison aux autres mesures de qualité de règles séquentielles.	Julien Blanchard, Fabrice Guillet, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000841	http://editions-rnti.fr/render_pdf.php?p=1000841
Revue des Nouvelles Technologies de l'Information	ASI	2009	Analyse statistique implicative entre variables vectorielles	Nous nous plaçons ici dans le cadre de la méthode d'analyse de données, l'analyse statistique implicative (A.S.I.). A l'instar de ce que nous avons fait pour passer des variables binaires aux variables numériques ou aux variables-intervalles, nous étendons le champ des traitements aux variables à valeurs vectorielles. Nous établissons un indice permettant de mesurer la qualité d'une règle entre variables vectorielles. Nous traitons des exemples portant l'un sur le baccalauréat, l'autre sur l'examen des critères de convergence des économies de l'Union Européenne.	Raphaël Couturier, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000842	http://editions-rnti.fr/render_pdf.php?p=1000842
Revue des Nouvelles Technologies de l'Information	ASI	2009	Analyse statistique implicative et didactique des mathématiques	L'analyse statistique implicative a toujours été considérée comme une méthode d'analyse de données particulièrement heuristique et fructueuse pour la didactique des mathématiques. Nous soutenons ici que ces choix méthodologiques tiennent au fait que les liens implicatifs que révèle l'analyse statistique implicative peuvent être interprétés en termes de règles et de régulations d'actions. Nous étayons ceci par des exemples précis de recherches menées en didactique des mathématiques tout d'abord, puis dans d'autres didactiques, portant sur la reconstruction et la compréhension d'actions tant matérielles que discursives, actions régulées des élèves, actions régulées et régulatrices des enseignants.	Dominique Lahanier-Reuter	http://editions-rnti.fr/render_pdf.php?p1&p=1000860	http://editions-rnti.fr/render_pdf.php?p=1000860
Revue des Nouvelles Technologies de l'Information	ASI	2009	Applications à la sociologie	Cet article reprend l'analyse statistique implicative de la dynamique socioprofessionnelle dans la première moitié du 19e à Genève que nous avons présentée aux rencontres ASI4 (Oris et Ritschard, 2007) et la compare à une analyse supervisée des dissimilarités entre transitions. Les données considérées résultent de l'appariement deux à deux de 6 recensements. Plus précisément, nous considérons le groupe socioprofessionnel (GSP) des individus retenus et son changement entre deux recensements successifs. Nous nous intéressons aux types de transition (stable, devenir actif, cesser l'activité, ...) ainsi qu'aux nouveaux venus (immigrés et naissances) et disparus (émigrés et décédés). L'analyse statistique implicative donne une vision synthétique des liens entre ces dynamiques et les GSP concernés, ainsi qu'avec un certain nombre de variables démographiques et culturelles (sexe, âge, état-civil, religion). Elle met en lumière notamment des polarisations autour de variables clé. L'analyse des dissimilarités permet quant à elle de segmenter la population en groupes homogènes en fonction des caractéristiques démographiques et culturelles. Le recours à l'intensité d'implication pour identifier les transitions typiques des groupes ainsi obtenus s'avère une précieuse aide à l'interprétation et donne les éléments nécessaires à la comparaison avec les résultats du graphe implicatif.	Gilbert Ritschard, Matthias Studer, Michel Oris	http://editions-rnti.fr/render_pdf.php?p1&p=1000873	http://editions-rnti.fr/render_pdf.php?p=1000873
Revue des Nouvelles Technologies de l'Information	ASI	2009	Approche bayésienne 	A partir d'une analyse de manuels présentant un premier enseignement des probabilités en relation avec la statistique, nous esquissons l'espace de travail potentiel existant sur ce thème en classe de Première en France. L'approche statistique implicative nous permet de dégager certaines règles de fonctionnement de cet espace et de décrire une tendance soit très fortement formelle et calculatoire soit assez confuse où joue alors ce que nous appelons l'intrication des signifiés	Pablo Carranza, Alain Kuzniak	http://editions-rnti.fr/render_pdf.php?p1&p=1000862	http://editions-rnti.fr/render_pdf.php?p=1000862
Revue des Nouvelles Technologies de l'Information	ASI	2009	Arbre de décision pour données déséquilibrées : sur la complémentarité de l'intentisé d'implication et de l'entropie décentrée	Cet article porte sur l'induction d'arbres de classification pour des données déséquilibrées, c'est-à-dire lorsque certaines catégories de la variable à prédire sont beaucoup plus rares que d'autres. Plus particulièrement nous nous intéressons à deux aspects: d'une part, à définir des critères de construction de l'arbre qui exploitent efficacement la nature déséquilibrée des données, et d'autre part la pertinence de la conclusion à associer aux feuilles de l'arbre. Nous avons récemment abordé cette problématique sous deux angles indépendants: l'un était axé sur le recours à des entropies décentrées, l'autre s'appuyant sur des mesures d'intensités d'implication issues de l'ASI. Nous nous proposons ici de comparer et d'établir les similarités entre ces deux approches. Une première expérimentation sommaire est présentée.	Gilbert Ritschard, Simon Marcellin, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000843	http://editions-rnti.fr/render_pdf.php?p=1000843
Revue des Nouvelles Technologies de l'Information	ASI	2009	Derrière les réseaux de variables, il y a des individus ... à écouter !	L'analyse statistique implicative permet d'organiser en réseaux orientés un ensemble de variables sur lesquelles s'est projetée une population d'individus (par questionnaire par exemple, sous forme de réponses à des questions fermées, binaires ou modales). Le recueil conjoint d'avis plus ouverts, sous forme de textes explicitant les réponses fournies, va, après identification des individus les plus représentatifs des réseaux mis en évidence par l'analyse, faciliter le travail d'interprétation du chercheur en quête du sens portés par lesdits réseaux.Dans le cadre d'une recherche visant à comprendre comment des enseignants en lycée professionnel se représentent les tensions résultant du croisement, à l'intérieur de ce type d'établissements, de plusieurs logiques (scolaire, économique, politique et administrative), a minima en tension, voire contradictoires, nous avons proposé aux enseignants des lycées professionnels de l'académie de Caen un large questionnaire composé de questions essentiellement fermées, mais pour lesquelles nous avions, conformément au choix présenté plus haut, laissé des espaces ouverts sollicitant commentaires ou explicitations.Nous présentons ci-dessous l'analyse, menée avec le logiciel CHIC, des 257 questionnaires qui nous ont été retournés.	Marc Bailleul, Sylvain Godard	http://editions-rnti.fr/render_pdf.php?p1&p=1000874	http://editions-rnti.fr/render_pdf.php?p=1000874
Revue des Nouvelles Technologies de l'Information	ASI	2009	Étude de représentations d'élèves en éducation physique et sportive	Ce chapitre concerne le traitement par le logiciel CHIC d'un questionnaire proposé à des élèves de première de l'enseignement agricole français. Les questions abordent les représentations des élèves vis-à-vis des activités physiques et sportives et plus particulièrement du volley-ball. Plusieurs réseaux de variables permettant de profiler des types d'élèves apparaissent. L'étude des contributions de deux variables, sexe et genre, ajoutées aux réseaux mis en évidence, permet d'améliorer et de faciliter la sélection des élèves « représentants des réseaux »pour des futurs travaux basés sur des études de cas.	Ingrid Verscheure, Catherine-Marie Chiocca	http://editions-rnti.fr/render_pdf.php?p1&p=1000867	http://editions-rnti.fr/render_pdf.php?p=1000867
Revue des Nouvelles Technologies de l'Information	ASI	2009	Fondements théoriques de l'analyse statistique implicative	La partie 1 vise à exposer en 9 chapitres, la théorie de l'Analyse Statistique implicative (ASI). Nous cherchons à y définir le plus précisément possible les concepts et les théorèmes de cette théorie ainsi que leurs fondements épistémologiques et méthodologiques. Parmi ceux-ci, citons : relation de quasi-implication, coefficient et indice d'implication, de propension, graphe implicatif, classification hiérarchique orientée, etc. De plus, chaque concept est illustré par un exemple.	Régis Gras, Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000836	http://editions-rnti.fr/render_pdf.php?p=1000836
Revue des Nouvelles Technologies de l'Information	ASI	2009	Graphe de règles d'implication statistique pour le raisonnement courant. Comparaison avec les réseaux bayésiens et les treillis de Galois	Les règles d'implication statistique ressemblent aux règles du raisonnement mathématique. Ce qui permet de les utiliser facilement pour raisonner sur les données. Toutefois, le modèle sous-jacent aux règles d'implication statistique n'est pas le modèle de la logique formelle utilisé en mathématique, mais un modèle statistique aboutissant à des relations approximatives. Contrairement au raisonnement mathématique, le raisonnement courant se satisfait de règles approximatives. Mais il a besoin d'un graphe pour savoir quels enchaînements de règles sont possibles car en faisant se succéder des approximations, on finit par arriver à des incohérences. On montrera dans ce chapitre comment fonctionne l'enchaînement de ces règles, notamment à travers la construction du graphe des règles d'implication tel que proposé dans les différentes versions de CHIC et on comparera ce modèle statistique des données à deux autres modèles proches : un modèle algébrique, les treillis de Galois, et un modèle probabiliste, les réseaux bayésiens. Pour permettre des comparaisons aisées, le fonctionnement des trois modèles sera illustré à l'aide d'un même jeu de données médicales librement disponible sur Internet.	Martine Cadot	http://editions-rnti.fr/render_pdf.php?p1&p=1000844	http://editions-rnti.fr/render_pdf.php?p=1000844
Revue des Nouvelles Technologies de l'Information	ASI	2009	Guide d'utilisation des principales fonctionnalités du logiciel CHIC	Dans ce chapitre, nous avons choisi d'utiliser les données d'une recherche en cours, pour présenter et décrire les étapes et procédures à suivre dans un traitement par le logiciel CHIC dans le but, d'une part, d'obtenir des graphes implicatifs et des représentations des hiérarchies cohésitives et, d'autre part, d'en faciliter la lecture grâce aux diverses options et fonctionnalités de ce logiciel. Nous aborderons par étape la description des données recueillies, leur transformation, le choix des variables supplémentaires, la préparation du fichier-texte au format *.CSV requis par CHIC, les traitements pour obtenir un graphe implicatif en particulier avec la notion de conjonction de prémisses et de seuil d'originalité, le traitement pour obtenir une représentation de la hiérarchie cohésitive, complété par les notions de niveaux significatifs, de contribution de groupes optimaux, de typicalité de variables supplémentaires ou d'individus. Dans un dernier nous aborderons par le traitement de variables sur intervalles.	Harrisson Ratsimba-Rajohn	http://editions-rnti.fr/render_pdf.php?p1&p=1000858	http://editions-rnti.fr/render_pdf.php?p=1000858
Revue des Nouvelles Technologies de l'Information	ASI	2009	Historique et fonctionnalités de CHIC	CHIC permet d'utiliser la plupart des méthodes définies dans le cadre de l'ASI (Implication Statistique Implicative). Il a pour objectif de découvrir les implications les plus pertinentes entre les variables d'un ensemble de données. Pour cela, il propose d'organiser les implications sous forme d'une hiérarchie cohésitive (orientée) ou un graphe implicatif. De plus, il permet d'obtenir une hiérarchie des similarités (non orientée) basée sur les ressemblances des variables. Ce papier décrit l'historique, les caractéristiques et l'usage de CHIC.	Raphaël Couturier, Saddo Ag Almouloud	http://editions-rnti.fr/render_pdf.php?p1&p=1000856	http://editions-rnti.fr/render_pdf.php?p=1000856
Revue des Nouvelles Technologies de l'Information	ASI	2009	Iconographie médiévale en histoire de l'art et analyse statistique implicative	Si l'utilisation de la statistique en l'histoire de l'art, et en particulier en iconographie médiévale, est acquise depuis nombre d'années, le recours à l'analyse statistique implicative marque un pas supplémentaire dans l'approche des sujets thématiques. Les statistiques sont une aide à la recherche : elles mettent en évidence des permanences ou des originalités, mais sans pour autant participer à une réflexion sur le sujet. L'ASI apporte un regard nouveau sur la lecture du sujet : elle permet de connecter des éléments sélectionnés au préalable par le chercheur, faisant apparaître d'autres liens internes à l'image et offre une nouvelle manière d'appréhender l'étude thématique en histoire de l'art.	Jean-Claude Régnier, Magali Guénot	http://editions-rnti.fr/render_pdf.php?p1&p=1000877	http://editions-rnti.fr/render_pdf.php?p=1000877
Revue des Nouvelles Technologies de l'Information	ASI	2009	Interprétation de graphes implicatifs : étude clinique auprès d'une chercheure en iconographie médiévale	En Analyse Statistique Implicative, le logiciel CHIC offre des interfaces efficaces pour un usage accessible au non-spécialiste. Ce dernier est confronté à la lecture et l'interprétation des représentations graphiques produites par ce logiciel. Or contrairement à une idée répandue : « un dessin, un graphique parle de lui-même », l'interprétation des graphiques requiert des connaissances minimales sur l'outil même que le chercheur doit alors acquérir. Ce travail poursuit la réflexion développée dans (Acioly-Régnier & Régnier, 2005, 2008), (Régnier & Acioly-Régnier, 2007) sur les obstacles à la conceptualisation liés aux représentations symboliques et met l'accent sur les difficultés rencontrées dans le cadre d'une recherche en iconographie médiévale. Nous cherchons à les expliciter de façon clinique lors de l'interprétation des graphes d'implication. La forme même d'une représentation graphique peut induire des lectures erronées des propriétés des données comme l'ont montré des études sur l'interprétation des signes et leur dépendance des expériences préalables des sujets.	Nadja Maria Acioly-Régnier, Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000866	http://editions-rnti.fr/render_pdf.php?p=1000866
Revue des Nouvelles Technologies de l'Information	ASI	2009	Mesurer l'écart entre une analyse a priori et la contingence en didactique	En didactique des Mathématiques, mais plus généralement en sciences humaines, de nombreuses recherches utilisent des analyses qualitatives pour falsifier expérimentalement des hypothèses formulées a priori, c'est-à-dire en amont de la recherche. Une telle approche méthodologique, appliquée à une enquête, s'avère le plus souvent insuffisante pour analyser toutes les variables en jeu dans des phénomènes contingents d'enseignement/ apprentissage, même si dans certains cas (analyse ponctuelle de protocoles, de vidéos, etc.), elle permet de déceler quelques relations intéressantes. Mais si le nombre de sujets devient trop volumineux, l'analyse qualitative ne réussit plus à extraire toutes les relations existant entre les variables en jeu. Une analyse quantitative sur une base statistique s'imposera et sera complétée par une analyse qualitative, indispensable à une interprétation contextuelle. Cette communication vise à présenter une mesure permettant de confronter statistiquement, l'analyse a priori et la contingence.	Filippo Spagnolo, Régis Gras, Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000839	http://editions-rnti.fr/render_pdf.php?p=1000839
Revue des Nouvelles Technologies de l'Information	ASI	2009	Problème de données manquantes dans un tableau numérique, une application de l'ASI	Une base de données croisant variables et sujets issues d'observations présente souvent des vides dus, par exemple, à des absences de réponse ou à l'impossibilité matérielle de la recueillir. Or, pour en effectuer un traitement, il est essentiel de disposer d'un tableau complet. L'analyse statistique implicative, entre autres méthodes d'analyse de données, à l'oeuvre au moyen du logiciel de traitement C.H.I.C. (Couturier et Gras., 2005), impose que les vides soient comblés. Se pose alors le problème de déterminer quelle valeur la plus vraisemblable attribuer à la variable non observée sur tel sujet ou, de façon symétrique, quelle valeur attribuer à un sujet sur une variable donnée et muette sur lui. Nous présentons ici une méthode qui, au vu du comportement de réponse observé par le sujet sur d'autres variables, intimement liées à la variable muette, permet de pallier la carence locale. Un exemple numérique illustre l'usage de cette méthode sur un tableau incomplet.	Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000840	http://editions-rnti.fr/render_pdf.php?p=1000840
Revue des Nouvelles Technologies de l'Information	ASI	2009	Qualité d'un graphe implicatif : variance implicative	Un graphe pondéré, sans cycle, constitue une des représentations d'un ensemble de règles d'association implicative extraites d'un tableau numérique croisant variables et sujets. Le problème de son homogénéité, de sa cohérence et donc de la pertinence des interprétations de l'expert se pose dès lors qu'en Analyse Statistique Implicative (A.S.I.) il est possible de faire varier le seuil de représentation des règles partielles. Nous présentons ici le concept de variance implicative à l'instar du concept classique de variance afin de qualifier l'homogénéité de la représentation. Elle s'appuie sur une métaphore de répulsion vs consistance implicatives mutuelles entre deux variables binaires à partir de leur différence symétrique.	Régis Gras, Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000838	http://editions-rnti.fr/render_pdf.php?p=1000838
Revue des Nouvelles Technologies de l'Information	ASI	2009	Statistique de rangs et analyse statistique implicative	Nous discutons de l'apport de la méthode d'analyse statistique implicative au sens de R. Gras, à l'étude de la concordance/discordance des rangs accordés par des juges à des objets. Cette dernière est à comprendre au sens de Friedman ou de Kendall. Ici nous comparons une analyse de préférences exprimées par les rangs, avec l'analyse de la propension entre variables modales de J. B. Lagrange. Nous nous affranchissons de l'hypothèse d'absence de lien a priori entre les variables. Nous affectons d'une mesure de qualité des énoncés de la forme : « si l'objet a est rangé par les juges alors, généralement, l'objet b est rangé à un rang meilleur par les mêmes juges », et représentons par un graphe les relations de préférences de l'ensemble des objets rangés. Nous nous limitons aux deux cas des rangements complets et incomplets mais sans ex æquo de q objets par k juges. Le texte présenté ici reprend en partie (Régnier et Gras, 2005)	Régis Gras, Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000837	http://editions-rnti.fr/render_pdf.php?p=1000837
Revue des Nouvelles Technologies de l'Information	ASI	2009	Test de Mac Nemar et analyse statistique implicative	Nous tentons de comparer, dans ce chapitre, les trois approches pour étudier des liens vraisemblables entre deux variables binaires, que sont : l'ASI, le test de Mac Nemar et le test d'indépendance fondé sur la mesure du &#967;2. Pour ce faire, nous faisons un retour sur des données issues de nos travaux passés dans le champ de la didactique des mathématiques.	Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000854	http://editions-rnti.fr/render_pdf.php?p=1000854
Revue des Nouvelles Technologies de l'Information	ASI	2009	Un exemple d'analyse implicative en neuro-psychologie : la comparaison de groupes contrastés	La psychométrie classique utilise des indices de liaisons entre variables qui sont symétriques. Par exemple, la corrélation entre a et b sera la même que celle entre b et a, chacune des variables pouvant prendre indifféremment le statut de variable expliquée ou de variable explicative : il est impossible d'ordonner des corrélations entre variables en séquence implicative. L'analyse implicative des données offre une possibilité d'atteindre cet objectif d'ordonnancement séquentiel des variables. Dans ce chapitre, nous présenterons un exemple de recours à ce type d'analyse pour comparer des groupes contrastés. L'intérêt de ce texte ne se situe pas dans les résultats pour eux-mêmes, mais dans les perspectives ouvertes par la démarche utilisée.	Tarek Bellaj, Daniel Pasquier	http://editions-rnti.fr/render_pdf.php?p1&p=1000871	http://editions-rnti.fr/render_pdf.php?p=1000871
Revue des Nouvelles Technologies de l'Information	ASI	2009	Une étude comparative pour la détection de dépendance multiples	La recherche de dépendances entre variables à partir d'exemples est un problème important en optimisation. De nombreuses méthodes ont été proposées pour résoudre ce problème mais peu d'évaluations à grande échelle ont été effectuées. La plupart de ces méthodes reposent sur des mesures de probabilité conditionnelle. L'ASI proposant un autre point de vue sur les dépendances, il était important de comparer les résultats obtenus grâce à cette approche avec l'une des meilleures méthodes existantes actuellement pour cette tâche : l'heuristique max-min. L'ASI n'étant pas directement utilisable pour traiter ce problème, nous avons conçu une extension à cette mesure spécifiquement adaptée. Nous avons réalisé un grand nombre d'expériences en faisant varier des paramètres tels que le nombre de dépendances, le nombre de variables concernées ou le type de prédiction effectuée pour comparer les deux approches. Les résultats montrent une forte complémentarité des deux méthodes.	Elham Salehi, Jayashree Nyayachavadi, Robin Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000853	http://editions-rnti.fr/render_pdf.php?p=1000853
Revue des Nouvelles Technologies de l'Information	ASI	2009	Une méthode implicative pour l'analyse de données d'expression de gènes	Nous présentons une méthode d'extraction d'associations basée sur l'analyse statistique implicative et la notion de rang.Nous avons adapté le concept d'intensité d'implication à des classements pour découvrir des relations partielles robustes vis à vis du bruit et des variations d'amplitude. Appliquée aux données de puces à ADN, cette méthode met en évidence des relations entre des formes d'expressions particulières de gènes. Ces associations peuvent être révélatrices de mécanismes de corégulation génique et donc contribuer à l'analyse biomédicale. Nous montrons que cette définition de l'intensité d'implication apporte une connaissance plus fine des relations entre les gènes que les méthodes usuelles de corrélation et qu'elle permet notamment de discriminer entre différents phénotypes avec une précision comparable aux techniques de classification les plus abouties dans ce domaine.	Gérard Ramstein	http://editions-rnti.fr/render_pdf.php?p1&p=1000875	http://editions-rnti.fr/render_pdf.php?p=1000875
Revue des Nouvelles Technologies de l'Information	ASI	2009	Utilisation de la statistique implicative pour la construction d'un référentiel de compétences comportementales	Cinq Centres Interinstitutionnels de Bilan de Compétences de la Région Midi-Pyrénées se sont groupés pour créer une méthode d'appui individualisé au profit des personnes engagées dans une démarche de Validation des Acquis de l'Expérience. PerformanSe, éditeur d'outils dévaluation des compétences comportementales, a conçu pour cet usage un système informatisé permettant aux conseillers des CIBC d'être informés sur les comportements - favorables ou défavorables - susceptibles d'être adoptés par chacun des bénéficiaires qu'ils accompagnent dans le cadre d'une VAE.	Dominique Follut, Laurence Diot, Martine Lassere Azema, Serge Baquedano	http://editions-rnti.fr/render_pdf.php?p1&p=1000872	http://editions-rnti.fr/render_pdf.php?p=1000872
Revue des Nouvelles Technologies de l'Information	CAL	2009	Architecture des IHM	Pour les sociétés de services en informatique (SSII), les phases de capture des exigences et de conception utilisent couramment l'ingénierie des modèles pour avoir une représentation de l'architecture de la solution. Malheureusement, l'architecture des interfaces homme-machine (IHM) est très peu étudiée dans ces phases et bien souvent l'architecture des interfaces est laissée à la responsabilité des développeurs qui n'ont généralement pas eu de réflexions avec le client et les utilisateurs finaux. Nous proposons un langage de description de l'architecture des IHM appelé SNI (Schéma Navigationnel des Interfaces) qui cherche à faciliter le dialogue avec les clients et définir l'architecture globale des interfaces. Dans ce cadre, nous présentons un outil qui permet de réaliser ce type de diagrammes et qui permet in fine de générer des maquettes afin d'obtenir un retour des utilisateurs finaux.	Nicolas Ferry, Jean-Bernard Crampes, Salah Sadou	http://editions-rnti.fr/render_pdf.php?p1&p=1000834	http://editions-rnti.fr/render_pdf.php?p=1000834
Revue des Nouvelles Technologies de l'Information	CAL	2009	Correction d'assemblages de composants impliquant des interfaces paramétrées	La démarche de construction du logiciel en partant de l'architecture, nécessite la prise en compte de la correction à différentes étapes afin d'assurer la qualité du logiciel final. Ainsi la correction est une préoccupation qui doit être prise en compte au niveau des composants et de leurs assemblages pour élaborer l'architecture logicielle. Kmelia est un langage et un modèle à composants multi-service où les composants sont abstraits et formels de façon à pouvoir y exprimer des propriétés et les vérifier. Les services de Kmelia peuvent êtres paramétrés par des données et sont dotés d'assertions (pré/post-conditions opérant sur les données). Dans cet article nous nous intéressons à la correction des modèles à composants en couvrant différents aspects: la correction au niveau des services et la correction des assemblages du point de vue des données présentes dans les interfaces des services. Nous présentons les enrichissements du langage de données de Kmelia permettant de traiter la correction au niveau des services et de l'architecture. Nous illustrons l'étude par un exemple.	Pascal André, Christian Attiogbé, Mohamed Messabihi	http://editions-rnti.fr/render_pdf.php?p1&p=1000835	http://editions-rnti.fr/render_pdf.php?p=1000835
Revue des Nouvelles Technologies de l'Information	CAL	2009	EnTiMid: un middleware au service de la maison	Face aux enjeux de société liés au vieillissement de la population, la domotique est souvent citée comme une solution pour favoriser le maintien à domicile des personnes âgées et la coordination des acteurs autour de cette problématique. Cet article liste les exigences auxquelles doit faire face une plate-forme domotique. Il montre que ces exigences rendent inopérantes les solutions commerciales existantes, d'autant que ces dernières cherchent bien souvent à imposer sur le marché une solution propriétaire qui ne peut prétendre à la richesse fonctionnelle nécessaire. Il propose, en conséquence, un ensemble de propriétés souhaitables pour un intergiciel orienté domotique, permettant le déploiement d'une solution à l'échelle d'une agglomération. L'architecture d'un intergiciel construit au dessus d'une plateforme OSGI, et respectant ces propriétés est ensuite présentée. Enfin, une expérience de déploiement de cet intergiciel dans le cadre du laboratoire domotique de l'université de Rennes 1 met en évidence la pertinence de la solution proposée.	Grégory Nain, Olivier Barais, Régis Fleurquin, Jean-Marc Jézéquel	http://editions-rnti.fr/render_pdf.php?p1&p=1000846	http://editions-rnti.fr/render_pdf.php?p=1000846
Revue des Nouvelles Technologies de l'Information	CAL	2009	Exploitation des techniques de virtualisation pour l'administration autonome d'infrastructures logicielles réparties	Les grappes de machines (ou clusters) sont souvent utilisées pour gérer une infrastructure de serveurs dupliqués, afin de répondre à des charges importantes et surtout variables. Un exemple est la gestion d'une architecture J2EE sur un cluster, implantant un serveur Web dynamique. Dans des travaux récents, nous avons exploré la conception d'un système autonome permettant d'ajuster dynamiquement le degré de duplication des serveurs en fonction de la charge. Cependant, la modification de l'infrastructure logicielle nécessite l'arrêt et/ou le redémarrage de serveurs, provoquant une perte de l'état d'exécution. Une alternative est d'exploiter les techniques de virtualisation de systèmes d'exploitation, permettant la migration d'un système sans perte de données. Les serveurs s'exécutant sur des systèmes virtualisés peuvent ainsi être répartis sur un ensemble de noeuds physiques variable en fonction de la charge et sans perte d'état. Nous décrivons l'implantation de cette approche et son évaluation par rapport à la précédente.	Alain Tchana, Suzy Temate, Benoit Combemale, Laurent Broto, Daniel Hagimont	http://editions-rnti.fr/render_pdf.php?p1&p=1000848	http://editions-rnti.fr/render_pdf.php?p=1000848
Revue des Nouvelles Technologies de l'Information	CAL	2009	Extension d'ABAReL par les Propriétés d'Exécution	Dans un travail antérieur, nous avons proposé une annexe comportementale ABAReL (AADL Behavioral Annex based on generalized Rewriting Logic), basée sur la logique de réécriture révisée, décrivant le composant "thread AADL" tout en préservant sa syntaxe initiale. Un modèle mathématique représenté par une théorie de réécriture décrivant son comportement simplifié, a été alors validé et implémenté sous l'environnement Maude. L'objectif de cet article est de raffiner l'annexe ABAReL et l'enrichir par d'autres constructions formelles de la logique de réécriture, à travers une extension de son langage Maude qui est RT-Maude (Real Time Maude). Cet enrichissement permettra de décrire les modes et les propriétés relatives au temps d'exécution théorique (la période, la politique d'expédition, etc.), déclarées dans le composant thread et leur prise en compte lors de son exécution. L'approche de formalisation proposée offre un cadre sémantique général, approprié pour raisonner sur le comportement de ces unités d'exécution concurrentes et pour pouvoir ensuite les analyser en tirant profit des outils existant autour de Maude.	Malika Benammar, Faiza Belala, Kamel Barkaoui, Nadira Benlahrache	http://editions-rnti.fr/render_pdf.php?p1&p=1000845	http://editions-rnti.fr/render_pdf.php?p=1000845
Revue des Nouvelles Technologies de l'Information	CAL	2009	Une architecture de composants répartis adaptables	Plusieurs travaux récents proposent des solutions ou des frameworks dédiés au développement d'applications adaptables qui peuvent ainsi dynamiquement changer leur comportement pendant l'exécution afin de s'adapter au contexte d'exécution courant. Cependant, avec ces approches, les tâches à la charge des développeurs sont encore complexes. Ces tâches incluent la définition des variantes et la spécification des actions d'adaptation, qui dans le contexte des systèmes répartis, incluent des contraintes liées aux parties distribuées. Dans cet article, nous présentons une approche de développement d'applications réparties adaptables permettant la génération correcte des variantes d'une application et des actions d'adaptation à exécuter en vue de faciliter la tâche des développeurs	An Phung-Khac, Jean-Marie Gilliot, Maria-Teresa Segarra	http://editions-rnti.fr/render_pdf.php?p1&p=1000847	http://editions-rnti.fr/render_pdf.php?p=1000847
Revue des Nouvelles Technologies de l'Information	CAL	2009	Vers une architecture MVSOA pour la mise en oeuvre des composants multivue	L'objectif de cet article est de proposer un processus de mise oeuvre des systèmes à base de composants multivue selon une architecture orientée services multivue (MVSOA : MultiView SOA). Cette architecture repose sur une extension du standard WSDL appelée MVWSDL (MultiView Web Service Description Language) et sur un ensemble d'adaptateurs pour la publication et la sélection des services multivue. Dans cet article, nous présentons en premier lieu un modèle métier à base de composants multivue. Ensuite, et via des règles de transformations, ce dernier sera transformé vers une plateforme services web selon l'architecture MVSOA. Pour ce faire, nous avons défini deux transformations : La première consiste en la définition des règles de transformation pour générer la description d'un composant multivue selon le format MVWSDL. La deuxième transformation cible la plateforme J2EE en se basant sur le méta-modèle J2EE pour générer l'implémentation du composant correspondant.	Bouchra El Asri, Adil Kenzi, Mahmoud Nassar, Abdelaziz Kriouile	http://editions-rnti.fr/render_pdf.php?p1&p=1000833	http://editions-rnti.fr/render_pdf.php?p=1000833
Revue des Nouvelles Technologies de l'Information	EDA	2009	Algorithmes pour la sélection des vues à matérialiser avec garantie de performance	Nous considérons dans cet article le problème du choix des vues à matérialiser dans le contexte des cubes de données. Contrairement à la plupart des autres travaux, la contrainte que l'on prend en compte ici est relative à la performance des requêtes non pas l'espace mémoire disponible. Nous montrons l'avantage de notre approche avec des arguments théoriques et nous le confirmons par des expériences menées sur des jeux de données différents.	Nicolas Hanusse, Sofian Maabout, Radu Tofan	http://editions-rnti.fr/render_pdf.php?p1&p=1000865	http://editions-rnti.fr/render_pdf.php?p=1000865
Revue des Nouvelles Technologies de l'Information	EDA	2009	Approche de modélisation multidimensionnelle des données complexes : Application aux données médicales	La vocation d'un entrepôt de données est l'analyse de données pourl'aide à la décision dans les entreprises. La modélisation multidimensionnelleest la base des entrepôts de données et de l'analyse en ligne (OLAP). Cestechniques sont efficaces pour traiter les données simples numériques, maiselles ne sont pas adaptées aux données variées et hétérogènes provenant dedifférentes sources, appelés communément données complexes. Dans cetarticle, nous abordons le problème de la modélisation multidimensionnelle desdonnées complexes à travers le cas des données médicales du projet MAP(Médecine d'Anticipation Personnalisée). Nous proposons un métamodèlemultidimensionnel étendu pour les données médicales en généralisant lemodèle cardiovasculaire du projet MAP. Enfin, nous avons spécifié et réaliséun outil d'aide à la conception d'entrepôt de données médicales.	Sid Ahmed Djallal Midouri, Jérôme Darmont, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000870	http://editions-rnti.fr/render_pdf.php?p=1000870
Revue des Nouvelles Technologies de l'Information	EDA	2009	Cloud Computing : un nouveau défi pour les entrepôts de données		N. Raspal, R. Delacour	http://editions-rnti.fr/render_pdf.php?p1&p=1000851	http://editions-rnti.fr/render_pdf.php?p=1000851
Revue des Nouvelles Technologies de l'Information	EDA	2009	Infrastructures pour les entrepôts de données		Cedrine Madera	http://editions-rnti.fr/render_pdf.php?p1&p=1000850	http://editions-rnti.fr/render_pdf.php?p=1000850
Revue des Nouvelles Technologies de l'Information	EDA	2009	Les Entrepôts de Données Actifs : Modélisation des règles d'analyse	L'entrepôt de données actif est un système permettant d'intégrer dansune même base cible des données et des traitements. Le mécanisme qui rendune telle approche active se base sur les règles d'analyse. Dans ce papier, nousproposons une formalisation simplifiée des règles d'analyse, appelées égalementrègles actives, suivie d'une modélisation permettant d'assurer le stockage physiquedes ces règles dans l'entrepôt	Sonia Bouattour, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000869	http://editions-rnti.fr/render_pdf.php?p=1000869
Revue des Nouvelles Technologies de l'Information	EDA	2009	OLAP query suggestion and discovery driven analysis	Interactive analysis of datacube, in which a user navigates a cubewith a sequence of queries to find and understand unexpected data, is often tedious.To better support this process, we propose in this paper to connect twotechniques proposed earlier in this domain. These techniques are, on the onehand, discovery driven analysis, that guides the user towards regions of the cubethey will find of interest, and on the other hand, query recommendation, thattakes advantage of what the other users did during former analyses. Benefitingfrom these techniques we propose a framework for recommending OLAPqueries to the user by taking into account what previous users found of interestand the explanation they worked out.	Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000857	http://editions-rnti.fr/render_pdf.php?p=1000857
Revue des Nouvelles Technologies de l'Information	EDA	2009	Optimisation heuristique et génétique de visualisations 2D et 3D dans OLAP : premiers résultats	Nous étudions dans cet article comment réorganiser les données dansune analyse OLAP afin d'améliorer la visualisation présentée aux décideurs.Après un état de l'art sur la problématique de la réorganisation de matrices et decubes OLAP, nous présentons deux méthodes. La première est une méthode heuristiquepermettant de replacer les modalités des dimensions en maximisant lasimilarité entre deux modalités voisines. La deuxième méthode est un algorithmegénétique permettant de faire évoluer le cube de données afin de maximiser uncritère d'évaluation de la visualisation. Nous comparons ces deux méthodes avecd'autres approches sur des bases réelles afin d'optimiser la visualisation de matrices(2D) et de cubes (3D) et en présentant les visualisations obtenues. Nousdétaillons l'intégration de ces méthodes dans un serveur OLAP ainsi qu'une premièreébauche de visualisation obtenue en réalité virtuelle.	Florian Sureau, Fatma Bouali, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000861	http://editions-rnti.fr/render_pdf.php?p=1000861
Revue des Nouvelles Technologies de l'Information	EDA	2009	Personnalisation dans les entrepôts de données : bilan et perspectives	Nous étudions dans cet article les enjeux et les opportunités relevantde la prise en compte des utilisateurs au sein des entrepôts de données. Pour cefaire, nous présentons tout d'abord un panorama des travaux sur la personnalisationdans les domaines connexes des bases de données et de la recherched'information. Nous présentons ensuite les quelques travaux qui émergent dansle domaine des entrepôts de données et menons une étude comparative selondifférents angles d'approche. Ceci nous permet de faire émerger des perspectivesde recherche sur la personnalisation dans les entrepôts de données.	Fadila Bentayeb, Omar Boussaid, Cécile Favre, Franck Ravat, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000852	http://editions-rnti.fr/render_pdf.php?p=1000852
Revue des Nouvelles Technologies de l'Information	EDA	2009	Recommandations Personnalisées de Requêtes MDX	Une session d'analyse OLAP peut être définie comme une sessioninteractive durant laquelle un utilisateur lance des requêtes pour naviguer dansun cube. Très souvent, choisir quelle partie du cube va être naviguée par la suite,et, de ce fait, concevoir la prochaine requête, est une tâche difficile. Dans cetarticle, nous proposons d'utiliser ce que tous les utilisateurs du système OLAPont fait pendant leurs précédentes explorations du cube afin de recommander desrequêtes MDX à l'utilisateur	Elsa Negre	http://editions-rnti.fr/render_pdf.php?p1&p=1000855	http://editions-rnti.fr/render_pdf.php?p=1000855
Revue des Nouvelles Technologies de l'Information	EDA	2009	Règles graduelles et cubes de données : quand les blocs s'empilent !	Le couplage des méthodes de fouille de données et d'entrepôts de données permetd'extraire des informations pertinentes à partir de cubes de données. Dans ce contexte,de nombreuses approches ont été proposées, permettant par exemple d'extraire des règlesd'association ou des motifs séquentiels. Cependant, il n'existe pas de méthodes permettantd'extraire des règles graduelles. Dans cet article nous nous intéressons donc à la découvertede telles règles corrélant des variations sur un ensemble de dimensions ordonnées avec desvariations sur la mesure du cube. Nous découvrons par exemple des règles du type "Plus laville est de taille importante et la catégorie socio-professionnelle de catégorie supérieure,plus le nombre de produits vendus est grand". Afin de découvrir ces règles de manièreefficace et en prenant en compte les grandes tendances issues des cubes de données, nousnous appuyons sur des travaux précédents permettant d'extraire des blocs de données homogènes.	Lisa Di-Jorio, Yeow Wei Choong, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000859	http://editions-rnti.fr/render_pdf.php?p=1000859
Revue des Nouvelles Technologies de l'Information	EDA	2009	SISRO(M2C) Un Outil de Modélisation Conceptuelle à base Ontologique d'un entrepôt de données	Concevoir un entrepôt de données passe souvent par trois niveaux de modélisation: conceptuel, logique et physique. Contrairement aux deux derniers niveaux, le niveau conceptuel n'a pas eu le même intérêt de la part de la communauté des entrepôts de données, et peu de méthodes de conception d'un entrepôt ont été proposées. Délaisser cette modélisation a fortement contribué à l'échec de plusieurs projets d'entreposage. Dans cet article, nous proposons une méthode, appelée SISROM2C et un outil de conception multidimensionnelle d'un entrepôt de données qui suppose l'existence d'une ontologie de domaine décrite en OWL (Web Ontology Language) couvrant toutes les sources participant dans le processus de construction de l'entrepôt de données. Elle confronte les besoins et les sources à priori, au niveau ontologique, et offre plus de présence des concepteurs dans la génération du schéma conceptuel.	Selma Khouri, Ladjel Bellatreche, Chimène Fankam	http://editions-rnti.fr/render_pdf.php?p1&p=1000868	http://editions-rnti.fr/render_pdf.php?p=1000868
Revue des Nouvelles Technologies de l'Information	EDA	2009	Une Démarche Conjointe de Fragmentation et de Placement dans le Cadre des Entrepôts de Données Parallèles	Traditionnellement, concevoir un entrepôt de données parallèle consisted'abord à partitionner son schéma ensuite allouer les fragments générés sur lesnoeuds d'une machine parallèle. L'inconvénient majeur d'une telle approche estson ignorance de l'interdépendance entre les processus de fragmentation et d'allocation.Une des entrées du problème d'allocation est l'ensemble de fragmentsgénérés par la fragmentation. Notons que les deux processus cherchent à optimiserle même ensemble de requêtes. Dans ce papier, nous proposons une approchede conception d'un entrepôt de données relationnel parallèle selon une architecturedistribuée (shared nothing) intégrant les processus de fragmentation etd'allocation. Ensuite, une méthode de répartition de charges sur les noeuds de lamachine parallèle est proposée. Finalement, une validation de nos propositionsen utilisant le banc d'essai APB-1 release II est présentée.	Ladjel Bellatreche, Soumia Benkrid	http://editions-rnti.fr/render_pdf.php?p1&p=1000864	http://editions-rnti.fr/render_pdf.php?p=1000864
Revue des Nouvelles Technologies de l'Information	EDA	2009	Utilisation des règles d'association pour la prédiction de valeurs manquantes	Le traitement des valeurs manquantes est une problématique importantedans le domaine des entrepôts de données. Plusieurs solutions ont été proposéespour la prédiction de valeurs manquantes, présentant les caractéristiquessuivantes : (i) la prédiction traite soit des valeurs continues soit des valeurs discrètes,et (ii) la prédiction est approximative (soit elle est associée à une probabilitésoit elle concerne un ensemble de valeurs). Récemment, une méthodede prédiction permettant de traiter indépendamment les cas continu et discret aété proposée, en se basant sur les règles d'association. Cette méthode permet deprédire, avec une confiance toujours égale à 1, soit un ensemble de valeurs dansle cas discret, soit un intervalle de valeurs dans le cas continu.Dans cet article, nous reprenons cette approche basée sur l'extraction de règlesd'association et nous montrons comment générer des règles de prédictionsportant sur une unique valeur et dont la confiance est toujours égale à 1. Afind'obtenir de telles règles, notre méthode suppose que l'on dispose d'une hiérarchiedécrivant des concepts généralisant les valeurs qui peuvent être prédites.	Tao-Yuan Jen, Dominique Laurent, Gorgoumack Sambe	http://editions-rnti.fr/render_pdf.php?p1&p=1000863	http://editions-rnti.fr/render_pdf.php?p=1000863
Revue des Nouvelles Technologies de l'Information	EDA	2009	Warehousing TheWorld: Challenges From New Types of Data		Torben Bach Pedersen	http://editions-rnti.fr/render_pdf.php?p1&p=1000849	http://editions-rnti.fr/render_pdf.php?p=1000849
Revue des Nouvelles Technologies de l'Information	EGC	2009	A Contextualization Service for a Personalized Access Model	Personalization paradigm aims at providing users with the most rel-evant content and services according to many factors such as interest center orlocation at the querying time. All this knowledge and requirements are orga-nized into user profiles and contexts. A user profile encompasses metadata de-scribing the user whereas a context groups information about the environmentof interaction between the user and the system. An interesting problem is there-fore to identify which part of the profile is significant in a given context. Thispaper proposes a contextualization service which allows defining relationshipsbetween user preferences and contexts. Further, we propose an approach forthe automatic discovery of these mappings by analyzing user behavior extractedfrom log files.	Sofiane Abbar, Mokrane Bouzeghoub, Dimitre Kostadinov, Stéphane Lopes	http://editions-rnti.fr/render_pdf.php?p1&p=1000771	http://editions-rnti.fr/render_pdf.php?p=1000771
Revue des Nouvelles Technologies de l'Information	EGC	2009	Accompagner au début du 21ème siècle les organisations dans la mise en place d'une gestion des connaissances : retour d'expérience	Cet article présente succinctement le retour d'expérience d'Ardansdans l'implantation de systèmes de gestion de connaissances dans des organisationstrès variées au début de ce 21ème siècle.	Alain Berger, Jean-Pierre Cotton, Pierre Mariot	http://editions-rnti.fr/render_pdf.php?p1&p=1000811	http://editions-rnti.fr/render_pdf.php?p=1000811
Revue des Nouvelles Technologies de l'Information	EGC	2009	Acquisition de la théorie ontologique d'un système d'extraction d'information	La conception de systèmes d'Extraction d'Information (EI) destinésà extraire les réseaux d'interactions géniques décrits dans la littérature scientifiqueest un enjeu important. De tels systèmes nécessitent des représentationssophistiquées, s'appuyant sur des ontologies, afin de définir différentes relationsbiologiques, ainsi que les dépendances récursives qu'elles présentent entre elles.Cependant, l'acquisition de ces dépendances n'est pas possible avec les techniquesd'apprentissage automatique actuellement employées en EI, car ces dernièresne gèrent pas la récursivité. Afin de palier ces limitations, nous présentonsune application à l'EI de la Programmation Logique Inductive, en mode multipredicats.Nos expérimentations, effectuées sur un corpus bactérien, conduisentà un rappel global de 67.7% pour une précision de 75.5%.	Alain-Pierre Manine	http://editions-rnti.fr/render_pdf.php?p1&p=1000788	http://editions-rnti.fr/render_pdf.php?p=1000788
Revue des Nouvelles Technologies de l'Information	EGC	2009	Acquisition, annotation et exploration interactive d'images stéréoscopiques en réalité virtuelle : application en dermatologie	Nous présentons dans cet article le système Skin3D qui implémentetous les composants matériels et logiciels nécessaires pour extraire desinformations dans des images 3D de peau. Il s'agit à la fois du matérield'éclairage et d'acquisition à base d'appareils photographiquesstéréoscopiques, d'une méthode de calibration de caméras utilisant lesalgorithmes génétiques, de matériel de réalité virtuelle pour restituer lesimages en stéréoscopie et interagir avec elles, et enfin d'un ensemble defonctionnalités interactives pour annoter les images, partager ces annotations etconstruire un hypermédia 3D. Nous présentons une étude comparativeconcernant la calibration et une application réelle de Skin3D sur des images devisages.	Mohammed Haouach, Karim Benzeroual, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000774	http://editions-rnti.fr/render_pdf.php?p=1000774
Revue des Nouvelles Technologies de l'Information	EGC	2009	Aggregative and Neighboring Approximations to Query Semi-Structured Documents	Structures heterogeneity in Web resources is a constant concern inelement retrieval (i.e. tag retrieval in semi-structured documents). In this paperwe present the SHIRI 1 querying approach which allows to reach more or lessstructured document parts without an a priori knowledge on their structuring.	Yassine Mrabet, Nathalie Pernelle, Nacéra Bennacer, Mouhamadou Thiam	http://editions-rnti.fr/render_pdf.php?p1&p=1000808	http://editions-rnti.fr/render_pdf.php?p=1000808
Revue des Nouvelles Technologies de l'Information	EGC	2009	An approach for handling risk and uncertainty in multiarmed bandit problems	An approach is presented to deal with risk in multiarmed bandit prob-lems. Specifically, the well known exploration-exploitation dilemma is solvedfrom the point of view of maximizing an utility function which measures thedecision maker's attitude towards risk and uncertain outcomes. A link withthe preference theory is thus established. Simulations results are provided forin order to support the main ideas and to compare the approach with existingmethods, with emphasis on the short term (small sample size) behavior of theproposed method.	Fabrice Clérot, Stefano Perabò	http://editions-rnti.fr/render_pdf.php?p1&p=1000744	http://editions-rnti.fr/render_pdf.php?p=1000744
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse de dissimilarités par arbre d'induction	Dans cet article1, nous considérons des objets pour lesquels nous dis-posons d'une matrice des dissimilarités et nous nous intéressons à leurs liensavec des attributs. Nous nous centrons sur l'analyse de séquences d'états pourlesquelles les dissimilarités sont données par la distance d'édition. Toutefois, lesméthodes développées peuvent être étendues à tout type d'objets et de mesurede dissimilarités. Nous présentons dans un premier temps une généralisation del'analyse de variance (ANOVA) pour évaluer le lien entre des objets non mesu-rables (p. ex. des séquences) avec une variable catégorielle. La clef de l'approcheest d'exprimer la variabilité en termes des seules dissimilarités ce qui nous per-met d'identifier les facteurs qui réduisent le plus la variabilité. Nous présentonsun test statistique général qui peut en être déduit et introduisons une méthodeoriginale de visualisation des résultats pour les séquences d'états. Nous présen-tons ensuite une généralisation de cette analyse au cas de facteurs multiples et endiscutons les apports et les limites, notamment en terme d'interprétation. Fina-lement, nous introduisons une nouvelle méthode de type arbre d'induction quiutilise le test précédent comme critère d'éclatement. La portée des méthodesprésentées est illustrée à l'aide d'une analyse des facteurs discriminant le plusles trajectoires occupationnelles .	Gilbert Ritschard, Matthias Studer, Nicolas S. Müller, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1000733	http://editions-rnti.fr/render_pdf.php?p=1000733
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse de données pour la construction de modèles de procédures neurochirurgicales	Dans cet article, nous appliquons une méthode d'analyse sur desdescriptions de procédures de neurochirurgie dans le but d'en améliorer lacompréhension. La base de données XML utilisée dans cette étude estconstituée de la description de 157 chirurgies de tumeurs. Trois cent vingtdeux variables ont été identifiées et décomposées en variables prédictives(connues avant l'opération) et variables à prédire (décrivant des gesteschirurgicaux). Une analyse factorielle des correspondances (AFC) a étéréalisée sur les variables prédictives, ainsi qu'un arbre de décision basé sur undendrogramme préalablement établi. Six classes principales de variablesprédictives ont ainsi été identifiées. Puis, pour chacune de ces classes, uneanalyse AFC a été réalisée sur les variables à prédire, ainsi qu'un arbre dedécision. Bien que le nombre de cas et le choix des variables constituent unelimite à cette étude, nous avons réussi à prédire certaines caractéristiques liéesaux procédures en partant de données prédictives.	Brivael Trelhu, Florent Lalys, Laurent Riffaud, Xavier Morandi, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1000789	http://editions-rnti.fr/render_pdf.php?p=1000789
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse et application de modèles de régression pour optimiser le retour sur investissement d'opérations commerciales	Les activités de négoce de matériaux sont un marché extrêmementcompétitif. Pour les acteurs de ce marché, les méthodes de fouille de donnéespeuvent s'avérer intéressantes en permettant de dégager des gains de rentabilitéimportants. Dans cet article, nous présenterons le retour d'expérience du projetde fouille de données mené chez VM Matériaux pour améliorer le retour surinvestissement d'opérations commerciales. La synergie des informaticiens, dumarketing et des experts métier a permis d'améliorer l'extraction des connais-sances à partir des données de manière à aboutir à la connaissance actionnable laplus pertinente possible et ainsi aider les experts métier à prendre des décisions.	Thomas Piton, Julien Blanchard, Henri Briand, Laurent Tessier, Gaëtan Blain	http://editions-rnti.fr/render_pdf.php?p1&p=1000735	http://editions-rnti.fr/render_pdf.php?p=1000735
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse multigraduelle OLAP	Les systèmes décisionnels reposent sur des bases de données multidimensionnellesqui offrent un cadre adéquat aux analyses OLAP. L'articleprésente un nouvel opérateur OLAP nommé « BLEND » rendant possible desanalyses multigraduelles. Il s'agit de transformer la structuration multidimensionnellelors des interrogations pour analyser les mesures selon des niveauxde granularité différents recombinées comme un même paramètre. Nous menonsune étude des combinaisons valides de l'opération dans le contexte deshiérarchies strictes. Enfin, une première série d'expérimentations implantel'opération dans le contexte R-OLAP en montrant le faible coût de l'opération.	Gilles Hubert, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000768	http://editions-rnti.fr/render_pdf.php?p=1000768
Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse sémantique spatio-temporelle pour les ontologies OWL-DL	L'analyse sémantique est un nouveau paradigmed'interrogation du Web Sémantique qui a pour objectif d'identifier lesassociations sémantiques reliant des individus décrits dans desontologies OWL-DL. Pour déduire davantage d'associationssémantiques et augmenter la précision de l'analyse, l'informationspatio-temporelle attachée aux ressources doit être prise en compte. Aces fins - et pour combler l'absence actuelle de raisonneurs spatiotemporeldéfini pour les ontologies RDF(S) et OWL-, nous proposonsle système de représentation et d'interrogation d'ontologies spatiotemporellesONTOAST, compatible avec le langage OWL-DL. Nousprésentons les principes de base de l'algorithme de découverted'associations sémantiques entre individus intégré dans ONTOAST.Cet algorithme utilise deux contextes, l'un spatial et l'autre temporelqui permettent d'affiner la recherche. Nous décrivons enfin l'approchemise en oeuvre pour la déduction de connexions spatiales entreindividus.	Alina Dia Miron, Jérôme Gensel, Marlène Villanova-Oliver	http://editions-rnti.fr/render_pdf.php?p1&p=1000784	http://editions-rnti.fr/render_pdf.php?p=1000784
Revue des Nouvelles Technologies de l'Information	EGC	2009	Assessing the uncertainty in knn Data Fusion		Tomàs Aluja-Banet, Josep Daunis-i-Estadella, Enric Ripoll	http://editions-rnti.fr/render_pdf.php?p1&p=1000794	http://editions-rnti.fr/render_pdf.php?p=1000794
Revue des Nouvelles Technologies de l'Information	EGC	2009	Binary Sequences and Association Graphs for Fast Detection of Sequential Patterns	We develop an efficient algorithm for detecting frequent patterns thatoccur in sequence databases under certain constraints. By combining the useof bit vector representations of sequence databases with association graphs weachieve superior time and low memory usage based on a considerable reductionof the number of candidate patterns.	Dan A. Simovici, Selim Mimaroglu	http://editions-rnti.fr/render_pdf.php?p1&p=1000781	http://editions-rnti.fr/render_pdf.php?p=1000781
Revue des Nouvelles Technologies de l'Information	EGC	2009	Caractérisation automatique des classes découvertes en classification non supervisée	Dans cet article, nous proposons une nouvelle approche de classifi- cation et de pondération des variables durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage de ces cartes topologiques est combiné à un mécanisme d'esti- mation de pertinences des différentes variables sous forme de poids d'influence sur la qualité de la classification. Nous proposons deux types de pondérations adaptatives : une pondération des observations et une pondération des distances entre observations. L'apprentissage simultané des pondérations et des prototypes utilisés pour la partition des observations permet d'obtenir une classification op- timisée des données. Un test statistique est ensuite utilisé sur ces pondérations pour élaguer les variables non pertinentes. Ce processus de sélection de variables permet enfin, grâce à la localité des pondérations, d'exhiber un sous ensemble de variables propre à chaque groupe (cluster) offrant ainsi sa caractérisation. L'approche proposée a été validé sur plusieurs bases de données et les résultats expérimentaux ont montré des performances très prometteuses.	Nistor Grozavu, Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000737	http://editions-rnti.fr/render_pdf.php?p=1000737
Revue des Nouvelles Technologies de l'Information	EGC	2009	Ciblage des règles d'association intéressantes guidé par les connaissances du décideur	L'usage du modèle des règles d'association en fouille de données estlimité par la quantité prohibitive de règles qu'il fournit et nécessite la mise enplace d'une phase de post-traitement efficace afin de cibler les règles les plusutiles. Cet article propose une nouvelle approche intégrant explicitement lesconnaissances du décideur afin de filtrer et cibler les règles intéressantes.	Claudia Marinica, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000805	http://editions-rnti.fr/render_pdf.php?p=1000805
Revue des Nouvelles Technologies de l'Information	EGC	2009	CISNA un système hybride LD+Règles pour gérer des connaissances		Alexis Bultey, François Rousselot, Cecilia Zanni, Denis Cavallucci	http://editions-rnti.fr/render_pdf.php?p1&p=1000821	http://editions-rnti.fr/render_pdf.php?p=1000821
Revue des Nouvelles Technologies de l'Information	EGC	2009	Classification des Images de Télédétection avec ENVI FX	D'importants volumes d'images satellites et aériennes de tout type(panchromatiques, multispectrales, hyperspectrales) sont généréesquotidiennement, et leur classification par des méthodes semi-automatiquesdevient nécessaire. Le logiciel ENVI Feature eXtractionTM (ENVI FXTM) sebase sur une approche « objet » -par opposition à une approche pixelsclassique- et sur des algorithmes innovants, pour la segmentation et laclassification des images de télédétection avec un haut niveau de précision.	Franck Le Gall, Damien Barache, Ahmed Belaidi	http://editions-rnti.fr/render_pdf.php?p1&p=1000813	http://editions-rnti.fr/render_pdf.php?p=1000813
Revue des Nouvelles Technologies de l'Information	EGC	2009	Collaborative Outlier Mining for Intrusion Detection	Intrusion detection is an important topic dealing with security of in-formation systems. Most successful Intrusion Detection Systems (IDS) rely onsignature detection and need to update their signature as fast as new attacks areemerging. On the other hand, anomaly detection may be utilized for this purpose,but it suffers from a high number of false alarms. Actually, any behaviour whichis significantly different from the usual ones will be considered as dangerousby an anomaly based IDS. Therefore, isolating true intrusions in a set of alarmsis a very challenging task for anomaly based intrusion detection. In this paper,we consider to add a new feature to such isolated behaviours before they can beconsidered as malicious. This feature is based on their possible repetition fromone information system to another. We propose a new outlier mining principleand validate it through a set of experiments.	Goverdhan Singh, Florent Masseglia, Céline Fiot, Alice Marascu, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000776	http://editions-rnti.fr/render_pdf.php?p=1000776
Revue des Nouvelles Technologies de l'Information	EGC	2009	Comment valider automatiquement des relations syntaxiques induites	Nous présentons dans cet article des approches visant à valider desrelations syntaxiques induites de type Verbe-Objet. Ainsi, nous proposons d'u-tiliser dans un premier temps une approche s'appuyant sur des vecteurs séman-tiques déterminés à l'aide d'un thésaurus. La seconde approche emploie unevalidation Web. Nous effectuons des requêtes sur un moteur de recherche asso-ciées à des mesures statistiques afin de déterminer la pertinence d'une relationsyntaxique. Nous proposons enfin de combiner ces deux méthodes. La qualitéde nos approches de validation de relations syntaxiques a été évaluée en utilisantdes courbes ROC.	Nicolas Béchet, Mathieu Roche, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000749	http://editions-rnti.fr/render_pdf.php?p=1000749
Revue des Nouvelles Technologies de l'Information	EGC	2009	Comparaison de distances et noyaux classiques par degré d'équivalence des ordres induits	Le choix d'une mesure pour comparer les données est au coeur destâches de recherche d'information et d'apprentissage automatique. Nous considéronsici ce problème dans le cas où seul l'ordre induit par la mesure importe,et non les valeurs numériques qu'elle fournit : cette situation est caractéristiquedes moteurs de recherche de documents par exemple. Nous étudions dans cecadre les mesures de comparaison classiques pour données numériques, tellesque les distances et les noyaux les plus courants. Nous identifions les mesureséquivalentes, qui induisent toujours le même ordre ; pour les mesures non équivalentes,nous quantifions leur désaccord par des degrés d'équivalence basés surle coefficient de Kendall généralisé. Nous étudions les équivalences et quasiéquivalencesà la fois sur les plans théorique et expérimental.	Maria Rifqi, Marcin Detyniecki, Marie-Jeanne Lesot	http://editions-rnti.fr/render_pdf.php?p1&p=1000738	http://editions-rnti.fr/render_pdf.php?p=1000738
Revue des Nouvelles Technologies de l'Information	EGC	2009	Constraint Programming for Data Mining		Luc De Raedt	http://editions-rnti.fr/render_pdf.php?p1&p=1000731	http://editions-rnti.fr/render_pdf.php?p=1000731
Revue des Nouvelles Technologies de l'Information	EGC	2009	Construction de descripteurs pour classer à partir d'exemples bruités	En classification supervisée, la présence de bruit sur les valeurs desdescripteurs peut avoir des effets désastreux sur la performance des classifieurset donc sur la pertinence des décisions prises au moyen de ces modèles. Traiterce problème lorsque le bruit affecte un attribut classe a été très étudié. Il estplus rare de s'intéresser au bruit sur les autres attributs. C'est notre contextede travail et nous proposons la construction de nouveaux descripteurs robusteslorsque ceux des exemples originaux sont bruités. Les résultats expérimentauxmontrent la valeur ajoutée de cette construction par la comparaison des qualitésobtenues (e.g., précision) lorsque l'on utilise les méthodes de classification àpartir de différentes collections de descripteurs.	Nazha Selmaoui-Folcher, Jean-François Boulicaut, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1000742	http://editions-rnti.fr/render_pdf.php?p=1000742
Revue des Nouvelles Technologies de l'Information	EGC	2009	Contrôle des observations pour la gestion des systèmes de flux de données.	Les systèmes d'analyse de flux de données prennent de plus en plusd'importance dans un contexte où les données circulant sur les réseaux sont deplus en plus volumineuses et où la volonté de réagir au plus vite, en temps réel,devient un besoin nécessaire. Afin de permettre des analyses aussi rapides etefficaces que possible, il convient de pouvoir contrôler les flots de données et defocaliser les traitements sur les données pertinentes. Le protocole présenté dansce papier donne au module de traitement des capacités d'action et de contrôle surles observations remontantes en fonction de l'état de l'analyse. La diminutiondes flux résultant de telles focalisations permet des traitements beaucoup plusefficaces, plus pertinents et moins consommateurs de ressources. Les premiersrésultats montrent un réel gain de performances sur nos applications (facteur100).	Pierre Le Maigat, Christophe Dousson	http://editions-rnti.fr/render_pdf.php?p1&p=1000798	http://editions-rnti.fr/render_pdf.php?p=1000798
Revue des Nouvelles Technologies de l'Information	EGC	2009	Correspondances de Galois pour la manipulation de contextes flous multi-valués	L'analyse formelle de concepts est une méthode fondée sur la correspondancede Galois et qui permet de construire des hiérarchies de conceptsformels à partir de tableaux de données binaires. Cependant de nombreux problèmesréels abordés en fouille de données comportent des données plus complexes.Afin de traiter de tels problèmes, nous proposons une conversion de donnéesfloues multi-valuées en attributs histogrammes et une correspondance deGalois adaptée à ce format. Notre propos est illustré avec un jeu de donnéessimples. Enfin, nous évaluons brièvement les résultats et les apports de cettecorrespondance de Galois par rapport à l'approche classique	Aurélie Bertaux, Florence Le Ber, Agnès Braud	http://editions-rnti.fr/render_pdf.php?p1&p=1000763	http://editions-rnti.fr/render_pdf.php?p=1000763
Revue des Nouvelles Technologies de l'Information	EGC	2009	DBFrequentQueries : Extraction de requêtes fréquentes		Lucie Copin, Nicolas Pecheur, Anne Laurent, Yudi Augusta, Budi Sentana, Dominique Laurent, Tao-Yuan Jen	http://editions-rnti.fr/render_pdf.php?p1&p=1000822	http://editions-rnti.fr/render_pdf.php?p=1000822
Revue des Nouvelles Technologies de l'Information	EGC	2009	De l'utilisation de l'Analyse de Données Symboliques dans les Systèmes multi-agents	L'exploitation en temps réel de connaissances complexes est un défidans de nombreux domaines, tels que le web sémantique, la simulation ou lessystèmes multi-agents (SMA). Dans le paradigme multi-agents, des travaux ré-cents montrent que les communications multi-parties (CMP) offrent des oppor-tunités intéressantes en termes de réalisme des communications, diffusion desconnaissances et sémantique des actes de langage. Cependant, ces travaux seheurtent à la difficulté de mise en oeuvre des CMP, pour lesquelles les supportsde communications classiques sont insuffisants. Dans cet article, nous propo-sons d'utiliser le formalisme de l'Analyse de Données Symboliques (ADS) pourmodéliser les informations et les besoins des agents. Nous appuyons le routagedes messages sur cette modélisation dans le cadre d'un environnement de com-munication pour les systèmes multi-agents. Afin d'illustrer notre propos, nousutiliserons l'exemple de la gestion des communications dans un poste d'appelsd'urgence. Nous présentons ensuite notre retour d'expérience, et discutons lesperspectives ouvertes par la fertilisation croisée de l'ADS et des SMA.	Flavien Balbo, Julien Saunier, Edwin Diday, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1000746	http://editions-rnti.fr/render_pdf.php?p=1000746
Revue des Nouvelles Technologies de l'Information	EGC	2009	Définition d'une stratégie de résolution de problèmes pour un robot humanoïde	Nous avons développé un système dont le but est d'obtenir le logicielde commande d'un robot capable de simuler le comportement d'un humainplacé en situation de résolution de problèmes. Nous avons résolu ce problèmedans un environnement psychologique particulier où les comportements humainspeuvent être interprétés comme des 'observables' de leurs stratégies derésolution de problèmes. Notre solution contient de plus celle d'un autre problème,celui de construire une boucle complète commençant avec le comportementd'un groupe d'humains, son analyse et son interprétation en termesd'observables humaines, la définition des stratégies utilisées par les humains (ycompris celles qui sont inefficaces), l'interprétation des observables humainesen terme de mouvements du robot, la définition de ce qu'est une "stratégie derobot " en terme de stratégies humaines. La boucle est bouclée avec un langagede programmation capable de programmer ces stratégies robotiques, qui deviennentainsi à leur tour des observables, tout comme l'ont été les stratégieshumaines du début de la boucle. Nous expliquons comment nous avons été capablesdéfinir de façon objective ce que nous appelons une stratégie de robot.Notre solution assemble deux facteurs différents. L'un permet d'éviter lescomportements 'inhumains' et se fonde sur la moyenne des comportementsdes humains que nous avons observés. L'autre fournit une sorte 'd'humanité'au robot en lui permettant de dévier de cette moyenne par n fois l'écart typeobservé chez les humains qu'il doit simuler. Il devient alors possible de programmerdes comportements complètements humains.	Mary Felkin, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000806	http://editions-rnti.fr/render_pdf.php?p=1000806
Revue des Nouvelles Technologies de l'Information	EGC	2009	DEMON : Découverte de motifs séquentiels pour les puces adn	Prometteuses en terme de prévention, de dépistage, de diagnostic etd'actions thérapeutiques, les puces à ADN mesurent l'intensité des expressionsde plusieurs milliers de gènes. Dans cet article, nous proposons une nouvelleapproche appelée DEMON, pour extraire des motifs séquentiels à partir de don-nées issues des puces ADN et qui utilise des connaissances du domaine.	Paola Salle, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000803	http://editions-rnti.fr/render_pdf.php?p=1000803
Revue des Nouvelles Technologies de l'Information	EGC	2009	DEMON-Visualisation : un outil pour la visualisation des motifs séquentiels extraits à partir de données biologiques		Wei Xing, Paola Salle, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000818	http://editions-rnti.fr/render_pdf.php?p=1000818
Revue des Nouvelles Technologies de l'Information	EGC	2009	DesEsper: un logiciel de pré-traitement de flux appliqué à la surveillance des centrales hydrauliques		Frédéric Flouvat, Sébastien Gassmann, Jean-Marc Petit, Alain Ribière	http://editions-rnti.fr/render_pdf.php?p1&p=1000819	http://editions-rnti.fr/render_pdf.php?p=1000819
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection de séquences atypiques basée sur un modèle de Markov d'ordre variable	Récemment, le nombre et le volume des bases de données séquentiellesbiologiques ont augmenté de manière considérable. Dans ce contexte, l'identificationdes anomalies est essentielle. La plupart des approches pour lesextraire se fondent sur une base d'apprentissage ne contenant pas d'outlier. Or,dans de très nombreuses applications, les experts ne disposent pas d'une tellebase. De plus, les méthodes existantes demeurent exigeantes en mémoire, cequi les rend souvent impossibles à utiliser. Nous présentons dans cet article unenouvelle approche, basée sur un modèle de Markov d'ordre variable et sur unemesure de similarité entre objets séquentiels. Nous ajoutons aux méthodes existantesun critère d'élagage pour contrôler la taille de l'espace de rechercheet sa qualité, ainsi qu'une inégalité de concentration précise pour la mesure desimilarité, conduisant à une meilleure détection des outliers. Nous démontronsexpérimentalement la validité de notre approche.	Cécile Low-Kam, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000766	http://editions-rnti.fr/render_pdf.php?p=1000766
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection d'intrusions dans un environnement collaboratif sécurisé	Pour pallier le problème des attaques sur les réseaux de nouvelles ap-proches de détection d'anomalies ou d'abus ont été proposées ces dernières an-nées et utilisent des signatures d'attaques pour comparer une nouvelle requêteet ainsi déterminer s'il s'agit d'une attaque ou pas. Cependant ces systèmes sontmis à défaut quand la requête n'existe pas dans la base de signature. Généra-lement, ce problème est résolu via une expertise humaine afin de mettre à jourla base de signatures. Toutefois, il arrive fréquemment qu'une attaque ait déjàété détectée dans une autre organisation et il serait utile de pouvoir bénéficier decette connaissance pour enrichir la base de signatures mais cette information estdifficile à obtenir car les organisations ne souhaitent pas forcément indiquer lesattaques qui ont eu lieu sur le site. Dans cet article nous proposons une nouvelleapproche de détection d'intrusion dans un environnement collaboratif sécurisé.Notre approche permet de considérer toute signature décrite sous la forme d'ex-pressions régulières et de garantir qu'aucune information n'est divulguée sur lecontenu des différents sites.	Nischal Verma, François Trousset, Pascal Poncelet, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000775	http://editions-rnti.fr/render_pdf.php?p=1000775
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection d'objets atypiques dans un flot de données : une approche multi-résolution		Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000801	http://editions-rnti.fr/render_pdf.php?p=1000801
Revue des Nouvelles Technologies de l'Information	EGC	2009	Détermination du nombre des classes dans l'algorithme CROKI2 de classification croisée	Un des problèmes majeurs de la classification non supervisée est ladétermination ou la validation du nombre de classes dans la population. Ce problèmes'étend aux méthodes de bipartitionnement ou block clustering. Dans cepapier, nous nous intéressons à l'algorithme CROKI2 de classification croiséedes tableaux de contingence proposé par Govaert (1983). Notre objectif est dedéterminer le nombre de classes optimal sur les lignes et les colonnes à traversun ensemble de techniques de validation de classes proposés dans la littératurepour les méthodes classiques de classification.	Malika Charrad, Yves Lechevallier, Gilbert Saporta, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000797	http://editions-rnti.fr/render_pdf.php?p=1000797
Revue des Nouvelles Technologies de l'Information	EGC	2009	Diagnostic multi-sources adaptatif Application à la détection d'intrusion dans des serveursWeb	Le but d'un système adaptatif de diagnostic est de surveiller et diagnostiquerun système tout en s'adaptant à son évolution. Ceci passe par l'adaptationdes diagnostiqueurs qui précisent ou enrichissent leur propre modèle poursuivre au mieux le système au fil du temps. Pour détecter les besoins d'adaptation,nous proposons un cadre de diagnostic multi-sources s'inspirant de lafusion d'information. Des connaissances fournies par le concepteur sur des relationsattendues entre les diagnostiqueurs mono-source forment un méta-modèledu diagnostic. La compatibilité des résultats du diagnostic avec le méta-modèleest vérifiée en ligne. Lorsqu'une de ces relations n'est pas vérifiée, les diagnostiqueursconcernés sont modifiés.Nous appliquons cette approche à la conception d'un système adaptatif de détectiond'intrusion à partir d'un flux de connexions à un serveur Web. Les évaluationsdu système mettent en évidence sa capacité à améliorer la détection desintrusions connues et à découvrir de nouveaux types d'attaque.	Thomas Guyet, Wei Wang    , Rene Quiniou, Marie-Odile Cordier	http://editions-rnti.fr/render_pdf.php?p1&p=1000777	http://editions-rnti.fr/render_pdf.php?p=1000777
Revue des Nouvelles Technologies de l'Information	EGC	2009	Empreintes conceptuelles et spatiales pour la caractérisation des réseaux sociaux	Cet article propose une méthode reposant sur l'utilisation del'Analyse Formelle de Concepts et des treillis de Galois pour l'analyse desystèmes complexes. Des statistiques reposant sur ces treillis permettent decalculer la distribution conceptuelle des objets classifiés par le treillis.L'expérimentation sur des échantillons de trois réseaux sociaux en ligneillustre l'utilisation de ces statistiques pour la caractérisation globale et pour lefiltrage automatique de ces systèmes.	Bénédicte Le Grand, Marie-Aude Aufaure, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1000779	http://editions-rnti.fr/render_pdf.php?p=1000779
Revue des Nouvelles Technologies de l'Information	EGC	2009	Exploration de données de traçabilité issues de la RFID par apprentissage non-supervisé	La RFID (Radio Frequency IDentification) est une technologie avancée d'enregistrementde données spatio-temporelles de traçabilité. L'objectif de ce travail est de transformer cesdonnées spatio-temporelles en connaissances exploitables par les utilisateurs par l'intermé-diaire d'une méthode de classification automatique des données. Les systèmes RFID peuventêtre utilisés pour étudier les sociétés animales, qui sont des systèmes dynamiques complexescaractérisés par beaucoup d'interactions entre les individus (Fresneau et al., 1989). Le cadreapplicatif choisi pour ce travail est l'étude de la structure d'un groupe d'individus en interactionsociale et en particulier la division du travail au sein d'une colonie de fourmis1.La RFID générant d'importants volumes de données, il est nécessaire de développer desméthodes appropriées afin d'en comprendre le sens. Nous proposons pour cela un algorithmede classification topographique non-supervisée pour l'exploration de ce type de données, ca-pable de détecter les groupes d'individus exprimant le même comportement. L'algorithmeDS2L-SOM (Density-based Simultaneous Two-Level - SOM, Cabanes et Bennani (2008)) estcapable de détecter non seulement les groupes définis par une zone vide de donnée, grâce àune estimation de la pertinence des connexions entre référents, mais aussi les groupes défi-nis seulement par une diminution de densité, grâce à une estimation de la densité autour desréférents pendant l'apprentissage.	Guénaël Cabanes, Younès Bennani, Dominique Fresneau	http://editions-rnti.fr/render_pdf.php?p1&p=1000799	http://editions-rnti.fr/render_pdf.php?p=1000799
Revue des Nouvelles Technologies de l'Information	EGC	2009	Exploration des corrélations dans un classifieur Application au placement d'offres commerciales	Cet article présente une nouvelle méthode permettant d'explorer lesprobabilités délivrées par un modèle prédictif de classification. L'augmentationde la probabilité d'occurrence de l'une des classes du problème étudié est analyséeen fonction des variables explicatives prises isolément. La méthode proposéeest posée et illustrée dans un cadre général, puis explicitement dédiée au classifieurBayesien naïf. Son illustration sur les données du challenge PAKDD 2007montre que ce type d'exploration permet de créer des indicateurs performantsd'aide à la vente.	Vincent Lemaire, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1000739	http://editions-rnti.fr/render_pdf.php?p=1000739
Revue des Nouvelles Technologies de l'Information	EGC	2009	Explorer3D : classification et visualisation de données		Matthieu Exbrayat, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1000817	http://editions-rnti.fr/render_pdf.php?p=1000817
Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction de motifs fermés dans des relations n-aires bruitées	L'extraction de motifs fermés dans des relations binaires a été trèsétudiée. Cependant, de nombreuses relations intéressantes sont n-aires avec n >2 et bruitées (nécessité d'une tolérance aux exceptions). Récemment, ces deuxproblèmes ont été traités indépendamment. Nous introduisons notre propositionpour combiner de telles fonctionnalités au sein d'un même algorithme.	Loïc Cerf, Jérémy Besson, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000748	http://editions-rnti.fr/render_pdf.php?p=1000748
Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction de Règles de Corrélation Décisionnelles	Dans cet article, nous introduisons deux nouveaux concepts : les règlesde corrélation décisionnelles et les vecteurs de contingence. Le premier résulted'un couplage entre les règles de corrélation et les règles de décision. Il permetde mettre en évidence des liens pertinents entre certains ensembles de motifsd'une relation binaire et les valeurs d'un attribut cible (appartenant à cette mêmerelation) en se basant à la fois sur la mesure du Khi-carré et sur le support desmotifs extraits. De par la nature du problème, les algorithmes par niveaux fontque l'extraction des résultats a lieu avec des temps de réponse élevés et uneoccupation mémoire importante. Afin de palier à ces deux inconvénients, nousproposons un algorithme basé sur l'ordre lectique et les vecteurs de contingence.	Alain Casali, Christian Ernst	http://editions-rnti.fr/render_pdf.php?p1&p=1000761	http://editions-rnti.fr/render_pdf.php?p=1000761
Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction efficace de règles graduelles	Les règles graduelles suscitent depuis quelques années un intérêt croissant.De telles règles, de la forme "Plus (moins) A1 et ... plus (moins) An alorsplus (moins) B1 et ... plus (moins) Bn" trouvent application dans de nombreuxdomaines tels que la bioinformatique, les contrôleurs flous, les relevés de capteursou encore les flots de données. Ces bases, souvent composées d'un grandnombre d'attributs, restent un verrou pour l'extraction automatique de connaissances,car elles rendent inefficaces les techniques de fouille habituelles (règlesd'association, clustering...). Dans cet article, nous proposons un algorithme efficaced'extraction d'itemset graduels basé sur l'utilisation des treillis. Nous définissonsformellement les notions de gradualité, ainsi que les algorithmes associés.Des expérimentations menées sur jeux de données synthétiques et réelsmontrent l'intérêt de notre méthode	Lisa Di-Jorio, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000764	http://editions-rnti.fr/render_pdf.php?p=1000764
Revue des Nouvelles Technologies de l'Information	EGC	2009	FCP-Growth, une adaptation de FP-Growth pour générer des règles d'association de classe		Emna Bahri, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000804	http://editions-rnti.fr/render_pdf.php?p=1000804
Revue des Nouvelles Technologies de l'Information	EGC	2009	Fouille de données dans les bases relationnelles pour l'acquisition d'ontologies riches en hiérarchies de classes	De par leur caractère structuré, les bases de données relationnellessont des sources précieuses pour la construction automatisée d'ontologies. Ce-pendant, une limite persistante des approches existantes est la production d'onto-logies de structure calquée sur celles des schémas relationnels sources. Dans cetarticle, nous décrivons la méthode RTAXON dont la particularité est d'identifierdes motifs de catégorisation dans les données afin de produire des ontologiesplus structurées, riches en hiérarchies. La méthode formalisée combine analyseclassique du schéma relationnel et fouille des données pour l'identification destructures hiérarchiques.	Farid Cerbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000786	http://editions-rnti.fr/render_pdf.php?p=1000786
Revue des Nouvelles Technologies de l'Information	EGC	2009	Fusion Symbolique pour la Recommandation de Programmes Télévisées	Nous proposons une approche générique pour la fusion d'informa-tions qui repose sur l'utilisation du modèle des Graphes Conceptuels et l'opé-ration de jointure maximale. Nous validons notre approche par le biais d'ex-périmentations. Ces expérimentations soulignent l'importance des heuristiquesmises en place.	Claire Laudy, Jean-Gabriel Ganascia	http://editions-rnti.fr/render_pdf.php?p1&p=1000796	http://editions-rnti.fr/render_pdf.php?p=1000796
Revue des Nouvelles Technologies de l'Information	EGC	2009	Générer des règles de classification par Dopage de Concepts Formels	La classification supervisée est une tâche de fouille de données (DataMining), qui consiste à construire un classifieur à partir d'un ensemble d'exemplesétiquetés par des classes (phase d'apprentissage) et ensuite prédire les classesdes nouveaux exemples avec ce classifieur (phase de classification). En classi-fication supervisée, plusieurs approches ont été proposées dont l'approche ba-sée sur l'Analyse de Concepts Formels. L'apprentissage de Concepts Formelsest basé généralement sur la structure mathématique du treillis de Galois (outreillis de concepts). Cependant, la complexité exponentielle de génération d'untreillis de Galois a limité les champs d'application de ces systèmes. Dans cetarticle, nous présentons plusieurs méthodes de classification supervisée baséessur l'Analyse de Concepts Formels. Nous présentons aussi le boosting (dopage)de classifieurs, une technique de classification innovante. Enfin, nous proposonsle boosting de concepts formels, une nouvelle méthode adaptative qui construitseulement une partie du treillis englobant les meilleurs concepts. Ces conceptssont utilisés comme étant des règles de classification. Les résultats expérimen-taux réalisés ont prouvé l'intérêt de la méthode proposée par rapport à cellesexistantes.	Mondher Maddouri, Nida Meddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000760	http://editions-rnti.fr/render_pdf.php?p=1000760
Revue des Nouvelles Technologies de l'Information	EGC	2009	Graphes des liens et anti liens statistiquement valides entre les mots d'un corpus textuel : test de randomisation TourneBool sur le corpus Reuters	La définition du voisinage est un élément central en fouille de données, et de nombreuses définitions ont été avancées. Nous en proposons ici une version statistique issue de notre test de randomisation TourneBool, qui permet, à partir d'un tableau de relations binaires objets décrits/descripteurs, d'établir quelles relations entre descripteurs sont dues au hasard, et lesquelles ne le sont pas, sans faire d'hypothèse sur les lois de répartitions sous-jacentes, c'est à dire en tenant compte de lois de tous types sans avoir besoin de les spécifier. Ce test est basé sur la génération et l'exploitation d'un ensemble de matrices randomisées ayant les mêmes sommes marginales en lignes et colonnes que la matrice d'origine. Après une première application encourageante à un corpus textuel réduit, nous avons opéré le passage à l'échelle adéquat pour traiter des corpus textuels de taille réelle, comme celui des dépêches Reuters. Nous caractérisons le graphe des mots de ce corpus au moyen d'indicateurs classiques comme le coefficient de clustering, la distribution des degrés et de la taille des communautés, etc. Une autre caractéristique de TourneBool est qu'il permet aussi de dégager les "anti liens" entre mots, à savoir les mots qui s'évitent plus qu'attendu du fait du hasard. Le graphe des liens et celui des anti-liens seront caractérisés de la même façon.	Martine Cadot, Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1000783	http://editions-rnti.fr/render_pdf.php?p=1000783
Revue des Nouvelles Technologies de l'Information	EGC	2009	Handling Texts ? A Challenge for Data Mining	The amount of data in free form by far surpasses the structured records in databases in theirnumber. However, standard learning algorithms require observations in the form of vectorsgiven a fixed set of attributes. For texts, there is no such fixed set of attributes. The bag ofwords representation yields vectors with as many components as there are words in a language.Hence, the classification of documents represented as bag of word vectors demands efficientlearning algorithms. The TCat model for the support vector machine (Joachims 2002) offers asound performance estimation for text classification.The huge mass of documents, in principle, offers answers to many questions and is oneof the most important sources of knowledge. However, information retrieval and text classi-fication deliver merely the document, in which the answer can be found by a human reader ?not the answer itself. Hence, information extraction has become an important topic: if we canextract information from text, we can apply standard machine learning to the extracted facts(Craven et al. 1998). First, information extraction has to recognize Named Entities (see, e.g.,Roessler, Morik 2005). Second, relations between these become the nucleus of events. Ex-tracting events from a complex web site with long documents allows to automatically discoverregularities which are otherwise hidden in the mass of sentences (see, e.g., Jungermann, Morik2008).	Katharina Morik	http://editions-rnti.fr/render_pdf.php?p1&p=1000732	http://editions-rnti.fr/render_pdf.php?p=1000732
Revue des Nouvelles Technologies de l'Information	EGC	2009	La carte GHSOM comme alternative à la SOM pour l'analyse exploratoire de données	L'objecif de cet article est de faire de la carte auto-organisatrice hiérarchique(GHSOM) un outil utilisable dans le cadre d'une démarche d'analyseexploratoire de données. La visualisation globale est un outil indispensable pourrendre les résultats d'une segmentation intelligibles pour un utilisateur. Nousproposons donc différents outils de visualisation pour la GHSOM équivalents àceux de la SOM.	Françoise Fessant, Fabrice Clérot, Pascal Gouzien	http://editions-rnti.fr/render_pdf.php?p1&p=1000734	http://editions-rnti.fr/render_pdf.php?p=1000734
Revue des Nouvelles Technologies de l'Information	EGC	2009	La « créativité calculatoire » et les heuristiques créatives en synthèse de prédicats multiples	Nous présentons une approche à ce que nous appelons la « créativitécalculatoire », c'est-à-dire les procédés par lesquels une machine peut fairemontre d'une certaine créativité. Dans cet article, nous montronsessentiellement que la synthèse de prédicats multiples en programmationlogique inductive (ILP) et la synthèse de programmes à partir de spécificationsformelles (SPSF), deux domaines de l'informatique qui s'attaquent à desproblèmes où la notion de créativité est centrale, ont été amenés à ajouter àleur formalisme de base (l'ILP pour l'un, les tableaux de Beth pour l'autre)toute une série d'heuristiques. Cet article présente une collectiond'heuristiques qui sont destinées à fournir au programme une forme decréativité calculatoire. Dans cette présentation, l'accent est plutôt mis sur lesheuristiques de l'ILP mais lorsque cela était possible sans de trop longsdéveloppements, nous avons aussi présenté quelques heuristiques de la SPSF.L'outil indispensable de la créativité calculatoire est ce que nous appelons un'générateur d'atouts' dont une spécification (forcément informelle commenous le verrons) est fournie comme première conclusion aux exemples décritsdans le corps de l'article.	Marta Franová, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000747	http://editions-rnti.fr/render_pdf.php?p=1000747
Revue des Nouvelles Technologies de l'Information	EGC	2009	Le logiciel SYR pour l'Analyse de Données Symboliques		Filipe Afonso, Edwin Diday, Wassim Khaskhoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1000823	http://editions-rnti.fr/render_pdf.php?p=1000823
Revue des Nouvelles Technologies de l'Information	EGC	2009	Logiciel « DtmVic » Data and Text Mining: Visualisation, Inférence, Classification		Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1000815	http://editions-rnti.fr/render_pdf.php?p=1000815
Revue des Nouvelles Technologies de l'Information	EGC	2009	L'Analyse Formelle de Concepts pour l'Extraction de Connaissances dans les Données d'Expression de Gènes	L'analyse formelle de concepts (AFC, Ganter etWille (1999)) est uneméthode pertinente d'extraction de connaissances à partir de données complexesd'expression de gènes (Blachon et al. (2007), Motameny et al. (2008)). Dans cepapier, nous proposons d'extraire des groupes de gènes partageant un compor-tement similaire montrant des changements "significatifs" à travers divers envi-ronnements biologiques, servant d'hypothèses à la fonction des gènes.	Mehdi Kaytoue-Uberall, Sébastien Duplessis, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000793	http://editions-rnti.fr/render_pdf.php?p=1000793
Revue des Nouvelles Technologies de l'Information	EGC	2009	Management des connaissances dans le domaine du patrimoine culturel		Stefan du Château, Danielle Boulanger, Eunika Mercier-Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000792	http://editions-rnti.fr/render_pdf.php?p=1000792
Revue des Nouvelles Technologies de l'Information	EGC	2009	Méthode de regroupement par graphe de voisinage	Ce travail s'inscrit dans la problématique de l'apprentissage non su-pervisé. Dans ce cadre se retrouvent les méthodes de classification automatiquenon paramétriques qui reposent sur l'hypothèse que plus des individus sontproches dans l'espace de représentation, plus ils ont de chances de faire par-tie de la même classe. Cet article propose une nouvelle méthode de ce type quiconsidère la proximité à travers la structure fournie par un graphe de voisinage.	Fabrice Muhlenbach	http://editions-rnti.fr/render_pdf.php?p1&p=1000782	http://editions-rnti.fr/render_pdf.php?p=1000782
Revue des Nouvelles Technologies de l'Information	EGC	2009	Modèle de préférences contextuelles pour les analyses OLAP	Cet article présente un environnement pour la personnalisation desanalyses OLAP afin de réduire la charge de navigation de l'utilisateur. Nousproposons un modèle de préférences contextuelles qui permet de restituer lesdonnées en fonction des préférences de l'utilisateur et de son contexted'analyse.	Houssem Jerbi, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000769	http://editions-rnti.fr/render_pdf.php?p=1000769
Revue des Nouvelles Technologies de l'Information	EGC	2009	Modélisation des connaissances dans le cadre de bibliothèques numériques spécialisées	Nous présentons une application innovante de la modélisation desconnaissances au domaine des bibliothèques numériques spécialisées. Nous utilisonsla spécification experte de la TEI (Text Encoding Initiative) pour modéliserla connaissance apportée par les chercheurs qui travaillent sur des archivesmanuscrites. Nous montrons les limites de la TEI dans le cas d'une approchediachronique du document, cette dernière impliquant la construction simultanéede structures de données concurrentes. Nous décrivons un modèle qui présentele problème et permet d'envisager des solutions. Enfin, nous justifions les structuresarborescentes sur lesquelles se base ce modèle.	Sylvie Calabretto, Pierre-Edouard Portier	http://editions-rnti.fr/render_pdf.php?p1&p=1000785	http://editions-rnti.fr/render_pdf.php?p=1000785
Revue des Nouvelles Technologies de l'Information	EGC	2009	Okmed et Wokm	Cet article traite de la problématique de la classification recouvrante(overlapping clustering) et propose deux variantes de l'approche OKM : OKMEDet WOKM. OKMED généralise k-médoïdes au cas recouvrant, il permet d'organiserun ensemble d'individus en classes non-disjointes, à partir d'une matricede distances. La méthode WOKM (Weighted-OKM) étend OKM par une pondérationlocale des classes ; cette variante autorise chaque individu à appartenir àplusieurs classes sur la base de critères différents. Des expérimentations sont réaliséessur une application cible : la classification de textes. Nous montrons alorsque OKMED présente un comportement similaire à OKM pour la métrique euclidienne,et offre la possibilité d'utiliser des métriques plus adaptées et d'obtenirde meilleures performances. Enfin, les résultats obtenus avec WOKM montrentun apport significatif de la pondération locale des classes	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1000736	http://editions-rnti.fr/render_pdf.php?p=1000736
Revue des Nouvelles Technologies de l'Information	EGC	2009	Online and Adaptive Anomaly Detection: Detecting Intrusions in Unlabelled Audit Data Streams		Wei Wang    , Thomas Guyet, Rene Quiniou, Marie-Odile Cordier, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000802	http://editions-rnti.fr/render_pdf.php?p=1000802
Revue des Nouvelles Technologies de l'Information	EGC	2009	Partitionnement d'ontologies pour le passage à l'échelle des techniques d'alignement	L'alignement d'ontologies est une tâche importante dans les systèmesd'intégration puisqu'elle autorise la prise en compte conjointe de ressourcesdécrites par des ontologies différentes, en identifiant des appariements entreconcepts. Avec l'apparition de très grandes ontologies dans des domaines commela médecine ou l'agronomie, les techniques d'alignement, qui mettent souventen oeuvre des calculs complexes, se trouvent face à un défi : passer à l'échelle.Pour relever ce défi, nous proposons dans cet article deux méthodes de partition-nement, conçues pour prendre en compte, le plus tôt possible, l'objectif d'ali-gnement. Ces méthodes permettent de décomposer les deux ontologies à aligneren deux ensembles de blocs de taille limitée et tels que les éléments susceptiblesd'être appariés se retrouvent concentrés dans un ensemble minimal de blocs quiseront effectivement comparés. Les résultats des tests effectuées avec nos deuxméthodes sur différents couples d'ontologies montrent leur efficacité.	Fayçal Hamdi, Brigitte Safar, Haïfa Zargayouna, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000787	http://editions-rnti.fr/render_pdf.php?p=1000787
Revue des Nouvelles Technologies de l'Information	EGC	2009	Privacy and Data Mining: New Developments and Challenges	There is little doubt that data mining technologies create new challenges in the area of dataprivacy. In this talk, we will review some of the new developments in Privacy-preserving DataMining. In particular, we will discuss techniques in which data mining results can reveal per-sonal data, and how this can be prevented. We will look at the practically interesting situationswhere data to be mined is distributed among several parties. We will mention new applica-tions in which mining spatio-temporal data can lead to identification of personal information.We will argue that methods that effectively protect personal data, while at the same time pre-serve the quality of the data from the data analysis perspective, are some of the principal newchallenges before the field.	Stan Matwin	http://editions-rnti.fr/render_pdf.php?p1&p=1000730	http://editions-rnti.fr/render_pdf.php?p=1000730
Revue des Nouvelles Technologies de l'Information	EGC	2009	Probabilistic Multi-classifier by SVMs from voting rule to voting features		Anh Phuc Trinh, David Buffoni, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000790	http://editions-rnti.fr/render_pdf.php?p=1000790
Revue des Nouvelles Technologies de l'Information	EGC	2009	RDBToOnto : un logiciel dédié à l'apprentissage d'ontologies à partir de bases de données relationnelles	RDBToOnto1 est un logiciel extensible qui permet d'élaborer des on-tologies précises à partir de bases de données relationnelles. Le processus sup-porté est largement automatisé, de l'extraction des données à la génération dumodèle de l'ontologie et son instanciation. Pour affiner le résultat, le processuspeut être orienté par des contraintes locales définies interactivement. C'est aussiun cadre facilitant la mise en oeuvre de nouvelles méthodes d'apprentissage.	Farid Cerbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000820	http://editions-rnti.fr/render_pdf.php?p=1000820
Revue des Nouvelles Technologies de l'Information	EGC	2009	Regroupement des Définitions de Sigles Biomédicaux	L'application présentée permet de regrouper les définitions de siglesissues des sciences du vivant par des mesures de proximité lexicale (approcheautomatique) et une intervention de l'expert (approche manuelle).	Ousmane Djanga, Hanine Hamzioui, Mickaël Hatchi, Isabelle Mougenot, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000816	http://editions-rnti.fr/render_pdf.php?p=1000816
Revue des Nouvelles Technologies de l'Information	EGC	2009	Résumé hybride de flux de données par échantillonnage et classification automatique	Face à la grande volumétrie des données générées par les systèmes informatiques,l'hypothèse de les stocker en totalité avant leur interrogation n'estplus possible. Une solution consiste à conserver un résumé de l'historique duflux pour répondre à des requêtes et pour effectuer de la fouille de données.Plusieurs techniques de résumé de flux de données ont été développées, tellesque l'échantillonnage, le clustering, etc. Selon le champ de requête, ces résuméspeuvent être classés en deux catégories: résumés spécialisés et résumés généralistes.Dans ce papier, nous nous intéressons aux résumés généralistes. Notreobjectif est de créer un résumé de bonne qualité, sur toute la période temporelle,qui nous permet de traiter une large panoplie de requêtes. Nous utilisons deuxalgorithmes : CluStream et StreamSamp. L'idée consiste à les combiner afin detirer profit des avantages de chaque algorithme. Pour tester cette approche, nousutilisons un Benchmark de données réelles "KDD_99". Les résultats obtenussont comparés à ceux obtenus séparément par les deux algorithmes.	Nesrine Gabsi, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000767	http://editions-rnti.fr/render_pdf.php?p=1000767
Revue des Nouvelles Technologies de l'Information	EGC	2009	SoftJaccard : une mesure de similarité entre ensembles de chaînes de caractères pour l'unification d'entités nommées	Parmi lesmesures de similarité classiques utilisables sur des ensemblesfigure l'indice de Jaccard. Dans le cadre de cet article, nous en proposons uneextension pour comparer des ensembles de chaînes de caractères. Cette mesurehybride permet de combiner une distance entre chaînes de caractères, telle que ladistance de Levenstein, et l'indice de Jaccard. Elle est particulièrement adaptéepourmettre en correspondance des champs composés de plusieurs chaînes de caractères,comme par exemple, lorsqu'on se propose d'unifier des noms d'entitésnommées.	Christine Largeron, Bernard Kaddour, Maria Fernandez	http://editions-rnti.fr/render_pdf.php?p1&p=1000795	http://editions-rnti.fr/render_pdf.php?p=1000795
Revue des Nouvelles Technologies de l'Information	EGC	2009	SPAMS, une nouvelle approche incrémentale pour l'extraction de motifs séquentiels fréquents dans les Data streams	L'extraction de motifs séquentiels fréquents dans les datastreams est un enjeu important traité par la communauté des chercheursen fouille de données. Plus encore que pour les bases de données, denombreuses contraintes supplémentaires sont à considérer de par la na-ture intrinsèque des streams. Dans cet article, nous proposons un nouvelalgorithme en une passe : SPAMS, basé sur la construction incrémentale,avec une granularité très fine par transaction, d'un automate appelé SPA,permettant l'extraction des motifs séquentiels dans les streams. L'infor-mation du stream est apprise à la volée, au fur et à mesure de l'insertionde nouvelles transactions, sans pré-traitement a priori. Les résultats ex-périmentaux obtenus montrent la pertinence de la structure utilisée ainsique l'efficience de notre algorithme appliqué à différents jeux de données.	Lionel Vinceslas, Jean-Emile Symphor, Alban Mancheron, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000765	http://editions-rnti.fr/render_pdf.php?p=1000765
Revue des Nouvelles Technologies de l'Information	EGC	2009	SVM incrémental et parallèle sur GPU	Nous présentons un nouvel algorithme incrémental et parallèle deSéparateur à Vaste Marge (SVM ou Support Vector Machine) pour laclassification de très grands ensembles de données en utilisant le processeur dela carte graphique (GPUs, Graphics Processing Units). Les SVMs et lesméthodes de noyaux permettent de construire des modèles avec une bonneprécision mais ils nécessitent habituellement la résolution d'un programmequadratique ce qui requiert une grande quantité de mémoire et un long tempsd'exécution pour les ensembles de données de taille importante. Nousprésentons une extension de l'algorithme de Least Squares SVM (LS-SVM)proposé par Suykens et Vandewalle pour obtenir un algorithme incrémental etparallèle. Le nouvel algorithme est exécuté sur le processeur graphique pourobtenir une bonne performance à faible coût. Les résultats numériques sur lesensembles de données de l'UCI et Delve montrent que notre algorithmeincrémental et parallèle est environ 70 fois plus rapide sur GPU que sur CPUet significativement plus rapide (plus de 1000 fois) que les algorithmesstandards tels que LibSVM, SVM-perf et CB-SVM.	François Poulet, Thanh-Nghi Do, Van-Hoa Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000743	http://editions-rnti.fr/render_pdf.php?p=1000743
Revue des Nouvelles Technologies de l'Information	EGC	2009	TAAABLE : système de recherche et de création, par adaptation, de recettes de cuisine	TAAABLE is a textual case-based reasoning system that, according to requested/forbiddeningredients, dish types and/or dish origins, retrieves cooking recipes. If no recipe satisifies theconstraints, TAAABLE adapts existing recipes by replacing some ingredients by other ones.	Amélie Cordier, Jean Lieber, Emmanuel Nauer, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1000812	http://editions-rnti.fr/render_pdf.php?p=1000812
Revue des Nouvelles Technologies de l'Information	EGC	2009	TraMineR: une librairie R pour l'analyse de données séquentielles		Alexis Gabadinho, Nicolas S. Müller, Gilbert Ritschard, Matthias Studer	http://editions-rnti.fr/render_pdf.php?p1&p=1000814	http://editions-rnti.fr/render_pdf.php?p=1000814
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un algorithme stable de décomposition pour l'analyse des réseaux sociaux dynamiques	Les réseaux dynamiques soulèvent de nouveaux problèmes d'analyses.Un outils efficace d'analyse doit non seulement permettre de décomposerces réseaux en groupes d'éléments similaires mais il doit aussi permettre la détectionde changements dans le réseau. Nous présentons dans cet article une nouvelleapproche pour l'analyse de tels réseaux. Cette technique est basée sur unalgorithme de décomposition de graphe en groupes chevauchants (ou chevauchement).La complexité de notre algorithme est O(|E| · deg2max +|V | · log(|V |))).La faible sensibilité de cet algorithme aux changements structuraux du réseaupermet d'en détecter les modifications majeures au cours du temps.	Romain Bourqui, Paolo Simonetto, Fabien Jourdan	http://editions-rnti.fr/render_pdf.php?p1&p=1000778	http://editions-rnti.fr/render_pdf.php?p=1000778
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un critère d'évaluation Bayésienne pour la construction d'arbres de décision	Nous présentons dans cet article un nouvel algorithme automatiquepour l'apprentissage d'arbres de décision. Nous abordons le problème selon uneapproche Bayésienne en proposant, sans aucun paramètre, une expression ana-lytique de la probabilité d'un arbre connaissant les données. Nous transformonsle problème de construction de l'arbre en un problème d'optimisation : nousrecherchons dans l'espace des arbres de décision, l'arbre optimum au sens ducritère Bayésien ainsi défini, c'est à dire l'arbre maximum a posteriori (MAP).L'optimisation est effectuée en exploitant une heuristique de pré-élagage. Desexpérimentations comparatives sur trente bases de l'UCI montrent que notreméthode obtient des performances prédictives proches de celles de l'état de l'arttout en étant beaucoup moins complexes.	Nicolas Voisine, Marc Boullé, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1000740	http://editions-rnti.fr/render_pdf.php?p=1000740
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un nouvel algorithme de forêts aléatoires d'arbres obliques particulièrement adapté à la classification de données en grandes dimensions	L'algorithme des forêts aléatoires proposé par Breiman permet d'ob-tenir de bons résultats en fouille de données comparativement à de nombreusesapproches. Cependant, en n'utilisant qu'un seul attribut parmi un sous-ensembled'attributs tiré aléatoirement pour séparer les individus à chaque niveau de l'arbre,cet algorithme perd de l'information. Ceci est particulièrement pénalisant avecles ensembles de données en grandes dimensions où il peut exister de nom-breuses dépendances entre attributs. Nous présentons un nouvel algorithme deforêts aléatoires d'arbres obliques obtenus par des séparateurs à vaste marge(SVM). La comparaison des performances de notre algorithme avec celles del'algorithme de forêts aléatoires des arbres de décision C4.5 et de l'algorithmeSVM montre un avantage significatif de notre proposition.	Thanh-Nghi Do, Stéphane Lallich, Nguyen-Khang Pham, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1000741	http://editions-rnti.fr/render_pdf.php?p=1000741
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un prototype cross-lingue multi-métiers : vers la Gestion Sémantique de Contenu d'Entreprise au service du Collaboratif Opérationnel	Le domaine « Qualité, Hygiène, Sécurité et Environnement »(QHSE) représente à l'heure actuelle un vecteur de progrès majeur pourl'industrie européenne. Le prototype « Semantic Quality Environment » (SQE)introduit dans cet article vise à démontrer la validité d'une architecturesémantique cross-lingue vouée à la collaboration multi-métiers et multilingue,dans le cadre d'un système banalisé de gestion de contenu d'entreprise dédié àl'industrie navale européenne.	Christophe Thovex, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000809	http://editions-rnti.fr/render_pdf.php?p=1000809
Revue des Nouvelles Technologies de l'Information	EGC	2009	Un système pour l'extraction de corrélations linéaires dans des données de génomique médicale		Arriel Benis, Mélanie Courtine	http://editions-rnti.fr/render_pdf.php?p1&p=1000807	http://editions-rnti.fr/render_pdf.php?p=1000807
Revue des Nouvelles Technologies de l'Information	EGC	2009	Une méthode de classification supervisée sans paramètre pour l'apprentissage sur les grandes bases de données	Dans ce papier, nous présentons une méthode de classification super-visée sans paramètre permettant d'attaquer les grandes volumétries. La méthodeest basée sur des estimateurs de densités univariés optimaux au sens de Bayes,sur un classifieur Bayesien naïf amélioré par une sélection de variables et unmoyennage de modèles exploitant un lissage logarithmique de la distribution aposteriori des modèles. Nous analysons en particulier la complexité algorith-mique de la méthode et montrons comment elle permet d'analyser des bases dedonnées nettement plus volumineuses que la mémoire vive disponible. Nous pré-sentons enfin les résultats obtenu lors du récent PASCAL Large Scale LearningChallenge, où notre méthode a obtenu des performances prédictives de premierplan avec des temps de calcul raisonnables.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000770	http://editions-rnti.fr/render_pdf.php?p=1000770
Revue des Nouvelles Technologies de l'Information	EGC	2009	Une nouvelle approche pour la classification non supervisée en segmentation d'image	La segmentation des images en régions est un problème crucial pourl'analyse et la compréhension des images. Parmi les approches existantes pourrésoudre ce problème, la classification non supervisée est fréquemment em-ployée lors d'une première étape pour réaliser un partitionnement de l'espacedes intensités des pixels (qu'il s'agisse de niveaux de gris, de couleurs ou de ré-ponses spectrales). Puisqu'elle ignore complètement les notions de voisinagedes pixels, une seconde étape d'analyse spatiale (étiquetage en composantesconnexes par exemple) est ensuite nécessaire pour identifier les régions issuesde la segmentation. La non prise en compte de l'information spatiale est une li-mite majeure de ce type d'approche, ce qui a motivé de nombreux travaux où laclassification est couplée à d'autres techniques pour s'affranchir de ce problème.Dans cet article, nous proposons une nouvelle formulation de la classificationnon supervisée permettant d'effectuer la segmentation des images sans faire ap-pel à des techniques supplémentaires. Plus précisément, nous élaborons une mé-thode itérative de type k-means où les données à partitionner sont les pixels eux-mêmes (et non plus leurs intensités) et où les distances des points aux centresdes classes ne sont plus euclidiennes mais topographiques. La segmentation estalors un processus itératif, et à chaque itération, les classes obtenues peuvent êtreassimilées à des zones d'influence dans le contexte de la morphologie mathéma-tique. Ce parallèle nous permet de bénéficier des algorithmes efficaces proposésdans ce domaine (tels que ceux basés sur les files d'attente), tout en y ajoutantle caractère itératif des méthodes de classification non supervisée considéréesici. Nous illustrons finalement le potentiel de l'approche proposée par quelquesrésultats préliminaires de segmentation sur des images artificielles.	Sébastien Lefèvre	http://editions-rnti.fr/render_pdf.php?p1&p=1000745	http://editions-rnti.fr/render_pdf.php?p=1000745
Revue des Nouvelles Technologies de l'Information	EGC	2009	Utilisation de l'analyse factorielle des correspondances pour la recherche d'images à grande échelle	Nous nous intéressons à l'utilisation de l'Analyse Factorielle des Cor-respondances (AFC) pour la recherche d'images par le contenu dans une base dedonnées d'images volumineuse. Nous adaptons l'AFC, méthode originellementdéveloppée pour l'Analyse des Données Textuelles (ADT), aux images en utili-sant des descripteurs locaux SIFT. En ADT, l'AFC permet de réduire le nombrede dimensions et de trouver des thèmes. Ici, l'AFC nous permettra de limiter lenombre d'images à examiner au cours de la recherche afin d'accélérer le tempsde réponse pour une requête. Pour traiter de grandes bases d'images, nous pro-posons une version incrémentale de l'algorithme AFC. Ce nouvel algorithmedécoupe une base d'images en blocs et les charge dans la mémoire l'un aprèsl'autre. Nous présentons aussi l'intégration des informations contextuelles (e.g.la Mesure de Dissimilarité Contextuelle (Jegou et al., 2007)) dans notre structurede recherche d'images. Cela améliore considérablement la précision. Nous ex-ploitons cette intégration dans deux axes: (i) hors ligne (la structure de voisinageest corrigée hors ligne) et (ii) à la volée (la structure de voisinage des images estcorrigée au cours de la recherche sur un petit ensemble d'images).	Nguyen-Khang Pham, Annie Morin, Patrick Gros, Quyet-Thang Le	http://editions-rnti.fr/render_pdf.php?p1&p=1000773	http://editions-rnti.fr/render_pdf.php?p=1000773
Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers la simulation et la détection des changements des données évolutives d'usage du Web	Dans le domaine des flux des données, la prise en compte du tempss'avère nécessaire pour l'analyse de ces données car leur distribution sous-jacentepeut changer au cours du temps. Un exemple typique concerne les modèles desprofils de navigation des internautes. Notre objectif est d'analyser l'évolutionde ces profils, celle-ci peut être liée au changement d'effectifs ou aux déplacementde clusters au cours du temps. Afin d'analyser la validité de notre approche,nous mettons en place uneméthodologie pour la simulation des données d'usageà partir de laquelle il est possible de contrôler l'occurrence des changements	Alzennyr Da Silva, Yves Lechevallier, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1000800	http://editions-rnti.fr/render_pdf.php?p=1000800
Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers le traitement à grande échelle de données symboliques		Omar Merroun, Edwin Diday, Philippe Rigaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000791	http://editions-rnti.fr/render_pdf.php?p=1000791
Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers une utilisation améliorée de relations spatiales pour l'apprentissage de données dans les modèles graphiques	Nous nous intéressons dans cet article aux représentations des relationsspatiales pour l'extraction d'information et la modélisation des donnéesvisuelles, en particulier dans le contexte de la catégorisation d'images. Nousmontrons comment la prise en compte d'une relation spatiale entre deux élémentsentraîne l'apparition d'une information supplémentaire entre ces élémentset le reste de l'ensemble à modéliser, ce qui est rarement exploité explicitement.Une représentation floue des relations dans unmodèle graphique est bien adaptéepour les algorithmes d'apprentissage utilisés actuellement et permet d'intégrerce type d'information complémentaire qui concerne l'absence d'une interactionplutôt que sa présence. Nous tentons d'évaluer les bénéfices de cette approchesur un problème de traitement d'images.	Isabelle Bloch, Emanuel Aldea	http://editions-rnti.fr/render_pdf.php?p1&p=1000772	http://editions-rnti.fr/render_pdf.php?p=1000772
Revue des Nouvelles Technologies de l'Information	FDO	2009	A combination of opinion mining and social network techniques for discussion analysis	Mining opinion data that reside in online discussions is a way to trackopinions of people on specific subjects. Many of the existing techniques modela discussion as a social network of users and they represent it with a user-basedgraph. In this paper we propose a new framework for discussion analysis. Wecombine Social Network Analysis and Opinion Mining in order to give structureto a discussion. Such techniques have not been combined until now.We proposethe use of an opinion-based graph whose vertices contain message objects and its«reply-to» edges are labeled with opinion polarities. We compare the opinionbasedwith the user-based graphs and we analyze the different information thatcan be extracted from them. Our experiments validate the proposed frameworkand show that the representation of discussions by opinion-based graphs givesinformation that cannot be provided by a user-based graph.	Anna Stavrianou, Julien Velcin, Jean-Hugues Chauchat	http://editions-rnti.fr/render_pdf.php?p1&p=1000878	http://editions-rnti.fr/render_pdf.php?p=1000878
Revue des Nouvelles Technologies de l'Information	FDO	2009	Analyse de discours évaluatif, modèle linguistique et applications	Notre étude porte sur le discours évaluatif. L'approche que nous suivonsest celle d'une analyse symbolique en vue d'un traitement sémantique del'expression d'opinions.Un travail préliminaire d'observation sur corpus a permis une première modélisationlinguistique qui révèle la complexité du phénomène étudié. Nous en décrivonsune mise en oeuvre destinée à constituer un outil d'aide à l'observationpour l'expert linguiste et permettre un retour sur le modèle.Nous présentons et discutons ensuite deux approches informatiques testées pourla classification de documents d'opinions dans le cadre du défi DEFT07. Lapremière s'appuie sur des n-grammes de mots, la deuxième reprend en partie lemodèle linguistique, complété par un processus de classification fondé sur desrègles d'association généralisées.	Stéphane Ferrari, Thierry Charnois, Yann Mathet, François Rioult, Dominique Legallois	http://editions-rnti.fr/render_pdf.php?p1&p=1000880	http://editions-rnti.fr/render_pdf.php?p=1000880
Revue des Nouvelles Technologies de l'Information	FDO	2009	Approches Statistique et Linguistique Pour la Classification de Textes d'Opinion Portant sur les Films	Les sites communautaires sont par nature des lieux consacrés à l'expressionet au partage d'avis et d'opinions. www.flixster.com est un exemple desite participatif où se retrouvent chaque jour des dizaines de millions de fansdans le but de partager leurs impressions et sentiments sur les films. Une étudeapprofondie de cette richesse d'information permettrait une meilleure connaissancedes utilisateurs, de leurs attentes, de leurs besoins. Pour y parvenir, uneétape nécessaire est la classification automatique d'opinion.Dans ce papier nousdécrivons trois approches permettant de classer des textes selon l'opinion qu'ilsexpriment. La première approche consiste à étiqueter lesmots porteurs d'opinionà l'aide de techniques linguistiques, ces mots permettant par la suite de classerles textes. La deuxième approche est basée sur des techniques statistiques. Ladernière approche est une approche hybride qui combine approche linguistique,pour prétraiter le corpus, et approche statistique, afin de classer les textes.	Damien Poirier, Françoise Fessant, Cécile Bothorel, Emilie Guimier De Neef, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000884	http://editions-rnti.fr/render_pdf.php?p=1000884
Revue des Nouvelles Technologies de l'Information	FDO	2009	Catégorisation des évaluations dans un corpus de blogs multi-domaine	Dans le cadre de la fouille d'opinion, nous proposons une méthodeautomatique pour la détection et la catégorisation des évaluations localementexprimées dans un corpus de blogs multi-domaine. Cette méthode s'appuie surdeux théories linguistiques modélisant le processus d'évaluation dans le langagenaturel et sur des ressources lexicales. Nous présentons deux composants logicielsqui s'intègrent à la plateforme UIMA 1 et qui permettent : l'extractionautomatique de structures symboliques spécifiques à l'expression évaluative etla catégorisation des évaluations à partir des structures symboliques apprises.L'outil de catégorisation vise en particulier à analyser la signification axiologique,la modalité et la configuration énonciative d'une évaluation. L'objectif àterme est d'associer les évaluations ainsi catégorisées à leur sujet.	Matthieu Vernier, Laura Monceaux, Béatrice Daille, Estelle Dubreil	http://editions-rnti.fr/render_pdf.php?p1&p=1000879	http://editions-rnti.fr/render_pdf.php?p=1000879
Revue des Nouvelles Technologies de l'Information	FDO	2009	DEFT'07 : une campagne d'évaluation en fouille d'opinion	Depuis 2005, les campagnes nationales d'évaluation « DEFT » proposentdes thématiques de recherche exploratoires axées sur la fouille de texte.L'édition 2007 a porté sur la classification de textes d'opinion : la tâche consistaità attribuer une classe d'opinion à chaque texte d'un corpus, parmi 2 ou 3 classesallant d'un jugement défavorable à un jugement favorable. Quatre corpus ont étémis à la disposition des participants : débats parlementaires sur un projet de loi,critiques de jeux vidéos, critiques de films et de livres, et relectures d'articlesde conférences. Dans cet article, nous décrivons d'abord la phase préparatoirede la campagne, avec la collecte des corpus, la définition des mesures d'évaluation,et des tests humains de la tâche. Nous présentons ensuite une analyse desrésultats des participants, et les remarques qui en découlent concernant les différentstypes de corpus. Enfin, nous faisons un bilan synthétique des méthodesproposées à l'évaluation.	Cyril Grouin, Martine Hurault-Plantet, Patrick Paroubek, Jean-Baptiste Berthelin	http://editions-rnti.fr/render_pdf.php?p1&p=1000876	http://editions-rnti.fr/render_pdf.php?p=1000876
Revue des Nouvelles Technologies de l'Information	FDO	2009	Extracting Opinions and Facts for Business Intelligence	Finding information about companies on multiple sources on theWeb has become increasingly important for business analysts. In particular,since the emergence of the Web 2.0, opinions about companies and their servicesor products need to be found and distilled in order to create an accuratepicture of a business entity. Without appropriate text mining tools, companyanalysts would have to read hundreds of textual reports, newspaper articles, forums'postings and manually dig out factual as well as subjective information.This paper describes a series of experiments to assess the value of a number oflexical, morpho-syntactic, and sentiment-based features derived from linguisticprocessing and from an existing lexical database for the classification of evaluativetexts. The paper describes experiments carried out with two different websources: one source contains positive and negative opinions while the other containsfine grain classifications in a 5-point qualitative scale. The results obtainare positive and in line with current research in the area. Our aim is to use theresult of classification in a practical application that will combine factual andopinionated information in order to create the reputation of a business entity.	Adam Funk, Horacio Saggion	http://editions-rnti.fr/render_pdf.php?p1&p=1000882	http://editions-rnti.fr/render_pdf.php?p=1000882
Revue des Nouvelles Technologies de l'Information	FDO	2009	Extraction de sentiments et d'opinions basée sur des règles	Nous présentons ici trois méthodes différentes pour effectuer une classificationautomatique de textes d'opinion. La première méthode est symbolique,la seconde statistique et la dernière, hybride, est une combinaison des deux premières.Nous montrons comment la combinaison des méthodes symbolique etstatistique permet de tirer parti des avantages des deux méthodes, à savoir larobustesse de l'apprentissage automatique statistique et la possibilité de configurationmanuelle offerte par la méthode symbolique, permettant une utilisationdans des applications réelles. Les textes classés par ces méthodes viennent desources informationnelles non structurées de type forum sur Internet	Sigrid Maurel, Paolo Curtoni, Luca Dini	http://editions-rnti.fr/render_pdf.php?p1&p=1000881	http://editions-rnti.fr/render_pdf.php?p=1000881
Revue des Nouvelles Technologies de l'Information	FDO	2009	Recherche d'informations sur le Web basée sur des données d'opinion	Avec la diversification du contenu disponible sur le Web, aussi bienen quantité qu'en qualité, retrouver l'information pertinente est problématique.Les moteurs de recherche d'informations sur le Web ont comme objectif de permettrede retrouver efficacement l'information pertinente. Ces moteurs implémententdifférentes stratégies pour déterminer la pertinence des pages Web. Cesdernières sont classées en se basant principalement sur des critères d'évaluationautomatisés, ce qui crée un vide entre la pertinence effective d'un contenu et lapertinence calculée. Afin de réduire cet écart, nous présentons un cadre d'applicationde moteur de recherche où la pertinence d'une page Web est calculée àpartir des évaluations des internautes du contenu de la page (texte, image, audio,vidéo,...). Pour illustrer l'exploitation du cadre proposé, nous avons développéle moteur de recherche expérimental SocialSeeker(Adda, 2008). Finalement, uneétude empirique préliminaire du potentiel du prototype est présentée et discutée	Petko Valtchev, Rokia Missaoui, Mehdi Adda	http://editions-rnti.fr/render_pdf.php?p1&p=1000885	http://editions-rnti.fr/render_pdf.php?p=1000885
Revue des Nouvelles Technologies de l'Information	LMO	2009	Analyse et prédiction de l'impact de changements dans un système à objets : Approche probabiliste	Nous proposons dans cet article une approche probabiliste utilisant les réseaux bayésiens pour analyser et prédire les impacts des changements dans les systèmes à objets. Un modèle d'impact à été construit et des probabilités ont été affectées aux différents sommets du réseau. Des données récoltées sur un système réel sont utilisées pour étudier empiriquement des hypothèses (relations) de causalité entre d'une part, des attributs internes de logiciel, et d'autre part, l'impact du changement. Pour ce faire, plusieurs scénarios ont été exécutés sur le réseau. Les résultats obtenus  ont d'une part, confirmé certains résultats déjà trouvés lors de travaux antérieurs, mais d'autre part, remis en cause d'autres conclusions. Cette étude entre dans le cadre général d'une démarche tendant à proposer des modèles d'estimation de la qualité du produit logiciel ; elle montre qu'un modèle probabiliste constitue une alternative intéressante, aux modèles non probabiliste proposés dans la littérature.	M.K. Abdi, H. Lounis, Houari Sahraoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000762	http://editions-rnti.fr/render_pdf.php?p=1000762
Revue des Nouvelles Technologies de l'Information	LMO	2009	Contrôler la visibilité des aspects avec  Aspectboxes	La composition et l'interaction des aspects est un domaine de recherche très actif. Bien que plusieurs solutions existent, telles que l'agencement des aspects et des advices, les approches proposées par des langages à aspects supposent qu'une connaissance générale des aspects soit nécessaire pour pouvoir les composer, et même ceci ne permet pas d'éviter les interactions implicites résultant d'une composition. Cet article présente les aspectboxes, un mécanisme de visibilité pour aspects. L'unité élémentaire de visibilité est un aspectbox. Un aspectbox encapsule des définitions d'aspects. Un aspectbox peut être utilisé par d'autres aspectboxes pour aider la construction incrémentale de logiciel à base d'aspects. Une classe peut utiliser un aspectbox dans le but de bénéficier des aspects définis.	Alexandre Bergel	http://editions-rnti.fr/render_pdf.php?p1&p=1000754	http://editions-rnti.fr/render_pdf.php?p=1000754
Revue des Nouvelles Technologies de l'Information	LMO	2009	Essentials of Scala		Martin Odersky	http://editions-rnti.fr/render_pdf.php?p1&p=1000751	http://editions-rnti.fr/render_pdf.php?p=1000751
Revue des Nouvelles Technologies de l'Information	LMO	2009	Évaluation de efficacité des implémentations de l'héritage multiple en typage statique	La programmation par objets présente une apparente incompatibilité entre trois termes : l'héritage multiple, l'efficacité et l'hypothèse du monde ouvert -- en particulier, le chargement dynamique. Cet article présente des résultats d'expérimentations exhaustives comparant l'efficacité de différentes techniques d'implémentation (coloration, BTD, hachage parfait, ...) dans le contexte de différents schémas de compilation (de la compilation séparée avec chargement dynamique à la compilation purement globale). Les tests sont effectués avec et sur le compilateur du langage Prm. Ils confirment pour l'essentiel les résultats théoriques antérieurs tout en montrant une sur-additivité marquée des surcoûts. Les schémas d'optimisation globale démontrent un gain significatif par rapport à la coloration qui fait fonction de référence. Des techniques comme la simulation des accesseurs ou le hachage parfait entraînent un surcoût limité, mais la combinaison des deux double le surcoût total.	Floréal Morandat, Roland Ducournau, Jean Privat	http://editions-rnti.fr/render_pdf.php?p1&p=1000753	http://editions-rnti.fr/render_pdf.php?p=1000753
Revue des Nouvelles Technologies de l'Information	LMO	2009	Flexible Pointcut Implementation : An Interpreted Approach	One of the main elements of an Aspect-Oriented Programming (AOP) language or framework is its pointcut language. A pointcut is a predicate which selects program execution points and determines at which points the execution should be affected by an aspect. Experimenting with AspectJ shows that two basic primitive pointcuts, call and execution, dealing with method invocation from the caller and callee standpoints, respectively, lead to confusion. This is due to a subtle interplay between the use of static and dynamic types to select execution points, dynamic lookup, and the expectation to easily select the caller and callee execution points related to the same invocation. As a result, alternative semantics have been proposed but have remained paper design.In this article, we reconsider these various semantics in a practical way by implementing them using CALI, our Common Aspect Language Interpreter. This framework reuses both Java as a base language and AspectJ as a way to select the program execution points of interest. An additional interpretation layer can then be used to prototype interesting AOP variants in a full-blown environment. The paper illustrates the benefits of applying such a setting to the case of the call and execution pointcuts. We show that alternative semantics can be implemented very easily and exercised in the context of AspectJ without resorting to complex compiler technology.	Ali Assaf, Jacques Noyé	http://editions-rnti.fr/render_pdf.php?p1&p=1000755	http://editions-rnti.fr/render_pdf.php?p=1000755
Revue des Nouvelles Technologies de l'Information	LMO	2009	Génération de transformation de modèles par application de  l'ARC sur des exemples	Les transformations de modèles sont au coeur de l'ingénierie dirigée par les modèles. Elles sont habituellement développées par des programmeurs spécialisés et leur code doit être remis à jour lors de toute variation des besoins ou des métamodèles impliqués. Pour faciliter le développement de ces transformations, une approche possible consiste à générer des règles de transformation à partir d'exemples de transformation. L'avantage des exemples est qu'ils peuvent être d'une part écrits dans une syntaxe concrète, plus accessible pour l'utilisateur qu'un langage de transformation et d'autre part plus faciles à définir que les règles elles-mêmes. Dans cet article, nous spécifions une méthode qui utilise l'analyse relationnelle de concepts pour capturer et organiser par spécialisation les schémas récurrents qui apparaissent dans les exemples de transformation. Ces schémas récurrents peuvent ensuite être exploités pour la production de règles de transformation.	Xavier Dolques, Marianne Huchard, Clémentine Nebut	http://editions-rnti.fr/render_pdf.php?p1&p=1000756	http://editions-rnti.fr/render_pdf.php?p=1000756
Revue des Nouvelles Technologies de l'Information	LMO	2009	ILOG : vingt ans dans les objets,  rétro et perspectives	Nous prenons prétexte de la récente acquisition d'ILOG par IBM pour parcourir vingt années d'exploration des technologies objets à la frontière sans cesse mouvante de la recherche et de l'industrie. Retour sur un territoire sur lequel ILOG a fait office de passeur, d'accélérateur parfois, et à l'occasion d'explorateur.	Patrick Albert	http://editions-rnti.fr/render_pdf.php?p1&p=1000750	http://editions-rnti.fr/render_pdf.php?p=1000750
Revue des Nouvelles Technologies de l'Information	LMO	2009	Les Zero-Safe Nets pour la Préservation de la TTC dans les  Diagrammes d'Activité d'UML	Avec les extensions d'UML 2.0, les réseaux de Petri, utilisés comme cadre sémantique formel pour les diagrammes d'activité d'UML, ne permettent plus d'exprimer les nouvelles constructions de haut niveau telles que la traverse-to-completion. Cette dernière nécessite une synchronisation globale entre les noeuds fork et join, totalement absente dans les réseaux de Petri ordinaires offrant une synchronisation locale. Afin de préserver le comportement des diagrammes d'activité, nous proposons l'adoption des réseaux de Petri zero-safe, une classe particulière des réseaux de Petri. Nous définissons un passage générique des diagrammes d'activité d'UML vers cette classe des réseaux de Petri. La formalisation que nous proposons assure la préservation de la sémantique opérationnelle des diagrammes d'activité en mettant l'accent sur le principe de la traverse-to-completion et la synchronisation des noeuds fork et join supportant ainsi les flots de contrôle et donnée et la concurrence des threads déclenchés par le noeud fork.	Sabine Boufenara, Faiza Belala, Chafia Bouanaka	http://editions-rnti.fr/render_pdf.php?p1&p=1000758	http://editions-rnti.fr/render_pdf.php?p=1000758
Revue des Nouvelles Technologies de l'Information	LMO	2009	Matrice de dépendances enrichie	Les matrices de dépendance (DSM - Dependency Structure Matrix), développées dans le cadre de l'optimisation de processus, ont fait leurs preuves pour identifier les dépendances logicielles entre des packages ou des sous-systèmes. Il existe plusieurs algorithmes pour structurer une matrice de façon à ce qu'elle reflète l'architecture des éléments analysés et mette en évidence des cycles entre les sous-systèmes. Cependant, les implémentations de matrices de dépendance existantes manquent d'informations importantes pour apporter une réelle aide au travail de réingénierie. Par exemple, le poids des relations qui posent problème ainsi que leur type ne sont pas clairement présentés. Ou encore, des cycles indépendants sont fusionnés. Il est également difficile d'obtenir une visualisation centrée sur un package. Dans ce papier, nous améliorons les matrices de dépendance en ajoutant des informations sur (i) le type de références, (ii) le nombre d'entités référençantes, (iii) le nombre d'entités référencées. Nous distinguons également les cycles indépendants. Ce travail a été implémenté dans l'environnement de réingénierie open-source Moose. Il a été appliqué à des études de cas complexes comme le framework Morphic UI contenu dans les environnements Smalltalk open-source Squeak et Pharo. Les résultats obtenus ont été appliqués dans l'environnement de programmation Pharo et ont mené à des améliorations.	Jannik Laval, Alexandre Bergel, Stéphane Ducasse, Romain Piers	http://editions-rnti.fr/render_pdf.php?p1&p=1000759	http://editions-rnti.fr/render_pdf.php?p=1000759
Revue des Nouvelles Technologies de l'Information	LMO	2009	Méta-modélisation de la transformation de modèles  par l'exemple : approche par méta-heuristiques	La plupart des contributions en transformation de modèles sont concernées par la définition de langages pour exprimer des règles de transformation. La définition de ces règles est une tâche difficile, car de nombreux problèmes, liés à l'écriture/génération des règles, doivent être anticipés surtout dans le cas des formalismes source/cible qui ne sont pas largement utilisés. Dans cet article, nous proposons de considérer le problème de transformation comme un problème d'optimisation combinatoire où un modèle cible peut être automatiquement généré à partir d'un nombre réduit d'exemples de transformations. Nous proposons en particulier, une méta-modélisation de notre approche MOTOE, basée sur la méta-heuristique recuit simulé, qui combine un ensemble de solutions de transformation pour converger vers une solution optimale tenant compte de la cohérence entre les transformations. Les résultats de la validation sur des données industrielles montrent que les modèles obtenus sont comparables à ceux proposés par les experts de notre partenaire industriel.	Marouane Kessentini, Houari Sahraoui, Mounir Boukadoum	http://editions-rnti.fr/render_pdf.php?p1&p=1000757	http://editions-rnti.fr/render_pdf.php?p=1000757
Revue des Nouvelles Technologies de l'Information	LMO	2009	Prévention du déréférencement de références nulles dans un langage à objets	Le déréférencement de références nulles est une erreur de programmation courante dans les langages à objets. Pour la prévenir, certaines approches garantissent statiquement son absence à l'aide de systèmes de types ou d'annotations mais réduisent l'expressivité du langage. D'autres approches analysent plutôt le code source pour identifier les erreurs potentielles, mais peuvent trouver des faux-positifs et ne garantissent pas l'absence d'erreurs à l'exécution. Dans cet article, nous proposons une approche offrant la garantie statique d'absence d'erreur de déréférencement dans une grande portion du code. L'approche consiste en un système de types statiques simple, des vérifications dynamiques et un opérateur de test dynamique. En plus de préserver l'expressivité du langage, notre approche limite la zone de danger à la construction des instances et permet la détection précoce des erreurs à l'exécution. Nos mesures expérimentales démontrent une grande étendue des garanties statiques et l'efficacité de la détection précoce des erreurs.	Jean-Sébastien Gélinas, Étienne M. Gagnon, Jean Privat	http://editions-rnti.fr/render_pdf.php?p1&p=1000752	http://editions-rnti.fr/render_pdf.php?p=1000752
Revue des Nouvelles Technologies de l'Information	LMO	2009	Réflexions autour de la construction dirigée par les modèles  d'un atelier de composition d'orchestrations	Il est aujourd'hui courant d'utiliser une approche «orientée service» pour définir des applications complexes. Dans ce cadre, les services constituent les unités de base, assemblés par des mécanismes de plus haut niveau comme les orchestrations. Nous présentons ici un atelier logiciel construit dans une démarche dirigée par les modèles permettant la composition d'orchestrations de services, et discutons les choix effectués ainsi que les difficultés rencontrées lors de sa mise en oeuvre.	Sébastien Mosser, Mireille Blay-Fornarino	http://editions-rnti.fr/render_pdf.php?p1&p=1000780	http://editions-rnti.fr/render_pdf.php?p=1000780
Revue des Nouvelles Technologies de l'Information	SFdS	2009	Asymptotic properties of functional maximum-likelihood ARH parameter estimators	Dans cet article, nous nous intéressons au problème du calcul de lavariance asymptotique des estimateurs du maximum de vraisemblance des paramètresde projection de processus autorégressifs hilbertiens. Ces estimateursobtenus à partir d'observations fonctionnelles gaussiennes incomplètes ont étéétudiés dans Ruiz-Medina et Salmerón (2010). Notre approche consiste à estimerces variances à l'aide d'une version fonctionnelle de l'algorithme SEM(Supplemented Expectation Maximization) par Meng et Rubin (1991). La miseen oeuvre de l'algorithme est basée sur l'extraction des fonctions propres et valeurspropres de l'opérateur d'autocorrélation et de son adjoint. Les effets del'ordre de troncature du spectre et de la discrétisation des fonctions sont illustréspar des simulations.	M.D.Ruiz-Medina, R. Salmerón	http://editions-rnti.fr/render_pdf.php?p1&p=1001042	http://editions-rnti.fr/render_pdf.php?p=1001042
Revue des Nouvelles Technologies de l'Information	SFdS	2009	Classification en référence à une matrice stochastique	Étant donné un tableau de données portant sur un ensemble d'objets etune matrice stochastique qui peut être assimilée à une matrice de passage d'unemarche aléatoire, nous proposons une méthode de partitionnement consistant àidentifier les différents états stationnaires associés à la matrice stochastique. Lesclasses formant la partition sont déterminées à partir de ces états stationnaires.De manière pratique, la matrice stochastique peut être définie à partir d'une matricede mesure de ressemblance entre les objets du tableau de données. Différentesmesures sont étudiées (plus proches voisins, noyaux de densité...). L'approcheest présentée sur la base des propriétés des graphes et illustrée sur desdonnées simulées. La méthode de classification proposée est également comparéeà une autre approche de classification, la classification spectrale	Stéphane Verdun, Véronique Cariou, El Mostafa Qannari	http://editions-rnti.fr/render_pdf.php?p1&p=1001045	http://editions-rnti.fr/render_pdf.php?p=1001045
Revue des Nouvelles Technologies de l'Information	SFdS	2009	De l'interprétation statistique des données à une modélisation probabiliste actualisée par les données : la synthèse bayésienne. Principes et exemples.	Statisticiens plongés dans un domaine d'application où les donnéessont rares et hétérogènes, nous proposons d'utiliser successivement deux approchesbayésiennes complémentaires : les réseaux bayésiens et la statistiquebayésienne. Dans une première partie, les deux approches sont brièvement rappeléespour montrer qu'à part le théorème de Bayes, elles n'ont, dans nos acceptionsdes deux concepts, rien en commun. S'appuyant ensuite sur la modélisationdu nombre de campylobactérioses en France liées à la consommation de poulets,un nouveau point de vue est suggéré pour l'interprétation des données. Il s'agit(1) de modéliser en soi le phénomène d'intérêt à l'aide d'un réseau bayésien ;puis (2) de l'étendre pour définir la vraisemblance des données disponibles etextraire l'information qu'elles contiennent par conditionnement, c'est-à-dire enappliquant le principe de la statistique bayésienne. R et les logiciels de la familleBUGS se révèlent bien adaptés pour la réalisation pratique de cette proposition.	Jean-Baptiste Denis, Isabelle Albert	http://editions-rnti.fr/render_pdf.php?p1&p=1001047	http://editions-rnti.fr/render_pdf.php?p=1001047
Revue des Nouvelles Technologies de l'Information	SFdS	2009	Mélange de distributions comme fonction d'importance dans l'échantillonnage préférentiel combiné avec l'algorithme de Monte Carlo par Chaîne de Markov	Les algorithmes de Monte Carlo par Chaîne de Markov (MCMC) sonttrès souvent utilisés pour estimer les lois a posteriori ainsi que leurs momentsdans le cadre d'un modèle Bayésien. En effet, selon les modèles, les lois a posterioriou leurs moments peuvent ne pas avoir d'expression analytique et le recoursa des méthodes d'approximation est donc indispensable. Lors de l'étudeempirique d'estimateurs, différents jeux donnés sont simulés sous le même modèle(et avec les mêmes valeurs de paramètres) et pour chaque jeu de données,les estimations a posteriori des paramètres sont obtenus via MCMC. Cette procédureest réitérée pour d'autres valeurs des paramètres. Globalement, les temps decalcul peuvent être très importants. L'échantillonnage préférentiel (ImportanceSampling en anglais, IS) combiné avec les algorithmes MCMC est une solutionpermettant de réduire ce temps de calcul. En effet, l'IS nécessite le choix d'unefonction d'importance que nous proposons construite comme mélange de loisa posteriori présélectionnées sur quelques jeux données simulés, lois a posterioridéjà estimées via MCMC. Les autres calculs ne nécessitent plus le recoursaux algorithmes MCMC. Les approches évoquées ici sont illustrées sur deuxexemples de modèles de Poisson.	Dorota Gajda, Chantal Guihenneuc-Jouyaux, Judith Rousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001043	http://editions-rnti.fr/render_pdf.php?p=1001043
Revue des Nouvelles Technologies de l'Information	SFdS	2009	Modèle à processus latent et algorithme EM pour la régression non linéaire	Cet article propose une méthode de régression non linéaire qui s'appuiesur un modèle intégrant un processus latent qui permet d'activer préférentiellement,et de manière souple, des sous-modèles de régression polynomiaux.Les paramètres du modèle sont estimés par la méthode du maximum de vraisemblancemise en oeuvre par un algorithme EM dédié. Une étude expérimentalemenée sur des données simulées et des données réelles met en évidence debonnes performances de l'approche proposée	Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin	http://editions-rnti.fr/render_pdf.php?p1&p=1001041	http://editions-rnti.fr/render_pdf.php?p=1001041
Revue des Nouvelles Technologies de l'Information	SFdS	2009	On the way to remote access to German official microdata: a glimpse of work in progress	A number of methodological and technical preconditions have to be met in order to provide (automated) remote data access. This paper outlines the German approach to achieving this goal with a strong focus on the scientific and methodological challenges, in particular, regarding the access to cross-sectional and longitudinal economic statistics. The paper concentrates mainly on the generation of anonymised data with a structure similar to that of the real data, which are made available to data users. The data are provided in the form of what are called data structure files, which can be produced by using specific variants of microaggregation, multiplicative stochastic noise and multiple imputation. These data structure files are sent to the researcher after he has submitted a request for remote access. They allow checking a program code for syntactic and semantic errors before it can be finally applied to the original data by remote execution.	Rainer Lenz	http://editions-rnti.fr/render_pdf.php?p1&p=1001048	http://editions-rnti.fr/render_pdf.php?p=1001048
Revue des Nouvelles Technologies de l'Information	SFdS	2009	Sélection de variables avec lasso dans la régression logistique conditionnelle	Nous proposons une procédure de sélection de modèle dans le cadredes études cas-témoin appariées et, plus précisément, pour la régression logistiqueconditionnelle. La méthode se base sur une pénalisation de type L1 descoefficients de régression dans la vraisemblance conditionnelle. Cette pénalisation,permettant d'éliminer de façon automatique les variables considérées nonpertinentes, est particulièrement adaptée aux problèmes où le nombre de variablesexplicatives est élevé (par rapport au nombre d'événements) ou en casde colinéarité. Des méthodes de rééchantillonnage sont appliquées pour le choixdu terme de régularisation ainsi que pour l'évaluation de la stabilité du modèlesélectionné. La mise en oeuvre de la méthode est illustrée par deux exemples.	Marta Avalos	http://editions-rnti.fr/render_pdf.php?p1&p=1001040	http://editions-rnti.fr/render_pdf.php?p=1001040
Revue des Nouvelles Technologies de l'Information	SFdS	2009	Tests d'ajustement fondés sur la méthode Monte Carlo randomisée pour des distributions exponentielles	La distribution exponentielle est largement utilisée pour la modélisationde données sur la durée des événements statistiques, en statistique, enéconométrie et en finance. Ainsi, les tests d'exponentialité sont un importantproblème lorsque l'on s'intéresse à l'étude des données. Cependant, la plupartdes tests proposés sont limités à la distribution exponentielle avec un seul paramètreet les tableaux de valeurs critiques sont disponibles uniquement pourun nombre limité de tailles d'échantillons. Ceci peut être la source d'importantsproblèmes de niveaux et de puissances des tests.Dans cette étude, nous proposons d'abord l'utilisation de la technique des testsde Monte Carlo randomisés, en vue de contrôler la taille de différents tests d'exponentialitécomportant un param`tre d'échelle et un paramètre de localisation.Nous montrons que les tests obtenus de cette façon sont exacts pour toute tailled'échantillon. Nous proposons aussi des modifications de la procédure fondéessur des estimateurs de moments et nous montrons que ces modifications améliorentsensiblement les puissances de plusieurs tests proposés dans la littératureantérieure.Cette étude est achevée par la proposition de nouveaux tests basés sur les procéduresdes combinaisons de plusieurs statistiques de tests. Ces tests ont montréde très bonnes propriétés de puissance.	Jean-Marie Dufour, Abdeljelil Farhat	http://editions-rnti.fr/render_pdf.php?p1&p=1001044	http://editions-rnti.fr/render_pdf.php?p=1001044
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Analyse factorielle multiple de données mixtes : application à la comparaison de deux codages.	L'analyse factorielle multiple (AFM) est appliquée à un ensemble de variables d'échelles considérées à la fois comme quantitatives et qualitatives. On illustre ainsi une façon de prendre en compte, dans une analyse factorielle, ces deux types de variables simultanément en tant qu'éléments actifs.	Jérôme Pagès, Sergio Camiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001738	http://editions-rnti.fr/render_pdf.php?p=1001738
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Apport de la spatialisation des données en analyse multidimensionnelle pour évaluer l'impact des activités agricoles sur la teneur en nitrates des eaux	Commencé en 1994, le programme de maîtrise des pollutions d'origine agricole (PMPOA I et II) s'est terminé en décembre 2006. Il visait à limiter les risques de pollution par les effluents d'élevage des eaux souterraines par infiltrations et des eaux superficielles par écoulements. Si les travaux dans les élevages sont terminés pour le PMPOA I (période 1994-1999), ceux du PMPOA II (2002-2006) ne s'achèveront qu'en 2010. Seul le PMPOA I est donc soumis pour l'instant à l'évaluation de l'impact des mesures prises.Pour mesurer l'impact de ce programme sur l'évolution des teneurs en nitrates des eaux souterraines et des eaux superficielles, des analyses multidimensionnelles ont été réalisées en croisant les activités de productions agricoles (issues du recensement agricole 2000), les teneurs en nitrates (issues du réseau de surveillance directive nitrates pour les campagnes de 1996-1997 et 2004-2005), et en tenant compte de la proximité géographique entre cantons. Après un bref rappel du contexte, nous développons la méthodologie employée. Les résultats sont comparés à ceux d'une démarche n'intégrant pas les contiguïtés spatiales.	Marion Ferrand, Daniel Lequenne, Vincent Manneville, Philippe Jannot, Carlos Lopez	http://editions-rnti.fr/render_pdf.php?p1&p=1001753	http://editions-rnti.fr/render_pdf.php?p=1001753
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Avant le sudoku : le carré latin magique	Cette note met en évidence la similitude entre le jeu de sudoku, qui connaît un engouement considérable depuis 2005, et la notion de carré latin magique, qui a été présentée par Federer en 1955, dans le domaine de l'expérimentation agronomique.	Pierre Dagnelie	http://editions-rnti.fr/render_pdf.php?p1&p=1001760	http://editions-rnti.fr/render_pdf.php?p=1001760
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Comment extraire des connaissances à partir des concepts de vos bases de données ? Les deux étapes de l'analyse des données symboliques. 	Vos bases de données contiennent des concepts sous-jacents. Ils sont associés aux catégories issues de produits cartésiens de variables qualitatives ou de classifications automatiques. Ces concepts constituent alors des unités d'étude d'un niveau de généralité supérieur aux données initiales. Ce niveau est souvent désiré par les utilisateurs mais freinés par le carcan des données classiques qui ne tiennent pas compte de la variation des instances de ces concepts. L'analyse des données symboliques (ADS) a pour objectif dans une première étape de constituer ces concepts et de les décrire en prenant en compte leur variation interne par des variables dites « symboliques » (à valeur intervalle, histogramme, lois etc.) car non manipulables comme des nombres. La seconde étape d'une ADS consiste à les analyser. Pour cela on est amené à étendre les méthodes de la statistique exploratoire et de la fouille de données aux données symboliques (ces méthodes deviennent alors des cas particuliers d'ADS) et de développer des outils nouveaux spécifiques. On montre que ces données ne peuvent pas être réduites à des données classiques. On décrit les quatre espaces de la modélisation sous-jacente où les concepts sont modélisés par des objets symboliques, puis la modélisation mathématique des données (sous forme de variables à valeur variable aléatoire) et des classes ainsi que de leur structure en généralisant les treillis de Galois, hiérarchies, pyramides classiques aux données symboliques. On introduit leur classification spatiale étendant les cartes de Kohonen à des données et des structures pyramidales plus riches. On termine enfin par une application industrielle et la présentation du logiciel SODAS issu de deux projets européens d'EUROSTAT.	Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001730	http://editions-rnti.fr/render_pdf.php?p=1001730
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Corrélations partielles et "corrélations duales"	Par analogie avec l'interprétation géométrique de la « corrélation partielle », tirée de la formule fondamentale du triangle sphérique, on introduit la notion de « corrélation duale ». Les propriétés de ces corrélations, examinées d'abord sur un cas d'école, se révèlent un moyen souple d'investigation de données géochimiques et astronomiques. Quatre sources indépendantes de calcium et quatre sources indépendantes de radon, sont mises en évidences dans l'aérosol marin de l'Atlantique Nord. Une comparaison est faite avec le radon de l'île Amsterdam	Michel Lesty	http://editions-rnti.fr/render_pdf.php?p1&p=1001746	http://editions-rnti.fr/render_pdf.php?p=1001746
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	De la régression simple et l'analyse de la variance aux modèles linéaires généralisés, synthèse et chronologie	Ce document présente la filiation qui conduit de la régression simple et de l'analyse de la variance la plus élémentaire aux différentes formes de modèles linéaires : modèle linéaire général, modèle linéaire mixte et modèles linéaires généralisés. Il peut servir d'introduction, pour les personnes peu compétentes en la matière, et de rappel ou de synthèse, pour les personnes plus averties.Á ces dernières personnes, ce document peut suggérer une présentation éventuellement utile dans une optique de vulgarisation, en particulier dans certains enseignements de base et lors de contacts de type "consultation statistique". Il nous semble en effet que, dans de nombreuses circonstances, les modèles les plus élaborés ont intérêt a être présentés en partant des modèles les plus simples, et non pas de façon trop abrupte.	Pierre Dagnelie	http://editions-rnti.fr/render_pdf.php?p1&p=1001757	http://editions-rnti.fr/render_pdf.php?p=1001757
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Entre réalisme métrologique et conventions d'équivalence : les ambiguïtés de la sociologie quantitative	La " méthodologie statistique " en sciences sociales est en général présentée en deux parties : d'une part, la construction des données, puis leur traitement et leur interprétation. Le monde de la " construction de données " possède deux façons de rendre compte de ses pratiques : la mesure, issue du langage des sciences de la nature, le codage conventionnel, inspiré, selon les cas, du droit, des sciences politiques, ou des sciences cognitives. Cette ambiguïté, caractéristique des sciences sociales quantitatives, peut être analysée à travers une mise en perspective historique, reliant les questions soulevées par la construction des données à celles qui résultent de la diversité des modes d'analyse. L'exemple des différences entre " régression logistique " et " analyse des données à la française " est notamment présenté ici.	Alain Desrosières	http://editions-rnti.fr/render_pdf.php?p1&p=1001722	http://editions-rnti.fr/render_pdf.php?p=1001722
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Etude préliminaire à un modèle de prévision à court terme de l'activité d'un transporteur sous température dirigée	Cet article décrit une problématique de recherche opérationnelle. Un transporteur sous température dirigée cherche à optimiser la planification de ses ressources humaines et matérielles par la prévision à très court terme de son activité. Le challenge réside dans le fait de trouver un modèle de prévision unique s'adaptant, sans intervention humaine, aux spécificités des 57 agences du transporteur. La matière première est l'information récoltée par le transporteur depuis plus de six ans. Les outils sont des algorithmes mathématiques utilisés pour la prévision des séries temporelles. Le travail décrit ici, vise à combiner ces outils pour qu'ils extraient le maximum d'information déterministe capable d'être anticipée.L'introduction pose la problématique et son contexte économique. Elle est suivie d'un descriptif des procédures utilisées et d'un argumentaire pour défendre leur choix. Les solutions informatiques adoptées sont inventoriées. Enfin, la conclusion renvoie à des pistes d'études.	Wilfried Despagne	http://editions-rnti.fr/render_pdf.php?p1&p=1001754	http://editions-rnti.fr/render_pdf.php?p=1001754
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Evaluation de la saisonnalité mobile due au calendrier lunaire sur les séries chronologiques par application du modèle espace d'état.	La décomposition des séries chronologiques en tendance, cycle, composante saisonnière et résidu, s'avère dans plusieurs cas, insuffisante dans la mesure où la présence d'autocorrélation est remarquée au niveau du résidu. L'extraction d'autres composantes est donc nécessaire. Parmi elles, il est possible de citer les composantes d'événements liés au calendrier Hégirien : Ramadan et les fêtes religieuses ainsi que la composante du calendrier. La particularité des premiers événements est leur mobilité par rapport au calendrier Grégorien. Dans cet article, le modèle espace d'état est utilisé pour modéliser des séries chronologiques mensuelles et les composantes précitées sont introduites. L'intérêt de ce modèle est qu'il permet de modéliser les effets fixes et variables. Les paramètres du modèle sont estimés par maximisation de la fonction vraisemblance en utilisant une méthode quasi newtonienne. Chaque composante n'est maintenue au modèle que si elle permet le respect des tests de validation. Le critère d'information de Akaike (AIC) est utilisé pour choisir entre les modèles qui vérifient ces tests.	Abedelhalim Skalli, Ibrahim Amrani	http://editions-rnti.fr/render_pdf.php?p1&p=1001748	http://editions-rnti.fr/render_pdf.php?p=1001748
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	L'industrialisation des analyses - Besoins, outils & applications	Le data mining est aujourd'hui de plus en plus utilisé dans les entreprises les plus compétitives. Ce développement, rendu possible par la disponibilité grandissante de masses de données importantes, pose des contraintes tant théoriques (quels algorithmes utiliser pour produire des modèles d'analyses exploitant des milliers de variables pour des millions d'exemples) qu'opérationnelles (comment mettre en production et contrôler le bon fonctionnement de centaines de modèles). Je présenterai ces contraintes issues des besoins des entreprises ; je montrerai comment exploiter des résultats théoriques (provenant des travaux de Vladimir Vapnik) pour produire des modèles robustes; je donnerai des exemples d'applications réelles en gestion de la relation client. Nous verrons ainsi comment il est possible d'industrialiser le data mining et en faire ainsi un composant facilement exploitable dès qu'on dispose de données.	Françoise Fogelman-Soulié, Erik Marcadé	http://editions-rnti.fr/render_pdf.php?p1&p=1001732	http://editions-rnti.fr/render_pdf.php?p=1001732
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	La place de l'a priori dans l'analyse des données économiques ou le programme fort des méthodes inductives au service de l'hétérodoxie.	Le positionnement épistémologique de Jean-Paul Benzécri a été explicité dans un article écrit pour l'Encyclopedia Universalis qui fait figure de véritable manifeste car il en appelle à refonder la pratique des statisticiens sur la base d'un Novius Organum qui serait basé sur l'analyse des correspondances. L'hypothèse que nous formulons est que ce « Novius Organum » que Jean-Paul Benzécri appelle de ses voeux a fonctionné dans les faits plutôt comme un outil de déconstruction au service des tendances hétérodoxes de l'époque en économie, et plus généralement dans les sciences sociales.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001761	http://editions-rnti.fr/render_pdf.php?p=1001761
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	La qualité dans les enquêtes	Il existe une littérature académique très abondante sur la qualité des enquêtes. Mais il existe peu d'ouvrages traitant de l'ensemble des sources d'erreur dans une enquête. Les manuels de Théorie des Sondages sont essentiellement consacrés aux méthodes de sondage, c'est-à-dire aux méthodes de tirage de l'échantillon, aux méthodes d'estimation qui en découlent, et à l'erreur d'échantillonnage. Ils ne consacrent qu'un ou deux chapitres aux autres sources d'erreurs dans les enquêtes.Cet article a pour objectifs de donner une vue d'ensemble des différentes sources d'erreur pouvant affecter la qualité des enquêtes et des méthodes possibles pour les prévenir et/ou y remédier, et de permettre ainsi aux utilisateurs de données d'enquête d'en évaluer la qualité.	Anne-Marie Dussaix	http://editions-rnti.fr/render_pdf.php?p1&p=1001758	http://editions-rnti.fr/render_pdf.php?p=1001758
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	La régression Partial Least-Squares boostée	Ce papier présente la régression Partial Least-Squares (PLS) comme appartenant à la famille de méthodes de boosting à fonction coût L2. D'une part, la régression PLS linéaire classique appartient à cette catégorie en considérant une variable latente ou composante principale comme base d'apprentissage (base learner) rendant robuste le modèle face au problème de la rareté des données et de la multi-corrélation des variables. D'autre part, l'usage des B-splines et de leurs produits tensoriels dans la construction de la base d'apprentissage, exploite de façon naturelle le potentiel du boosting L2 de PLS pour produire des modèles non-linéaires additifs qui capturent les effets principaux ainsi que les interactions significatives. La performance du boosting PLS en régression comme en classification supervisée est montrée sur trois exemples.	Jean-François Durand	http://editions-rnti.fr/render_pdf.php?p1&p=1001727	http://editions-rnti.fr/render_pdf.php?p=1001727
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Le plan d'expérience évolue... 	Cet article, de type tutorial ou review, esquisse l'évolution du concept de plan d'expérience, essentiellement dans le domaine agronomique et dans le domaine industriel, en partant des principes développés par Ronald Aylmer Fisher au cours des années 1920.Dans un premier temps, nous donnons quelques indications relatives aux années antérieures à 1920 (paragraphe 2.1), nous rappelons ce qu'a été l'apport de Fisher (paragraphe 2.2), et nous présentons les orientations qui ont fait suite à ses travaux (paragraphes 2.3 à 2.6). Nous voyons alors la manière dont les principes de Fisher ont été relativement négligés dans le domaine industriel, en matière de répétition (paragraphe 3.1), ainsi qu'en matière de randomisation et de blocking (paragraphe 3.2). Après quoi, nous mettons en évidence quelques pistes qui permettent de remédier dans une certaine mesure à la situation observée, à savoir l'utilisation du principe du split-plot (paragraphes 4.1 et 4.2), de nouvelles formes de blocking (paragraphe 4.3), et le recours à des dispositifs expérimentaux insensibles à certaines dérives (paragraphe 4.4). Nous terminons par quelques conclusions et recommandations (paragraphe 5), et par une assez importante bibliographie.Diverses informations complémentaires sont également données en annexe.	Pierre Dagnelie	http://editions-rnti.fr/render_pdf.php?p1&p=1001721	http://editions-rnti.fr/render_pdf.php?p=1001721
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Les sinistres graves en assurance automobile : une nouvelle approche par la théorie des valeurs extrêmes	La construction des classes de risque en assurance automobile est stratégique pour que le principe de mutualisation soit fonctionnel dans cet environnement concurrentiel. Ces classes, constituées à partir de caractéristiques de l'assuré et du véhicule, sont supposées homogènes en termes de sinistralité.La présence de sinistres graves (rares) dans une classe vient perturber cette hypothèse d'homogénéité des classes et de stabilité des indicateurs de risque comme la prime pure. En général, face à de tels événements, les assureurs effectuent des écrêtements et répartissent la charge sur l'ensemble du portefeuille, mais la question se pose de déterminer ce qu'est un sinistre grave pour une classe de risque donnée afin d'assurer une certaine stabilité des indicateurs de sinistralité et donc une adéquation entre la prime de référence et la sinistralité.La théorie des valeurs extrêmes permet d'obtenir des estimations fiables d'événements rares. Trois méthodes classiques (valeurs record, moyenne des excès, approximation par la loi de Pareto généralisée) sont proposées pour la détermination d'un seuil d'écrêtement au-delà duquel un événement est considéré comme atypique. A priori, cette démarche n'a pas été utilisée en assurance automobile.Ces méthodes sont comparées sur des données du portefeuille d'une mutuelle d'assurance française, puis une nouvelle méthode basée sur une combinaison convexe des précédentes, qui minimise la variance, est proposée, permettant ainsi de décider entre ces différentes stratégies.	Noureddine Benlagha, Michel Grun-Réhomme, Olga Vasechko	http://editions-rnti.fr/render_pdf.php?p1&p=1001751	http://editions-rnti.fr/render_pdf.php?p=1001751
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	L'analyse des correspondances multiples « à la hollandaise »: introduction à l'analyse d'homogénéité.	L'analyse des correspondances multiples est une méthode exploratoire multidimensionnelle qui fournit une représentation synthétique des catégories issues d'une batterie de critères qualitatifs, référentiel d'un protocole d'expérimentation ou d'enquête. Cette note a pour but d'aider les utilisateurs de SPSS dans la mise en oeuvre de l'analyse des correspondances multiples au moyen de l'analyse d'homogénéité (procédure HOMALS du logiciel SPSS ). Cette mise en oeuvre concerne l'analyse de tableaux de données construits à partir de variables nominales. L'équivalence entre l'analyse d'homogénéité et l'analyse des correspondances multiples est illustrée à partir d'un exemple répertorié dans la littérature statistique. La note est complétée par un exposé algébrique consacré à l'analyse d'homogénéité.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001743	http://editions-rnti.fr/render_pdf.php?p=1001743
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Préface		Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001719	http://editions-rnti.fr/render_pdf.php?p=1001719
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Quelques défis des indicateurs scientométriques : « déminage des données », mesure des flux de connaissance, questions de diversité	La scientométrie et la bibliométrie sont confrontées à la fois à un fort développement de la demande d'indicateurs (en évaluation de la recherche, en économie des sciences et de l'innovation) et à l'apparition de nouvelles formes d'offre (sources et statistiques sur les publications scientifiques ; développements Internet et outils en ligne). Cette situation ouvre des perspectives contrastées, d'un côté les "hit-parades" spectaculaires et une certaine frénésie du chiffre, de l'autre des systèmes d'évaluation élaborés et prudents, ancrés dans une meilleure compréhension de la diversité et de la dynamique des systèmes scientifiques. Cet article esquisse quelques-uns des défis rencontrés par les indicateurs scientométriques : déminage des données, mesure des flux de connaissance, questions de diversité. La réponse à ces défis conditionne la mise au point d'indicateurs fiables. Elle ne prémunit pas, toutefois, contre les dérives dans leur utilisation.	Michel Zitt, Elise Bassecoulard	http://editions-rnti.fr/render_pdf.php?p1&p=1001726	http://editions-rnti.fr/render_pdf.php?p=1001726
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Recherche d'une typologie de comportement face au soleil des volontaires de la cohorte SU.VI.MAX.	Une exposition solaire excessive engendre une accélération du vieillissement et une augmentation du risque de survenue de tumeurs cutanées. Dans le but d'estimer le risque lié à différents types de comportement, une typologie de comportement d'exposition et de protection solaire a été recherchée. Un questionnaire explorant ce sujet auprès d'hommes et de femmes adultes français a été développé dans le cadre de l'étude épidémiologique SU.VI.MAX. Une analyse des correspondances multiples a été effectuée pour résumer l'information. Puis, une classification ascendante hiérarchique (méthode de Ward) a été réalisée à partir des composantes principales retenues. Ensuite, un arbre de décision a été construit afin d'affecter facilement n'importe quel individu à une classe (algorithme CART). Sept types de comportement ont été identifiés pour les femmes, et six pour les hommes. Ces résultats nous permettront de cibler des groupes d'individus à risque pour des campagnes d'information de santé publique et/ou des études d'intervention.	Julie Latreille, Emmanuelle Mauger, Denis Malvy, Laurence Ambroisine, Pilar Galan, Serge Hercberg, Michel Tenenhaus, Christiane Guinot	http://editions-rnti.fr/render_pdf.php?p1&p=1001742	http://editions-rnti.fr/render_pdf.php?p=1001742
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Scoring sur données d'entreprises : instrument de diagnostic individuel et outil d'analyse de portefeuille d'une clientèle	La plupart des études statistiques sur le risque de crédit et l'analyse discriminante se concentrent sur les techniques de construction d'un score. Or la réalisation d'un outil efficace de détection du risque doit relier cette construction à l'utilisation future de cet instrument et les propriétés qu'il doit en conséquence nécessairement satisfaire. Les utilisateurs d'un scoring sur données d'entreprises seront des décideurs, pour la plupart experts en analyse financière, ou responsables du risque de crédit dans les banques, ou superviseurs bancaires. Le présent article s'efforce de relier construction et utilisation en mettant en exergue les propriétés requises pour les utilisateurs et les implications techniques qu'elles entraînent. Il en résultera une réflexion sur la sélection des données, sur le choix du modèle, sur l'estimation de la probabilité de défaut. Dès lors l'utilisation de l'outil sera approfondie sous deux aspects : le diagnostic individuel de l'entreprise et l'analyse du risque d'une population d'entreprises emprunteuses. Enfin le score comme indicateur probabilisé du risque de crédit joue un rôle important dans les recherches en économie.Beaucoup des problèmes abordés ici se rencontrent sur d'autres champs d'application de l'analyse discriminante. Toutefois il convient de s'adapter dans chaque cas aux spécificités de l'utilisation.	Mireille Bardos	http://editions-rnti.fr/render_pdf.php?p1&p=1001735	http://editions-rnti.fr/render_pdf.php?p=1001735
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Si j'avais un laboratoire...		Jean-Paul Benzecri	http://editions-rnti.fr/render_pdf.php?p1&p=1001720	http://editions-rnti.fr/render_pdf.php?p=1001720
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2008	Utilisation des modèles à équations structurelles en analyse sensorielle. Michel TENENHAUS.	Deux écoles concurrentes se sont imposées dans le domaine de la modélisation des équations structurelles. La première approche, appelée « Covariance-based SEM » (SEM = Structural Equation Modeling), s'est développée autour de Karl Jöreskog. Cette approche a pour objectif de modéliser la matrice de covariance des variables observées. Elle peut être considérée comme une généralisation de l'analyse factorielle en facteurs communs et spécifiques au cas de plusieurs tableaux de données reliés entre eux par des liens de causalité. La seconde approche s'est développée autour de Herman Wold sous le nom de « PLS » (Partial Least Squares), puis de « Component-based SEM ». Elle a pour objectif la construction de scores résumant au mieux chaque bloc de variables tout en tenant compte du réseau de causalité. C'est une généralisation de l'analyse en composantes principales au cas de plusieurs tableaux de données reliés entre eux par des liens de causalité. Plus récemment Hwang et Takane (2004) ont proposé une méthode « Component-based SEM » : la méthode GSCA (Generalized Structural Component Analysis). Elle permet une recherche de scores optimisant un critère global. Nous allons discuter de l'utilisation de ces trois approches en donnant des exemples d'utilisation en analyse sensorielle	Michel Tenenhaus	http://editions-rnti.fr/render_pdf.php?p1&p=1001729	http://editions-rnti.fr/render_pdf.php?p=1001729
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Construction d'attributs pour l'extraction de connaissances à partir de séquences biologiques	Dans cet article nous étudions un problème de prétraitement dedonnées : la construction d'attributs décrivant des séquences biologiques. Afind'assurer l'extraction de connaissances à partir de séquences biologiques (ADN,ARN et protéines), tout système de fouille de données (datamining) se confronteà la représentation non habituelle de ce type de données. Une séquencebiologique est représentée, en structure primaire, par une chaîne de caractères.La construction d'attributs décrivant les séquences biologiques est une étape deprétraitement inévitable. Dans cet article, nous étudions les méthodes existantesde construction d'attributs décrivant des séquences biologiques, notamment,celles qui se basent sur les n-grammes, l'arbre de suffixes généralisés et lesmodèles de Markov cachés. Notre contribution dans cet axe a été la propositionde la méthode des descripteurs discriminants et la présentation d'une étudecomparative approfondie de ces méthodes en les appliquant à des problèmesbiologiques typiques comme la reconnaissance de sites promoteurs des gènesde E. Coli, la reconnaissance de sites de jonction de Primate et la classificationdes protéines. Une confrontation des résultats de chaque méthode avec labanque de motifs Pfam sera aussi présentée.	Mondher Maddouri, Faouzi Mhamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1000499	http://editions-rnti.fr/render_pdf.php?p=1000499
Revue des Nouvelles Technologies de l'Information	AAFD	2008	CRM Analytique - L'apport du Data Mining	Nous présentons le système d'information CRM et décrivons lescomposants du CRM Analytique : alors que la Business Intelligence exploiteles données du passé pour produire des tableaux de bord synthétisant lesévolutions des indicateurs de performance de l'entreprise, le data miningproduit, à partir de ces mêmes données, des modèles exploratoires permettantde comprendre les indicateurs de performance (facteurs influençants, structuredes variables explicatives, segments) et des modèles prédictifs permettantd'anticiper et donc de planifier des actions CRM plus efficaces. Nousintroduisons ces différents éléments et les illustrons sur quelques exemples.Alors que le volume des données croît de façon exponentielle, peud'entreprises sont aujourd'hui capables de mettre en oeuvre le nombre demodèles nécessaires pour les exploiter toutes, nombre qui devrait lui aussi setrouver en croissance exponentielle. Il faut pour cela mettre en place des«usines à modèles » industrialisant le processus de production de modèles.Nous explicitons ces concepts, indiquons ce que serait le cahier des chargespour une usine à modèles. Nous indiquons comment l'outil de data miningKXEN permet de réaliser des usines à modèles et présentons ensuite quelquesexemples d'utilisation de KXEN pour la réalisation de telles usines.	Françoise Fogelman-Soulié	http://editions-rnti.fr/render_pdf.php?p1&p=1000495	http://editions-rnti.fr/render_pdf.php?p=1000495
Revue des Nouvelles Technologies de l'Information	AAFD	2008	État de l'art sur les méthodes statistiques d'apprentissage actif	L'apprentissage statistique désigne un vaste ensemble de méthodes etd'algorithmes qui permettent à un modèle d'apprendre un comportement grâce àdes exemples. L'apprentissage actif regroupe un ensemble de méthodes de sélectiond'exemples utilisées pour construire l'ensemble d'apprentissage du modèlede manière itérative, en intéraction avec un expert humain. Toutes les stratégiesont en commun de chercher à utiliser le moins d'exemples possible et de selectionnerles exemples les plus informatifs. Après avoir formalisé le problème del'apprentissage actif et après l'avoir situé par rapport aux autresmodes d'apprentissageexistant dans la littérature, cet article synthétise les principales approchesd'apprentissage actif et les illustre grâce à des exemples simples.	Alexis Bondu, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1000503	http://editions-rnti.fr/render_pdf.php?p=1000503
Revue des Nouvelles Technologies de l'Information	AAFD	2008	L'analyse relationnelle pour la fouille de grandes bases de données.	Dans cet article nous montrerons, brièvement, les possibilités offertespar la théorie de l'analyse relationnelle, initiée dans les années 1980 à IBMCorp.Nous nous concentrerons sur les avancées théoriques et méthodologiquesobtenues grâce à cette théorie pour fusionner l'information et pour traiteret analyser de grandes quantités de données qu'elles soient de type structuré ounon structuré. Nous aborderons brièvement la théorie de la similarité régularisée,théorie basée sur l'analyse relationnelle et la généralisant mais plus récente.Nous montrerons aussi des formules de transfert permettant d'exprimerdes problèmes combinatoires bien connus sous forme de fonctions économiqueslinéaires appropriées pour différents type de problématique (tels que desproblèmes de classification automatique ou des problèmes d'association,). Cecien plus de la complexité linéaire O(N) de l'algorithmique sous jacente quipermet à cette approche d'être tout à fait convenable pour différentes applicationsréelles.	Hamid Benhadda, François Marcotorchino	http://editions-rnti.fr/render_pdf.php?p1&p=1000501	http://editions-rnti.fr/render_pdf.php?p=1000501
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Méthodes à noyaux appliquées aux textes structurés	Cet article ébauche un état de l'art sur l'utilisation des noyaux pour letraitement des données structurées. Les applications modernes de la fouille dedonnées sont de plus en plus confrontés à des données structurées, notammenttextuelles. Les algorithmes d'apprentissage doivent donc être capables de tirerparti des informations apportées par la structure, ce qui pose d'intéressants problèmesde représentation des données. L'une des approches possibles consisteà utiliser les noyaux de Mercer. Ces noyaux permettent de calculer la similaritéentre deux données de type quelconque, et peuvent être utilisés par une largegamme d'algorithmes d'apprentissage (Machines à Vecteur de Support, ACP,Analyse Discriminante, Perceptron, etc). Nous présentons dans cet article lesprincipaux noyaux proposés ces dernières années pour le traitement des structurestelles que les séquences, les arbres et les graphes.	Sujeevan Aseervatham, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1000502	http://editions-rnti.fr/render_pdf.php?p=1000502
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Partitionnement des données pour les problèmes de classement difficiles: Combinaison des cartes topologiques mixtes et SVM	Dans ce papier, nous présentons un modèle pour aborder les problèmes de classement difficiles. Ces problèmes ont souvent la particularité d'avoir des taux d'erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problème, nous proposons d'utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteur de supports (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. Dans ce papier, nous présentons une combinaison de deux modèles qui permettent d'améliorer la visualisation des données et d'augmenter les performances en classement. Ce modèle consiste à entraîner des cartes autoorganisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation de notre modèle (CTSVM), nous avons utilisé quatre jeux de données. La première base est un extrait d'une grande base médicale sur l'étude de l'obésité à l'Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature. Les résultats obtenus montrent l'apport de ce modèle dans la visualisation et le classement de données complexes.	Mustapha Lebbah, Mohamed Ramzi Temanni, Christine Poitou-Bernert, Karine Clément, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000496	http://editions-rnti.fr/render_pdf.php?p=1000496
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Réduction des dimensions des données en apprentissage artificiel	Depuis plusieurs décénies, le volume des données disponibles ne cessede croître ; alors qu'au début des années 80 le volume des bases de données semesurait en mega-octets, il s'exprime aujourd'hui en tera-octets et parfois mêmeen peta-octets. Le nombre de variables et le nombre d'exemples peuvent prendredes valeurs très élevés, et cela peut poser un problème lors de l'exploration etl'analyse des données. Ainsi, le développement d'outils de traitement adaptésaux données volumineuses est un enjeu majeur de la fouille de données. La réductiondes dimensions permet notament de faciliter la visualisation et la compréhensiondes données, de réduire l'espace de stockage nécessaire et le tempsd'exploitation, et enfin d'identifier les facteurs pertinents. Dans cet article, nousprésentons un panarama des techniques de réduction des dimensions essentiellementbasées sur la sélection de variables supervisée et non supervisée, et surles méthodes géométriques de réduction de dimensions.	Younès Bennani, S. Guérif, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1000500	http://editions-rnti.fr/render_pdf.php?p=1000500
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Un aperçu de la fouille visuelle de données	Nous présentons dans cet article un aperçu de la fouille visuelle dedonnées. Pour commencer, nous situons ce domaine par rapport à d'autresapproches et nous en rappelons les principes fondateurs. Ensuite, nousmontrons qu'il existe de nombreux points de vue pour aborder les travaux enfouille visuelle de données : les données ou connaissances à visualiser, la tâcheà accomplir, la représentation visuelle choisie, la méthode de calcul de cettereprésentation ou encore le domaine d'application traité. Nous choisissons toutd'abord le point de vue des données à visualiser en détaillant des approchesreprésentatives pour la visualisation de données numériques, de donnéeshiérarchiques et de documents. Ensuite, notre prenons le point de vue de lareprésentation visuelle choisie en présentant le domaine des métaphoresvisuelles utilisées pour la fouille de données. Nous finissons en traitant d'undomaine thématique particulier, l'analyse d'audience d'un site Web, et enconcluant sur les perspectives en fouille visuelle de données.	Hanane Azzag, David Da Costa, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000494	http://editions-rnti.fr/render_pdf.php?p=1000494
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Un raisonnement approximatif pour l'apprentissage supervisé de règles	Le cadre de ce travail est celui de la méthode d'apprentissagesupervisé SUCRAGE qui se base sur la génération automatique de règles declassification. Ces règles sont exploitées par un moteur d'inférence classique :seules les règles dont les prémisses sont vérifiées par la nouvelle observation àclasser sont déclenchées. Ce moteur a été étendu à une inférenceapproximative qui permet de déclencher les règles pas trop éloignées de lanouvelle observation. Nous proposons une utilisation originale duraisonnement approximatif non plus comme un mode d'inférence mais commeun moyen d'affiner l'apprentissage. Le raisonnement approximatif est utilisépour générer de nouvelles règles dont les prémisses sont élargies : lesimprécisions des observations sont alors prises en compte et les problèmes liésà la discrétisation des attributs continus sont atténués. Notre approche a ététestée avec différentes bases d'apprentissage et confrontée à une applicationréelle dans le domaine du traitement d'images.	Amel Borgi	http://editions-rnti.fr/render_pdf.php?p1&p=1000498	http://editions-rnti.fr/render_pdf.php?p=1000498
Revue des Nouvelles Technologies de l'Information	AAFD	2008	Une infrastructure pour l'annotation linguistique de documents issus du web : le projet ALVIS	Cet article présente une architecture logicielle, la plate-forme Ogmios,permettant l'annotation automatique de documents issus du web. Cette architectureest fondée sur l'intégration de composants d'analyse linguistique et présenteune double originalité : elle peut être adaptée en fonction du domaine visé et ellepeut analyser de manière robuste des collections de documents hétérogènes, cequi est le propre des collections construites à partir du web. Cet article prendcomme exemple une collection de documents du domaine de la biologie. Nousmontrons comment la plateforme Ogmios peut être adaptée à ce domaine et nousdétaillons les performances obtenues suite à cette adaptation. Les résultats del'analyse des documents par la plate-forme peuvent ensuite être pris en comptepar des moteurs spécialisés sur internet.	Sophie Aubin, Julien Derivière, Thierry Hamon , Adeline Nazarenko, Thierry Poibeau, Davy Weissenbacher	http://editions-rnti.fr/render_pdf.php?p1&p=1000497	http://editions-rnti.fr/render_pdf.php?p=1000497
Revue des Nouvelles Technologies de l'Information	C	2008	 Indice de comparaison de hiérarchies semi-floues : application aux hiérarchies classiques.pdf 	Nous définissons la notion de hiérarchie semi-floue à partir de celle de hiérarchie classique. Nous proposons des indices mesurant d'une part l'imbrication d'un ensemble flou dans une hiérarchie semi-floue, et d'autre part l'imbrication d'une hiérarchie semi-floue dans une autre. Ces indices sont des outils permettant de comparer deux hiérarchies classiques.	Sahondra Ravonialimanana, Henri Ralambondrainy, Jean Diatta	http://editions-rnti.fr/render_pdf.php?p1&p=1000687	http://editions-rnti.fr/render_pdf.php?p=1000687
Revue des Nouvelles Technologies de l'Information	C	2008	 Une base pour les rèles d'association valides au sens de la mesure de qualité MGK.pdf 	Ce papier concerne les règles d'association valides au sens de la mesure de qualité MGK 1 . D'une part, MGK est normalisée en ce sens que ses valeurs sont comprises entre &#8722;1 et +1 et reflètent les situations de référence telles que l'incompatibilité, la dépendance négative, l'indépendance, la dépendance positive et l'implication logique entre la prémisse et le conséquent d'une règle. D'autre part, ses propriétés permettent non seulement de considérer les règles positives et négatives (à droite, à gauche, à gauche et à droite), mais aussi de se restreindre uniquement aux règles positives et à celles négatives à droite. Ainsi, nous proposons une base pour les règles positives exactes, une base pour les règles négatives exactes, une base pour les règles positives approximatives et une base pour les règles négatives approximatives. La réunion de ces quatre bases constitue une base pour toutes les règles d'association (positives et négatives) valides au sens de MGK.	Daniel R. Feno, Jean Diatta, André Totohasina	http://editions-rnti.fr/render_pdf.php?p1&p=1000685	http://editions-rnti.fr/render_pdf.php?p=1000685
Revue des Nouvelles Technologies de l'Information	C	2008	Accélération des cartes auto-organisatrices sur tableau de  dissimilarités par séparation et évaluation	Dans cet article, nous proposons une nouvelle implémentation d'une adaptation des cartes auto-organisatrices de Kohonen (SOM) aux tableaux de dissimilarités. Cette implémentation s'appuie sur le principe de séparation et évaluation afin de réduire le temps de calcul global de l'algorithme. Une propriété importante de ce nouvel algorithme tient au fait que les résultats produits sont strictement identiques à ceux de l'algorithme original.	Brieuc Conan-Guez, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1000679	http://editions-rnti.fr/render_pdf.php?p=1000679
Revue des Nouvelles Technologies de l'Information	C	2008	Améliorer la prévision multipas  par réseaux de neurones récurrents	Nous évaluons sur le problème de la prévision multipas un algorithme de boosting pour les réseaux de neurones récurrents (RNRs). Cet algorithme combine un grand nombre de RNRs, chacun d'entre eux étant généré en apprenant sur une version différente de l'ensemble d'apprentissage d'origine. Ces versions sont obtenues selon une variante de la méthode du boosting, qui permet de concentrer l'apprentissage sur les exemples difficiles mais, à la différence de l'algorithme d'origine, en prenant en compte tous les exemples disponibles. Nous l'appliquons au problème de la prévision pour différents horizons de trois séries temporelles de référence en testant à chaque fois trois fonctions de coût différentes et en variant les valeurs du paramètre principal. Nous comparons notre algorithme avec l'algorithme Back-Propagation Through Time (BPTT) et avec d'autres méthodes appliquées sur les mêmes ensembles de données, dont des approches locales	Mohammad Assad, Romuald Boné, Hubert Cardot	http://editions-rnti.fr/render_pdf.php?p1&p=1000723	http://editions-rnti.fr/render_pdf.php?p=1000723
Revue des Nouvelles Technologies de l'Information	C	2008	Analyse des groupes de gènes co-exprimés (AGGC) : un outil  automatique pour l'interprétation de données de biopuces	La technologie des biopuces permet de mesurer les niveaux d'expression de milliers de gènes dans différentes conditions biologiques générant ainsi des masses de données à analyser. De nos jours, l'interprétation de ces volumineux jeux de données à la lumière des différentes sources d'information est l'un des principaux défis dans la bio-informatique. Nous avons développé une nouvelle méthode appelée AGGC (Analyse des Groupes de Gènes Co-exprimés) qui permet de constituer de manière automatique des groupes de gènes à la fois fonctionnellement riches, i.e. qui partagent les mêmes annotations fonctionnelles, et co-exprimés. AGGC intègre l'information issue des biopuces, i.e. les profils d'expression des gènes, avec les annotations fonctionnelles des gènes obtenues à partir des sources d'information génomiques comme Gene Ontology. Les expérimentations menées avec cette méthode ont permis de mettre en évidence les principaux groupes de gènes fonctionnellement riches et co-exprimés dans des expériences de biopuces.	Ricardo Martinez, Nicolas Pasquier, Claude Pasquier, Martine Collard, Lucero Lopez	http://editions-rnti.fr/render_pdf.php?p1&p=1000729	http://editions-rnti.fr/render_pdf.php?p=1000729
Revue des Nouvelles Technologies de l'Information	C	2008	Analyse en composantes principales d'un &#64258;ux de données  d'espérance variable dans le temps	On considère un flux de données représenté par une suite de vecteurs de données. On suppose que chaque vecteur de données est une réalisation d'un vecteur aléatoire dont l'espérance mathématique varie dans le temps selon un modèle linéaire pour chacune des composantes. On utilise des processus d'approximation stochastique pour estimer en ligne les paramètres des modèles linéaires et en même temps les facteurs de l'ACP du vecteur aléatoire.	Jean-Marie Monnez	http://editions-rnti.fr/render_pdf.php?p1&p=1000682	http://editions-rnti.fr/render_pdf.php?p=1000682
Revue des Nouvelles Technologies de l'Information	C	2008	Analyse en Composantes Principales Mixte	Le traitement simultané de données mixtes (quantitatives et qualitatives) ne peut pas se réaliser directement par les méthodes classiques de la statistique exploratoire multidimensionnelle. Dans ce travail, l'analyse factorielle sur données mixtes proposée est une analyse en composantes principales normée après transformation des indicatrices des variables qualitatives en variables quantitatives au travers de projections de nuages de points dans l'espace des individus correspondant à des analyses de la variance multivariée. La méthode est évaluée sur la base d'une application sur données réelles mixtes.	Rafik Abdesselam	http://editions-rnti.fr/render_pdf.php?p1&p=1000681	http://editions-rnti.fr/render_pdf.php?p=1000681
Revue des Nouvelles Technologies de l'Information	C	2008	Classification de variables et classification croisée utilisées  préalablement à la recherche de règles d'association	La recherche de règles d'association conduit souvent à l'obtention d'un très grand nombre de règles, alors inexploitables. De plus, il est parfois difficile de faire varier les paramètres d'extraction des règles : le support et la confiance. En effet, dans le cas où les variables sont des événements rares, il est nécessaire de choisir des seuils de support très faibles. Nous avons proposé d'utiliser de manière conjointe la classification de variables et la recherche de règles d'association. La classification préalable des variables permet de construire des groupes homogènes où les variables sont liées. La recherche de règles à l'intérieur de chaque groupe conduit à réduire le nombre de règles à analyser. Les techniques de classification croisée permettent un double partitionnement sur les variables et sur les individus. Nous souhaitons étudier les apports d'une telle classification à notre approche. Cet article présente une comparaison des deux types de classification utilisés préalablement à la recherche de règles d'association. Nous présentons les résultats obtenus sur plusieurs échantillons de données issues de l'industrie automobile.	Marie Plasse, Ndeye Niang, Gilbert Saporta, Alexandre Villeminot, Laurent Leblond	http://editions-rnti.fr/render_pdf.php?p1&p=1000683	http://editions-rnti.fr/render_pdf.php?p=1000683
Revue des Nouvelles Technologies de l'Information	C	2008	Classification hiérarchique de variables discrètes fondée sur l'information mutuelle en pré-traitement d'un algorithme de sélection de variables pertinentes	Le travail présenté a pour contexte la sélection de variables pertinentes dans les problèmes de discrimination caractérisés par un grand nombre de variables potentiellement discriminantes toutes discrètes ou nominales. Dans ce cadre, nous proposons une procédure de sélection fondée sur une troncature k-additive de l'information mutuelle et utilisant une classification ascendante hiérarchique des variables potentiellement discriminantes afin de réduire le nombre de sous-ensembles dont la pertinence est estimée.	Hélène Daviet, Ivan Kojadinovic, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000686	http://editions-rnti.fr/render_pdf.php?p=1000686
Revue des Nouvelles Technologies de l'Information	C	2008	Consensus de classifications  basé sur les regroupements fréquents	Les classifications considérées ici sont des ensembles de classes deux à deux incomparables pour l'inclusion, par exemple des partitions, ou simplement une classe unique. Soit D = (D1, D2,..., Dk) un profil de telles classifications sur un ensemble fixé fini E, que l'on veut agréger en une classification unique D. Pour un entier p compris entre 1 et k (inclus), on définit un consensus par regroupements fréquents en considérant les classes maximales incluses dans des éléments d'au moins p des Di. On étudie les propriétés de cette règle de consensus et on en donne trois caractérisations.	Bruno Leclerc	http://editions-rnti.fr/render_pdf.php?p1&p=1000713	http://editions-rnti.fr/render_pdf.php?p=1000713
Revue des Nouvelles Technologies de l'Information	C	2008	Correction et complétude d'un algorithme de recherche  d'information par treillis de concepts	Dans cet article nous présentons BR-Explorer, un algorithme de recherche d'information correct et complet qui s'appuie sur la classification par treillis de concepts. L'algorithme BR-Explorer a pour objectif la recherche d'objets pertinents pour une requête donnée. Initialement, nous disposons d'un contexte formel représentant la relation entre un ensemble d'objets décrits par un ensemble d'attributs et du treillis de concepts correspondant à ce contexte. Étant donné une requête, l'algorithme BR-Explorer commence par générer un concept formel représentant la requête puis l'insère dans le treillis de concepts. Ensuite, BR-Explorer localise, dans le treillis résultant, un "concept pivot" à partir duquel il construit la réponse à la requête étape par étape en effectuant un parcours en largeur des concepts subsumants le concept pivot jusqu'au concept le plus général du treillis. Finalement BR-Explorer retourne en réponse l'ensemble des objets pertinents ordonnés selon leur degré de pertinence pour la requête.	Nizar Messai, Marie-Dominique Devignes, Amedeo Napoli, Malika Smail-Tabbone	http://editions-rnti.fr/render_pdf.php?p1&p=1000721	http://editions-rnti.fr/render_pdf.php?p=1000721
Revue des Nouvelles Technologies de l'Information	C	2008	Détection par Boosting de Données Aberrantes en Régression	Nous proposons une méthode basée sur le boosting, pour la détection des données aberrantes en régression. Le boosting privilégie naturellement les observations difficiles à prévoir, en les surpondérant de nombreuses fois au cours des itérations. La procédure utilise la réitération du boosting pour sélectionner parmi elles les données effectivement aberrantes. L'idée de base consiste à sélectionner l'observation la plus fréquemment rééchantillonnée lors des itérations du boosting puis de recommencer après l'avoir retirée. Le critère de sélection est basé sur l'inégalité de Tchebychev appliquée au maximum du nombre moyen d'apparitions dans les échantillons bootstrap. Ainsi, la procédure ne fait pas d'hypothèses sur la loi du bruit. Des exemples tests bien connus sont considérés et une étude comparative avec deux méthodes classiques illustrent le comportement de la méthode.	Nathalie Cheze, Jean-Michel Poggi	http://editions-rnti.fr/render_pdf.php?p1&p=1000722	http://editions-rnti.fr/render_pdf.php?p=1000722
Revue des Nouvelles Technologies de l'Information	C	2008	Évaluation des méthodes supervisées pour la discrimination  de protéines	Nous évaluons différentes méthodes supervisées dans le cadre de la discrimination de protéines. Les descripteurs étant automatiquement générés, nous avons utilisé les n-grammes, la taille de l'espace de représentation est très élevée par rapport au nombre d'observations. Un grand nombre de descripteurs ne sont pas pertinents. Il apparaît que les méthodes linéaires telles que les SVM linéaires ou la régression PLS sont les plus performantes. Une étude détaillée montre que ce succès repose essentiellement sur la robustesse face à la dimensionalité qui devient, de fait, le critère le plus important dès lors que l'on traite des domaines où les descripteurs sont générés automatiquement en grand nombre. Dans ce contexte, une procédure de sélection de variables, fusse-t-elle très fruste, modifie significativement le comportement des algorithmes d'apprentissage.	Ricco Rakotomalala, Faouzi Mhamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1000728	http://editions-rnti.fr/render_pdf.php?p=1000728
Revue des Nouvelles Technologies de l'Information	C	2008	Extraction de concepts guidée par le contexte	Les ontologies constituent la brique supportant les échanges et le partage des informations en étendant l'interopérabilité syntaxique du web en une interopérabilité sémantique. Dans cet article, nous présentons une méthode d'extraction de concepts ontologiques utilisant un algorithme de clustering non supervisé et guidé par le contexte à partir de pages Web. Notre méthode est basée sur une approche unifiée intégrant des dimensions complémentaires pour l'acquisition de connaissances conceptuelles. En particulier, nous exploitons les caractéristiques structurelles des documents HTML afin de localiser et de définir un contexte approprié pour chaque terme en respectant ses différentes positions dans le corpus. Notre définition contextuelle permet de sélectionner les co-occurrents sémantiquement proches et de définir une mesure de pondération appropriée pour chaque couple de termes. Notre méthode se base sur une évaluation interactive et incrémentale de la qualité des clusters par l'utilisateur. Nous l'avons expérimentée sur un corpus du domaine portant sur le tourisme. Les premiers résultats obtenus montrent bien que la prise en compte du contexte des termes guidant le clustering améliore considérablement la pertinence des concepts extraits	Lobna Karoui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000719	http://editions-rnti.fr/render_pdf.php?p=1000719
Revue des Nouvelles Technologies de l'Information	C	2008	Filiation de manuscrits sanskrits et arbres phylogénétiques	La fabrication d'un stemma codicum est l'une des approches les plus rigoureuses de la critique textuelle. Elle exige la reconstruction de l'histoire du texte en classifiant le corpus pour décider si un groupe de manuscrits est engendré par un intermédiaire perdu. Pour classifier notre corpus, nous employons des méthodes de l'analyse textuelle informatisée et de la reconstruction phylogénétique afin d'établir un arbre de la filiation. Les techniques employées sont dédiées à un corpus de manuscrits sanskrits avec toutes les spécificités de cette langue.	M. Le Pouliquen, J.P. Barthélemy, Patrice Bertrand	http://editions-rnti.fr/render_pdf.php?p1&p=1000716	http://editions-rnti.fr/render_pdf.php?p=1000716
Revue des Nouvelles Technologies de l'Information	C	2008	Le point d'égalité et le maximum de F(1).pdf	De la définition du point d'égalité entre la précision et le rappel, les liens entre ce point d'égalité et le point où F(1) est maximum, d'abord théoriquement dans le cas où les scores sont distribués normalement. Les expériences menées sur 3 ensembles de documents montrent que ces deux points sont toujours très proches, ce qui permet de gagner du temps dans la recherche du maximum pour F(1). De plus, si on remplace ce point maximum par le point d'égalité, les résultats obtenus sur un jeux de test indépendant ne sont pas significativement différents.	Jean Beney	http://editions-rnti.fr/render_pdf.php?p1&p=1000684	http://editions-rnti.fr/render_pdf.php?p=1000684
Revue des Nouvelles Technologies de l'Information	C	2008	Nouvelle méthode de classification adaptée aux données de  grande dimension : application aux données de biopuces	Nous proposons une nouvelle méthode de classification adaptée aux données de grande dimension. Pour ces données la distance de Chebyshev semble intéressante, car elle nécessite moins de temps de calcul comparée à la distance Euclidienne, plus utilisée en raison de ses bonnes propriétés géométriques. La méthode proposée combine les méthodes de regroupement hiérarchique et par partition pour obtenir le nombre de classes dans les données. Des données issues d'expériences de biopuces sont utilisées pour illustrer les performances de la méthode proposée.	Doulaye Dembélé	http://editions-rnti.fr/render_pdf.php?p1&p=1000727	http://editions-rnti.fr/render_pdf.php?p=1000727
Revue des Nouvelles Technologies de l'Information	C	2008	Segmentation d'images par la Classification floue basée  sur la programmation DC et DCA	Dans ce papier, nous proposons une nouvelle méthode de segmentation d'image via la classification floue, et basée sur la programmation DC (Difference of Convex functions) et DCA (DC Algorithm). En exploitant un schéma de DCA simple et robuste pour la classification floue, nous proposons un nouveau modèle par l'introduction des informations spatiales au modèle FCM standard. DCA appliqué au nouveau modèle est de la même forme que DCA pour le modèle FCM standard, et avec les informations spatiales, notre algorithme améliore nettement la segmentation des images bruitées. Pour trouver un bon point initial de l'algorithme, nous proposons une procédure alternative de DCA et FCM. Les simulations numériques sur plusieurs imageries médicales, qui sont en très haute résolution issue d'un signal, montrent l'efficacité de notre approche par rapport aux méthodes standards sur le temps de calcul et la qualité des solutions.	Le Thi Hoai An, Le Hoai Minh, Nguyen Trong Phuc, Pham Dinh Tao	http://editions-rnti.fr/render_pdf.php?p1&p=1000726	http://editions-rnti.fr/render_pdf.php?p=1000726
Revue des Nouvelles Technologies de l'Information	C	2008	Sélection de modèle PLS par rééchantillonnage bootstrap	Le problème de la sélection de modèle en régression PLS est primordial pour la modélisation de phénomènes physiques même si le nombre des variables, pouvant être supérieur à celui des individus, paraît au premier abord peu important pour la mise en oeuvre de la méthode. Les techniques de sélection consistent à retenir, parmi les modèles ayant un bon pouvoir de prédiction, ceux qui font intervenir le minimum de variables explicatives. La méthode que nous présentons dans ce papier est basée sur l'utilisation du bootstrap. Elle permet de calculer la distribution empirique des coefficients du modèle et de n'en conserver que les plus significatifs grâce à des tests statistiques. Elle mesure, par ailleurs, le pouvoir prédictif des modèles de régression construits aussi bien pour chaque individu que globalement. Nous illustrons cette approche en l'appliquant à un jeu de données.	Abdelaziz Faraj, Hicham Noçairi, Michel Constant	http://editions-rnti.fr/render_pdf.php?p1&p=1000724	http://editions-rnti.fr/render_pdf.php?p=1000724
Revue des Nouvelles Technologies de l'Information	C	2008	Traitements des signaux radar pour la reconnaissance/  identification de cibles aériennes	Le domaine de la reconnaissance de formes connaît aujourd'hui une activité importante en raison de la grande panoplie d'applications qu'il permet d'aborder, ceci face à la fois à la croissance du nombre et à la complexité des demandes exprimées dans les secteurs porteurs comme les systèmes de défense et de la surveillance pour la sécurité. Dans ce papier, nous nous intéressons à l'application concernant la reconnaissance et l'identification de cibles radar. Nous nous interrogeons davantage sur les moyens nécessaires et disponibles en matière de traitements et de méthodologie d'évaluation des résultats. La problématique générale présentée dans ce papier concerne les systèmes intelligents, dédiés pour l'aide à la prise de décision dans le domaine radar. En ce sens, on se retrouve à un carrefour d'approches aussi variées que spécifiques dans le contexte du processus d'extraction de connaissances à partir de données (ECD) (Frawley et al., 1993).	Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000725	http://editions-rnti.fr/render_pdf.php?p=1000725
Revue des Nouvelles Technologies de l'Information	C	2008	Une méthode de classi&#64257;cation visuelle et interactive	L'objectif de notre travail est de pouvoir représenter visuellement des ensembles de données et de laisser l'expert du domaine procéder interactivement à la définition d'une classification de ces données. Notre approche se base sur l'existence d'une fonction de similarité afin de pouvoir traiter des données de tous types (numériques, symboliques, images, textes, etc). Elle permet à l'expert du domaine, grâce à l'utilisation d'une visualisation à base de points d'intérêt (POIs), de définir des classes dans les données grâce à des opérations de sélections graphiques. Nous comparons notre approche interactive avec la classification ascendante hiérarchique (CAH) sur un ensemble de bases classiques. Nous montrons qu'un expert des données peut arriver à des performances similaires à celle d'un algorithme automatique, tout en bénéficiant d'informations complémentaires sur les classes.	David Da Costa, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000680	http://editions-rnti.fr/render_pdf.php?p=1000680
Revue des Nouvelles Technologies de l'Information	CAL	2008	Alignement d'ontologies dirigé par la structure	L'alignement d'ontologies met en évidence les relations sémantiques entre les entités de deux ontologies à confronter. L'outil de choix pour l'alignement est une mesure de similarité sur les couples d'entités. Certaines méthodes d'alignement performantes font dépendre la similarité d'un couple de celles des couples voisins. La circularité dans les définitions résultantes est traitée par le calcul itératif d'un point fixe. Nous proposons un cadre unificateur, appelé alignement dirigé par la structure, qui permet de décrire ces méthodes en dépit de divergences d'ordre technique. Celui-ci combine l'appariement de graphes et le calcul matriciel. Nous présentons son application à la ré-implémentation de l'algorithme OLA, baptisée OLA2.	Jean François Djoufak Kengue, Jérôme Euzenat, Petko Valtchev	http://editions-rnti.fr/render_pdf.php?p1&p=1000676	http://editions-rnti.fr/render_pdf.php?p=1000676
Revue des Nouvelles Technologies de l'Information	CAL	2008	AlkoWeb: Un outil pour modéliser l'architecture des  applications Web riches	La couche présentation des architectures n-tiers a besoin d'être conçue avec des modules correctement structurés et réutilisables. Dans ce article, nous présentons un modèle de composants hiérarchiques qui permet aux développeurs de modéliser, de générer du code et de réutiliser ce niveau logiciel des applications Web riches. Dans ce modèle, les composants peuvent être inter-connectés via leurs interfaces pour construire des composants plus complexes. Ces modèles peuvent être réutilisés avec leur code grâce à un mécanisme d'association. Comme nous le montrons dans cet article, ceci est une propriété intéressante pour assister les développeurs pour naviguer dans le code source et les modèles de leurs applications. Ceci permet par ailleurs de maintenir une cohérence entre les artefacts de ces deux étapes du processus de développement (conception et implémentation).	Reda Kadri, Chouki Tibermacine, Régis Fleurquin, Salah Sadou, François Marciol	http://editions-rnti.fr/render_pdf.php?p1&p=1000669	http://editions-rnti.fr/render_pdf.php?p=1000669
Revue des Nouvelles Technologies de l'Information	CAL	2008	Assemblage automatique et adaptation d'applications à base  de composants	Dans cet article, nous introduisons MADCAR, un modèle de moteurs dédiés à la construction et à la reconfiguration dynamique et automatique d'applications à base de composants. Dans MADCAR, la description d'une application regroupe la définition des configurations valides et les règles de transfert de l'état de l'application lors des adaptations. Cette description est découplée de toute implémentation et peut donc être réutilisée avec différents jeux de composants. Partant d'une description d'application, un moteur MADCAR construit un problème de contraintes dont la résolution permet le choix de la configuration cible et des composants à utiliser. Ce choix prend en compte le coût de la configuration cible et son adéquation avec les ressources disponibles. Afin d'assurer la cohérence de l'application, le moteur utilise les règles de transfert d'état pour initialiser les attributs des composants de l'assemblage cible à partir des attributs des composants de l'assemblage de départ.	Guillaume Grondin, Noury Bouraqadi, Laurent Vercouter	http://editions-rnti.fr/render_pdf.php?p1&p=1000673	http://editions-rnti.fr/render_pdf.php?p=1000673
Revue des Nouvelles Technologies de l'Information	CAL	2008	Construction dynamique d'annuaires de composants par  classification de services	Les annuaires de composants permettent d'indexer et de localiser rapidement les composants selon les services qu'ils offrent. Ils donnent ainsi aux assemblages en cours d'exécution la possibilité d'évoluer dynamiquement par remplacement de composants, en cas de défaillance, ou par intégration de nouvelles fonctionnalités, en réponse à de nouveaux besoins. Dans ce travail, nous visons des méthodes semi-automatiques d'évolution. Nous posons les bases théoriques d'une utilisation de l'Analyse Formelle de Concepts pour une construction incrémentale des annuaires de composants basée sur les définitions syntaxiques des services requis et fournis. Dans ces annuaires, les composants sont organisés de manière plus intelligible et les descriptions externes de composants plus abstraits et plus réutilisables sont suggérées. Mais surtout, cette organisation rend plus efficaces les tâches automatisées d'assemblage et de remplacement.	Gabriela Arévalo, Nicolas Desnos, Marianne Huchard, Christelle Urtado, Sylvain Vauttier	http://editions-rnti.fr/render_pdf.php?p1&p=1000674	http://editions-rnti.fr/render_pdf.php?p=1000674
Revue des Nouvelles Technologies de l'Information	CAL	2008	Enrichissement de l'architecture ANSI/SPARC pour expliciter la sémantique des données : une approche fondée sur les ontologies	L'architecture de bases de données ANSI/SPARC a été principalement définie pour permettre l'accès aux données indépendamment de leur représentation physique. La conception d'une base de données selon cette architecture passe par la transformation d'un modèle conceptuel en un modèle logique. Cette transformation peut engendrer une perte de sémantique de données. Ce qui pose des problèmes d'intégration et d'échange de plusieurs bases de données, ou de génération d'interfaces d'accès aux données pour un utilisateur final. En tant que modèle permettant d'exprimer la sémantique des données, les ontologies constituent une solution pertinente à ces problèmes ANSI/SPARC par l'ajout d'un niveau ontologique, permettant de conserver les ontologies décrivant la sémantique des données contenues dans une base de données. Notons que cette extension n'aura aucune incidence sur les applications conçues autour de l'architecture initiale. Nous analysons cette architecture en termes de besoin d'exploitation puis discutons de l'implantation d'une telle architecture.	Chimène Fankam, Stéphane Jean, Guy Pierra, Ladjel Bellatreche	http://editions-rnti.fr/render_pdf.php?p1&p=1000648	http://editions-rnti.fr/render_pdf.php?p=1000648
Revue des Nouvelles Technologies de l'Information	CAL	2008	Expression qualitative de politiques d'adaptation pour les  composants Fractal	Les plates-formes d'exécution récentes telles que Fractal ou OpenCOM offrent de nombreuses facilités pour assurer la prise en compte de propriétés extra-fonctionnelles (introspection, sondes, chargement dynamique, etc). Cependant, l'intégration de politiques d'adaptation reste délicate car elle nécessite de corréler la configuration du système avec l'évolution de son environnement. Le travail présenté dans cet article propose une description qualitative des évolutions de l'environnement et une interprétation possible basée sur de la logique floue. L'article présente également une extension de la plate-forme Fractal implémentant les mécanismes nécessaires à l'exécution de ces politiques d'adaptation de haut niveau. L'approche est illustrée à l'aide d'un serveur HTTP qui modifie sa configuration (architecturale et locale) en fonction de plusieurs paramètres extra-fonctionnels tels que la charge du serveur et la dispersion des requêtes.	Franck Chauvel, Olivier Barais, Isabelle Borne, Noël Plouzeau, Jean-Marc Jézéquel	http://editions-rnti.fr/render_pdf.php?p1&p=1000672	http://editions-rnti.fr/render_pdf.php?p=1000672
Revue des Nouvelles Technologies de l'Information	CAL	2008	Extraction métaheuristique d'une architecture à base de  composants à partir d'un système orienté objet	La modélisation et la représentation des architectures logicielles sont devenues une des phases principales du processus de développement de systèmes complexes. En effet, la représentation de l'architecture fournit de nombreux avantages pendant tout le cycle de vie du logiciel. Cependant pour beaucoup de systèmes existants, aucune représentation fiable de leurs architectures n'est disponible. Afin de pallier cette absence, source de nombreuses difficultés, nous proposons, dans cet article une approche, appelée ROMANTIC, visant à extraire une architecture à base de composants à partir d'un système orienté objet existant. L'idée première de cette approche est de proposer un processus quasi automatique d'identification d'architecture en formulant le problème comme un problème d'optimisation et en le résolvant au moyen de métaheuristiques. Ces dernières explorent l'espace composé des architectures pouvant être abstraites du système.	Sylvain Chardigny, Abdelhak Seriai, Mourad Oussalah, Dalila Tamzalit	http://editions-rnti.fr/render_pdf.php?p1&p=1000675	http://editions-rnti.fr/render_pdf.php?p=1000675
Revue des Nouvelles Technologies de l'Information	CAL	2008	Filling in the Whitespace: Research Opportunities in Model-Driven Software Development and Model-Driven Engineering		Bran Selic	http://editions-rnti.fr/render_pdf.php?p1&p=1000644	http://editions-rnti.fr/render_pdf.php?p=1000644
Revue des Nouvelles Technologies de l'Information	CAL	2008	Meta-grid  Un premier pas vers un framework grid basé sur les agents	Vu l'importance et l'ampleur accordée au développement des grilles de calcul (grid), il est nécessaire d'avoir une abstraction adéquate afin de les modéliser. Dans ce but, cet article proposera une plate-forme propice au bon fonctionnement d'un futur framework orienté agents développé au dessus de grids existants. Ce meta-grid, qui est une sorte de grid abstrait représentant et incarnant des grids "idéaux" existants, prendra la forme d'une couche qui sera développée pour faire face aux problèmes dus aux différences existantes entre les grids, et aux détails techniques spécifiques à chacun d'entre eux. L'idée consiste à cacher ces détails derrière un modèle uniforme que nous allons spécifier et modéliser. La création d'une telle plate-forme exemptera le programmeur d'applications grid de se soucier des problèmes spécifiques aux environnements.	Badr Jabari, Vincent Englebert, El Mostafa Oualim, Jean-Pol Vigneron	http://editions-rnti.fr/render_pdf.php?p1&p=1000645	http://editions-rnti.fr/render_pdf.php?p=1000645
Revue des Nouvelles Technologies de l'Information	CAL	2008	Un ADL pour les Architectures Distribuées à Composants  Hétérogènes	Dans cet article, nous présentons un ADL pour les architectures à composants hétérogènes et distribuées, et son utilisation au moment du déploiement. Actuellement, il n'existe pas de solution générique pour déployer une architecture distribuée basée sur différents intergiciels. Pour répondre à cette problématique, nous proposons dans cet article une approche pour décrire une telle architecture et un support pour le déploiement. Notre solution s'appuie sur un langage de description d'architecture possédant des notions de dépendances verticales et horizontales. Un exemple simple est présenté pour illustrer notre langage et pour valider nos contributions.	Guillaume Dufrêne, Lionel Seinturier	http://editions-rnti.fr/render_pdf.php?p1&p=1000650	http://editions-rnti.fr/render_pdf.php?p=1000650
Revue des Nouvelles Technologies de l'Information	CAL	2008	Un langage de contexte de preuve  pour la validation formelle de modèles logiciels	Pour améliorer les pratiques dans le domaine de la validation formelle de modèles, nous explorons un axe de recherche dans lequel nous formalisons la notion de « contexte de preuve » intégrant la description du comportement de l'environnement interagissant avec le modèle et les propriétés à vérifier dans ce contexte. L'article présente le langage CDL (Context Description Language) proposé à l'utilisateur pour la description des contextes de preuve. Ceux-ci sont exploités, actuellement dans nos travaux, par une technique de vérification de type model-checking avec la mise en oeuvre d'observateurs. Dans une approche Ingénierie Dirigée par les Modèles (IDM), les modèles de contextes sont transformés en modèles d'automates temporisés puis en codes exploitables par l'outil OBP/IFx (Observer-Based Prover). Ce travail a donné lieu à plusieurs expérimentations industrielles comme la validation formelle d'un protocole de communication avionique pour l'AIRBUS A380. Dans cet article, nous décrivons l'application de notre approche pour la validation d'un modèle de contrôleur de système aérien conçu par THALES. L'article rend compte de la mise en oeuvre du langage CDL et d'un retour d'expérience.	Philippe Dhaussy, Julien Auvray, Stéphane de Belloy, Frédéric Boniol, Eric Landel	http://editions-rnti.fr/render_pdf.php?p1&p=1000678	http://editions-rnti.fr/render_pdf.php?p=1000678
Revue des Nouvelles Technologies de l'Information	CAL	2008	Utilisation de BPEL pour la gestion des processus métier  d'une application locale  Étude de cas du calcul des allocations familiales  luxembourgeoises	Nous présentons ici les différentes étapes et techniques de développement choisies pour l'informatisation du traitement des prestations familiales des travailleurs frontaliers du Luxembourg. Après avoir expliqué le cadre, les enjeux et le déroulement global du projet, nous présentons notre utilisation d'un moteur de processus basé sur BPEL et de développement dirigé par des modèles, ces techniques étant utilisées dans le cadre d'un développement agile. A travers le travail réalisé, nous pointons les atouts et les désavantages de l'utilisation de BPEL pour gérer les flux de données d'une application locale ainsi que les enjeux scientifiques intéressants dans ce cadre.	Nicolas Biri, Pascal Bauler, Fernand Feltz, Nicolas Médoc, Céline Thomase	http://editions-rnti.fr/render_pdf.php?p1&p=1000647	http://editions-rnti.fr/render_pdf.php?p=1000647
Revue des Nouvelles Technologies de l'Information	CAL	2008	Vérification d'architecture embarquées : un enjeu aux multiples facettes		Frédéric Boniol	http://editions-rnti.fr/render_pdf.php?p1&p=1000643	http://editions-rnti.fr/render_pdf.php?p=1000643
Revue des Nouvelles Technologies de l'Information	CAL	2008	Vers l'Intégration des Propriétés non Fonctionnelles dans le langage SADL	La notion d'architecture logicielle est apparue aux alentours des années 1990 et est maintenant présentée comme le coeur d'une discipline à part entière. De nombreux langages de description d'architecture (ADLs) ont été proposés dans la littérature. Ils offrent des capacités complémentaires pour le développement et l'analyse architecturale d'un système logiciel. Comme l'objectif principal d'un ADL est de fournir les moyens pour une spécification explicite, plusieurs caractéristiques pour les composants et les connecteurs sont souhaitables. Parmi ces caractéristiques, les propriétés non fonctionnelles spécifiant les besoins des utilisateurs, sont très importantes et favorisent une implantation correcte du logiciel. SADL est un langage de description d'architecture dont la vocation n'étant pas l'analyse, nous l'avons alors étendu pour prendre en compte les spécifications non fonctionnelles des systèmes architecturaux. Nous proposons dans cet article, un cadre formel, basé logique de réécriture, permettant la description des propriétés non fonctionnelles des composants et des connecteurs du langage SADL ainsi que l'analyse d'une application architecturale garantissant ces propriétés.	Faiza Belala, Fateh Latreche, Malika Benammar	http://editions-rnti.fr/render_pdf.php?p1&p=1000651	http://editions-rnti.fr/render_pdf.php?p=1000651
Revue des Nouvelles Technologies de l'Information	CAL	2008	Vers la génération de modèles de sûreté de fonctionnement	La conception et le développement de systèmes embarqués critiques sont assujettis à la fois à des objectifs économiques mais également au respect des normes de sécurité. Dès lors, la qualité des analyses de sûreté de fonctionnement et des interactions entre les experts de sûreté de fonctionnement et les équipes de développement est primordiale. Partant du constat que les échanges entre ces équipes ne sont pas encore suffisamment automatisés, nous proposons des techniques de génération automatique de modèles de sûreté de fonctionnement à partir de spécifications exprimées sous forme de modèle. L'algorithme générique proposé a été implanté par un code de transformation de modèles AADL en AltaRica et une expérimentation a été réalisée sur une spécification d'un asservissement de gouverne avionique.	Xavier Dumas, Claire Pagetti, Laurent Sagaspe, Pierre Bieber, Philippe Dhaussy	http://editions-rnti.fr/render_pdf.php?p1&p=1000677	http://editions-rnti.fr/render_pdf.php?p=1000677
Revue des Nouvelles Technologies de l'Information	CAL	2008	Vers l'intégration dynamique de contrats dans des  architectures orientées services : une expérience applicative  du modèle au code	La flexibibilité offerte par les nouvelles architectures orientées services (SOA) renforce les besoins en contractualisation des services fournis et usages de ces services. L'expression de ces exigences et contrats se situe alors à tous les niveaux du cycle de vie des applications et nécessite des mises en oeuvre différentes en fonction de leur nature et des plates-formes ciblées. Dans le cadre du RNTL FAROS, nous avons mené une première expérimentation pour l'introduction de contrats dans une application basée sur une architecture SOA. Nous présentons ici la démarche, les mises en oeuvre et discutons les avantages attendus de l'approche dirigée par les modèles pour gérer les évolutions de l'application.	Sébastien Mosser, Mireille Blay-Fornarino, Philippe Collet, Philippe Lahire	http://editions-rnti.fr/render_pdf.php?p1&p=1000649	http://editions-rnti.fr/render_pdf.php?p=1000649
Revue des Nouvelles Technologies de l'Information	CAL	2008	Vers une Architecture de Type Agent BDI  pour un Ordonnanceur de Grille Adaptatif	Un Ordonnanceur de Grille (OG) a pour objectif d'optimiser l'utilisation des ressources mises à sa disposition. Le plus souvent, ce composant manque de flexibilité dans sa conception et a des difficultés à s'adapter aux diverses perturbations (fluctuation de la qualité de service, arrivée ou départ de nouvelles ressources, arrivée de travaux prioritaires, etc). L'objectif de cet article est de proposer une architecture orientée agents de type BDI (Belief Desire Intention) pour permettre à l'OG de s'adapter à ces perturbations et de travailler en coopération avec ses accointances. L'OG proposé fonctionne en boucle selon un schéma perception-délibération-action. La perception de l'environnement et de ses perturbations est rendue possible ici à l'aide d'un modèle conceptuel qui décrit à la fois la structure de la Grille et son fonctionnement. La délibération décide des actions à entreprendre (migration d'application, changement de plan d'ordonnancement, changement de politique d'attribution des tâches,..) pour s'adapter aux perturbations sur la base de règles qui proposent différents types d'adaptation selon les événements détectés. L'action correspond à la mise en oeuvre des décisions prises lors de la délibération.	Inès Thabet, Chihab Hanachi, Khaled Ghédira	http://editions-rnti.fr/render_pdf.php?p1&p=1000646	http://editions-rnti.fr/render_pdf.php?p=1000646
Revue des Nouvelles Technologies de l'Information	EDA	2008	A Conceptual Modeling Approach for Data Warehouses		Juan Trujillo	http://editions-rnti.fr/render_pdf.php?p1&p=1000688	http://editions-rnti.fr/render_pdf.php?p=1000688
Revue des Nouvelles Technologies de l'Information	EDA	2008	Deriving Ontologies from XML Schema	In this paper, we present a method and a tool for deriving a skeletonof an ontology from XML schema files. We first recall what an is ontology andits relationships with XML schemas. Next, we focus on ontology buildingmethodology and associated tool requirements. Then, we introduce Janus, atool for building an ontology from various XML schemas in a given domain.We summarize the main features of Janus and illustrate its functionalitiesthrough a simple example. Finally, we compare our approach to other existingontology building tools.	Ivan Bedini, Georges Gardarin, Benjamin Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000689	http://editions-rnti.fr/render_pdf.php?p=1000689
Revue des Nouvelles Technologies de l'Information	EDA	2008	Du XML au multidimensionnel : Conception de magasins de données	Avec l'ouverture des entreprises sur l'Internet, les sources dedonnées englobent davantage des données échangées avec les partenaires et/ouissues du Web. Dans ce cadre organisationnel ouvert, les documents XMLconstituent des sources de données plus utilisées aussi bien pour le stockageque pour les échanges transactionnels. Ce nouveau format de données a motivédes propositions de modèles pour les entrepôts de données XML et desdémarches de conception d'entrepôts à partir de sources XML. Cependant, lesdémarches proposées requièrent une intense intervention qui nécessite unedouble expertise dans le domaine des sources XML et celui d'entreposage dedonnées. Cet article propose une démarche automatique de conception deschémas multidimensionnels ayant un potentiel analytique à partir de sourcesXML. Il détaille les étapes de la démarche et l'illustre sur un exemple.	Yasser Hachaichi, Jamel Feki, Hanene Ben-Abdallah	http://editions-rnti.fr/render_pdf.php?p1&p=1000692	http://editions-rnti.fr/render_pdf.php?p=1000692
Revue des Nouvelles Technologies de l'Information	EDA	2008	Fouille de Données Multidimensionnelles : Différentes Stratégies pour Prendre en Compte la Mesure	Les entrepôts de données contiennent de gros volumes de données historiséesstockées à des fins d'analyse. Des techniques d'extraction de motifs séquentielsmultidimensionnels ont été développées afin de mettre en exergue descorrélations entre des positions sur des dimensions au cours du temps. Même sices méthodes offrent unemeilleure appréhension des données sources en prenanten compte certaines spécificités des cubes de données (e.g. multidimensionnalité,hiérarchies, relation d'ordre), aucune méthode ne permet de prendre directementen compte la valeurs des agrégats (mesure) dans l'extraction des motifs.Dans cet article, nous définissons deux méthodes de comptage du supportd'une séquence multidimensionnelle en s'appuyant sur les valeurs des agrégatsdes cellules qui supportent cette séquence. Des expérimentations sont décrites etmontrent l'intérêt de notre proposition.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000693	http://editions-rnti.fr/render_pdf.php?p=1000693
Revue des Nouvelles Technologies de l'Information	EDA	2008	Fragmentation Primaire et Dérivée: Étude de Complexité, Algorithmes de Sélection et Validation sous ORACLE10g	La fragmentation horizontale a été largement adoptée par la communautédes bases de données. Elle a une place à part entière dans la conceptionphysique. Plusieurs systèmes de gestion de bases de données (SGBD) commerciauxont proposé un langage de définition de données pour partitionner destables relationnelles en utilisant différents modes. Dans ce papier, nous présentonsd'abord l'évolution de la fragmentation ces dernières années au sein desSGBDs. Deuxièmement, nous étudions le problème de sélection de schéma defragmentation d'un entrepôt de données relationnel, et nous montrons qu'il estNP-complet. Vu sa complexité, nous développons un algorithme de hill climbing(méthode de voisinage) pour sélectionner un schéma de fragmentation quasi optimal.Nous effectuons des expérimentations afin de comparer cet algorithmeavec deux autres algorithmes: un génétique et un recuit simulé en utilisant unmodèle de coûtmathématique. Finalement, nous effectuons une validation réellede nos algorithmes sous ORACLE10g en utilisant les données issues du bancd'essai APB1.	Boukhalfa Kamel, Ladjel Bellatreche, Pascal Richard	http://editions-rnti.fr/render_pdf.php?p1&p=1000698	http://editions-rnti.fr/render_pdf.php?p=1000698
Revue des Nouvelles Technologies de l'Information	EDA	2008	Impact de l'évolution de la nomenclature des membres de dimension sur l'entrepôt de données	Les systèmes courants de gestion d'entrepôts de données (Data Warehouses : DW) permettent de traiter les évolutions des faits, mais pas les évolutions affectant les schémas des DW et les instances de dimension. Plusieurs solutions, basées essentiellement sur l'historisation et/ou le bersionnement des élèments de DW, ont été proposées pour ces dernières. Néanmoins, peu sont les travaux qui ont traités des dévolutions des nomenclatures des instances de dimension et de leurs effets sur les analyses. Dans cet article, nous proposons une classification des différents types d'évolution de nomenclature, et étudions les effets de ces évolutions sur la cohérence des analyses. Les solutions que nous envisageons à chacun de ces aspects d'évolutions se situent dans le cadre d'un système de gestion de DW temporel multiversions.	Inès Zouari Turki, Faiza Ghozzi, Rafik Bouaziz	http://editions-rnti.fr/render_pdf.php?p1&p=1000690	http://editions-rnti.fr/render_pdf.php?p=1000690
Revue des Nouvelles Technologies de l'Information	EDA	2008	Maintenance de charge pour l'optimisation des entrepôts de données évolutifs : aide à l'administrateur	Dans un contexte où les entrepôt de données sont amenés à subirdes évolutions, nous proposons d'aider l'administrateur à la maintenance dela charge (ensemble de requêtes) qui sert à l'évaluation des performances. Enrépercutant les évolutions de l'entrepôt de données sur la charge, il est alorspossible pour l'administrateur d'avoir une gestion pro-active des performances,évitant d'attendre que l'entrepôt mis à jour soit interrogé par les utilisateurs pourconstruire une charge valide pour optimiser les performances.	Cécile Favre, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000697	http://editions-rnti.fr/render_pdf.php?p=1000697
Revue des Nouvelles Technologies de l'Information	EDA	2008	Modélisation Intégrée de la Dynamique des Systèmes d'Information Décisionnels	Les systèmes d'information décisionnels (SID) sont des systèmes d'information(SI) qui ont pour objectif de faciliter la prise de décision à partir d'informationrésultant de processus complexes de dérivation et de préparation desdonnées de SI sources. Ces processus sont généralement peu modélisés et sontdirectement implantés avec des logiciels spécifiques au cours des projets décisionnelsbien que trois modèles particuliers ont été proposés pour représenterces processus. En effet, ces modèles utilisent de nouvelles notations distinctes decelles de la modélisation des données qu'ils proposent. Ils requièrent deux schémasdistincts pour les données et les processus alors que les schémas conceptuelsdes SID sont déjà nombreux et énormes en raison de la taille des projets et desspécificités des domaines.Ainsi, nous proposons des outils pour la prise en compte de la dynamique desSID et une modélisation intégrée des processus dans le schéma des SID. Nousdéfinissons les spécificités de la dynamique des SID. L'objectif est de prendreen compte dans un unique schéma aussi bien les processus liés à la dérivationdes données étudiés par les travaux précédents que ceux liés à la préparation desdonnées de l'environnement de prise de décision.	Estella Annoni, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000691	http://editions-rnti.fr/render_pdf.php?p=1000691
Revue des Nouvelles Technologies de l'Information	EDA	2008	ParAdmin: Un Outil d'Assistance à l'Administration et Tuning d'un Entrepôt de Données	Les entrepôts de données ont rendu les tâches d'administration et detuning plus complexes que dans les bases de données traditionnelles. Cela estdû aux caractéristiques des entrepôts de données : la volumétrie, les requêtescomplexes, les délais de réponse exigés par les décideurs et la gestion de l'évolution.Dans ce contexte, une panoplie de techniques d'optimisation ont été proposéesdurant la phase de conception physique. Pour chacune d'entre elles unnombre important d'algorithmes de sélection existe, chacun ayant ses propresparamètres. Un autre choix déterminant est à faire entre les techniques d'optimisationsimilaires - pour optimiser une requête donnée, deux techniques peuventêtre candidates. Durant la tâche de conception physique, l'administrateur doitdonc effectuer de multiple choix. Dans ce papier, nous montrons d'abord lesdifficultés qu'un administrateur peut rencontrer durant la phase de conceptionphysique. Deuxièmement, nous présentons une méthode de tuning basée sur lafragmentation horizontale et les index de jointure binaires. Finalement, nous proposonsun outil d'administration, appelé ParAdmin, qui permet d'assister l'administrateuren proposant la réalisation interactive des tâches de conception physiqueet de tuning. Elles sont définies à l'aide des choix: de la ou des structuresd'optimisation, des algorithmes de sélection, des paramètres, de tuner ou non etde visualiser les recommandations de performance.	Ladjel Bellatreche, Boukhalfa Kamel, Sybille Caffiau	http://editions-rnti.fr/render_pdf.php?p1&p=1000696	http://editions-rnti.fr/render_pdf.php?p=1000696
Revue des Nouvelles Technologies de l'Information	EDA	2008	Top_Keyword : agrégation de mots-clefs dans un environnement d'analyse en ligne (OLAP)	Depuis plus d'une décennie, les travaux de recherche sur OLAP etles bases de données multidimensionnelles ont produit des méthodes, des outilset des moyens d'analyse de données numériques. L'accroissement de la disponibilitédes documents numériques entraîne un besoin pour l'ajout de documentsXML principalement constitués de données textuelles au sein de basesde données multidimensionnelles et d'un environnement adapté à leur analyse.En réponse à ce besoin, cet article présente une nouvelle fonction d'agrégationpermettant l'agrégation de données textuelles au sein d'un environnementOLAP, au même titre que les fonctions d'agrégation arithmétique traditionnellesle permettent pour des données numériques. La fonction TOP_KEYWORD(ou TOP_KW) résume un ensemble de documents par leurs termes les plus significatifs,en employant une fonction de pondération issue de la recherched'information : tf.idf.	Franck Ravat, Olivier Teste, Ronan Tournier, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000695	http://editions-rnti.fr/render_pdf.php?p=1000695
Revue des Nouvelles Technologies de l'Information	EDA	2008	Une approche de répartition des données d'un entrepôt basée sur l'Optimisation par Essaim Particulaire	Dans le contexte des entrepôts de données, le partitionnement des tables, des index et des vues matérialisées en fragments stockés et consultés séparément apporte des améliorations considérables en terme de gestion des données et de coût de traitement. Lors de leurs conceptions, ces techniques se basent sur l'analyse d'informations statistiques recueillies à partir des requêtes les plus fréquentes. Cependant, en raison de leurs caractéristiques les requêtes décisionnelles rendent ce contexte d'analyse très variable. Ceci rend avec le temps, les schémas de fragmentation réalisés inappropriés et par conséquent une dégradation des performances du système. A partir de ce constat, et considérant que la validité d'une approche de partitionnement est soumise à l'épreuve du temps et dépend des besoins et de l'environnement, on propose dans ce papier une approche de répartition des données de l'entrepôt basée sur une méthode issue des approches biomimétiques.	Hacène Derrar, Omar Boussaid, Mohamed Ahmed-Nacer	http://editions-rnti.fr/render_pdf.php?p1&p=1000699	http://editions-rnti.fr/render_pdf.php?p=1000699
Revue des Nouvelles Technologies de l'Information	EDA	2008	Une méthode flexible de fusion de références	Dans cet article, nous nous intéressons au problème de fusion de référencesqui se pose une fois que les réconciliations entre références sont calculées.Il s'agit d'une tâche ayant comme objectif de fusionner les descriptions deréférences qui réfèrent à la même entité du monde réel pour en obtenir une seulereprésentation. Afin de pallier le problème d'incertitude dans les valeurs associéesaux attributs, nous avons choisi de représenter les résultats de la fusion deréférences dans un formalisme fondé sur les ensembles flous. Nous indiquonscomment les degrés de confiance sont calculés. Nous distinguons trois modespossibles de fusion. Enfin nous en proposons une représentation en RDF-Flouainsi que son interrogation.	Fatiha Saïs, Rallou Thomopoulos	http://editions-rnti.fr/render_pdf.php?p1&p=1000694	http://editions-rnti.fr/render_pdf.php?p=1000694
Revue des Nouvelles Technologies de l'Information	EGC	2008	A spatial rough set for extracting the periurban fringe	To date the availability of spatial data is increasing together withtechniques and methods adopted in geographical analysis. Despite this tendency,classifying in a sharp way every part of the city is more and more complicated.This is due to the growth of city complexity. Rough Set theory maybe a useful method to employ in combining great amounts of data in order tobuild complex knowledge about territory. It represents a different mathematicalapproach to uncertainty by capturing the indiscernibility. Two differentphenomena can be indiscernible in some contexts and classified in the sameway when combining available information about them. Several experiencesexist in the use of Rough Set theory in data mining, knowledge analysis andapproximate pattern classification, but the spatial component lacks in all theseresearch streams.This paper aims to the use of Rough Set methods in geographical analyses.This approach has been applied in a case of study, comparing the resultsachieved by means of both Map Algebra technique and Spatial Rough set. Thestudy case area, Potenza Province, is particularly suitable for the application ofthis theory, because it includes 100 municipalities with a different number ofinhabitants and morphologic features.	Beniamino Murgante, Giuseppe Las Casas, Anna Sansone	http://editions-rnti.fr/render_pdf.php?p1&p=1001234	http://editions-rnti.fr/render_pdf.php?p=1001234
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Algorithmes rapides de boosting de SVM	Les algorithmes de boosting de Newton Support Vector Machine (NSVM), Proximal Support Vector Machine (PSVM) et Least-Squares Support Vector Machine (LS-SVM) que nous présentons visent à la classification de très grands ensembles de données sur des machines standard. Nous présentons une extension des algorithmes de NSVM, PSVM et LS-SVM, pour construire des algorithmes de boosting. A cette fin, nous avons utilisé un terme de régularisation de Tikhonov et le théorème Sherman-Morrison- Woodbury pour adapter ces algorithmes au traitement d'ensembles de données ayant un grand nombre de dimensions. Nous les avons ensuite étendus par construction d'algorithmes de boosting de NSVM, PSVM et LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances des algorithmes sont évaluées sur des grands ensembles de données de l'UCI comme Adult, KDDCup 1999, Forest Covertype, Reuters-21578 et RCV1-binary sur une machine standard (PC-P4, 2,4 GHz, 1024 Mo RAM).	Thanh-Nghi Do, Jean-Daniel Fekete, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000617	http://editions-rnti.fr/render_pdf.php?p=1000617
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Analyse exploratoire d'opinions cinématographiques : co-clustering de corpus textuels communautaires	Les sites communautaires sont un endroit privilégié pour s'exprimer et publier des opinions. Le site www.flixster.com est un exemple de site participatif sur lequel se rassemblent plus de 20 millions de cinéphiles qui partagent des commentaires sur les films qu'ils ont ou non aimés. Explorer les contenus autoproduits est un challenge pour qui veut comprendre les attentes des internautes. Par une méthode d'apprentissage non supervisée, nous montrerons qu'il est possible de mieux comprendre le vocabulaire utilisé pour décrire des opinions. En particulier, grâce à une méthode de co-clustering, nous montrerons qu'un rapprochement peut être fait entre des films particuliers sur la base de l'usage d'un vocabulaire particulier. L'analyse des résultats peut conduire à retrouver une certaine typologie de films ou encore des rapprochements entre films. Cette étude peut être complémentaire avec des analyses linguistiques des corpus, ou encore être exploitée dans un contexte applicatif de recommandation de contenus multimédias.	Damien Poirier, Cécile Bothorel, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000654	http://editions-rnti.fr/render_pdf.php?p=1000654
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Apport des traitements morpho-syntaxiques pour l'alignement des définitions par une classification SVM	Cet article propose une méthode d'alignement automatique de définitions destinée à améliorer la fusion entre des terminologies spécialisées et un vocabulaire médical généraliste par un classifieur de type SVM (Support Vecteur Machine) et une représentation compacte et pertinente d'un couple de définitions par concaténation d'un ensemble de mesures de similarité, afin de tenir compte de leur complémentarité, auquelle nous ajoutons les longueurs de chacune des définitions. Trois niveaux syntaxiques ont été investigués. Le modèle fondé sur un apprentissage à partir des groupes nominaux de type Noms-Adjectifs aboutit aux meilleures performances.	Laura Diosan, Alexandrina Rogozan, Jean-Pierre Pécuchet	http://editions-rnti.fr/render_pdf.php?p1&p=1000582	http://editions-rnti.fr/render_pdf.php?p=1000582
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Approche d'annotation automatique des événements	Quotidiennement, plusieurs agences de presse publient des milliers d'articles contenant plusieurs événements de toutes sortes (politiques, économiques, culturels, etc.). Les preneurs de décision, se trouvent face à ce grand nombre d'événements dont seulement quelques uns les concernent. Le traitement automatique de tels événements devient de plus en plus nécessaires. Pour cela, nous proposons une approche, qui se base sur l'apprentissage automatique, et qui permet d'annoter les articles de presse pour générer un résumé automatique contenant les principaux événements. Nous avons validé notre approche par le développement du système "AnnotEv".	Rim Faiz, Aymen Elkhlifi	http://editions-rnti.fr/render_pdf.php?p1&p=1000554	http://editions-rnti.fr/render_pdf.php?p=1000554
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Approche hybride de classification à base de treillis de Galois: application à la reconnaissance de visages	La recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de l'apprentissage automatique. Le problème de classification de visages peut être considéré comme un problème d'apprentissage supervisé où les exemples d'apprentissage sont les visages étiquetés. Notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme d'apprentissage automatique supervisé. Ainsi, en se basant sur le fondement mathématique des treillis de Galois et leur utilisation pour la classification supervisée, nous proposons un nouvel algorithme de classification baptisé CITREC ainsi que son application pour la reconnaissance de visages. L'originalité de notre approche provient de la combinaison de l'analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne ou à plus proches voisins. Une validation expérimentale est décrite sur un benchmark du domaine de la reconnaissance de visages.	Yahya Slimani, Cherif Chiraz Latiri, Brahim Douar	http://editions-rnti.fr/render_pdf.php?p1&p=1000618	http://editions-rnti.fr/render_pdf.php?p=1000618
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Approches de type n-grammes pour l'analyse de parcours de vie familiaux	Cet article porte sur l'analyse de parcours de vie représentés sous forme de séquences d'événements. Plus spécifiquement, on examine les possibilités d'exploiter des codages de type n-grammes de ces séquences pour en extraire des connaissances. En fait, compte tenu de la simultanéité de certains événements, une procédure stricte de n-grammes comme on peut par exemple l'appliquer sur des textes, n'est pas applicable ici. Nous discutons diverses alternatives qui s'avèrent finalement plus proches de la fouille de séquences fréquentes. Les concepts discutés sont illustrés sur des données de l'enquête biographique rétrospective réalisée par le Panel suisse de ménages en 2002. Enfin, on précisera sur quels aspects l'approche proposée peut apporter un éclairage complémentaire utile par rapport à d'autres techniques plus classiques d'analyse exploratoire de parcours de vie.	Matthias Studer, Alexis Gabadinho, Nicolas S. Müller, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1000639	http://editions-rnti.fr/render_pdf.php?p=1000639
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Assignation automatique de solutions à des classes de plaintes liées aux ambiances intérieures polluées	Nous présentons dans cet article un système informatique pour le traitement des plaintes en lien avec des situations de pollution domestique écrites en français. Après la construction automatique d'une base de scenarii de plaintes, un module de recherche apparie la plainte à traiter à la thématique de la plainte la plus similaire. Enfin, il s'agit d'assigner au problème courant la solution correspondante au scénario de pollution auquel est affectée la plainte pertinente. Nous montrons ici l'intérêt de l'introduction dans l'appariement des textes de l'aspect sémantique géré par un dictionnaire généraliste de synonymes et en quoi il n'est pas réalisable pour notre problème particulier de construire une ontologie.	Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1000656	http://editions-rnti.fr/render_pdf.php?p=1000656
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Binary Block GTM : Carte auto-organisatrice probabiliste pour les grands tableaux binaires	Ce papier présente un modèle génératif et son estimation permettant la visualisation de données binaires. Notre approche est basée sur un modèle de mélange de lois de Bernoulli par blocs et les cartes de Kohonen probabilistes. La méthode obtenue se montre à la fois parcimonieuse et pertinente en pratique.	Rodolphe Priam, Mohamed Nadif, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1000614	http://editions-rnti.fr/render_pdf.php?p=1000614
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Cas d'utilisation réelle de Nautilus : calculs d'indicateurs chez un opérateur télécom	Nautilus est un logiciel d'analyse de bases de données. Le but de cette application est de généraliser l'utilisation de données clients au sein des entreprises. Elle facilite l'accès aux données en permettant de visualiser et manipuler les données du SGBD sous forme de concepts métiers. Elle inclut un générateur de requêtes SQL et un outil de gestion de tâches désignées pour l'agrégation de grands volumes de données. Le principe de fonctionnement est basé sur l'enchaînement de phases permettant la création des données d'analyse : importation des métadonnées du SGBD ; construction d'un dictionnaire de des concepts métiers ; spécification des champs à calculer. Les différents traitements tels que les jointures et l'alimentation des tables sont optimisés afin de rendre l'application utilisable sur des SGBD d'entreprise	Adrien Schmidt, Serge Fantino	http://editions-rnti.fr/render_pdf.php?p1&p=1000601	http://editions-rnti.fr/render_pdf.php?p=1000601
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Classification adaptative de séries temporelles : application à l'identification des gènes exprimés au cours du cycle cellulaire	Ce travail s'inscrit dans le cadre de l'étude de la division cellulaire assurant la prolifération des cellules. Une meilleure compréhension de ce phénomène biologique nécessite l'identification des gènes caractérisant chaque phase du cycle cellulaire. Le procédé d'identification est généralement basé sur un ensemble de gènes dits gènes de référence, sélectionnés expérimentalement et considérés comme caractérisant les phases du cycle cellulaire. Les niveaux d'expression des gènes étudiés sont mesurés durant le cycle de la division cellulaire et permettent de construire des profils d'expression. Chaque gène étudié est affecté à la phase du cycle cellulaire correspondant au groupe de gènes de référence le plus similaire. Cette approche classique souffre de deux limites. D'une part les mesures de proximité les plus couramment utilisés entre profils d'expression de gènes sont basées sur les écarts en valeurs sans tenir compte de la forme des profils. D'autre part, dans la littérature, il n'y a pas consensus quant à l'ensemble des gènes de référence à considérer. Dans cet article, notre but est de proposer une classification adaptative, basée sur un indice de dissimilarité incluant les proximités en valeurs et en forme des profils d'expression de gènes, permettant d'identifier les phases d'expression des gènes étudiés, et de présenter un nouvel ensemble de gènes de référence validé par une connaissance biologique.	Alpha Diallo, Ahlame Douzal-Chouakria, Françoise Giroud	http://editions-rnti.fr/render_pdf.php?p1&p=1000637	http://editions-rnti.fr/render_pdf.php?p=1000637
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Classification de documents en réseaux petits mondes en vue d'apprentissage		Mohamed Khazri, Mohamed Tmar, Mohand Boughanem, Mohamed Abid	http://editions-rnti.fr/render_pdf.php?p1&p=1000581	http://editions-rnti.fr/render_pdf.php?p=1000581
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Clustering en haute dimension par accumulation de clusterings locaux	Le clustering est une tâche fondamentale de la fouille de données. Ces dernières années, les méthodes de type cluster ensembles ont été l'objet d'une attention soutenue. Il s'agit d'agréger plusieurs clusterings d'un jeu de données afin d'obtenir un clustering "moyen". Les clusterings individuels peuvent être le résultat de différents algorithmes. Ces méthodes sont particulièrement utiles lorsque la dimensionalité des données ne permet pas aux méthodes classiques basées sur la distance et/ou la densité de fonctionner correctement. Dans cet article, nous proposons une méthode pour obtenir des clusterings individuels à faible coût, à partir de projections partielles du jeu de données. Nous évaluons empiriquement notre méthode et la comparons à trois méthodes de différents types. Nous constatons qu'elle donne des résultats sensiblement supérieurs aux autres.	Marc-Ismaël Akodjènou-Jeannin, Kavé Salamatian, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000612	http://editions-rnti.fr/render_pdf.php?p=1000612
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Clustering Visuel Semi-Supervisé pour des systèmes en coordonnées en étoiles 3D	Dans cet article, nous proposons une approche qui combine les méthodes statistiques avancées et la flexibilité des approches interactives manuelles en clustering visuel. Nous présentons l'interface Semi-Supervised Visual Clustering (SSVC). Sa contribution principale est l'apprentissage d'une métrique de projection optimale pour la visualisation en coordonnées en étoiles ainsi que pour l'extension 3D que nous avons développée. La métrique de distance de projection est apprise à partir des retours de l'utilisateur soit en termes de similarité/ dissimilarité entre les items, soit par l'annotation directe. L'interface SSVC permet, de plus, une utilisation hybride dans laquelle un ensemble de paramètres sont manuellement fixés par l'utilisateur tandis que les autres paramètres sont déterminés par un algorithme de distance optimale.	Loïc Lecerf, Boris Chidlovskii	http://editions-rnti.fr/render_pdf.php?p1&p=1000559	http://editions-rnti.fr/render_pdf.php?p=1000559
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Co-classification sous contraintes par la somme des résidus quadratiques	Dans de nombreuses applications, une co-classification est plus facile à interpréter qu'une classification mono-dimensionnelle. Il s'agit de calculer une bi-partition ou collection de co-clusters : chaque co-cluster est un groupe d'objets associé à un groupe d'attributs et les interprétations peuvent s'appuyer naturellement sur ces associations. Pour exploiter la connaissance du domaine et ainsi améliorer la pertinence des partitions, plusieurs méthodes de classification sous contraintes ont été proposées pour le cas mono-dimensionnel, e.g., l'exploitation de contraintes "must-link" et "cannot-link". Nous considérons ici la co-classification sous contraintes avec la gestion de telles contraintes étendues aux dimensions des objets et des attributs, mais aussi l'expression de contraintes de contiguité dans le cas de domaines ordonnés. Nous proposons un algorithme itératif qui minimise la somme des résidus quadratiques et permet l'exploitation active des contraintes spécifiées par les analystes. Nous montrons la valeur ajoutée de ce type d'extraction sur deux applications en analyse du transcriptome.	Ruggero G. Pensa, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000665	http://editions-rnti.fr/render_pdf.php?p=1000665
Revue des Nouvelles Technologies de l'Information	EGC	2008	Conception de systèmes d'information spatio-temporelle adaptatifs avec ASTIS	Les avancées technologiques récentes du Web et du sans fil,conjuguées au succès des applications spatialisées grand public, sont àl'origine d'un accès accru aux systèmes d'information spatio-temporelle(SIST) par une grande diversité d'utilisateurs, munis des dispositifs d'accèset dans des contextes d'utilisation variés. Adapter ces systèmes à l'utilisateurdevient donc une nécessité, un gage d'utilisabilité et de pérennité. Cet articleprésente une approche générique pour la conception et la génération desystèmes d'information spatio-temporelle adaptés à l'utilisateur, appeléASTIS. ASTIS offre des modalités générales de mise en oeuvre del'adaptation à l'utilisateur, visant tant le contenu que la présentation desapplications. Elle permet aux concepteurs d'intégrer ces modalitésd'adaptation dans des applications traitant des données spatio-temporelles.Afin de définir les besoins et types d'adaptation propres à leur application, ilsuffit aux concepteurs de créer des modèles conceptuels, par spécialisation etinstanciation des modèles offerts par notre architecture	Bogdan Moisuc, Jérôme Gensel, Hervé Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001232	http://editions-rnti.fr/render_pdf.php?p=1001232
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Data mining for activity extraction in video data	The exploration of large video data is a task which is now possible because of the advances made on object detection and tracking. Data mining techniques such as clustering are typically employed. Such techniques have mainly been applied for segmentation/indexation of video but knowledge extraction of the activity contained in the video has been only partially addressed. In this paper we present how video information is processed with the ultimate aim to achieve knowledge discovery of people activity in the video. First, objects of interest are detected in real time. Then, in an off-line process, we aim to perform knowledge discovery at two stages: 1) finding the main trajectory patterns of people in the video. 2) finding patterns of interaction between people and contextual objects in the scene. An agglomerative hierarchical clustering is employed at each stage. We present results obtained on real videos of the Torino metro (Italy).	Monique Thonnat, Jose Luis Patino, Etienne Corvée, François Brémond	http://editions-rnti.fr/render_pdf.php?p1&p=1000632	http://editions-rnti.fr/render_pdf.php?p=1000632
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Découverte de motifs séquentiels et de règles inattendus	Les travaux autour de l'extraction de motifs séquentiels se sont particulièrement focalisés sur la définition d'approches efficaces pour extraire, en fonction d'une fréquence d'apparition, des corrélations entre des éléments dans des séquences. Même si ce critère de fréquence est déterminant, le décideur est également de plus en plus intéressé par des connaissances qui sont représentatives d'un comportement inattendu dans ces données (erreurs dans les données, fraudes, nouvelles niches, ... ). Dans cet article, nous introduisons le problème de la détection de motifs séquentiels inattendus par rapport aux croyances du domaine. Nous proposons l'approche USER dont l'objectif est d'extraire les motifs séquentiels et les règles inattendues dans une base de séquences.	Dong (Haoyuan) Li, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000641	http://editions-rnti.fr/render_pdf.php?p=1000641
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Délestage pour l'analyse multidimensionnelle de flux de données	Dans le contexte de la gestion de flux de données, les données entrent dans le système à leur rythme. Des mécanismes de délestage sont à mettre en place pour qu'un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement. Le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié. Dans cet article, nous nous plaçons dans le cas où le système est un cube de données, dont la structure est connue a priori, alimenté par un flux de données. Nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube. Nous exploitons l'inégalité de Hoeffding pour obtenir une borne probabiliste sur l'écart entre la valeur attendue et la valeur estimée.	Sylvain Ferrandiz, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000580	http://editions-rnti.fr/render_pdf.php?p=1000580
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Détection de groupes atypiques pour une variable cible quantitative	Une tâche importante en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes d'individus. Quelles sont les catégories d'individus qui gagnent de particulièrement forts salaires ou au contraire, quelles sont celles qui ont de très faibles salaires ? Nous présentons le problème d'extraction de tels groupes atypiques vis-à-vis d'une variable cible quantitative, comme par exemple la variable "salaire", et plus particulièrement pour les faibles et fortes valeurs d'un intervalle déterminé par l'utilisateur. Il s'agit donc de rechercher des conjonctions de variables dont la distribution diffère significativement de celle de l'ensemble d'apprentissage pour les faibles et fortes valeurs de l'intervalle de cette variable cible. Une adaptation d'une mesure statistique existante, l'intensité d'inclination, nous permet de découvrir de tels groupes atypiques. Cette mesure nous libère de l'étape de transformation des variables quantitatives, à savoir l'étape de discrétisation suivie d'un codage disjonctif complet. Nous proposons donc un algorithme d'extraction de tels groupes avec des règles d'élagage pour réduire la complexité du problème. Cet algorithme a été développé et intégré au logiciel d'extraction de connaissances WEKA. Nous terminons par un exemple d'extraction sur la base de données IPUMS du bureau de recensement américain.	Sylvie Guillaume, Florian Guillochon, Michel Schneider	http://editions-rnti.fr/render_pdf.php?p1&p=1000627	http://editions-rnti.fr/render_pdf.php?p=1000627
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Discretization of Continuous Features by Resampling	Les arbres de décision sont largement utilisés pour générer des classificateurs à partir d'un ensemble de données. Le processus de construction est une partitionnement récursif de l'ensemble d'apprentissage. Dans ce contexte, les attributs continus sont discrétisés. Il s'agit alors, pour chaque variable à discrétiser de trouver l'ensemble des points de coupure. Dans ce papier nous montrons que la recherche des ces points de coupure par une méthode de ré-échantillonnage, comme le BOOTSTRAP conduit à des meilleurs résultats. Nous avons testé cette approche avec les méthodes principales de discrétisation comme MDLPC, FUSBIN, FUSINTER, CONTRAST, Chi-Merge et les résultats sont systématiquement meilleurs en utilisant le bootstrap. Nous exposons ces principaux résultats et ouvrons de nouvelles pistes pour la construction d'arbres de décision.	Taimur Qureshi, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000622	http://editions-rnti.fr/render_pdf.php?p=1000622
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Echantillonnage adaptatif de jeux de données déséquilibrés pour les forêts aléatoires	Dans nombre d'applications, les données présentent un déséquilibre entre les classes. La prédiction est alors souvent détériorée pour la classe minoritaire. Pour contourner cela, nous proposons un échantillonnage guidé, lors des itérations successives d'une forêt aléatoire, par les besoins de l'utilisateur.	Elie Prudhomme, Julien Thomas, Pierre-Emmanuel Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1000588	http://editions-rnti.fr/render_pdf.php?p=1000588
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Echantillonnage pour l'extraction de motifs séquentiels : des bases de données statiques aux flots de données	Depuis quelques années, la communauté fouille de données s'est intéressée à la problématique de l'extraction de motifs séquentiels à partir de grandes bases de données en considérant comme hypothèse que les données pouvaient être chargées en mémoire centrale. Cependant, cette hypothèse est mise en défaut lorsque les bases manipulées sont trop volumineuses. Dans cet article, nous étudions une technique d'échantillonnage basée sur des réservoirs et montrons comment cette dernière est particulièrement bien adaptée pour résumer de gros volumes de données. Nous nous intéressons ensuite à la problématique plus récente de la fouille sur des données disponibles sous la forme d'un flot continu et éventuellement infini ("data stream"). Nous étendons l'approche d'échantillonnage à ce nouveau contexte et montrons que nous sommes à même d'extraire des motifs séquentiels de flots tout en garantissant les taux d'erreurs sur les résultats. Les différentes expérimentations menées confirment nos résultats théoriques.	Chedy Raïssi, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000567	http://editions-rnti.fr/render_pdf.php?p=1000567
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Echantillonnage spatio-temporel de flux de données distribués	Ces dernières années, sont apparues de nombreuses applications, utilisant des données potentiellement infinies, provenant de façon continue de capteurs distribués. On retrouve ces capteurs dans des domaines aussi divers que la météorologie (établir des prévisions), le domaine militaire (surveiller des zones sensibles), l'analyse des consommations électriques (transmettre des alertes en cas de consommation anormale),... Pour faire face à la volumétrie et au taux d'arrivée des flux de données, des traitements sont effectués 'à la volée' sur les flux. En particulier, si le système n'est pas assez rapide pour traiter toutes les données d'un flux, il est possible de construire des résumés de l'information. Cette communication a pour objectif de faire un premier point sur nos travaux d'échantillonnage dans un environnement de flux de données fortement distribués. Notre approche est basée sur la théorie des sondages, l'analyse des données fonctionnelles et la gestion de flux de données. Cette approche sera illustrée par un cas réel : celui des mesures de consommations électriques	Raja Chiky, Jérôme Cubillé, Alain Dessertaine, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000569	http://editions-rnti.fr/render_pdf.php?p=1000569
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Enhancing Personal File Retrieval in Semantic File Systems with Tag-Based Context	Recently, tagging systems are widely used on the Internet. On desktops, tags are also supported by some semantic file systems and desktop search tools. In this paper, we focus on personal tag organization to enhance personal file retrieval. Our approach is based on the notion of context. A context is a set of tags assigned to a file by a user. Based on tag popularity and relationships between tags, our proposed algorithm creates a hierarchy of contexts on which a user can navigate to retrieve files in an effective manner.	Ba-Hung Ngo, Frédérique Silber-Chaussumier, Christian Bac	http://editions-rnti.fr/render_pdf.php?p1&p=1000558	http://editions-rnti.fr/render_pdf.php?p=1000558
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Étude comparative de deux approches de classification recouvrante : MOC vs. OKM	La classification recouvrante désigne les techniques de regroupements de données en classes pouvant s'intersecter. Particulièrement adaptés à des domaines d'application actuels (e.g. Recherche d'Information, Bioinformatique) quelques modèles théoriques de classification recouvrante ont été proposés très récemment parmi lesquels le modèle MOC (Banerjee et al. (2005a)) utilisant les modèles de mélanges et l'approche OKM (Cleuziou (2007)) consistant à généraliser l'algorithme des k-moyennes. La présente étude vise d'une part à étudier les limites théoriques et pratiques de ces deux modèles, et d'autre part à proposer une formulation de l'approche OKM en terme de modèles de mélanges gaussiens, laissant ainsi entrevoir des perspectives intéressantes quant à la variabilité des schémas de recouvrements envisageables.	Guillaume Cleuziou, Jacques-Henri Sublemontier	http://editions-rnti.fr/render_pdf.php?p1&p=1000666	http://editions-rnti.fr/render_pdf.php?p=1000666
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Étude de l'interaction entre variables pour l'extraction des règles d'influence	Cet article présente une méthode efficace pour l'extraction de règles d'influence quantitatives positives et négatives. Ces règles d'influence introduisent une nouvelle sémantique qui vise à faciliter l'analyse d'un volume important de données. Cette sémantique fixe la direction de la règle entre deux variables en positionnant, au préalable, l'une comme étant l'influent et l'autre comme étant l'influé. Elle permet, de ce fait, d'exprimer la nature de l'influence : positive, en maximisant le nombre d'éléments en commun ou négative, en maximisant le nombre d'éléments qui violent l'influé. Notre approche s'appuie sur une stratégie qui comporte cinq étapes dont deux exécutées en parallèle. Ces deux étapes constituent les étapes clé de notre approche. La première combine une méthode d'élagage et de regroupement tabulaire basée sur les tableaux de contingence. Cette dernière construit et classe les zones potentiellement intéressantes. La seconde, injecte la sémantique et évalue le degré d'influence que produirait l'introduction d'une nouvelle variable sur un ensemble de variables en utilisant une nouvelle mesure d'intérêt, l'Influence. Cette étape vient affiner les résultats de la première étape, et permet de se focaliser sur des zones valides par rapport aux contraintes spécifiées. Enfin, un système de règles d'influence jugées intéressantes est construit basé sur la juxtaposition des résultats des deux étapes clé de notre approche.	Leila Nemmiche Alachaher, Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1000629	http://editions-rnti.fr/render_pdf.php?p=1000629
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Évaluation des critères asymétriques pour les arbres de décision	Pour construire des arbres de décision sur des données déséquilibrées, des auteurs ont proposés des mesures d'entropie asymétriques. Le problème de l'évaluation de ces arbres se pose ensuite. Cet article propose d'évaluer la qualité d'arbres de décision basés sur une mesure d'entropie asymétrique.	Gilbert Ritschard, Simon Marcellin, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000587	http://editions-rnti.fr/render_pdf.php?p=1000587
Revue des Nouvelles Technologies de l'Information	EGC 	2008	ExpLSA : utilisation d'informations syntaxico-sémantiques associées à LSA pour améliorer les méthodes de classification conceptuelle	L'analyse sémantique latente (LSA - Latent Semantic Analysis) est aujourd'hui utilisée dans de nombreux domaines comme la modélisation cognitive, les applications éducatives mais aussi pour la classification. L'approche présentée dans cet article consiste à ajouter des informations grammaticales à LSA. Différentes méthodes pour exploiter ces informations grammaticales sont étudiées dans le cadre d'une tâche de classification conceptuelle.	Nicolas Béchet, Mathieu Roche, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000658	http://editions-rnti.fr/render_pdf.php?p=1000658
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction d'itemsets compacts	L'extraction d'itemsets fréquents est un sujet majeur de l'ECD et son but est de découvrir des corrélations entre les enregistrements d'un ensemble de données. Cependant, le support est calculé en fonction de la taille de la base dans son intégralité. Dans cet article, nous montrons qu'il est possible de prendre en compte des périodes difficiles à déceler dans l'organisation des données et qui contiennent des itemsets fréquents sur ces périodes. Nous proposons ainsi la définition des itemsets compacts, qui représentent un comportement cohérent sur une période spécifique et nous présentons l'algorithme DEICO qui permet leur découverte.	Bashar Saleh, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000628	http://editions-rnti.fr/render_pdf.php?p=1000628
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction de Motifs Séquentiels Multidimensionnels Clos sans Gestion d'Ensemble de Candidats	L'extraction de motifs séquentiels permet de découvrir des corrélations entre événements au cours du temps. Introduisant plusieurs dimensions d'analyse, les motifs séquentiels multidimensionnels permettent de découvrir des motifs plus pertinents. Mais le nombre de motifs obtenus peut devenir très important. C'est pourquoi nous proposons, dans cet article, de définir une représentation condensée garantie sans perte d'information : les motifs séquentiels multidimensionnels clos extraits ici sans gestion d'ensemble de candidats.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000642	http://editions-rnti.fr/render_pdf.php?p=1000642
Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction d'un modèle numérique de terrain à partir de photographies par drone	Dans le suivi et la modélisation de l'érosion en montagne, lareprésentation fine du relief est une composante importante. En effet, laconnaissance des zones de concentration des eaux, notamment à traversl'apparition de rigoles élémentaires, est fondamentale pour bien décrire lesconnectivités entre les zones de mobilisation des sédiments sur le versant et leréseau hydrographique stabilisé. La résolution au sol permise par lesphotographies aériennes classiques ne permet pas d'accéder à unereprésentation 3D suffisamment fine des ravines élémentaires. Nous testonsl'utilisation de photographies stéréoscopiques à résolution centimétrique prisesà basse altitude par un drone pour obtenir un MNT précis. La question majeureconcerne les règles à suivre pour un meilleur compromis entre précision etfacilité d'élaboration, et l'évaluation de l'importance relative de chaque étapesur la qualité finale de la restitution. La zone d'étude est située dans lesBadlands de Draix (Alpes de Haute Provence).	Andres Jacome, Christian Puech, Damien Raclot, Jean-Stéphane Bailly, Bruno Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001233	http://editions-rnti.fr/render_pdf.php?p=1001233
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction et exploitation des annotations contextuelles	Dans la perspective d'offrir un web sémantique, des travaux ont cherché à automatiser l'extraction des annotations sémantiques à partir de textes pour représenter au mieux la sémantique que vise à transmettre une page web. Dans cet article nous proposons une approche d'extraction des annotations qui représentent le plus précisément possible le contenu d'un document. Nous proposons de prendre en compte la notion de contexte modélisé par des relations contextuelles émanant, à la fois, de la structure et de la sémantique du texte.	Noureddine Mokhtari, Rose Dieng-Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000551	http://editions-rnti.fr/render_pdf.php?p=1000551
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction et validation par croisement des relations d'une ontologie de domaine		Lobna Karoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000594	http://editions-rnti.fr/render_pdf.php?p=1000594
Revue des Nouvelles Technologies de l'Information	EGC 	2008	FIASCO : un nouvel algorithme d'extraction d'itemsets fréquents dans les flots de données	Nous présentons dans cet article un nouvel algorithme permettant la construction et la mise à jour incrémentale du FIA : FIASCO. Notre algorithme effectue un seul passage sur les données et permet de prendre en compte les nouveaux batches, itemset par itemset et pour chaque itemset, item par item.	Lionel Vinceslas, Jean-Emile Symphor, Alban Mancheron, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000603	http://editions-rnti.fr/render_pdf.php?p=1000603
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Fouille de données audio pour la classification automatique de mots homophones	Cet article présente une contribution à la modélisation acoustique des mots à partir de grands corpus oraux, faisant appel aux techniques de fouilles de données. En transcription automatique, de nombreuses erreurs concernent des mots fréquents homophones. Deux paires de mots (quasi-)homophones à/a et et/est sont sélectionnées dans les corpus, pour lesquels sont définis et examinés 41 descripteurs acoustiques permettant potentiellement de les distinguer. 17 algorithmes de classification, mis à l'épreuve pour la discrimination automatique de ces deux paires de mots, donnent en moyenne 77% de classification correcte sur les 5 meilleurs algorithmes. En réduisant le nombre de descripteurs à 10 (sélectionnés par l'algorithme le plus performant), les résultats de classification restent proches du résultat obtenu avec 41 attributs. Cette comparaison met en évidence le caractère discriminant de certains attributs, qui pourront venir enrichir à la fois la modélisation acoustique et nos connaissances des prononciations de l'oral.	Rena Nemoto, Martine Adda-Decker, Iona Vasilescu	http://editions-rnti.fr/render_pdf.php?p1&p=1000633	http://editions-rnti.fr/render_pdf.php?p=1000633
Revue des Nouvelles Technologies de l'Information	EGC 	2008	From Mining the Web to Inventing the New Sciences Underlying the Internet	As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business. However, because of its novelty and the scale of data sets involved, few companies have figured out how to properly make use of this data. In this talk, I will review some of the challenges and opportunities in the utilization of data to drive this new generation of marketing systems. I will provide several examples of how data is utilized in critical ways to drive some of these capabilities. The discussion will be framed with theMore general framework of Grand Challenges for data mining : pragmatic and technical. I will conclude this presentation with a consideration of the larger issues surrounding the Internet as a technology that is ubiquitous in our lives, yet one where very little is understood, at the scientific level, in defining and understanding many of the basics the Internet enables : Community, Personalization, and the new Microeconomics of the web. This leads to an overview of the new Yahoo ! Research organization and its aims : inventing the new sciences underlying what we do on the Internet, focusing on areas that have received little attention in the traditional academic circles. Some illustrative examples will be reviewed to make the ultimate goals more concrete.	Usama M. Fayyad	http://editions-rnti.fr/render_pdf.php?p1&p=1000550	http://editions-rnti.fr/render_pdf.php?p=1000550
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Génération de séquence résumé par une nouvelle approche basée sur le Soft Computing	Cet article propose une approche d'abstraction des séquences vidéo basée sur le soft computing. Etant donné une longueur cible du condensé vidéo, on cherche les segments vidéo qui couvrent le maximum du visuel de la vidéo originale en respectant la longueur du condensé.	Youssef Hadi, Rachid El Meziane, Rachid Oulad Haj Thami	http://editions-rnti.fr/render_pdf.php?p1&p=1000586	http://editions-rnti.fr/render_pdf.php?p=1000586
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Gradients de prototypicalité conceptuelle et lexicale	Longtemps les ontologies ont été limitées à des domaines scientifiques et techniques, favorisant au passage l'essor du concept de « connaissances universelles et objectives ». Avec l'émergence et l'engouement actuel pour les sciences cognitives, couplés à l'application des ontologies à des domaines relatifs aux Sciences Humaines et Sociales (SHS), la subjectivité des connaissances devient une dimension incontournable qui se doit d'être intégrée et prise en compte dans le processus d'ingénierie ontologique (IO). L'objectif de nos travaux est de développer la notion d'Ontologie Pragmatisée Vernaculaire de Domaine (OPVD). Le principe sous-jacent à de telles ressources consiste à considérer que chaque ontologie est non seulement propre à un domaine, mais également à un endogroupe donné, doté d'une pragmatique qui est fonction tant de la culture que de l'apprentissage et de l'état émotionnel du dit endogroupe. Cette pragmatique, qui traduit un processus d'appropriation et de personnalisation de l'ontologie considérée, est qualifiée à l'aide de deux mesures : un gradient de prototypicalité conceptuelle et un gradient de prototypicalité lexicale	Xavier Aimé, Frédéric Fürst, Pascale Kuntz, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000564	http://editions-rnti.fr/render_pdf.php?p=1000564
Revue des Nouvelles Technologies de l'Information	EGC	2008	HyperSmooth : calcul et visualisation de cartes de potentiel interactives	Le groupe de recherche Hypercarte propose HyperSmooth,un nouvel outil cartographique pour l'analyse spatiale de phénomènessociaux économiques mettant en oeuvre une méthode de calcul depotentiel. L'objectif est de pouvoir représenter de façon continue et enchangeant d'échelle d'analyse une information statistiqueéchantillonnée sur toutes sortes de maillages, réguliers ou non. Le défitechnologique est de fournir un outil accessible sur le Web, interactifet rapide, ceci malgré le coût élevé du calcul, et qui assure laconfidentialité des données. Nous présentons notre solution basée surune architecture client serveur : le serveur calcule les cartes depotentiel en utilisant des techniques d'optimisation particulières, alorsque le client est en charge de la visualisation et du paramétrage del'analyse, et les deux parties communiquent via un protocole Web.	Christine Plumejeaud, Jean-Marc Vincent, Claude Grasland, Jérôme Gensel, Hélène Mathian, Serge Guelton, Joël Boulier	http://editions-rnti.fr/render_pdf.php?p1&p=1001230	http://editions-rnti.fr/render_pdf.php?p=1001230
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Industrialiser le data mining : enjeux et perspectives	L'informatique décisionnelle est un secteur en forte croissance dans toutes les entreprises. Les techniques classiques (reporting simple & Olap), qui s'intéressent essentiellement à présenter les données, sont aujourd'hui très largement déployées. Le data mining commence à se répandre, apportant des capacités de prévision à forte valeur ajoutée pour les entreprises les plus compétitives. Ce développement est rendu possible par la disponibilité croissante de masses de données importantes et la puissance de calcul dorénavant disponible. Cependant, la mise en IJuvre industrielle des projets de data mining pose des contraintes tant théoriques (quels algorithmes utiliser pour produire des modèles d'analyses exploitant des milliers de variables pour des millions d'exemples) qu'opérationnelles (comment mettre en production et contrôler le bon fonctionnement de centaines de modèles). Je présenterai ces contraintes issues des besoins des entreprises ; je montrerai comment exploiter des résultats théoriques (provenant des travaux de Vladimir Vapnik) pour produire des modèles robustes ; je donnerai des exemples d'applications réelles en gestion de la relation client et en analyse de qualité. Je conclurai en présentant quelques perspectives (utilisation du texte et des réseaux sociaux).	Françoise Fogelman-Soulié	http://editions-rnti.fr/render_pdf.php?p1&p=1000548	http://editions-rnti.fr/render_pdf.php?p=1000548
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Intégration de contraintes dans les cartes auto-organisatrices	Le travail présenté dans cet article décrit une nouvelle version des cartes topologiques que nous appelons CrTM. Cette version consiste à modifier l'algorithme de Kohonen de telle façon à ce qu'il contrôle les violations des contraintes lors de la construction de la topologie de la carte. Nous validons notre approche sur des données connues de la littérature en utilisant des contraintes artificielles. Une validation supplémentaire sera faite sur des données réelles issues d'images médicales pour la classification des mélanomes chez l'humain sous contraintes médicales.	Anouar Benhassena, Khalid Benabdeslem, Fazia Bellal, Alexandre Aussem, Bruno Canitia	http://editions-rnti.fr/render_pdf.php?p1&p=1000663	http://editions-rnti.fr/render_pdf.php?p=1000663
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Intégration de la structure dans un modèle probabiliste de document	En fouille de textes comme en recherche d'information, différents modèles, de type probabiliste, vectoriel ou booléen, se sont révélés bien adaptés pour représenter des documents textuels mais, ces modèles présentent l'inconvénient de ne pas tenir compte de la structure du document. Or la plupart des informations disponibles aujourd'hui sur Internet ou dans des bases documentaires sont fortement structurées. Dans cet article, nous proposons d'étendre le modèle probabiliste de représentation des documents de façon à tenir compte du poids d'une certaine catégorie d'éléments structurels : les balises représentant la structure logique et la structure de mise en forme. Ce modèle a été évalué à l'aide de la collection de la campagne d'évaluation INEX 2006.	Mathias Géry, Christine Largeron, Franck Thollard	http://editions-rnti.fr/render_pdf.php?p1&p=1000660	http://editions-rnti.fr/render_pdf.php?p=1000660
Revue des Nouvelles Technologies de l'Information	EGC	2008	Interprétation automatique d'itinéraires à partir d'un corpus de récits de voyages pilotée par un usage pédagogique.	De larges corpus à fort ancrage territorial deviennent disponibles sousforme numérique dans les médiathèques et plus particulièrement dans les médiathèquesde dimension régionale. Les défis qu'offrent ces gigas octets de documentsbruts sont énormes en terme de traitement automatique des contenus.Nous proposons dans cet article deux modèles computationnels et une méthodecomplète permettant de réaliser un traitement automatique afin d'extraire des itinérairesdans des textes relatant des récits de voyage. Le premier modèle est unmodèle des attendus. Il s'intéresse au concept d'itinéraire et adopte le point devue du pédagogue et fait intervenir très tôt les usages envisagés. Le deuxièmemodèle est un modèle d'extraction, il permet de modéliser l'expression du déplacementdans des textes du genre récit de voyage. Nous proposons alors uneméthode automatique pour : d'une part extraire et interpréter automatiquementles déplacements d'un récit et d'autre part passer des déplacements à l'itinéraire,c'est-à-dire alimenter de manière automatique le modèle des attendus à partir dumodèle d'extraction. Nous montrons également comment les itinéraires extraitsinterviennent soit dans la phase de construction d'activités pédagogiques soitdirectement comme matériau dans une activité d'apprentissage. Nous présentonsenfin ¼R, un Prototype pour l'Interprétation d'Itinéraires dans des Récitsde voyages, qui implémente notre approche. Il prend en entrée un texte brut etfournit l'interprétation de l'itinéraire décrit dans le texte. Il permet également devisualiser sur un fond cartographique l'itinéraire extrait.	Pierre Loustau, Mauro Gaio, Thierry Nodenot	http://editions-rnti.fr/render_pdf.php?p1&p=1001237	http://editions-rnti.fr/render_pdf.php?p=1001237
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Interprétation d'images basée sur une approche évolutive guidée par une ontologie	Les approches de fouille et d'interprétation d'images consistant à considérer les pixels de façon indépendante ont montré leurs limites pour l'analyse d'images complexes. Pour résoudre ce problème, de nouvelles méthodes s'appuient sur une segmentation préalable de l'image qui consiste en une agrégation des pixels connexes afin de former des régions homogènes au sens d'un certain critère. Cependant le lien est souvent complexe entre la connaissance de l'expert sur les objets qu'il souhaite identifier dans l'image et les paramètres nécessaires à l'étape segmentation permettant de les identifier. Dans cet article la connaissance de l'expert est modélisée dans une ontologie qui est ensuite utilisée pour guider un processus de segmentation par une approche évolutive. Cette méthode trouve automatiquement des paramètres de segmentation permettant d'identifier les objets décrits par l'expert dans l'ontologie.	Germain Forestier, Sébastien Derivaux, Cédric Wemmert, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000635	http://editions-rnti.fr/render_pdf.php?p=1000635
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Khiops: outil de préparation et modélisation des données pour la fouille des grandes bases de données	Khiops est un outil de préparation des données et de modélisation pour l'apprentissage supervisé et non supervisé. L'outil permet d'évaluer de façon non paramétrique la corrélation entre tous types de variables dans le cas non supervisé et l'importance prédictive des variables et paires de variables dans le cas de la classification supervisée. Ces évaluations sont effectuées au moyen de modèles de discrétisation dans le cas numérique et de groupement de valeurs dans le cas catégoriel, ce qui permet de rechercher une représentation des données efficace au moyen d'un recodage des variables. L'outil produit également un modèle de scoring pour les tâches d'apprentissage supervisé, selon un classifieur Bayesien naif avec sélection de variables et moyennage de modèles. L'outil est adapté à l'analyse des grandes bases de données, avec des centaines de milliers d'individus et des dizaines de milliers de variables, et a permis de participer avec succès à plusieurs challenges internationaux récents.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000598	http://editions-rnti.fr/render_pdf.php?p=1000598
Revue des Nouvelles Technologies de l'Information	EGC 	2008	La prise en compte de la dimension temporelle dans la classification de données	Dans un contexte d'ingénierie de la connaissance, l'analyse des données relationnelles évolutives est une question centrale. La représentation de ce type de données sous forme de graphe optimisé en facilite l'analyse et l'interprétation par l'utilisateur non expert. Cependant, ces graphes peuvent rapidement devenir trop complexes pour être étudiés dans leur globalité, il faut alors les décomposer de manière à en faciliter la lecture et l'analyse. Pour cela, une solution est de les simplifier, dans un premier temps, en un graphe réduit dont les sommets représentent chacun un groupe distinct de sommets : acteurs ou termes du domaine étudié. Dans un second temps, il faut les décomposer en instances (un graphe par période) afin de prendre en compte la dimension temporelle.La plateforme de veille stratégique Tétralogie, développée dans notre laboratoire, permet de synthétiser les données relationnelles évolutives sous forme de matrices de cooccurrence 3D et VisuGraph, son module de visualisation, permet de les représenter sous forme de graphes évolutifs.VisuGraph assimile les différentes périodes à des repères temporels et chaque sommet est placé en fonction de son degré d'appartenance aux différentes périodes. Ce prototype est aussi doté d'un module de la classification interactive de données relationnelles basé sur une technique de Markov Clustering, qui conduit à une visualisation sous forme de graphe réduit. Nous proposons ici de prendre en compte la dimension temporelle dans notre processus de classification des données. Ainsi, par la visualisation successive des différentes instances, il devient plus facile d'analyser l'évolution des classes au niveau intra mais aussi au niveau inter classes.	Eloïse Loubier, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1000653	http://editions-rnti.fr/render_pdf.php?p=1000653
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Le FIA: un nouvel automate permettant l'extraction efficace d'itemsets fréquents dans les flots de données	Le FIA (Frequent Itemset Automaton) est un nouvel automate qui permet de traiter de façon efficace la problématique de l'extraction des itemsets fréquents dans les flots de données. Cette structure de données est très compacte et informative, et elle présente également des propriétés incrémentales intéressantes pour les mises à jour avec une granularité très fine. L'algorithme développé pour la mise à jour du FIA effectue un unique passage sur les données qui sont prises en compte tout d'abord par batch (i.e., itemset par itemset), puis pour chaque itemset, item par item. Nous montrons que dans le cadre d'une approche prédictive et par l'intermédiaire de la bordure statistique, le FIA permet d'indexer les itemsets véritablement fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur la pertinence statistique des itemsets indexés avec la P-valeur.	Jean-Emile Symphor, Alban Mancheron, Lionel Vinceslas, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000568	http://editions-rnti.fr/render_pdf.php?p=1000568
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Le forage de réseaux sociaux	L'exploitation des réseaux sociaux pour l'extraction de connaissances n'est pas nouvelle. Les anthropologues, sociologues et épidémiologies se sont déjà penchés sur la question. C'est probablement le succès du moteur de recherche Google qui a vulgarisé l'utilisation des parcours aléatoires des réseaux sociaux pour l'ordonnancement par pertinence. Plusieurs applications ont depuis vu naissance. La découverte des communautés dans les réseaux sociaux est aussi une nouvelle tendance de recherche très prisée. Durant cet exposé nous parlerons de l'analyse des réseaux sociaux, la découverte de communautés, et présenterons quelques applications dont l'ordonnancement dans les bases de données	Osmar R. Zaïane	http://editions-rnti.fr/render_pdf.php?p1&p=1000549	http://editions-rnti.fr/render_pdf.php?p=1000549
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Le logiciel SODAS : avancées récentes Un outil pour analyser et visualiser des données symboliques		Myriam Touati, Mohamed Rahal, Filipe Afonso, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000607	http://editions-rnti.fr/render_pdf.php?p=1000607
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Les cartes cognitives hiérarchiques	Une carte cognitive fournit une représentation graphique d'un réseau d'influence entre des concepts. Les cartes cognitives de dimensions importantes ont l'inconvénient d'être difficiles à appréhender, interpréter et exploiter. Cet article présente un modèle de cartes cognitives hiérarchiques permettant au concepteur d'effectuer des regroupements de concepts qui sont ensuite utilisés dans un mécanisme permettant à l'utilisateur d'obtenir des vues partielles et synthétiques d'une carte.	Lionel Chauvin, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000560	http://editions-rnti.fr/render_pdf.php?p=1000560
Revue des Nouvelles Technologies de l'Information	EGC	2008	L'intelligence collective géospatiale au service du diagnostic de territoire : GEOdoc	Le diagnostic de territoire constitue une étape obligatoire dans toutprojet d'aménagement ou dans toute volonté politique de modifier durablementl'espace. Les décideurs politiques doivent avoir une vision objective des actionsà mener en fondant leurs réflexions sur des études et des documents ;qu'ils soient à caractère géographique ou non. Il est donc fondamentald'améliorer l'accès et la consultation, par les décideurs stratégiques, de ce quel'on peut appeler des documents géographiques. Le but de cet article est deprésenter certains concepts et solutions technologiques qui peuvent être utilisésafin de mieux organiser, de naviguer (dans) et de visualiser ces documents. Ilpropose une mise en perspective commune de certaines de ces approches, surlaquelle est fondée la conception d'une première maquette d'un outil de visualisation(et de navigation) de documents géographiques nommé GEOdoc.	Stéphane Roche, Benoit Kiene, Claude Caron	http://editions-rnti.fr/render_pdf.php?p1&p=1001231	http://editions-rnti.fr/render_pdf.php?p=1001231
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Mesures hiérarchiques pondérées pour l'évaluation d'un système semi-automatique d'annotation de génomes utilisant des arbres de décision	L'annotation d'une protéine consiste, entre autres, à lui attribuer une classe dans une hiérarchie fonctionnelle. Celle-ci permet d'organiser les connaissances biologiques et d'utiliser un vocabulaire contrôlé. Pour estimer la pertinence des annotations, des mesures telles que la précision, le rappel, la spécificité et le Fscore sont utilisées. Cependant ces mesures ne sont pas toujours bien adaptées à l'évaluation de données hiérarchiques, car elles ne permettent pas de distinguer les erreurs faites aux différents niveaux de la hiérarchie. Nous proposons ici une représentation formelle pour les différents types d'erreurs adaptés à notre problème.	Lucie Gentils, Jérôme Azé, Claire Toffano-Nioche, Valentin Loux, Anne Poupon, Jean-François Gibrat, Christine Froidevaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000565	http://editions-rnti.fr/render_pdf.php?p=1000565
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Méthodologie de définition de e-services pour la gestion des connaissances à partir d'un plateau de créativité : application au e-learning instrumental	En s'appuyant sur la théorie de l'activité, nous avons mis au point une méthodologie de gestion des connaissances à base de e-services sur un plateau de créativité visant à faire piloter le processus de fabrication métier par celui des usages. Nous l'avons testé avec la réalisation d'un e-service d'apprentissage instrumental de pièces de musique à la guitare (E-guitare).	David Grosser, Noël Conruyt, Olivier Sebastien	http://editions-rnti.fr/render_pdf.php?p1&p=1000591	http://editions-rnti.fr/render_pdf.php?p=1000591
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Méthodologie d'Evaluation Intelligente des Concepts Ontologiques	Un des problèmes majeurs dans la gestion des ontologies est son évaluation. Cet article traite l'évaluation des concepts ontologiques qui sont extraits de pages Web. Pour cela, nous avons proposé une méthodologie d'évaluation des concepts basée trois critères révélateurs : "le degré de crédibilité"; "le degré de cohésion" et "le degré d'éligibilité". Chaque critère correspond à un apport de connaissance pour la tâche d'évaluation. Notre méthode d'évaluation assure une évaluation qualitative grâce aux associations de mots ainsi qu'une évaluation quantitative par le biais des trois degrés. Nos résultats et discussions avec les experts et les utilisateurs ont montré que notre méthode facilite la tâche d'évaluation.	Lobna Karoui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000566	http://editions-rnti.fr/render_pdf.php?p=1000566
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Mining Implications from Lattices of Closed Trees	We propose a way of extracting high-confidence association rules from datasets consisting of unlabeled trees. The antecedents are obtained through a computation akin to a hypergraph transversal, whereas the consequents follow from an application of the closure operators on unlabeled trees developed in previous recent works of the authors. We discuss in more detail the case of rules that always hold, independently of the dataset, since these are more complex than in itemsets due to the fact that we are no longer working on a lattice.	José L. Balcázar, Albert Bifet, Antoni Lozano	http://editions-rnti.fr/render_pdf.php?p1&p=1000625	http://editions-rnti.fr/render_pdf.php?p=1000625
Revue des Nouvelles Technologies de l'Information	EGC	2008	Modélisation conceptuelle des trajectoires	Une perception intelligente du mouvement d'objets mobiles(personnes, voitures, colis, etc.) est à la base de nombreuses applications (parexemple le suivi d'une distribution postale à travers le monde, l'optimisation dutrafic routier ou l'étude de la migration d'animaux). Les systèmes de gestion debases de données actuels n'offrent ni les concepts ni les fonctions nécessaires àune analyse sémantique du mouvement, se limitant au stockage et àl'interrogation de positions spatiales individuelles, hors contexte temporel. Destravaux de recherche précédents ont introduit et développé le concept d'objetmobile ou spatio-temporel. Dans cet article nous allons plus loin en proposantle concept de trajectoire comme unité sémantique de mouvement sur laquellese construit la vision applicative. Nous proposons de décrire les trajectoires, auniveau conceptuel, avec leurs aspects géométriques, temporels et sémantiqueset leurs composants structurels : point de départ, point d'arrivée, arrêts etdéplacements intermédiaires. Chaque élément, trajectoire, arrêt, déplacement,voire partie de déplacement, peut recevoir des annotations sémantiques sousforme de valeurs d'attributs ou de liens vers des objets de la base. L'approchede modélisation décrite dans cet article est basée sur les patrons demodélisation, qui permettent une solution générique pour modéliser lescaractéristiques standard des trajectoires tout en étant ouverte auxcaractéristiques spécifiques à l'application envisagée. Enfin, l'implémentationdans une base de données relationnelle étendue est présentée.	Christine Parent, Stefano Spaccapietra, Christelle Vangenot, Maria-Luisa Damiani, José de Macedo, Fabio Porto	http://editions-rnti.fr/render_pdf.php?p1&p=1001236	http://editions-rnti.fr/render_pdf.php?p=1001236
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Nouvelle approche pour la recherche d'images par le contenu	On utilise l'analyse factorielle des correspondances (AFC) pour la recherche d'images par le contenu en s'inspirant directement de son utilisation en analyse des données textuelles (ADT). L'AFC permet ici de réduire les dimensions du problème et de sélectionner des indicateurs pertinents pour la recherche par le contenu. En ADT, l'AFC est appliquée à un tableau de contingence croisant mots et documents. La première étape consiste donc à définir des « mots visuels » dans les images (analogue des mots dans les textes). Ces mots sont construits à partir des descripteurs locaux (SIFT) des images. La méthode a été testée sur la base Caltech4 (Sivic et al., 2005) sur laquelle elle fournit de meilleurs résultats (qualité des résultats de recherche et temps d'exécution) que des méthodes plus classiques comme TF*IDF/Rocchio (Rocchio, 1971) ou pLSA (Hofmann, 1999a, 1999b). Enfin, pour passer à l'échelle et améliorer la qualité de recherche, nous proposons un nouveau prototype de recherche qui utilise des fichiers inversés basés sur la qualité de représentation des images sur les axes après avoir fait une AFC. Chaque fichier inversé est associé à une partie d'un axe (positive ou négative) et contient des images ayant une bonne qualité de représentation sur cet axe. Les tests réalisés montrent que ce nouveau prototype réduit le temps de recherche sans perte de qualité de résultat et dans certains cas, améliore le taux de précision par rapport à la méthode exhaustive.	Nguyen-Khang Pham, Annie Morin	http://editions-rnti.fr/render_pdf.php?p1&p=1000636	http://editions-rnti.fr/render_pdf.php?p=1000636
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Ontologies et raisonnement à partir de cas : Application à l'analyse des risques industriels	L'analyse de risques est un processus visant à décrire les scénarios conduisant à des phénomènes dangereux et à des accidents potentiels sur une installation industrielle. Pour réaliser une analyse de risques, un expert dispose de nombreuses ressources : rapports, études de dangers, bases d'accidents, etc. Ces ressources sont cependant souvent difficiles à exploiter parce qu'elles ne sont pas suffisamment structurées ni formalisées. Dans le cadre du projet KMGR (Knowledge Management pour la Gestion des Risques), mené en partenariat avec l'Institut National de l'Environnement industriel et des RISques (INERIS), nous proposons de traiter ce problème en développant un système de recherche d'information basé sur des ontologies, et de le compléter par un système de raisonnement à partir de cas (RàPC) pour tenir compte des expériences passées.	Amjad Abou Assali, Dominique Lenne, Bruno Debray	http://editions-rnti.fr/render_pdf.php?p1&p=1000595	http://editions-rnti.fr/render_pdf.php?p=1000595
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Optimisation du Primal pour les SVM	L'apprentissage de SVM par optimisation directe du primal est très étudié depuis quelques temps car il ouvre de nouvelles perspectives notamment pour le traitement de données structurées. Nous proposons un nouvel algorithme de ce type qui combine de façon originale un certain nombre de techniques et idées comme la méthode du sous-gradient, l'optimisation de fonctions continues non partout différentiables, et une heuristique de shrinking.	Trinh Minh Tri Do, Thierry Artières	http://editions-rnti.fr/render_pdf.php?p1&p=1000615	http://editions-rnti.fr/render_pdf.php?p=1000615
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Optimisation incrémentale de réseaux de neurones RBF pour la régression via un algorithme évolutionnaire : RBF-Gene	Les réseaux de neurones RBF sont d'excellents régresseurs. Ils sont cependant difficiles à utiliser en raison du nombre de paramètres libres : nombre de neurones, poids des connexions, ... Des algorithmes évolutionnaires permettent de les optimiser mais ils sont peu nombreux et complexes.Nous proposons ici un nouvel algorithme, RBF-Gene, qui permet d'optimiser la structure et les poids du réseau, grâce à une inspiration biologique. Il est compétitif avec les autres techniques de régression mais surtout l'évolution peut choisir dynamiquement le nombre de neurones et la précision des différents paramètres.	Virginie Lefort, Guillaume Beslon	http://editions-rnti.fr/render_pdf.php?p1&p=1000620	http://editions-rnti.fr/render_pdf.php?p=1000620
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Pondération locale des variables en apprentissage numérique non-supervisé	Dans cet article, nous proposons une nouvelle approche de pondérations des variables durant un processus d'apprentissage non supervisé. Cette méthode se base sur l'algorithme « batch » des cartes auto-organisatrices. L'estimation des coefficients de pondération se fait en parallèle avec la classification automatique. Ces pondérations sont locales et associées à chaque référent de la carte auto-organisatrice. Elles reflètent l'importance locale de chaque variable pour la classification. Les pondérations locales sont utilisées pour la segmentation de la carte topologique permettant ainsi un découpage plus riche tenant compte des pertinences des variables. Les résultats de l'évaluation montrent que l'approche proposée, comparée à d'autres méthodes de classification, offre une segmentation plus fine de la carte et de meilleure qualité.	Nistor Grozavu, Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000619	http://editions-rnti.fr/render_pdf.php?p=1000619
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Prétraitement des bases de données de réactions chimiques pour la fouille de schémas de réactions	Un grand nombre de réactions chimiques sont aujourd'hui répertoriées dans des bases de données. Les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents. Deux obstacles s'opposent à cela : d'une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes d'extraire les schémas de réactions fréquents. D'autre part les bases de données contiennent des descriptions de réactions souvent incomplètes, ambiguës ou erronées. Le présent article décrit un processus de prétraitement opérationnel qui permet de filtrer, compléter puis transformer le contenu d'une base de réactions en des données fiables constituées de graphes abstraits répondant au problème de la fouille de schémas de réactions. Le processus place ainsi les bases de réactions à portée des techniques de fouille de graphes comme en attestent les résultats expérimentaux.	Frédéric Pennerath, Géraldine Polaillon, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000652	http://editions-rnti.fr/render_pdf.php?p=1000652
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Principes d'Analyse des données symboliques et application à la détection d'anomalies sur des ouvrages publics	L'analyse des données Symboliques a pour objectif de fournir des résultatscomplémentaires à ceux fournis par la fouille de données classique encréant des concepts issus de données simples ou complexes puis en analysantces concepts par des descriptions symboliques où les variables expriment lavariation des instances de ces concepts en prenant des valeurs intervalle, histogramme,suites, munies de règles et de taxonomies, etc.	Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001800	http://editions-rnti.fr/render_pdf.php?p=1001800
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Processus d'acquisition d'un dictionnaire de sigles et de leurs définitions à partir d'un corpus	Le logiciel présenté dans cet article s'appuie sur une approche d'acquisition de sigles à partir de données textuelles	Vladislav Matviico, Nicolas Muret, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000599	http://editions-rnti.fr/render_pdf.php?p=1000599
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Proposition d'une nouvelle approche de détection d'intrusions basée sur les règles associatives génériques de classification	Les systèmes de détection d'intrusions (SDIs) ont pour objectif la sécurité des réseaux informatiques. Dans ce papier, nous proposons une nouvelle approche de détection d'intrusions basée sur des règles associatives génériques de classification pour améliorer la qualité de la détection d'intrusions.	Imen Brahmi, Sadok Ben Yahia, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1000589	http://editions-rnti.fr/render_pdf.php?p=1000589
Revue des Nouvelles Technologies de l'Information	EGC	2008	Proposition pour l'intégration de l'analyse spatiale et de l'analyse multidimensionnelle	L'introduction de l'information spatiale dans les modèlesmultidimensionnels a donné naissance au concept de Spatial OLAP (SOLAP).Dans cet article, nous montrons en quoi les spécificités de l'informationgéographique et de l'analyse spatiale ne sont pas entièrement prises en comptedans l'analyse et les modèles multidimensionnels SOLAP. Pour pallier ceslimites, nous proposons le concept de dimension géographique et décrivons lesdifférents types de hiérarchies associées. Nous proposons l'introduction denouveaux opérateurs qui permettent d'adapter les opérateurs d'analyse spatialeau paradigme multidimensionnel. Enfin, nous présentons notre prototype quioffre une interface web de navigation spatiale et multidimensionnelle, etpermet l'intégration de ces nouveaux concepts.	Sandro Bimonte,  Anne Tchounikine, Maryvonne Miquel, Robert Laurini	http://editions-rnti.fr/render_pdf.php?p1&p=1001235	http://editions-rnti.fr/render_pdf.php?p=1001235
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche adaptative de structures de régulation génétique	Nous avons proposé un algorithme original de Fouille de Données, LICORN, afin d'inférer des relations de régulation coopérative à partir de données d'expression. LICORN donne de bons résultats s'il est appliqué à des données de levure, mais le passage à l'échelle sur des données plus complexes (e.g., humaines) est difficile. Dans cet article, nous proposons une extension de LICORN afin qu'il puisse gérer une contrainte de co-régulation adaptative. Une évaluation préliminaire sur des données de transcriptome de tumeurs de vessie montre que les réseaux significatifs sont obtenus à l'aide d'une contrainte de corégulation adaptative de manière beaucoup plus efficace, et qu'ils ont des performances de prédiction équivalentes voire meilleures que celles obtenues par LICORN.	Mohamed Elati, Céline Rouveirol	http://editions-rnti.fr/render_pdf.php?p1&p=1000631	http://editions-rnti.fr/render_pdf.php?p=1000631
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche de motifs spatio-temporels de cas atypiques pour le trafic routier urbain	Un large panel de domaines d'application utilise des réseaux de capteurs géoréférencés pour mesurer divers évènements. Les séries temporelles fournies par ces réseaux peuvent être utilisées dans le but de dégager des connaissances sur les relations spatio-temporelles de l'activité mesurée. Dans cet article, nous proposons une méthode permettant d'abord de détecter des situations atypiques (au sens de l'occurrence) puis de construire des motifs spatio-temporels relatant leur propagation sur un réseau. Le cas étudié est celui du trafic routier urbain. Notre raisonnement se fonde sur l'application de la méthode Space-Time Principal Component Analysis (STPCA) et de la combinaison entre l'information mutuelle et l'algorithme Isomap. Les résultats expérimentaux exécutés sur des données réelles de trafic routier démontrent l'efficacité de la méthode introduite à identifier la propagation de cas atypiques fournissant ainsi un outil performant de prédiction de la circulation intraday à court et moyen terme.	Marc Joliveau, Florian De Vuyst	http://editions-rnti.fr/render_pdf.php?p1&p=1000640	http://editions-rnti.fr/render_pdf.php?p=1000640
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche d'images par noyaux sur graphes de régions	Dans le cadre de la recherche interactive d'images dans une base de données, nous nous intéressons à des mesures de similarité d'image qui permettent d'améliorer l'apprentissage et utilisables en temps réel lors de la recherche. Les images sont représentées sous la forme de graphes d'adjacence de régions floues. Pour comparer des graphes valués nous employons des noyaux de graphes s'appuyant sur des ensembles de chaînes, extraites des graphes comparés. Nous proposons un cadre général permettant l'emploi de différents noyaux et différents types de chaînes(sans cycle, avec boucles) autorisant des appariements inexacts. Nous avons effectué des comparaisons sur deux bases issues de Columbia et Caltech et montré que des chaînes de très faible dimension (longueur inférieur à 3) sont les plus efficaces pour retrouver des classes d'objets.	Philippe-Henri Gosselin, Justine Lebrun, Sylvie Philipp-Foliguet	http://editions-rnti.fr/render_pdf.php?p1&p=1000634	http://editions-rnti.fr/render_pdf.php?p=1000634
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche d'information personnalisée dans les bibliothèques numériques scientifiques	Dans cet article nous présentons nos travaux sur la recherche d'information personnalisée dans les bibliothèques numériques. Nous utilisons des profils utilisateurs qui représentent des intérêts et des préférences des utilisateurs. Les résultats de recherche peuvent être retriés en tenant compte des besoins d'informations spécifiques de différentes personnes, ce qui donne une meilleure précision. Nous étudions différentes méthodes basées sur les citations, sur le contenu textuel des documents et des approches hybrides. Les résultats des expérimentations montrent que nos approches sont efficaces et applicables dans le cadre des bibliothèques numériques.	Thanh-Trung Van, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1000556	http://editions-rnti.fr/render_pdf.php?p=1000556
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Requêtes alternatives dans le contexte d'un entrepôt de données génomiques	Afin d'aider les biologistes à annoter des génomes, ce qui nécessite l'analyse, le croisement, et la comparaison de données provenant de sources diverses, nous avons conçu un entrepôt de données de génomique microbienne. Nous présentons la structure globale flexible de l'entrepôt et son architecture multi-niveaux et définissons des correspondances entre ces niveaux. Nous introduisons ensuite la notion de requête alternative et montrons comment le système peut construire l'ensemble des requêtes alternatives à une requête initiale. Pour cela, nous introduisons un mécanisme d'interrogation qui repose sur l'architecture multi-niveaux, et donnons un algorithme de calcul des requêtes alternatives.	Christine Froidevaux, Frédéric Lemoine	http://editions-rnti.fr/render_pdf.php?p1&p=1000557	http://editions-rnti.fr/render_pdf.php?p=1000557
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Segmentation hiérarchique des cartes topologiques	Dans ce papier, nous présentons une nouvelle mesure de similarité pour la classification des référents de la carte auto-organisatrice qui sera réalisée à l'aide d'une nouvelle approche de classification hiérarchique. (1) La mesure de similarité est composée de deux termes : la distance de Ward pondérée et la distance euclidienne pondérée par la fonction de voisinage sur la carte topologique. (2) Un algorithme à base de fourmis artificielles nommé AntTree sera utilisé pour segmenter la carte auto-organisatrice.Cet algorithme a l'avantage de prendre en compte le voisinage entre les référents et de fournir une hiérarchie des référents avec une complexité proche du nlog(n). La segmentation incluant la nouvelle mesure est validée sur plusieurs bases de données publiques.	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1000662	http://editions-rnti.fr/render_pdf.php?p=1000662
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Semantics of Spatial Window over Spatio-Temporal Data Stream	Dans les systèmes DSMS (Data Stream Management Systems), les données en entrée sont infinies et les requêtes sur celles-ci sont actives tout le temps. Dans le but de satisfaire ces caractéristiques, le fenêtrage temporel est largement utilisée pour convertir le flux infini de données sous forme de relations finies. Mais cette technique est inadaptée pour de nombreuses applications émergentes, en particulier les services de localisation. De nombreuses requêtes ne peuvent pas être traitées en utilisant le fenêtrage temporel, ou seraient traitées plus ecacement à l'aide d'un fenêtrage basé sur l'espace (fenêtrage spatial). Dans cet article, nous analysons la nécessité d'un fenêtrage spatial sur des flux de données spatio-temporels, et proposons, sur la base du langage de requêtes CQL (Continuous Query Language), une syntaxe et une sémantique associées au fenêtrage spatial.	Yi Yu, Talel Abdessalem	http://editions-rnti.fr/render_pdf.php?p1&p=1000570	http://editions-rnti.fr/render_pdf.php?p=1000570
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Sémantique et Réutilisation d'ontologie générique	Dans ce papier, nous enrichissons la méthode Terminae de construction d'ontologie à partir de textes en proposant une semi-automatisation de la construction du modèle conceptuel. Nous présentons un algorithme permettant la conceptualisation d'un terme en s'appuyant sur les informations linguistiques contenues dans l'ontologie générique de référence.	Sylvie Desprès, Sylvie Szulman	http://editions-rnti.fr/render_pdf.php?p1&p=1000563	http://editions-rnti.fr/render_pdf.php?p=1000563
Revue des Nouvelles Technologies de l'Information	EGC 	2008	SOM pour la Classification Automatique Non supervisée de Documents Textuels basés sur Wordnet	Dans cet article, nous proposons la méthode des SOM (cartes auto-organisatrices de Kohonen) pour la classification non supervisée de documents textuels basés sur les n-grammes. La même méthode basée sur les synsets de WordNet comme termes pour la représentation des documents est étudiée par la suite. Ces combinaisons sont évaluées et comparées.	Mimoun Malki, Abdelmalek Amine, Zakaria Elberrichi, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1000596	http://editions-rnti.fr/render_pdf.php?p=1000596
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Stratégies de classification non supervisée basées sur fenêtres superposées : application aux données d'usage du Web	Un problème majeur se pose dans le domaine des flux de données : la distribution sous-jacente des données peut changer sur le temps. Dans cet article, nous proposons trois stratégies de classification non supervisée basée sur des fenêtres superposées. Notre objectif est de pouvoir repérer ces changements dans le temps. Notre approche est appliquée sur un benchmark de données réelles et les conclusions obtenues sont basées sur deux indices de comparaison de partitions.	Alzennyr Da Silva, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1000592	http://editions-rnti.fr/render_pdf.php?p=1000592
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Structure Inference of Bayesian Networks from Data: A New Approach Based on Generalized Conditional Entropy	We propose a novel algorithm for extracting the structure of a Bayesian network from a dataset. Our approach is based on generalized conditional entropies, a parametric family of entropies that extends the usual Shannon conditional entropy. Our results indicate that with an appropriate choice of a generalized conditional entropy we obtain Bayesian networks that have superior scores compared to similar structures obtained by classical inference methods.	Dan A. Simovici, Saaid Baraty	http://editions-rnti.fr/render_pdf.php?p1&p=1000621	http://editions-rnti.fr/render_pdf.php?p=1000621
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Suppression des Itemsets Clés Non Essentiels en Classification basée sur les Règles d'Association	En classification basée sur les règles d'association, les itemsets clés sont essentiels : la suppression des itemsets non clés n'affecte pas la précision du classifieur en construction. Ce travail montre que parmi ces itemsets clés, on peut s'intéresser seulement à ceux de petites tailles. Plus loin encore, il étudie une généralisation d'une propriété importante des itemsets non clés et montre que parmi les itemsets clés de petites tailles, il y a ceux qui ne sont pas significatifs pour la classification. Ces itemsets clés sont dits non essentiels. Ils sont définis via un test de 2. Les expériences menées sur les grands jeux de données montrent que l'optimisation par la suppression de ces itemsets est correcte et efficace.	Viet Phan Luong	http://editions-rnti.fr/render_pdf.php?p1&p=1000626	http://editions-rnti.fr/render_pdf.php?p=1000626
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Système multi-agent argumentatif pour la classification des connaissances cruciales	Dans cet article, nous proposons une approche multi-agent argumentative permettant d'automatiser la résolution des conflits entre décideurs dans un système d'aide à l'identification des connaissances cruciales nommé K-DSS. En effet, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. Notre objectif à travers ce travail est de proposer une approche argumentative permettant de résoudre les conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs.	Inès Saad, Imène Brigui-Chtioui	http://editions-rnti.fr/render_pdf.php?p1&p=1000667	http://editions-rnti.fr/render_pdf.php?p=1000667
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un algorithme de classification topographique non supervisée à deux niveaux simultanés	Une des questions les plus importantes pour la plupart des applications réelles de la classification est de déterminer un nombre approprié de groupes (clusters). Déterminer le nombre optimal de groupes est un problème difficile, puisqu'il n'y a pas de moyen simple pour connaître ce nombre sans connaissance a priori. Dans cet article, nous proposons un nouvel algorithme de classification non supervisée à deux niveaux, appelé S2L-SOM (Simultaneous Twolevel Clustering - Self Organizing Map), qui permet de déterminer automatiquement le nombre optimal de groupes, pendant l'apprentissage d'une carte auto-organisatrice. L'estimation du nombre correct de groupes est en relation avec la stabilité de la segmentation et la validité des groupes générés. Pour mesurer cette stabilité nous utilisons une méthode de sous-échantillonnage. Le principal avantage de l'algorithme proposé, comparé aux méthodes classiques de classification, est qu'il n'est pas limité à la détection de groupes convexes, mais est capable de détecter des groupes de formes arbitraires. La validation expérimentale de cet algorithme sur un ensemble de problèmes fondamentaux pour la classification montre sa supériorité sur les méthodes standards de classification à deux niveaux comme SOM+K-Moyennes et SOM+Hierarchical- Agglomerative-Clustering.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000661	http://editions-rnti.fr/render_pdf.php?p=1000661
Revue des Nouvelles Technologies de l'Information	EGC	2008	Un cyber cartogramme gravitationnel pour l'analyse visuelle de données spatiotemporelles complexes	Le cartogramme présenté dans cet article est destiné à faciliterl'analyse visuelle de données spatiotemporelles complexes. Pour cela, il offrela possibilité de représenter simultanément les trois dimensions nécessaires àtoute forme d'analyse géographique que sont les dimensions spatiale (où),thématique (quoi) et temporelle (quand), à partir de trois composantes principales: (1) une représentation unidimensionnelle (1D) de l'espace géographiquede forme semi-circulaire centrée sur une origine (ex. le Canada) ; (2) desentités géographiques (ex. pays) qui viennent graviter autour de cette origineen fonction de valeurs attributaires ; et (3) une ligne de temps interactive permettantd'explorer la dimension temporelle de l'information représentée. Lacombinaison de ces trois composantes offre de multiples potentialités pourl'analyse spatio-temporelle de différentes formes de proximités qu'elles soientéconomiques, culturelles, sociales ou démographiques. Les fonctionnalités etpotentialités de ce cartogramme développé en source ouverte sont illustrées àpartir d'exemples issus de l'atlas cybercartographique du commerce Canadien.Cet article reprend les grandes lignes d'une communication présentée lors de laconférence SAGEO 2007.	Sébastien Caquard, Jean-Pierre Fiset	http://editions-rnti.fr/render_pdf.php?p1&p=1001229	http://editions-rnti.fr/render_pdf.php?p=1001229
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un modèle d'espace vectoriel de concepts pour noyaux sémantiques	Les noyaux ont été largement utilisés pour le traitement de données textuelles comme mesure de similarité pour des algorithmes tels que les Séparateurs à VasteMarge (SVM). Le modèle de l'espace vectoriel (VSM) a été amplement utilisé pour la représentation spatiale des documents. Cependant, le VSM est une représentation purement statistique. Dans ce papier, nous présentons un modèle d'espace vectoriel de concepts (CVSM) qui se base sur des connaissances linguistiques a priori pour capturer le sens des documents. Nous proposons aussi un noyau linéaire et un noyau latent pour cet espace. Le noyau linéaire exploite les concepts linguistiques pour l'extraction du sens alors que le noyau latent combine les concepts statistiques et linguistiques. En effet, le noyau latent utilise des concepts latents extraits par l'Analyse Sémantique Latente (LSA) dans le CVSM. Les noyaux sont évalués sur une tâche de catégorisation de texte dans le domaine biomédical. Le corpus Ohsumed, bien connu pour sa difficulté de catégorisation, a été utilisé. Les résultats ont montré que les performances de catégorisation sont améliorées dans le CSVM.	Sujeevan Aseervatham	http://editions-rnti.fr/render_pdf.php?p1&p=1000659	http://editions-rnti.fr/render_pdf.php?p=1000659
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un modèle et une algèbre pour les systèmes de gestion d'ontologies	Nous présentons ici une approche pour la gestion de bases d'ontologies basée sur un modèle comprenant, outre la définition formelle des concepts (sous forme d'axiomes de logique de description), d'autres éléments descriptifs (termes, commentaires et arguments), ainsi que leurs liens d'alignement avec des concepts d'autres ontologies. L'adaptation ou la combinaison d'ontologies se font grâce à une algèbre comprenant des opérations telles que la sélection, la projection, l'union ou la jointure d'ontologies. Ces opérations agissent au niveau des axiomes, des éléments descriptifs et des liens d'alignement.	Gilles Falquet, Claire-Lise Mottaz Jiang, Jacques Guyot	http://editions-rnti.fr/render_pdf.php?p1&p=1000670	http://editions-rnti.fr/render_pdf.php?p=1000670
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un nouveau système immunitaire artificiel pour l'apprentissage non supervisé	Nous proposons dans ce papier un nouveau système immunitaire artificiel (SIA) appelé système NK, pour la détection de comportement du soi non soi avec une approche non supervisée basée sur le mécanisme de cellule NK (Naturel Killer). Dans ce papier, le système NK est appliqué à la détection de fraude en téléphonie mobile.	Rachid Elmeziane, Ilham Berrada, Ismail Kassou	http://editions-rnti.fr/render_pdf.php?p1&p=1000585	http://editions-rnti.fr/render_pdf.php?p=1000585
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un processus d'acquisition d'information pour les besoins de l'enrichissement des BDG	Les données constituent l'élément central d'un Système d'Information Géographiques (SIG) et leur coût est souvent élevé en raison de l'investissement substantiel qui permet leur production. Cependant, ces données sont souvent restreintes à un service ou pour une catégorie d'utilisateurs. Ce qui a fait ressortir la nécessité de proposer des moyens d'enrichissement en informations pertinentes pour un nombre plus important d'utilisateurs. Nous présentons dans ce papier notre approche d'enrichissement de données qui se déroule selon trois étapes : une identification de segments et de thèmes associés, une délégation et enfin, un filtrage textuel. Un processus de raffinement est également offert. Notre approche globale a été intégrée à un SIG. Son évaluation a été accomplie montrant ainsi sa performance.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1000668	http://editions-rnti.fr/render_pdf.php?p=1000668
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un système de vote pour la classification de textes d'opinion	Les tâches de classification textuelle ont souvent pour objectif de regrouper thématiquement différents textes. Dans cet article, nous nous sommes intéressés à la classification de documents en fonction des opinions et jugements de valeurs qu'ils contiennent. L'approche proposée est fondée sur un système de vote utilisant plusieurs méthodes de classification.	Michel Plantié, Mathieu Roche, Gérard Dray	http://editions-rnti.fr/render_pdf.php?p1&p=1000657	http://editions-rnti.fr/render_pdf.php?p=1000657
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une aide à la découverte de mappings dans SomeRDFS	Dans cet article, nous nous intéressons à la découverte de mises en correspondance entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données P2P SomeRDFS. Plus précisément, nous montrons comment exploiter les mécanismes de raisonnement mis en oeuvre dans SomeRDFS pour aider à découvrir des mappings entre ontologies. Ce travail est réalisé dans le cadre du projet MediaD en partenariat avec France Telecom R&D.	François-Élie Calvier, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000671	http://editions-rnti.fr/render_pdf.php?p=1000671
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une approche ensembliste inspirée du boosting en classification non supervisée	En classification supervisée, de nombreuses méthodes ensemblistes peuvent combiner plusieurs hypothèses de base afin de créer une règle de décision finale plus performante. Ainsi, il a été montré que des méthodes comme le bagging ou le boosting pouvaient se révéler intéressantes, tant dans la phase d'apprentissage qu'en généralisation. Dès lors, il est tentant de vouloir s'inspirer des grands principes d'une méthode comme le boosting en classification non supervisée. Or, il convient préalablement de se confronter aux difficultés connues de la thématique des ensembles de regroupeurs (correspondance des classes, agrégation des résultats, qualité) puis d'introduire l'idée du boosting dans un processus itératif. Cet article propose une méthode ensembliste inspirée du boosting, qui, à partir d'un partitionnement flou obtenu par les c-moyennes floues (fuzzy-c-means), va insister itérativement sur les exemples difficiles pour former une partition dure finale plus pertinente.	Romain Billot, Henri-Maxime Suchier, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000624	http://editions-rnti.fr/render_pdf.php?p=1000624
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une approche ontologique pour automatiser le contrôle de conformité dans le domaine du bâtiment	Cet article présente la méthode et le système C3R pour vérifier de façon semi-automatique la conformité d'un projet de construction par rapport à des normes du bâtiment. Les projets de construction sont représentés par des graphes RDF et les normes par des requêtes SPARQL ; le processus de contrôle consiste en l'appariement des requêtes et des graphes. Son efficacité repose sur l'acquisition de connaissances ontologiques et sur un processus d'extraction de connaissances guidé par ce but spécifique de contrôle de conformité qui prend en compte les connaissances ontologiques acquises. Elle repose ensuite sur des méta-connaissances acquises auprès des experts du CSTB qui permettent de guider le contrôle lui-même : les requêtes représentant les normes sont annotées et organisées selon ces annotations. Ces annotations sont également utilisées dans les interactions avec l'utilisateur de C3R pour expliquer les résultats du processus de validation, en particulier en cas d'échec.	Anastasiya Yurchyshyna, Catherine Faron-Zucker, Nhan Le Thanh, Celson Lima	http://editions-rnti.fr/render_pdf.php?p1&p=1000562	http://editions-rnti.fr/render_pdf.php?p=1000562
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une J-mesure orientée pour élaguer des modèles de chroniques		Marc Le Goc, Nabil Benayadi	http://editions-rnti.fr/render_pdf.php?p1&p=1000593	http://editions-rnti.fr/render_pdf.php?p=1000593
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une mesure de similarité contextuelle pour l'aide à la navigation dans un treillis	La recherche d'information et la navigation dans les pages web s'avèrent complexes du fait du volume croissant des données et de leur manque de structure. La formalisation conceptuelle d'un contexte associé à une ontologie rend possible l'amélioration de ce processus. Nous définissons un contexte conceptuel comme étant l'association d'un treillis de concepts construit à partir de pages web avec des ontologies. La recherche et la navigation peuvent alors s'effectuer à plusieurs niveaux d'abstraction : le niveau des données, le niveau conceptuel et le niveau sémantique. Cet article s'intéresse essentiellement au niveau conceptuel grâce à une représentation par les treillis de concepts des documents selon les termes qu'ils ont en commun. Notre objectif est de proposer une mesure de similarité permettant à l'utilisateur de mieux naviguer dans le treillis. En effet, une bonne interprétation du treillis devrait passer par un choix rigoureux des concepts, objets, relations et propriétés les plus intéressants. Pour faciliter la navigation, il faut pouvoir indiquer à l'utilisateur les concepts les plus pertinents par rapport au concept correspondant à sa requête ou pouvoir lui proposer un point de départ. L'originalité de notre proposition réside dans le fait de considérer un lien sémantique entre les concepts du treillis, basé sur une extension des mesures de similarité utilisées dans le cadre des ontologies, afin de permettre une meilleure exploitation de ce treillis. Nous présentons les résultats expérimentaux de l'application de cette mesure sur des treillis construits à partir de pages web dans le domaine du tourisme.	Saoussen Sakji, Marie-Aude Aufaure, Géraldine Polaillon, Bénédicte Le Grand, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1000561	http://editions-rnti.fr/render_pdf.php?p=1000561
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une nouvelle approche du boosting face aux données bruitées	La réduction de l'erreur en généralisation est l'une des principales motivations de la recherche en apprentissage automatique. De ce fait, un grand nombre de travaux ont été menés sur les méthodes d'agrégation de classifieurs afin d'améliorer, par des techniques de vote, les performances d'un classifieur unique. Parmi ces méthodes d'agrégation, le boosting est sans doute le plus performant grâce à la mise à jour adaptative de la distribution des exemples visant à augmenter de façon exponentielle le poids des exemples mal classés. Cependant, en cas de données fortement bruitées, cette méthode est sensible au sur-apprentissage et sa vitesse de convergence est affectée. Dans cet article, nous proposons une nouvelle approche basée sur des modifications de la mise à jour des exemples et du calcul de l'erreur apparente effectuées au sein de l'algorithme classique d'AdaBoost. Une étude expérimentale montre l'intérêt de cette nouvelle approche, appelée Approche Hybride, face à AdaBoost et à BrownBoost, une version d'AdaBoost adaptée aux données bruitées.	Emna Bahri, Mondher Maddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000623	http://editions-rnti.fr/render_pdf.php?p=1000623
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une nouvelle méthode divisive en classification non supervisée pour des données symboliques intervalles	Dans cet article nous présentons une nouvelle méthode de classification non supervisée pour des données symboliques intervalles. Il s'agit de l'extension d'une méthode de classification non supervisée classique à des données intervalles. La méthode classique suppose que les points observés sont la réalisation d'un processus de Poisson homogène dans k domaines convexes disjoints de Rp. La première partie de la nouvelle méthode est une procédure monothétique divisive. La règle de coupure est basée sur une extension à des données intervalles du critère de classification des Hypervolumes. L'étape d'élagage utilise un test statistique basé sur le processus de Poisson homogène. Le résultat est un arbre de décision. La seconde partie de la méthode consiste en une étape de recollement, qui permet, dans certains cas, d'améliorer la classification obtenue à la fin de la première partie de l'algorithme. La méthode est évaluée sur un ensemble de données réelles.	Nathanaël Kasoro, André Hardy	http://editions-rnti.fr/render_pdf.php?p1&p=1000664	http://editions-rnti.fr/render_pdf.php?p=1000664
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une proposition pour l'extraction de relations non prédicatives	Les relations sémantiques généralement reconnues par les méthodes d'extraction sont portées par des structures de type prédicats-arguments. Or, l'information recherchée est souvent répartie sur plusieurs phrases. Pour détecter ces relations dites complexes, nous proposons un modèle de représentation des connaissances basé sur les graphes conceptuels.	Mouna Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1000590	http://editions-rnti.fr/render_pdf.php?p=1000590
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Utilisation du Web Sémantique pour la gestion d'une liste de diffusion d'une CoP	Cet article décrit une approche de création semi-automatique d'ontologies et d'annotations sémantiques à partir de messages électroniques échangés dans une liste de diffusion dédiée au support informatique. Les ressources sémantiques générées permettront d'identifier les questions fréquemment posées (FAQ) à travers une recherche guidée par cette ontologie.	Bassem Makni, Khaled Khelif, Rose Dieng-Kuntz, Hacène Cherfi	http://editions-rnti.fr/render_pdf.php?p1&p=1000553	http://editions-rnti.fr/render_pdf.php?p=1000553
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers des Machines à Vecteurs de Support "Actionnables" : Une Approche Fondée sur le Classement	Une des principales critiques que l'on puisse faire aux Séparateurs à Vaste Marge (SVM) est le manque d'intelligibilité des résultats. En effet, il s'agit d'une technique "boite noire" qui ne fournit pas d'explications ni d'indices quant aux raisons d'une classification. Les résultats doivent être pris tels quels en faisant confiance au système qui les a produits. Pourtant selon notre expérience pratique, les experts du domaine préfèrent largement une méthode d'apprentissage avec explications et recommandation d'actions plutôt qu'une boite noire, aussi performante et prédictive soit-elle. Dans cette thématique, nous proposons une nouvelle approche qui consiste a rendre les SVM plus "actionnables". Ce but est atteint en couplant des modèles de classement des résultats des SVM à des méthodes d'apprentissage de concepts. Nous présentons une application de notre méthode sur diverses données dont des données médicales concernant des patients de l'athérosclérose. Nos résultats empiriques semblent très prometteurs et montrent l'utilité de notre approche quant à l'intelligibilité et l'actionnabilité des résultats produits par SVM.	Ansaf Salleb-Aouissi, Bert C. Huang, David L. Waltz	http://editions-rnti.fr/render_pdf.php?p1&p=1000616	http://editions-rnti.fr/render_pdf.php?p=1000616
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers l'exploitation de grandes masses de données	Une tendance lourde depuis la fin du siècle dernier est l'augmentation exponentielle du volume des données stockées. Cette augmentation ne se traduit pas nécessairement par une information plus riche puisque la capacité à traiter ces données ne progresse pas aussi rapidement. Avec les technologies actuelles, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l'information produite. Nous proposons une approche industrielle permettant d'augmenter considérablement notre capacité à transformer des données en information grâce à l'automatisation des traitements et à la focalisation sur les seules données pertinentes.	Raphaël Feraud, Marc Boullé, Fabrice Clérot, Françoise Fessant	http://editions-rnti.fr/render_pdf.php?p1&p=1000609	http://editions-rnti.fr/render_pdf.php?p=1000609
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers l'intégration de la prédiction dans les cubes OLAP		Anouck Bodin-Niemczuk, Riadh Ben Messaoud, Sabine Loudcher, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000584	http://editions-rnti.fr/render_pdf.php?p=1000584
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers une fouille sémantique des brevets : Application au domaine biomédical	Les brevets sont une source d'information très riche puisque ce sont des documents qui servent à décrire les inventions. L'accès aux documents de brevets en ligne est possible grâce aux efforts des offices nationaux de la propriété intellectuelle. Par ailleurs, ayant des objectifs différents, la présentation de ces documents a pris des formes variées loin d'être unifiées. Ce papier présente une méthode et un système permettant l'analyse de brevets "Patent Mining" pour générer des annotations sémantiques. L'idée principale est de pouvoir prendre en considération la structure des brevets pour pouvoir trouver un lien entre le contenu du brevet et les concepts des différentes ontologies.	Nizar Ghoula, Khaled Khelif, Rose Dieng-Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000552	http://editions-rnti.fr/render_pdf.php?p=1000552
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Visualisation des motifs séquentiels extraits à partir d'un corpus en Ancien Français	Cet article présente une interface permettant de visualiser des motifs séquentiels extraits à partir de données textuelles en Ancien Français.	Julien Rabatel, Yuan Lin, Yoann Pitarch, Hassan Saneifar, Claire Serp, Mathieu Roche, Anne Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000605	http://editions-rnti.fr/render_pdf.php?p=1000605
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Visualisation et classification des parcours de vie	Cet article propose une méthodologie pour la visualisation et la classification des parcours de vie. Plus spécifiquement, nous considérons les parcours de vie d'individus suisses nés durant la première moitié du XXème siècle en utilisant les données provenant de l'enquête biographique rétrospective menée en 2002 par le Panel suisse de ménages. Nous nous sommes concentrés sur ces événements du parcours de vie : le départ du foyer parental, la naissance du premier enfant, le premier mariage et le premier divorce. A partir des données de base sur ces événements, nous discutons de leur transformation en séquences d'états. Nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie. Cette méthodologie repose sur des distances calculées par un algorithme d'optimal matching. Ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à l'aide de techniques de « Multi Dimensional Scaling ». Cet article s'intéresse en particulier aux problématiques entourant l'application de ces méthodes aux données de parcours de vie.	Nicolas S. Müller, Sylvain Lespinats, Gilbert Ritschard, Matthias Studer, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1000638	http://editions-rnti.fr/render_pdf.php?p=1000638
Revue des Nouvelles Technologies de l'Information	EGC 	2008	Web Content Data Mining : la classification croisée pour l'analyse textuelle d'un site Web	Notre objectif dans cet article est l'analyse textuelle d'un site Web indépendamment de son usage. Notre approche se déroule en trois étapes. La première étape consiste au typage des pages afin de distinguer les pages de navigation ou pages « auxiliaires » des pages de contenu. La deuxième étape consiste au prétraitement du contenu des pages de contenu afin de représenter chaque page par un vecteur de descripteurs. La dernière étape consiste au block clustering ou la classification simultanée des lignes et des colonnes de la matrice croisant les pages aux descripteurs de pages afin de découvrir des biclasses de pages et de descripteurs. L'application de cette approche au site de tourisme de Metz prouve son efficacité et son applicabilité. L'ensemble de classes de pages groupés en thèmes facilitera l'analyse ultérieure de l'usage du site.	Malika Charrad, Yves Lechevallier, Gilbert Saporta, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000555	http://editions-rnti.fr/render_pdf.php?p=1000555
Revue des Nouvelles Technologies de l'Information	LMO	2008	Adaptation d'un processus de construction d'abstractions  basé IDM à des modèles bi-niveaux éléments / méta-éléments  Application aux logiques de description	L'Analyse Relationnelle de Concepts (ARC) permet la découverte d'abstractions dans différents artéfacts logiciels : code Java ou modèle UML par exemple. Une approche utilisant le paradigme d'Ingénierie Dirigée par les Modèles (IDM) appliqué à l'ARC permet d'obtenir un processus d'abstraction générique : il est possible de construire des abstractions pour n'importe quel artéfact d'entrée (UML ou Java par exemple) en paramétrant simplement l'approche pour le type d'artéfact d'entrée. Cette approche a jusqu'ici été appliquée uniquement à des artéfacts à un niveau (modèles de classes essentiellement). Nous proposons ici une étude pour l'application à des artéfacts à deux niveaux, mélangeant des éléments et des méta-éléments (mélange de classes et d'instances, ou d'associations et de liens par exemple). Cette étude est appliquée aux logiques de description pour lesquelles nous avons développé une extension de l'outil ARC-IDM.	Xavier Dolques, Jean-Rémy Falleri, Marianne Huchard, Clémentine Nebut	http://editions-rnti.fr/render_pdf.php?p1&p=1000606	http://editions-rnti.fr/render_pdf.php?p=1000606
Revue des Nouvelles Technologies de l'Information	LMO	2008	Alignement d'ontologies dirigé par la structure	L'alignement d'ontologies met en évidence les relations sémantiques entre les entités de deux ontologies à confronter. L'outil de choix pour l'alignement est une mesure de similarité sur les couples d'entités. Certaines méthodes d'alignement performantes font dépendre la similarité d'un couple de celles des couples voisins. La circularité dans les définitions résultantes est traitée par le calcul itératif d'un point fixe. Nous proposons un cadre unificateur, appelé alignement dirigé par la structure, qui permet de décrire ces méthodes en dépit de divergences d'ordre technique. Celui-ci combine l'appariement de graphes et le calcul matriciel. Nous présentons son application à la ré-implémentation de l'algorithme OLA, baptisée OLA2.	Jean François Djoufak Kengue, Jérôme Euzenat, Petko Valtchev	http://editions-rnti.fr/render_pdf.php?p1&p=1000577	http://editions-rnti.fr/render_pdf.php?p=1000577
Revue des Nouvelles Technologies de l'Information	LMO	2008	Assemblage automatique et adaptation d'applications à base de composants	Dans cet article, nous introduisons MADCAR, un modèle de moteurs dédiés à la construction et à la reconfiguration dynamique et automatique d'applications à base de composants. Dans MADCAR, la description d'une application regroupe la définition des configurations valides et les règles de transfert de l'état de l'application lors des adaptations. Cette description est découplée de toute implémentation et peut donc être réutilisée avec différents jeux de composants. Partant d'une description d'application, un moteur MADCAR construit un problème de contraintes dont la résolution permet le choix de la configuration cible et des composants à utiliser. Ce choix prend en compte le coût de la configuration cible et son adéquation avec les ressources disponibles. Afin d'assurer la cohérence de l'application, le moteur utilise les règles de transfert d'état pour initialiser les attributs des composants de l'assemblage cible à partir des attributs des composants de l'assemblage de départ.	Guillaume Grondin, Noury Bouraqadi, Laurent Vercouter	http://editions-rnti.fr/render_pdf.php?p1&p=1000574	http://editions-rnti.fr/render_pdf.php?p=1000574
Revue des Nouvelles Technologies de l'Information	LMO	2008	Construction dynamique d'annuaires de composants par classification de services	Les annuaires de composants permettent d'indexer et de localiser rapidement les composants selon les services qu'ils offrent. Ils donnent ainsi aux assemblages en cours d'exécution la possibilité d'évoluer dynamiquement par remplacement de composants, en cas de défaillance, ou par intégration de nouvelles fonctionnalités, en réponse à de nouveaux besoins. Dans ce travail, nous visons des méthodes semi-automatiques d'évolution. Nous posons les bases théoriques d'une utilisation de l'Analyse Formelle de Concepts pour une construction incrémentale des annuaires de composants basée sur les définitions syntaxiques des services requis et fournis. Dans ces annuaires, les composants sont organisés de manière plus intelligible et les descriptions externes de composants plus abstraits et plus réutilisables sont suggérées. Mais surtout, cette organisation rend plus efficaces les tâches automatisées d'assemblage et de remplacement.	Gabriela Arévalo, Nicolas Desnos, Marianne Huchard, Christelle Urtado, Sylvain Vauttier	http://editions-rnti.fr/render_pdf.php?p1&p=1000575	http://editions-rnti.fr/render_pdf.php?p=1000575
Revue des Nouvelles Technologies de l'Information	LMO	2008	Détection visuelle d'anomalies de conception	De nos jours, les logiciels doivent être flexibles pour pouvoir accommoder d'éventuels changements. Les anomalies de conception introduites durant l'évolution du logiciel causent souvent des difficultés de maintenance. Cependant, la détection d'anomalies de conception n'est pas triviale. La détection manuelle est coûteuse en temps et en ressources, alors que la détection automatique génère trop de faux positifs. Dans cet article, nous proposons une approche semiautomatique pour la détection visuelle d'anomalies de conception. Pour aider la détection visuelle, nous utilisons un générateur qui transforme une tâche d'analyse décrite dans un langage de haut niveau en un scénario d'utilisation pour un environnement de visualisation. Finalement, nous illustrons notre approche avec un problème particulier de détection d'anomalies en utilisant la visualisation et les métriques de code.	Karim Dhambri, Salima Hassaine, Houari Sahraoui, Pierre Poulin	http://editions-rnti.fr/render_pdf.php?p1&p=1000600	http://editions-rnti.fr/render_pdf.php?p=1000600
Revue des Nouvelles Technologies de l'Information	LMO	2008	Étude de la changeabilité des systèmes orientés objet	Plusieurs études montrent qu'avec le temps, la plupart des systèmes deviennent difficiles à maintenir et que leur croissance ralentit. Il existe cependant certains systèmes qui utilisent les mécanismes fournis par le paradigme des objets pour soutenir un rythme de développement élevé. Dans cet article, nous étudions les facteurs qui affectent la changeabilité de quatre logiciels libres populaires. Deux applications et deux librairies ont été sélectionnées, puis caractérisées avec des métriques orientées objet classiques. Ces informations ont été utilisées pour bâtir des modèles de prédiction de changement avec des techniques d'apprentissage automatique. Dans le cas de deux librairies avec des modèles de domaine suffisamment précis, les modèles prédictifs ont été capables d'estimer correctement le taux de changement dans le code. Dans le cas de deux applications, ces modèles étaient beaucoup moins précis, mais il a été toutefois possible de prédire les changements dans les classes responsables des interfaces graphiques.	Stéphane Vaucher, Houari Sahraoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000604	http://editions-rnti.fr/render_pdf.php?p=1000604
Revue des Nouvelles Technologies de l'Information	LMO	2008	Expression qualitative de politiques d'adaptation pour les composants Fractal	Les plates-formes d'exécution récentes telles que Fractal ou OpenCOM offrent de nombreuses facilités pour assurer la prise en compte de propriétés extra-fonctionnelles (introspection, sondes, chargement dynamique, etc). Cependant, l'intégration de politiques d'adaptation reste délicate car elle nécessite de corréler la configuration du système avec l'évolution de son environnement. Le travail présenté dans cet article propose une description qualitative des évolutions de l'environnement et une interprétation possible basée sur de la logique floue. L'article présente également une extension de la plate-forme Fractal implémentant les mécanismes nécessaires à l'exécution de ces politiques d'adaptation de haut niveau. L'approche est illustrée à l'aide d'un serveur HTTP qui modifie sa configuration (architecturale et locale) en fonction de plusieurs paramètres extra-fonctionnels tels que la charge du serveur et la dispersion des requêtes.	Franck Chauvel, Olivier Barais, Noël Plouzeau, Isabelle Borne, Jean-Marc Jézéquel	http://editions-rnti.fr/render_pdf.php?p1&p=1000573	http://editions-rnti.fr/render_pdf.php?p=1000573
Revue des Nouvelles Technologies de l'Information	LMO	2008	Extraction métaheuristique d'une architecture à base de  composants à partir d'un système orienté objet	La modélisation et la représentation des architectures logicielles sont devenues une des phases principales du processus de développement de systèmes complexes. En effet, la représentation de l'architecture fournit de nombreux avantages pendant tout le cycle de vie du logiciel. Cependant pour beaucoup de systèmes existants, aucune représentation fiable de leurs architectures n'est disponible. Afin de pallier cette absence, source de nombreuses difficultés, nous proposons, dans cet article une approche, appelée ROMANTIC, visant à extraire une architecture à base de composants à partir d'un système orienté objet existant. L'idée première de cette approche est de proposer un processus quasi automatique d'identification d'architecture en formulant le problème comme un problème d'optimisation et en le résolvant au moyen de métaheuristiques. Ces dernières explorent l'espace composé des architectures pouvant être abstraites du système.	Sylvain Chardigny, Abdelhak Seriai, Mourad Oussalah, Dalila Tamzalit	http://editions-rnti.fr/render_pdf.php?p1&p=1000576	http://editions-rnti.fr/render_pdf.php?p=1000576
Revue des Nouvelles Technologies de l'Information	LMO	2008	Filling in the Whitespace: Research Opportunities in Model-Driven Software Development and Model-Driven	Model-driven development (MDD) is an approach to development of software systems that is steadily gaining new adherents in industry. This is due to its clear potential for major improvements in productivity and product quality over more traditional development methods. There have been numerous successful applications of MDD in industrial and other production environments that have unequivocally demonstrated its viability and effectiveness. However, at present, MDD comprises a rather mixed collection of ad hoc technologies and methods, mostly of which were developed by industrial teams responding to specific problems and immediate market requirements. Consequently, there is very little or no clear theoretical foundation that would provide the basis for systematic and technically sound introduction and application of this important new approach. This has not only resulted in technical problems, but has also greatly impeded the wider penetration of MDD in practice. In this talk, we first look at some of the key characteristics of MDD, discuss the current state of the art, and review some of the salient results achieved in practice. Next, we examine what is needed to provide the necessary theoretical basis for MDD and identify specific key areas of required research. Finally, we describe a new initiative by the Ontario Centres of Excellence and the IBM Centre for Advanced Studies created specifically to support the exploration of this fertile new terra incognita.	Bran Selic	http://editions-rnti.fr/render_pdf.php?p1&p=1000571	http://editions-rnti.fr/render_pdf.php?p=1000571
Revue des Nouvelles Technologies de l'Information	LMO	2008	Génération automatique d'algorithmes de détection des  défauts de conception	Les défauts de conception sont des problèmes récurrents de conception qui diminuent la qualité des programmes. Plusieurs approches outillées de détection des défauts ont été proposées dans la littérature mais, à notre connaissance, elles utilisent toutes des algorithmes de détection ad-hoc, ce qui rend difficile leur généralisation à d'autres défauts. De plus, elles sont basées principalement sur des métriques, qui ne rendent pas compte de certaines caractéristiques importantes des systèmes analysés, telles que leur architecture. Dans cet article, nous développons notre approche basée sur un méta-modèle des défauts de conception en présentant une génération automatique des algorithmes de détection à partir de gabarits. Nous présentons aussi les performances de la génération et évaluons les algorithmes générés en terme de précision et de rappel. Nous fournissons ainsi des moyens concrets pour automatiser la génération des algorithmes de détection et donc détecter de nouveaux défauts tout en prenant en compte toutes les caractéristiques des systèmes.	Naouel Moha, Foutse Khomh, Yann-Gaël Guéhéneuc, Laurence Duchien, Anne-Françoise Le Meur	http://editions-rnti.fr/render_pdf.php?p1&p=1000602	http://editions-rnti.fr/render_pdf.php?p=1000602
Revue des Nouvelles Technologies de l'Information	LMO	2008	Object-Oriented Technologies: A Scientific Perspective	For the last three decades, a great many object-oriented methodologies, languages, and frameworks have been proposed, applied, and disseminated throughout the academic and practitioners' communities. Each new conference features new ideas, tools, or viewpoints. However, what do we know about their applicability, cost, and benefits? For most of them, next to nothing. Adopting one technology among a set of alternatives is mostly a matter of opinion, an act of faith. And god knows opinions abound on these matters. Can we somehow assess and adopt technologies in a more rational fashion? Can we be more scientific about the way we discuss new ways of solving a problem? Can we be more engineering-like by being more driven by problems, and perhaps less by ideology? This talk will discuss how the experimental paradigm what is sometimes referred to as the scientific method can be instrumental in providing the information we need to make informed decisions, as this is the case in many other scientific fields. I will focus the discussion on object-oriented technologies (e.g., UML modeling) and will illustrate my talk with examples of real experiments and case studies.	Lionel Brian	http://editions-rnti.fr/render_pdf.php?p1&p=1000572	http://editions-rnti.fr/render_pdf.php?p=1000572
Revue des Nouvelles Technologies de l'Information	LMO	2008	Synthèse d'observateurs à partir d'exigences temporelles	A contrario des normes UML 2.1 et SysML, le profil UML TURTLE (Timed UML and RT-LOTOS Environment) dispose d'une sémantique formelle et d'une méthodologie. Avec les systèmes temps réel pour cible, cette méthodologie met l'accent sur la vérification formelle du comportement des objets. Le profil TURTLE est doté d'un langage graphique et formalisé d'expression d'exigences temporelles. La contribution de cet article réside dans la présentation d'algorithmes de génération d'observateurs à partir d'exigences temporelles exprimées dans ce langage. Ces observateurs sont destinés à guider la vérification formelle et en particulier à confronter le comportement des objets aux exigences temporelles tout en traçant ces dernières au long de la trajectoire de conception du système en cours d'étude. Un dispositif de charge d'une batterie de véhicule hybride sert d'étude de cas.	Benjamin Fontan, Pierre de Saqui-Sannes, Ludovic Apvrille	http://editions-rnti.fr/render_pdf.php?p1&p=1000613	http://editions-rnti.fr/render_pdf.php?p=1000613
Revue des Nouvelles Technologies de l'Information	LMO	2008	Un framework de traçabilité pour des transformations à  caractère impératif	Cet article s'inscrit dans le cadre de l'ingénierie dirigée par les modèles et apporte une contribution au problème de la traçabilité des artefacts de modélisation durant une chaîne de transformations écrites dans un langage impératif. L'approche que nous proposons nécessite peu d'interventions de l'utilisateur. Nous introduisons un métamodèle générique des traces qui permet entre autres d'apporter une dimension multi-échelles aux traces grâce à l'application du patron de conception composite. Le principe de notre approche est de surveiller certaines catégories d'opérations intéressantes pour la génération de traces pertinentes. Ces catégories sont définies à l'aide du type des objets manipulés par les opérations. Une fois les catégories définies, la trace est générée par du code dédié qui est injecté automatiquement dans la transformation, autour des opérations caractérisées par les catégories définies. Un prototype a été réalisé pour les transformations de modèles écrites en Java, sur le framework EMF. L'injection du code dédié à la traçabilité est réalisée à l'aide de la programmation par aspects.	Bastien Amar, Jean-Rémy Falleri, Marianne Huchard, Clémentine Nebut, Hervé Leblanc	http://editions-rnti.fr/render_pdf.php?p1&p=1000608	http://editions-rnti.fr/render_pdf.php?p=1000608
Revue des Nouvelles Technologies de l'Information	LMO	2008	Un langage de contexte de preuve  pour la validation formelle de modèles logiciels	Pour améliorer les pratiques dans le domaine de la validation formelle de modèles, nous explorons un axe de recherche dans lequel nous formalisons la notion de « contexte de preuve » intégrant la description du comportement de l'environnement interagissant avec le modèle et les propriétés à vérifier dans ce contexte. L'article présente le langage CDL (Context Description Language) proposé à l'utilisateur pour la description des contextes de preuve. Ceux-ci sont exploités, actuellement dans nos travaux, par une technique de vérification de type model-checking avec la mise en oeuvre d'observateurs. Dans une approche Ingénierie Dirigée par les Modèles (IDM), les modèles de contextes sont transformés en modèles d'automates temporisés puis en codes exploitables par l'outil OBP/IFx (Observer-Based Prover). Ce travail a donné lieu à plusieurs expérimentations industrielles comme la validation formelle d'un protocole de communication avionique pour l'AIRBUS A380. Dans cet article, nous décrivons l'application de notre approche la validation d'un modèle de contrôleur de système aérien conçu par THALES. L'article rend compte de la mise en oeuvre du langage CDL et d'un retour d'expérience.	Philippe Dhaussy, Julien Auvray, Stéphane de Belloy, Frédéric Boniol, Eric Landel	http://editions-rnti.fr/render_pdf.php?p1&p=1000579	http://editions-rnti.fr/render_pdf.php?p=1000579
Revue des Nouvelles Technologies de l'Information	LMO	2008	Utilisation de SysML pour la modélisation des réseaux de  capteurs	SysML est le nouveau langage de modélisation défini par l'OMG. Il peut être vu comme une extension d'UML destinée à la modélisation d'un large spectre de systèmes complexes. Son champ d'application est en ce sens plus large que celui d'UML mais sa filiation le rend tout particulièrement intéressant pour la modélisation de systèmes embarqués majoritairement composés de logiciel. Les logiciels déployés sur les réseaux de capteurs sans fil (WSN) sont un bon exemple de ce type d'application puisque la prise en compte de l'interaction forte entre le matériel et le logiciel inhérente à ce type de système est une condition importante pour une modélisation efficace. Dans cet article nous décrivons notre retour sur expérience concernant la modélisation d'un système utilisant des capteurs mobiles sans fil afin de mesurer les flux de personnes dans une ville. Dans cette étude, nous avons utilisé à la fois SysML pour la modélisation du système et UML pour la modélisation des parties logicielles. Nous présentons les points de recouvrements des deux langages d'une part, et nous en comparons les diagrammes statiques d'autre part.	Nicolas Belloir, Jean-Michel Bruel, Natacha Hoang, Congduc Pham	http://editions-rnti.fr/render_pdf.php?p1&p=1000611	http://editions-rnti.fr/render_pdf.php?p=1000611
Revue des Nouvelles Technologies de l'Information	LMO	2008	Vers l'Exécutabilité des Modèles de Procédés Logiciels	L'un des enjeux majeurs de l'ingénierie dirigée par les modèles est d'augmenter la productivité des logiciels à travers la manipulation de modèles dès les premières phases de développement. La finalité étant de pouvoir utiliser les modèles non seulement pour des fins de compréhension et de description mais aussi de production. Les modèles de procédés de développement logiciels sont au coeur de la démarche de construction du logiciel. Cependant, à ce jour, ils ne sont utilisés que pour documenter les procédés et demeurent des modèles contemplatifs. Le but de nos travaux est de les rendre productifs, permettant ainsi une meilleure coordination entre les équipes de développement, l'automatisation des tâches répétitives et non interactives et une gestion plus efficace des moyens utilisés pendant les phases de développement. A cet effet, nous proposons UML4SPM, un langage exécutable et orienté modèle pour la modélisation de procédés de développement logiciel.	Reda Bendraou, Marie-Pierre Gervais, Xavier Blanc, Jean-Marc Jézéquel	http://editions-rnti.fr/render_pdf.php?p1&p=1000610	http://editions-rnti.fr/render_pdf.php?p=1000610
Revue des Nouvelles Technologies de l'Information	LMO	2008	Vers la génération de modèles de sûreté de fonctionnement	La conception et le développement de systèmes embarqués critiques sont assujettis à la fois à des objectifs économiques mais également au respect des normes de sécurité. Dès lors, la qualité des analyses de sûreté de fonctionnement et des interactions entre les experts de sûreté de fonctionnement et les équipes de développement est primordiale. Partant du constat que les échanges entre ces équipes ne sont pas encore suffisamment automatisés, nous proposons des techniques de génération automatique de modèles de sûreté de fonctionnement à partir de spécifications exprimées sous forme de modèle. L'algorithme générique proposé a été implanté par un code de transformation de modèles AADL en AltaRica et une expérimentation a été réalisée sur une spécification d'un asservissement de gouverne avionique.	Xavier Dumas, Claire Pagetti, Laurent Sagaspe, Pierre Bieber, Philippe Dhaussy	http://editions-rnti.fr/render_pdf.php?p1&p=1000578	http://editions-rnti.fr/render_pdf.php?p=1000578
Revue des Nouvelles Technologies de l'Information	LMO	2008	Vers une nouvelle approche d'extraction de la logique métier d'une application orientée objet	Les compagnies font face à d'énormes coûts pour maintenir leurs applications informatiques. Ces applications contiennent des connaissances corporatives importantes qui deviennent difficiles à récupérer après plusieurs années d'opération et d'évolution. Plusieurs approches ont été proposées afin d'extraire du code source des abstractions pour aider les développeurs à assimiler ces connaissances. Cependant, l'abstraction extraite par la plupart des approches combine la logique métier de l'application et son architecture. Nous proposons une nouvelle approche pour extraire le modèle d'une application orientée objet. Ce modèle est donné comme un diagramme de classes UML présentant les classes métier de l'application et leurs interrelations. Cette approche a été validée sur plusieurs systèmes écrits en Java et donne de bons résultats pour les systèmes bien structurés avec un bon style de programmation.	Ismaïl Khriss, Gino Chénard	http://editions-rnti.fr/render_pdf.php?p1&p=1000597	http://editions-rnti.fr/render_pdf.php?p=1000597
Revue des Nouvelles Technologies de l'Information	MC	2008	Coopération de connaissances hétérogènes pour la construction et la validation de l'expertise d'un domaine	Ce travail se situe dans le contexte général de la construction et dela validation de l'expertise d'un domaine. Il vise la coopération de deux typesde connaissances, hétérogènes par leur niveau de granularité et par leur formalisme: des dires d'experts représentés dans le modèle des graphes conceptuelset des données expérimentales représentées dans le modèle relationnel. Nousproposons d'automatiser deux étapes : d'une part, la génération d'une ontologiesimple (partie terminologique du modèle des graphes conceptuels) guidée à lafois par le schéma relationnel et par les données qu'il contient ; d'autre part,l'évaluation de la validité des dires d'experts au sein des données expérimentales.La méthode que nous introduisons pour cela est fondée sur l'utilisation degraphes conceptuels patrons annotés. Ces résultats ont été implémentés au seind'une application concrète concernant le contrôle de la qualité alimentaire.	Rallou Thomopoulos, Jean-François Baget, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1000708	http://editions-rnti.fr/render_pdf.php?p=1000708
Revue des Nouvelles Technologies de l'Information	MC	2008	Découverte d'associations quantitatives générales et atypiques	Dans ce papier, nous proposons un nouveau type d'association mettant en jeu au moins une variable quantitative. Il s'agit de rechercher dans une population d'individus, les catégories qui s'écartent significativement du comportement normal de cette population. Plus exactement nous recherchons les catégories d'individus qui sont sur-représentées ou au contraire sous-représentées pour les fortes ou les faibles valeurs de la variable cible. Pour caractériser l'association, nous avons utilisé et étendu une mesure existante, l'intensité d'inclination. Comme toutes les catégories d'individus (ou associations de variables) ne sont pas d'égal intérêt pour l'utilisateur, ce type de connaissance permet, dans un premier temps, d'avoir une vision globale de l'association. Dans un deuxième temps il est possible de rechercher les intervalles de valeurs pour lesquels les écarts interviennent. Cette recherche des intervalles s'appuie sur les tableaux de contingence des écarts entre la situation observée et la situation attendue.	Sylvie Guillaume, Leila Nemmiche Alachaher, Michel Schneider	http://editions-rnti.fr/render_pdf.php?p1&p=1000712	http://editions-rnti.fr/render_pdf.php?p=1000712
Revue des Nouvelles Technologies de l'Information	MC	2008	Fusion et normalisation de réseaux possibilistes quantitatifs	La fusion d'informations incertaines, issues de différentes sources,est un problème important dans de nombreuses applications. Dans cet article,nous développons une méthodologie d'analyse des opérateurs de combinaisonspossibilistes où l'information incertaine est exprimée à l'aide de réseaux possibilistesquantitatifs (basés sur l'opérateur produit). Nous montrons que la fusion,basée sur le produit, de réseaux possibilistes ayant des structures identiques, sefait en un temps polynômial. Nous étudions ensuite la fusion de réseaux possibilistesayant différentes structures, mais dont l'union de ces réseaux ne contientpas de cycles. La dernière partie de cet article traite le problème de la sousnormalisationqui reflète la présence de conflits entre les différentes sourcesd'informations.	Salem Benferhat, Faiza Titouna, Mohamed Tayeb Laskri	http://editions-rnti.fr/render_pdf.php?p1&p=1000700	http://editions-rnti.fr/render_pdf.php?p=1000700
Revue des Nouvelles Technologies de l'Information	MC	2008	Génération d'ontologie à partir d'un modèle métier UML annoté	Dans cet article nous montrons de quelle manière les techniquesd'ingénierie dirigée par les modèles peuvent être utilisées dans un processus decréation d'une ontologie. À partir d'un modèle métier UML, déjà existant, etannoté avec un ensemble de stéréotypes, nous décrivons les différentes phasesde la génération de l'ontologie. Le processus de création permet également, àpartir d'objets métier, de peupler une base de connaissance structurée parl'ontologie générée. Ces travaux s'appuient sur la spécification ODM dontnous présentons les principes pour expliquer les concepts qui ont guidé notredémarche. Ce travail a été développé dans le domaine du journalisme pourproduire une ontologie nécessaire à la navigation entre les différentes informationsfournies par des dépêches.	Jean-Yves Lafaye, Cyril Faucher, Frédéric Bertrand	http://editions-rnti.fr/render_pdf.php?p1&p=1000703	http://editions-rnti.fr/render_pdf.php?p=1000703
Revue des Nouvelles Technologies de l'Information	MC	2008	Indexation multi-facettes des ressources pédagogiques pour faciliter leur ré-utilisation	Nous proposons dans cet article un modèle de représentation desobjets pédagogiques visant à faciliter leur re-utilisabilité. La représentation desobjets inclut les méta-données, l'usage des objets à travers les scénarii danslesquels ils sont utilisés et leur composition ; nous respectons en cela lesnormes actuelles d'apprentissage en ligne. Nous enrichissons cesreprésentations par la prise en compte de la sémantique des contenus des objetspédagogiques. Un autre apport de nos propositions concerne le fait que cettereprésentation multi-facettes repose sur des ontologies qui permettent unemeilleure représentation sémantique et facilitent la communication entre lamachine et les utilisateurs	Nathalie Hernandez, Josiane Mothe, Andriantiana Bertin Olivier Ramamonjisoa, Bachelin Ralalason, Patricia Stolf	http://editions-rnti.fr/render_pdf.php?p1&p=1000710	http://editions-rnti.fr/render_pdf.php?p=1000710
Revue des Nouvelles Technologies de l'Information	MC	2008	Métamodélisation pour le Web sémantique	Le Web sémantique nécessite la standardisation des mécanismes dereprésentation qui permettent à la connaissance contenue dans les pages Webd'être identifiée et traitée au niveau sémantique. La popularité de RDF est deplus en plus grande, cependant il existe de nombreux documents disponiblessur le Web qui utilisent d'autres modes de représentation. Nous proposons danscet article d'utiliser les graphes conceptuels comme langage pivot du Web. Lesgraphes conceptuels sont souvent identifiés comme un langage clé dans la représentationde la connaissance de par leur simplicité, leur expressivité et leursimilitude à d'autres langages de modélisation graphiques tels UML ou Entité-Relation. Nous proposons donc d'utiliser les techniques de métamodélisationpour mettre en oeuvre ce langage pivot, et ainsi tirer profit, entre autres, de laproximité inhérente entre les graphes conceptuels et la langue naturelle pourfaciliter l'implantation d'outils automatiques de traduction entre diverses représentations.	Olivier Gerbé, Guy Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000704	http://editions-rnti.fr/render_pdf.php?p=1000704
Revue des Nouvelles Technologies de l'Information	MC	2008	Méthode, Modèle et Outil Ardans de capitalisation des connaissances	La méthode de modélisation et l'outil Ardans ont pour vocation lacapitalisation d'un patrimoine de savoir-faire, sa structuration, saformalisation, sa diffusion, sa valorisation dans le travail quotidien des acteursde l'organisation, ainsi que sa mise à jour et son évolution. Ils supportent ainsitout son cycle de vie. Cet article présente les grands principes qui sous-tendentla méthode, les principales primitives de modélisation, l'outil Ardans et sesperspectives de connexion à OWL.	Pierre Mariot, Christine Golbreich, Jean-Pierre Cotton, François Vexler, Alain Berger	http://editions-rnti.fr/render_pdf.php?p1&p=1000709	http://editions-rnti.fr/render_pdf.php?p=1000709
Revue des Nouvelles Technologies de l'Information	MC	2008	Modélisation des connaissances hétérogènes des documents multimédia	Les objets multimédia sont des supports de la connaissance qui sedéveloppent rapidement. Le défi à relever consiste à proposer des outils performantspour atteindre le contenu de ces objets et fournir une approche sémantiquede ces très vastes bases de connaissances. Nous proposons une approchebipolaire. D'un point de vue externe sur ces objets multimédia, nousutilisons les ontologies terminologiques pour repérer les éléments sémantiquespertinents dans les métadonnées et péritextes. D'un point de vue interne, nousconstruisons des indicateurs de niveaux sémantiques croissants à partir desimages. Enfin, nous proposons une fusion de ces données hétérogènes pour essayerde fournir des outils performants de recherche de contenus dans les basesd'objets multimédia. Nous illustrons nos propos par un corpus de films d'animation.	Françoise Deloule, Daniel Beauchêne, Bogdan Ionescu, Patrick Lambert	http://editions-rnti.fr/render_pdf.php?p1&p=1000707	http://editions-rnti.fr/render_pdf.php?p=1000707
Revue des Nouvelles Technologies de l'Information	MC	2008	Modélisation ontologique des connaissances : Pour une lecture conceptuelle dans les manuels scolaires électroniques	Les manuels scolaires électroniques sont classiquement conçus àpartir des savoir-faire issus de l'édition papier. Afin d'exploiter au mieux lespotentialités des TICE nous avons imaginé un manuel scolaire électroniqued'un nouveau genre offrant une véritable hypertextualité dans la structurationdes documents, dans le discours du lecteur et dans la navigation del'utilisateur. Dans cet hypermédia ontologique les concepts sont organisés dansun espace à 3 dimensions sémantiques. La navigation se fait sur 3 axesontologiques et bijectifs où chaque déplacement correspond à une phrasemettant en jeu deux concepts via une des trois relations possibles. Chaquedéplacement est également signifiant sur les plans logiques, sémantiques et del'action. Le lecteur conçoit son propre parcours pédagogique au fil de sesdéplacements. Le formalisme conceptuel et la navigation proposés ici,réduisent la désorientation, améliorent la recherche d'information etaugmentent très nettement la compréhension et le raisonnement.	Stephan Renayd	http://editions-rnti.fr/render_pdf.php?p1&p=1000711	http://editions-rnti.fr/render_pdf.php?p=1000711
Revue des Nouvelles Technologies de l'Information	MC	2008	Représenter, opérationnaliser, aligner et évaluer des Ontologies Denses : une approche et un outil fondés sur le modèle des Graphes Conceptuels	À l'heure actuelle, les ontologies sont au coeur de nombreuses applicationscar, outre le fait d'établir un consensus sur le vocabulaire conceptueld'un domaine, elles permettent également de raisonner sur des assertions de cedomaine. Néanmoins, dans de nombreux contextes et en particulier dans le cadredu Web Sémantique dont le défi est d'offrir la possibilité d'automatiser la miseen oeuvre de raisonnements, il s'avère de plus en plus indispensable de considérerdes ontologies denses, i.e. des ontologies intégrant l'ensemble des axiomespermettant de fixer toute la sémantique du domaine considéré, en comparaisonaux ontologies dites légères qui, elles, n'incluent pas d'axiomes et sont uniquementfondées sur des hiérarchies de concepts et de relations, éventuellementenrichies de propriétés classiques telles que les propriétés algébriques des relationsou l'exclusion de deux concepts (cf. l'expressivité du langage OWL). Peude travaux relevant de l'ingénierie des ontologies prennent en compte la représentationexplicite des axiomes ne pouvant s'exprimer à l'aide de ces propriétésclassiques. Or, la représentation formelle de ce type d'axiomes (et leur prise encompte au sein de mécanismes de raisonnement) s'avère cruciale dans de nombreusesapplications. Cet article présente un atelier d'ingénierie des ontologiesdenses, basé sur le modèle des Graphes Conceptuels. Cet atelier, appelé Too-CoM, met l'accent sur la prise en compte des axiomes, tant d'un point de vuemodélisation et représentation des connaissances que d'un point de vue mise enoeuvre de raisonnements à des fins d'opérationnalisation des connaissances (i.e.intégration d'une ontologie au sein d'un Système à Base de Connaissances),d'évaluation (i.e. vérification et validation) et d'alignement d'ontologies (i.e. interopérabilitésémantique).	Frédéric Fürst, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000701	http://editions-rnti.fr/render_pdf.php?p=1000701
Revue des Nouvelles Technologies de l'Information	MC	2008	Sur l'évaluation de la quantité d'information d'un concept dans une taxonomie et la proposition de nouvelles mesures	L'évaluation de la similarité entre concepts structurés dans une taxonomieconnaît un réel essor lié au développement du web sémantique. En effet,les mesures sémantiques constituent une aide au développement et à l'exploitationdes ontologies qui restent des tâches complexes dans un processus globaled'ingénierie des connaissances. Certaines mesures sémantiques sont baséessur la notion clef de contenu informationnel initialement proposé par Resnik(1995). Cet article expose notre vision du contenu informationnel à travers denouveaux estimateurs indépendants de tout corpus. Nous généralisons la notionde contenu informationnel à un groupe de concepts et discutons la base du logarithmeutilisée. Quelques propositions de mesures montrent l'analogie que l'onpeut faire pour réutiliser des résultats bien connus sur une représentation ensembliste.Nous mettons en évidence la pertinence de notre approche par le biais derésultats statistiques.	Emmanuel Blanchard, Mounira Harzallah, Pascale Kuntz, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000706	http://editions-rnti.fr/render_pdf.php?p=1000706
Revue des Nouvelles Technologies de l'Information	MC	2008	Une synthèse des modèles de représentation des connaissances à base de Graphes Conceptuels et OWL	Nous présentons et comparons deux approches de modélisation, formelleset concrètes, pour représenter et manipuler des connaissances d'un domaine.Le modèle des graphes conceptuels permet de modéliser des connaissancesen terme de graphes, basés sur un support. Cette approche de modélisationest intensionnelle, est munie d'une sémantique en logique du premier ordre,et fait l'hypothèse d'un monde fermé pour ses raisonnements. Le langage OWLpermet de décrire des ontologies et des faits sur le Web, suivant une approchede modélisation extensionnelle. Il possède une sémantique issue des logiques dedescriptions, et fait l'hypothèse d'un monde ouvert pour ses raisonnements.	Thomas Raimbault, Henri Briand, David Genest, Rémi Lehn, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000702	http://editions-rnti.fr/render_pdf.php?p=1000702
Revue des Nouvelles Technologies de l'Information	MC	2008	Validation et enrichissement d'annotations : application à la veille médiatique	Cet article présente un service de validation et d'enrichissement d'annotationsconçu pour un outil industriel de gestion des connaissances basé sur lelangage des Topic Maps (TM). Un tel service nécessitant la mise en oeuvre deraisonnements sur les connaissances, nous avons été amené à doter le langagedes TM d'une sémantique formelle. Ceci a été réalisé par l'intermédiaire d'unetransformation des TM vers le formalisme logique des graphes conceptuels quicomme les TM dispose d'une représentation graphique des connaissances. Lasolution a été mise en oeuvre dans une application conçue pour la veille médiatique.Des annotations sont extraites automatiquement de dépêches sur l'actualitééconomique puis ajoutées à la base de connaissances. Elles sont ensuitefournies au service de validation qui décide de leur validité, les enrichit et évalueleur pertinence afin de faciliter le travail du veilleur.	Olivier Carloni, Michel Leclère, Marie-Laure Mugnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000705	http://editions-rnti.fr/render_pdf.php?p=1000705
Revue des Nouvelles Technologies de l'Information	RS	2008	FSRO : une ontologie de relations spatiales floues pour l'interprétation d'images	Dans le domaine de l'interprétation d'images, les relations spatialesjouent un rôle important dans la description et la reconnaissance des objets :elles permettent en effet de lever l'ambiguïté entre des objets d'apparences similaireset sont souvent plus stables que les caractéristiques des objets eux-mêmes.D'autre part, l'interprétation sémantique des images peut bénéficier de représentationsdes concepts utiles et de leurs relations sous la forme d'ontologies. Danscet article nous proposons une ontologie générique de relations spatiales afin deguider l'interprétation d'une image et la reconnaissance des structures qu'ellecontient par des informations structurelles sur l'agencement spatial de ces structures.Une contribution originale est l'enrichissement de cette ontologie par desreprésentations floues des relations spatiales, qui en précisent la sémantique, etpermettent de faire le lien entre ces concepts, souvent exprimés sous forme linguistique,et les informations que l'on peut extraire des images, contribuant ainsià réduire le fossé sémantique. Dans l'approche proposée, les paramètres des représentationsfloues des relations spatiales sont appris automatiquement sur unebase d'exemples. Enfin, nous montrons comment cette connaissance structurelleet spatiale peut être utilisée pour guider l'interprétation d'images.	Céline Hudelot, Jamal Atif, Isabelle Bloch	http://editions-rnti.fr/render_pdf.php?p1&p=1000717	http://editions-rnti.fr/render_pdf.php?p=1000717
Revue des Nouvelles Technologies de l'Information	RS	2008	Sélection aléatoire d'espaces de représentation pour la décision binaire en environnement non-stationnaire: application à la segmentation d'images texturées	Nous proposons dans cet article une méthode de sélection d'espacesde représentation, dans le but d'optimiser ou de préserver les performances d'unsystème décisionnel en présence de bruit, de perte d'information ou de nonstationnarité.Cette méthode consiste à définir tout d'abord un espace de représentationle plus exhaustif possible, correspondant aux attributs les plus appropriéspour porter les informations utiles au problème traité. Ensuite on sélectionneau hasard des sous-espaces de dimension réduite, obtenus par projectionde l'espace initial. La segmentation d'images texturées constitue une applicationtout à fait appropriée pour illustrer cette méthode et évaluer ses performances.Nous traitons ici un problème à deux classes de textures pour lequel il s'agit dechoisir le meilleur espace de représentation en termes de décision aux frontièresentre deux classes. Le principe de la méthode consiste à évaluer, par apprentissage,les performances d'un classifieur donné pour chaque espace de représentationsélectionné. Ensuite, l'étape finale de segmentation est effectuée sur uneimage composée de deux classes de textures. La décision est prise sur la based'un vote pondéré des décisions prises par le classifieur dans chaque espace dereprésentation. Nous présentons quelques résultats qui nous semblent justifier ladémarche adoptée et nous concluons sur les perspectives qu'ils nous inspirent.	Pierre Beauseroy, Andre Smolarz, Xiyan He	http://editions-rnti.fr/render_pdf.php?p1&p=1000718	http://editions-rnti.fr/render_pdf.php?p=1000718
Revue des Nouvelles Technologies de l'Information	RS	2008	Synthèse sur les modèles de représentation des relations spatiales dans les images symboliques	La description des relations spatiales entre objets dans les imagesfournit une sémantique forte qui vient enrichir les techniques bas niveau de représentationdu contenu visuel des images, et qui se prête à de nombreux scénariosde recherche dans les bases d'images. Depuis les années 80 avec les travauxde Chang et al. (1987), un grand nombre d'approches ont été proposées pourdécrire les relations spatiales dans les images dites symboliques, dans lesquellesles objets d'intérêt sont déjà extraits et identifiés. Cet article dresse un panoramades modèles existants. La typologie choisie sépare les approches dites implicites,qui produisent une représentation globale des relations spatiales existantentre tous les objets de l'image, des approches dites explicites, où la structureproduite décrit directement toutes les relations spatiales entre objets. Toutes lesapproches présentées sont comparées selon plusieurs critères, notamment : typedes relations spatiales décrites, volume de stockage, complexité de l'algorithmede comparaison d'images et scénarios applicatifs	Valérie Gouet-Brunet, Maude Manouvrier, Marta Rukoz	http://editions-rnti.fr/render_pdf.php?p1&p=1000715	http://editions-rnti.fr/render_pdf.php?p=1000715
Revue des Nouvelles Technologies de l'Information	RS	2008	Un cadre conceptuel pour modéliser les relations spatiales	Diverses approches sous-tendent la modélisation des relations spatiales,qui est un domaine hétérogène et interdisciplinaire. Dans cet article, nousprésentons un cadre conceptuel pour la description des caractéristiques des différentsmodèles et la manière dont ils se rapportent les uns aux autres. Unepremière classification est réalisée entre trois niveaux de représentation : géométrique,informatique, et utilisateur. Au niveau géométrique, les objets spatiauxpeuvent être considérés comme des ensembles de points, et les relationspeuvent être formellement définies en termes mathématiques. Au niveau informatique,les objets sont représentés en tant que types de données spatiaux etles relations sont calculées au moyen d'opérateurs spatiaux. Au niveau utilisateur,les objets et les relations appartiennent à une ontologie dépendante ducontexte. Un autre moyen de fournir une catégorisation provient de l'espacegéométrique qui décrit les relations : on distingue les relations topologiques,projectives, et métriques. Ensuite, nous considérons la cardinalité des relationsspatiales, qui est définie comme le nombre d'objets qui participent à la relation.Un autre critère est la granularité de la relation, qui peut être plus générale ouplus détaillée. Nous considérons également la dimension des différents objetsgéométriques et de l'espace comme un moyen fondamental de classifier les relations.	Eliseo Clementini, Robert Laurini	http://editions-rnti.fr/render_pdf.php?p1&p=1000714	http://editions-rnti.fr/render_pdf.php?p=1000714
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	A new support measure for items in streams	Mining streams is a challenging problem, because the data can only be looked at once, and only small summaries of the data can be stored. We present a new frequency measure for items in streams that does not rely on a fixed window length or a time-decaying factor. Based on the properties of the measure, an algorithm to compute it is shown. Experimental evaluation supports the claim that the new measure can be computed from a summary with very small memory requirements, that can be maintained and updated efficiently. In this extended abstract, the main points of the presentation are discussed.	Toon Calders, Nele Dexters, Bart Goethals	http://editions-rnti.fr/render_pdf.php?p1&p=1001737	http://editions-rnti.fr/render_pdf.php?p=1001737
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	A practical evaluation of load shedding in data stream management systems for network monitoring	In network monitoring, an important issue is the number of tuples the data stream management system (DSMS) can handle for different network loads. In order to gracefully handle overload situations, some DSMSs are equipped with a tuple dropping functionality, also known as load shedding. These DSMSs register and relate the number of received and dropped tuples, i.e., the relative throughput, and perform different kinds of calculations on them. Over the past few years, several solutions and methods have been suggested to efficiently perform load shedding. The simplest approach is to keep a count of all the dropped tuples, and to report this to the end user. In our experiments, we study two DSMSs, i.e., TelegraphCQ with support for load shedding, and STREAM without this support. We use three particular network monitoring tasks to evaluate the two DSMS with respect to their ability of load shedding and performance. We demonstrate that it is important to investigate the correctness of load shedding by showing that the reported number of dropped tuples is not always correct.	Jarle Soberg, Kjetil H. Hernes, Matti Siekkinen, Vera Goebel, Thomas Plagemann	http://editions-rnti.fr/render_pdf.php?p1&p=1001731	http://editions-rnti.fr/render_pdf.php?p=1001731
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	A temporal representation of point interval relations including calendar events	In this paper we shall discuss knowledge discovery problems arising in time series. A relation algebra is applied for the purpose of representing both dependencies among single items and dependencies between single items and intervals. This enables one to specify and recognise even complex interrelationships among items in data streams. In addition, intervals like calendar events can be incorporated into the patterns. An example is set for the case of thrown alarms in communication networks.	Hanna Bauerdick, Björn Gottfried	http://editions-rnti.fr/render_pdf.php?p1&p=1001764	http://editions-rnti.fr/render_pdf.php?p=1001764
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Analyse en Facteurs : présentation et comparaison des logiciels SAS, SPAD et SPSS.	En analyse des données, les méthodes factorielles sont fondamentales. Ces techniques peuvent être utilisées comme but en soi, il s'agit alors de faire ressortir des facteurs sous-jacents communs à un groupe de variables. Elles peuvent également constituer une étape préalable à d'autres études.Elles consistent alors à réduire la dimension des données en remplaçant les variables d'origine, qui peuvent être corrélées, par un plus petit nombre de variables linéairement indépendantes. Lorsque les données sont quantitatives, l'Analyse en Facteurs (A.F.) est une des méthodes possibles. L'objectif de cet article est de dresser une présentation synthétique du modèle d'A.F., peu développé dans les manuels francophones, mais fréquent dans la littérature anglo-saxonne, et souvent présent dans les logiciels statistiques. La présentation des techniques d'estimation du modèle d'A.F. permet d'établir le lien existant entre l'Analyse en Composantes Principales (A.C.P.) et l'A.F. Il s'agit également de montrer l'utilité des techniques de rotation, qui peuvent faciliter l'interprétation des résultats. Un exemple d'application sur des données de criminalité de villes américaines permet enfin de décrire les résultats fournis par trois des logiciels statistiques les plus utilisés : SAS, SPAD et SPSS, et ainsi de clarifier le vocabulaire, parfois confus pour l'utilisateur	Marie Chavent, Vanessa Kuentz, Jérôme Saracco	http://editions-rnti.fr/render_pdf.php?p1&p=1001713	http://editions-rnti.fr/render_pdf.php?p=1001713
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Analyzing the evolution of Web usage data	Analyzing Web usage has become a very important strategy for Web site operators as it provides them a better understanding of the users' behavior. This insight can enable the operators to improve their service and thereby attract more visitors. Taking into account the temporal dimension in such analyses has become a necessity since the way a site is visited can indeed evolve due to modifications in the structure and content of the site, or even due to changes in the behavior of certain user groups. Consequently, the models associated with these behaviors must be continuously updated in order to reflect the actual behavior of the users. One solution to this problem, proposed in this article, is to update these models using summaries obtained by means of an evolutionary approach based on clustering methods. To do so, we carry out various clustering strategies that are applied on time sub-periods. We compare the results obtained using this method with the results obtained by a traditional global analysis.	Alzennyr Da Silva	http://editions-rnti.fr/render_pdf.php?p1&p=1001745	http://editions-rnti.fr/render_pdf.php?p=1001745
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Binary data flow visualization on factorial axes	Data streams are one of the most relevant new data sources, they refer to flows of data that come at a very high rate. Let us consider a stock-exchange market, where n different stocks with p considered attributes (e.g. price, quantity, seller/buyer id, . . .) are negotiated all day long. The distinguishing feature in data streams analysis is that the focus is on transient relations. The present paper proposes a visualization tool exploiting Multidimensional Data Analisis (MDA) techniques to represent the evolving association structures among attributes over different time-frames. The general aim is to detect the stability of the deviation from indipendence in the occurrence of an observed set of attributes stored as binary stream.	Alfonso Iodice D'Enza, Francesco Palumbo	http://editions-rnti.fr/render_pdf.php?p1&p=1001752	http://editions-rnti.fr/render_pdf.php?p=1001752
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Data-stream and load-forecasting: some ideas for a research project at EDF	In this article, we are going to highlight the interest in using the data resulting from the future 32 millions of communicating meters which will equip all the French customers from now to 2013, in order to build up Court-terms forecasts and Means-terms concerning the EDF electric consumption (total or by wallets) in an environment ot data stream. First of all we will develop our reflections and afterwards we will evoke some tracks of research. These tracks should enable us to approach some modelling and forecasts by aggregation/disintegration of curves as well as modelling and forecasts on Hilbertian data. Finally we will place side by side these ideas with the Stream-Mining type approaches.	Alain Dessertaine	http://editions-rnti.fr/render_pdf.php?p1&p=1001759	http://editions-rnti.fr/render_pdf.php?p=1001759
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Détermination du nombre optimal de classes présentant un fort degré de chevauchement	Dans cet article, nous présentons un nouvel indice pour la détermination du nombre optimal et correct de classes nommé VMEP basé sur le Principe du Maximum d'Entropie. Les performances de ce nouvel indice déduit d'une combinaison originale entre des méthodes d'analyse des données et le critère du maximum d'entropie, sont montrées à travers un ensemble d'exemples simulés et réels. La procédure est complètement automatique dans le sens qu'elle ne nécessite aucun paramètre de réglage. VMEP montre une grande robustesse, et une supériorité par rapport à d'autres indices déjà existants et assez récents, particulièrement dans le cas du chevauchement spatial entre classes.	O. Ammor, N. Raiss , K. Slaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001714	http://editions-rnti.fr/render_pdf.php?p=1001714
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Electricity load forecast using data streams techniques	Sensors distributed all around electrical-power distribution networks produce streams of data at high-speed. From a data mining perspective, this sensor network problem is characterized by a large number of variables (sensors), producing a continuous flow of data, in a dynamic non-stationary environment. In this work we analize the most relevant data mining problems and issues: online learning and change detection. We propose an architecture based on an online clustering algorithm where each cluster (group of sensors with high correlation) contains a neural-network based predictive model. The goal is to maintain in real-time a clustering model and a predictive model able to incorporate new information at the speed data arrives, detecting changes and adapting the decision models to the most recent information. We present preliminary results illustrating the advantages of the proposed architecture.	Pedro Rodrigues, João Gama	http://editions-rnti.fr/render_pdf.php?p1&p=1001756	http://editions-rnti.fr/render_pdf.php?p=1001756
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Entre METEO et CLIMAT.	En codant les cartes météo captées par radio toutes les 12 heures pendant 7 ans on a pu constituer un fichier des variations de pression atmosphérique sur une grande région couvrant l'Atlantique Nord et l'Europe. L'analyse statistique de ces données montre une très grande variabilité des saisons, qui n'est pas favorable à la prévision, mais dont l'une des causes est une oscillation bisannuelle de forte amplitude qui agite les 4 premières années. La période analysée est beaucoup trop courte pour tirer des conclusions et savoir si ces phénomènes sont susceptibles de se reproduire périodiquement, mais on espère avoir montré l'intérêt qu'il y aurait à étendre ce type d'analyse à une période plus longue.	Bernard Labbé	http://editions-rnti.fr/render_pdf.php?p1&p=1001715	http://editions-rnti.fr/render_pdf.php?p=1001715
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	FACIL - An approach for classifying data streams by decision rules and border examples	This paper describes FACIL, a classifier based on decision rules and border examples that avoids unnecessary revisions when virtual drifts are present in data. Rules in FACIL are both pure - consistent - and impure - inconsistent -. Pure rules classify new test examples by covering and impure rules classify them by distance as the nearest neighbor algorithm. In addition, the system provides an implicit forgetting heuristic so that positive and negative examples are removed from a rule when they are not near one another.	Francisco J. Ferrer-Troyano, Jesús S. Aguilar-Ruiz, José C. Riquelme	http://editions-rnti.fr/render_pdf.php?p1&p=1001749	http://editions-rnti.fr/render_pdf.php?p=1001749
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Feature selection for genomic data	Building predictive models for genomic mining requires feature selection, as an essential preliminary step to reduce the large number of available variable. Feature selection in the process of select a generally smaller subset of variables (features) that can be considered the best, from a statistical point of view, with respect to the employed model for the analysis. In gene expression microarray data, being able to select a few number of important genes not only makes data analysis efficient but also helps their biological interpretation. Microarray data have typically several thousands of genes (features) but only tens of samples. Problems which can occur due to the small sample size have not been addressed well in the literature. Our aim is to discuss some issues on feature selection applied to microarray data in order to select the most important genes from a predictive point of view.	Paola Cerchiello, Silvia Figini	http://editions-rnti.fr/render_pdf.php?p1&p=1001762	http://editions-rnti.fr/render_pdf.php?p=1001762
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Fundamentals of analyzing and mining data streams	Many scenarios, such as network analysis, utility monitoring, and financial applications, generate massive streams of data. These streams consist of millions or billions of simple updates every hour, and must be processed to extract the information described in tiny pieces. This survey provides an introduction the problems of data stream monitoring, and some of the techniques that have been developed over recent years to help mine the data while avoiding drowning in these massive flows of information. In particular, this tutorial introduces the fundamental techniques used to create compact summaries of data streams: sampling, sketching, and other synopsis techniques. It describes how to extract features such as clusters and association rules. Lastly, we see methods to detect when and how the process generating the stream is evolving, indicating some important change has occurred.	Graham Cormode	http://editions-rnti.fr/render_pdf.php?p1&p=1001723	http://editions-rnti.fr/render_pdf.php?p=1001723
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Incremental generalized Eigenvalue classification on data streams	As applications on massive data sets are emerging with an increasing frequency, we are facing the problem of analyzing the data as soon as they are produced. This is true in many fields of science and engineering: in high energy physics, experiments have been done to transfer data at a sustained rate of 150 gigabits per second. In Y2007, that speed will enable the delivery to users of data continuously produced by the LHC particle accelerator located at CERN. Other examples can be found in network traffic analysis, telecommunications data mining, discrimination of data from sensors that monitor pollution and biological hazards, video and audio surveillance. In all cases, computational procedures have to deal with a large amount of data that are delivered in form of data streams. Traditional data mining techniques assume that the dataset is static and, to increment knowledge, random samples are extracted from the dataset. In this study, we use Incremental Regularized Generalized Eigenvalue Classification (I-ReGEC), a supervised learning algorithm, to continuously train a classification model from a data stream. The advantage of this technique is that the classification model can be update incrementally.  The algorithm online decides which are the points that contain new information and updates the available classification model. We show through numerical experiments, on a synthetic dataset, the method performance, highlighting its behavior with respect to the number of incremental training set, the accuracy classification and the throughput of the data stream.	Mario R. Guarracino, Salvatore Cuciniello, Davide Feminiano	http://editions-rnti.fr/render_pdf.php?p1&p=1001750	http://editions-rnti.fr/render_pdf.php?p=1001750
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Initiation à R	Ce document a pour objectif de familiariser son lecteur avec le langage et l'environnement de programmation R dans sa version 2.4.1 (décembre 2006). Il ne constitue pas une référence complète mais plutôt un aperçu des capacités de R et un point d'entrée vers d'autres documents plus complets. À plusieurs reprises, le lecteur est invité à exécuter les commandes proposées dans une session R afin de s'habituer à la syntaxe.	Romain François, Jean-Michel Marin	http://editions-rnti.fr/render_pdf.php?p1&p=1001717	http://editions-rnti.fr/render_pdf.php?p=1001717
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Knowledge extraction by dynamical clustering of sea waves streaming data	Data stream can be thought as a sequence of ordered data items, where the input arrives more or less continuously as time progress. There exist several applications producing data stream, e.g. telecommunication system, stock markets customer click streams etc. In this paper we consider the problem of extracting knowledge by a dinamical clustering algorithm of sea waves streaming data, that is to say evolving streaming of data coming from a multisensor system. For this purpose we develop an updating version of Dynamical Clustering Algorithm [5]. This problem is very interesting from a practical point of view. It is based on the computation of a prototypal wave through a free-knot smoothing spline, optimizing a non linear problem. Thanks to this approach, it is possible to investigate in which way the incoming data change according to the various steps of process registration, and to have a summary description of the entire data thought prototypals flowing curves using a small amount of memory and time.	Elvira Romano, Antonio Balzanella, Rosanna Verde	http://editions-rnti.fr/render_pdf.php?p1&p=1001747	http://editions-rnti.fr/render_pdf.php?p=1001747
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	L'enseignement virtuel dans une économie émergente : perception et motivation des étudiants	Cette étude tente d'analyser les perceptions et les attentes des étudiants d'un pays à économie émergente vis-à-vis de cette forme d'enseignement alternative aux méthodes traditionnelles. Les résultats de ce travail font apparaître plusieurs dimensions dans la perception des étudiants. L'interaction apparaît comme étant la dimension la mieux considérée par les étudiants. L'utilité de ce type d'enseignement vient en second lieu. D'autre part, la maîtrise de la technologie informatique représente un souci majeur pour les étudiants. La flexibilité est le concept le moins bien perçu, la gestion du temps n'est pas bien considérée par les étudiants.La relation entre l'engagement dans l'enseignement virtuel et les autres facteurs explicatifs à savoir : l'interaction, l'utilité, la maitrise de la technologie et la flexibilité est étudiée. L'utilisation de la régression PLS a permis de surmonter les problèmes de dépendance et de multi normalité des variables explicatives.L'effet médiateur du profil de l'apprenant dans l'explication de son engagement dans cette voie alternative de formation est étudié dans ce travail. En effet la réussite et la pérennité de cette forme d'enseignement dépend essentiellement de l'apprenant, de sa perception, de ses besoins et de la qualité de la formation reçue qui doit être évaluée en permanence afin de minimiser le risque d'échec.Une typologie de la motivation vis-à-vis du e-Learning est proposée dans ce travail, de plus un indicateur de mesure de la motivation a été élaboré. La comparaison entre l'analyse factorielle discriminante et le modèle logistique emboité montre la supériorité de ce dernier essentiellement en ce qui concerne la forme contrainte de la motivation.	Hatem Dellagi	http://editions-rnti.fr/render_pdf.php?p1&p=1001716	http://editions-rnti.fr/render_pdf.php?p=1001716
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Merging data streams in operational risk management	The motivation of this paper is to develop efficient statistical methods aimed at measuring the performance of business controls. Scorecard and actuarial methods represent the methods mostly employed, in the literature as well as in professional practice. The usage of the two approaches depends on the available data streams. A further problem is that, especially for rare events, a third data stream may be considered: external loss data. It becomes thus necessary to develop a statistical methodology that is able to merge three different data streams in an appropriate way, yet maintaining simplicity of interpretation and predictive power. In the paper we propose a flexible nonparametric approach that can reach this objective.	Paolo Giudici	http://editions-rnti.fr/render_pdf.php?p1&p=1001724	http://editions-rnti.fr/render_pdf.php?p=1001724
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Mining sequential patterns from data streams: a centroid approach	In recent years, emerging applications introduced new constraints for data mining methods. These constraints are typical of a new kind of data: the data streams. In data stream processing, memory usage is restricted, new elements are generated continuously and have to be considered as fast as possible, no blocking operator can be performed and the data can be examined only once. At this time only a few methods has been proposed for mining sequential patterns in data streams. We argue that the main reason is the combinatory phenomenon related to sequential pattern mining. In this paper, we propose an algorithm based on sequences alignment for mining approximate sequential patterns in Web usage data streams. To meet the constraint of one scan, a greedy clustering algorithm associated to an alignment method is proposed. We will show that our proposal is able to extract relevant sequences with very low thresholds.	Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1001739	http://editions-rnti.fr/render_pdf.php?p=1001739
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	On reservoir sampling with deletions	Perhaps the most flexible synopsis of a database is a random sample of the data; such samples are widely used to speed up processing of analytic queries and data-mining tasks, enhance query optimization, and facilitate information integration. In this paper, we describe a recently proposed method for incrementally maintaining a uniform random sample of the items in a dataset in the presence of an arbitrary sequence of insertions and deletions. Our scheme, called "random pairing" (RP), maintains a bounded-size uniform sample by using newly inserted data items to compensate for previous deletions. The RP algorithm is the first extension of the almost 40-year-old reservoir sampling algorithm to handle deletions; RP reduces to the "passive" algorithm in [1] when the insertions and deletions correspond to a moving window over a data stream. We also prove that it is not possible to "resize" a bounded-size random sample upwards without accessing the base data.	Rainer Gemulla, Wolfgang Lehner, Peter J. Haas	http://editions-rnti.fr/render_pdf.php?p1&p=1001728	http://editions-rnti.fr/render_pdf.php?p=1001728
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Processing data stream by relational analysis	In the business intelligence (BI) context, the majority of the strategic information comes from relational sources and the relevance of extracted knowledge usually depends on considering data evolution and their interactions. The multidimensional approach (nD) may bring forth a solution to identify and understand the underlying structures or strategies. But non-expert users get easily lost. Knowing that our team is experienced in knowledge extraction, we have already a platform called Tetralogie that is specialized for strategic scanning and another tool called Xplor which is dedicated to business intelligence. As a consequence, we provide a unified system to generate and manage relational data and extract implicit knowledge, whose content and format are adapted to decision-makers that are not experts in nD or BI.	Ilhème Ghalamallah, Aziz Grimeh, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001741	http://editions-rnti.fr/render_pdf.php?p=1001741
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Processing data streams: you only get one look	Continuous data streams arise naturally, for example, in the network installations of large Telecom and Internet service providers where detailed usage information from different parts of the network needs to be continuously collected and analyzed for interesting trends. This talk will provide an overview of the key research results surrounding the systems issues in data stream processing. I will overview recent results on data stream system architectures, query processing, languages for querying data streams, and complex event processing. I will conclude with some open research questions in these areas.	Johannes Gerhke	http://editions-rnti.fr/render_pdf.php?p1&p=1001734	http://editions-rnti.fr/render_pdf.php?p=1001734
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Quelle est la « bonne » formule de l'écart-type ?		Emmanuel Grenier	http://editions-rnti.fr/render_pdf.php?p1&p=1001718	http://editions-rnti.fr/render_pdf.php?p=1001718
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Random sampling over data streams for sequential pattern mining	In recent years the emergence of new real-world applications such as network traffic monitoring, intrusion detection systems, sensor network data analysis, click stream mining and dynamic tracing of financial transactions, calls for studying a new kind of model. Named data stream, this model is in fact a continuous and potentially infinite flow of information as opposed to finite and statically stored data sets. We study the problem of sequential pattern mining in data streams. This problem has be en extensively studied for the conventional case of disk resident data sets. In the case of data streams, this problem becomes more challenging as the volume of data is usually too huge to be stored on permanent devices, main memory or to be scanned thoroughly more than once. In this case, it may be acceptable to generate approximable solutions for our mining problem. In this paper we introduce a new approach based on biased reservoir sampling to achieve a more efficient mining of sequential patterns. Furthermore, we theoretically prove that our biased reservoir size is always bounded whatever the size of the stream is. This property often allows us to keep the entire relevant reservoir in main memory. We also show a simple algorithm to build the biased reservoir for the special case of sequential pattern mining. Experimental evaluation supports the claim that sequential pattern mining based on biased reservoir sampling needs small memory requirements. Besides, we also propose an adapted approach to handle the case of mining sequential patterns in a sliding window model. The experiment show that the results are accurate.	Pascal Poncelet, Chedy Raïssi	http://editions-rnti.fr/render_pdf.php?p1&p=1001740	http://editions-rnti.fr/render_pdf.php?p=1001740
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Real-time ranking of electrical feeders using expert advice	We are using machine learning to construct a failure-susceptibility ranking of feeders that supply electricity to the boroughs of New York City. The electricity system is inherently dynamic and driven by environmental conditions and other unpredictable factors, and thus the ability to cope with concept drift in real time is central to our solution. Our approach builds on the ensemble-based notion of learning from expert advice as formulated in the continuous version of the Weighted Majority algorithm [16]. Our method is able to adapt to a changing environment by periodically building and adding new machine learning models (or "experts") based on the latest data, and letting the online learning framework choose what experts to use as predictors based on recent performance. Our system is currently deployed and being tested by New York City's electricity distribution company.	Hila Becker, Marta Arias	http://editions-rnti.fr/render_pdf.php?p1&p=1001755	http://editions-rnti.fr/render_pdf.php?p=1001755
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Risk profile assessment embedded into the Bayesian framework	Adverse events in organizations are more than a serious concern. Over the last few years the awareness of this problem has raised and different organizational solutions have been tried. We focus on the problem of managing operational and clinical risks, in terms of events that influence the success of service delivery. This paper is aimed at proposing risk management as the basic methodological approach to deal with adverse events and risks. We propose Bayesian networks (BNs) to assess risk profiles given a context of application and benchmarks by Bayesian decisional theory to evaluate the profiles, i.e. defining the acceptability of them. The method is described both at a theoretical and an empirical level, thanks to its application to health care (haemodialysis department) and banking field. The occurrences of these top events are modeled by Bayesian networks which gather posterior risk profiles for each patient or banking business line. The comparison of them with a reference risk profile is input for decision making. BNs augmented with decisional nodes and scenario analysis complete the risk management process. The ultimate goal is to improve risk profile and, consequently, service supply quality in the organization.	Chiara Cornalba	http://editions-rnti.fr/render_pdf.php?p1&p=1001763	http://editions-rnti.fr/render_pdf.php?p=1001763
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Rule learning from data streams: an overview	Classification is a very well-studied task in data mining. In the last years, important works have been published to scale up classification algorithms in order to handle large datasets. However, due to the high rate of streams of data, a number of emerging applications are demanding new approaches. Rule learning is an efficient alternative to address non-stationary environments. The talk presents an overview of rule-based learning algorithms for data streams and emphasizes some important aspects of these techniques.	Jesús S. Aguilar-Ruiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001736	http://editions-rnti.fr/render_pdf.php?p=1001736
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Smart Alarming Methods: an overview,highlight on statistical methods	Methods of Smart Alarming intend to detect as soon as possible novelty or anomaly in Data Streams. A review is proposed to highlight the key points of using them. In case of univariate data, the more suitable method is not the same as for stationary variable or non-stationary variable. Multivariate data set are often dealt with using unsupervised learning based methods, either with fac-tor analysis (mostly PCA) or clustering algorithms. Each of these methods must be applied in a spe-cific situation: prior knowledge of possible anomalies should be needed or not, learning data set can be large sized or not, and so on. Some examples are outlined. Discussion underlines the importance of having a prior knowledge of variable behaviour, and to consider the global flow chart, including eventually a data preprocessing.	Jean-Paul Valois, Christophe Blondeau, Simplice Dossou-Gbete, Laurent Bordes	http://editions-rnti.fr/render_pdf.php?p1&p=1001744	http://editions-rnti.fr/render_pdf.php?p=1001744
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Summarizing a 3 way relational data stream	In this paper, we present a novel method to summarize a group of three data streams sharing a relational link between each other. That is, using the relational data base model as a reference, two entity streams and one stream of relationships. Each entity stream contains a stream of entities, each one of them referenced by a key, much in the same way as a primary key references all objects in a regular database table. The stream of relationships contains key couples identifying links between objects in the two entity streams as well as attributes characterising this particular link. The algorithm presented here produces not only a summary of both entity streams considered independently of each other, but also gives a summary of the relations existing between the two entity streams as well as information on the join obtained joining the two entity streams through the relationship stream.	Baptiste Csernel, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001725	http://editions-rnti.fr/render_pdf.php?p=1001725
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2007	Using data stream management systems to analyze electric power consumption data	With the development of AMM (Automatic Metering Management), it will be possible for electric power suppliers to acquire from the customers their electric power consumption up to every second. This will generate data arriving in multiple, continuous, rapid, and time-varying data streams. Data Stream Management Systems (DSMS) - currently available as prototypes - aim at facilitating the management of such data streams. This paper describes an experimental study which analyzes the advantages and limitations of using a DSMS for the management of electric power consumption data.	Talel Abdessalem, Raja Chiky, Georges Hébrail, Jean Louis Vitti	http://editions-rnti.fr/render_pdf.php?p1&p=1001733	http://editions-rnti.fr/render_pdf.php?p=1001733
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Application des vecteurs sémantiques à la fouille de texte	L'approche présentée ici se base sur un traitement du contenu syntaxicosémantiquepar un analyseur du Français, le système SYGFRAN(SYGFRAN) ,pour retrouver un ensemble de phrases appartenant à différents discours du présidentFrançois Mitterrand plongées dans un ensemble de phrases appartenantà différents discours du président Jacques Chirac. Ce traitement se fait par calculde vecteurs sémantiques de phrases ( méthodologie définie dans l'article ) etpar la définition d'une relation de similitude décrivant l'inclination de vecteursdont l'inclinaison, ou distance angulaire, est proche. A l'aide de cette relation,des phrases sont attribuées par le système à l'un ou l'autre des auteurs, et l'articleindique des F-mesures obtenues sur le premier corpus, dit d'apprentissage,légèrement supérieures à 80%.	Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000488	http://editions-rnti.fr/render_pdf.php?p=1000488
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Bilan du Premier Défi Francophone de Fouille de Textes	Le DÉ Fouille de Textes (DEFT) a consisté à supprimer lesphrases non pertinentes dans un corpus de discours politiques en français.Il a eu lieu en 2005 et réuni onze équipes, totalisant une trentaine departicipants. Cet article décrit les prétraitements eectués sur les corpusde F. Mitterrand et de J. Chirac dans le cadre de ce dé. Notamment,la conversion au format texte, le découpage en phrases, le classement desdiscours, l'introduction de phrases de F. Mitterrand dans les discours deJ. Chirac et l'identication des dates et noms de personnes. Les résultatsobtenus par les onze équipes participantes sont aussi présentés.	Jérôme Azé, Mathieu Roche, Erick Alphonse, Ahmed Amrani, Thomas Heitz, Amar-Djalil Mezaour	http://editions-rnti.fr/render_pdf.php?p1&p=1000473	http://editions-rnti.fr/render_pdf.php?p=1000473
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Dépendances syntaxiques et méthodes de détection de passages pour une segmentation sur le locuteur et le thème	Nous montrons ici l'intérêt des dépendances syntaxiques et desméthodes de détection de passages pour la détection du locuteur dans lesdiscours. Nous nous appuyons sur les méthodes développées dans le cadre dela campagne de fouilles de texte DEFT'05 dans le but de distinguer le discoursde François Mitterrand de celui de Jacques Chirac. Nous évaluons l'utilisationdes dépendances syntaxiques en tant qu'unité caractérisant les différencesentre les deux discours. Ces unités, obtenues par traitement linguistique,constituent donc les descripteurs retenus pour la représentation des discours, etsur lesquels nous appliquons un apprentissage. Les deux présidents ayant desdiscours sur des thématiques différentes nous combinons cet apprentissageavec l'utilisation d'une méthode de segmentation thématique pour détecter leschangements de locuteur.	Loic Maisonnasse, Caroline Tambellini	http://editions-rnti.fr/render_pdf.php?p1&p=1000485	http://editions-rnti.fr/render_pdf.php?p=1000485
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Identification de thème et reconnaissance du style d'un auteur pour une tâche de filtrage de textes	Pour résoudre une tâche de filtrage des textes d'un auteur, insérés dans les textes d'un autre auteur, nous avons utilisé à la fois le style de l'auteur et la structure thématique du texte. Nous caractérisons le style d'un auteur par un modèle de langage n-grammes de mots ou de caractères entraîné sur un corpus d'apprentissage. Nous appliquons ensuite les modèles sur chaque phrase du corpus de test pour en calculer l'auteur le plus probable. Un algorithme de lissage transforme ensuite les résultats en segments continus pour chaque auteur. Parallèlement, nous avons élaboré une méthode d'identification du thème de chaque auteur dans un document. Nous déterminons d'abord les segments de texte de plus grande densité, pour chaque mot du document, par chaînage lexical. Puis, nous recherchons les chaînes lexicales principales des deux thèmes, par hypothèse celles dont les segments respectifs sont les plus étendus et se recouvrent le moins. Les résultats des deux méthodes sont finalement fusionnés.	Michèle Jardino, Martine Hurault-Plantet, Gabriel Illouz	http://editions-rnti.fr/render_pdf.php?p1&p=1000486	http://editions-rnti.fr/render_pdf.php?p=1000486
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Le défi Fouille de Textes : Quels paradigmes pour la reconnaissance automatique d'auteurs ?	Les campagnes d'évaluation en traitement automatique du langage naturelet en informatique documentaire sont devenues un passage obligé pour lareconnaissance des différentes techniques employées. Le Défi Fouille de Texte apour objectif de permettre aux chercheurs du monde francophone de confronterleurs travaux avec un problème, plus que de primer une équipe, une méthode, ouun outil. Dans cet article nous évoquons les diverses problématiques de la fouillede textes, à savoir la recherche d'information, l'extraction ou l'enrichissement deconnaissances, la classification/catégorisation de documents, la segmentation detextes, le profilage. La reconnaissance d'auteur, objet de ce premier défi, est unetâche complexe et composite qui nécessite de traiter simultanément de la segmentation,de la catégorisation et du profilage. L'idée générale est que la miseen place des défis est un outil de cartographie des diverses avancées en fouille detextes, et également un instrument scientifique de compréhension de problèmesde nature complexe.	Violaine Prince, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000472	http://editions-rnti.fr/render_pdf.php?p=1000472
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Modèles multi-thématiques markoviens pour la segmentation de textes	Dans cet article, nous montrons comment des outils génériques de lafouille statistique de textes peuvent être utilisés pour résoudre une tâche d'apprentissagesupervisée: le DÉfi Fouille de Textes 2005. Dans un premier temps,nous étudions comment capturer une partie des spécificités de la tâche à l'aide demodèles de Markov cachés. Nous détaillons ensuite une modélisation des textespar un mélange de distributions multinomiales sur les comptes de mots, danslaquelle chaque composante correspond à un thème particulier. Les paramètresdes distributions thématiques sont estimés grâce à l'algorithme EM. Ce modèleest utilisé pour diviser en sous-thèmes les discours des deux présidents. Nousdiscutons finalement des performances obtenues en combinant ces deux outils.	Fabrice Clérot, Loïs Rigouste, Olivier Cappé, François Yvon	http://editions-rnti.fr/render_pdf.php?p1&p=1000484	http://editions-rnti.fr/render_pdf.php?p=1000484
Revue des Nouvelles Technologies de l'Information	DEFT	2007	Un duel probabiliste pour départager deux Présidents	Nous présentons une palette de modèles probabilistes que nous avonsemployés dans le cadre du défi DEFT'05. La tâche proposée conjuguait deuxproblématiques distinctes du Traitement Automatique du Langage : l'identificationde l'auteur (au sein de discours de Jacques Chirac, a pu être insérée uneséquence de phrases de François Mitterrand) et la détection de ruptures thématiques(les thèmes abordés par les deux auteurs sont censés être différents). Pouridentifier la paternité de ces séquences, nous avons utilisé des chaînes de Markov,des modèles bayésiens, et des procédures d'adaptation de ces modèles. Pource qui est des ruptures thématiques, nous avons appliqué une méthode probabilistemodélisant la cohérence interne des discours. Son ajout améliore les performances.Une comparaison avec diverses approches montre la supériorité d'unestratégie combinant apprentissage, cohérence et adaptation. Les résultats quenous obtenons, en termes de précision (0,890), rappel (0,955) et Fscore (0,925)sur le sous-corpus de test sont très encourageants.	Marc El-Bèze, Juan-Manuel Torres-Moreno, Frédéric Béchet	http://editions-rnti.fr/render_pdf.php?p1&p=1000474	http://editions-rnti.fr/render_pdf.php?p=1000474
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Actuariat et data mining: prise en compte des dépendances	Cet article vise à présenter un outil central en modélisation et en représentationde la dépendance entre risques: les copules. La corrélation n'ayantpas de motivation en assurance, il convient de pouvoir utiliser des notions plusgénérales, et nous verrons que les rangs sont en particulier des notions de premièreimportance. Nous évoquerons également les problèmes de corrélationsentre évènements extrêmes, particulièrement importants en gestion des risques	Arthur Charpentier	http://editions-rnti.fr/render_pdf.php?p1&p=1001492	http://editions-rnti.fr/render_pdf.php?p=1001492
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Améliorer les performances d'un modèle prédictif: perspectives et réalité	Dans cet article, nous montrons que les performances d'un modèleprédictif dépendent généralement plus de la qualité des données et du soin apportéà leur préparation et à leur sélection, que de la technique de modélisationelle-même. Entre deux techniques, l'écart de performance est souvent négligeableen regard des incertitudes résultant de la définition de la variable à expliqueret de la représentativité de l'échantillon d'étude. Toutefois, le rééchantillonnageet l'agrégation de modèles peuvent permettre de réduire drastiquement lavariance et parfois même le biais de certains modèles. De bons résultats peuventaussi être obtenus simplement par la partition de modèles, c'est-à-dire en partitionnanten classes l'échantillon initial et en construisant un modèle sur chaqueclasse.	Stéphane Tuffery	http://editions-rnti.fr/render_pdf.php?p1&p=1001487	http://editions-rnti.fr/render_pdf.php?p=1001487
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Analyse discriminante sur données binaires lorsque les populations d'apprentissage et de test sont différentes	L'analyse discriminante généralisée suppose que l'échantillon d'apprentissageet l'échantillon test, qui contient les individus à classer, sont issusd'une même population. Lorsque ces échantillons proviennent de populationspour lesquelles les paramètres des variables descriptives sont différents, l'analysediscriminante généralisée consiste à adapter la règle de classification issuede la population d'apprentissage à la population test, en estimant un lien entreces deux populations. Ce papier étend les travaux existant dans un cadre gaussienau cas des variables binaires. Afin de relever le principal défi de ce travail, quiconsiste à déterminer un lien entre deux populations binaires, nous supposonsque les variables binaires sont issues de la discrétisation de variables gaussienneslatentes. Une méthode d'estimation et des tests sur simulations sont présentés,puis des applications dans des contextes biologique et d'assurance illustrent cetravail	Julien Jacques, Christophe Biernacki	http://editions-rnti.fr/render_pdf.php?p1&p=1001490	http://editions-rnti.fr/render_pdf.php?p=1001490
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Approche des Valeurs Extrêmes dans la Modélisation des Séries Financières	Dans les années 60, les travaux de Mandelbrot sur les fluctuationsboursières montrèrent que le modèle gaussien ne convenait pas pour décrire lesrendements d'actifs. Mandelbrot (1963) puis Fama (1965) proposèrent la distributionLévy-stable, dont les propriétés sont très proches de celles des distributionsempiriques à queues lourdes, comme alternative pour modéliser les sériesfinancières. Ce choix est justifié par au moins deux bonnes raisons ; la premièreest le théorème central limite généralisé selon lequel les lois stables sont lesseules distributions limites possibles pour des sommes, convenablement normaliséeset centrées, de variables aléatoires (v.a.) indépendantes et identiquementdistribuées (i.i.d.) et la deuxième est le fait que les distributions stables peuventêtre dissymétriques et permettent des queues épaisses de telle sorte qu'ellesajustent les distributions empiriques beaucoup mieux que ne le font les distributionsgaussiennes. C'est ce que confirme l'exemple des rendements quotidiensde l'indice boursier CAC 40 que nous traitons à la fin de cet article dans lequelnous nous intéressons à l'estimation des paramètres caractérisant les lois Lévystables,ce qui constitue une étape essentielle dans le processus de modélisationdes séries financières	Djamel Meraghni, Abdelhakim Necir	http://editions-rnti.fr/render_pdf.php?p1&p=1001495	http://editions-rnti.fr/render_pdf.php?p=1001495
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Évaluation de la régression bornée	Le modèle linéaire est très fréquemment utilisé en statistique et particulièrementdans les secteurs de l'assurance, de la banque et du marketing. Ilpermet de déterminer les variables explicatives qui interviennent dans le risquemesuré chez les assurés et dans les choix effectués par la clientèle. Le problèmeconsidéré dans cet article apparaît lorsque ces variables sont liées statistiquement,par exemple le revenu et la catégorie socioprofessionnelle. Les estimationsdonnées par le critère des moindres carrés ordinaires deviennent alors instableset peuvent prendre des valeurs en contradiction avec les valeurs réelles. Il existede nombreuses méthodes adaptées à ce type de données. Nous proposons icid'évaluer l'efficacité de la régression bornée en procédant par simulation. Lesrésultats sont clairs : le gain en précision et en stabilité des coefficients de régressionest impressionnant.	Thierry Foucart	http://editions-rnti.fr/render_pdf.php?p1&p=1001491	http://editions-rnti.fr/render_pdf.php?p=1001491
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Incertitude du Ciblage Marketing	Cet article présente une logique de recherche d'information pour le ciblagedes clients dans le cadre de la fidélisation et de la prospection d'un marchépour une marque. Il est mis en exergue l'approche du dataminer qui a pour objectifd'améliorer la connaissance et la pertinence de la cible client. Ceci s'illustrepar le besoin de diminuer l'incertitude à prendre une décision sur l'affectationd'un client à une campagne. La méthode présentée consiste à opérer une successionde segmentations pour décider de l'appartenance d'un client à un segment,celle-ci pilotée par le minimum de l'indicateur d'incertitude : la mesurede l'entropie des probabilités a posteriori. L'incertitude se réduit au fur et à mesureque sont ajoutées des informations de sources différentes, en l'occurrence,l'expérience faite dans cet article, des données contextuelles de vie du client.On propose une méthode simple de regroupement des zones géographiques àfort potentiel et un algorithme d'optimisation des micros zones candidates pourune action marketing. Cette méthode a l'avantage de consolider la définition dessegments de la population cible pour une interprétation robuste du marqueteur	Stéphane Chauvin	http://editions-rnti.fr/render_pdf.php?p1&p=1001489	http://editions-rnti.fr/render_pdf.php?p=1001489
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Interactive Clustering Tree: Une méthode de classification descendante adaptée aux grands ensembles de données	Nous présentons une nouvelle méthode de segmentation non supervisée,particulièrement bien adaptée aux gros volumes de données et aux besoinsopérationnels du secteur Banque Assurance. Cette méthode de classificationdescendante hiérarchique présente la segmentation finale sous la formed'un arbre de décision dont l'appartenance aux classes (ou segments) dépendde règles logiques faisant intervenir les variables de l'analyse. De ce fait, laméthode hérite des propriétés inhérentes aux arbres de décision (interactivité,choix des variables de coupure, élagage/développement de l'arbre). Il en résulteune segmentation très simple d'interprétation et directement opérationnelle pourl'affectation d'un nouvel individu à l'une des classes. Enfin, elle intègre la possibilitéde construire l'arbre avec d'autres variables que celles dont on mesurel'inertie. En ce sens, nous pouvons la considérer comme une généralisation aucas multicibles, plusieurs variables à prédire simultanément, des méthodes supervisées.Cet article présente les fondements théoriques de la méthode et s'appuiesur un exemple pratique pour illustrer les résultats obtenus et les compareraux méthodes usuelles.	Ricco Rakotomalala, Tanguy Le Nouvel	http://editions-rnti.fr/render_pdf.php?p1&p=1001488	http://editions-rnti.fr/render_pdf.php?p=1001488
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Le Traitement des Refusés dans le Risque Crédit	Nous présentons la problématique du traitement des refusés dans lecadre de l'octroi de crédit. Les modèles d'octroi sont basés sur l'historique deremboursement du crédit par les clients acceptés par l'organisme de crédit, l'informationsur les refusés manque donc systématiquement : il en résulte queles modèles sont construits sur des échantillons intrinsèquement biaisés. Dansle cadre législatif actuel, les organismes doivent industrialiser leurs procédureset mettre en place des procédures de traitement documentées, notamment pourles refusés. Nous passons en revue quelques unes des techniques utilisées aujourd'huipar les grands organismes de crédit (reclassification, parcelling, reweightinget groupe de contrôle). Nous montrons sur un cas réel le comportementde ces techniques et évaluons l'impact du traitement sur le risque : en particulier,l'utilisation d'un groupe de contrôle bien choisi est évaluée. La capacitéà évaluer rapidement et effectivement les diverses techniques apparaît commeun point clé pour l'industrialisation des techniques de traitement des refusés.	Françoise Fogelman-Soulié, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001486	http://editions-rnti.fr/render_pdf.php?p=1001486
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Methodes d'estimation de durees de vie de contrats d'assurances automobiles	Dans cet article nous étudions le phénomène de résiliation de contrats en utilisant les méthodes classiques (non paramétriques, paramétriques et semi-paramétriques) de durées de vie appliquées à un portefeuille d'une compagnie de taille significative sur le marché français d'assurance non-vie.	Jean-Marie Marion, J.M. Loizeau, Abder Oulidi	http://editions-rnti.fr/render_pdf.php?p1&p=1001494	http://editions-rnti.fr/render_pdf.php?p=1001494
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Modern data analysis tools in personal financial services: a quantitative revolution	I argue that dramatic advances in methods and tools of data analysisare having an equally dramatic effect on how decisions are made in the personalfinancial services sector. A series of examples are given, and the sorts of toolsneeded are described	David J. Hand	http://editions-rnti.fr/render_pdf.php?p1&p=1001484	http://editions-rnti.fr/render_pdf.php?p=1001484
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Nouvelles méthodologies de la modélisation: Théorie de Vapnik et mise en oeuvre par KXEN	Nous présentons ici quelques éléments de la théorie de Vladimir Vapnik.Nous montrons comment les concepts de précision et de robustesse permettentde définir un «bon» modèle. Le théorème de Vapnik, que nous présentons,indique les contraintes pour qu'un modèle soit «bon »: la classe defonctions où l'on recherche ce modèle doit avoir une VC dimension finie et cetteclasse de fonctions est choisie selon le principe de SRM (minimisation structurelledu risque). Nous décrivons ensuite comment KXEN a implémenté ceséléments théoriques pour réaliser un moteur de modélisation efficace et robuste.	Michel Bera	http://editions-rnti.fr/render_pdf.php?p1&p=1001485	http://editions-rnti.fr/render_pdf.php?p=1001485
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Relaxations de la régression logistique : modèles pour l'apprentissage sur une sous-population et la prédiction sur une autre	Habituellement en analyse discriminante on a à prédire le groupe d'appartenanceà partir des variables de description ou covariables. La règle de prédictionest élaborée en utilisant un échantillon d'apprentissage soumis aux mêmesconditions externes que les individus à prédire. Dans ce travail, on s'intéresse àla prédiction d'individus d'une certaine sous-population utilisant un échantillond'apprentissage d'une autre sous-population. En assurance-finance, le problèmeapparaît quand il faut inférer le groupe d'appartenance de sociétaires-clients soumisà certaines conditions externes et que la règle est élaborée à partir d'individussoumis à d'autres. On propose différents modèles étendant la discriminationlogistique classique. Ces modèles se fondent sur des relations acceptables entreles fonctions scores que l'on associerait à chacune des sous-populations en présence.	Farid Beninel, Christophe Biernacki	http://editions-rnti.fr/render_pdf.php?p1&p=1001497	http://editions-rnti.fr/render_pdf.php?p=1001497
Revue des Nouvelles Technologies de l'Information	DMAS	2007	Une approche de tarification en assurance automobile par Réseaux de Neurones	Une approche d'estimation de la prime de risque, en assurance automobile,en utilisant la technique des réseaux de neurones, est proposée. L'entréedu réseau est constituée d'un vecteur de facteurs de risque signifiants. La sortieest un vecteur de classes de risque appropriées. La règle d'apprentissage quenous proposons, et qui est adaptée aux caractéristiques du problème traité, permetl'affectation d'un client (assuré) qui rentre dans le système, à une classede risque correspondante. Un intervalle de confiance pour la prime de base estdéterminé pour chaque classe de risque.	K. Boukhetala, M. Yahiaoui, T. Laadjel	http://editions-rnti.fr/render_pdf.php?p1&p=1001496	http://editions-rnti.fr/render_pdf.php?p=1001496
Revue des Nouvelles Technologies de l'Information	EDA	2007	Context-based Exploitation of DataWarehouses	An OLAP analysis can be defined as an interactive session during which an user launches queries over a data warehouse. The launched queries are often interdependent, and they can be either newly defined queries or they can be existing ones that are browsed and reused. Moreover, in a collaborative environment, queries may be shared among users. This notion of OLAP analysis has never been formally defined. In this paper, we propose a clear definition of this notion, by introducing a model for sharing, browsing and reusing OLAP queries over a data warehouse.	Yeow Wei Choong, Arnaud Giacometti, Dominique Laurent, Patrick Marcel, Elsa Negre, Nicolas Spyratos	http://editions-rnti.fr/render_pdf.php?p1&p=1000490	http://editions-rnti.fr/render_pdf.php?p=1000490
Revue des Nouvelles Technologies de l'Information	EDA	2007	Du relationnel au multidimensionnel - Conception de magasins de données	En vue d'assister le concepteur décisionnel, nous présentons une méthode ascendante de construction de schémas en étoile à partir d'une source relationnelle. Pour cela, nous étudions la structure des relations et nous proposons une classification en relation-associations et relation-entités permettant de construire des faits et des dimensions respectivement. Notre méthode a le mérite d'être indépendante de la sémantique du système d'information source. Elle exploite les contraintes de clés primaires et référentielles pour extraire les concepts multidimensionnels et affecte un niveau de pertinence à chaque concept extrait.	Jamel Feki, Yasser Hachaichi	http://editions-rnti.fr/render_pdf.php?p1&p=1000477	http://editions-rnti.fr/render_pdf.php?p=1000477
Revue des Nouvelles Technologies de l'Information	EDA	2007	DynaClose : Une approche de data mining pour la sélection des index de jointure binaires dans les entrepôts de données	L'indexation est l'une des techniques d'optimisation redondantes qui accélère les requêtes OLAP. Deux types d'index sont disponibles : les mono-index (B-tree, index binaire, projection, etc.) et les multi-index (index de jointure). Pour un entrepôt représenté par un schéma en étoile, les index de jointure binaires sont souvent utilisés pour accélérer les requêtes de jointure en étoile connues pour leur nombre important d'opérations de jointure. La sélection des index de jointure binaires est un problème difficile vu le nombre important des attributs candidats participant à la construction des index. Pour surmonter cette difficulté, nous proposons la démarche suivante : (1) nous adaptons d'abord un algorithme de fouille de données, appelé, Close qui permet de générer un ensemble d'itemsets fermés fréquents qui représentent les attributs candidats pour le processus de sélection des index. (2) Une fois les attributs candidats générés, nous proposons un algorithme itératif qui sélectionne un ensemble d'index de jointure binaires en prenant en compte l'ensemble des attributs candidats. Ces index doivent minimiser le coût d'exécution d'un ensemble de requêtes fréquentes et respecter une contrainte de stockage. Finalement, notre approche est validée par une étude expérimentale en la comparant avec les solutions existantes.	Hamid Necir, Ladjel Bellatreche, Rokia Missaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000482	http://editions-rnti.fr/render_pdf.php?p=1000482
Revue des Nouvelles Technologies de l'Information	EDA	2007	Echantillonnage optimisé de données temporelles distribuées pour l'alimentation des entrpôts de données	Les entrepôts de données sont de plus en plus alimentés par des données provenant d'un grand nombre de capteurs. Les capteurs trouvent leur utilité dans plusieurs domaines : médical, militaire, trafic routier, météorologie ou encore des données de consommation électrique. Pour faire face à la volumétrie et au taux d'arrivée des flux de données, des traitements sont effectués à la volée sur les flux avant leur enregistrement dans les entrepôts de données. Nous présentons des algorithmes d'échantillonnage optimisé sur des flux de données provenant de capteurs distribués. L'efficacité des algorithmes proposés a été testée sur un jeu de données de consommation électrique.	Raja Chiky, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000480	http://editions-rnti.fr/render_pdf.php?p=1000480
Revue des Nouvelles Technologies de l'Information	EDA	2007	Evolution de modèle dans les entrepôts de données - existant et perspectives	Un entrepôt de données permet d'intégrer des sources de données hétérogènes à des fins d'analyse. Un des points clés de la réussite du processus d'entreposage de données réside dans la définition du modèle de l'entrepôt en fonction des sources de données et des besoins d'analyse. Une fois l'entrepôt conçu, le contenu et la structure des sources de données, tout comme les besoins d'analyse sont amenés à évoluer et nécessitent ainsi une évolution du modèle de l'entrepôt (schéma et données). Dans cet article, nous présentons un panorama de différents travaux portant sur l'évolution du modèle dans les entrepôts de données. Nous comparons et discutons ces travaux selon les critères qui nous semblent pertinents pour cette problématique. Nous dressons également les perspectives de recherche qui en découlent.	Cécile Favre, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000478	http://editions-rnti.fr/render_pdf.php?p=1000478
Revue des Nouvelles Technologies de l'Information	EDA	2007	Evolution de schéma par classification automatique pour les  entrepôts de données	Les modèles et outils OLAP actuels gèrent les dimensions d'analyse d'un entrepôt de données de manière statique. Par conséquent, les axes d'analyse restent souvent figés malgré l'évolution des besoins et des données. Dans cet article, nous proposons une approche d'évolution de schéma basée sur une technique de classification automatique. Pour cela, nous cherchons le meilleur regroupement des instances d'un niveau d'analyse choisi par l'utilisateur en utilisant la méthode des k-means. Un nouvel axe d'analyse est ensuite construit à partir du résultat de cette classification. Pour choisir les descripteurs du niveau d'analyse à classifier, nous proposons deux solutions: la première utilise directement les attributs décrivant le niveau à classifier. Par contre, la deuxième solution décrit le niveau d'analyse par les mesures dans la table des faits. Pour valider notre approche, nous l'avons intégrée et testée à l'intérieur du SGBD (Système de Gestion de Bases de Données) Oracle 10g.	Fadila Bentayeb, Ony Rakotoarivelo	http://editions-rnti.fr/render_pdf.php?p1&p=1000483	http://editions-rnti.fr/render_pdf.php?p=1000483
Revue des Nouvelles Technologies de l'Information	EDA	2007	Extraction d'outliers dans des cubes de données : une aide à  la navigation	La recherche d'algorithmes d'extraction de connaissances à partir de cubes de données est un domaine actuellement très actif qui trouve de très nombreuses applications dans les entrepôts de données disponibles maintenant dans la plupart des entreprises et milieux scientifiques (biologie, santé, etc.). Nous nous intéressons ici à l'extraction de comportements atypiques (dénommés outliers) dans de tels cubes de données quand l'utilisateur veut identifier des séquences anormales. Par exemple, un directeur marketing aimerait savoir quelle zone géographique ne suit pas le même comportement que les autres afin de pouvoir y remédier. Pour ce faire, nous définissons une mesure de similarité capable d'appréhender de telles données complexes et définissons les algorithmes associés que nous avons testés sur différentes bases. Notons que nous considérons des cubes de données très denses, ce qui complexifie le problème de l'extraction.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000489	http://editions-rnti.fr/render_pdf.php?p=1000489
Revue des Nouvelles Technologies de l'Information	EDA	2007	Fragmentation des entrepôts de données XML	Les entrepôts de données XML proposent une base intéressante pour les applications décisionnelles qui exploitent des données hétérogènes et provenant de sources multiples. Cependant, les performances des SGBD natifs XML étant actuellement limitées en termes de temps de réponse et de volume des données, il est nécessaire de trouver des moyens pour les optimiser. Dans cet article, nous proposons une adaptation de la fragmentation horizontale dérivée définie dans le contexte relationnel aux entrepôts de données XML pour traiter ces deux problèmes. Nos expérimentations montrent que cette démarche permet de réduire le temps de traitement de requêtes décisionnelles XQuery de façon significative.	Hadj Mahboubi, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1000493	http://editions-rnti.fr/render_pdf.php?p=1000493
Revue des Nouvelles Technologies de l'Information	EDA	2007	From data warehousing to Active Information Integration	Enterprises have gathered operational business information from multiple structured data sources and stored it in a central repository, called data warehousing, for decision support functionalities and data analysis. The enterprises are now realizing to integrate their entire information sources, including "unstructured" contents, for deeper and richer information analysis. Several applications, such as processing warranty claims, finding promotional materials in real-time based on user's transaction value, detecting health insurance claim processing frauds in (near) real-time by integrating information from various data sources (some of them may be from the competitors), etc., require integration of both structured and unstructured information based on events and business policies. Thus, it is vital for data warehousing to enable the integration of data and content sources to provide real-time read and write access, to transform data for business analysis and data interchange, and to data placement for performance, currency and availability.	Mukesh Mohania	http://editions-rnti.fr/render_pdf.php?p1&p=1000475	http://editions-rnti.fr/render_pdf.php?p=1000475
Revue des Nouvelles Technologies de l'Information	EDA	2007	Intégration de données environnementales : une approche  basée sur les entrepôts de documents XML et les ontologies	Cet article présente l'approche que nous avons adoptée pour résoudre le problème d'intégration de données dans le contexte du projet SIC-Sénégal dont l'objectif est de permettre à plusieurs organismes partenaires de partager leurs sources de données environnementales. Nous réalisons une intégration en deux phases. Une première phase d'intégration structurelle, basée sur l'utilisation d'entrepôts de documents XML, permet de créer un entrepôt pour chaque organisme participant au projet. Une deuxième phase consiste alors à effectuer l'intégration de ces entrepôts de documents XML en associant une ontologie à chaque entrepôt. Cela se fait par une construction automatique d'ontologie OWL à partir des données XML de l'entrepôt et par une réutilisation de l'ontologie AGROVOC.	Ousmane Sall, Moussa Lo	http://editions-rnti.fr/render_pdf.php?p1&p=1000491	http://editions-rnti.fr/render_pdf.php?p=1000491
Revue des Nouvelles Technologies de l'Information	EDA	2007	LH*P2Prs Une Structure de Données Scalable et Distribuée Pour l'Environnement P2P	LH*P2Prs est une Structure de Données Scalable et Distribuée (SDDS), conçu spécialement pour l'environnement P2P. Sa propriété caractéristique est que toute requête adressée à un pair incorrect, est reacheminée vers le pair correct en un seul message. Cette propriété unique à l'heure actuelle est probablement impossible à améliorer, dans le cadre de l'axiomatique habituelle des SDDS et de systèmes P2P. Elle résulte d'un système particulier de notifications entre les paires quand les nouveaux pairs sont incorporés. La méthode offre aussi la /k/-disponibilité, protégeant les données stockées contre la panne simultanée de /k />= 1 pairs et contre le \"churn\" typique de systèmes P2P. La valeur de /k /est scalable. La protection résulte du maintien de la parité, calculée par un code à correction d'effacements particulièrement efficace, de type Reed-Salomon.	Witold Litwin	http://editions-rnti.fr/render_pdf.php?p1&p=1000476	http://editions-rnti.fr/render_pdf.php?p=1000476
Revue des Nouvelles Technologies de l'Information	EDA	2007	Modèle conceptuel pour l'analyse multidimensionnelle de documents	OLAP et les entrepôts de données sont utilisés pour l'analyse de données transactionnelles. De nos jours, avec l'évolution d'Internet et le développement de formats d'échange de données semi-structurées comme par exemple XML, il est possible de considérer les documents comme source d'analyse. En conséquence, un environnement d'analyse multidimensionnel adapté à ce type de données est nécessaire. Dans cet article, nous introduisons un modèle conceptuel multidimensionnel adapté à l'analyse de données documentaires, reposant sur un unique concept : une dimension. Nous définissons aussi un ensemble d'opérateurs d'analyse multidimensionnelle adaptés.	Franck Ravat, Olivier Teste, Ronan Tournier, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000492	http://editions-rnti.fr/render_pdf.php?p=1000492
Revue des Nouvelles Technologies de l'Information	EDA	2007	Structure réutilisable pour le calcul et la manipulation des cubes de données	Les cubes de données sont de plus en plus utilisés pour le pré-calcul de requêtes OLAP afin de permettre essentiellement à des analystes de trouver des tendances ou des anomalies dans de grandes quantités de données. Il se révèle que tout problème lié aux cubes de données est coûteux, que ce soit pour la construction, la matérialisation, la manipulation ou la mise à jour. Dans cet article, nous introduisons la notion de pré-calcul de cubes de données et la caractérisation associée qui est basée sur le modèle partitionnel. A notre connaissance, aucune des approches actuelles ne s'est intéressée à la réutilisation de pré-calcul des cubes de données. Pourtant cette dernière permet de calculer et de manipuler efficacement des cubes de données dans plusieurs contextes comme les applications météorologiques, le calcul de requêtes à la volée ou encore le calcul de plusieurs cubes de données en réseau ou en local.	Hassani Hachim, Noel Novelli	http://editions-rnti.fr/render_pdf.php?p1&p=1000481	http://editions-rnti.fr/render_pdf.php?p=1000481
Revue des Nouvelles Technologies de l'Information	EDA	2007	Vers l'échantillonnage d'un entrepôt de données	L'afflux de données sur les usages des produits et services nécessite des traitements lourds pour les transformer en information. Or la capacité à traiter les données ne peut pas suivre l'augmentation exponentielle des volumes stockés. Avec les technologies actuelles, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l'information produite. Nous proposons une approche basée sur l'échantillonnage d'un entrepôt de données pour déployer à moindre coût un système d'information décisionnel utilisant tout notre potentiel d'information. La brique technologique essentielle pour construire ce système repose sur un opérateur d'échantillonnage des jointures.	Raphaël Feraud, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000479	http://editions-rnti.fr/render_pdf.php?p=1000479
Revue des Nouvelles Technologies de l'Information	EGC	2007	Alignement de ressources sémantiques à partir de règles	Ce papier présente une approche automatique pour aligner des ressources sémantiques. L'alignement se traduit par la mise en correspondance des entités (termes, concepts, rôles) appartenant à des ressources d'un même domaine qui peuvent avoir des niveaux de formalisation différents. Les entités correspondantes sont de même nature et un coefficient caractérise leur degré de ressemblance.L'approche proposée est fondée sur des règles d'appariement entre les entités des deux ressources. Dans une première phase, ces règles d'appariement sont identifiées empiriquement. Des algorithmes combinant les différentes règles identifiées sont ensuite définis afin d'établir des correspondances entre les entités des ressources considérées.Ce papier présente un ensemble de règles d'appariement exploitant des éléments situés à différents niveaux conceptuels. Cet ensemble constitue un cadre pour l'alignement automatique des ressources sémantiques. Les résultats d'une première expérimentation qui a porté sur l'alignement de deux ressources du domaine de l'accidentologie sont également présentés.	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1001444	http://editions-rnti.fr/render_pdf.php?p=1001444
Revue des Nouvelles Technologies de l'Information	EGC	2007	Annotation et navigation de données archéologiques	Dans cet article, nous proposons un cadre et un outil pour l'annotation et la navigation de données archéologiques. L'objectif principal est de structurer les annotations de façon à permettre une navigation incrémentale où l'utilisateur peut, à partir d'un ensemble d'objets initialement retournés par une requête, découvrir des liens approximatifs avec d'autres objets de la base. L'approche a été implémentée et est en cours de validation.	Bernardo Lopez, Samira Hammiche, Samir Sebahi, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001350	http://editions-rnti.fr/render_pdf.php?p=1001350
Revue des Nouvelles Technologies de l'Information	EGC	2007	Annotation sémantique floue de tableaux guidée par une ontologie	Nous présentons dans cet article différentes étapes de l'annotation de tableaux de données à l'aide d'une ontologie. Tout d'abord, nous distinguons les colonnes de données numériques et symboliques. Les données symboliques sont ensuite annotées de manière floue à l'aide des termes de l'ontologie. Cette annotation nous permet de déduire le type des colonnes de données symboliques. Pour trouver le type des colonnes de données numériques, nous utilisons à la fois le titre de la colonne et les valeurs numériques et unités présentes dans la colonne. Chaque étape de notre annotation est validée expérimentalement.	Gaëlle Hignette, Patrice Buche, Juliette Dibie-Barthélemy, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1001441	http://editions-rnti.fr/render_pdf.php?p=1001441
Revue des Nouvelles Technologies de l'Information	EGC	2007	Application des réseaux bayésiens à l'analyse des facteurs impliqués dans le cancer du Nasopharynx	L'apprentissage de la structure des réseaux bayésien à partir de données est un problème NP-difficile. Une nouvelle heuristique de complexité polynômiale, intitulée Polynomial Max-Min Skeleton (PMMS), a été proposée en 2005 par Tsamardinos et al. et validée avec succès sur de nombreux bancs d'essai. PMMS présente, en outre, l'avantage d'être performant avec des jeux de données réduits. Néanmoins, comme tous les algorithmes sous contraintes, celui-ci échoue lorsque des dépendances fonctionnelles (déterministes) existent entre des groupes de variables. Il ne s'applique, par ailleurs, qu'aux données complètes. Aussi, dans cet article, nous apportons quelques modifications pour remédier à ces deux problèmes. Après validation sur le banc d'essai Asia, nous l'appliquons aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC) de 1289 observations, 61 variables et 5% de données manquantes issues d'un questionnaire. L'objectif est de dresser un profil statistique type de la population étudiée et d'apporter un éclairage utile sur les différents facteurs impliqués dans le NPC	Alexandre Aussem, Sergio Rodrigues de Morais, Marilys Corbex	http://editions-rnti.fr/render_pdf.php?p1&p=1001318	http://editions-rnti.fr/render_pdf.php?p=1001318
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apport du Web sémantique dans la réalisation d'un moteur de recherche géo-localisé à usage des entreprises	La recherche d'une entreprise sur le Web, relative à un savoir-faire particulier, n'est pas une tâche toujours facile à mener. Les outils mis à la disposition de l'internaute ne donnent pas entièrement satisfaction. D'un côté les moteurs de recherche éprouvent des difficultés à faire ressortir clairement le résultat escompté. De l'autre côté, les annuaires spécialisés (type Pages Jaunes) sont tributaires d'une organisation figée, nuisant à leur efficacité. Face à ce constat, nous nous proposons de créer un nouveau moteur spécialisé dans la recherche d'entreprise, associant Web sémantique et géo-localisation. Cette approche novatrice nécessite l'implémentation d'une ontologie ayant pour objectif la formalisation des connaissances du domaine. Cette tâche a mis en évidence l'intérêt des structures économiques, maintenues par l'INSEE, et leur utilisation au sein de l'ontologie. Les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises. La structure des unités administratives, telle que gérée au sein du fichier SIRENE, s'est avérée judicieuse pour répondre à la problématique de géo-localisation des entreprises. Une opération de désambiguïsation est réalisée en associant à chaque noeud d'activité les mots clés et synonymes lui correspondant. Enfin, nous comparons les résultats obtenus par notre moteur à ceux obtenu par le principal moteur de recherche d'activités géo-localisées en France : les Pages jaunes. Que ce soit au niveau de la précision et du rappel, notre moteur obtient des résultats significativement meilleurs.	Frédéric Triou, Fabien Picarougne, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1001307	http://editions-rnti.fr/render_pdf.php?p=1001307
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage actif d'émotions dans les dialogues Homme-Machine	La prise en compte des émotions dans les interactions Homme-machine permet de concevoir des systèmes intelligents, capables de s'adapter aux utilisateurs. Les techniques de redirection d'appels dans les centres téléphoniques automatisés se basent sur la détection des émotions dans la parole. Les principales difficultés pour mettre en oeuvre de tels systèmes sont l'acquisition et l'étiquetage des données d'apprentissage. Cet article propose l'application de deux stratégies d'apprentissage actif à la détection d'émotions dans des dialogues en interaction homme-machine. L'étude porte sur des données réelles issues de l'utilisation d'un serveur vocal et propose des outils adaptés à la conception de systèmes automatisés de redirection d'appels.	Alexis Bondu, Vincent Lemaire, Barbara Poulain	http://editions-rnti.fr/render_pdf.php?p1&p=1001428	http://editions-rnti.fr/render_pdf.php?p=1001428
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage semi-supervisé de fonctions d'ordonnancement	Nous présentons dans cet article un algorithme inductif semi-supervisé pour la tâche d'ordonnancement bipartite. Les algorithmes semi-supervisés proposés jusqu'à maintenant ont été étudiés dans le cadre strict de la classification. Récemment des travaux ont été réalisés dans le cadre transductif pour étendre les modèles existants en classification au cadre d'ordonnancement. L'originalité de notre approche est qu'elle est capable d'inférer un ordre sur une base test non- utilisée pendant la phase d'apprentissage, ce qui la rend plus générique qu'une méthode transductive pure. Les résultats empiriques sur la base CACM contenant les titres et les résumés du journal Communications of the Association for Computer Machinery montrent que les données non-étiquetées sont bénéfiques pour l'apprentissage de fonctions d'ordonnancement.	Tuong-Vinh Truong, Massih-Reza Amini	http://editions-rnti.fr/render_pdf.php?p1&p=1001425	http://editions-rnti.fr/render_pdf.php?p=1001425
Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage statistique de la topologie d'un ensemble de données étiquetées	Découvrir la topologie d'un ensemble de données étiquetées dans un espace Euclidien peut aider à construire un meilleur système de décision. Dans ce papier, nous proposons un modèle génératif basé sur le graphe de Delaunay de plusieurs prototypes représentant les données étiquetées dans le but d'extraire de ce graphe la topologie des classes.	Pierre Gaillard, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001419	http://editions-rnti.fr/render_pdf.php?p=1001419
Revue des Nouvelles Technologies de l'Information	EGC	2007	Approche connexionniste pour l'extraction de profils cas-témoins du cancer du Nasopharynx à partir des données issues d'une étude épidémiologique	Dans cet article, nous présentons un système de découverte de connaissances à partir de données issues d'une étude épidémiologique cas-témoins du cancer du Nasopharynx (NPC). Ces données étant obtenues par une collecte de questionnaires, elles ont d'une part, la particularité d'être qualitatives et, d'autre part, de présenter des valeurs manquantes. Prenant en compte ces deux dernières contraintes, le système que nous proposons suit une démarche d'exploration de données qui consiste à (1) définir une procédure de codage des données qualitatives en présence de valeurs manquantes ; (2) étudier les propriétés de l'algorithme des cartes auto-organisatrices de Kohonen et son adaptation à ce type de données dans un cadre de découverte et de visualisation de groupes homogènes des cas cancer / non-cancer ; (3) post-traiter le resultat de cet algorithme par une classification automatique pour optimiser le nombre de groupes ainsi trouvés, et (4) donner une interprétation sémantique des profils extraits de chaque groupe. L'objectif général de cette étude est d'éclater le profil statistique global de la population étudiée en un ensemble de profils types (cancer ou non-cancer) et d'extraire pour chaque profil l'ensemble de variables explicatives du NPC à partir d'une cartographie bidimensionnelle.	Khalid Benabdeslem, Mustapha Lebbah, Alexandre Aussem, Marilys Corbex	http://editions-rnti.fr/render_pdf.php?p1&p=1001418	http://editions-rnti.fr/render_pdf.php?p=1001418
Revue des Nouvelles Technologies de l'Information	EGC	2007	Approche logique pour la réconciliation de références	Le problème de réconciliation de références consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel. Dans cet article, nous étudions ce problème quand le schéma des données est décrit en RDFS étendu par certaines primitives de OWL-DL. Nous décrivons et montrons l'intérêt d'une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma. Ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma. Les premiers résultats ont été obtenus sur des données réelles dans le cadre du projet PICSEL 3 en collaboration avec France Telecom R&D.	Fatiha Saïs, Nathalie Pernelle, Marie-Christine Rousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001449	http://editions-rnti.fr/render_pdf.php?p=1001449
Revue des Nouvelles Technologies de l'Information	EGC	2007	Calcul et représentation efficace de cubes de données pour une visualisation orientée pixel	Les cubes de données fournissent une aide non négligeable lorsqu'il s'agit d'interroger des entrepôts de données. Un cube de données représente un pré-calcul de toutes les requêtes OLAP et ainsi améliore leur temps de réponses. Les approches proposées jusqu'à présent réduisent les temps de calcul et d'entrée sortie mais leur utilisation reste très coûteuse. D'autres travaux de recherche se sont intéressés à la visualisation de données pour les exploiter de façon interactive.Nous proposons une adaptation de la représentation condensée des cubes de données basée sur le modèle partitionnel. Cette technique nous permet de calculer efficacement un cube de données et de représenter les liens entre les données pour la visualisation. La visualisation proposée dans cet article est basée sur des techniques de visualisation orientée pixel et sur des techniques de diagramme de liens entre noeuds pour offrir à la fois une vision globale et locale pour l'exploitation. Cette nouvelle approche utilise d'une part les calculs efficaces de cubes de données et d'autre part les techniques avancées de visualisation.	Noel Novelli, David Auber	http://editions-rnti.fr/render_pdf.php?p1&p=1001362	http://editions-rnti.fr/render_pdf.php?p=1001362
Revue des Nouvelles Technologies de l'Information	EGC	2007	Caractérisation des transitions temporisées dans les logs de conversation de services Web	La connaissance du protocole de conversation d'un service Web est importante pour les utilisateurs et les fournisseurs, car il en modélise le comportement externe ; mais, il n'est souvent pas spécifié lors de la conception. Notre travail s'inscrit dans une thématique d'extraction du protocole de conversation d'un service existant à partir de ses données d'exécution. Nous en étudions un sous-problème important qui est la découverte des transitions temporisées (i.e. les changements d'état liés à des contraintes temporelles). Nous proposons un cadre formel aboutissant à la définition des expirations propres, qui représentent un équivalent dans les logs des transitions temporisées. A notre connaissance, ceci représente la première contribution à la résolution de ce problème.	Didier Devaurs, Fabien De Marchi, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001302	http://editions-rnti.fr/render_pdf.php?p=1001302
Revue des Nouvelles Technologies de l'Information	EGC	2007	Cartographie de l'organisation : une approche topologique des connaissances	La gestion des connaissances est devenue aujourd'hui un enjeu majeur pour toute organisation. Celle-ci a pour but de capitaliser et de rendre accessible à ses acteurs la connaissance détenue par l'organisation. Cet article s'intéresse particulièrement à la visualisation à deux niveaux de ces connaissances (macroscopique - relatif aux connaissances globales détenues par l'organisation - et microscopique - relatif aux connaissances locales détenues par chaque membre organisationnel). La caractérisation des connaissances détenues par les acteurs repose sur quatre dimensions complémentaires (formelle, conative, cognitive, et socio-cognitive). Les deux types de visualisation proposés s'appuient sur les cartes auto-organisatrices et permettent une navigation dans différentes représentations des connaissances de l'organisation.	Marc Boyer, Marie-Françoise Canut, Max Chevalier, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1001437	http://editions-rnti.fr/render_pdf.php?p=1001437
Revue des Nouvelles Technologies de l'Information	EGC	2007	Choix des conclusions et validation des règles issues d'arbres de classification	Cet article traite de la validation de règles dans un contexte de ciblage où il s'agit de déterminer les profils type des différentes valeurs de la variable à prédire. Les concepts de l'analyse statistique implicative fondée sur la différence entre nombre observé de contre-exemples et nombre moyen que produirait le hasard, s'avèrent particulièrement bien adaptés à ce contexte. Le papier montre comment les notions d'indice et d'intensité d'implication de Gras s'appliquent aux règles produites par les arbres de décision et présente des alternatives inspirées de résidus utilisés en modélisation de tables de contingence. Nous discutons ensuite sur un jeu de données réelles deux usages de ces indicateurs de force d'implication pour les règles issues d'arbres. Il s'agit d'une part de l'évaluation individuelle des règles, et d'autre part de leur utilisation comme critère pour le choix de la conclusion de la règle.	Vincent Pisetta, Gilbert Ritschard, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001424	http://editions-rnti.fr/render_pdf.php?p=1001424
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classement des fragments de documents XML par une méthode d'aide à la décision	Vu l'accroissement constant du volume d'information accessible en ligne sous format XML, il devient primordial de proposer des modèles adaptés à la recherche d'information dans les documents XML. Tandis que la recherche d'information classique repose sur l'indexation du contenu des documents, la recherche d'information dans les documents XML tente d'améliorer la qualité des résultats en tirant profit de la sémantique véhiculée par la structure des documents. Dans cet article, nous présentons une méthode de classement des items (éléments XML) retournés lors d'une recherche dans une collection de documents XML. Le classement repose sur la prise en compte d'un ensemble de critères discriminants. La particularité de notre approche réside dans la façon dont nous les utilisons : Nous employons une méthode décisionnelle pour classer les items en les comparant deux-à-deux là où en général une fonction de scoring globale est utilisée.	Faiza Abbaci, Pascal Francq	http://editions-rnti.fr/render_pdf.php?p1&p=1001393	http://editions-rnti.fr/render_pdf.php?p=1001393
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification de fonctions continues à l'aide d'une distribution et d'une densité définies dans un espace de dimension infinie	Il n'est pas rare que des données individu soient caractérisées par une distribution continue et non une seule valeur. Ces données fonctionnelles peuvent être utilisées pour classer les individus. Une solution élémentaire est de réduire les distributions à leurs moyennes et variances. Une solution plus riche a été proposée par Diday (2002) et mise en oeuvre par Vrac et al. (2001) et Cuvelier et Noirhomme-Fraiture (2005). Elle utilise des points de coupures dans les distributions et modélise ces valeurs conjointes par une distribution multidimensionnelle construite à l'aide d'une copule. Nous avons montré dans un précédent travail que, si cette technique apporte de bons résultats, la qualité de la classification dépend néanmoins du nombre et de l'emplacement des coupures. Les questions du choix du nombre et de l'emplacement des coupures restaient des questions ouvertes. Nous proposons une solution à ces questions, lorsque le nombre de coupures tend vers l'infini, en proposant une nouvelle distribution de probabilité adaptée à l'espace de dimension infinie que forment les données fonctionnelles. Nous proposons aussi une densité de probabilité adaptée à la nature de cette distribution en utilisant la dérivée directionnelle de Gâteaux. La direction choisie pour cette dérivée est celle de la dispersion des fonctions à classer. Les résultats sont encourageants et offrent des perspectives multiples dans tous les domaines où une distribution de données fonctionnelles est nécessaire.	Etienne Cuvelier, Monique Noirhomme-Fraiture	http://editions-rnti.fr/render_pdf.php?p1&p=1001456	http://editions-rnti.fr/render_pdf.php?p=1001456
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification de grands ensembles de données avec un nouvel algorithme de SVM	Le nouvel algorithme de boosting de Least-Squares Support Vector Machine (LS-SVM) que nous présentons vise à la classification de très grands ensembles de données sur des machines standard. Les méthodes de SVM et de noyaux permettent d'obtenir de bons résultats en ce qui concerne la précision mais la tâche d'apprentissage pour de grands ensembles de données demande une grande capacité mémoire et un temps relativement long. Nous présentons une extension de l'algorithme de LS-SVM proposé par Suykens et Vandewalle pour le boosting de LS-SVM. A cette fin, nous avons ajouté un terme de régularisation de Tikhonov et utilisé la formule de Sherman-Morrison-Woodbury pour traiter des ensembles de données ayant un grand nombre de dimensions. Nous l'avons ensuite étendu par application du boosting de LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances de l'algorithme sont évaluées sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et NDC sur une machine standard (PC-P4, 3GHz, 512 Mo RAM).	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001466	http://editions-rnti.fr/render_pdf.php?p=1001466
Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification supervisée de séquences biologiques basée sur les motifs et les matrices de substitution	La classification des séquences biologiques est l'un des importants défis ouverts dans la bioinformatique, tant pour les séquences protéiques que pour les séquences nucléiques. Cependant, la présence de ces données sous la forme de chaînes de caractères ne permet pas de les traiter par les outils standards de classification supervisée, qui utilisent souvent le format relationnel. Pour remédier à ce problème de codage, plusieurs travaux se sont basés sur l'extraction des motifs pour construire une nouvelle représentation des séquences biologiques sous la forme d'un tableau binaire. Nous décrivons une nouvelle approche qui étend les méthodes précédents par l'utilisation de matrices de substitution dans les cas des séquences protéiques. Nous présentons ensuite une étude comparative qui prend en compte l'effet de chaque méthode sur la précision de la classification mais aussi le nombre d'attributs générés et le temps de calcul.	Rabie Saidi, Mondher Maddouri, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001409	http://editions-rnti.fr/render_pdf.php?p=1001409
Revue des Nouvelles Technologies de l'Information	EGC	2007	Clustering : from model-based approaches to heuristic algorithms	Les méthodes du 'clustering' ont pour but de diviser un ensemble (large) d'objets dans un petit nombre de groupes homogènes (clusters), basé sur des données relevées ou observées qui décrivent les (dis-)similarités qui existent entre les objets - en espérant que ces clusters soient utiles pour l'application concernée. Il existe une multitude d'approches, et cette contribution présente quelques-unes qui sont les plus importantes ou actuelles.	Hans-Hermann Bock	http://editions-rnti.fr/render_pdf.php?p1&p=1001287	http://editions-rnti.fr/render_pdf.php?p=1001287
Revue des Nouvelles Technologies de l'Information	EGC	2007	Combinaison des cartes topologiques mixtes et des machines à vecteurs de support : une application pour la prédiction de perte de poids chez les obèses	Cet article présente un modèle pour aborder les problèmes de classement difficiles, en particulier dans le domaine médical. Ces problèmes ont souvent la particularité d'avoir des taux d'erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problèmes, nous proposons d'utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. La combinaison de ces deux modèles permet non seulement d'améliorer la visualisation des données mais aussi en les performances en généralisation. Ce modèle (CT-SVM) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation expérimentale de notre modèle (CT-SVM), nous avons utilisé quatre jeux de données. La première base est un extrait d'une grande base médicale sur l'étude de l'obésité réalisée à l'Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature.	Mohamed Ramzi Temanni, Mustapha Lebbah, Christine Poitou-Bernert, Karine Clément, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001298	http://editions-rnti.fr/render_pdf.php?p=1001298
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction coopérative de carte de thèmes : vers une modélisation de l'activité socio-sémantique	Nous présentons dans cette contribution un cadre de modélisation recourant conjointement au modèle Hypertopic (Cahier et al., 2004) pour la représentation des connaissances de domaine et au modèle SeeMe (Herrmann et al., 1999) pour la représentation de l'activité. Ces deux approches apparaissent complémentaires, et nous montrons comment elles peuvent être combinées, pour mieux ancrer, sur les plans formel et méthodologique, les approches de cartographie collective des connaissances.	L'Hédi Zaher, Jean-Pierre Cahier, Christophe Lejeune, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1001304	http://editions-rnti.fr/render_pdf.php?p=1001304
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction d'ontologie à partir de corpus de textes	Cet article présente une méthode semi-automatique de construction d'ontologie à partir de corpus de textes sur un domaine spécifique. Cette méthode repose en premier lieu sur un analyseur syntaxique partiel et robuste des textes, et en second lieu, sur l'utilisation de l'analyse formelle de concepts "FCA" pour la construction de classes d'objets en un treillis de Galois. La construction de l'ontologie, c'est à dire d'une hiérarchie de concepts et d'instances, est réalisée par une transformation formelle de la structure du treillis. Cette méthode s'applique dans le domaine de l'astronomie.	Amedeo Napoli, Yannick Toussaint, Rokia Bendaoud	http://editions-rnti.fr/render_pdf.php?p1&p=1001364	http://editions-rnti.fr/render_pdf.php?p=1001364
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction et analyse de résumés de données évolutives : application aux données d'usage du Web	La manière dont une visite est réalisée sur un site Web peut changer en raison de modifications liées à la structure et au contenu du site lui-même, ou bien en raison du changement de comportement de certains groupes d'utilisateurs ou de l'émergence de nouveaux comportements. Ainsi, les modèles associés à ces comportements dans la fouille d'usage du Web doivent être mis à jour continuellement afin de mieux refléter le comportement actuel des internautes. Une solution, proposée dans cet article, est de mettre à jour ces modèles à l'aide des résumés obtenus par une approche évolutive des méthodes de classification.	Alzennyr Da Silva, Yves Lechevallier, Fabrice Rossi, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001434	http://editions-rnti.fr/render_pdf.php?p=1001434
Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction incrémentale et visualisation de graphes de voisinage par des fourmis artificielles	Cet article décrit un nouvel algorithme incrémental nommé AntGraph pour la construction de graphes de voisinage. Il s'inspire du comportement d'autoassemblage observé chez des fourmis réelles où ces dernières se fixent progressivement à un support fixe puis successivement aux fourmis déjà fixées afin de créer une structure vivante. Nous utilisons ainsi une approche à base de fourmis artificielles où chaque fourmi représente une donnée. Nous indiquons comment ce comportement peut être utilisé pour construire de manière incrémentale un graphe à partir d'une mesure de similarité entre les données. Nous montrons finalement que notre algorithme obtient de meilleurs résultats en comparaison avec le graphe de Voisins Relatifs, notamment en terme de temps de calcul.	Julien Lavergne, Hanane Azzag, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001323	http://editions-rnti.fr/render_pdf.php?p=1001323
Revue des Nouvelles Technologies de l'Information	EGC	2007	Découverte de chroniques à partir de séquences d'événements pour la supervision de processus dynamiques	Ce papier adresse le problème de la découverte de connaissances temporelles à partir des données datées, générées par le système de supervision d'un processus de fabrication. Par rapport aux approches existantes qui s'appliquent directement aux données, notre méthode d'extraction des connaissances se base sur un modèle global construit à partir des données. L'approche de modélisation adoptée, dite stochastique, considère les données datées comme une séquence d'occurrences de classes d'événements discrets. Cette séquence est représentée sous les formes duales d'une chaîne de Markov homogène et d'une superposition de processus de Poisson. L'algorithme proposé, appelé BJT4R, permet d'identifier les motifs séquentiels, les plus probables entre deux classes d'événements discrets et les représentent sous la forme de modèles de chroniques. Ce papier présente les premiers résultats de l'application de cet algorithme sur des données générées par un processus de fabrication de semi-conducteur d'un site de production du groupe STMicroelectronics.	Nabil Benayadi, Marc Le Goc, Philippe Bouché	http://editions-rnti.fr/render_pdf.php?p1&p=1001389	http://editions-rnti.fr/render_pdf.php?p=1001389
Revue des Nouvelles Technologies de l'Information	EGC	2007	Des fonctions d'oubli intelligentes dans les entrepôts de données	Les entrepôts de données stockent des quantités de données de plus en plus massives et arrivent vite à saturation. Un langage de spécifications de fonctions d'oubli est défini pour résoudre ce problème. Dans le but d'offrir la possibilité d'effectuer des analyses sur l'historique des données, les spécifications définissent des résumés par agrégation et par échantillonnage à conserver parmi les données à "oublier". Cette communication présente le langage de spécifications ainsi que les principes et les algorithmes pour assurer de façon mécanique la gestion des fonctions d'oubli.	Aliou Boly, Sabine Goutier, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001380	http://editions-rnti.fr/render_pdf.php?p=1001380
Revue des Nouvelles Technologies de l'Information	EGC	2007	Détermination du niveau de consommation des abonnés en téléphonie mobile par la théorie des ensembles flous	La détermination du niveau de consommation chez les clients est essentielle pour tout objectif de segmentation stratégique et de churn. Nous présentons sur un cas réel l'utilisation de la théorie des ensembles flous pour la définition d'une fonction d'appartenance permettant d'évaluer, de manière précise, le niveau de consommation, des abonnés en téléphonie mobile.	Rachid El Meziane, Ilham Berrada, Ismail Kassou, Karim Baïna	http://editions-rnti.fr/render_pdf.php?p1&p=1001378	http://editions-rnti.fr/render_pdf.php?p=1001378
Revue des Nouvelles Technologies de l'Information	EGC	2007	Ensemble prédicteur fondé sur les cartes auto-organisatrices adapté aux données volumineuses	Le stockage massif des données noie l'information pertinente et engendre des problèmes théoriques liés à la volumétrie des données disponibles. Ces problèmes dégradent la capacité prédictive des algorithmes d'extraction des connaissances à partir des données. Dans cet article, nous proposons une méthodologie adaptée à la représentation et à la prédiction des données volumineuses. A cette fin, suite à un partitionnement des attributs, des groupes d'attributs non-corrélés sont créés qui permettent de contourner les problèmes liés aux espaces de grandes dimensions. Un Ensemble est alors mis en place, apprenant chaque groupe par une carte auto-organisatrice. Outre la prédiction, ces cartes ont pour objectif une représentation pertinente des données. Enfin, la prédiction est réalisée par un vote des différentes cartes. Une expérimentation est menée qui confirme le bien-fondé de cette approche.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001422	http://editions-rnti.fr/render_pdf.php?p=1001422
Revue des Nouvelles Technologies de l'Information	EGC	2007	Evaluation d'une approche de classification conceptuelle	L'objectif de ce travail est d'évaluer la perte d'information au sens de l'inertie entre des méthodes de partitionnement ou de classification hiérarchiques et une approche de classification conceptuelle. Nous voulons répondre à la question suivante : l'aspect simpliste du processus monothétique d'une méthode conceptuelle implique-t-il des partitions de moins bonne qualité au sens du critère de l'inertie ? Nous proposons de réaliser cette expérience sur 6 bases de l'UCI, trois de ces bases sont des tableaux de données quantitatives, les trois autres sont des tableaux de données qualitatives.	Marie Chavent, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001455	http://editions-rnti.fr/render_pdf.php?p=1001455
Revue des Nouvelles Technologies de l'Information	EGC	2007	Evaluation supervisée de métrique : application à la préparation de données séquentielles	De nos jours, le statisticien n'a plus nécessairement le contrôle sur la récolte des données. Le besoin d'une analyse statistique vient dans un second temps, une fois les données récoltées. Par conséquent, un travail est à fournir lors de la phase de préparation des données afin de passer d'une représentation informatique à une représentation statistique adaptée au problème considéré. Dans cet article, nous étudions un procédé de sélection d'une bonne représentation en nous basant sur des travaux antérieurs. Nous proposons un protocole d'évaluation de la pertinence d'une représentation par l'intermédiaire d'une métrique, dans le cas de la classification supervisée. Ce protocole exploite une méthode de classification non paramétrique régularisée, garantissant l'automaticité et la fiabilité de l'évaluation. Nous illustrons le fonctionnement et les apports de ce protocole par un problème réel de préparation de données de consommation téléphonique. Nous montrons également la fiabilité et l'interprétabilité des décisions qui en résultent.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001392	http://editions-rnti.fr/render_pdf.php?p=1001392
Revue des Nouvelles Technologies de l'Information	EGC	2007	Evolution de l'ontologie et gestion des annotations sémantiques inconsistantes	Les ontologies et les annotations sémantiques sont deux composants importants dans un système de gestion des connaissances basé sur le Web sémantique. Dans l'environement dynamique et distribué du Web sémantique, les ontologies et les annotations pourraient être changées pour s'adapter à l'évolution de l'organisation concernée. Ces changements peuvent donc entraîner des inconsistances à détecter et traiter. Dans cet article, nous nous focalisons principalement sur l'évolution des annotations sémantiques en soulignant le contexte où les modifications de l'ontologie entraînent des inconsistances sur ces annotations. Nous présentons une approche basée sur des règles permettant de détecter les inconsistances dans les annotations sémantiques devenues obsolètes par rapport à l'ontologie modifiée. Nous décrivons aussi les stratégies d'évolution nécessaires pour guider le processus de résolution de ces inconsistances grâce à des règles correctives.	Phuc-Hiep Luong, Rose Dieng-Kuntz, Alain Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001450	http://editions-rnti.fr/render_pdf.php?p=1001450
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extension sémantique du modèle de similarité basé sur la proximité floue des termes	Le modèle flou de proximité repose sur l'hypothèse que plus les occurrences des termes d'une requête se trouvent proches dans un document, plus ce dernier est pertinent. Cette mesure floue est très avantageuse dans le traitement des documents à textes courts, toutefois elle ne tient pas compte de la sémantique des termes. Nous présentons dans cet article l'intégration d'une métrique conceptuelle au modèle de proximité floue des termes pour la formalisation de notre propre modèle.	Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1001399	http://editions-rnti.fr/render_pdf.php?p=1001399
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction d'entités dans des collections évolutives	Nous nous intéressons à l'extraction d'entités nommées avec comme but d'exploiter un ensemble de rapports pour en extraire une liste de partenaires. À partir d'une liste initiale, nous utilisons un premier ensemble de documents pour identifier des schémas de phrase qui sont ensuite validés par apprentissage supervisé sur des documents annotés pour en mesurer l'efficacité avant d'être utilisés sur l'ensemble des documents à explorer. Cette approche est inspirée de celle utilisée pour l'extraction de données dans les documents semi-structurés (wrappers) et ne nécessite pas de ressources linguistiques particulières ni de larges collections de tests. Notre collection de documents évoluant annuellement, nous espérons de plus une amélioration de notre extraction dans le temps.	Thierry Despeyroux, Eduardo Fraschini, Anne-Marie Vercoustre	http://editions-rnti.fr/render_pdf.php?p1&p=1001433	http://editions-rnti.fr/render_pdf.php?p=1001433
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de connaissances d'adaptation par analyse de la base de cas	En raisonnement à partir de cas, l'adaptation d'un cas source pour résoudre un problème cible est une étape à la fois cruciale et difficile à réaliser. Une des raisons de cette difficulté tient au fait que les connaissances d'adaptation sont généralement dépendantes du domaine d'application. C'est ce qui motive la recherche sur l'acquisition de connaissances d'adaptation (ACA). Cet article propose une approche originale de l'ACA fondée sur des techniques d'extraction de connaissances dans des bases de données (ECBD). Nous présentons CABAMAKA, une application qui réalise l'ACA par analyse de la base de cas, en utilisant comme technique d'apprentissage l'extraction de motifs fermés fréquents. L'ensemble du processus d'extraction des connaissances est détaillé, puis nous examinons comment organiser les résultats obtenus de façon à faciliter la validation des connaissances extraites par l'analyste.	Amedeo Napoli, Jean Lieber, Fadi Badra	http://editions-rnti.fr/render_pdf.php?p1&p=1001464	http://editions-rnti.fr/render_pdf.php?p=1001464
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de données sur Internet avec Retroweb	Ce document décrit Retroweb, une boite à outils qui permet l'extraction de données structurées à partir de pages Web. Notre solution est semi-automatique car les données à extraire sont préalablement dénies par l'utilisateur. L'intérêt de cette approche est qu'elle permet l'extraction de données ciblées et conformes aux besoins de l'application utilisatrice (migrateur, moteur de recherche, outil de veille). Retroweb se caractérise aussi par une grande facilité d'utilisation car il ne nécessite aucune connaissance de langage particulier, la définition des règles d'extraction se faisant directement de manière interactive dans le navigateur Internet. Ce document décrit les trois principaux processus de notre méthode.	Fabrice Estievenart, Jean-Roch Meurisse	http://editions-rnti.fr/render_pdf.php?p1&p=1001338	http://editions-rnti.fr/render_pdf.php?p=1001338
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de séquences multidimensionnelles convergentes et divergentes	Les motifs séquentiels sont un domaine de la fouille de données très étudié depuis leur introduction par Agrawal et Srikant.Même s'il existe de nombreux travaux (algorithmes, domaines d'application), peu d'entre eux se situent dans un contexte multidimensionnel avec la prise en compte de ses spécificités : plusieurs dimensions, relations hiérarchiques entre les éléments de chaque dimension, etc. Dans cet article, nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue : du général au particulier ou vice et versa. Nous définissons ainsi le concept de séquences multidimensionnelles convergentes ou divergentes ainsi que l'algorithme associé, M2S_CD, basé sur le paradigme "pattern growth". Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l'intérêt de notre approche aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001388	http://editions-rnti.fr/render_pdf.php?p=1001388
Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction des Top-k Motifs par Approximer-et-Pousser	Cet article porte sur l'extraction de motifs sous contraintes globales. Contrairement aux contraintes usuelles comme celle de fréquence minimale, leur vérification est problématique car elle entraine de multiples comparaisons entre les motifs. Typiquement, la localisation des k motifs maximisant une mesure d'intérêt, i.e. satisfaisant la contrainte top-k, est difficile. Pourtant, cette contrainte globale se révèle très utile pour trouver les motifs les plus significatifs au regard d'un critère choisi par l'utilisateur. Dans cet article, nous proposons une méthode générale d'extraction de motifs sous contraintes globales, appelée Approximer-et-Pousser. Cette méthode peut être vue comme une méthode de relaxation d'une contrainte globale en une contrainte locale évolutive. Nous appliquons alors cette approche à l'extraction des top-k motifs selon une mesure d'intérêt. Les expérimentations montrent l'efficacité de l'approche Approximer-et-Pousser.	Arnaud Soulet, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1001387	http://editions-rnti.fr/render_pdf.php?p=1001387
Revue des Nouvelles Technologies de l'Information	EGC	2007	Filtrage des sites Web à caractère violent par analyse du contenu textuel et structurel	Dans cet article, nous proposons une solution pour la classification et le filtrage des sites Web à caractère violent. A la différence de la majorité de systèmes commerciaux basés essentiellement sur la détection de mots indicatifs ou l'utilisation d'une liste noire manuellement collectée, notre solution baptisée, "WebAngels Filter", s'appuie sur un apprentissage automatique par des techniques de data mining et une analyse conjointe du contenu textuel et structurel de la page Web. Les résultats expérimentaux obtenus lors de l'évaluation de notre approche sur une base de test sont assez bons. Comparé avec des logiciels, parmi les plus populaires, "WebAngels Filter" montre sa performance en terme de classification.	Abdelmajid Ben Hamadou, Mohamed Hammami, Radhouane Guermazi	http://editions-rnti.fr/render_pdf.php?p1&p=1001395	http://editions-rnti.fr/render_pdf.php?p=1001395
Revue des Nouvelles Technologies de l'Information	EGC	2007	Finding interesting queries in relational databases	La découverte de motifs dans des bases de données relationnelles quelconques est un problème intéressant pour lequel il existe très peu de méthodes efficaces. Nous présentons un cadre dans lequel des paires de requêtes sur les données sont utilisées comme des motifs et nous discutons du problème de la découverte d'associations utiles entre elles. Plus spécifiquement, nous considérons des petites sous-classes de requêtes conjonctives qui nous permettent de découvrir des motifs intéressants de manière efficace.	Bart Goethals	http://editions-rnti.fr/render_pdf.php?p1&p=1001285	http://editions-rnti.fr/render_pdf.php?p=1001285
Revue des Nouvelles Technologies de l'Information	EGC	2007	Fusion des approches visuelles et contextuelles pour l'annotation des images médicales	Dans le contexte de la recherche d'information sur Internet, nous proposons une architecture d'annotation automatique des images médicales, extraites à partir des documents de santé en ligne. Notre système est conçu pour extraire des informations médicales spécifiques (i.e. modalité médicale, région anatomique) à partir du contenu et du contexte des images. Nous proposons une architecture de fusion des approches contenu/contexte adaptée aux images médicales. L'approche orientée sur le contenu des images, consiste à annoter des images inconnues par la catégorisation des représentations visuelles compactes. Nous utilisons en même temps le contexte des images (les régions textuelles) ainsi que des ontologies médicales spécialement adaptées aux informations recherchées. Finalement, nous démontrons qu'en fusionnant les décisions des deux approches, nous améliorons les performances globales du système d'annotation.	Filip Florea, Valeriu Cornea, Alexandrina Rogozan, Abdelaziz Bensrhair, Stéfan Jacques Darmoni	http://editions-rnti.fr/render_pdf.php?p1&p=1001416	http://editions-rnti.fr/render_pdf.php?p=1001416
Revue des Nouvelles Technologies de l'Information	EGC	2007	Génération et enrichissement automatique de listes de patrons de phrases pour les moteurs de questions-réponses	Nous utilisons un algorithme d'amorce mutuelle (Riloff et Jones 99), entre des couples de termes d'une relation et des patrons de phrase. À partir de couples d'amorce, le système génère des listes de patrons qui sont ensuite enrichies de façon semi-supervisée, puis utilisées pour trouver de nouveaux couples. Ces couples sont à leur tour réutilisés pour générer, par itérations successives, de nouveaux patrons. L'originalité de l'étude réside dans l'interprétation du rappel, estimé comme la couverture d'un patron sur l'ensemble des exemples auxquels il s'applique	Cédric Vidrequin, Juan-Manuel Torres-Moreno, Jean-Jacques Schneider, Marc El-Bèze	http://editions-rnti.fr/render_pdf.php?p1&p=1001363	http://editions-rnti.fr/render_pdf.php?p=1001363
Revue des Nouvelles Technologies de l'Information	EGC	2007	Intégration des connaissances utilisateurs pour des analyses personnalisées dans les entrepôts de données évolutifs	Dans cet article, nous proposons une approche d'évolution de schéma dans les entrepôts de données qui permet aux utilisateurs d'intégrer leurs propres connaissances du domaine afin d'enrichir les possibilités d'analyse de l'entrepôt. Nous représentons cette connaissance sous la forme de règles de type "si-alors". Ces règles sont utilisées pour créer de nouveaux axes d'analyse en générant de nouveaux niveaux de granularité dans les hiérarchies de dimension. Notre approche est fondée sur un modèle formel d'entrepôts de données évolutif qui permet de gérer la mise à jour des hiérarchies de dimension.	Cécile Favre, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001379	http://editions-rnti.fr/render_pdf.php?p=1001379
Revue des Nouvelles Technologies de l'Information	EGC	2007	Interestingness in Data Mining	Interestingness measures play an important role in data mining regardless of the kind of patterns being mined. These measures are intended for selecting and ranking patterns according to their potential interest to the user. Good measures also allow the time and space cost of the mining process to be reduced. Measuring the interestingness of discovered patterns is an active and important area of data mining research. Although much work has been conducted in this area, so far there is no widespread agreement on a formal definition of interestingness in this context. Based on the diversity of definitions presented to date, interestingness is perhaps best treated as a broad concept, which emphasizes conciseness, coverage, reliability, peculiarity, diversity, novelty, surprisingness, utility, and actionability. This presentation reviews interestingness measures for rules and summaries, classifies them from several perspectives, compares their properties, identifies their roles in the data mining process, gives strategies for selecting appropriate measures for applications, and identifies opportunities for future research in this area.	Howard J. Hamilton	http://editions-rnti.fr/render_pdf.php?p1&p=1001283	http://editions-rnti.fr/render_pdf.php?p=1001283
Revue des Nouvelles Technologies de l'Information	EGC	2007	L'émergence de connaissances dans les communautés de pratique	Cet article est le résultat d'une recherche sur le processus, peu explicité dans la littérature, de création de connaissances dans les communautés de pratique. Nous commençons par établir une définition de travail pour ce concept de communauté de pratique qui permet l'échange et le partage de connaissances au sein de groupes de plus en plus virtuels. Nous analysons ensuite les communautés de pratique sous l'angle de la théorie de l'émergence. Nous proposons, alors, la modélisation d'un outil de support pour ces communautés qui améliore les échanges entre les membres et favorise l'émergence de nouvelles connaissances. Cet outil manipule les connaissances implicites ainsi qu'explicites et propose des possibilités pour la publication et la recherche d'informations. De plus, il s'adapte à chaque membre de la communauté par un processus de personnalisation.	Caroline Wintergerst, Thomas Ludwig, Danielle Boulanger	http://editions-rnti.fr/render_pdf.php?p1&p=1001442	http://editions-rnti.fr/render_pdf.php?p=1001442
Revue des Nouvelles Technologies de l'Information	EGC	2007	L'outil SDET pour le complètement des données descriptives liées aux bases de données géographiques	L'enrichissement des bases de données est un moyen visant à offrir un supplément informationnel aux utilisateurs. Dans le cas des données géographiques, cette activité représente de nos jours un problème crucial. Sa résolution permettrait de meilleures prises de décisions ne reposant pas uniquement sur les informations limitées. Notre outil SDET (Semantic Data Enrichment Tool) vient proposer une solution d'enrichissement faisant du Système d'Information Géographiques (SIG) initial une source riche d'informations.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001337	http://editions-rnti.fr/render_pdf.php?p=1001337
Revue des Nouvelles Technologies de l'Information	EGC	2007	Les itemsets essentiels fermés : une nouvelle représentation concise	Devant l'accroissement constant des grandes bases de données, plusieurs travaux de recherche en fouille de données s'orientent vers le développement de techniques de représentation compacte. Ces recherches se développent suivant deux axes complémentaires : l'extraction de bases génériques de règles d'association et l'extraction de représentations concises d'itemsets fréquents.Dans ce papier, nous introduisons une nouvelle représentation concise exacte des itemsets fréquents. Elle se situe au croisement de chemins de deux autres représentations concises, à savoir les itemsets fermés et ceux dits essentiels. L'idée intuitive est de profiter du fait que tout opérateur de fermeture induit une fonction surjective. Dans ce contexte, nous introduisons un nouvel opérateur de fermeture permettant de calculer les fermetures des itemsets essentiels. Ceci a pour but d'avoir une représentation concise de taille réduite tout en permettant l'extraction des supports négatif et disjonctif d'un itemset en plus de son support conjonctif. Un nouvel algorithme appelé D-CLOSURE permettant d'extraire les itemsets essentiels fermés est aussi présenté. L'étude expérimentale que nous avons menée a permis de confirmer que la nouvelle approche présente un bon taux de compacité comparativement aux autres représentations concises exactes.	Tarek Hamrouni, Islem Denden, Sadok Ben Yahia, Engelbert Mephu Nguifo, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001384	http://editions-rnti.fr/render_pdf.php?p=1001384
Revue des Nouvelles Technologies de l'Information	EGC	2007	Logiciel d'aide à l'évaluation des catégorisations	Les méthodes de classification automatique sont employées dans des domaines variés et de nombreux algorithmes ont été proposés dans la littérature. Au milieu de cette "jungle", il semble parfois difficile à un simple utilisateur de choisir quel algorithme est le plus adapté à ses besoins. Depuis le milieu des années 90, une nouvelle thématique de recherches, appelée clustering validity, tente de répondre à ce genre d'interrogation en proposant des indices pour juger de la qualité des catégorisations obtenues. Mais le choix est parfois difficile entre ces indices et il peut s'avérer délicat de prendre la bonne décision. C'est pourquoi nous proposons un logiciel adapté à cette problématique d'évaluation.	Julien Velcin, William Vacher, Jean-Gabriel Ganascia	http://editions-rnti.fr/render_pdf.php?p1&p=1001333	http://editions-rnti.fr/render_pdf.php?p=1001333
Revue des Nouvelles Technologies de l'Information	EGC	2007	Mesure d'entropie asymétrique et consistante	Les mesures d'entropie, dont la plus connue est celle de Shannon, ont été proposées dans un contexte de codage et de transmission d'information. Néanmoins, dès le milieu des années soixante, elles ont été utilisées dans d'autres domaines comme l'apprentissage et plus particulièrement pour construire des graphes d'induction et des arbres de décision. L'usage brut de ces mesures n'est cependant pas toujours bien approprié pour engendrer des modèles de prédiction ou d'explication pertinents. Cette faiblesse résulte des propriétés des entropies, en particulier le maximum nécessairement atteint pour la distribution uniforme et l'insensibilité à la taille de l'échantillon. Nous commençons par rappeler ces propriétés classiques. Nous définissons ensuite une nouvelle axiomatique mieux adaptée à nos besoins et proposons une mesure empirique d'entropie plus flexible vérifiant ces axiomes.	Djamel Abdelkader Zighed, Simon Marcellin, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1001309	http://editions-rnti.fr/render_pdf.php?p=1001309
Revue des Nouvelles Technologies de l'Information	EGC	2007	Mesure non symétrique pour l'évaluation de modèles, utilisation pour les jeux de données déséquilibrés	Les critères servant à l'évaluation de modèles d'apprentissage supervisé ainsi que ceux utilisés pour bâtir des arbres de décision sont, pour la plupart, symétriques. De manière pragmatique, cela signifie que chacune des modalités de la variable endogène se voit assigner une importance identique. Or, dans nombre de cas pratiques cela n'est pas le cas. Ainsi, on peut notamment prendre l'exemple de jeux de données fortement déséquilibrés pour lesquels l'objectif principal est l'identification des objets représentatifs de la modalité minoritaire (Aide au diagnostic, identification de phénomènes inhabituels : fraudes, pannes...). Dans ce type de situation il apparaît clairement qu'assigner une importance identique aux erreurs de prédiction ne constitue pas la meilleure des solutions. Nous proposons dans cet article un critère (pouvant servir à la fois pour l'évaluation de modèles d'apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision) prenant en compte cet aspect non symétrique de l'importance associée à chacune des modalités de la variable endogène. Nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés.	Julien Thomas, Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1001426	http://editions-rnti.fr/render_pdf.php?p=1001426
Revue des Nouvelles Technologies de l'Information	EGC	2007	Méthodes statistiques et modèles thermiques compacts	Dans le domaine thermique, la plupart des études reposent sur des modèles à éléments finis. Cependant, le coût en calcul et donc en temps de ces méthodes ont renforcé le besoin de modèles plus compacts. Le réseau RC équivalent est la solution la plus souvent utilisée. Toutefois, ses paramètres doivent souvent être ajustés à l'aide de mesures ou de simulation. Dans ce contexte d'identification de système, les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique.	Hubert Polaert, Philippe Leray, Grégory Mallet	http://editions-rnti.fr/render_pdf.php?p1&p=1001377	http://editions-rnti.fr/render_pdf.php?p=1001377
Revue des Nouvelles Technologies de l'Information	EGC	2007	Navigation et appariement d'objets géographiques dans une ontologie	L'ACI FoDoMuSt se propose d'élaborer un processus de fouille de données multi-stratégies pour la reconnaissance automatique d'objets géographiques sur des images satellitaires ou aériennes. Ces dernières sont segmentées afin d'isoler des polygones définis par un ensemble de descripteurs de bas niveaux. Afin de leur affecter une sémantique, on applique dans un premier temps une classification. Si aucun objet géographique n'est identifié, on tente alors un appariement du polygone avec les concepts d'une ontologie d'objets géographiques. Un algorithme de navigation dans l'ontologie et une mesure de comparaison sémantique ont ainsi été développés, paramétrables selon le contexte d'appariement. Cette mesure évalue la pertinence d'un appariement et comprend une composante locale (comparaison au niveau du concept) et une composante globale (combinaison linéaire de mesures locales). La méthode proposée a été développée en JAVA et intégrée à la plate-forme FoDoMuSt. Les premières expérimentations et évaluations humaines sont très encourageantes.	Rémy Brisson, Omar Boussaid, Pierre Gançarski, Anne Puissant, Nicolas Durand	http://editions-rnti.fr/render_pdf.php?p1&p=1001402	http://editions-rnti.fr/render_pdf.php?p=1001402
Revue des Nouvelles Technologies de l'Information	EGC	2007	Notion de conversation dans les communications interpersonnelles instantanées sur IP	Dans cet article nous étudions la contribution des techniques de fouille de données à l'amélioration des services de communications instantanées sur IP tel que la messagerie instantanée (IM) et la téléphonie sur IP (ToIP).	Alexandre Bouchacourt, Luigi Lancieri	http://editions-rnti.fr/render_pdf.php?p1&p=1001359	http://editions-rnti.fr/render_pdf.php?p=1001359
Revue des Nouvelles Technologies de l'Information	EGC	2007	OKM : une extension des k-moyennes pour la recherche de classes recouvrantes	Dans cet article nous abordons le problème de la classification (ou clustering) dans le but de découvrir des classes avec recouvrements. Malgré quelques avancées récentes dans ce domaines, motivées par des besoins applicatifs importants (traitements des données multimédia par exemple), nous constatons l'absence de solutions théoriques à ce problème. Notre étude consiste alors à proposer une nouvelle formulation du problème de classification par partitionnement, adaptée à la recherche d'un recouvrement des données en classes d'objets similaires. Cette approche se fonde sur la dénition d'un critère objectif de qualité d'un recouvrement et d'une solution algorithmique visant à optimiser ce critère. Nous proposons deux évaluations de ce travail permettant d'une part d'appréhender le fonctionnement global de l'algorithme sur des données simples (vitesse de convergence, visualisation des résultats) et d'autre part d'évaluer quantitativement le bénéfice d'une telle approche sur une application de classification de documents textuels.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001458	http://editions-rnti.fr/render_pdf.php?p=1001458
Revue des Nouvelles Technologies de l'Information	EGC	2007	Optimal histogram representation of large data sets: Fisher vs piecewise linear approximation	Histogram representation of a large set of data is a good way to summarize and visualize data and is frequently performed in order to optimize query estimation in DBMS. In this paper, we show the performance and the properties of two strategies for an optimal construction of histograms on a single real valued descriptor on the base of a prior choice of the number of buckets. The first one is based on the Fisher algorithm, while the second one is based on a geometrical procedure for the interpolation of the empirical distribution function by a piecewise linear function. The goodness of fit is computed using the Wasserstein metric between distributions. We compare the proposed method performances against some existing ones on artificial and real datasets.	Antonio Irpino, Elvira Romano	http://editions-rnti.fr/render_pdf.php?p1&p=1001314	http://editions-rnti.fr/render_pdf.php?p=1001314
Revue des Nouvelles Technologies de l'Information	EGC	2007	Partitionnement d'un réseau de sociabilité à fort coefficient de clustering	Afin de comparer l'organisation sociale d'une paysannerie médiévale avant et après la guerre de Cent Ans nous étudions la structure de réseaux sociaux construits à partir d'un corpus de contrats agraires. Faibles diamètres et fort clustering révèlent des graphes en petit monde. Comme beaucoup de grands réseaux d'interaction étudiés ces dernières années ces graphes sont sans échelle typique. Les distributions des degrés de leurs sommets sont bien ajustées par une loi de puissance tronquée par une coupure exponentielle. Ils possèdent en outre un club-huppé, c'est à dire un noyau dense et de faible diamètre regroupant les individus à forts degrés. La forme particulière des éléments propres du laplacien permet d'extraire des communautés qui se répartissent en étoile autour du club huppé.	Romain Boulet, Bertrand Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1001438	http://editions-rnti.fr/render_pdf.php?p=1001438
Revue des Nouvelles Technologies de l'Information	EGC	2007	Peut-on capturer la sémantique à travers la syntaxe ? - découverte des règles d'exception simultanée	L'objectif de la fouille de données est la découverte sophistiquée de connaissances lisibles, surprenantes et possiblement utiles. Les aspects surprenant et utile font partie de la sémantique et nécessitent l'utilisation des connaissances du domaine, ce qui cause souvent le problème d'acquisition de la connaissance. Notre découverte des règles d'exception simultanée peut être une réponse à ce problème. Nous envisageons de trouver les connaissances surprenantes et possiblement utiles à travers notre forme de paire de règles d'exception. Les autres méthodes inventées concernent l'index d'évaluation et la recherche exhaustive. Plusieurs applications médicales seront présentées sur lesquelles nos propositions ont été appliquées.	Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001280	http://editions-rnti.fr/render_pdf.php?p=1001280
Revue des Nouvelles Technologies de l'Information	EGC	2007	Préservation de l'Intimité dans les Protocoles de Conversations	Le travail présenté dans cet article, rentre dans le cadre de la gestion des données privées en vue de la substitution, appelée remplaçabilité, dynamique des services Web. Trois contributions sont apportées, (1) modélisation des politiques privées spécifiant les règles d'utilisation des données privées, prenant en compte des aspects se rapportant aux services Web, (2) étendre les protocoles de conversations des services Web par le modèle proposé, afin d'apporter les primitives nécessaires pour l'analyse des protocoles en présence de ces règles, (3) définition d'un mécanisme d'analyse de la remplaçabilité d'un service par un autre en vue de ses politiques privées.	Nawal Guermouche, Salima Benbernou, Emmanuel Coquery, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001360	http://editions-rnti.fr/render_pdf.php?p=1001360
Revue des Nouvelles Technologies de l'Information	EGC	2007	RAS : Un outil pour l'annotation de documents basée sur les liens de citation	RAS (Reference Annotation System) est un outil d'annotation de documents. Cet outil est le résultat de l'implémentation de notre approche d'annotation basée sur le contexte de citation. L'approche est indépendante du contenu et utilise un regroupement thématique des références construit à partir d'une classification floue non-supervisée. L'outil présenté dans cet article a été expérimentée et évaluée avec la base de documents scientifiques Citeseer.	Lylia Abrouk, Danièle Hérin	http://editions-rnti.fr/render_pdf.php?p1&p=1001341	http://editions-rnti.fr/render_pdf.php?p=1001341
Revue des Nouvelles Technologies de l'Information	EGC	2007	Ré-ordonnancement pour l'apprentissage de transformations de documents HTML	Notre objectif est de transformer les documents Web vers un schéma médiateur XML défini a priori. C'est une étape nécessaire pour de nombreuses tâches de recherche d'information concernant le Web Sémantique, les documents semi-structurés, le traitement de sources hétérogènes, etc. Elle permet d'associer une structure sémantiquement riche à des documents dont le formats ne contient que des informations de présentation. Nous proposons de traiter ce problème comme un problème d'apprentissage structuré en le formalisant comme une transformation d'arbre en arbre.Notre méthode de transformation comporte deux étapes. Dans une première étape, une grammaire hors-contexte probabiliste permet de générer un ensemble de solutions candidates. Dans une deuxième étape, ces solutions candidates sont ordonnées grâce à un algorithme de ré-ordonnancement à base de perceptron à noyau. Cette étape d'ordonnancement nous permet d'utiliser de manière efficace des caractéristiques complexes définies à partir du document d'entrée et de la solution candidate.	Guillaume Wisniewski, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1001461	http://editions-rnti.fr/render_pdf.php?p=1001461
Revue des Nouvelles Technologies de l'Information	EGC	2007	Réduction de dimension pour l'analyse de données vidéo	Les données vidéo ont la particularité d'être très volumineuses alors qu'elles contiennent peu d'information sémantique. Pour les analyser, il faut réduire la quantité d'information dans l'espace de recherche. Les données vidéo sont souvent considérées comme l'ensemble des pixels d'une succession d'images analysées séquentiellement. Dans cet article, nous proposons d'utiliser une analyse en composantes principales (ACP) pour réduire la dimensionnalité des informations sans perdre la nature tridimensionnelle des données initiales. Nous commençons par considérer des sous-séquences, dont le nombre de trames est le nombre de dimensions dans l'espace de représentation. Nous appliquons une ACP pour obtenir un espace de faible dimension où les points similaires sémantiquement sont proches. La sous-séquence est ensuite divisée en blocs tridimensionnels dont on projette l'ellipsoïde d'inertie dans le premier plan factoriel. Nous déduisons enfin le mouvement présent dans les blocs à partir des ellipses ainsi obtenues. Nous présenterons les résultats obtenus pour un problème de vidéosurveillance.	Nicolas Verbeke, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001404	http://editions-rnti.fr/render_pdf.php?p=1001404
Revue des Nouvelles Technologies de l'Information	EGC	2007	Régression floue et crédibiliste par SVM pour la classification des images sonar	La classification des images sonar est d'une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins. En effet, le sonar offre des capacités d'imagerie plus performantes que les capteurs optiques en milieu sous-marin. La classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu. De nombreuses approches ont été proposées sans donner de bons résultats, celles-ci ne tenant pas compte des imperfections des données. Pour modéliser ce type de données, il est judicieux d'utiliser les théories de l'incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance. Les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation. Il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de l'algorithme de classification. L'approche de la régression par SVM que nous avons introduite permet cette modélisation des imperfections. Nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes, dans le cadre de la classification des images sonar.	Hicham Laanaya, Arnaud Martin, Driss Aboutajdine, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001295	http://editions-rnti.fr/render_pdf.php?p=1001295
Revue des Nouvelles Technologies de l'Information	EGC	2007	Segmentation thématique par calcul de distance thématique	Dans cet article, nous présentons une approche de la segmentation thématique fondée sur une représentation en vecteurs sémantiques des phrases et des calculs de distance entre ces vecteurs. Les vecteurs sémantiques sont générés par le système SYGFRAN, un analyseur morpho-syntaxique et conceptuel de la langue française. La segmentation thématique s'effectue elle en recherchant des zones de transition au sein du texte grâce aux vecteurs sémantiques. L'évaluation de cette méthode s'est faite sur les données du défi DEFT'06.	Jacques Chauché, Alexandre Labadié	http://editions-rnti.fr/render_pdf.php?p1&p=1001398	http://editions-rnti.fr/render_pdf.php?p=1001398
Revue des Nouvelles Technologies de l'Information	EGC	2007	Sémantique et contextes conceptuels pour la recherche d'information	Cet article propose une méthodologie de recherche d'information qui utilise l'analyse conceptuelle conjointement avec la sémantique dans le but de fournir des réponses contextuelles à des requêtes sur le web. Le contexte conceptuel défini dans cet article peut être global - c'est-à-dire stable - ou instantané - c'est-à-dire borné par le contexte global. Notre méthodologie consiste en une première phase de pré traitement permettant de construire le contexte global, et une seconde phase de traitement en ligne des requêtes des utilisateurs, associées au contexte instantané. Notre processus de recherche d'information est illustré à travers une expérimentation dans le domaine du tourisme.	Michel Soto, Bénédicte Le Grand, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001440	http://editions-rnti.fr/render_pdf.php?p=1001440
Revue des Nouvelles Technologies de l'Information	EGC	2007	Sous-bases k-faibles pour des règles d'association valides au sens de la confiance	Nous introduisons la notion de sous-base k-faible pour les règles d'association valides au sens de la confiance. Ces sous-bases k-faibles sont caractérisées en termes d'opérateurs de fermeture correspondant à des familles de Moore k-faiblement hiérarchiques.	Jean Diatta, Régis Girard	http://editions-rnti.fr/render_pdf.php?p1&p=1001383	http://editions-rnti.fr/render_pdf.php?p=1001383
Revue des Nouvelles Technologies de l'Information	EGC	2007	SPoID : Extraction de motifs séquentiels pour les bases de données incomplètes	Les bases de données issues du monde réel contiennent souvent de nombreuses informations non renseignées. Durant le processus d'extraction de connaissances dans les bases de données, une phase de traitement spécifique de ces données est souvent nécessaire, permettant de les supprimer ou de les compléter. Lors de l'extraction de séquences fréquentes, ces données incomplètes sont la plupart du temps occultées. Ceci conduit parfois à l'élimination de plus de la moitié de la base et l'information extraite n'est plus représentative. Nous proposons donc de ne plus éliminer les enregistrements incomplets, mais d'utiliser l'information partielle qu'ils contiennent. La méthode proposée ignore en fait temporairement certaines données incomplètes pour les séquences recherchées. Les expérimentations sur jeux de données synthétiques montrent la validité de notre proposition aussi bien en terme de qualité des motifs extraits que de robustesse aux valeurs manquantes.	Céline Fiot, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001460	http://editions-rnti.fr/render_pdf.php?p=1001460
Revue des Nouvelles Technologies de l'Information	EGC	2007	SyRQuS - Recherche par combinaison de graphes RDF	Nous nous intéressons à un mécanisme permettant la construction de réponses combinés à partir de plusieurs graphes RDF. Nous imposons, par souci de cohérence, que cette combinaison soit réalisée uniquement si les graphes RDF ne se contredisent pas. Pour déterminer la non-contradiction entre deux graphes RDF nous utilisons une mesure de similarité, calculée au moment de l'ajout de documents RDF dans la base de documents.	Adrian Tanasescu	http://editions-rnti.fr/render_pdf.php?p1&p=1001347	http://editions-rnti.fr/render_pdf.php?p=1001347
Revue des Nouvelles Technologies de l'Information	EGC	2007	Traitement de données de consommation électrique par un Système de Gestion de Flux de Données	Avec le développement de compteurs communicants, les consommations d'énergie électrique pourront à terme être télérelevées par les fournisseurs d'électricité à des pas de temps pouvant aller jusqu'à la seconde. Ceci générera des informations en continu, à un rythme rapide et en quantité importante. Les Systèmes de Gestion de Flux de Données (SGFD), aujourd'hui disponibles sous forme de prototypes, ont vocation à faciliter la gestion de tels flux. Cette communication décrit une étude expérimentale pour analyser les avantages et limites de l'utilisation de deux prototypes de SGFD (STREAM et TelegraphCQ) pour la gestion de données de consommation électrique.	Talel Abdessalem, Raja Chiky, Georges Hébrail, Jean Louis Vitti	http://editions-rnti.fr/render_pdf.php?p1&p=1001432	http://editions-rnti.fr/render_pdf.php?p=1001432
Revue des Nouvelles Technologies de l'Information	EGC	2007	Traitement et exploration du fichier Log du Serveur Web pour l'extraction des connaissances : Web Usage Mining	Le but dans ce travail consiste à concevoir et réaliser un Outil Logiciel, en utilisant les concepts du Web Usage Mining pour offrir aux web masters l'ensemble des connaissances, y inclut les statistiques sur leurs sites, afin de prendre les décisions adéquates. Il s'agit en fait, d'extraire de l'information à partir du fichier log du serveur Web, hébergeant le site Web, et de prendre les décisions pour découvrir les habitudes des internautes, et de répondre à leurs besoins en adaptant le contenu, la forme et l'agencement des pages web.	Mostafa Hanoune, Faouzia Benabbou	http://editions-rnti.fr/render_pdf.php?p1&p=1001344	http://editions-rnti.fr/render_pdf.php?p=1001344
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un algorithme multi-agent de classification pour la construction d'ontologies dynamiques	La construction d'ontologies à partir de textes reste une tâche coûteuse en temps qui justifie l'émergence de l'Ontology Learning. Notre système, Dynamo, s'inscrit dans cette mouvance, en apportant une approche originale basée sur une architecture multi-agent adaptative. En particulier, l'article présente le coeur de notre approche, un algorithme distribué de classification hiérarchique qui s'applique sur les résultats d'un analyseur syntaxique. Cet algorithme est évalué et comparé à un algorithme centralisé plus conventionnel. Forts de ces résultats, nous discutons ses limites et dressons en perspective les aménagements à effectuer pour aller vers une solution complète de construction d'ontologies.	Kévin Ottens, Nathalie Aussenac-Gilles	http://editions-rnti.fr/render_pdf.php?p1&p=1001452	http://editions-rnti.fr/render_pdf.php?p=1001452
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un cadre théorique pour la gestion de grandes bases de motifs	Les algorithmes de fouille de données sont maintenant capables de traiter de grands volumes de données mais les utilisateurs sont souvent submergés par la quantité de motifs générés. En outre, dans certains cas, que ce soit pour des raisons de confidentialité ou de coûts, les utilisateurs peuvent ne pas avoir accès directement aux données et ne disposer que des motifs. Les utilisateurs n'ont plus alors la possibilité d'approfondir à partir des données initiales le processus de fouille de façon à extraire des motifs plus spécifiques. Pour remédier à cette situation, une solution consiste à gérer les motifs. Ainsi, dans cet article, nous présentons un cadre théorique permettant à un utilisateur de manipuler, en post-traitement, une collection de motifs préalablement extraite. Nous proposons de représenter la collection sous la forme d'un graphe qu'un utilisateur pourra ensuite exploiter à l'aide d'opérateurs algébriques pour y retrouver des motifs ou en chercher de nouveaux.	François Jacquenet, Baptiste Jeudy, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001385	http://editions-rnti.fr/render_pdf.php?p=1001385
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un outil pour la visualisation de relations entre gènes	La reconstruction de réseaux de gènes est un des défis majeurs de la post-génomique. A partir de données d'expression issues de puces à ADN, différentes techniques existent pour inférer des réseaux de gènes. Nous proposons dans ce papier une approche pour la visualisation de réseaux d'interactions entre gènes à partir de données d'expression. L'originalité de notre approche est de superposer des règles avec des sémantiques différentes au sein d'un même support visuel et de ne générer que les règles qui impliquent des gènes dits centraux. Ceux-ci sont spécifiés en amont par les experts et permettent de limiter la génération des règles aux seuls gènes qui intéressent les spécialistes. Une implémentation a été réalisée dans le logiciel libre MeV de l'institut TIGR.	Jean-Marc Petit, Marie Agier	http://editions-rnti.fr/render_pdf.php?p1&p=1001342	http://editions-rnti.fr/render_pdf.php?p=1001342
Revue des Nouvelles Technologies de l'Information	EGC	2007	Un segmenteur de texte en phrases guidé par l'utilisateur	Ce programme effectue une segmentation en phrases d'un texte. Contrairement aux procédures classiques, nous n'utilisons pas d'annotations préliminaires et tirons parti d'un apprentissage guidé par l'utilisateur.	Thomas Heitz	http://editions-rnti.fr/render_pdf.php?p1&p=1001334	http://editions-rnti.fr/render_pdf.php?p=1001334
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche de classification non supervisée basée sur la détection de singularités et la corrélation de séries temporelles pour la recherche d'états : application à un bioprocédé fed-batch	Nous proposons dans cet article une méthode de clustering qui combine l'analyse dynamique et l'analyse statistique pour caractériser des états. Il s'agit d'une méthode de fouille de données qui travaille sur des ensembles de séries temporelles pour détecter des états; ces états représentent les informations les plus significatives du système. L'objectif de cette méthode non supervisée est d'extraire de la connaissance à partir de l'analyse des séries temporelles multiples. Elle s'appuie sur la détection de singularités dans les séries temporelles et sur l'analyse des corrélations des séries entre les intervalles définis par ces singularités. Pour l'application présentée, les séries temporelles sont des signaux biochimiques mesurés durant un bioprocédé. Cette approche est donc utilisée pour confirmer et enrichir la connaissance des experts du domaine des bioprocédés sans utiliser la connaissance a priori de ces experts. Elle est appliquée à la recherche d'états physiologiques dans un bioprocédé de type fed-batch.	Sébastien Régis	http://editions-rnti.fr/render_pdf.php?p1&p=1001453	http://editions-rnti.fr/render_pdf.php?p=1001453
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche non paramétrique Bayésienne pour l'estimation de densité conditionnelle sur les rangs	Nous nous intéressons à l'estimation de la distribution des rangs d'une variable cible numérique conditionnellement à un ensemble de prédicteurs numériques. Pour cela, nous proposons une nouvelle approche non paramétrique Bayesienne pour effectuer une partition rectangulaire optimale de chaque couple (cible, prédicteur) uniquement à partir des rangs des individus. Nous montrons ensuite comment les effectifs de ces grilles nous permettent de construire un estimateur univarié de la densité conditionnelle sur les rangs et un estimateur multivarié utilisant l'hypothèse Bayesienne naïve. Ces estimateurs sont comparés aux meilleures méthodes évaluées lors d'un récent Challenge sur l'estimation d'une densité prédictive. Si l'estimateur Bayésien naïf utilisant l'ensemble des prédicteurs se révèle peu performant, l'estimateur univarié et l'estimateur combinant deux prédicteurs donne de très bons résultats malgré leur simplicité.	Marc Boullé, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1001317	http://editions-rnti.fr/render_pdf.php?p=1001317
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche sociotechnique pour le Knowledge Management (KM)	Cet article présente un cadre sociotechnique pour le KM. Cette vision sociotechnique du KM permet : (1) d'écarter le KM d'un souci commercial ; (2) faire le clivage des différentes technologies du KM ; et (3) de s'interroger sur les paradigmes associés aux composants social et technique du KM. C'est précisément ce dernier point que cet article développe afin d'identifier les mécanismes génériques du KM. Plus précisément, l'aspect social est décrit à travers l'approche organisationnelle du KM, l'approche managériale du KM, et l'approche biologique du KM, alors que l'aspect technique est décrit à travers l'approche ingénierie des connaissances et compétences du KM. Ces approches nous conduisent aussi à donner un tableau comparatif entre ces visions organisationnelles, managériales et biologiques du KM.	Leoncio Jiménez	http://editions-rnti.fr/render_pdf.php?p1&p=1001436	http://editions-rnti.fr/render_pdf.php?p=1001436
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une étude des algorithmes de construction d'architecture des réseaux de neurones multicouches	Le problème de choix d'architecture d'un réseau de neurones multicouches reste toujours très difficile à résoudre dans un processus de fouille de données. Ce papier recense quelques algorithmes de recherche d'architectures d'un réseau de neurones pour les tâches de classification. Il présente également une analyse théorique et expérimentale de ces algorithmes. Ce travail confirme les difficultés de choix des paramètres d'apprentissage (modèle, nombre de couches, nombre de neurones par couches, taux d'apprentissage, algorithme d'apprentissage,...) communs à tout processus de construction de réseaux de neurones et les difficultés de choix de paramètres propres à certains algorithmes.	Norbert Tsopzé, Engelbert Mephu Nguifo, Gilbert Tindo	http://editions-rnti.fr/render_pdf.php?p1&p=1001288	http://editions-rnti.fr/render_pdf.php?p=1001288
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une extension de XQuery pour la recherche textuelle d'information dans des documents XML	Nous présentons dans cet article une extension de XQuery que nous avons développée pour interroger le contenu et la structure de documents XML. Cette extension consiste à intégrer dans XQuery le langage NEXI, un sous-ensemble de XPath, défini dans le cadre de l'initiative INEX. Notre proposition est double : (i) équiper NEXI d'une sémantique floue, (ii) intégrer NEXI dans XQuery au moyen d'une métafonction appelée nexi, ayant une requête NEXI comme paramètre, et d'une extension de la clause for de l'opérateur FLWOR de XQuery. De plus, nous décrivons le prototype paramétrable que nous avons développé au dessus de deux moteurs XQuery classiques : Galax et Saxon.	Jacques Le Maitre, Nicolas Faessel	http://editions-rnti.fr/render_pdf.php?p1&p=1001401	http://editions-rnti.fr/render_pdf.php?p=1001401
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une méthode d'interprétation de scores	Cet article présente une méthode permettant d'interpréter la sortie d'un modèle de classification ou de régression. L'interprétation se base sur l'importance de la variable et l'importance de la valeur de la variable. Cette approche permet d'interpréter la sortie du modèle pour chaque instance.	Raphaël Feraud, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001348	http://editions-rnti.fr/render_pdf.php?p=1001348
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une méthode optimale d'évaluation bivariée pour la classification supervisée	En préparation des données pour la classification supervisée, les méthodes filtres usuellement utilisées pour la sélection de variables sont efficaces en temps de calcul. Néanmoins, leur nature univariée ne permet pas de détecter les redondances ou les interactions constructives entre variables. Cet article présente une nouvelle méthode permettant d'évaluer l'importance prédictive jointe d'une paire de variables de façon automatique, rapide et fiable. Elle est basée sur un partitionnement de chaque variable exogène, en intervalles dans le cas numérique et groupes de valeurs dans le cas catégoriel. La grille de données exogène résultante permet alors d'évaluer la corrélation entre la paire de variables exogènes et la variable endogène. Le meilleur partitionnement bivarié est recherché au moyen d'une approche Bayésienne de la sélection de modèle. Les expérimentations démontrent les apports de la méthode, notamment une amélioration significative des performances en classification.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001420	http://editions-rnti.fr/render_pdf.php?p=1001420
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une nouvelle approche de la programmation DC et DCA pour la classification floue	Dans cet article, nous nous intéressons à Fuzzy C-Means (FCM), une technique très connue pour la classification floue. Nous proposons un algorithme efficace basé sur la programmation DC (Difference of Convexe functions) et DCA (DC Algorithm) pour résoudre ce problème. Les expériences numériques comparatives avec l'algorithme standard FCM sur les données réelles montrent la robustesse, la performance de cet nouvel algorithme DCA et sa supériorité par rapport à FCM.	Le Thi Hoai An, Le Hoai Minh, Pham Dinh Tao	http://editions-rnti.fr/render_pdf.php?p1&p=1001459	http://editions-rnti.fr/render_pdf.php?p=1001459
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une nouvelle méthode d'alignement et de visualisation d'ontologies OWL-Lite	Dans ce papier, une nouvelle plate-forme d'alignement et de visualisation des ontologies, appelée POVA (Prototype OWL-Lite Visual Alignment), est décrite. Le module d'alignement implémente une nouvelle approche d'alignement d'ontologies remédiant au problème de la circularité et de l'intervention de l'utilisateur.	Sami Zghal, Karim Kamoun, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001355	http://editions-rnti.fr/render_pdf.php?p=1001355
Revue des Nouvelles Technologies de l'Information	EGC	2007	Une règle d'exception en Analyse Statistique Implicative	En fouille de règles, certaines situations exceptionnelles défient le bon sens. C'est le cas de la règle R : a --> c et b --> c et (a et b) --> non c. Une telle règle, que nous étudions dans l'article, est appelée règle d'exception. A la suite des travaux précurseurs de E. Suzuki et Y. Kodratoff (1999), qui ont étudié un autre type de règle d'exception, nous cherchons ici à caractériser les conditions d'apparition de la règle R dans le cadre de l'Analyse Statistique Implicative.	Régis Gras, Pascale Kuntz, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001312	http://editions-rnti.fr/render_pdf.php?p=1001312
Revue des Nouvelles Technologies de l'Information	EGC	2007	Utilisation de WordNet dans la catégorisation de textes multilingues	Cet article est consacré au problème de la catégorisation multilingue qui consiste à catégoriser des documents de différentes langues en utilisant le même classifieur. L'approche que nous proposons est basée sur l'idée d'étendre l'utilisation de WordNet dans la catégorisation monolingue vers la catégorisation multilingue.	Mohamed Amine Bentaallah, Mimoun Malki	http://editions-rnti.fr/render_pdf.php?p1&p=1001353	http://editions-rnti.fr/render_pdf.php?p=1001353
Revue des Nouvelles Technologies de l'Information	EGC	2007	Validation des visualisations par axes principaux de données numériques et textuelles	Parmi les outils de visualisation de données multidimensionnelles figurent d'une part les méthodes fondées sur la décomposition aux valeurs singulières, et d'autre part les méthodes de classification, incluant les cartes auto-organisées de Kohonen. Comment valider ces visualisations ? On présente sept procédures de validation par bootstrap qui dépendent des données, des hypothèses, des outils : a) le bootstrap partiel, qui considère les réplications comme des variables supplémentaires; b) le bootstrap total de type 1, qui réanalyse les réplications avec changements éventuels de signes des axes; c) le bootstrap total de type 2 qui corrige aussi les interversions d'axes; d) le bootstrap total de type 3, sur lequel on insistera, qui corrige les réplications par rotations procrustéenne; e) le bootstrap spécifique (cas des hiérarchies d'individus statistiques et des données textuelles). f) le bootstrap sur variables. g) les extensions des procédures précédentes à certaines cartes auto-organisées.	Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001332	http://editions-rnti.fr/render_pdf.php?p=1001332
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers un algorithme multi-agents de clustering dynamique	Dans cet article, nous présentons un algorithme multi-agents de clustering dynamique. Ce type de clustering doit permettre de gérer des données évolutives et donc être capable d'adapter en permanence les clusters construits.	Bruno Mermet, Gaële Simon, Dominique Fournier	http://editions-rnti.fr/render_pdf.php?p1&p=1001357	http://editions-rnti.fr/render_pdf.php?p=1001357
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers un système hybride pour l'annotation sémantique d'images IRM du cerveau	Cet article montre l'intérêt de combiner des méthodes numériques et symboliques pour obtenir une annotation sémantique des images IRM du cerveau humain. Il s'agit d'identifier des structures anatomiques du cortex cérébral humain, en utilisant conjointement des connaissances a priori de nature numérique et une ontologie des structures corticales du cerveau représentée en OWL DL, étendue par des règles SWRL. Ces connaissances symboliques a priori représentées dans des langages standards du Web deviennent non seulement partageables mais permettent aussi un raisonnement automatique qui aide l'utilisateur à la labellisation des structures anatomiques mises en évidence dans des images IRM du cerveau d'un individu donné.	Ammar Mechouche, Christine Golbreich, Bernard Gibaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001417	http://editions-rnti.fr/render_pdf.php?p=1001417
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une base de connaissances biographique : extraction d'information et ontologie	Le projet B-Ontology a pour but l'extraction, l'organisation et l'exploitation de connaissances biographiques à partir de dépêches de presse. Sa réalisation requiert l'intégration de diverses technologies, principalement l'extraction d'information, les ontologies et bases de connaissances, les techniques de data mining. Cet article propose un aperçu des choix réalisés dans le cadre du projet. Cette démarche permet également de définir un environnement d'outils utiles pour les applications d'extraction et de gestion de connaissances.	Laurent Kevers, Cédrick Fairon	http://editions-rnti.fr/render_pdf.php?p1&p=1001400	http://editions-rnti.fr/render_pdf.php?p=1001400
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une nouvelle approche d'extraction des motifs séquentiels non-dérivables	L'extraction de motifs séquentiels est un défi important pour la communauté fouille de données. Même si les représentations condensées ont montré leur intérêt dans le domaine des itemsets, à l'heure actuelle peu de travaux considèrent ce type de représentation pour extraire des motifs. Cet article propose d'établir les premières bases formelles pour obtenir les bornes inférieures et supérieures du support d'une séquence S. Nous démontrons que ces bornes peuvent être dérivées à partir des sous-séquences de S et prouvons que ces règles de dérivation permettent la construction d'une nouvelle représentation condensée de l'ensemble des motifs fréquents. Les différentes expérimentations menées montrent que notre approche offre une meilleure représentation condensée que celles des motifs clos et cela sans perte d'information.	Pascal Poncelet, Chedy Raïssi	http://editions-rnti.fr/render_pdf.php?p1&p=1001391	http://editions-rnti.fr/render_pdf.php?p=1001391
Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une plate-forme interactive pour la visualisation de grands ensembles de règles d'association	La recherche de règles d'association est une question centrale en Extraction de Connaissances dans les Données (ECD). Dans cet article, nous nous intéressons plus particulièrement à la restitution visuelle de règles pertinentes dans un corpus très important. Nous proposons ainsi un prototype basé sur une approche de type "wrapper" par intégration des phases d'extraction et de visualisation de l'ECD. Tout d'abord, le processus d'extraction génère une base générique de règles et dans un second temps, la tâche de visualisation s'appuie sur un processus de regroupement ("clustering") permettant de grouper et de visualiser un sous-ensemble de règles d'association génériques. Le rendu visuel à l'écran exploite une représentation de type "Fisheye view" de manière à obtenir simultanément une représentation globale des différents groupes de règles et une vue détaillée du groupe sélectionné.	Olivier Couturier, Tarek Hamrouni, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001381	http://editions-rnti.fr/render_pdf.php?p=1001381
Revue des Nouvelles Technologies de l'Information	EGC	2007	Visualisation de graphes avec Tulip : exploration interactive de grandes masses de données en appui à la fouille de données et à l'extraction de connaissances	Cet article décrit une étude de cas exhibant les qualités de la plateforme de visualisation de graphes Tulip, démontrant l'apport de la visualisation à la fouille de données interactive et à l'extraction de connaissances. Le calcul d'un graphe à partir d'indices de similarité est un exemple typique où l'exploration visuelle et interactive de graphes vient en appui au travail de fouille de données. Nous penchons sur le cas où l'on souhaite étudier une collection de documents afin d'avoir une idée des thématiques abordées dans la collection.	David Auber, Yves Chiricota, Maylis Delest, Jean-Philippe Domenger, Patrick Mary, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001330	http://editions-rnti.fr/render_pdf.php?p=1001330
Revue des Nouvelles Technologies de l'Information	EGC	2007	Visualisation exploratoire des résultats d'algorithmes d'arbre de décision	Nous présentons une méthode d'exploration des résultats des algorithmes d'apprentissage par arbre de décision (comme C4.5). La méthode présentée utilise simultanément une visualisation radiale, focus+context, fisheye et hiérarchique pour la représentation et l'exploration des résultats des algorithmes d'arbre de décision. L'utilisateur peut ainsi extraire facilement des règles d'induction et élaguer l'arbre obtenu dans une phase de post-traitement. Cela lui permet d'avoir une meilleure compréhension des résultats obtenus. Les résultats des tests numériques avec des ensembles de données réelles montrent que la méthode proposée permet une bien meilleure compréhension des résultats des arbres de décision.	Thanh-Nghi Do, Nguyen-Khang Pham, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001331	http://editions-rnti.fr/render_pdf.php?p=1001331
Revue des Nouvelles Technologies de l'Information	EGC	2007	WebDocEnrich : enrichissement sémantique flexible de documents semi-structurés	WebdocEnrich est une approche d'enrichissement sémantique automatique de documents HTML hétérogènes qui exploite une description du domaine pour enrichir le contenu des documents et les représenter en XML.	Mouhamadou Thiam, Nacéra Bennacer, Nathalie Pernelle	http://editions-rnti.fr/render_pdf.php?p1&p=1001365	http://editions-rnti.fr/render_pdf.php?p=1001365
Revue des Nouvelles Technologies de l'Information	FW	2007	Annotation sémantique de documents basée sur la relation de citation	L'annotation sémantique est le coeur du principe du Web sémantique.Effectivement, l'annotation des ressources par des métadonnées rend possiblel'exploitation des ressources par les utilisateurs et les agents logiciels, mêmeen étant dans un contexte distribué et de taille importante comme le Web. Cependant,l'annotation ne devrait pas être considérée comme garantie, particulièrementavec le nombre croissant des ressources où il est quasiment impossibled'effectuer une annotation manuelle. Nous proposons dans cet article, une approchesemi-automatique d'annotation de ressources basée sur le contexte decitation et la propagation des annotations entre les ressources. L'approche présentéedans cet article a été expérimentée et évaluée sur la base de documentsscientifiques Citeseer. L'évaluation montre l'avantage d'utiliser, non seulementle contenu des ressources pour l'annotation, mais également les liens de citationcomme contexte pour décrire les thèmes des ressources.	Lylia Abrouk, Abdelkader Gouaïch, Danièle Hérin	http://editions-rnti.fr/render_pdf.php?p1&p=1000468	http://editions-rnti.fr/render_pdf.php?p=1000468
Revue des Nouvelles Technologies de l'Information	FW	2007	Approche connexionniste pour l'analyse des données issues d'usage d'Internet : Classification et Visualisation	Dans ce papier, nous présentons une chaîne complète de fouille dedonnées comportementales issues des navigations des clients de sites webcommerciaux. Nous présentons plus particulièrement, le développement d'uneapproche d'apprentissage non supervisé pour ce type de données stockées sousforme de traces de navigation dans des fichiers Log.La première partie de cette étude concerne le problème du codage des donnéesqui seront ensuite utilisées pour l'analyse. En effet, actuellement, les sites websont dynamiques et les pages ne peuvent pas être caractérisées par des variablesfixes comme : la hiérarchie des adresses URL, le contenu, etc. Ellessont représentées par des identificateurs numériques n'ayant aucun « sens » etqui servent comme adresses de récupération des informations dans des basesde données pour remplir les contenus des pages. Pour cette raison, nous proposonsune nouvelle méthode de codage de sessions à partir du fichier Log.Cette technique consiste à caractériser une page donnée par un vecteur depoids d'importance de passage, i.e. par ses poids de précédence et de successionsrelatives à toutes les autres pages qui apparaissent dans le fichier Log.Dans la deuxième partie de ce travail, nous analysons les propriétés des cartestopologiques de Kohonen et nous proposons une version adaptée aux donnéescomportementales. Cette étape nous permet (1) de construire une cartographiedu site web tel qu'il est aperçu par les clients (2) de regrouper les pages pourun objectif de codage de sessions (3) et de projeter les interactions des clientsdu fichier Log sur la cartographie sous forme de trajectoires symbolisant leurscomportements.	Khalid Benabdeslem, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000469	http://editions-rnti.fr/render_pdf.php?p=1000469
Revue des Nouvelles Technologies de l'Information	FW	2007	Diverses approches permettant l'introduction du temps dans la fouille de données d'usage du Web	Le volume des transactions commerciales réalisées en ligne sur l'Internetne cesse de croître. LeWeb est donc devenu l'une des plates-formes commercialesles plus importantes et beaucoup d'opérateurs de sitesWeb sont incitésà analyser l'utilisation de leurs sites afin d'améliorer leur rentabilité. Cependant,la manière dont un site Web est visité peut changer en raison de modificationsliées à la structure et au contenu du site lui-même, ou bien en raison de l'évolutiondu comportement de certains groupes d'utilisateurs. Ainsi, les modèlesd'usage doivent être mis à jour continuellement afin de refléter le comportementréel des visiteurs. Dans ce contexte, nous présentons une brève vue del'ensemble des techniques appliquées à la fouille des données temporelles. Uneattention particulière est apportée aux méthodes consacrées à la découverte demodèles sur les données d'usage du Web. Nous évoquons également certainesquestions en suspens dans ce contexte.	Alzennyr Da Silva	http://editions-rnti.fr/render_pdf.php?p1&p=1000470	http://editions-rnti.fr/render_pdf.php?p=1000470
Revue des Nouvelles Technologies de l'Information	FW	2007	Techniques structurelles d'alignement pour portails Web	Le travail décrit dans cet article a pour objectif d'unifier l'accès auxdocuments d'un domaine d'application. Il permet en particulier d'augmenter lenombre de documents accessibles à partir de portails Web sans en modifierl'interface d'interrogation. L'accès aux documents est supposé s'appuyer surdes taxonomies que nous proposons d'aligner. Cet article porte spécifiquementsur les techniques structurelles mises en oeuvre, qui sont originales etparticulières dans la mesure où elles sont adaptées au traitement de taxonomiesdont les structures sont hétérogènes et dissymétriques. Nous présentons etanalysons ensuite les résultats de trois expérimentations effectuées sur diversestaxonomies, des taxonomies réelles qui ont motivé notre approche ainsi quedes taxonomies tests mises à disposition des chercheurs de la communauté.	Chantal Reynaud, Brigitte Safar	http://editions-rnti.fr/render_pdf.php?p1&p=1000471	http://editions-rnti.fr/render_pdf.php?p=1000471
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	A C-space sensitivity analysis of Earliest Deadline First scheduling	This paper presents a sensitivity analysis for the dimensioning ofreal-time systems in which sporadic tasks are executed according to the preemptiveEarliest Deadline First (EDF) scheduling policy. The timeliness constraintsof the tasks are expressed in terms of late termination deadlines. A general caseis considered, where the task deadlines are independent of the task sporadicityintervals (also called periods). New results for EDF are shown, which enable usto determine the C-space feasibility domain, such that any task set with its worstcaseexecution times in the C-space domain is feasible with EDF. We show thatthe C-space domain is convex, a property that can be used to reduce the numberof inequalities characterizing the C-space domain.	Jean-François Hermant, Laurent George	http://editions-rnti.fr/render_pdf.php?p1&p=1000530	http://editions-rnti.fr/render_pdf.php?p=1000530
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	A Simplified Approach for Testing Real-Time Systems Based on Action Refinement	We propose a new method for generating digital-clock tests for realtime systems. That is, tests which can observe time with only a finite precision. Our goal from testing is to check the conformance of a given implementation with respect to a given specification (the model). The method is based on the socalled action refinement techniques. The main benefit of the method is to save memory space needed to build and to store tests. One important contribution of this work is a simplified way for both modelling and testing real-time systems. We first write a (high-level) simplified version of the model of the system, as an input-output transition system (IOTS) and then we refine it into a more detailed (low-level) model as a timed input-output transition system (TIOTS). This same mechanism applies to the test generation procedure.	Saddek Bensalem, Moez Krichen, Lotfi Majdoub, Riadh Robbana, Stavros Tripakis	http://editions-rnti.fr/render_pdf.php?p1&p=1000545	http://editions-rnti.fr/render_pdf.php?p=1000545
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Avionic Software Verification by Abstract Interpretation		Patrick Cousot	http://editions-rnti.fr/render_pdf.php?p1&p=1000525	http://editions-rnti.fr/render_pdf.php?p=1000525
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Development of Transportation Systems		Dines Bjorner	http://editions-rnti.fr/render_pdf.php?p1&p=1000528	http://editions-rnti.fr/render_pdf.php?p=1000528
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Don't care in SMT - Buildin flexible yet efficient abstraction/refinement solver	This paper describes a method for combining "off-the-shelf" SATand constraint solvers for building an efficient Satisfiability Modulo Theories(SMT) solver for a wide range of theories. Our method follows the abstraction/refinement approach to simplify the implementation of customSMT solvers.The expected performance penalty by not using an interweaved combination ofSAT and theory solvers is reduced by generalising a Boolean solution of anSMT problem first via assigning don't care to as many variables as possible. Wethen use the generalised solution to determine a thereby smaller constraint setto be handed over to the constraint solver for a background theory. We showthat for many benchmarks and real-world problems, this optimisation results inconsiderably smaller and less complex constraint problems.The presented approach is particularly useful for assembling a practically viableSMT solver quickly, when neither a suitable SMT solver nor a corresponding incrementaltheory solver is available. We have implemented our approach in theABSOLVER framework and applied the resulting solver successfully to an industrialcase-study: The verification problems arising in verifying an electroniccar steering control system impose non-linear arithmetic constraints, which donot fall into the domain of any other available solver.	Andreas Bauer, Martin Leucker, Christian Schallhart, Michael Tautschnig	http://editions-rnti.fr/render_pdf.php?p1&p=1000540	http://editions-rnti.fr/render_pdf.php?p=1000540
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Fault-Tolerance Analysis of Mixed CAN/Switched Ethernet Architecture	CAN is a well known fieldbus standard used in safety critical applicationsof embedded systems. However, steadily increasing amount of exchangedinformation in such systems has led to the use of Switched Ethernet like solutions.Mixed CAN/Switched Ethernet architectures allow to bypass CAN limitationswhile preserving the widely used CAN technology. In order to use this kindof architecture in safety critical applications a complete fault tolerance analysisis mandatory. In this paper, we use a simulation-based fault-injection techniqueto analyse the impact of different types of errors on the percentage of applicationframes missing their deadlines. Results show that different types of errors don'thave the same impact on different types of traffic. Moreover, it is shown thatthe re-emission of corrupted frames can have a negative impact on the system'sglobal performance.	Cláudia Betous-Almeida, Jean-Luc Scharbarg, Christian Fraboul	http://editions-rnti.fr/render_pdf.php?p1&p=1000534	http://editions-rnti.fr/render_pdf.php?p=1000534
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Formal Modeling of Data, a case study for space applications	This paper reports on a case study investigating the interest of formal data modeling approaches on functional parts and solve difficulties coming from lack of formalism in the description and handling of data analysis of space software data and their classifiation into families (section 2) and the elaboration of an A.S.N.1 data model for a case study representative of these data families (section 3). The second part illustrates two important outcome of such formal data models : the capability to automatically generate " Interface Control Documents", the contractual documents describing software interfaces and data (section 4) and automatically generate the needed interfacing code with the appropriate format encoders and decoders (section 5)	Jean-Paul Blanquart, Gérard Bulsa, David Lesens, George Mamais, Maxime Perrotin	http://editions-rnti.fr/render_pdf.php?p1&p=1000539	http://editions-rnti.fr/render_pdf.php?p=1000539
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Formal models of Fractal Component Based Systems for performance analysis	Component based system (CBS) development is now a well accepteddesign approach in software engineering. Although specific tools used for buildingCBS perform several checks on the built system, few of them provide formalverification of behavioural properties nor performance evaluation. In this context,we have developed a general method associating to a CBS a formal model,based on Stochastic Well formed Nets, a class of high level Petri Nets, allowingqualitative behavioural analysis and performance evaluation of the CBS. Thedefinition of the model heavily depends on the (run time) component model usedto describe the CBS. In this paper, we apply our method to Fractal CBS and itsreference Java implementation Julia, concentrating on performance evaluation.The main interest of our method is to take advantage of the compositional definitionof such systems to carry out an efficient analysis, starting from the Fractalarchitectural description of a CBS.	Nabila Salmi, Patrice Moreaux, Malika Ioualalen	http://editions-rnti.fr/render_pdf.php?p1&p=1000533	http://editions-rnti.fr/render_pdf.php?p=1000533
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Functionally Deterministic Scheduling	The aim of this paper is to prove that the question "is a schedulingfunctionally deterministic for a given set of periodic real time tasks and a givenconservative scheduling policy" is decidable. For thar purpose, we encode thetasks and the scheduler by an acyclic stopwatch automaton. We show then thatthe previous question can be encoded by a decidable reachability problem.	Frédéric Boniol, Claire Pagetti, François Revest	http://editions-rnti.fr/render_pdf.php?p1&p=1000531	http://editions-rnti.fr/render_pdf.php?p=1000531
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Model Based Development of Embedded Control Systems : Historical Perspective and Recent Advances	The thesis we wish to illustrate consists of showing that the safety-critical control industryhas designed a very strong model-based development method in order to face the severesafety requirements it has to face. We shall illustrate this thesis based on the methods in useat Airbus in the design of early fly-by-wire systems. Then we describe the evolution of thismethod from these pioneering times up to the present time and show what could be an idealworkflow today. Then we present a recent advance in this framework consisting of faithfullyimplementing control systems in a multi-tasking framework in such a way that the implementationmatches what has been modelled. We show how we can achieve this goal andshow that this approach has also be taken in the Matlab/Simulink code generator. We concludeby drawing some perspectives for the future.	Paul Caspi	http://editions-rnti.fr/render_pdf.php?p1&p=1000526	http://editions-rnti.fr/render_pdf.php?p=1000526
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	New Worst-Case Response Time Analysis Technique For Real-Time Transactions	In this paper, we present a new worst-case response time analysistechnique for transactions scheduled by fixed priorities. In the general contextof tasks with offsets (general transactions), only exponentialmethods are knownto calculate the exact worst-case response time of a task. The known pseudopolynomialtechniques give an upper bound of the worst-case response time.The new analysis technique presented in this article gives a better (i.e. lower)pseudo-polynomial upper bound of worst-case response time. The main idea ofthis approach is to combine the principle of exact calculation and the principleof approximation calculation, in order to decrease the pessimism of Worst-caseresponse time analysis, thus allowing to improve the upper bound of the responsetime provided while preserving a pseudo-polynomial complexity.	Ahmed Rahni, Emmanuel Grolleau, Michaël Richard	http://editions-rnti.fr/render_pdf.php?p1&p=1000529	http://editions-rnti.fr/render_pdf.php?p=1000529
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	OASIS formal approach for distributed safety-critical real-time system design	OASIS provides an environment for real time multitasking and communicationdesign, as well as an execution environment based on a safety orientedembedded real time kernel. The formal approach of real-time designavoids many difficulties: it allows implementing efficient advanced real-timefunctionalities without any safety loss. The concepts and methodology presentedin this paper ensure the most important safety properties. Within this framework,our goal is to rely on formal and algebraic tools that can automatically bring theproof of correctness for safety-critical design issues. Such a constructive approachcan easily speed up the system development by the formalization of theoff-line analysis.	Jean-Sylvain Camier, Damien Chabrol, Vincent David, Christophe Aussaguès	http://editions-rnti.fr/render_pdf.php?p1&p=1000543	http://editions-rnti.fr/render_pdf.php?p=1000543
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Preservation of timed properties during an incremental development by components	We are interested in the preservation of local properties of timed componentsduring their integration in a timed system. Timed components are modeledas timed automata or timed automata with deadlines. Properties consideredare all safety and liveness properties which can be expressed with the timedlinear logic MITL (Metric Interval Linear Logic), as well as non-zenoness anddeadlock-freedom. Integration of components is a kind of incremental developmentwhich consists in checking locally the properties of the components, beforeintegrating them in the complete system, using some composition operator. Ofcourse, established properties have to be preserved by this integration. Checkingpreservation can be achieved by means of the verification of timed -simulationrelations. Composability, compatibility and compositionality of these relationsw.r.t. composition operators are properties which allow to reduce the cost ofthis verification. We examine these properties when integration is achieved withtwo different timed composition operators: the classic operator usually takenfor timed systems and which uses a CSP-like composition paradigm, and a nonblockingoperator closer to the CCS paradigm.	Jacques Julliand, Hassan Mountassir, Emilie Oudot	http://editions-rnti.fr/render_pdf.php?p1&p=1000542	http://editions-rnti.fr/render_pdf.php?p=1000542
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Proved Development of the Real-Time Properties of the IEEE 1394 Root Contention Protocol with the Event B Method	We present a model of the IEEE 1394 Root Contention Protocol witha proof of Safety. This model has real-time properties which are expressed inthe language of the event B method: first-order classical logic and set theory.Verification is done by proof using the event B method and its prover, we alsohave a way to model-check models. Refinement is used to describe the studiedsystem at different levels of abstraction: first without time to fix the schedulingof events abstracly, and then with more and more time constraints.	Dominique Cansell, Joris Rehm	http://editions-rnti.fr/render_pdf.php?p1&p=1000544	http://editions-rnti.fr/render_pdf.php?p=1000544
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Qualitative Abstraction based Verification for Analog Circuits	The verification of analog designs is a challenging and exhaustivetask that requires deep understanding of the physical behaviors. In this paper,we propose a qualitative based predicate abstraction method for the verificationof a class of non-linear analog circuits. The method is based on combiningtechniques from constraint solving and computer algebra along with symbolicmodel checking. We have implemented the proposed verification algorithmsusing the computer algebra system Mathematica and the SMV model checker.	Mohammed Zaki, Sofiène Tahar, Guy Bois	http://editions-rnti.fr/render_pdf.php?p1&p=1000541	http://editions-rnti.fr/render_pdf.php?p=1000541
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Reliable and Precise WCET and Stack Size Determination for a Real-life Embedded Application	Failure of a safety-critical application on an embedded processor canlead to severe damage or even loss of life. Here we are concerned with two kindsof failure: stack overflow, which usually leads to run-time errors that are difficultto diagnose, and failure to meet deadlines, which is catastrophic for systemswith hard real-time characteristics. Classical software validation methods likesimulation and testing with debugging require a lot of effort, are expensive, anddo not really help in proving the absence of such errors.AbsInt's tools StackAnalyzer and aiT (timing analyzer) provide a solution tothese problems. They use abstract interpretation as a formal method that leadsto statements valid for all program runs. Both tools have been used successfullyat Hispano-Suiza to analyze applications running on a Motorola PowerPCMPC555. They turned out to be well-suited for analyzing large safety-criticalapplications developed at Hispano-Suiza. They can be used either during thedevelopment phase providing information about stack usage and runtime behaviorwell in advance of any run of the analyzed application, or during thevalidation phase for acceptance tests prior to the certification review.	Philippe Baufreton, Reinhold Heckmann	http://editions-rnti.fr/render_pdf.php?p1&p=1000532	http://editions-rnti.fr/render_pdf.php?p=1000532
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Seven at one stroke: LTL model checking for High-level Specifications in B, Z, CSP, and more	The size of formal models is steadily increasing and there is a demandfrom industrial users to be able to use expressive temporal query languages forvalidating and exploring high-level formal specifications. We present an extensionof LTL, which is well adapted for validating B, Z and CSP specifications.We present a generic, flexible LTL model checker, implemented inside the PROBtool, that can be applied to a multitude of formalisms such as B, Z, CSP, BkCSP,as well as Object Petri nets, compensating CSP, and dSL. Our algorithm can dealwith deadlocking states, partially explored state spaces, past operators, and canbe combined with existing symmetry reduction techniques of PROB. We establishcorrectness of our algorithm in general, as well as combined with symmetryreduction. Finally, we present various applications and empirical results of ourtool, showing that it can be applied successfully in practice.	Michael Leuschel, Daniel Plagge	http://editions-rnti.fr/render_pdf.php?p1&p=1000535	http://editions-rnti.fr/render_pdf.php?p=1000535
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	The Aeronautical Systems Development Challenges for Airbus	Over the last years, aeronautic industry has changed under the effects of the Europeanmarket and globalisation. In particular, it is crucial to reduce aircraft development costand lead time in order to be competitive on the market while ensuring a high level of safety.In parallel to these economic objectives, the aeronautic growth foreseen in the two next decadesleads to deliver an important number of aircrafts per year inducing strong industrialisationconstraints. These economic and industrial drivers have a strong impact on all the phasesof the aircraft development process. Consequently, the industrial practises shall evolve makingresearch and innovation crucial to aeronautics further development and conducive toEuropean competitiveness.This talk will focus on Airbus systems development challenges in the context of aworld-wide market. The key evolutions to conduct in terms of processes, methods and toolsfor the next generation of avionics systems will be pointed out. Finally, some possible avenuesof cooperation with the research community will be highlighted.	Odile Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000527	http://editions-rnti.fr/render_pdf.php?p=1000527
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Using Analogy to Promote Conceptual Modeling Reuse	This paper argues in favor of a database conceptual schema and SemanticWeb ontology design discipline that explores analogy mappings to reusethe structure and integrity constraints of conceptual models, stored in a repository.We presuppose that a team of expert conceptual designers wouldbuild a standard repository of source conceptual models, which less experienceddesigners would use to create new target conceptual models in otherdomains. The target models will then borrow the structure and the integrityconstraints from the source models by analogy. The concepts are expressed inthe contexts of Description Logics, the RDF model and OWL to reinforce thebasic principles and explore additional questions, such as the consistency ofthe target model.	Karin Koogan Breitman, Simone Diniz Junqueira Barbosa, Marco A. Casanova, Antonio L. Furtado, Michael G. Hinchey	http://editions-rnti.fr/render_pdf.php?p1&p=1000538	http://editions-rnti.fr/render_pdf.php?p=1000538
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Using Formal Methods to increase confidence in one Home Network System implementation: Case study	A home network system consists of multiple networked appliances,intended to provide more convenient and comfortable living for home users.Before being deployed, one has to guarantee the correctness, the safety and thesecurity of the system. Here, we present the approach chosen to validate the Javaimplementation of one home network system. We relies on the Java ModelingLanguage (JML), to formaly specify and validate a model of the system. it.	Lydie du Bousquet, Masahide Nakamura, Ben Yan, Hiroshi Igaki	http://editions-rnti.fr/render_pdf.php?p1&p=1000546	http://editions-rnti.fr/render_pdf.php?p=1000546
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Using Invariant Detection Mechanism in Black Box Inference	The testing and formal verification of black box software componentsis a challenging domain. The problem is even harder when specifications ofthese components are not available. An approach to cope with this problemis to combine testing with learning techniques, such that the learned modelsof the components can be used to explore unknown implementation and thusfacilitate testing efforts. In recent years, we have contributed to this approach byproposing techniques for learning parameterized state machine models and thenuse them in the integration testing of black box components. The major problemin this technique left unaddressed was the selection of parameter values duringthe learning process. In this paper, we propose to use an invariant detectionmechanism to select values in the learning process, thus refining model inferenceand testing approach. Initial experiments with small examples yielded positiveresults.	Muzammil Shahbaz, Roland Groz	http://editions-rnti.fr/render_pdf.php?p1&p=1000547	http://editions-rnti.fr/render_pdf.php?p=1000547
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Verification of embbeded systems with preemption: a negative result	The aim of this article is to explore the problem of verification ofpreemptive communicating timed processes, i.e., timed processes which can besuspended and resumed by an on-line scheduler. The contribution of the articleis to show that this problem is unfortunately undecidable. We discuss then analternative verification method to overcome this negative result.	Jérôme Ermont, Frédéric Boniol	http://editions-rnti.fr/render_pdf.php?p1&p=1000537	http://editions-rnti.fr/render_pdf.php?p=1000537
Revue des Nouvelles Technologies de l'Information	ISoLA	2007	Verification, Diagnosis and Adaptation: Tool-supported enhancement of the model-driven verification process	In this paper, we use a case study from an autonomous aerospace context as running example to show how to apply a game-based model-checking approach a as a powerful technique for the verification, diagnosis and adaptation of temporal properties. This work is part of our contribution within the SHADOWS project, where we provide a number of enabling technologies for model-driven self-healing. We propose here to use GEAR, a game-based model checker for the full modal &#956;-calculus and derived, more user-oriented logics, as a user friendly tool that can offer automatic proofs of critical properties of such systems. Designers and engineers can interactively investigate automatically generated winning strategies for the games, this way exploring the connection between the property, the system, and the proof.	Bernhard Steffen, Marco Bakera, Tiziana Margaria, Clemens D. Renner	http://editions-rnti.fr/render_pdf.php?p1&p=1000536	http://editions-rnti.fr/render_pdf.php?p=1000536
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Cadre général et algorithmes de constructions pour des représentations symboliques adaptatives de séries temporelles	Les séries temporelles constituent un domaine très important de la fouille de données. En effet, les très gros volumes de données numériques généralement entreposés ne se prêtent pas à une analyse directe. Dans un but à la fois de réduction de la dimensionnalité et d'extraction d'information, la fouille de données de séries temporelles donne généralement lieu à un changement de représentation des séries temporelles. Dans un objectif d'intelligibilité de l'information extraite lors du changement de représentation, on peut avoir recours à des représentations symboliques de séries temporelles. Nous proposons un cadre général de représentation de séries temporelles, ainsi que deux représentations particulières (Clustering-Based Symbolic Representations: CBSR et Segmentation-Based Symbolic Representation with Linear models of 0th order : SBSR-L0) s'inscrivant dans ce cadre général.	Bernard Hugueney	http://editions-rnti.fr/render_pdf.php?p1&p=1001690	http://editions-rnti.fr/render_pdf.php?p=1001690
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Créer un outil avec EXCEL pour les non-professionnels de la statistique : récit d'une expérience	Docteur en Biologie-Santé, les quelques années pendant lesquelles l'auteur a travaillé dans le secteur de la recherche en milieux hospitalier, universitaire et pharmaceutique lui ont donné l'occasion d'apprécier comment les médecins, chercheurs et scientifiques utilisent l'outil statistique. La remarque principale est que la plupart des logiciels de statistiques, étant des instruments de calcul particulièrement performants et très complets, ne sont par nature accessibles qu'aux professionnels de la statistique ; ce que ne souhaitent pas nécessairement devenir les acteurs de la recherche en France. L'auteur liste les principaux problèmes auxquels ces utilisateurs non-professionnels de la statistique sont confrontés et propose des solutions.	Stéphane Herault	http://editions-rnti.fr/render_pdf.php?p1&p=1001693	http://editions-rnti.fr/render_pdf.php?p=1001693
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Ecart ? type : une question d'orthographe.		Jacques Goupy	http://editions-rnti.fr/render_pdf.php?p1&p=1001711	http://editions-rnti.fr/render_pdf.php?p=1001711
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Économétrie, prévision et analyse des données.	L'analyse des données et l'analyse des séries temporelles ont toutes deux une longue histoire mais curieusement leurs chemins se sont rarement croisés, au moins jusqu'à récemment. Dans les dix dernières années, avec la mise à disposition d'énormes ensembles de données temporelles, on a assisté à une explosion d'intérêt pour l'exploration de ces fichiers gigantesques. Des centaines de papiers ont alors présenté et diffusé des méthodes et algorithmes pour indexer, classer, discriminer, segmenter des séries temporelles. C'est à ces nouveaux liens que le présent travail s'intéresse en se concentrant sur le domaine de l'analyse de la conjoncture et la prévision à court terme des grands agrégats économiques.	Dominique Ladiray	http://editions-rnti.fr/render_pdf.php?p1&p=1001703	http://editions-rnti.fr/render_pdf.php?p=1001703
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Estimation des titres viraux : une programmation pratique et fiable sur calculatrice de poche, et accessible par l'Internet	Les auteurs proposent une méthode basée sur la procédure itérative de Newton-Raphson pour estimer la concentration des suspensions virales à partir des résultats des essais par effet cytopathique " tout-ou-rien " liés à la courbe dose-réponse. La procédure de calcul fournit l'estimation du maximum de vraisemblance avec ses limites de confiance à 95% et détermine la valeur de Chi-2 pour tester la qualité de l'ajustement. Un programme est présenté (calculatrices HP des séries 41 et 48). Il convient pour les conditions expérimentales les plus variées et est également à présent accessible librement sur le site Internet créé.	Jocelyne Husson van Vliet, Philippe Roussel	http://editions-rnti.fr/render_pdf.php?p1&p=1001694	http://editions-rnti.fr/render_pdf.php?p=1001694
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Initiation à l'analyse des séries temporelles et à la prévision.	Nous présentons l'analyse des séries temporelles qui est employée dans de nombreuses sciences et techniques. Nous mettons notamment l'accent sur les méthodes de prévision telles qu'elles sont utilisées, en particulier pour la prévision des ventes dans les entreprises. Nous insistons sur les aspects statistiques et économétriques de ces méthodes et sur leurs limites respectives. Nous abordons également l'analyse spectrale. Outre quelques illustrations simples des méthodes, nous donnons des indications sur les logiciels employés, les cours disponibles et quelques aspects de calcul.	Guy Melard	http://editions-rnti.fr/render_pdf.php?p1&p=1001707	http://editions-rnti.fr/render_pdf.php?p=1001707
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Invitation à la lecture du petit livre rouge de William Playfair		Antoine De Falguerolles	http://editions-rnti.fr/render_pdf.php?p1&p=1001710	http://editions-rnti.fr/render_pdf.php?p=1001710
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Le logiciel MIXMOD d'analyse de mélange pour la classification et l'analyse discriminante.	Le logiciel mixmod est dévolu à l'analyse de mélanges de lois de probabilité sur des données multidimensionnelles dans un but d'estimation de densité, de classification ou d'analyse discriminante. Il propose un choix important d'algorithmes pour estimer les paramètres d'un mélange (EM, Classification EM, Stochastic EM). Il est possible de combiner ces algorithmes de multiples façons pour obtenir un maximum local pertinent de la vraisemblance ou de la vraisemblance complétée d'un modèle. Pour des variables quantitatives, mixmod utilise des mélanges de lois normales multidimensionnelles. Il propose ainsi quatorze modèles gaussiens différents selon des hypothèses faites sur les éléments spectraux des matrices de variance des composants. Pour des variables qualitatives, mixmod utilise des mélanges de lois multinomiales multidimensionnelles sous une hypothèse d'indépendance conditionnelle des variables sachant le composant du mélange. Grâce à une reparamétrisation des probabilités multinomiales, il propose cinq modélisations différentes. Par ailleurs, différents critères d'information sont proposés pour choisir un modèle parcimonieux et permettent notamment de choisir un nombre de composants pertinents. L'emploi de l'un ou l'autre de ces critères dépend de l'objectif poursuivi (estimation de densité, classification supervisée ou non). Écrit en C++, mixmod possède des interfaces avec Scilab et Matlab. Le logiciel, sa documentation statistique et son guide d'utilisation sont disponibles à l'adresse suivante :http://www-math.univ-fcomte.fr/mixmod/index.php.	Christophe Biernacki, Gilles Celeux, Anwuli Echenim, Gérard Govaert, Florent Langrognet	http://editions-rnti.fr/render_pdf.php?p1&p=1001704	http://editions-rnti.fr/render_pdf.php?p=1001704
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Les plans d'Expériences	Les plans d'expériences permettent d'organiser au mieux les essais qui accompagnent les études industrielles. Ils sont applicables aux problèmes où l'on recherche le lien qui existe entre une grandeur d'intérêt et un certain nombre de facteurs influents. On montre comment les plans d'expériences cherchent à obtenir le maximum de renseignements avec le minimum d'expériences. Pour cela, il faut suivre des règles et adopter une démarche que l'on décrit. Il existe de nombreux plans d'expériences adaptés aux principaux cas rencontrés par l'expérimentateur. Les principes fondamentaux sont indiqués et les principaux plans sont passés en revue.	Jacques Goupy	http://editions-rnti.fr/render_pdf.php?p1&p=1001695	http://editions-rnti.fr/render_pdf.php?p=1001695
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Luca Pacioli : mathématicien de la Renaissance, vulgarisateur de la géométrie euclidienne et de la comptabilité en partie double		Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001712	http://editions-rnti.fr/render_pdf.php?p=1001712
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	L'analyse des données : histoire, bilan, projets, ..., perspective. 		Jean-Paul Benzecri	http://editions-rnti.fr/render_pdf.php?p1&p=1001702	http://editions-rnti.fr/render_pdf.php?p=1001702
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	L'inférence bayésienne pour l'analyse des données expérimentales.	Ce tutoriel se situe dans la ligne des articles précédemment publiés dans la Revue de Modulad : Lecoutre (1996b, 2005/1997, 2005) ; Lecoutre, Poitevineau & Lecoutre (2005). Il s'appuie sur l'utilisation de programmes informatiques qui ont également fait l'objet d'une présentation dans un numéro précédent (Lecoutre & Poitevineau, 2005). La motivation de ce tutoriel est avant tout méthodologique, et le choix du cadre bayésien ne devrait pas paraître idéologique. Plus précisément, l'objectif est d'apporter aux questions essentielles soulev´ees par l'analyse des données expérimentales des réponses mieux adaptées que les tests de signification de l'hypothèse nulle. Basées sur des définitions opérationnelles plus utiles que les procédures traditionnelles (tests, intervalles de confiance), les méthodes bayésiennes offrent une souplesse considérable, en rendant tous les choix explicites. De plus, la philosophie bayésienne met en avant la nécessité de réfléchir sur l'information fournie par les données disponibles - "qu'est-ce que les données ont à dire ?" - au lieu d'appliquer des procédures rituelles.Des procédures bayésiennes de routine sont désormais faciles à mettre en oeuvre pour toutes les situations courantes. Leurs résultats peuvent être présentés sous une forme intuitivement séduisante et facilement interprétable. Elles ouvrent une nouvelle voie prometteuse dans la méthodologie statistique.	Bruno Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001708	http://editions-rnti.fr/render_pdf.php?p=1001708
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Méthodologie d'estimation des coûts de production agricole : comparaison de deux méthodes sur la base du RICA.	La renégociation de la Politique agricole commune, l'élargissement de l'Union européenne à certains pays d'Europe centrale et orientale, suscitent tant dans le contexte de marchés concurrentiels que de marchés régulés, des besoins récurrents d'estimation des coûts de production des principaux produits agricoles. Cet article présente la méthodologie économétrique utilisée dans le modèle microéconomique d'estimation des coûts de production des principaux produits agricoles à partir du Réseau d'information comptable agricole. La première partie de cet article retrace les principales étapes du développement de ce modèle estimant les coefficients d'une matrice d'input-output croisant les charges et les productions, avec ses différentes utilisations, tant dans un contexte national qu'européen. La seconde partie présente une comparaison de deux spécifications distinctes du modèle économétrique : une spécification standard basée sur un modèle à équations simultanées, une spécification non paramétrique fondée sur l'utilisation des particularités de la régression des moindres carrés partiels. Les résultats présentés permettent de dégager l'intérêt respectif de chacun des modèles examinés en proposant des procédures empiriques de validation des estimations. L'article se conclut sur un argumentaire en faveur d'un observatoire des coûts de production agricole, le situant dans le contexte du futur programme-cadre européen de la recherche.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001705	http://editions-rnti.fr/render_pdf.php?p=1001705
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Multiple time series: New approches and new tools in data mining applications to cancer epidemiology	Des résultats innovants en fouille de données complexes fournissent des approches originales pour les épidémiologistes qui bénéficient de traitements interactifs pour aborder leurs données conjointement sous toutes leurs entrées et en tirer des résultats. L'étude présente des algorithmes qui travaillent sur des espaces multidimensionnels de fonctions (ici des chroniques ou bien encore des distributions discrètes ou discrétisées à support fini) avec moins de perte d'information que dans les codages habituels par agrégation, quantiles ou autres ; ils ont été implémentés dans le logiciel DELTA Suite : chaque cellule d'une table étudiée contient une donnée complexe (par exemple une série temporelle). Delta Suite est utilisé ici dans deux études épidémiologiques de l'évolution des cancers dans le temps et dans l'espace: en un premier temps pour la visualisation simultanée et l'exploration des chroniques de taux de mortalité par cancer sur cinq entrées conjointes ( géographiques, temporelles, âge, sexe et pathologies) puis dans un deuxième temps pour la comparaison géographique des courbes d'évolution des cancers du poumon pour 51 pays et 21 années par généralisation des approches de classification automatique.	Mireille Gettler Summa, Frédérick Vautrain, Laurent Schwartz, Mathieu Barrault, Jean Marc Steyaert, Nicolas Hafner	http://editions-rnti.fr/render_pdf.php?p1&p=1001692	http://editions-rnti.fr/render_pdf.php?p=1001692
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Notes de lecture : Approche Pragmatique de la Classification		Josiane Confais, Jean-Pierre Nakache, Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001697	http://editions-rnti.fr/render_pdf.php?p=1001697
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Notes de lecture : Fouille de Données Complexes		Omar Boussaid, Pierre Gançarski, Florent Masseglia, Brigitte Trousse, Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001696	http://editions-rnti.fr/render_pdf.php?p=1001696
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Pierre Émile Levasseur (1828-1911). À l'interface des sciences sociales. 	Émile Levasseur fut une figure majeure de la science sociale du 19e siècle, et un savant de réputation internationale. Son oeuvre présente l'originalité de connecter des champs scientifiques rarement associés avant lui : l'histoire, l'économie politique et sociale, la géographie et la statistique. Il y fait largement appel aux représentations graphiques, par carte, courbes ou diagrammes. Il participe à la diffusion et à la vulgarisation des méthodes de la statistique graphique et contribue au débat théorique sur la classification des procédés.	Gilles Palsky	http://editions-rnti.fr/render_pdf.php?p1&p=1001706	http://editions-rnti.fr/render_pdf.php?p=1001706
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Premiers pas en régression linéaire avec SAS.	Ce tutoriel accessible par internet montre de façon intuitive et sans formalisme excessif, les principales notions théoriques nécessaires à la compréhension et à l'interprétation des résultats d'analyses de régression linéaire et multiple produits par la procédure REG de SAS® et par le menu FIT de SAS/INSIGHT .Ce document est issu d'un cours enseigné par les auteurs dans différentes formations : ISUP, DEA & DESS de Paris 1, formation permanente du CNRS, au CEPE de l'INSEE. Il fait suite à un premier document de travail publié à l'Unité Méthodes Statistiques de l'INSEE.Nous avons ajouté de nombreux graphiques et affichages de SAS/INSIGHT, qui par ses possibilités de visualisation et d'interactivité, facilitent la compréhension à la fois des données et des techniques. Nous avons profité des possibilités d'internet pour ajouter des liens vers des applets ou d'autres documents accessibles sur le web.Nous insistons dans ce tutoriel, sur l'importance des graphiques exploratoires, et sur les limites des résultats obtenus par une régression linéaire, si l'étape de vérification des suppositions n'est pas systématiquement entreprise.	Josiane Confais, Monique Le Guen	http://editions-rnti.fr/render_pdf.php?p1&p=1001709	http://editions-rnti.fr/render_pdf.php?p=1001709
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Une biographie de Yule		Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001698	http://editions-rnti.fr/render_pdf.php?p=1001698
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2006	Une introduction à la méthodologie de Box et Jenkins : l'utilisation de modèles ARIMA avec SPSS	Cette note initie l'utilisateur débutant à la mise en oeuvre des procédures du module Séries chronologiques du logiciel SPSS pour Windows correspondant à la méthodologie de Box et Jenkins pour la modélisation à partir de processus ARIMA. Cette mise en oeuvre concerne l'analyse et la prévision du prix des produits agricoles, en particulier celui du blé tendre. Le listage de chaque procédure d'analyse statistique est commenté par la présentation du formulaire et l'interprétation des résultats obtenus.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001691	http://editions-rnti.fr/render_pdf.php?p=1001691
Revue des Nouvelles Technologies de l'Information	ECD	2006	CASOM : Carte auto-organisée pour l'analyse exploratoire des tableaux de contingence	La visualisation des connaissances par des méthodes d'extraction de l'information pour un corpus de données multimédia est une question pertinente aussi bien en recherche d'information où l'on cherche les meilleurs documents répondant à une requête, qu'en analyse des données où l'on cherche à comprendre quantitativement le contenu du corpus. En effet, en recherche d'information, les corrélations inter-variables permettent d'enrichir la requête de la même façon qu'elles renseignent sur les liaisons interprétables en analyse des données. Une manière générale de répondre à l'objectif posé est l'emploi de méthodes de réduction efficaces qui permettent de mettre en évidence les différentes caractéristiques principales et locales du corpus. Les méthodes de carte auto-organisatrice entrent dans cette optique tout en apportant la dimension supplémentaire d'une carte projective de la distribution et partitionnant le plan en diverses thématiques adjacentes. Ces méthodes rendent appréhendable par l'humain un nuage de points plongé dans un espace de grande dimension par la construction d'une surface discrète qui épouse la forme de sa distribution. Elles offrent ainsi une propriété de cartographie apte à montrer une structure sous-jacente. Dans ce cadre, nous développons une représentation originale des cartes de Kohonen pour des vecteurs textuels bruts. Nous fournissons des indicateurs numériques interprétables et aboutissons à la définition d'une méthode de visualisation synthétique et globale d'un corpus : un biplot sous la forme d'un graphe de mots révélant leurs liaisons statistiques, superposable à la projection des documents. La méthode est illustrée par le graphe du vocabulaire extrait d'un corpus de données réelles.	Rodolphe Priam	http://editions-rnti.fr/render_pdf.php?p1&p=1001521	http://editions-rnti.fr/render_pdf.php?p=1001521
Revue des Nouvelles Technologies de l'Information	ECD	2006	Découverte interactive de règles d'association via une  interface visuelle	En nous appuyant sur des hypothèses majoritairement empruntées à des travaux sur les systèmes anthropocentrés d'aide à la décision, nous décrivons dans cet article un environnement interactif de fouille de règles d'association dans lequel l'utilisateur pilote le processus, en jouant le rôle d'une heuristique dans un environnement de recherche complexe. Afin de permettre à la fois une représentation visuelle accessible et une instanciation aisée des outils d'interactivité le modèle choisi est ici un graphe en niveaux - les niveaux étant associés aux cardinaux des sous-ensembles d'attributs des prémisses. Le processus a été déployé dans un logiciel prototype dont l'analyse des résultats ouvre de nouvelles perspectives sur l'analyse comportementale d'un utilisateur en situation de fouille.	Pascale Kuntz, Rémi Lehn, Fabrice Guillet, Bruno Pinaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001523	http://editions-rnti.fr/render_pdf.php?p=1001523
Revue des Nouvelles Technologies de l'Information	ECD	2006	Détection et interprétation visuelle  d'outliers dans les grands ensembles de données	Nous présentons un algorithme hybride de détection d'outliers (individus atypiques) dans de grands ensembles de données, utilisant un algorithme génétique pour la sélection des attributs et une approche basée sur la distance pour la détection de l'élément outlier (atypique) suivant ce sous-ensemble d'attributs. Une fois l'outlier trouvé, nous essayons de l'expliquer : est-ce une erreur, un bruit ou une valeur significativement différente des autres ? Pour ce faire, on utilise des méthodes visuelles telles que les coordonnées parallèles. Nous évaluons les performances de notre méthode sur différents ensembles de données de grandes dimensions et le comparons avec les algorithmes existants.	Lydia Boudjeloud, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001525	http://editions-rnti.fr/render_pdf.php?p=1001525
Revue des Nouvelles Technologies de l'Information	ECD	2006	La plate-forme DynaSpat : Les dynamiques spatiales	La plate-forme DynaSpat est une plate-forme de simulation et de visualisation pour la description, l'analyse et la modélisation de comportements d'acteurs économiques. Elle intègre un ensemble de logiciels et de bibliothèques permettant d'exécuter des simulations basées sur les algorithmes génétiques et de résumer une information complexe, issue d'un jeu de données, par des variables pertinentes. Elle permet ainsi de comprendre l'influence réciproque entre la structuration d'un territoire et des comportements d'acteurs économiques ou sociaux.	Sandrine Coelho, Christine Thomas-Agnan, Nicolas Lassabe, Yves Duthen	http://editions-rnti.fr/render_pdf.php?p1&p=1001526	http://editions-rnti.fr/render_pdf.php?p=1001526
Revue des Nouvelles Technologies de l'Information	ECD	2006	Représentation et comparaison de séquences par visualisation	Dans cet article, nous présentons un outil de visualisation de séquences modélisées par des arbres de suffixes probabilistes (Prediction Suffix Trees - PST). Ce type d'arbre permet de représenter une chaîne de Markov d'ordre variable. Dans différentes applications, il s'est avéré plus efficaces qu'une chaîne de Markov d'ordre fixe, avec un coût calculatoire moindre. Pour ces raisons, il nous a paru intéressant d'exploiter le caractère arborescent de ce mode de représentation des séquences, non seulement d'un point de vue algorithmique, mais aussi d'un point de vue visuel. Le logiciel que nous avons développé dans ce but fournit une représentation graphique d'un PST appris à partir de séquences et, il permet de le comparer à un autre. Dans un contexte de classement supervisé d'une nouvelle séquence, il apport une information complémentaire par rapport au PST en mettant en évidence les sous-séquences qui n'ont pas été observées dans la nouvelle séquence bien qu'elles soient caractéristiques du modèle sous-jacent à sa classe d'affectation. Ainsi, il permet de mieux appréhender la structure des séquences et d'améliorer le processus de fouille de données par leur visualisation.	Christine Largeron, Cedric Dreissia	http://editions-rnti.fr/render_pdf.php?p1&p=1001524	http://editions-rnti.fr/render_pdf.php?p=1001524
Revue des Nouvelles Technologies de l'Information	ECD	2006	Techniques visuelles de recherche d'information	Nous exposons dans cet article un état de l'art sur les techniques visuelles pouvant être utilisées dans la recherche d'information documentaire sur Internet ou dans un système d'information. Nous détaillons dans un premier temps les techniques qui permettent aux utilisateurs de formuler et d'affiner leurs requêtes de façon interactive et visuelle. Nous présentons ensuite les techniques permettant de représenter un document isolément puis les techniques et systèmes permettant de représenter un ensemble de documents. Nous analysons les atouts et faiblesses de ces méthodes et nous dégageons des perspectives pour ce domaine prometteur.	Fewzi Mokaddem, Fabien Picarougne, Hanane Azzag, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001519	http://editions-rnti.fr/render_pdf.php?p=1001519
Revue des Nouvelles Technologies de l'Information	ECD	2006	Vis-SVM : approche coopérative en fouille de données	La compréhension des résultats en sortie d'un algorithme de fouille de données est aussi importante que d'obtenir de bons taux de précision. Malheureusement, les modèles obtenus par les algorithmes de support vector machines ou séparateurs à vaste marge (SVM) fournissent seulement les vecteurs support qui sont utilisés comme une « boîte noire » pour classifier efficacement les données avec de bons taux de précision. Il est donc indispensable d'améliorer la compréhensibilité des modèles de SVM. Cet article présente différentes coopérations entre des méthodes de visualisation et des algorithmes de SVM en fouille de données. En post-traitement d'algorithmes de SVM, nous présentons une approche coopérative graphique interactive pour interpréter des résultats de classification, régression et détection d'individus atypiques. Nous étendons l'approche d'interprétation graphique pour améliorer les résultats obtenus par la classification de SVM. Nous présentons ensuite une approche coopérative permettant d'impliquer plus significativement l'utilisateur dans la tâche de classification à l'aide de SVM. Ce type d'approche présente notamment comme avantage la possibilité d'utiliser les capacités humaines en reconnaissance de formes par le biais de méthodes de visualisation. L'utilisateur a une meilleure compréhension du modèle construit et une meilleure confiance dans ce modèle parce qu'il a participé activement à sa construction. Nous montrons comment l'utilisateur peut utiliser des outils coopératifs pour construire des modèles de SVM. Une étape de prétraitement est également utilisée dans notre outil coopératif pour pouvoir traiter de grands ensembles de données. Nous évaluons les performances de la nouvelle approche coopérative sur les ensembles de données de l'UCI, Delve, Statlog et biomédicales.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001520	http://editions-rnti.fr/render_pdf.php?p=1001520
Revue des Nouvelles Technologies de l'Information	ECD	2006	Visualisation exploratoire, généricité, exhaustivité et facteur d'échelle	L'objectif de cet article est d'étudier les limites des outils de visualisation exploratoire actuels et d'explorer de nouvelles voies pour dépasser ces limites. Ce travail est mené selon deux axes fortement liés :	o Impact du facteur d'échelle : les outils actuels, procèdent par isolation, simplification et réduction, afin de ramener la masse de données vers un niveau acceptable. Ce mode opératoire a une dimension arbitraire qui fait disparaître des liens d'ordre structurel et sémantique, ce qui réduit les chances d'extraction d'information.	o Impact du paradigme : regard critique sur le choix en termes d'hypothèses et prétraitements sous-jacents -tel que le filtrage- effectués par ces outils sur les données elles-mêmes. En effet, ces hypothèses et prétraitements conditionnent fortement, par leur effet réducteur, les informations que les outils pourront ou non extraire des données. Les deux points précédents posent la question de la généricité, de l'exhaustivité et de la neutralité des outils de visualisation. La prise en compte de ces contraintes nous fournit les bases pour des outils de visualisation exploratoire génériques, exhaustifs et pouvant résister au facteur d'échelle.	Bénédicte Le Grand, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1001518	http://editions-rnti.fr/render_pdf.php?p=1001518
Revue des Nouvelles Technologies de l'Information	ECD	2006	Visualisation par l'exemple des dépendances dans les  bases de données relationnelles	Comprendre la sémantique des bases de données relationnelles existantes est important pour de nombreuses applications. Cette sémantique est principalement véhiculée par les dépendances fonctionnelles (DF) et les dépendances d'inclusion (DI) ; elles généralisent respectivement les notions de clé et de clé étrangère. Toutefois, il est fréquent que les bases de données opérationnelles deviennent désordonnées dans le temps ; dans ce cas, les contraintes d'intégrité doivent être retrouvées à partir des donn ées. Plusieurs méthodes ont été proposées pour la découverte des DF ou des DI. Ces algorithmes fournissent à l'administrateur un ensemble de dépendances satisfaites dans les données. Se pose alors le problème de la compréhension des dépendances extraites, incluant des aspects liés à la visualisation des connaissances. Cette étape doit permettre, par exemple, d'assister l'utilisateur nal à sélectionner les règles intéressantes, ou à comprendre pourquoi une dépendance attendue n'est pas satisfaite dans les données. Nous proposons de fournir à l'administrateur ou l'analyste, en complément de la liste des règles, un échantillon de la base de données, vériant exactement les même DF et DI, appelé base de données d'Armstrong informative (BDAI). Ces exemples nous semblent particulièrement adaptés pour faciliter les échanges entre l'administrateur et les experts du domaine. Nous donnons certaines propri étés sur l'existence et la taille des BDAI, ainsi que des algorithmes pour les construire. Des expérimentations sur une base réelle issue du web montrent l'intérêt pratique de cette proposition.	Fabien De Marchi, Jean-Marc Petit	http://editions-rnti.fr/render_pdf.php?p1&p=1001522	http://editions-rnti.fr/render_pdf.php?p=1001522
Revue des Nouvelles Technologies de l'Information	EDA	2006	Conception et construction d'entrepôts en XML	Les entreprises sont de plus en plus concernées par des données dites complexes, se présentant sous une forme autre que numérique ou symbolique, issues de sources différentes et ayant des formats hétérogènes. Pour une exploitation à des fins décisionnelles, ces données complexes nécessitent un travail préparatoire pour les structurer et les homogénéiser.La prolifération des données sous forme de documents XML incite à une solution d'entreposage. Nous proposons dans ce papier une approche basée sur XML, d'entreposage de données complexes contenues dans des documents XML, appelée X-Warehousing. Celle-ci définit une méthodologie pour concevoir des entrepôts de données complexes à l'aide du formalisme XML. Pour valider notre approche, nous avons implémenté une application Java et nous avons réalisé une étude de cas sur des données complexes concernant les régions suspectes sur des mammographies.	Omar Boussaid, Riadh Ben Messaoud, Rémy Choquet, Stéphane Anthoard	http://editions-rnti.fr/render_pdf.php?p1&p=1001472	http://editions-rnti.fr/render_pdf.php?p=1001472
Revue des Nouvelles Technologies de l'Information	EDA	2006	Entrepôts de contenu autour de XML et des services Web	Les entreprises et plus généralement les communautés organisées autour d'un intérêt commun, peuvent bénéficier de la construction, de l'enrichissement, du monitorage, et de la gestion de gros volumes de contenus, avec les méthodes pour accéder, analyser, et annoter cette information. Le contenu consiste en des données (relationnelles, courriels, lettres, rapports, etc.), méta-données, connaissance (ontologies), ou servicesWeb (e.g., formulaire deWeb). Un entrepôt de contenu est un logiciel de gestion de ce genre d'information. Nous proposons d'utiliser XML et les services Web comme base pour les entrepôts de contenu.Du fait de l'ubiquité de l'information, on peut observer une tendance à transférer les tâches de gestion d'information des bases de données au réseaux. Nous discuterons des travaux récents sur les entrepôts de contenu en pair-à-pair.On mentionnera le projet RNTL webContent de plateforme de gestion de contenu du Web.	Serge Abiteboul	http://editions-rnti.fr/render_pdf.php?p1&p=1001471	http://editions-rnti.fr/render_pdf.php?p=1001471
Revue des Nouvelles Technologies de l'Information	EDA	2006	Évolution de schémas dans les entrpôts de données : modèle à base de règles	La mise en oeuvre d'un entrepôt de données nécessite un important travail d'étude de l'existant et de recueil de données pour bien traiter les besoins d'analyses. Hors, il est impossible de définir de façon exhaustive ces besoins pour l'ensemble des utilisateurs. La prise en compte de nouveaux besoins d'analyses nécessite l'évolution du schéma de l'entrepôt de données. Deux alternatives existent : la première propose la mise à jour du schéma comme dans Hurtado et al. (1999), la seconde consiste à gérer différentes versions du schéma comme dans Morzy et Wrembel (2004). Ces deux alternatives apportent une réponse à l'émergence de nouveaux besoins d'analyses qui sont engendrés par l'évolution des données, mais pas lorsqu'ils sont engendrés par des connaissances du domaine dont disposent les utilisateurs. En effet, dans les modèles existants, seules les données sont utilisées pour atteindre les objectifs d'analyses, les connaissances étant exclues du processus d'analyse. Notre objectif est donc de pouvoir intégrer les connaissances du domaine dans le processus d'entreposage afin d'apporter au modèle une flexibilité en termes d'évolution des contextes d'analyses. Pour atteindre cet objectif, nous proposons un nouveau modèle conceptuel d'entrepôt de données à base de règles, baptisé R-DW (Rule-based Data Warehouse), dans lequel les règles permettent d'intégrer les connaissances du domaine, exprimées par l'utilisateur.	Cécile Favre, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001482	http://editions-rnti.fr/render_pdf.php?p=1001482
Revue des Nouvelles Technologies de l'Information	EDA	2006	HYPE : prise en compte des hiéarchies lors de l'extraction de motifs séquentiels multidimensionnels	Les entrepôts de données contiennent de grosses masses de données historisées stockées à des fins d'analyse. Si les méthodes et outils d'analyse sont maintenant bien connus (OLAP), il reste difficile de fournir aux utilisateurs des outils de fouille de données permettant la prise en compte des spécificités de ces contextes (e.g. multidimensionnalité, hiérarchies, données historisées). Dans cet article, nous proposons une méthode originale d'extraction de motifs séquentiels prenant en compte les hiérarchies. Cette méthode extrait des connaissances plus précises et étend notre approche précédente M²SP. Nous définissons les concepts liés à notre problématique ainsi que les algorithmes associés. Les expérimentations que nous avons menés montrent l'intérêt de notre proposition.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001481	http://editions-rnti.fr/render_pdf.php?p=1001481
Revue des Nouvelles Technologies de l'Information	EDA	2006	Méta-modélisation des bases de données multidimensionnelles annotées	Cet article présente le concept de Mémoire d'Expertises Décisionnelles (MED). Une MED comprend d'une part des fonctionnalités d'un système d'aide à la décision et d'autre part des fonctionnalités de mémorisation et d'exploitation du capital immatériel des décideurs. Notre système d'aide à la décision repose sur une Base de Données Multidimensionnelles (BDM) composée de faits, dimensions, mesures, hiérarchies et paramètres. Le capital immatériel des décideurs est articulé au travers d'annotations associées aux différents concepts et valeurs de la BDM. Nous définissons un modèle conceptuel de BDM annotées et le méta-modèle associé.	Guillaume Cabanac, Max Chevalier, Franck Ravat, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1001474	http://editions-rnti.fr/render_pdf.php?p=1001474
Revue des Nouvelles Technologies de l'Information	EDA	2006	Modélisation adaptée aux besoins utilisateurs dans le développement des systèmes d'information décisionnels	La démocratisation des systèmes d'information décisionnels (SID) nécessite le développement de méthodes de conception. Contrairement aux modèles de systèmes d'information (SI) qui n'ont pas pour objet d'être compris par les utilisateurs, les modèles des SID doivent être exploitables par les analystes et les décideurs. Parmi les méthodes d'ingénierie des SID qui ont été proposées, rares sont celles qui explicitent la tâche d'analyse des besoins.Pour ces raisons, nous proposons une démarche de collecte et de formalisation des besoins des utilisateurs du SID en utilisant des modèles proches de leur vision des données. A partir des besoins spécifiés sous forme de tableaux multidimensionnels, nous proposons des extensions de la modélisation objet afin de formaliser les besoins en terme de données et de traitements dans le contexte multidimensionnel.	Estella Annoni, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1001473	http://editions-rnti.fr/render_pdf.php?p=1001473
Revue des Nouvelles Technologies de l'Information	EDA	2006	Multi-source materialized views maintenance	In many information systems, the databases that make up the system are distributed in different modules or branch offices according to the requirements of the business enterprise. In these systems, it is often necessary to combine the information of all the organisation's databases in order to perform analysis and make decisions about the global operation. This is the case of Data Warehouse Systems. From a conceptual point of view, a Data Warehouse can be considered as a set of materialized views which are defined in terms of the tables stored in one or more databases. These materialized views store historical data that must be maintained in either real time or periodically by means of batch processes. During the maintenance process the systems must perform selections, projections, joins, etc. that can affect several databases. This is a complex problem since making a join among several tables requires (at least temporarily) having the information from these tables in the same place. This requires the Data Warehouse to store auxiliary materialized views that in many cases contain duplicated information. In this article, we study this problem, and we propose a method that minimizes the duplicated information in the auxiliary materialized views and also reduces the response time of the system.	Josep Silva, Jorge Belenguer, Matilde Celma	http://editions-rnti.fr/render_pdf.php?p1&p=1001476	http://editions-rnti.fr/render_pdf.php?p=1001476
Revue des Nouvelles Technologies de l'Information	EDA	2006	OLAP : un pas vers la navigation	Dans le contexte de l'analyse OLAP, le concept de navigation n'a jamais été défini formellement. Nous montrons pourquoi cette lacune est préjudiciable. Nous proposons ensuite une formalisation du concept de navigation, ainsi qu'une première ébauche de langage de définition de navigations.	Elsa Negre, Patrick Marcel, Arnaud Giacometti	http://editions-rnti.fr/render_pdf.php?p1&p=1001478	http://editions-rnti.fr/render_pdf.php?p=1001478
Revue des Nouvelles Technologies de l'Information	EDA	2006	Ontology evolution and source autonomy in ontology-based data warehouses	Ontology-based integration systems (OBIS) use ontologies in order to describe the semantic of sources and to make the content explicit. Two major architectures of OBISs are available: (i) those using an unique ontology, and (ii) those using multiple ontologies. In the first architecture, all sources are related to one shared ontology. This architecture suffers from changes of ontology and sources which can affect the conceptualization of the domain represented in the ontology. Any change in the ontology may affect the sources. Therefore, sources are not really autonomous. In hybrid ontologies, each source is described by its own ontology, called local ontology. Each one references/maps a shared ontology in order to guarantee that each source shares the same vocabulary. The articulation between local ontologies and the shared ontology can be done either a posteriori or a priori. Two major issues are raised in this architecture: (i) evolution of the shared ontology and its consequence on the integrated system, and (ii) autonomy of the ontology and the local schema of each source. In this paper, we propose an approach and a model to manage asynchronous evolution of warehouse integrated systems where the articulation is done in an a priori manner. The fundamental hypothesis of our work, called principle of ontological continuity, supposes that an evolution of an ontology does not make false an axiom that was previously true. This principle allows to manage each old instance using the actual ontology. Therefore, it simplifies significantly the management of the evolution process and allows a complete automation of the whole integration process. Our work is motivated by the automatic integration of catalogs of industrial components in engineering databases. It has been validated by a prototype using ECCO environment and EXPRESS language.	Dung Nguyen Xuan, Ladjel Bellatreche, Guy Pierra	http://editions-rnti.fr/render_pdf.php?p1&p=1001475	http://editions-rnti.fr/render_pdf.php?p=1001475
Revue des Nouvelles Technologies de l'Information	EDA	2006	Représentation et indexation d'objets mobiles dans un entrepôt de données	Le développement considérable des techniques de géo localisation et des périphériques mobiles mène à une profusion de bases d'objets mobiles et soulève la question de l'exploitation pour l'aide à la décision. Si les systèmes conventionnels d'analyse en ligne (OLAP) sont utilisés efficacement dans l'analyse des données basées sur la modélisation multidimensionnelle, ils ne sont pas adaptés aux bases d'objets mobiles. Ces objets possèdent outre les attributs descriptifs, une trajectoire variant continuellement dans des dimensions spatiales et temporelles. Cet article étudie le problème de l'entreposage d'objets mobiles dans un réseau. Ses contributions sont : (1) un modèle conceptuel qui étend le modèle multidimensionnel aux dimensions et aux faits continus ; (2) un modèle de représentation adapté ; (3) une structure d'indexation et des algorithmes optimaux pour les requêtes spatiotemporelles OLAP sur ces objets mobiles. Un prototype a été développé et les résultats de l'expérimentation, détaillés ici, montrent l'efficacité des méthodes proposées.	Tao Wan, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1001480	http://editions-rnti.fr/render_pdf.php?p=1001480
Revue des Nouvelles Technologies de l'Information	EDA	2006	Rhéa : entrepôt de données et aide à la décision pour les services de réanimation	Les organismes de santé publique estiment que les événements iatrogènes affectent chaque année environ 2 millions de patients hospitalisés aux USA, pour un coût approximatif de 57,6 milliards de dollars en 2000. Le but du projet RHEA est d'élaborer un système automatisé, évolutif, et individuel de veille des patients de réanimation, permettant d'estimer en temps réel, un jour donné, la probabilité de survie et les probabilités de survenue de complications, en vue d'améliorer la qualité des soins et le devenir des patients en réanimation. Ce système permet d'une part une amélioration du pronostic des patients en tenant compte de leur évolution pendant leur séjour en réanimation, et d'autre part l'identification et la détermination des facteurs de risque ainsi que le pronostic des infections nosocomiales (bactériémies, infections sur cathéter, pneumonies nosocomiales, sinusites nosocomiales) et des événements iatrogènes.	Elisabeth Métais, Didier Nakache, Jean-François Timsit, Charles de Laguiche	http://editions-rnti.fr/render_pdf.php?p1&p=1001483	http://editions-rnti.fr/render_pdf.php?p=1001483
Revue des Nouvelles Technologies de l'Information	EDA	2006	Sélection simultanée d'index et de vues matérialisées	Les index et les vues matérialisées sont des structures physiques qui accélèrent l'accès aux données d'un entrepôt. Ces structures engendrent cependant une surcharge de maintenance. Par ailleurs, elles partagent le même espace disque. Les travaux existants dans le domaine de la sélection d'index et de vues matérialisées traitent ces deux structures de manière isolée. Dans cet article, nous couplons au contraire la sélection d'index et de vues matérialisées de façon à prendre en compte les interactions entre ces structures de données et à permettre un partage efficace de l'espace de stockage commun qui leur est alloué. Pour cela, nous avons développé des modèles de coût qui évaluent le bénéfice de la matérialisation de vue et de l'indexation. Ces modèles de coût nous permettent, grâce à un algorithme glouton, de sélectionner une configuration pertinente d'index et de vues matérialisées. Nos expérimentations montrent que notre stratégie se révèle meilleure que celles qui opèrent une sélection isolée des index et des vues matérialisées.	Nora Maiz, Kamel Aouiche, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1001477	http://editions-rnti.fr/render_pdf.php?p=1001477
Revue des Nouvelles Technologies de l'Information	EDA	2006	Vers un entrepôt de données pour le trafic routier	Cet article présente la démarche multidisciplinaire que nous avons adoptée pour construire un système d'information pour l'aide à la décision dans la gestion du trafic routier. L'architecture du système, le schéma de l'entrepôt de données ainsi que les différentes représentations numériques et symboliques des séquences spatio-temporelles, stockées dans l'entrepôt, y sont détaillés.	Claudia Bauzer-Medeiros, Olivier Carles, Florian Devuyst, Georges Hébrail, Bernard Hugueney, Marc Joliveau, Geneviève Jomier, Maude Manouvrier, Yosr Naïja, Gérard Scemama, Laurent Steffan	http://editions-rnti.fr/render_pdf.php?p1&p=1001479	http://editions-rnti.fr/render_pdf.php?p=1001479
Revue des Nouvelles Technologies de l'Information	EGC	2006	Accès aux connaissances orales par le résumé automatique	Le temps nécessaire pour écouter un flux audio est un facteur réduisant l'accès efficace àde grandes archives de parole. Une première approche, la structuration automatique des données,permet d'utiliser un moteur de recherche pour cibler plus rapidement l'information. Leslistes de résultats générées sont longues dans un souci d'exhaustivité. Alors que pour des documentstextuels, un coup d'oeil discrimine un résultat interessant d'un résultat non pertinant,il faut écouter l'audio dans son intégralité pour en capturer le contenu. Nous proposons doncd'utiliser le résumé automatique afin de structurer les résultats des recherches et d'en réduirela redondance.	Benoît Favre, Jean-François Bonastre, Patrice Bellot, François Capman	http://editions-rnti.fr/render_pdf.php?p1&p=1000358	http://editions-rnti.fr/render_pdf.php?p=1000358
Revue des Nouvelles Technologies de l'Information	EGC	2006	Affectation pondérée sur des données de type intervalle	On s'intéresse à la construction d'arbres de décision sur des données symboliques de type intervalle en utilisant le critère de découpage binaire de Kolmogorov-Smirnov. Nous proposons une approche permettant d'affecter un individu à la fois aux deux noeuds fils générés par le partitionnement d'un noeud non terminal. Le but de cette méthode est de prendre en compte le positionnement de la donnée à classer par rapport à la donnée seuil de coupure.	Edwin Diday, Chérif Mballo	http://editions-rnti.fr/render_pdf.php?p1&p=1000373	http://editions-rnti.fr/render_pdf.php?p=1000373
Revue des Nouvelles Technologies de l'Information	EGC	2006	Aide en gestion hospitalière par visualisation des composantes de non-pertinence		Bernard Huet	http://editions-rnti.fr/render_pdf.php?p1&p=1000431	http://editions-rnti.fr/render_pdf.php?p=1000431
Revue des Nouvelles Technologies de l'Information	EGC	2006	Algorithme semi-interactif pour la sélection de dimensions	Nous présentons un algorithme génétique semi-interactif de sélectionde dimensions dans les grands ensembles de données pour la détectiond'individus atypiques (outliers). Les ensembles de données possédant unnombre élevé de dimensions posent de nombreux problèmes aux algorithmesde fouille de données, une solution est d'effectuer un pré-traitement afin de neretenir que les dimensions "intéressantes". Nous utilisons un algorithmegénétique pour le choix du sous-ensemble de dimensions à retenir. Par ailleursnous souhaitons donner un rôle plus important à l'utilisateur dans le processusde fouille, nous avons donc développé un algorithme génétique semi-interactifoù l'évaluation des solutions n'élimine pas complètement la fonctiond'évaluation mais la couple avec une évaluation de l'utilisateur. Enfin,l'importante réduction du nombre de dimensions nous permet de visualiser lesrésultats de l'algorithme de détection d'outlier. Cette visualisation permet àl'expert des données d'étiqueter les éléments atypiques (erreurs ou simplementdes individus différents de la masse).	Lydia Boudjeloud, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000366	http://editions-rnti.fr/render_pdf.php?p=1000366
Revue des Nouvelles Technologies de l'Information	EGC	2006	Alignement extensionnel et asymétrique de hiérarchies conceptuelles par découverte d'implications entre concepts	Dans la littérature, de nombreux travaux traitent de méthodes d'alignementd'ontologies. Ils utilisent, pour la plupart, des relations basées sur desmesures de similarité qui ont la particularité d'être symétriques. Cependant, peude travaux évaluent l'intérêt d'utiliser des mesures d'appariement asymétriquesdans le but d'enrichir l'alignement produit. Ainsi, nous proposons dans ce papierune méthode d'alignement extensionnelle et asymétrique basée sur la découvertedes implications significatives entre deux ontologies. Notre approche,basée sur le modèle probabiliste d'écart à l'indépendance appelé intensité d'implication,est divisée en deux parties consécutives : (1) l'extraction, à partir ducorpus textuel associé à l'ontologie, et l'association des termes aux concepts;(2) la découverte et sélection des implications génératrices les plus significativesentre les concepts. La méthode proposée est évaluée sur deux jeux de donnéesréels portant respectivement sur des profils d'entreprises et sur des cataloguesde cours d'universités. Les résultats obtenus montrent que l'on peut trouver desrelations pertinentes qui sont ignorées par un alignement basé seulement sur desmesures de similarité.	Jérôme David, Fabrice Guillet, Régis Gras, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000337	http://editions-rnti.fr/render_pdf.php?p=1000337
Revue des Nouvelles Technologies de l'Information	EGC	2006	Amélioration des indicateurs techniques pour l'analyse du marché financier	La technique des motifs fréquents a été utilisée pour améliorer lepouvoir prédictif des stratégies quantitatives. Innovant dans le contexte desmarchés financiers, notre méthode associe une signature aux configurations demarché fréquentes. Un système de « trading » automatique sélectionne lesmeilleures signatures par une procédure de « back testing » itérative et les utiliseen combinaison avec l'indicateur technique pour améliorer sa performance.L'application des motifs fréquents à cette problématique des indicateurstechniques est une contribution originale. Au sens du test t de Student,notre méthode améliore nettement les approches sans signatures. La techniquea été testé sur des données journalières type taux d'intérêt et actions. Notreanalyse des indicateurs (Williams%R, BN et croisement des moments) a montréque qu'une approche par signatures est particulièrement bien adaptée auxstratégies à mémoire courte.	Hunor Albert-Lorincz, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000428	http://editions-rnti.fr/render_pdf.php?p=1000428
Revue des Nouvelles Technologies de l'Information	EGC	2006	Analyse du Comportement des utilisateurs exploitant une base de données vidéo	Dans cet article, nous présentons un modèle de fouille des usages dela vidéo pour améliorer la qualité de l'indexation. Nous proposons une approchebasée sur un modèle à deux niveaux représentant le comportement des utilisateursexploitant un moteur de recherche vidéo. Le premier niveau consiste àmodéliser le comportement lors de la lecture d'une vidéo unique (comportementintra vidéo), le second à modéliser le comportement sur l'ensemble d'une session(comportement inter video). A partir de cette représentation, nous avonsdéveloppé un algorithme de regroupement, adapté à la nature particulière de cesdonnées. L'analyse des usages de la vidéo nous permet d'affiner l'indexationvidéo sur la base de l'intérêt des utilisateurs.	Sylvain Mongy	http://editions-rnti.fr/render_pdf.php?p1&p=1000376	http://editions-rnti.fr/render_pdf.php?p=1000376
Revue des Nouvelles Technologies de l'Information	EGC	2006	Annotation sémantique de pages web	Cet article présente un système automatique d'annotation sémantiquede pages web. Les systèmes d'annotation automatique existants sont essentiellementsyntaxiques, même lorsque les travaux visent à produire une annotationsémantique. La prise en compte d'informations sémantiques sur le domaine pourl'annotation d'un élément dans une page web à partir d'une ontologie supposed'aborder conjointement deux problèmes : (1) l'identification de la structuresyntaxique caractérisant cet élément dans la page web et (2) l'identification duconcept le plus spécifique (en termes de subsumption) dans l'ontologie dontl'instance sera utilisée pour annoter cet élément. Notre démarche repose sur lamise en oeuvre d'une technique d'apprentissage issue initialement des wrappersque nous avons articulée avec des raisonnements exploitant la structure formellede l'ontologie.	Sylvain Tenier, Amedeo Napoli, Xavier Polanco, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1001499	http://editions-rnti.fr/render_pdf.php?p=1001499
Revue des Nouvelles Technologies de l'Information	EGC	2006	Apprentissage de la structure des réseaux bayésiens à partir des motifs fréquents corrélés : application à l'identification des facteurs environnementaux du cancer du Nasopharynx	L'apprentissage de structure des réseaux bayésien à partir de donnéesest un problème NP-difficile pour lequel de nombreuses heuristiques ont été proposées.Dans cet article, nous proposons une nouvelle méthode inspirée des travauxsur la recherche de motifs fréquents corrélés pour identifier les causalitésentre les variables. L'algorithme opère en quatre temps : (1) la découvertepar niveau des motifs fréquents corrélés minimaux ; (2) la construction d'ungraphe non orienté à partir de ces motifs ; (3) la détection des V_structures etl'orientation partielle du graphe ; (4) l'élimination des arêtes superflues par destests d'indépendance conditionnelle. La méthode, appliquée au réseau Asia, permetde retrouver la structure du graphe initial. Nous l'appliquons ensuite auxdonnées d'une étude épidémiologique cas-témoins du cancer du nasopharynx(NPC). L'objectif est de dresser un profil statistique type de la population étudiéeet d'apporter un éclairage utile sur les différents facteurs impliqués dans leNPC.	Alexandre Aussem, Zahra Kebaili, Marilys Corbex, Fabien De Marchi	http://editions-rnti.fr/render_pdf.php?p1&p=1000420	http://editions-rnti.fr/render_pdf.php?p=1000420
Revue des Nouvelles Technologies de l'Information	EGC	2006	Approche entropique pour l'analyse de modèle de chroniques	Cet article propose d'utiliser l'entropie informationnelle pouranalyser des modèles de chroniques découverts selon une approchestochastique (Bouché et Le Goc, 2005). Il décrit une adaptation de l'algorithmeTemporalID3 (Console et Picardi, 2003) permettant de découvrir des modèlesde chroniques à partir d'un ensemble d'apprentissage contenant des séquencesd'occurrences d'événements discrets. Ces séquences représentent des suitesd'alarmes générées par un système à base de connaissance de monitoring et dediagnostic de systèmes dynamiques. On montre sur un exemple que l'approcheentropique complète l'approche stochastique en identifiant les classesd'événements qui contribuent le plus significativement à la prédiction d'uneoccurrence d'une classe particulière.	Nabil Benayadi, Marc Le Goc, Philippe Bouché	http://editions-rnti.fr/render_pdf.php?p1&p=1000397	http://editions-rnti.fr/render_pdf.php?p=1000397
Revue des Nouvelles Technologies de l'Information	EGC	2006	ARABASE : Base de données Web pour l'exploitation en reconnaissance optique de l'écriture Arabe		Noura Bouzrara, Nacéra Madani Aissaoui, Najoua Essoukri Ben Amara	http://editions-rnti.fr/render_pdf.php?p1&p=1000448	http://editions-rnti.fr/render_pdf.php?p=1000448
Revue des Nouvelles Technologies de l'Information	EGC	2006	Arbres de décision multi modes et multi cibles.	Nous présentons une nouvelle méthode d'induction d'arbre de décision appelée MuMTree (pour Multi Models Tree) utilisable pour les modes d'apprentissage supervisé, non supervisé, supervisé à plusieurs variables cibles. Nous présentons les différents principes nécessaires pour réaliser un tel arbre de décision. Nous illustrons ensuite, sur un cas de modélisation multi-cibles, les avantages de cette méthode par rapport à un arbre de décision classique.	Frank Meyer, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000401	http://editions-rnti.fr/render_pdf.php?p=1000401
Revue des Nouvelles Technologies de l'Information	EGC	2006	Archiview, un outil de visualisation topographique des paramètres d'un hôpital		Pierre P. Lévy, Jean-Philippe Villaréal, Pierre-Paul Couka, Fabrice Gallois, Laurence Herbin, Antoine Flahault	http://editions-rnti.fr/render_pdf.php?p1&p=1000449	http://editions-rnti.fr/render_pdf.php?p=1000449
Revue des Nouvelles Technologies de l'Information	EGC	2006	Biclustering of Gene Expression Data Based on Local Nearness	The analysis of gene expression data in DNA chips is an importanttool used in genomic research whose main objectives range from the study ofthe functionality of specific genes and their participation in biological processto the reconstruction of diseases's conditions and their subsequent prognosis.Gene expression data are arranged in matrices where each gene corresponds toone row and every column represents one specific experimental condition. Thebiclustering techniques have the purpose of finding subsets of genes that showsimilar activity patterns under a subset of conditions. Our approach consists ofa biclustering algorithm based on local nearness. The algorithm searches forbiclusters in a greedy fashion, starting with two-genes biclusters and includingas much as possible depending on a distance threshold which guarantees thesimilarity of gene behaviors.	Jesús S. Aguilar-Ruiz, Domingo S. Rodríguez, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1000427	http://editions-rnti.fr/render_pdf.php?p=1000427
Revue des Nouvelles Technologies de l'Information	EGC	2006	Bordures statistiques pour la fouille incrémentale de données dans les Data Streams	Récemment la communauté Extraction de Connaissances s'est intéressée à de nouveaux modèles où les données arrivent séquentiellement sous la forme d'un flot rapide et continu, i.e. les data streams. L'une des particularités importantes de ces flots est que seule une quantité d'information partielle est disponible au cours du temps. Ainsi après différentes mises à jour successives, il devient indispensable de considérer l'incertitude inhérente à l'information retenue. Dans cet article, nous introduisons une nouvelle approche statistique en biaisant les valeurs supports pour les motifs fréquents. Cette dernière a l'avantage de maximiser l'un des deux paramètres (précision ou rappel) déterminés par l'utilisateur tout en limitant la dégradation sur le paramètre non choisi. Pour cela, nous définissons les notions de bordures statistiques. Celles-ci constituent les ensembles de motifs candidats qui s'avèrent très pertinents à utiliser dans le cas de la mise à jour incrémentale des streams. Les différentes expérimentations effectuées dans le cadre de recherche de motifs séquentiels ont montré l'intérêt de l'approche et le potentiel des techniques utilisées.	Jean-Emile Symphor, Pierre-Alain Laur	http://editions-rnti.fr/render_pdf.php?p1&p=1000417	http://editions-rnti.fr/render_pdf.php?p=1000417
Revue des Nouvelles Technologies de l'Information	EGC	2006	Carte auto-organisatrice probabiliste sur données binaires	Lesméthodes factorielles d'analyse exploratoire statistique définissentdes directions orthogonales informatives à partir d'un ensemble de données.Elles conduisent par exemple à expliquer les proximités entre individus à l'aided'un groupe de variables caractéristiques.Dans le contexte du datamining lorsqueles tableaux de données sont de grande taille, une méthode de cartographie synthétiques'avère intéressante. Ainsi une carte auto-organisatrice (SOM) est uneméthode de partitionnement munie d'une structure de graphe de voisinage -surles classes- le plus souvent planaire. Des travaux récents sont développés pourétendre le SOM probabiliste Generative Topographic Mapping (GTM) aux modèlesde mélanges classiques pour données discrètes. Dans ce papier nous présentonset étudions un modèle génératif symétrique de carte auto-organisatricepour données binaires que nous appelons Bernoulli Aspect Topological Model(BATM). Nous introduisons un nouveau lissage et accélérons la convergence del'estimation par une initialisation originale des probabilités en jeu.	Mohamed Nadif, Rodolphe Priam	http://editions-rnti.fr/render_pdf.php?p1&p=1000386	http://editions-rnti.fr/render_pdf.php?p=1000386
Revue des Nouvelles Technologies de l'Information	EGC	2006	Champs de Markov conditionnels pour le traitement de séquences	Les modèles conditionnels du type modèles de Markov d'entropiemaximale et champs de Markov conditionnels apportent des réponses auxlacunes des modèles de Markov cachés traditionnellement employés pour laclassification et la segmentation de séquences. Ces modèles conditionnels ontété essentiellement utilisés jusqu'à présent dans des tâches d'extractiond'information ou d'étiquetage morphosyntaxique. Cette contribution explorel'emploi de ces modèles pour des données de nature différente, de type« signal », telles que la parole ou l'écriture en ligne. Nous proposons desarchitectures de modèles adaptées à ces tâches pour lesquelles nous avonsdérivé les algorithmes d'inférence et d'apprentissage correspondant. Nousfournissons des résultats expérimentaux pour deux tâches de classification etd'étiquetage de séquences.	Thierry Artières, Trinh Minh Tri Do	http://editions-rnti.fr/render_pdf.php?p1&p=1000419	http://editions-rnti.fr/render_pdf.php?p=1000419
Revue des Nouvelles Technologies de l'Information	EGC	2006	Choix du taux d'élagage pour l'extraction de la terminologie. Une approche fondée sur les courbes ROC	Le choix du taux d'élagage est crucial dans le but d'acquérir une terminologiede qualité à partir de corpus de spécialité. Cet article présente uneétude expérimentale consistant à déterminer le taux d'élagage le plus adapté.Plusieurs mesures d'évaluation peuvent être utilisées pour déterminer ce tauxtels que la précision, le rappel et le Fscore. Cette étude s'appuie sur une autremesure d'évaluation qui semble particulièrement bien adaptée pour l'extractionde la terminologie : les courbes ROC (Receiver Operating Characteristics).	Mathieu Roche, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000347	http://editions-rnti.fr/render_pdf.php?p=1000347
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification de documents XML à partir d'une représentation linéaire des arbres de ces documents	Cet article présente un nouveau modèle de représentation pour la classificationde documents XML. Notre approche permet de prendre en compte soitla structure seule, soit la structure et le contenu de ces documents. L'idée estde représenter un document par l'ensemble des sous-chemins de l'arbre XMLde longueur comprise entre n et m, deux valeurs fixées a priori. Ces cheminssont ensuite considérés comme de simples mots sur lesquels on peut appliquerdes méthodes standards de classification, par exemple K-means. Nous évaluonsnotre méthode sur deux collections: la collection INEX et les rapports d'activitéde l'INRIA. Nous utilisons un ensemble de mesures bien connues dans le domainede la recherche d'information lorsque les classes sont connues a priori.Lorsqu'elles ne sont pas connues, nous proposons une analyse qualitative desrésultats qui s'appuie sur les mots (chemins) les plus caractéristiques des classesgénérées.	Anne-Marie Vercoustre, Mounir Fegas, Yves Lechevallier, Thierry Despeyroux	http://editions-rnti.fr/render_pdf.php?p1&p=1000384	http://editions-rnti.fr/render_pdf.php?p=1000384
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification des comptes-rendus mammographiques à partir d'une ontologie radiologique en OWL	Dans cet article, nous proposons un système de classification descomptes-rendus mammographiques, reposant sur une ontologie radiologiquedécrivant les signes radiologiques et les différentes classes de la classificationACR des systèmes BIRADS dans le langage OWL. Le système est conçu pour,extraire les faits issus des textes libres de comptes-rendus en étant dirigé parl'ontologie, puis inférer la classe correspondante et en déduire l'attitude à tenirà partir de la classification ACR. Ce travail présente la construction d'une ontologieradiologique mammaire dans le langage OWL et son intérêt pour classerautomatiquement les comptes-rendus de mammographies.	Amel Boustil, Zaïdi Sahnoun, Ziad Mansouri, Christine Golbreich	http://editions-rnti.fr/render_pdf.php?p1&p=1000346	http://editions-rnti.fr/render_pdf.php?p=1000346
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification d'un tableau de contingence et modèle probabiliste	Ces dernières années, la classification croisée ou classification parblocs, c'est-à-dire la recherche simultanée d'une partition des lignes et d'unepartition des colonnes d'un tableau de données, est devenue un outil très utiliséen fouille de données. Dans ce domaine, l'information se présente souvent sousforme de tableaux de contingence ou tableaux de co-occurrence croisant les modalitésde deux variables qualitatives. Dans cet article, nous étudions le problèmede la classification croisée de ce type de données en nous appuyant sur un modèlede mélange probabiliste. En utilisant l'approche vraisemblance classifiante,nous proposons un algorithme de classification croisée basé sur la maximisationalternée de la vraisemblance associée à deux mélanges multinomiaux classiqueset nous montrons alors que sous certaines contraintes restrictives, on retrouveles critères du Chi2 et de l'information mutuelle. Des résultats sur des donnéessimulées et des données réelles illustrent et confirment l'efficacité et l'intérêt decette approche.	Gérard Govaert, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000387	http://editions-rnti.fr/render_pdf.php?p=1000387
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification non-supervisée de données relationnelles		Jérôme Maloberti, Shin Ando, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1000375	http://editions-rnti.fr/render_pdf.php?p=1000375
Revue des Nouvelles Technologies de l'Information	EGC	2006	Classifications hiérarchiques factorielles de variables	On présente deux méthodes de classification hiérarchique ascendantede variables quantitatives et de fréquences. Chaque noeud de ces hiérarchiesregroupe deux classes de variables à partir d'une analyse factorielle particulièrebasée sur les variables représentatives de ces deux classes. Par cette méthode,on dispose, à chaque pas, d'un plan factoriel permettant de représenter àla fois les variables des deux classes fusionnées et l'ensemble des individus.Ces derniers se positionnent dans ce plan suivant leurs valeurs pour les variablesconsidérées. Ainsi, l'interprétation des noeuds obtenus s'effectue facilementà partir de l'examen de ces représentations factorielles. La répartition desindividus observée dans chacun de ces plans factoriels permet également dedéfinir une segmentation des individus en total accord avec la hiérarchie desvariables obtenues. On montre le fonctionnement des méthodes sur des exemplesréels.	Sergio Camiz, Jean-Jacques Denimal 	http://editions-rnti.fr/render_pdf.php?p1&p=1000374	http://editions-rnti.fr/render_pdf.php?p=1000374
Revue des Nouvelles Technologies de l'Information	EGC	2006	Clustering dynamique d'un flot de données : un algorithme incrémental et optimal de détection des maxima de densité	L'extraction non supervisée et incrémentale de classes sur un flot dedonnées (data stream clustering) est un domaine en pleine expansion. La plupartdes approches visent l'efficacité informatique. La nôtre, bien que se prêtantà un passage à l'échelle en mode distribué, relève d'une problématiquequalitative, applicable en particulier au domaine de la veille informationnelle :faire apparaître les évolutions fines, les « signaux faibles », à partir des thématiquesextraites d'un flot de documents. Notre méthode GERMEN localise defaçon exhaustive les maxima du paysage de densité des données à l'instant t,en identifiant les perturbations locales du paysage à t-1 et modifications defrontières induites par le document présenté. Son caractère optimal provient deson exhaustivité (à une valeur du paramètre de localité correspond un ensembleunique de maxima, et un découpage unique des classes qui la rend indépendantede tout paramètre d'initialisation et de l'ordre des données.	Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1000320	http://editions-rnti.fr/render_pdf.php?p=1000320
Revue des Nouvelles Technologies de l'Information	EGC	2006	Combinaison de l'approche inductive (progressive) et linguistique pour l'étiquetage morphosyntaxique des corpus de spécialité	Les étiqueteurs morphosyntaxiques sont de plus en plus performantset cependant, un véritable problème apparaît lorsque nous voulons étiqueterdes corpus de spécialité pour lesquels nous n'avons pas de corpus annotés. Lacorrection des ambiguïtés difficiles est une étape importante pour obtenir uncorpus de spécialité parfaitement étiqueté. Pour corriger ces ambiguïtés et diminuerle nombre de fautes, nous utilisons une approche itérative appelée InductionProgressive. Cette approche est une combinaison d'apprentissage automatique,de règles rédigées par l'expert et de corrections manuelles qui secombinent itérativement afin d'obtenir une amélioration de l'étiquetage tout enrestreignant les actions de l'expert à la résolution de problèmes de plus en plusdélicats. L'approche proposée nous a permis d'obtenir un corpus de biologiemoléculaire « correctement » étiqueté. En utilisant ce corpus, nous avons effectuéune étude comparative de quatre étiqueteurs supervisés.	Ahmed Amrani, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000354	http://editions-rnti.fr/render_pdf.php?p=1000354
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comment formaliser les connaissances tacites d'une organisation ? Le cas de la conduite du changement à la SNCF		Anne Remillieux, Christian Blatter	http://editions-rnti.fr/render_pdf.php?p1&p=1000440	http://editions-rnti.fr/render_pdf.php?p=1000440
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison de deux modes de représentation de données faiblement structurées en sciences du vivant	Cet article présente deux modes de représentation de l'informationdans le cadre d'une problématique en sciences du vivant. Le premier, appliqué àla microbiologie prévisionnelle, s'appuie sur deux formalismes, le modèle relationnelet les graphes conceptuels, interrogés uniformément via une même interface.Le second, appliqué aux technologies des céréales, utilise le seul modèlerelationnel. Cet article décrit les caractéristiques des données et compare les solutionsde représentation adoptées dans les deux systèmes.	Rallou Thomopoulos, Patrice Buche, Ollivier Haemmerlé, Frédéric Mabille, Nongyao Mueangdee	http://editions-rnti.fr/render_pdf.php?p1&p=1000332	http://editions-rnti.fr/render_pdf.php?p=1000332
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison de dissimilarités pour l'analyse de l'usage d'un site web	L'obtention d'une classification des pages d'un site web en fonctiondes navigations extraites des fichiers "logs" du serveur peut s'avérer très utilepour évaluer l'adéquation entre la structure du site et l'attente des utilisateurs. Onconstruit une telle typologie en s'appuyant une mesure de dissimilarité entre lespages, définie à partir des navigations. Le choix de la mesure la plus appropriéeà l'analyse du site est donc fondamental. Dans cet article, nous présentons unsite de petite taille dont les pages sont classées en catégories sémantiques parun expert. Nous confrontons ce classement aux partitions obtenues à partir dediverses dissimilarités afin d'en étudier les avantages et inconvénients.	Fabrice Rossi, Francisco de Assis Tenório de Carvalho, Yves Lechevallier, Alzennyr Da Silva	http://editions-rnti.fr/render_pdf.php?p1&p=1000378	http://editions-rnti.fr/render_pdf.php?p=1000378
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison des mammographies par des méthodes d'apprentissage		Irina Diana Coman, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000433	http://editions-rnti.fr/render_pdf.php?p=1000433
Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison des mesures d'intérêt de règles d'association : une approche basée sur des graphes de corrélation	Le choix des mesures d'intérêt (MI) afin d'évaluer les règles d'associationest devenu une question importante pour le post-traitement des connaissanceen ECD. Dans la littérature, de nombreux auteurs ont discuté et comparéles propriétés des MI afin d'améliorer le choix des meilleures mesures. Cependant,il s'avère que la qualité d'une règle est contextuelle : elle dépend à la fois dela structure de données et des buts du décideur. Ainsi, certaines mesures peuventêtre appropriées dans un certain contexte, mais pas dans d'autres. Dans cet article,nous présentons une nouvelle approche contextuelle mise en applicationpar un nouvel outil, ARQAT, permettant à un décideur d'évaluer et de comparerle comportement des MI sur ses jeux de données spécifiques. Cette approche estbasée sur l'analyse visuelle d'un graphe de corrélation entre des MI objectives.Nous employons ensuite cette approche afin de comparer et de discuter le comportementde trente-six mesures d'intérêt sur deux ensembles de données a prioritrès opposés : un premier dont les données sont fortement corrélées et un secondaux données faiblement corrélées. Alors que nous attendions des différences importantesentre les graphes de corrélation de ces deux jeux d'essai, nous avonspu observer des stabilités de corrélation entre certaines MI qui sont révélatricesde propriétés indépendantes de la nature des données observées. Ces stabilitéssont récapitulées et analysées.	Xuan-Hiep Huynh, Fabrice Guillet, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000404	http://editions-rnti.fr/render_pdf.php?p=1000404
Revue des Nouvelles Technologies de l'Information	EGC	2006	Confrontation de Points de Vue dans le système Porphyry		Samuel Gesche, Sylvie Calabretto, Guy Caplat	http://editions-rnti.fr/render_pdf.php?p1&p=1000444	http://editions-rnti.fr/render_pdf.php?p=1000444
Revue des Nouvelles Technologies de l'Information	EGC	2006	Credit scoring, statistique et apprentissage	Basel 2 regulations brought new interest in supervised classification methodologies for predicting default probability for loans. An important feature of consumer credit is that predictors are generally categorical. Logistic regression and linear discriminant analysis are the most frequently used techniques but are often unduly opposed. Vapnik's statistical learning theory explains why a prior dimension reduction (eg by means of multiple correspondence analysis) improves the robustness of the score function. Ridge regression, linear SVM, PLS regression are also valuable competitors. Predictive capability is measured by AUC or Gini's index which are related to the well known non-parametric Wilcoxon-Mann-Whitney test. Among methodological problems, reject inference is an important one, since most samples are subject to a selection bias. There are many methods, none being satisfactory. Distinguish between good and bad customers is not enough, especially for long-term loans. The question is then not only "if", but "when" the customers default. Survival analysis provides new types of scores.	Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1000316	http://editions-rnti.fr/render_pdf.php?p=1000316
Revue des Nouvelles Technologies de l'Information	EGC	2006	Critère VT100 de sélection des règles d'association	L'extraction de règles d'association génère souvent un grand nombrede règles. Pour les classer et les valider, de nombreuses mesures statistiquesont été proposées ; elles permettent de mettre en avant telles ou telles caractéristiquesdes règles extraites. Elles ont pour point commun d'être fonctioncroissante du nombre de transactions et aboutissent bien souvent àl'acceptation de toutes les règles lorsque la base de données est de grandetaille. Dans cet article, nous proposons une mesure inspirée de la notion de valeur-test. Elle présente comme principale caractéristique d'être insensible à lataille de la base, évitant ainsi l'écueil des règles fallacieusement significatives.Elle permet également de mettre sur un même pied, et donc de les comparer,des règles qui auront été extraites de bases de données différentes. Elle permetenfin de gérer différents seuils de signification des règles. Le comportement dela mesure est détaillé sur un exemple.	Alain Morineau, Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1000409	http://editions-rnti.fr/render_pdf.php?p=1000409
Revue des Nouvelles Technologies de l'Information	EGC	2006	De l'analyse didactique à la modélisation informatique pour la conception d'un EIAH en chirurgie orthopédique	L'objet de la recherche présentée est de concevoir un environnementinformatique d'apprentissage qui permette de réduire l'écart entre la formationthéorique des chirurgiens et leur formation pratique, qui se dérouleprincipalement sur le mode du compagnonnage. L'article expose laméthodologie et quelques illustrations du travail didactique d'analyse desconnaissances et du système d'enseignement / apprentissage en milieuhospitalier (chirurgie orthopédique) ainsi que partie de la formalisationinformatique de cette connaissance. Cette modélisation permet la prise encompte dans l'environnement informatique de connaissances pragmatiquespour le diagnostic des connaissances de l'utilisateur en fonction des actionsqu'il effectue à l'interface pendant la résolution d'un problème (pose de visdans le bassin), et la prise de décision didactique qui suit : quelle rétroactionfournir pour affiner le diagnostic, et/ou permettre l'apprentissage souhaité.	Vanda Luengo, Lucile Vadcard, Dima Mufti-Alchawafa	http://editions-rnti.fr/render_pdf.php?p1&p=1000422	http://editions-rnti.fr/render_pdf.php?p=1000422
Revue des Nouvelles Technologies de l'Information	EGC	2006	Définition et diffusion de signatures sémantiques dans les systèmes pair-à-pair	Les systèmes pair-à-pair (peer-to-peer, P2P, égal-à-égal) se sont popularisésces dernières années avec les systèmes de partage de fichiers sur Internet.De nombreuses recherches concernant l'optimisation de la localisationdes données ont émergé et constituent un axe de recherche très actif. La priseen compte de la sémantique du contenu des pairs dans le routage des requêtespermet d'améliorer considérablement la localisation des données. Nous nousconcentrons sur l'approche PlanetP, faisant usage de la notion de filtre de Bloom,qui consiste à propager une signature sémantique des pairs (filtres de Bloom) àtravers le réseau. Nous présentons cette approche et en proposons une amélioration: la création de filtres de Bloom dynamiques, dans le sens où leur tailledépend de la charge des pairs (nombre de documents partagés).	Raja Chiky, Bruno Defude, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000388	http://editions-rnti.fr/render_pdf.php?p=1000388
Revue des Nouvelles Technologies de l'Information	EGC	2006	Des motifs séquentiels généralisés aux contraintes de temps étendues	Dans de nombreux domaines, la recherche de connaissances temporellesest très appréciée. Des techniques ont été proposées aussi bien en fouille dedonnées qu'en apprentissage, afin d'extraire et de gérer de telles connaissances,en les associant également à la spécification de contraintes temporelles (e.g.: fenêtretemporelle maximale), notamment dans le contexte de la recherche de motifsséquentiels. Cependant, ces contraintes sont souvent trop rigides ou nécessitentune bonne connaissance du domaine pour ne pas extraire des informationserronées. C'est pourquoi nous proposons une approche basée sur la constructionde graphes de séquences afin de prendre en compte des contraintes de tempsplus souples. Ces contraintes sont relâchées par rapport aux contraintes de tempsprécédemment proposées. Elles permettent donc d'extraire plus de motifs pertinents.Afin de guider l'analyse des motifs obtenus, nous proposons égalementun niveau de précision des contraintes temporelles pour les motifs extraits.	Maguelonne Teisseire, Céline Fiot, Anne Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000415	http://editions-rnti.fr/render_pdf.php?p=1000415
Revue des Nouvelles Technologies de l'Information	EGC	2006	EDA : algorithme de désuffixation du langage médical		Didier Nakache, Elisabeth Métais, Annabelle Dierstein	http://editions-rnti.fr/render_pdf.php?p1&p=1000429	http://editions-rnti.fr/render_pdf.php?p=1000429
Revue des Nouvelles Technologies de l'Information	EGC	2006	Enrichissement d'ontologies dans le secteur de l'eau douce en environnement Internet distribué et multilingue		Lylia Abrouk, Mathieu Lafourcade	http://editions-rnti.fr/render_pdf.php?p1&p=1000432	http://editions-rnti.fr/render_pdf.php?p=1000432
Revue des Nouvelles Technologies de l'Information	EGC	2006	ESIEA Datalab Logiciel de Nettoyage et Préparation de Données		Christopher Corsia	http://editions-rnti.fr/render_pdf.php?p1&p=1000451	http://editions-rnti.fr/render_pdf.php?p=1000451
Revue des Nouvelles Technologies de l'Information	EGC	2006	Exploration des paramètres discriminants pour les représentations vectorielles de la sémantique des mots	Les méthodes de représentation sémantique des mots à partir d'une analyse statistique sont basées sur des comptes de co-occurences entre mots et unités textuelles. Ces méthodes ont des paramétrages complexes, notamment le type d'unité textuelle utilisée comme contexte. Ces paramètres déterminent fortement la qualité des résultats obtenus. Dans cet article, nous nous intéressons au paramètrage de la technique dite Hyperspace Analogue to Language (HAL).Nous proposons une nouvelle méthode pour explorer ses paramètres discriminants. Cette méthode est basée sur l'analyse d'un graphe de voisinage d'une liste de mots de référence pré-classés. Nous expérimentons cette méthode et en donnons les premiers résultats qui renforcent et complètent des résultats issus de travaux précédents.	Frank Meyer, Vincent Dubois	http://editions-rnti.fr/render_pdf.php?p1&p=1000360	http://editions-rnti.fr/render_pdf.php?p=1000360
Revue des Nouvelles Technologies de l'Information	EGC	2006	Exploration interactive de bases de connaissances : un retour d'expérience	La navigation au sein de bases de connaissances reste un problèmeouvert. S'il existe plusieurs paradigmes de visualisation, peu de travaux sur lesretours d'expérience sont disponibles. Dans le cadre de cet article nous noussommes intéressés aux différents paradigmes de navigation interactive au seinde bases documentaires annotées sémantiquement ; l'accès à la base deconnaissances s'effectuant à travers l'ontologie du domaine d'application. Cesparadigmes ont été évalués dans le cadre d'une application industrielle(mécanique des fluides et échangeurs thermiques) en fonction de critèresdéfinis par les utilisateurs. L'analyse des retours d'expérience1 nous a permisde spécifier et de réaliser un nouveau navigateur dédié à la gestion dedocuments techniques annotés par une ontologie de domaine : le « Eye Tree »,navigateur de type « polar fisheye view ».	Christophe Tricot, Christophe Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000362	http://editions-rnti.fr/render_pdf.php?p=1000362
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extension de l'algorithme CURE aux fouilles de données volumineuses		Jerzy Korczak, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000402	http://editions-rnti.fr/render_pdf.php?p=1000402
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction automatique de champs numériques dans des documents manuscrits	Nous décrivons dans cet article une chaine de traitement complète etgénérique permettant d'extraire automatiquement les champs numériques (numérosde téléphone, codes clients, codes postaux) dans des documents manuscritslibres. Notre chaïne de traitement est constituée des trois étapes suivantes:localisation des champs numériques potentiels selon une approche markoviennesans reconnaissance chiffre ni segmentation, reconnaissance des séquences extraites,et vérification des hypothèses de localisation / reconnaissance en vue delimiter la fausse alarme génerée lors de l'étape de localisation. L'évaluation denotre système sur une base de 300 courriers manuscrits montre des performancesen rappel-précision intéressantes.	Clément Chatelain, Laurent Heutte, Thierry Paquet	http://editions-rnti.fr/render_pdf.php?p1&p=1000319	http://editions-rnti.fr/render_pdf.php?p=1000319
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction d'objets vidéo : Une approche combinant les contours actifs et le flot optique	Dans cet article, nous présentons une méthode mixte de segmentationd'objets visuels dans une séquence d'images d'une vidéo combinant à la foisune segmentation basée régions et l'estimation de mouvement par flot optique.L'approche développée est basé sur une minimisation d'une fonctionnelled'énergie (E) qui fait intervenir les probabilités d'appartenance (densité) avecune gaussienne, en tenant compte des informations perceptuelles de couleur etde texture des régions d'intérêt. Pour améliorer la méthode de détection et desuivi, nous avons étendu la formulation énergétique de notre modèle decontour actif en incluant une force supplémentaire issue du calcul du flot optique.Nous montrons l'intérêt de cette approche mixte en terme de temps de calculet d'extraction d'objets vidéo complexes, et nous présentons les résultatsobtenus sur des séquences de corpus vidéo couleur.	Youssef Zinbi, Youssef Chahir, Abder Elmoatz	http://editions-rnti.fr/render_pdf.php?p1&p=1000321	http://editions-rnti.fr/render_pdf.php?p=1000321
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction de motifs séquentiels dans les flots de données d'usage du Web	Ces dernières années, de nouvelles contraintes sont apparues pour lestechniques de fouille de données. Ces contraintes sont typiques d'un nouveaugenre de données : les "data streams". Dans un processus de fouille appliquésur un data stream, l'utilisation de la mémoire est limitée, de nouveaux élémentssont générés en permanence et doivent être traités le plus rapidement possible,aucun opérateur bloquant ne peut être appliqué sur les données et celles-ci nepeuvent être observées qu'une seule fois. A l'heure actuelle, la majorité des travauxrelatifs à l'extraction de motifs dans les data streams ne concernent pas lesmotifs temporels. Nous montrons dans cet article que cela est principalement dûau phénomène combinatoire qui est lié à l'extraction de motifs séquentiels. Nousproposons alors un algorithme basé sur l'alignement de séquences pour extraireles motifs séquentiels dans les data streams. Afin de respecter la contrainte d'unepasse unique sur les données, une heuristique gloutonne est proposée pour segmenterles séquences. Nous montrons enfin que notre proposition est capabled'extraire des motifs pertinents avec un support très faible.	Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000418	http://editions-rnti.fr/render_pdf.php?p=1000418
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction de relations dans les documents Web	Nous présentons un système pour l'inférence de programmes d'extraction de relations dans les documents Web. Il utilise les vues textuelle et structurelle sur les documents. L'extraction des relations est incrémentale et utilise des méthodes de composition et d'enrichissement. Nous montrons que notre système est capable d'extraire des relations pour les organisations existantes dans les documents Web (listes,  tables, tables tournées, tables croisées).	Rémi Gilleron, Patrick Marty, Marc Tommasi, Fabien Torre	http://editions-rnti.fr/render_pdf.php?p1&p=1000380	http://editions-rnti.fr/render_pdf.php?p=1000380
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction et identification d'entités complexes à partir de textes biomédicaux	Nous présentons ici un système d'extraction et d'identification d'entitésnommées complexes à l'intention des corpus de spécialité biomédicale. Nousavons développé une méthode qui repose sur une approche mixte à base d'ensemblede règles a priori et de dictionnaires contrôlés. Cet article expose lestechniques que nous avons mises en place pour éviter ou minimiser les problèmesde synonymie, de variabilité des termes et pour limiter la présence denoms ambigus. Nous décrivons l'intégration de ces méthodes au sein du processusde reconnaissance des entités nommées. L'intérêt de cet outil réside dans lacomplexité et l'hétérogénéité des entités extraites. Cette méthode ne se limitepas à la détection des noms des gènes ou des protéines, mais s'adapte à d'autresdescripteurs biomédicaux. Nous avons expérimenté cette approche en mesurantles performances obtenues sur le corpus de référence GENIA.	Julien Lorec, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1000350	http://editions-rnti.fr/render_pdf.php?p=1000350
Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction multilingue de termes à partir de leur structure morphologique		Delphine Bernhard	http://editions-rnti.fr/render_pdf.php?p1&p=1000356	http://editions-rnti.fr/render_pdf.php?p=1000356
Revue des Nouvelles Technologies de l'Information	EGC	2006	FaBR-CL : méthode de classification croisée de protéines	Dans cet article, nous proposons une méthode de classification croiséepermettant de classer des protéines, d'une part, et de classer des descripteurs (3-grammes) selon leurs pertinences par rapport aux groupes de protéines obtenus,d'autres part.	Walid Erray, Faouzi Mhamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1000446	http://editions-rnti.fr/render_pdf.php?p=1000446
Revue des Nouvelles Technologies de l'Information	EGC	2006	Faire vivre un référentiel métier dans l'industrie : le système de gestion de connaissances ICARE	La gestion des connaissances, enjeu majeur pour l'industrie, est entréedans une phase concrète de déploiement. La conjonction d'une maturitédes organisations dans la maîtrise de leur métier, la consolidation de méthodeset les outils évolutifs pour faire vivre un patrimoine de connaissances favorisentl'émergence de projets significatifs et leur diffusion opérationnelle au seinde grands groupes industriels. ICARE chez PSA Peugeot Citroën réalisé avecl'environnement Ardans Knowledge Maker en est ici l'exemple.	Alain Berger, Pierre Mariot, Christophe Coppens, Julien Laroque Malbert	http://editions-rnti.fr/render_pdf.php?p1&p=1000450	http://editions-rnti.fr/render_pdf.php?p=1000450
Revue des Nouvelles Technologies de l'Information	EGC	2006	Fast-MGB : Nouvelle Base Générique Minimale de Règles Associatives	Le problème de l'exploitation des règles associatives est devenu primordial,puisque le nombre des règles associatives extraites des jeux de donnéesréelles devient très élevé. Une solution possible consiste à ne dériver qu'unebase générique de règles associatives. Cet ensemble de taille réduite permet degénérer toutes les règles associatives via un système axiomatique adéquat. Danscet article, nous proposons une nouvelle approche FAST-MGB qui permet dedériver, directement à partir du contexte d'extraction formel, une base génériqueminimale de règles associatives.	Cherif Chiraz Latiri, Lamia Ben Ghezaiel, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000349	http://editions-rnti.fr/render_pdf.php?p=1000349
Revue des Nouvelles Technologies de l'Information	EGC	2006	Finding fragments of orders and total orders from 0-1 data	High-dimensional collections of 0-1 data occur in many applications. The attributes insuch data sets are typically considered to be unordered. However, in many cases there is anatural total or partial order underlying the variables of the data set. Examples of variablesfor which such orders exist include terms in documents and paleontological sites in fossil datacollections. We describe methods for finding fragments of total orders from such data, basedon finding frequently occurring patterns. We also discuss techniques for finding good totalorderings (seriation) based on spectral ordering and MCMC methods	Heikki Mannila	http://editions-rnti.fr/render_pdf.php?p1&p=1000315	http://editions-rnti.fr/render_pdf.php?p=1000315
Revue des Nouvelles Technologies de l'Information	EGC	2006	Fouille de données dans les systèmes Pair-à-Pair pour améliorer la recherche de ressources	La quantité de sources d'information disponible sur Internet fait dessystèmes d'échanges pair-à-pair (P2P) un genre nouveau d'architecture qui offreà une large communauté des applications pour partager des fichiers, des calculs,dialoguer ou communiquer en temps réel. Dans cet article, nous proposonsune nouvelle approche pour améliorer la localisation d'une ressource sur un réseauP2P non structuré. En utilisant une nouvelle heuristique, nous proposonsd'extraire des motifs qui apparaissent dans un grand nombre de noeuds du réseau.Cette connaissance est très utile pour proposer aux utilisateurs des fichierssouvent demandés (en requête ou en téléchargement) et éviter une trop grandeconsommation de la bande passante.	Florent Masseglia, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000390	http://editions-rnti.fr/render_pdf.php?p=1000390
Revue des Nouvelles Technologies de l'Information	EGC	2006	Fouille de données spatiales, Approche basée sur la programmation logique inductive	Ce qui caractérise la fouille de données spatiales est la nécessité de prendre en compte les interactions des objets dans l'espace. Les méthodes classiques de fouille de données sont mal adaptées pour ce type d'analyse. Nous proposons dans cet article une approche basée sur la programmation logique inductive. Elle se base sur deux idées. La première consiste à matérialiser ces interactions spatiales dans des tables de distances, ramenant ainsi la fouille de données spatiales à la fouille de fonnées multi-tables. La seconde transforme les données en logique du premier ordre et applique ensuite la programmation logique inductive. Cet article présentera cette approche. Il décrira son application à la classification supervisée par arbre de décision spatial. Il présentera aussi les expérimentations réalisées et les résultats obtenus sur l'analyse de la contamination des coquillages dans la lagune de Thau.	Nadjim Chelghoum, Karine Zeitouni, Thierry Laugier, Annie Fiandrino, Lionel Loubersac	http://editions-rnti.fr/render_pdf.php?p1&p=1000400	http://editions-rnti.fr/render_pdf.php?p=1000400
Revue des Nouvelles Technologies de l'Information	EGC	2006	Gestion de connaissances : Compétences et ressources pédagogiques		Olivier Gerbé, Thierno Diarra, Jacques Raynauld	http://editions-rnti.fr/render_pdf.php?p1&p=1000335	http://editions-rnti.fr/render_pdf.php?p=1000335
Revue des Nouvelles Technologies de l'Information	EGC	2006	Graphes de voisinage pour l'indexation et l'interrogation d'images par le contenu	La découverte d'informations cachées dans les bases de données multimédiasest une tâche difficile à cause de leur structure complexe et à la subjectivitéliée à leur interprétation. Face à cette situation, l'utilisation d'un indexest primordiale. Un index multimédia permet de regrouper les données selondes critères de similarité. Nous proposons dans cet article d'apporter une améliorationà une approche déjà existante d'interrogation d'images par le contenu .Nous proposons une méthode efficace pour mettre à jour, localement, les graphesde voisinage qui constituent notre structure d'index multimédia. Cette méthodeest basée sur une manière intelligente de localisation de points dans un espacemultidimensionnel. Des résultats prometteurs sont obtenus après des expérimentationssur diverses bases de données.	Hakim Hacid, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000318	http://editions-rnti.fr/render_pdf.php?p=1000318
Revue des Nouvelles Technologies de l'Information	EGC	2006	I-Semantec : une plateforme collaborative de capitalisation des connaissances métier en conception de produits industriels		Mohamed-Foued Sriti, Philippe Boutinaud, Nada Matta, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1000441	http://editions-rnti.fr/render_pdf.php?p=1000441
Revue des Nouvelles Technologies de l'Information	EGC	2006	Indexation de vues virtuelles dans un médiateur XML pour le traitement de XQuery Text	Intégrer le traitement de requêtes de recherche d'information dans unmédiateur XML est un problème difficile. Ceci est notamment dû au fait quecertaines sources de données ne permettent pas de recherche sur mot-clefs etdistance ni de classer les résultats suivant leur pertinence. Dans cet article nousabordons l'intégration des fonctionnalités principales du standard XQuery Textdans XLive, un médiateur XML/XQuery. Pour cela nous avons choisid'indexer des vues virtuelles de documents. Les documents virtuelssélectionnés sont transformés en objets des sources. L'opérateur de sélectiondu médiateur est étendu pour supporter des recherches d'information sur lesdocuments de la vue. La recherche sur mots-clefs et le classement de résultatsont ainsi supportés. Notre formule de classement de résultats est adaptée auformat de données semi-structurées, basé sur le nombre de mots-clefs dans lesdifférents éléments et la distance entre les éléments d'un résultat.	Clément Jamard, Georges Gardarin	http://editions-rnti.fr/render_pdf.php?p1&p=1000325	http://editions-rnti.fr/render_pdf.php?p=1000325
Revue des Nouvelles Technologies de l'Information	EGC	2006	Interrogation et Vérification de documents OWL dans le modèle des Graphes Conceptuels	OWL est un langage pour la description d'ontologies sur le Web. Cependant,en tant que langage, OWL ne fournit aucun moyen pour interpréter lesontologies qu'il décrit, et étant orienté machine, il reste difficilement compréhensiblepar l'humain. On propose une approche de visualisation, d'interrogationet de vérification de documents OWL, regroupées dans un unique environnementgraphique : le modèle des graphes conceptuels.	Thomas Raimbault, Henri Briand, Rémi Lehn, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000342	http://editions-rnti.fr/render_pdf.php?p=1000342
Revue des Nouvelles Technologies de l'Information	EGC	2006	La fouille de graphes dans les bases de données réactionnelles au service de la synthèse en chimie organique	La synthèse en chimie organique consiste à concevoir de nouvellesmolécules à partir de réactifs et de réactions. Les experts de la synthèse s'appuientsur de très grandes bases de données de réactions qu'ils consultent à traversdes procédures d'interrogation standard. Un processus de découverte denouvelles réactions leur permettrait de mettre au point de nouveaux procédés desynthèse. Cet article présente une modélisation des réactions par des graphes etintroduit une méthode de fouille de ces graphes de réaction qui permet de faireémerger des motifs génériques utiles à la prédiction de nouvelles réactions. Enfinl'article fait le point sur l'état actuel de ce travail de recherche en présentantle modèle général dans lequel s'intégrera un nouvel algorithme de fouille deréactions chimiques.	Frédéric Pennerath, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000398	http://editions-rnti.fr/render_pdf.php?p=1000398
Revue des Nouvelles Technologies de l'Information	EGC	2006	Le forage distribué des données : une méthode simple, rapide et efficace	Dans cet article nous nous attaquons au problème du forage de trèsgrandes bases de données distribuées. Le résultat visé est un modèle qui soit etprédictif et descriptif, appelé méta-classificateur. Pour ce faire, nous proposonsde miner à distance chaque base de données indépendamment. Puis, il s'agitde regrouper les modèles produits (appelés classificateurs de base), sachant quechaque forage produira un modèle prédictif et descriptif, représenté pour nos besoinspar un ensemble de règles de classification. Afin de guider l'assemblage del'ensemble final de règles, qui sera l'union des ensembles individuels de règles,un coefficient de confiance est attribué à chaque règle de chaque ensemble. Cecoefficient, calculé par des moyens statistiques, représente la confiance que nouspouvons avoir dans chaque règle en fonction de sa couverture et de son taux d'erreurface à sa capacité d'être appliquée correctement sur de nouvelles données.Nous démontrons dans cet article que, grâce à ce coefficient de confiance, l'agrégationpure et simple de tous les classificateurs de base pour obtenir un agrégatde règles produit un méta-classificateur rapide et efficace par rapport aux techniquesexistantes.	Mohamed Aounallah, Guy Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000328	http://editions-rnti.fr/render_pdf.php?p=1000328
Revue des Nouvelles Technologies de l'Information	EGC	2006	Maintaining an Online Bibliographical Database: The Problem of Data Quality	CiteSeer and Google-Scholar are huge digital libraries which provideaccess to (computer-)science publications. Both collections are operated likespecialized search engines, they crawl the web with little human interventionand analyse the documents to classify them and to extract some metadata fromthe full texts. On the other hand there are traditional bibliographic data baseslike INSPEC for engineering and PubMed for medicine. For the field of computerscience the DBLP service evolved from a small specialized bibliographyto a digital library covering most subfields of computer science. The collectionsof the second group are maintained with massive human effort. On the longterm this investment is only justified if data quality of the manually maintainedcollections remains much higher than that of the search engine style collections.In this paper we discuss management and algorithmic issues of data quality. Wefocus on the special problem of person names	Michael Ley, Patrick Reuther	http://editions-rnti.fr/render_pdf.php?p1&p=1000317	http://editions-rnti.fr/render_pdf.php?p=1000317
Revue des Nouvelles Technologies de l'Information	EGC	2006	Méthode de récolte de traces de navigation sur interface graphique et visualisation de parcours		Marc Damez	http://editions-rnti.fr/render_pdf.php?p1&p=1000452	http://editions-rnti.fr/render_pdf.php?p=1000452
Revue des Nouvelles Technologies de l'Information	EGC	2006	Modèle conceptuel pour bases de données multidimensionnelles annotées	Nos travaux visent à proposer une mémoire d'expertises décisionnellespermettant de conserver et de manipuler non seulement les données décisionnellesmais aussi l'expertise analytique des décideurs. Les données décisionnellessont représentées au travers de concepts multidimensionnels etl'expertise associée est matérialisée grâce au concept d'annotation	Guillaume Cabanac, Max Chevalier, Franck Ravat, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000330	http://editions-rnti.fr/render_pdf.php?p=1000330
Revue des Nouvelles Technologies de l'Information	EGC	2006	Modèle décisionnel basé sur la qualité des données pour sélectionner les règles d'associations légitimement intéressantes	Dans cet article nous proposons d'exploiter des mesures décrivant laqualité des données pour définir la qualité des règles d'associations résultantd'un processus de fouille. Nous proposons un modèle décisionnel probabilistebasé sur le coût de la sélection de règles légitimement, potentiellement intéressantesou inintéressantes si la qualité des données à l'origine de leur calcul estbonne, moyenne ou douteuse. Les expériences sur les données de KDD-CUP-98 montrent que les 10 meilleures règles sélectionnées d'après leurs mesuresde support et confiance ne sont intéressantes que dans le cas où la qualité deleurs données est correcte voire améliorée.	Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000410	http://editions-rnti.fr/render_pdf.php?p=1000410
Revue des Nouvelles Technologies de l'Information	EGC	2006	Modélisation informationnelle : un cadre méthodologique pour représenter des connaissances évolutives spatialisables	Pour comprendre et représenter les évolutions du bâti, question renouvelée avec le développement des NTIC, l'analyste s'appuie sur des connaissances évolutives ayant dans notre champ d'application - le patrimoine architectural - un caractère spatialisable (par l'attachement à un lieu lambda) mais aussi des caractéristiques handicapantes (hétérogénéité, incertitudes et contradictions, etc.). En réponse, nous utilisons ce caractère spatialisable pour intégrer les ressources constituant le jeu de connaissances propre à chaque édifice: théorie, sources documentaires, observations. Cette démarche que nous nommons modélisation informationnelle a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées. Notre contribution introduit les filiations de cette démarche, le cadre méthodologique qui la matérialise, et discute de son application au cas concret de la place centrale de Cracovie (Rynek Glowny) pour en évaluer l'apport potentiel en matière de gestion et de visualisation de connaissances.	Jean-Yves Blaise, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1000368	http://editions-rnti.fr/render_pdf.php?p=1000368
Revue des Nouvelles Technologies de l'Information	EGC	2006	Multi-catégorisation de textes juridiques et retour de pertinence	La fouille de données textuelles constitue un champ majeur dutraitement automatique des données. Une large variété de conférences, commeTREC, lui sont consacrées. Dans cette étude, nous nous intéressons à la fouillede textes juridiques, dans l'objectif est le classement automatique de ces textes.Nous utilisons des outils d'analyses linguistiques (extraction de terminologie)dans le but de repérer les concepts présents dans le corpus. Ces conceptspermettent de construire un espace de représentation de faible dimensionnalité,ce qui nous permet d'utiliser des algorithmes d'apprentissage basés sur desmesures de similarité entre individus, comme les graphes de voisinage. Nouscomparons les résultats issus du graphe et de C4.5 avec les SVM qui eux sontutilisés sans réduction de la dimensionnalité.	Vincent Pisetta, Hakim Hacid, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000353	http://editions-rnti.fr/render_pdf.php?p=1000353
Revue des Nouvelles Technologies de l'Information	EGC	2006	Outil de datamining spatial appliqué à l'analyse des risques liés au territoire		Schahrazed Zeghache, Farida Admane, Kamel Elaraba Ziane	http://editions-rnti.fr/render_pdf.php?p1&p=1000442	http://editions-rnti.fr/render_pdf.php?p=1000442
Revue des Nouvelles Technologies de l'Information	EGC	2006	Prédiction de solubilité de molécules à partir des seules données relationnelles	La recherche de médicaments passe par la synthèse de molécules candidatesdont l'efficacité est ensuite testée. Ce processus peut être accéléré enidentifiant les molécules non solubles, car celles-ci ne peuvent entrer dans lacomposition d'un médicament et ne devraient donc pas être étudiées. Des techniquesont été développées pour induire un modèle de prédiction de l'indice desolubilité, utilisant principalement des réseaux de neurones ou des régressionslinéaires multiples. La plupart des travaux actuels visent à enrichir les donnéesde caractéristiques supplémentaires sur les molécules. Dans cet article, nous étudionsl'intérêt de la construction automatique d'attributs basée sur la structureintrinsèquement multi-relationnelle des données. Les attributs obtenus sont utilisésdans un algorithme d'arbre de modèles, auquel on associe une méthodede bagging. Les tests réalisés montrent que ces méthodes donnent des résultatscomparables aux meilleures méthodes du domaine qui travaillent sur des attributsconstruits par les experts.	Sébastien Derivaux, Agnès Braud, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1000424	http://editions-rnti.fr/render_pdf.php?p=1000424
Revue des Nouvelles Technologies de l'Information	EGC	2006	Préparation des données Radar pour la reconnaissance/identification de cibles aériennes	La problématique générale présentée dans ce papier concerne lessystèmes intelligents, dédiés pour l'aide à la prise de décision dans le domaineradar. Les premiers travaux ont donc consisté après avoir adapté le processusd'extraction de connaissances à partir de données (ECD) au domaine radar, àmettre en oeuvre les étapes en amont de la phase de fouille de données. Nousnous limitons dans ce papier à la phase de préparation des données (imagesISAR : Inverse Synthetic Aperture Radar). Nous introduisons ainsi la notion dequalité comme moyen d'évaluer l'imperfection dans les données radarsexpérimentales.	Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000426	http://editions-rnti.fr/render_pdf.php?p=1000426
Revue des Nouvelles Technologies de l'Information	EGC	2006	Prétraitement de grands ensembles de données pour la fouille visuelle	Nous présentons une nouvelle approche pour le traitement des ensemblesde données de très grande taille en fouille visuelle de données. Les limitesde l'approche visuelle concernant le nombre d'individus et le nombre dedimensions sont connues de tous. Pour pouvoir traiter des ensembles de donnéesde grande taille, une solution possible est d'effectuer un prétraitement del'ensemble de données avant d'appliquer l'algorithme interactif de fouille visuelle.Pour ce faire, nous utilisons la théorie du consensus (avec une affectationvisuelle des poids). Nous évaluons les performances de notre nouvelle approchesur des ensembles de données de l'UCI et du Kent Ridge Bio MedicalDataset Repository.	François Poulet, Edwige Fangseu Badjio	http://editions-rnti.fr/render_pdf.php?p1&p=1000324	http://editions-rnti.fr/render_pdf.php?p=1000324
Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche de règles non redondantes par vecteurs de bits dans des grandes bases de motifs 		François Jacquenet, Christine Largeron, Cédric Udréa	http://editions-rnti.fr/render_pdf.php?p1&p=1000414	http://editions-rnti.fr/render_pdf.php?p=1000414
Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche de sous-structures fréquentes pour l'intégration de schémas XML	La recherche d'un schéma médiateur à partir d'un ensemble de schémasXML est une problématique actuelle où les résultats de recherche issusde la fouille de données arborescentes peuvent être adoptés. Dans ce contexte,plusieurs propositions ont été réalisées mais les méthodes de représentation desarborescences sont souvent trop coûteuses pour permettre un véritable passageà l'échelle. Dans cet article, nous proposons des algorithmes de recherche desous-schémas fréquents basés sur une méthode originale de représentation deschémas XML. Nous décrivons brièvement la structure adoptée pour ensuitedétailler les algorithmes de recherche de sous-arbres fréquents s'appuyant surune telle structure. La représentation proposée et les algorithmes associés ontété évalués sur différentes bases synthétiques de schémas XML montrant ainsil'intérêt de l'approche proposée	Federico Del Razo López, Anne Laurent, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000393	http://editions-rnti.fr/render_pdf.php?p=1000393
Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche en temps réel de préfixes massifs hiérarchiques dans un réseau IP à l'aide de techniques de stream mining	Au cours de ces dernières années, de nombreuses techniques de streammining ont été proposées afin d'analyser des flux de données en temps réel.Dans cet article, nous montrons comment nous avons utilisé des techniques destream mining permettant la recherche d'objets massifs hiérarchiques (hierarchicalheavy hitters) dans un flux de données pour identifier en temps réel dans unréseau IP les préfixes dont la contribution au trafic dépasse une certaine proportionde ce trafic pendant un intervalle de temps donné.	Pascal Cheung-Mon-Chan, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000323	http://editions-rnti.fr/render_pdf.php?p=1000323
Revue des Nouvelles Technologies de l'Information	EGC	2006	Reconnaissance automatique de concepts à partir d'une ontologie	Ce papier présente une approche qui s'appuie sur une ontologie pourreconnaître automatiquement des concepts spécifiques à un domaine dans uncorpus en langue naturelle. La solution proposée est non-supervisée et peuts'appliquer à tout domaine pour lequel une ontologie a été déjà construite. Uncorpus du domaine est utilisé dans lequel les concepts seront reconnus. Dansune première phase, des connaissances sont extraites de ce corpus en faisantappel à des fouilles de textes. Une ontologie du domaine est utilisée pour étiqueterces connaissance. Le papier donne un aperçu des techniques de fouillesemployées et décrit le processus d 'étiquetage. Les résultats d'une premièreexpérimentation dans le domaine de l'accidentologie sont aussi présentés	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1000351	http://editions-rnti.fr/render_pdf.php?p=1000351
Revue des Nouvelles Technologies de l'Information	EGC	2006	Reconnaissance automatique d'évènements survenant sur patients en réanimation à l'aide d'une méthode adaptative d'extraction en ligne d'épisodes temporels	Ce papier présente la version adaptative d'un algorithmed'extraction d'épisodes temporels développé précédemment. Les trois paramè-tres de réglages de l'algorithme ne sont plus fixes. Ils sont modifiés en ligne enfonction de la variance estimée du signal que l'on veut décomposer en épiso-des temporels. La version adaptative de l'algorithme a été utilisée pour recon-naître automatiquement des aspirations trachéales à partir de plusieures varia-bles physiologiques enregistrés sur des patients hospitalisés en réanimation.Des résultats préliminaires sont présentés dans ce papier.	Sylvie Charbonnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000333	http://editions-rnti.fr/render_pdf.php?p=1000333
Revue des Nouvelles Technologies de l'Information	EGC	2006	Règles d'association avec une prémisse composée : Mesure du gain d'information.	La communauté de fouille de données a développé un grand nombre d'indices permettantde mesurer la qualité des règles d'association (RA) selon diverses sémantiques (Guillet,2004). Cependant ces sémantiques, qui permettent d'interpréter les règles simples, s'avèrentd'utilisation trop complexe pour un expert dans le cas de règles à prémisse composée. Notreobjectif est donc de sélectionner les règles à prémisse composée de type AB&#8594;C quiapportent une information supplémentaire à celle des règles simples A&#8594;C et B&#8594;C. Pourcela nous définissons un indice de gain d'une règle composée par rapport aux règles simples.Dans l'application présentée, nous extrayons des RA de résultats de classifications pouren faciliter l'analyse . Le gain a permis de filtrer des règles d'interprétation simple	Martine Cadot, Pascal Cuxac, Claire François	http://editions-rnti.fr/render_pdf.php?p1&p=1000412	http://editions-rnti.fr/render_pdf.php?p=1000412
Revue des Nouvelles Technologies de l'Information	EGC	2006	Représentation d'expertise psychologique sous la forme de graphes orientés, codés en RDF		Yves Fossé, Stéphane Daviet, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000434	http://editions-rnti.fr/render_pdf.php?p=1000434
Revue des Nouvelles Technologies de l'Information	EGC	2006	Représentation des connaissances appliquées à la géotechnique : une approche		Nicolas Faure	http://editions-rnti.fr/render_pdf.php?p1&p=1000436	http://editions-rnti.fr/render_pdf.php?p=1000436
Revue des Nouvelles Technologies de l'Information	EGC	2006	Sélection de variables et modélisation d'expression d'émotion dans les dialogues Homme-Machine		Barbara Poulain	http://editions-rnti.fr/render_pdf.php?p1&p=1000438	http://editions-rnti.fr/render_pdf.php?p=1000438
Revue des Nouvelles Technologies de l'Information	EGC	2006	Sélection supervisée d'instances : une approche descriptive	La classification suivant le plus proche voisin est une règle simple etperformante. Sa mise en oeuvre pratique nécessite, tant pour des raisons de coûtde calcul que de robustesse, de sélectionner les instances à conserver. La partitionde Voronoi induite par les prototypes constitue la structure sous-jacente àcette règle. Dans cet article, on introduit un critère descriptif d'évaluation d'unetelle partition, quantifiant le compromis entre nombre de cellules et discriminationde la variable cible entre les cellules. Une heuristique d'optimisation estproposée, tirant partie des propriétés des partitions de Voronoi et du critère. Laméthode obtenue est comparée avec les standards sur une vingtaine de jeux dedonnées de l'UCI. Notre technique ne souffre d'aucun défaut de performanceprédictive, tout en sélectionnant un minimum d'instances. De plus, elle ne surapprendpas.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000382	http://editions-rnti.fr/render_pdf.php?p=1000382
Revue des Nouvelles Technologies de l'Information	EGC	2006	SVM incrémental, parallèle et distribué pour le traitement de grandes quantités de données	Nous présentons un nouvel algorithme de SVM (Support VectorMachine ou Séparateur à Vaste Marge) linéaire et non-linéaire, parallèle etdistribué permettant le traitement de grands ensembles de données dans untemps restreint sur du matériel standard. A partir de l'algorithme de Newton-GSVM proposé par Mangasarian, nous avons construit un algorithmeincrémental, parallèle et distribué permettant d'améliorer les performances entemps d'exécution et mémoire en s'exécutant sur un groupe d'ordinateurs. Cenouvel algorithme a la capacité de classifier un million d'individus en 20dimensions et deux classes en quelques secondes sur un ensemble de dix PC	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000322	http://editions-rnti.fr/render_pdf.php?p=1000322
Revue des Nouvelles Technologies de l'Information	EGC	2006	Système d'aide à la décision pour la surveillance de la qualité de l'air intérieur		Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1000445	http://editions-rnti.fr/render_pdf.php?p=1000445
Revue des Nouvelles Technologies de l'Information	EGC	2006	Techniques de fouille de données pour la réécriture de requêtes en présence de contraintes de valeurs	Dans cet article, nous montrons comment les techniques de fouilles de données peuvent résoudre efficacement le problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs. A partir d'une formalisation du problème de la réécriture dans le cadre de la logique de description ALN(Ov), nous montrons comment ce problème se rattache à un cadre de découverte de connaissances dans les bases de données. L'exploitation de ce cadre nous permet de bénéficier de solutions algorithmiques existantes pour la résolution du problème de réécriture. Nous proposons une implémentation de cette approche, puis nous l'expérimentons. Les premiers résultats démontrent l'intérêt d'une telle approche en termes de capacité à traiter un grand nombre de sources de données.	Hélène Jaudoin, Frédéric Flouvat	http://editions-rnti.fr/render_pdf.php?p1&p=1000326	http://editions-rnti.fr/render_pdf.php?p=1000326
Revue des Nouvelles Technologies de l'Information	EGC	2006	Teximus Expertise : un logiciel de gestion de connaissances	Le logiciel Teximus Expertise est un outil évolué de gestion dynamiquede connaissances basé sur les notions de référentiel sémantique. Cette suiteintégrée facilite le partage de connaissances et d'informations dans les entreprises.	Olivier Gerbé	http://editions-rnti.fr/render_pdf.php?p1&p=1000453	http://editions-rnti.fr/render_pdf.php?p=1000453
Revue des Nouvelles Technologies de l'Information	EGC	2006	Typicalité et contribution des sujets et des variables supplémentaires en Analyse Statistique Implicative	L'analyse statistique implicative traite des tableaux sujets xvariables afin d'extraire règles et métarègles statistiques entre les variables.L'article interroge les structures obtenues représentées par graphe et hiérarchieorientés afin de dégager la responsabilité des sujets ou des groupes de sujets(variables supplémentaires) dans la constitution des chemins du graphe ou desclasses de la hiérarchie. On distingue les concepts de typicalité pour signifier laproximité des sujets avec le comportement moyen de la population envers lesrègles statistiques extraites, puis de contribution pour quantifier le rôlequ'auraient les sujets par rapport aux règles strictes associées. Un exemple dedonnées réelles, traité à l'aide du logiciel CHIC, illustre et montre l'intérêt deces deux concepts.	Régis Gras, Jérôme David, Jean-Claude Régnier, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000370	http://editions-rnti.fr/render_pdf.php?p=1000370
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un automate pour évaluer la nature des textes	On ne peut s'intéresser aux textes sans s'intéresser à leur nature. La nature des textes permet de distinguer les textes d'un point de vue primaire. Elle est utilisée pour identifier les textes artificiels, pour la reconnaissance de la langue, afin d'identifier les SPAMS... En ce sens, la méthode la plus connue reste encore la méthode de Zipf. Cet article propose une nouvelle méthode basée sur un automate. L'automate construit un signal pour chaque texte. L'automate est présenté en détail et des expérimentations montrent son utilité dans les domaines aussi divers que ceux cités précédemment/	Hubert Marteau, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000355	http://editions-rnti.fr/render_pdf.php?p=1000355
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un logiciel permettant d'apprendre des règles et leurs exceptions : Area		Sylvain Lagrue, Jérémie Lussiez, Julien Rossit	http://editions-rnti.fr/render_pdf.php?p1&p=1000454	http://editions-rnti.fr/render_pdf.php?p=1000454
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un modèle de qualité de l'information	Ce travail s'intègre dans la problématique générale de la recherched'information ; et plus particulièrement dans la personnalisation et la qualitéd'information. Dans cet article nous proposons un modèle multidimensionnelde la qualité de l'information décrivant les différents facteurs de qualité influantsur la personnalisation de l'information. Ce modèle permet de structurerles différents facteurs de qualité de l'information dans une hiérarchie afind'assister l'utilisateur dans la construction de son propre profil selon ses besoinset ses exigences en termes de qualité.	Rami Harrathi, Sylvie Calabretto	http://editions-rnti.fr/render_pdf.php?p1&p=1000363	http://editions-rnti.fr/render_pdf.php?p=1000363
Revue des Nouvelles Technologies de l'Information	EGC	2006	Un modèle métier extensible adapté à la gestion de dépêches d'agences de presse		Frédéric Bertrand, Cyril Faucher, Marie-Christine Lafaye, Jean-Yves Lafaye, Alain Bouju	http://editions-rnti.fr/render_pdf.php?p1&p=1000447	http://editions-rnti.fr/render_pdf.php?p=1000447
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche distribuée pour l'extraction de connaissances : Application à l'enrichissement de l'aspect factuel des BDG	Les systèmes d'informations géographiques (SIG) sont utilisés pouraméliorer l'efficacité des entreprises et des services publics, en associantméthodes d'optimisation et prise en compte de la dimension géographique.Cependant, les bases de données géographiques (BDG) stockées dans les SIGsont restreintes à l'application pour laquelle elles ont été conçues. Souvent, lesutilisateurs demeurent contraints de l'existant et se trouvent dans le besoin dedonnées complémentaires pour une prise de décision adéquate. D'où, l'idée del'enrichissement de l'aspect descriptif des BDG existantes. Pour atteindre cetobjectif, nous proposons une approche qui consiste à intégrer un module defouille de données textuelles au SIG lui même. Il s'agit de proposer uneméthode distribuée de résumé de documents multiples à partir de corpus enligne.L'idée est de faire coopérer un ensemble d'agents s'entraidant afind'aboutir à un résumé optimal.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1000329	http://editions-rnti.fr/render_pdf.php?p=1000329
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche multi-agent adaptative pour la simulation de schémas tactiques	Ce papier est consacré à la simulation ou à la réalisation automatiquede schémas tactiques par un groupe d´agents footballeurs autonomes. Son objectifest de montrer ce que peuvent apporter des techniques d'apprentissagepar renforcement à des agents réactifs conçus pour cette tâche. Dans un premiertemps, nous proposons une plateforme et une architecture d'agents capabled'effectuer des schémas tactiques dans des cas relativement simples. Ensuite,nous mettons en oeuvre un algorithme d'apprentissage par renforcementpour permettre aux agents de faire face à des situations plus complexes. Enfin,une série d'expérimentations montrent le gain apporté aux agents réactifs parl'utilisation d'algorithmes d'apprentissage.	Aydano Machado, Yann Chevaleyre, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000334	http://editions-rnti.fr/render_pdf.php?p=1000334
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche simple inspirée des réseaux sociaux pour la hiérarchisation des systèmes autonomes de l'Internet	Le transit des flux d'information dans le réseau Internet à l'échellemondiale est régi par des accords commerciaux entre systèmes autonomes, accordsqui sont mis en oeuvre via le protocole de routage BGP. La négociationde ces accords commerciaux repose implicitement sur une hiérarchie des systèmesautonomes et la position relative de deux systèmes débouche sur un accordde type client/fournisseur (un des systèmes, le client, est nettement mieuxclassé que l'autre, le fournisseur, et le client paye le fournisseur pour le transitdes flux d'information) ou sur un accord de type "peering" (transit gratuit dutrafic entre les deux systèmes). En dépit de son importance, il n'existe pas dehiérarchie officielle de l'Internet (les clauses commerciales des accords entresystèmes autonomes ne sont pas nécessairement publiques) ni de consensus surla façon d'établir une telle hiérarchie. Nous proposons une heuristique simpleinspirée de la notion de "centralité spectrale" issue de l'analyse des réseaux sociauxpour analyser la position relative des systèmes autonomes de l'Internet àpartir des informations des seules informations de connectivité entre systèmesautonomes.	Fabrice Clérot, Quang Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000391	http://editions-rnti.fr/render_pdf.php?p=1000391
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une comparaison de certains indices de pertinence des règles d'association	Cet article propose une comparaison graphique de certains indices depertinence pour évaluer l'intérêt des règles d'association. Nous nous sommesappuyés sur une étude existante pour sélectionner quelques indices auxquelsnous avons ajouté l'indice de Jaccard et l'indice d'accords désaccords (IAD).Ces deux derniers nous semblent plus adaptés pour discriminer les règles intéressantesdans le cas où les items sont des événements peu fréquents. Une applicationest réalisée sur des données réelles issues du secteur automobile	Marie Plasse, Ndeye Niang, Gilbert Saporta, Laurent Leblond	http://editions-rnti.fr/render_pdf.php?p1&p=1000405	http://editions-rnti.fr/render_pdf.php?p=1000405
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une mesure de proximité et une méthode de regroupement pour l'aide à l'acquisition d'ontologies spécialisées	Cet article traite du regroupement d'unités textuelles dans une perspectived'aide à l'élaboration d'ontologies spécialisées. Le travail présenté s'inscritdans le cadre du projet BIOTIM. Nous nous concentrons ici sur l'une desétapes de construction semi-automatique d'une ontologie qui consiste à structurerun ensemble d'unités textuelles caractéristiques en classes susceptibles dereprésenter les concepts du domaine. L'approche que nous proposons s'appuiesur la dénition d'une nouvelle mesure non-symétrique permettant d'évaluer laproximité entre lemmes, en utilisant leurs contextes d'apparition dans les documents.En complément de cette mesure, nous présentons un algorithme declassication non-supervisée adapté à la problématique et aux données traitées.Les premières expérimentations présentées sur les données botaniques laissentpercevoir des résultats pertinents pouvant être utilisés pour assister l'expert dansla détermination et la structuration des concepts du domaine.	Guillaume Cleuziou, Sylvie Billot, Stanislas Lew, Lionel Martin, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000339	http://editions-rnti.fr/render_pdf.php?p=1000339
Revue des Nouvelles Technologies de l'Information	EGC	2006	Une nouvelle mesure sémantique pour le calcul de la similarité entre deux concepts d'une même ontologie	Les ontologies sont au coeur du processus de gestion des connaissances.Différentes mesures sémantiques ont été proposées dans la littératurepour évaluer quantitativement l'importance de la liaison sémantique entre pairesde concepts. Cet article propose une synthèse analytique des principales mesuressémantiques basées sur une ontologie modélisée par un graphe et restreinte iciaux liens hiérarchiques is-a. Après avoir mis en évidence différentes limites desmesures actuelles, nous en proposons une nouvelle, la PSS (Proportion of SharedSpecificity), qui sans corpus externe, tient compte de la densité des liens dans legraphe reliant deux concepts	Emmanuel Blanchard, Mounira Harzallah, Pascale Kuntz, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000344	http://editions-rnti.fr/render_pdf.php?p=1000344
Revue des Nouvelles Technologies de l'Information	EGC	2006	Utilisation de métadonnées pour l'aide à l'interprétation de classes et de partitions	Les résultats des méthodes de fouille de données sont difficilementinterprétables par un utilisateur n'ayant pas l'expertise requise. Dans ce papiernous proposons un outil permettant aux utilisateurs d'interpréter les résultatsissus des méthodes de classification non supervisée. Cet outil est basé sur desmétadonnées utilisées pour formaliser le processus d'interprétationautomatique. Ces métadonnées vont servir à l'utilisateur pour comprendre dansquelles circonstances les données originales ont été collectées et de quellemanière elles ont été agrégées puis classifiées. L'intérêt de ce travail porte surla souplesse qu'auront les utilisateurs à pouvoir interpréter facilement lesclasses obtenues. Nous développons notre approche basée sur l'utilisation desmétadonnées. Nous traduirons notre méthodologie par un exemple concret.	Abdourahamane Baldé, Yves Lechevallier, Brigitte Trousse, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000371	http://editions-rnti.fr/render_pdf.php?p=1000371
Revue des Nouvelles Technologies de l'Information	EGC	2006	Utilisation des réseaux bayésiens dans le cadre de l'extraction de règles d'association	Cet article aborde le problème de l'utilisation d'un modèle de connaissancedans un contexte de fouille de données. L'approche méthodologique proposéemontre l'intérêt de la mise en oeuvre de réseaux bayésiens couplée à l'extractionde règles d'association dites delta-fortes (membre gauche minimal, fréquenceminimale et niveau de confiance contrôlé). La découverte de règles potentiellementutiles est alors facilitée par l'exploitation des connaissances décritespar l'expert et représentées dans le réseau bayésien. Cette approche estvalidée sur un cas d'application concernant la fouille de données d'interruptionsopérationnelles dans l'industrie aéronautique.	Clément Fauré, Sylvie Delprat, Alain Mille, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000407	http://editions-rnti.fr/render_pdf.php?p=1000407
Revue des Nouvelles Technologies de l'Information	EGC	2006	Vers l'extraction de motifs rares	Un certain nombre de travaux en fouille de données se sont intéressés à l'extraction de motifs et à la génération de règles d'association à partir de ces motifs. Cependant, ces travaux se sont jusqu'à présent, centrés sur la notion de motifs fréquents. Le premier algorithme à avoir permis l'extraction de tous les motifs fréquents est Apriori mais d'autres ont été mis au point par la suite, certains n'extrayant que des sous-ensembles de ces motifs (motifs fermés fréquents, motifs fréquents maximaux, générateurs minimaux). Dans cet article, nous nous intéressons aux motifs rares qui peuvent également véhiculer des informations importantes. Les motifs rares correspondent au complémentaire des motifs fréquents. A notre connaissance, ces motifs n'ont pas encore été étudiés, malgré l'intérêt que certains domaines pourraient tirer de ce genre de modèle. C'est en particulier le cas de la médecine, où par exemple, il est important pour un praticien de repérer les symptômes non usuels ou les effets indésirables exceptionnels qui peuvent se déclarer chez un patient pour une pathologie ou un traitement donné.	Laszlo Szathmary, Sandy Maumus, Pierre Petronin, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000396	http://editions-rnti.fr/render_pdf.php?p=1000396
Revue des Nouvelles Technologies de l'Information	EGC	2006	Visualisation en Gestion des Connaissances Développement d'un nouveau modèle graphique Graph'Atanor	Les systèmes de gestion des connaissances servent de support pour lacréation et la diffusion de mémoires d'entreprises qui permettent de capitaliser,conserver et enrichir les connaissances des experts. Dans ces systèmes, l'interactionavec les experts est effectuée avec des outils adaptés dans lesquels uneformalisation graphique des connaissances est utilisée. Cette formalisation estsouvent basée au niveau théorique sur des modèles de graphes mais de façonpratique, les représentations visuelles sont souvent des arbres et des limitationsapparaissent par rapport aux représentations basées sur des graphes. Dans cetarticle nous présentons le modèle utilisé par le serveur de connaissances Atanorqui utilise des arbres pour visualiser les connaissances, et nous développons unenouvelle approche qui permet de représenter les mêmes connaissances sous laforme de graphes en niveaux. Une analyse comparative des deux méthodes dansun contexte industriel de maintenance permet de mettre en valeur l'apport desgraphes dans le processus de visualisation graphique des connaissances.	Bruno Pinaud, Pascale Kuntz, Fabrice Guillet, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000364	http://editions-rnti.fr/render_pdf.php?p=1000364
Revue des Nouvelles Technologies de l'Information	EGC	2006	Visualisation interactive de données avec des méthodes à base de points d'intérêt	Nous présentons dans cet article une méthode de visualisation interactivede données numériques ou symboliques permettant à un utilisateur expertdu domaine d'obtenir des informations et des connaissances pertinentes. Nousproposons une approche nouvelle en adaptant l'utilisation des points d'intérêtsdans un contexte de fouille visuelle de données. A partir d'un ensemble de pointsd'intérêt disposés sur un cercle, les données sont visualisées à l'intérieur de cecercle en fonction de leur similarité à ces points d'intérêt. Des opérations interactivessont alors définies : sélectionner, zoomer, changer dynamiquement lespoints d'intérêts. Nous évaluons les propriétés d'une telle visualisation sur desdonnées aux caractéristiques connues. Nous décrivons une application réelle encours dans le domaine de l'exploration de données issues d'enquêtes de satisfaction.	David Da Costa, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000367	http://editions-rnti.fr/render_pdf.php?p=1000367
Revue des Nouvelles Technologies de l'Information	EGC	2006	Web sémantique pour la mémoire d'expériences d'une communauté scientifique : le projet MEAT	Cet article décrit le projet MEAT (Mémoire d'Expériences pourl'Analyse du Transcriptome) dont le but est d'assister les biologistes travaillantdans le domaine des puces à ADN, pour l'interprétation et la validation de leursrésultats. Nous proposons une aide méthodologique et logicielle pour construireune mémoire d'expériences pour ce domaine. Notre approche, basée surles technologies du web sémantique, repose sur l'utilisation des ontologies etdes annotations sémantiques sur des articles scientifiques et d'autres sourcesde connaissances du domaine. Notre approche peut être généralisée à d'autresdomaines requérant des expérimentations et traitant un grand flux de données(protéomique, chimie,etc.).	Khaled Khelif, Rose Dieng-Kuntz, Pascal Barbry	http://editions-rnti.fr/render_pdf.php?p1&p=1000340	http://editions-rnti.fr/render_pdf.php?p=1000340
Revue des Nouvelles Technologies de l'Information	EGC	2006	Web Usage Mining : extraction de périodes denses à partir des logs	Les techniques de Web Usage Mining existantes sont actuellementbasées sur un découpage des données arbitraire (e.g. "un log par mois") ou guidépar des résultats supposés (e.g. "quels sont les comportements des clients pourla période des achats de Noël ? "). Ces approches souffrent des deux problèmessuivants. D'une part, elles dépendent de cette organisation arbitraire des donnéesau cours du temps. D'autre part elles ne peuvent pas extraire automatiquementdes "pics saisonniers" dans les données stockées. Nous proposons d'exploiterles données pour découvrir de manière automatique des périodes "denses" decomportements. Une période sera considérée comme "dense" si elle contient aumoins un motif séquentiel fréquent pour l'ensemble des utilisateurs qui étaientconnectés sur le site à cette période.	Florent Masseglia, Pascal Poncelet, Maguelonne Teisseire, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000377	http://editions-rnti.fr/render_pdf.php?p=1000377
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Athanor, une approche de gestion de connaissances procédurales pour la maintenance de systèmes	Un grand nombre d'entreprises sont confrontés à des problèmes stratégiques de gestion des connaissances, d'autant plus critiques que les connaissances portent sur des systèmes complexes, qui nécessitent la mise en oeuvre d'une démarche instrumentalisée intégrant le déploiement d'une plateforme opérationnelle dans les système d'information de l'entreprise.C'est dans ce cadre que s'inscrit la démarche Athanor pour la maintenance de systèmes complexes. En nous inspirant des méthodes de capitalisation et de formalisation des connaissances pour la conception de mémoires organisationnelles, nous avons conçu un serveur de connaissance orienté processus qui implémente les services de capitalisation-évolution des connaissances, d'aide à la décision pour le diagnostic et de formation, en conjonction avec des modèles en réalités virtuelles, des machines de tri et une documentation électronique.	Fabrice Guillet, Vincent Philippé, Jacques Philippé, Dominique Follut	http://editions-rnti.fr/render_pdf.php?p1&p=1000458	http://editions-rnti.fr/render_pdf.php?p=1000458
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Estimation des performances pour la conception des systèmes multicapteurs	Ce papier décrit les méthodes d'analyse de performances utilisables pour la conception d'un système multi-capteurs. Les fonctions évaluées sont les fonctions principales d'estimation, de classification et d'association. L'analyse de performances a des implications importantes non seulement dans la conception du système mais également dans l'allocation des ressources, la supervision des traitements et le contrôle du système.	Jean-François Grandin	http://editions-rnti.fr/render_pdf.php?p1&p=1000467	http://editions-rnti.fr/render_pdf.php?p=1000467
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Extraction d'une réalité tridimensionnelle du fond marin à partir d'un sonar interférométrique	Cet article concerne l'aide à la compréhension de scènes sous-marines grâce à l'utilisation d'un sonar interférométrique commercial prototype. Il s'agit pour l'opérateur sonar de ne plus visualiser une scène bidimensionnelle, mais grâce à l'interférométrie il est possible de visualiser cette même scène de manière tridimensionnelle, ce qui facilite la recherche et la classification d'objets. Ce sonar se révèle être un outil bien adapté à l'exploration des fonts sous-marins, parce qu'il fournit une image sonar latérale de haute définition, et en même temps une image bathymétrique à une résolution assez proche voir identique, ce qui autorise la transformation du pixel de l'image sonar en voxel sans recalage. Ce passage d'une information 2D à une information 3D permet du coup de corriger tous les défauts de représentation des données et ainsi faciliter les post traitements. Cette étude présente les concepts généraux de l'interférométrie et ses limitations de fonctionnement essentiellement dues au bruit. En effet, l'interférométrie qui est une méthode de triangulation d'écho, est handicapée par l'ambiguïté de la phase interférométrique, parce que cette triangulation est liée à un délai entre deux antennes modulo la longueur d'onde émise. Le Groupe d'Études Sous Marine de l'Atlantique a acheté un sonar interférométrique et à développé un processus général pour réduire ces erreurs d'ambiguïté. La conclusion principale de cet article concerne l'application potentielle de ce sonar et du processus associé à l'étude haute-résolution, d'épaves, de petits objets et pipelines. La vitesse d'utilisation permet de l'employer sur des grands espaces. La qualité des données rend la reconstructions 3D possible en vue d'une visualisation interactive.	Christophe Sintes, Michel Legris, Basel Solaiman	http://editions-rnti.fr/render_pdf.php?p1&p=1000462	http://editions-rnti.fr/render_pdf.php?p=1000462
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Gestion des connaissances et analyse multicritère pour un système interactif d'aide à la décision	Cet article propose un modèle conceptuel du processus d'apprentissage et de décision dans une organisation. La perspective d'analyse retenue relève d'une interprétation cybernétique du modèle cognitif de l'économiste H.A.Simon, qui montre que l'acquisition et le traitement de l'information apparaissent comme plus importants pour prendre une "bonne" décision dans une organisation que la recherche fine illusoire et trop simplificatrice d'une décision en apparence la meilleure. L' "homme administratif" n'est pas un "homo oeconomicus" aux objectifs clairs et explicites, stables dans le temps, mutuellement indépendants. Une bonne partie de la littérature sur les sciences de l'organisation se borne à le constater. cet article offre une alternative cybernétique au modèle du processus de décision vu par les sciences de l'organisation. Il propose une chaîne complète de traitement de l'information utile à la décision qui s'appuie sur des outils relevant du data-mining pour la phase d'apprentissage et sur des techniques multi-critères pour la sélection.	Jacky Montmain, Michel Plantié, Abdellah Akharraz	http://editions-rnti.fr/render_pdf.php?p1&p=1000461	http://editions-rnti.fr/render_pdf.php?p=1000461
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Interprétation d'images médicales	Dans de nombreuses spécialités médicales, les médecins font un usage croissant de l'image. L'interprétation des images médicales est une tâche difficile qui requiert la considération de nombreuses connaissances souvent hétérogènes. De ce fait, les recherches basées sur la fusion de l'information, situées entre les domaines du traitement de l'information et l'intelligence artificielle, se révèlent prometteuses. Le but de ce travail est l'extraction des structures anatomiques pertinentes à partir d'images échographiques, dans le but d'améliorer l'évaluation du degré de maturité des tumeurs de l'oesophage. Une méthodologie générale pour la représentation des connaissances est proposée. Basée sur la logique floue et la théorie de Bayes, celle-ci permet de prendre en compte les notions d'ambiguïté et d'imprécision probabiliste. Les connaissances sont organisées dans un schéma de raisonnement qui se rapproche d'une architecture "tableau noir". Les résultats de segmentations sont prometteurs. Des études plus poussées sont envisagées en conclusion.	Renaud Debon, Basel Solaiman, M. Robaszkiewicz	http://editions-rnti.fr/render_pdf.php?p1&p=1000457	http://editions-rnti.fr/render_pdf.php?p=1000457
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	La place et les formes de la décision en ingénierie des systèmes	Cet article développe particulièrement deux points, d'abord l'imprégnation de tout le processus d'ingénierie de systèmes par des problèmes de décision et ensuite l'adéquation d'une approche argumentative de la décision dans ces activités multidisciplinaires.	Patrice Micouin, Jean Paul Kieffer	http://editions-rnti.fr/render_pdf.php?p1&p=1000459	http://editions-rnti.fr/render_pdf.php?p=1000459
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	La simulation comme outil d'aide à la décision pour l'ingénierie des systèmes complexes	Les systèmes de défense sont toujours plus complexes, car intégrant davantage de composantes, hétérogènes et de durées de vie très disparates. La réduction des risques dans les différentes phases d'un programme d'armement (en amont de la faisabilité jusqu'à la mise en service, voire au retrait avec la prise en compte croissante de contraintes environnementales) devient alors un enjeu essentiel pour la maîtrise des coûts tout au long de la vie du programme. Au vu de l'évolution du contexte, il apparaît donc nécessaire de faire évoluer les méthodes d'ingénierie des systèmes et de proposer des outils de modélisation et de simulation au service des architectes, pour l'aide à l'analyse, à la conception, à la réalisation, à l'évalution, voire à la gestion de configurations et à l'entrainement des systèmes de défense du futur.cette démarche est en cours au sein du ministère de la défense, appliquée à plusieurs systèmes de systèmes, comme la défense aérienne élargie. Nous développons ces divers points dans les sections ultérieures.	Dominique Luzeaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000465	http://editions-rnti.fr/render_pdf.php?p=1000465
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Outils d'ingénierie système utilisés chez Dassault Aviation	Cet article passe en revue, en suivant les étapes d'un processus normalisé de développement de systèmes, les outils d'ingénierie utilisés chez Dassault Aviation pour le développement des systèmes d'avionique militaire. Conformément au thème général de la conférence, on souligne le rôle des systèmes d'information et des outils d'aide à la décision en conception, avec une attention particulière pour quelques outils à caractère scientifique dont l'utilisation opérationnelle est récente. On s'efforce en guise de conclusion, de dégager quelques leçons tirées des dix dernières années et des tendances pour l'avenir	Emmanuel Ledinot	http://editions-rnti.fr/render_pdf.php?p1&p=1000463	http://editions-rnti.fr/render_pdf.php?p=1000463
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Plate-forme de veille multi-agents pour l'aide à la décision	L'essor d'internet ces dix dernières années a favorisé l'apparition de nombreuses informations disponibles en ligne, trouvant une utilité dans la veille sur les domaines les plus divers tels que la veille stratégique & concurentielle, la veille technologique. Cependant, la quantité d'information publiée est telle qu'il est humainement impossible de prendre connaissance de l'ensemble des données. Ainsi, nombre de recherches éparses ont vu le jour pour traiter des problématiques ciblées de fouille de texte, mais peu de systèmes de veille automatisée sont actuellement mis en oeuvre, permettant de l'interopérabilité de différentes approches d'analyse de contenu. La plateforme qui sera réalisée, dont une présentation succincte est donnée, repose sur un système multi agent ou chaque entité joue un rôle dans le processus de veille, la partie innovante majeure provenant de la gestion par l'utilisateur des réseaux d'accointances des agents.	Nicolas Chanchevrier, Xavier Denis	http://editions-rnti.fr/render_pdf.php?p1&p=1000460	http://editions-rnti.fr/render_pdf.php?p=1000460
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Processus d'agrégation pour la décision	La robustesse, l'acuité et la réactivité repose le plus souvent sur l'utilisation conjointe de moyens d'observations complémentaires et donc sur l'agrégation d'informations disparates en termes d'incertitude, d'imprécision, d'incomplétude, de fiabilité, de subjectivité, ou de pertinence. La théorie de l'évidence constitue alors un cadre propice au développement d'un ensemble d'opérateurs cohérents propres à traiter les problèmes de modélisation unifiée d'informations variées d'association de données ambigues, de combinaison de sources disparates, conflictuelles et dépendantes et de prise de décision. La chaine de traitement complète et modulable qui peut etre constituée à l'aide de ces opérateurs est également applicable à la décision multicritère, qu'elle soit implantée au niveau décisionnel d'un système, qu'elle concerne la gestion de ses ressources ou qu'elle contribue à sa conception.	Alain Appriou	http://editions-rnti.fr/render_pdf.php?p1&p=1000466	http://editions-rnti.fr/render_pdf.php?p=1000466
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Segmentation de bois gravés utilisant les intégrales floues	Ce travail s'inscrit dans les préoccupations de la ville de Troyes qui s'attache à valoriser son riche patrimoine ancien. Afin de garder une traçabilité des tampons de bois gravés, la bibliothèque municipale de Troyes projette de cataloguer les échantillons collectés en constituant une base de données. L'objectif du travail ici consiste, en partant d'une prise de vue du tampon, à obtenir l'impression qu'il aurait produit sur papier. La stratégie que nous mettons en oeuvre pour résoudre cette problématique s'appuie sur un système de traitement utilisant l'analyse pyramidale des ondelettes de Haar et la fusion de données caractéristiques par les intégrales floues. Les résultats obtenus actuellement semblent confirmer la pertinence de notre approche.	Victor Chen, Michel Roussel	http://editions-rnti.fr/render_pdf.php?p1&p=1000456	http://editions-rnti.fr/render_pdf.php?p=1000456
Revue des Nouvelles Technologies de l'Information	SDPLDIS	2006	Systèmes d'information pour l'aide à la décision : Applications en téléphonie mobile et en données radar	L'objectif de cet article est de présenter en première partie le processus ECD et son application dans le cadre de la téléphonie mobile ou les travaux concernent la modélisation de la valeur des clients pour la mise en place d'indicateurs décisionnels dédiés aux experts marketing. Il est à préciser ici, que l'acquisition et la préparation des données en téléphonie sont considérés comme étant résolus par les standards et normes de qualités mis en oeuvre en amont de la chaine. La seconde partie à l'instar de la première, tient compte des possibles rétroactions en acquisition et préparation des données cela, de par le caractère physique du système d'acquisition et des possibles modélisation qui peuvent en découler. Cette partie intègre de plus une introduction aux indicateurs décisionnels prévus pour permettre une aide à la mise au point de la chaîne de reconnaissance voire une adaptation contrôlée en particulier du processus de préparation des données dédiées à la reconnaissance.	Cédric Archaux, Fabrice Pellen, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000464	http://editions-rnti.fr/render_pdf.php?p=1000464
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	A Reason why not to Ban Null Hypothesis Signiicance Tests.	On montre que l'on peut directement calculer un intervalle pour un contraste entremoyennes, étant donné seulement la valeur observée du contraste et la statistique detest t ou F associé (ou encore, de manière équivalente le seuil observé correspondant :\p-value"). Cet intervalle peut être vu comme un intervalle de conance fréquentiste oucomme un intervalle de crédibilité bayésien ou comme un intervalle duciaire. Cela donneaux utilisateurs des tests de signification usuels la possibilité d'une transition facile versdes pratiques statistiques plus appropriées. On met en avant les liens conceptuels entreles tests et les intervalles de conance ou de crédibilité	Bruno Lecoutre, Jacques Poitevineau, Marie-Paule Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001699	http://editions-rnti.fr/render_pdf.php?p=1001699
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Arbres de Décision.	Après avoir détaillé les points clés de la construction d'un arbre de décision à partir d'un petit exemple, nous présentons la méthode CHAID qui permet de répondre de manière cohérente à ces spécifications. Nous la mettons alors en oeuvre en utilisant un logiciel gratuit téléchargeable sur Internet. Les opérations sont décrites à l'aide de plusieurs copies d'écrans. L'accent est mis sur la lecture et l'interprétation des résultats. Nous mettons en avant également l'aspect interactif, très séduisant, de la construction des arbres. De manière plus générale, nous essayons de mettre en perspective les nombreuses techniques d'induction par arbre en faisant le bilan de l'état actuel de la recherche dans le domaine.	Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1001687	http://editions-rnti.fr/render_pdf.php?p=1001687
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Brève notice biographique de P. C. Mahalanobis.		Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001701	http://editions-rnti.fr/render_pdf.php?p=1001701
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Comparaison de manuscrits sanskrits	Face à une collection de manuscrits retranscrivant un même texte, fréquemment dispersés dans le temps et l'espace, le chercheur a recours à l'édition critique afin de prendre en compte tous les aspects du texte. En vue de faciliter leurs créations et d'offrir de nouvelles possibilités en matière d'analyse et d'interactivité, quelques éditions critiques informatisées ont déjà été proposées. Dans cet article, nous nous intéressons au cas de l'édition critique (électronique) de manuscrits écrits en sanskrit. Nous commençons par décrire les spécificités du sanskrit qui soulèvent un ensemble de problèmes touchant aussi bien à la typographie qu'à l'informatique et l'analyse de données. Puis, nous présentons brièvement les solutions informatiques actuelles permettant d'adapter les traitements à la typographie et la syntaxe particulière au sanskrit. Enfin, nous proposons une approche automatique pour identifier et évaluer les différences entre deux manuscrits en vue notamment de décrire les relations de filiation entre manuscrits connus d'un même texte. D'un point de vue algorithmique, notre approche est basée sur des techniques habituellement employées pour comparer des chaînes moléculaires.	Marc Csernel, Patrice Bertrand	http://editions-rnti.fr/render_pdf.php?p1&p=1001672	http://editions-rnti.fr/render_pdf.php?p=1001672
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	COURBOTREE Une Méthode de Classification de Courbes Appliquée au 	Depuis le 1er Juillet 2004, l'ensemble des clients industriels et professionnels peuvent choisir leur fournisseur d'électricité. Pour la majeure partie de ces clients, EDF ne dispose pas des courbes de consommation électrique mais seulement d'index qui permettent de calculer le volume total consommé entre 2 index consécutifs. Un profilage (load profiling), c'est-à-dire une estimation de la courbe de consommation électrique des clients, point par point sur cette période, est alors nécessaire. Celui-ci peut être réalisé en prenant en compte les connaissances métiers et par l'analyse des données récoltées à partir d'un échantillon de clients télérelevés. Ainsi, face à ces nouveaux enjeux commerciaux, des besoins spécifiques en techniques classificatoires appliquées aux courbes apparaissent. Nous proposons dans cet article une méthode de classification de courbes, appelée Courbotree, s'intégrant dans une démarche plus globale de load profiling. Cette méthode repose sur les techniques d'arbres de régression multivariée. Elle répond à un double objectif de classification et de prédiction de courbes. Dans le contexte multivarié, la construction de l'arbre est similaire à celle des méthodes AID et CART. La seule différence réside dans le choix du critère de coupure qui est celui de l'inertie calculée sur les composantes des courbes. A partir d'un échantillon de clients, sur lesquels on dispose d'une part de leurs caractéristiques et d'autre part d'une courbe de consommation, Courbotree fournit une classification de ces courbes directement interprétable par l'utilisateur en terme de règles d'affectation métiers. Elle permet ainsi de profiler tout nouveau client selon ses valeurs observées sur les variables explicatives.Mots-clésclassification de courbes, load profiling, arbre de régression, méthode AID, classification divisive.	Véronique Stéphan	http://editions-rnti.fr/render_pdf.php?p1&p=1001685	http://editions-rnti.fr/render_pdf.php?p=1001685
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Estimation et comparaison de niveaux de retour pour les vitesses extrêmes des vents	Plusieurs modélisations des valeurs extrêmes (ici la vitesse maximum journalière du vent) sont possibles :    par les lois des valeurs extrêmes généralisées (Fréchet, Gumbel et Weibull) dont on peut estimer les paramètres par maximum de vraisemblance ou par moments pondérés. Les hypothèses d'application de ces modèles ne sont pas toujours vérifiées. En dehors de la loi de Weibull, estimer des niveaux de retour à plus de 50 ans donne des valeurs et des intervalles de confiance inexploitables. Selon les stations météo, les lois obtenues peuvent être différentes.    par la méthode des dépassements de seuil (POT) et la loi de Pareto généralisée. Le choix du seuil n'est pas simple et les hypothèses contraignantes.On a pu constater des résultats différents selon les méthodes employées pour le même phénomène. Cela montre que cette modélisation, nécessaire en raison du faible nombre de données extrêmes, implique une mise en oeuvre délicate et une grande prudence dans l'exploitation des résultats, une validation exacte ne semblant pas possible.	Henri Klajnmic	http://editions-rnti.fr/render_pdf.php?p1&p=1001666	http://editions-rnti.fr/render_pdf.php?p=1001666
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Et si vous étiez un bayésien qui s'ignore ?	Cet article guide le lecteur, peu familiarisé, dans la découverte de l'inférence bayésienne. On expose à partir de l'exemple de l'intervalle de confiance sur une proportion la différence essentielle entre l'inférence classique et l'inférence bayésienne. On esquisse ensuite une présentation générale de l'inférence bayésienne.	Bruno Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001671	http://editions-rnti.fr/render_pdf.php?p=1001671
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Évolution du prix de vente du lait entre 1946 et 1957, Exemple d'Utilisation de l'Analyse en Composantes Principales dans la Discipline Historique. 	L'objectif de cet article est de montrer comment l'analyse en composantes principales peut apporter des réponses à l'historien qui étudie un phénomène évoluant au cours du temps. On s'interroge sur la portée d'une politique de fixation des prix de vente du lait adoptée par les pouvoirs publics entre 1946 et 1957 dans le département de la Moselle. L'analyse met en évidence les fluctuations du prix du lait dans le cadre inflationniste qui caractérise cette époque, dévoile les écarts de prix qui s'accentuent entre les différentes étapes de la distribution du lait, explicite les mécanismes de constitution des marges bénéficiaires et leurs répartitions dans la chaîne commerciale. L'article est accompagné du fichier des données (ficher Excel au format Zip) pour permettre au lecteur d'effectuer des analyses complémentaires et confronter ses résultats aux interprétations fournies.	Laurent Erbs	http://editions-rnti.fr/render_pdf.php?p1&p=1001673	http://editions-rnti.fr/render_pdf.php?p=1001673
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Former les Etudiants et les Chercheurs aux Méthodes Bayésiennes pour l'Analyse des Données Expérimentales	Les tests de signification fréquentistes de l'hypothèse nulle (en anglais "Null Hypothe-sis Significance Testing" = NHST) font tellement partie des habitudes des scientifiquesque l'on ne peut supprimer leur usage en les jetant par la fenêtre". Face àcette situation, la stratégie proposée pour former les étudiants et les chercheurs aux méthodes d'inférence statistique pour l'analyse des données expérimentales repose sur une transition en douceur vers le paradigme bayésien. Les principes de base de cette stratégie sont les suivants :(1) Présenter les interprétations bayésiennes naturelles des tests de signification usuels pour attirer l'attention sur leurs insuffisances.(2) Créer en conséquence le besoin d'un changement dans la présentation et l'interprétation des résultats.(3) Finalement fournir aux utilisateurs la possibilité réelle de penser de manière rationnelle les problèmes d'inférence statistique et de se comporter d'une façon plus raisonnable.La conclusion est que l'enseignement de l'approche bayésienne dans le contexte de l'analyse des données expérimentales apparaît à la fois désirable et faisable. Cette faisabilité est illustrée pour les méthodes d'analyse de variance.	Bruno Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001683	http://editions-rnti.fr/render_pdf.php?p=1001683
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	La place de l'aide à la décision dans le système d'information.	Cet article décrit comment un système d'aide à la décision fonctionne :alimentation, traitements, publication, outils de consultation. Il débute par une étude de casqui montre les difficultés intellectuelles et « politiques » que comporte sa conception.This paper presents how a Decision Aid System works: input, data processing,output and publication, available consultation tools, publishing environment. It begins with acase study underlining the intellectual and "political" aspects of its engineering.	Michel Volle	http://editions-rnti.fr/render_pdf.php?p1&p=1001668	http://editions-rnti.fr/render_pdf.php?p=1001668
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	La Procédure FREQ de SAS. Tests d'Indépendance et Mesures d'Association dans un Tableau de Contingence	Ce document présente de manière pédagogique, les divers tests et mesures d'association disponibles dans la procédure FREQ de SAS. Ces tests et mesures sont classés selon le type : nominale, ordinale des variables étudiées, puis ils sont décrits, commentés et appliqués sur des exemples variés. L'approche probabiliste basée sur les odds-ratio et le modèle logit est abordée. Afin de montrer les doutes que l'on doit avoir lors d'un test unique, une «curiosité » est rapportée, celle-ci révèle les discordances des résultats selon les points de vue. Un historique sur le test exact de Fisher permet au lecteur de conforter son opinion.	Josiane Confais, Yvette Grelet, Monique Le Guen	http://editions-rnti.fr/render_pdf.php?p1&p=1001688	http://editions-rnti.fr/render_pdf.php?p=1001688
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	La régression PLS 1. Cas particulier de la Régression Linéaire Séquentielle Orthogonale	La régression linéaire classique fournit une seule solution souvent basée sur le critère des moindres carrés. Lorsqu'il y a beaucoup de variables, on risque d'obtenir un modèle surparamétré, c'est-à-dire modélisant les erreurs. Pour éviter cette surparamétrisation, la régression PLS a été introduite car, étant une régression séquentielle, elle permet d'arrêter le processus de régression avant de modéliser l'erreur. Mais la PLS ne propose que quelques solutions et ne permet pas  vraiment d'optimisation. Nous proposons dans cet article une régression séquentielle orthogonale plus souple que la PLS, la régression linéaire séquentielle orthogonale ou RLSO. Cette régression permet d'éviter la surparamétrisation et elle facilite l'optimisation de la modélisation car les directions de projection peuvent être choisies au mieux des intérêts de l'étude. Les solutions possibles proposées par la RLSO sont en nombre infini. Ce qui laisse la possibilité d'utiliser toutes les techniques d'optimisation pour trouver la ou les meilleures solutions. Comme la régression linéaire séquentielle orthogonale (RLSO) est nouvelle, nous avons développé toutes les étapes de calcul. Un exemple numérique est fourni en annexes 9 et 10.	Jacques Goupy	http://editions-rnti.fr/render_pdf.php?p1&p=1001674	http://editions-rnti.fr/render_pdf.php?p=1001674
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Les simulations dans l'enseignement des sondages avec le logiciel Genesis sous SAS et la bibliothèque Sondages	L'objectif de cette présentation est de montrer l'intérêt d'utiliser des simulations dans un cours de théorie des sondages. En simulant des tirages d'échantillons dans une population et en calculant des indicateurs Monte Carlo pour les estimations obtenues à partir des échantillons tirés, on peut facilement comparer des méthodes d'échantillonnage ainsi que des méthodes d'estimation. Pour cela, on présente deux solutions : le logiciel GENESIS sous SAS et la bibliothèque "Sondages'' sous R. L'utilisation de chacune de ces solutions sera illustrée sur des bases de données réelles.	Aragon Yves, David Haziza, Anne Ruiz-Gazen	http://editions-rnti.fr/render_pdf.php?p1&p=1001670	http://editions-rnti.fr/render_pdf.php?p=1001670
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Planification d'Expériences Numériques	Malgré les progrès des outils informatiques, l'étude numérique de certains phénomènes physiques complexes reste très coûteuse en temps de calcul. Il est donc nécessaire de faire appel à des méthodes de planification d'expériences. Cet article présente et analyse les deux approches des expériences numériques les plus utilisées: la méthode classique des plans d'expériences et une méthode par krigeage. L'objectif est donner un aperçu de ce qui ce fait dans le domaine et de guider le lecteur vers une bibliographie détaillée.	Astrid Jourdan	http://editions-rnti.fr/render_pdf.php?p1&p=1001675	http://editions-rnti.fr/render_pdf.php?p=1001675
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Plans d'Expériences Optimaux : Un Exposé Didactique	Cet article est un exposé didactique sur les plans d'expériences optimaux, essentiellement pour les modules linéaires. Son objectif est de donner les définitions et les modes de construction des principaux plans optimaux rencontrés, avec un éclairage basé sur des exemples très simples. Le public visé est d'abord celui des personnels de recherche qui souhaitent améliorer leur démarche expérimentale, face à des problèmes expérimentaux caractérisques par des contraintes de divers types	Jean-Pierre Gauchi	http://editions-rnti.fr/render_pdf.php?p1&p=1001686	http://editions-rnti.fr/render_pdf.php?p=1001686
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Prévision météo 2005-2006 : Attendez-vous à encore plus de cyclones au cours de la saison prochaine		Bernard Labbé	http://editions-rnti.fr/render_pdf.php?p1&p=1001700	http://editions-rnti.fr/render_pdf.php?p=1001700
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	TANAGRA, une plate-forme d'expérimentation pour la fouille de données	TANAGRA est un logiciel « open source » gratuit dédié à la fouille de données. Il s'adresse à deux types de publics. D'un côté, il présente une interface graphique aux normes des logiciels de fouille de données actuels, y compris les logiciels commerciaux, le rendant ainsi accessible à une utilisation de type « chargé d'études » sur des données réelles. De l'autre, du fait que le code source est librement disponible et l'architecture interne très simplifiée, il se prête à une utilisation de chercheurs qui veulent avant tout expérimenter de nouvelles techniques en améliorant celles déjà implémentées ou en introduisant de nouvelles. TANAGRA est opérationnel ; les versions stables sont disponibles sur le Web depuis janvier 2004.	Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1001669	http://editions-rnti.fr/render_pdf.php?p=1001669
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Une Etude Comparative de Logiciels de Prévision Automatique de Séries Chronologiques.	L'objectif de cette étude est de comparer 6 logiciels de prévision automatique de séries chronologiques. On analyse leurs performances sur 50 séries, selon 6 critères à l'aide de plusieurs analyses en composantes principales. Les séries utilisées proviennent de domaines différents : macroéconomie, industrie, finance, démographie, environnement. Les données et les résultats sont au format Zip dans Annexe 1 et Annexe 2.	Valentina Stan	http://editions-rnti.fr/render_pdf.php?p1&p=1001684	http://editions-rnti.fr/render_pdf.php?p=1001684
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Une introduction au positionnement multidimensionnel	Le positionnement multidimensionnel est une méthode de représentation graphique d'un ensemble de similarités ou de dissimilarités mais aussi une procédure de construction d'échelles communes à un ensemble d'attributs subjectifs. Cette note a pour but d'aider les utilisateurs dans la mise en oeuvre du positionnement multidimensionnel au moyen des procédures SPSS PROXIMITIES et ALSCAL. Cette mise en oeuvre concerne l'analyse des tableaux de dissimilarité construits à partir de tableaux multidimensionnels de données individuelles. Le listage des résultats obtenus à partir d'exemples vient illustrer l'exposé théorique consacré aux méthodes.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001665	http://editions-rnti.fr/render_pdf.php?p=1001665
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Une nouvelle approche de simulations des parts de préférence après une analyse conjointe en profils complets : le PRBM.	L'analyse conjointe est très utilisée pour effectuer des prévisions de parts de préférence. Dans le cadre des analyses en profils complets, il n'existe pas à l'heure actuelle de modèle probabiliste prévisionnel performant. Une nouvelle procédure est proposée, le PRBM, basée sur la randomisation des « premiers choix » paramétrée par les résultats des MCO. Ce nouveau modèle prévisionnel contourne l'hypothèse IIA et n'est pas sensible à l'échelle de notation des profils, de plus la variance introduite par le bruit est en phase avec celle des MCO. Différentes simulations et un exemple illustrent ce travail.	Hervé Guyon	http://editions-rnti.fr/render_pdf.php?p1&p=1001667	http://editions-rnti.fr/render_pdf.php?p=1001667
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Une Nouvelle Approche du Sondage Aléatoire Simple	Une approche bayésienne du sondage aléatoire simple offre des solutions simples, pratiques et relativement faciles à exploiter numériquement. Il s'agit de solutions analytiques (loi de probabilité, fonction de répartition, espérance, variance) permettant de prédire, pour une classe donnée, le nombre de représentants dans la population	Martin Körnig	http://editions-rnti.fr/render_pdf.php?p1&p=1001676	http://editions-rnti.fr/render_pdf.php?p=1001676
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2005	Une Raison pour ne pas Abandonner les Tests de Signification de l'Hypothèse Nulle.	It is shown that an interval estimate for a contrast between means can be straight-forwardly computed, given only the observed contrast and the associated t or F teststatistic (or equivalently the corresponding p-value). This interval can be seen as a fre-quentist con¯dence interval, as a standard Bayesian credibility interval, or as a ¯ducialinterval.This gives Null Hypothesis Signi¯cance Tests (NHST) users the possibility of an easytransition towards more appropriate statistical practices. Conceptual links between NHSTand interval estimates are outlined.	Marie-Paule Lecoutre, Jacques Poitevineau, Bruno Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001689	http://editions-rnti.fr/render_pdf.php?p=1001689
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Adaptabilité à l'utilisateur dans le contexte des services Web	Les services Web sont des technologies émergentes permettant uneinteropérabilité entre les différents acteurs (fournisseurs et demandeurs deservices) du fait de leur architecture reposant sur des technologies standard.Cependant à ce jour, aucun des standards des services Web ne prend cependantréellement en charge le concept d'adaptation. Ceci est d'autant plusproblématique que les utilisateurs de services Web attendent d'eux nonseulement qu'ils répondant à leur besoin mais aussi qu'ils soient adaptés à leurprofil (caractéristique personnelles, et celles de leur environnement). Nousproposons une extension du standard de description des services Web(WSDL), appelée AWSDL, afin de supporter l'adaptation des services Web.Un module a été développé pour mettre en correspondance les descriptionsAWSDL des Services Web Adaptés (SWA) et les demandes des utilisateurs.	Jérôme Gensel, Marlène Villanova-Oliver, Céline Lopez-Velasco, Hervé Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1000245	http://editions-rnti.fr/render_pdf.php?p=1000245
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Améliorer la découverte de chroniques par une découpe intelligente d'un log d'alarmes	Cet article décrit une méthode de prétraitement destinée à faciliter ladécouverte de motifs fréquents dans un log d'alarmes. Au cours d'une premièreétape les types d'alarmes qui présentent un comportement temporel similairesont regroupés à l'aide d'une carte auto-organisatrice. Puis on recherche lesparties du log qui sont riches en alarmes pour les différents groupes. Des souslogs sont construits à partir des alarmes des zones sélectionnées. La méthode aété validée sur un log provenant d'un réseau ATM.	Françoise Fessant, Christophe Dousson, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000265	http://editions-rnti.fr/render_pdf.php?p=1000265
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Application des courbes de Peano en recherche d'image par le contenu	Dans cet article nous présentons une application des courbes de Peano pour la caractérisation de régions par leur texture et l'établissement d'inter-relations spatiales à des fins de "CBIR". Les résultats obtenus sont comparables à ceux d'un humain sur une base de 330 images aériennes.	Adel Hafiane , Bertrand Zavidovique	http://editions-rnti.fr/render_pdf.php?p1&p=1000294	http://editions-rnti.fr/render_pdf.php?p=1000294
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Apprentissage automatique des modèles structurels d'objets cartographiques	Pour reconnaître les objets cartographiques dans les images satellitales on a besoin d'un modèle d'objet qu'on recherche. Nous avons développé un système d'apprentissage qui construit le modèle structurel d'objets cartographiques automatiquement à partir des images satellitales segmentées. Les images contenants les objets sont décomposées en formes primitives et transformées en Graphes Relationnels Attribués (ARGs). Nous avons généré les modèles d'objets à partir de ces graphes en utilisant des algorithmes d'appariement de graphes. La qualité d'un modèle est évaluée par la distance d'édition des exemples à ce modèle.	Güray Erus, Nicolas Loménie	http://editions-rnti.fr/render_pdf.php?p1&p=1000293	http://editions-rnti.fr/render_pdf.php?p=1000293
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Apprentissage d'une hiérarchie de concepts pour la conception de modèles de domaine d'hypermédias	Nous décrivons comment apprendre automatiquement une hiérarchiede concepts à partir d'une collection de documents. Les concepts, identifiés pardes ensembles de mots-clés, sont organisés en une hiérarchie de typespécialisation/généralisation. Cette hiérarchie peut être utilisée pour construireun modèle de domaine pour des collections de documents hypermédias. Nousproposons des idées sur la façon de construire des modèles d'utilisateurs àpartir de tels modèles de domaine. Les modèles d'utilisateurs et de domainepeuvent être visualisés à l'aide d'outils efficaces comme les Treemaps.	Hermine Njike Fotzo, Thierry Artières, Patrick Gallinari, Julien Blanchard, Guillaume Letellier	http://editions-rnti.fr/render_pdf.php?p1&p=1000239	http://editions-rnti.fr/render_pdf.php?p=1000239
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Apprentissage relationnel de motifs temporels	Nous présentons deux expériences d'apprentissage relationnel de motifstemporels comportant des contraintes numériques - des chroniques à partirde séries temporelles. La première concerne l'apprentissage d'arythmiescardiaques à partir d'´électrocardiogrammes. La deuxième réalise l'apprentissagede règles prédisant la dégradation de la qualité de service dans un réseaude télécommunications à partir de données d'exploitation. L'influence de laméthode de discrétisation et de segmentation des données sur la qualité desrésultats est discutée.	Marie-Odile Cordier, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1000259	http://editions-rnti.fr/render_pdf.php?p=1000259
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Arbre BIC optimal et taux d'erreur	Nous reconsidérons dans cet article le critère BIC pour arbres d'inductionproposé dans Ritschard et Zighed (2003, 2004) et discutons deux aspects li´esà sa portée. Le premier concerne les possibilités de le calculer. Nous montronscomment il s'obtient à partir des statistiques du rapport vraisemblance utiliséespour tester l'indépendance ligne-colonne de tables de contingence. Le secondpoint porte sur son intérêt dans une optique de classification. Nous illustrons surl'exemple du Titanic la relation entre le BIC et le taux d'erreur en généralisationlorsqu'on regarde leur évolution selon la complexité de l'arbre. Nous esquissonsun plan d'expérimentation en vue de vérifier la conjecture selon laquelle le BICminimum assurerait en moyenne le meilleur taux d'erreur en généralisation.	Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1000303	http://editions-rnti.fr/render_pdf.php?p=1000303
Revue des Nouvelles Technologies de l'Information	AEGC	2005	ARQAT : plateforme exploratoire pour la qualité des règles d'association	Le choix de mesures d'intérêt pour la validation des règles d'association constitue un défi important dans le contexte de l'évaluation de la qualité en fouille de données. Mais, comme l'intérêt dépend à la fois de la structure des données et des buts de l'utilisateur (décideur, analyste), certaines mesures peuvent s'avérer pertinentes dans un contexte donné, et ne plus l'être dans un autre. Dans cet article, nous proposons un outil original ARQAT afin d'étudier le comportement spécifique de 34 mesures d'intérêt dans le contexte d'un jeu de règles, selon une approche résolument exploratoire mettant en avant l'interactivité et les représentations graphiques.	Xuan-Hiep Huynh, Fabrice Guillet, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000310	http://editions-rnti.fr/render_pdf.php?p=1000310
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Cartes cognitives de graphes conceptuels	Le modèle des cartes cognitives offre une représentation graphiqued'un réseau d'influences entre différentes notions. Nous proposons un nouveaumodèle de cartes cognitives qui intègre la partie représentation desconnaissances et l'opération de projection du modèle des graphes conceptuels	Stéphane Loiseau, David Genest	http://editions-rnti.fr/render_pdf.php?p1&p=1000206	http://editions-rnti.fr/render_pdf.php?p=1000206
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Causal Inference in Multi-Agent Causal Models		Sam Maes, Stijn Meganck, Bernard Manderick	http://editions-rnti.fr/render_pdf.php?p1&p=1000227	http://editions-rnti.fr/render_pdf.php?p=1000227
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Classement d'objets incomplets dans un arbre de décision probabiliste	Nous présentons une approche probabiliste pour déterminer les valeursmanquantes des objets incomplets pendant leur classement dans les arbres dedécision. Cette approche est dérivée de la méthode d'apprentissage superviséappelée Arbres d'Attributs Ordonnés proposée par Lobo et Numao en 2000, quiconstruit un arbre de décision pour chacun des attributs, selon un ordre croissanten fonction de l'information mutuelle entre chaque attribut et la classe. Notreapproche étend la méthode de Lobo et Numao d'une part en prenant en compteles dépendances entre les attributs pour la construction des arbres d'attributs, etd'autre part en fournissant un résultat de classement d'un objet incomplet sous laforme d'une distribution de probabilités (au lieu de la classe la plus probable).	Lamis Hawarah, Ana Simonet, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1000282	http://editions-rnti.fr/render_pdf.php?p=1000282
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Complexité de l'extraction des connaissances de données : une vison systémique	Les praticiens et les chercheurs dans le domaine d'extraction des connaissances de données (ECD) sont souvent confrontés à des difficultés qui sont relatives à la nature des données, à l'implication de l'opérateur humain et aux aspects algorithmiques. Aujourd'hui, s'il y a un consensus sur la "complexité" du processus d'ECD, ce n'est pas le cas pour la définition et la caractérisation de cette complexité. Définir la complexité de l'ECD, la caractériser et connaître ses sources sont des questions qui animent aujourd'hui la communauté de fouille de données. Dans cet article, pour répondre à ces questions, nous menons une réflexion sur la notion de complexité en ECD en utilisant l'approche systémique, une approche de modélisation de systèmes complexes.	Walid Ben Ahmed, Mounib Mekhilef, Michel Bigand, Yves Page	http://editions-rnti.fr/render_pdf.php?p1&p=1000277	http://editions-rnti.fr/render_pdf.php?p=1000277
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Data Freshness Evaluation in Different Application Scenarios	Data freshness has been identified as one of the most important dataquality attributes in information systems. This importance increases especially inthe context of systems that integrate a large set of autonomous data sources. In thispaper we describe a quality evaluation framework which allows evaluation of datafreshness in different architectural contexts. We also show how this quality factormay impact the reconfiguration of a data integration system to fulfill userexpectations.	Mokrane Bouzeghoub, Veronika Peralta	http://editions-rnti.fr/render_pdf.php?p1&p=1000297	http://editions-rnti.fr/render_pdf.php?p=1000297
Revue des Nouvelles Technologies de l'Information	AEGC	2005	De l'importance du prétraitement des données pour l'utilisation de l'inférence grammaticale en Web Usage Mining	Le Web Usage Mining est un processus d'extraction de connaissance qui permet la détection d'un type de comportement usager sur un site internet. Cette tâche relève de l'extraction de connaissances à partir de données : plusieurs étapes sont nécessaires à la réalisation du processus complet. Les données brutes, utilisées et souvent incomplètes correspondent aux requêtes enregistrées par un serveur. Le prétraitement nécessaire de ses données brutes pour les rendre exploitables se situe en amont du processus et est donc très important. Nous voulons travailler sur des modèles structurés, issus de l'inférence grammaticale. Nous détaillons un ensemble de techniques de traitement des données brutes et l'évaluons sur des données artificielles. Nous proposons, enfin, des expérimentations mettant en évidence l'affectation des algorithmes classiques d'inférence grammaticale par la mauvaise qualité des logs bruts.	Thierry Murgue	http://editions-rnti.fr/render_pdf.php?p1&p=1000237	http://editions-rnti.fr/render_pdf.php?p=1000237
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Entrepôts de données sur grilles de calcul		Pascal Wehrle, Maryvonne Miquel,  Anne Tchounikine	http://editions-rnti.fr/render_pdf.php?p1&p=1000255	http://editions-rnti.fr/render_pdf.php?p=1000255
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Exploration visuelle d'images IRMf basée sur des Gaz Neuronaux Croissants	Les algorithmes actuels de fouille de données ne supportent que de façon très limitée les mécanismes de guidage et d'engagement d'expert dans le processus de découverte. Dans cet article, nous présentons une nouvelle approche interactive de fouille des images IRMf, guidée par les données, permettant l'observation du fonctionnement cérébral. La discrimination des voxels d'image du cerveau qui présentent une réelle activité est en général très difficile à cause d'un faible rapport signal sur bruit et de la présence d'artefacts. L'exploration de donnée visuelle se focalise sur l'intégration de l'utilisateur dans le processus de découverte de connaissance en utilisant des techniques de visualisation efficaces, d'interaction et de transfert de connaissances. Dans ce travail, nous montrons sur les données réelles, que l'exploration visuelle permet d'accélérer le processus d'exploration d'images IRMf et aboutit à de meilleurs résultats dotées d'une confiance accrue.	Jerzy Korczak, Jean Hommet, Nicolas Lachiche, Christian Scheiber	http://editions-rnti.fr/render_pdf.php?p1&p=1000289	http://editions-rnti.fr/render_pdf.php?p=1000289
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction d' Information Pédagogique à l'aide de Fouilles de Données: une étude de cas	Les systèmes d'apprentissage qui utilisent les TIC peuvent enregistrersous forme électronique de nombreuses données. Ces données peuvent êtrefouillées par des logiciels adéquats pour en retirer des informationspédagogiques. Cet article illustre cette approche en prenant pour exemple leLogic-ITA, un système d'apprentissage pour les preuves formelles en logique.	Aghate Merceron	http://editions-rnti.fr/render_pdf.php?p1&p=1000248	http://editions-rnti.fr/render_pdf.php?p=1000248
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction de connaissances pour la description d'images satellitaires à très haute résolution spatiale	L'arrivée des images de télédétection à très haute résolution spatialeimpose de reconsidérer les méthodes de description des surfaces représentéesdans les images satellites. Dans ce qui suit, nous proposons une approche desegmentation morphologique auto-adaptative d'images satellitaires à très hauterésolution spatiale. La segmentation est associée à l'exploitation des donnéesélicitées tout au long du processus, dans l'objectif de collecter, modéliser ethomogénéiser ces données au sein de descripteurs. Exploiter l'informationainsi disponible sur les objets implique la prise en compte des relationsspatiales, décrivant les relations entre les objets, et leurs caractéristiques. Lamodélisation et la génération de descripteurs proposées rendent une telleapproche opérationnelle originale dans le contexte des méthodes detélédétection.	Florence Sedes, Erick Lopez-Ornelas	http://editions-rnti.fr/render_pdf.php?p1&p=1000296	http://editions-rnti.fr/render_pdf.php?p=1000296
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction de motifs temporels pour la détection dynamique de conflits ethno-politiques	Nous présentons une réalisation en cours sur l'extraction de motifstemporels à partir de séquences d'événements dans le cadre de la détection dynamiquedes conflits ethno-politiques. Notre contexte d'application présenteplusieurs difficultés : le phénomène que l'on cherche à modéliser est fortementvariable et les données sont bruitées. Mais nous disposons d'une connaissancea priori du domaine qui peut être exploitée pour guider l'apprentissage encontraignant l'espace de recherche des motifs. Nous proposons une méthodesupervisée d'apprentissage de scénarios dont l'originalité est d'utiliser une mesurede pertinence qualitative par opposition aux mesures basées sur la fréquence.Cette méthode intègre des concepts de logique floue.	Laure Mouillet, Bernadette Bouchon-Meunier, Emmanuel Collain	http://editions-rnti.fr/render_pdf.php?p1&p=1000266	http://editions-rnti.fr/render_pdf.php?p=1000266
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction de pépites de connaissances dans des réponses d'élèves en langage naturel	Le projet Pépite a pour objectif la construction d'un diagnostic descompétences d'élèves en algèbre élémentaire permettant aux enseignants degérer la diversité cognitive de leurs élèves. Dans cet article, nous présentonsune étude pluridisciplinaire (linguistique, didactique et informatique),s'appuyant sur un corpus de productions d'élèves utilisant le logiciel Pépite.Le corpus est analysé selon les points de vue croisés de la linguistique et de ladidactique. L'objectif de cette démarche est d'améliorer l'évaluation desréponses d'élèves aux questions ouvertes quand ces derniers répondent avecleurs propres mots. Après avoir situé notre étude, nous présentons laméthodologie retenue et les premiers résultats. Nous montrons ensuite lapertinence de ces résultats avec le point de vue de la recherche surl'enseignement des mathématiques. Nous terminons par les perspectivesouvertes par ce travail en nous interrogeant sur les apports d'approches EGC àla problématique du diagnostic de compétences	Sylvie Normand-Assadi, Lalina Coulange, Elisabeth Delozanne, Brigitte Grugeon	http://editions-rnti.fr/render_pdf.php?p1&p=1000252	http://editions-rnti.fr/render_pdf.php?p=1000252
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction de Règles en Incertain par la Méthode Implicative	En relation avec des approches classiques de l'incertain, l'analysestatistique implicative (A.S.I.) peut apparaître innovante, particulièrement pourl'opérateur d'implication. L'article montre en effet que la notion de variables àvaleurs intervalles et celle de variables-intervalles sont efficaces dans ladétermination de leur distribution et dans la recherche de règles entre variablesfloues. De plus, elles apportent de riches informations sur la qualité de cesrègles, tout en permettant d'étudier le rôle des variables supplémentaires dansl'existence de ces règles. Cette nouvelle perspective épistémologique del'incertain ouvre d'intéressantes perspectives d'application.	Régis Gras, Raphaël Couturier, Fabrice Guillet, Filippo Spagnolo	http://editions-rnti.fr/render_pdf.php?p1&p=1000300	http://editions-rnti.fr/render_pdf.php?p=1000300
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction d'indices spatiaux et temporels dans des séquences vidéo couleur	Dans cet article, nous considérons les séquences vidéo couleur comme des données complexes. Notre contribution porte sur deux méthodes adaptées à ce type de données et permettant d'extraire des indices spatiaux et temporels. Nous pensons que ces méthodes peuvent être intégrées avec succès dans un processus plus complexe de fouille de données multimédia, aspect qui ne sera pas abordé ici. Les méthodes présentées sont basées sur l'espace Teinte Saturation Luminance. L'extraction d'indices spatiaux est assimilée au problème de la séparation du fond et des objets, résolu par une approche multi résolution ne nécessitant qu'une seule image. L'extraction d'indices temporels correspond à la détection des changements de plans dans une séquence d'images, obtenue par l'utilisation de mesures de distances indépendantes du contexte. Les caractéristiques communes de nos deux méthodes sont l'utilisation de l'espace TSL, l'efficacité calculatoire, et la robustesse aux artefacts. Nous illustrons ces approches par des résultats obtenus sur des séquences vidéo sportives.	Sébastien Lefèvre, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000270	http://editions-rnti.fr/render_pdf.php?p=1000270
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Extraction non supervisée de motifs temporels, multidimensionnels et hétérogènes Application à la télésurveillance médicale à domicile	Une méthode générique pour l'extraction non supervisée de motifs dans des séquences temporelles multidimensionnelles et hétérogènes est proposée, puis expérimentée pour l'identification des comportements récurrents d'une personne à domicile. L'objectif est de concevoir un système d'apprentissage des habitudes de vie, à partir des données de capteurs, pour la détection d'évolutions critiques à long terme.	Florence Duchêne, Catherine Garbay, Vincent Rialle	http://editions-rnti.fr/render_pdf.php?p1&p=1000262	http://editions-rnti.fr/render_pdf.php?p=1000262
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Fouille de collections de documents en vue d'une caractérisation thématique de connaissances textuelles	De nos jours, les entreprises, organismes ou individus se trouventsubmergés par la quantité d'information et de documents disponibles. Lesutilisateurs ne sont plus capables d'analyser ou d'appréhender ces informationsdans leur globalité. Dans ce contexte, il devient indispensable de proposer denouvelles méthodes pour extraire et caractériser de manière automatique lesinformations contenues dans les bases documentaires. Nous proposons danscet article l'approche IC-Doc de caractérisation automatique et thématique ducontenu de collections de documents textuels. IC-Doc est basée sur uneméthode originale d'extraction et de classification de connaissances textuellesprenant en considération les co-occurrences contextuelles et le partage decontextes entre les différents termes représentatifs du contenu. IC-Doc permetainsi une extraction automatique de KDMs (Knowledge Dynamic Maps) sur lescontenus des bases documentaires. Ces KDMs permettent de guider et d'aiderles utilisateurs dans leurs tâches de consultations documentaires. Ce papierprésente également une expérimentation de notre approche sur des collectionsde documents textuels.	Abdenour Mokrane, Gérard Dray, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000273	http://editions-rnti.fr/render_pdf.php?p=1000273
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Fouille de données du génome à l'aide de modèles de Markov cachés	Nous décrivons un processus de fouille de données en bioinfor-matique. Il se traduit par la spécification de modèles de Markov cachésdu second-ordre, leur apprentissage et leur utilisation pour permettre unesegmentation de grandes séquences d'ADN en différentes classes qui tra-duisent chacune un état organisationnel et structural des motifs d'ADNlocaux sous-jacents. Nous ne supposons aucune connaissance a priori surles séquences que nous étudions. Dans le domaine informatique, ce tra-vail est dédié à la définition d'observations structurées (les k-d-k-mers)permettant la localisation en contexte d'irrégularités, ainsi qu'à la des-cription d'une méthode de classfication utilisant plusieurs classifieurs.Dans le domaine biologique, cet article décrit une méthode pour prédiredes ensembles de gènes co-régulés, donc susceptibles d'avoir des fonctionsliées en réponse à des conditions environnementales spécifiques.	Sébastien Hergalant, Bertrand Aigle, Pierre Leblond, Jean-François Mari	http://editions-rnti.fr/render_pdf.php?p1&p=1000285	http://editions-rnti.fr/render_pdf.php?p=1000285
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Fusion de classifieurs pour la classification d'images sonar	Nous présentons dans ce papier des approches de fusion d'informationshaut niveau applicables pour des données numériques ou des données symboliques.Nous étudions l'intérêt des telles approches particulièrement pour lafusion de classifieurs. Une étude comparative est présentée dans le cadre de lacaractérisation des fonds marins à partir d'images sonar. Reconnaître le type desédiments sur des images sonar est un problème difficile en soi en partie à causede la complexité des données. Nous comparons les approches de fusion d'informationshaut niveau et montrons le gain obtenu.	Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1000271	http://editions-rnti.fr/render_pdf.php?p=1000271
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Génération de descripteurs : interrogation d'images satellitaires par les métadonnées		Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1000247	http://editions-rnti.fr/render_pdf.php?p=1000247
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Gestion de connaissances et de données dans l'aide à la conception de Tissue Microarrays		Julie Bourbeillon, Catherine Garbay, Françoise Giroud	http://editions-rnti.fr/render_pdf.php?p1&p=1000242	http://editions-rnti.fr/render_pdf.php?p=1000242
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Inférence dans les HMM hiérarchiques et factorisés : changement de représentation vers le formalisme des Réseaux Bayésiens.		Sylvain Gelly, Nicolas Bredeche, Michèle Sebag	http://editions-rnti.fr/render_pdf.php?p1&p=1000216	http://editions-rnti.fr/render_pdf.php?p=1000216
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Interface adaptable de requêtes pour un service de MétaDonnées	Dans le cadre d'un projet pluridisciplinaire relatif à la gestionintégrée du littoral (projet Syscolag), nous proposons un système de mutualisationde ressources et de connaissances. Ce système repose sur unservice de métadonnées, une base de données inventaire d'objets géographiquesde référence et un vocabulaire thématique co-construit par l'ensembledes partenaires. L'accès aux ressources partagées est guidé par uneinterface adaptable au gré de l'usage et axée sur des critères de recherchethématique, spatiaux et temporels.	Julien Barde, Jacques Divol, Thérèse Libourel, Pierre Maurel	http://editions-rnti.fr/render_pdf.php?p1&p=1000240	http://editions-rnti.fr/render_pdf.php?p=1000240
Revue des Nouvelles Technologies de l'Information	AEGC	2005	IPEE : Indice Probabiliste d'Ecart à l'Equilibre pour l'évaluation de la qualité des règles	La mesure de la qualité des connaissances est une étape clefd'un processus de découverte de règles d'association. Dans cet article, nousprésentons IPEE, un indice de qualité de règle qui a la particularité uniqued'associer les deux caractéristiques suivantes : d'une part, il est fondé surun modèle probabiliste, et d'autre part, il mesure un écart à l'équilibre(incertitude maximum de la conclusion sachant la prémisse vraie).	Julien Blanchard, Fabrice Guillet, Henri Briand, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000301	http://editions-rnti.fr/render_pdf.php?p=1000301
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Le rôle de l'utilisateur dans un processus d'extraction de règles d'association	De nombreux travaux ont porté sur l'extraction de règlesd'association. Cependant, cette tâche continue à intéresser les chercheurs enfouille de données car elle soulève encore plusieurs défis. En particulier, sonutilisation en pratique reste difficile : d'une part, le nombre de règles apprisesest souvent très grand, d'autre part, le traitement des valeurs numériques danscette tâche est loin d'être maîtrisé. Nous nous intéressons dans cet article aurôle que peut jouer l'utilisateur pour pallier ces difficultés. Il s'agit d'impliquerl'utilisateur dans le processus de recherche de règles d'association qui est dansce cas interactif et guidé par des schémas de règles qu'il aurait choisis. Nousillustrons notre propos avec QuantMiner qui est un outil convivial et interactifque nous avons développé. La présence de l'expert reste indispensable duranttout le processus d'extraction de règles.	Cyril Nortet, Ansaf Salleb-Aouissi, Teddy Turmeaux, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000302	http://editions-rnti.fr/render_pdf.php?p=1000302
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Les Réseaux Bayésiens versus d'autres modèles probabilistes pour le diagnostic multiple de gros systèmes		Véronique Delcroix, Mohamed Amine Maalej, Sylvain Piechowiak	http://editions-rnti.fr/render_pdf.php?p1&p=1000223	http://editions-rnti.fr/render_pdf.php?p=1000223
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Logique Floue appliquée à l'inférence du « Risque Inhérent » en audit financier	Le Risque d'Audit est un indice d'existence d'erreurs dans les étatsfinanciers d'une entreprise. Trois modèles mathématiques sont associés à ceconcept du RA : un modèle « Bayesien », un modèle « évidentialiste », et unmodèle « flou ». Ces trois modèles accusent des incohérences mathématiqueset des difficultés d'application pratique, surtout au niveau de la composante« Risque Inhérent » du risque d'audit. Ils considèrent le processus cognitifd'estimation du RI en tant que « boîte noire ». Nous proposons un simplealgorithme d'inférence flou interprétable pour capter le processus cognitifd'estimation du RI, algorithme basé sur l'induction d'arbre de décision flou.Notre objectif est d'identifier les éléments de cette structure et de démontrerque l'utilisation d'une telle structure d'inférence floue est proche de la décisionréelle d'estimation du RI. Il s'agit d'une recherche exploratoire etexpérimentale.	Souhir Fendri-Kharrat, Hassouna Fedhila, Pierre-Yves Glorennec	http://editions-rnti.fr/render_pdf.php?p1&p=1000210	http://editions-rnti.fr/render_pdf.php?p=1000210
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Mesure d'audience sur Internet par populations de fourmis artificielles	Nous présentons dans ce travail un outil pour la mesure d'audiencesur Internet, reposant sur l'extraction de profils de navigation représentatifs del'activité des internautes sur les sites. Ces profils sont obtenus par l'applicationd'un algorithme de classification non supervisée - inspiré du système dereconnaissance chimique des fourmis - sur des sessions de navigationsconstruites à partir des fichiers log du site étudié. Cet algorithme declassification a été associé à une représentation multimodale des sessionsutilisateurs permettant d'employer l'ensemble des informations à dispositiondans les fichiers log (impacts sur les pages, heure de connexion, durée,séquence des pages, ...), ainsi qu'à une mesure de similarité adaptée pour créerles profils de chacun des clusters obtenus. Il reste cependant d'autres modalités(basées sur le contenu des documents accédés) qui pourraient améliorer lacapacité de l'outil à donner du sens aux profils découverts.	Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1000238	http://editions-rnti.fr/render_pdf.php?p=1000238
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Mesurer l'intérêt des règles d'association	Cet article présente nos travaux sur la mesure de l'intérêt des règles d'association. Une vingtaine de mesures ont été retenues, sur la base d'un critère d'éligibilité.Différentes propriétés sont d'abord proposées qui fondent une étude formelle des mesures. Cette étude formelle se double d'une étude de comportement, grâce à HERBS, une plate-forme développée pour expérimenter les mesures sur des bases de règles. Il est alors possible de confronter la typologie formelle des mesures et la typologie expérimentale. Une fois transformées en critères, ces propriétés fondent une méthode d'assistance au choix de l'utilisateur. Le problème de la validation est enfin abordé, où l'on présente une méthode de contrôle du risque multiple adaptée au problème.	Benoît Vaillant, Patrick Meyer, Elie Prudhomme, Stéphane Lallich, Philippe Lenca, Sébastien Bigaret	http://editions-rnti.fr/render_pdf.php?p1&p=1000313	http://editions-rnti.fr/render_pdf.php?p=1000313
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Méthode sémantique pour la classification et l'iinterrogation de sources de données biologiques	Nous présentons une méthode de classification et de recherche de sources biologiques. Elle consiste à construire un treillis de Galois à partir d'un ensemble de mété-données associées aux sources et converties en propriétés booléenne. Un concept construit à partir d'une requête utilisateur est ensuite inséré dans le treillis grâce à un algorithme de construction incrémentale. Le calcul du résultat se ramène à extraire l'ensemble des sources figurant dans les extensions des subsumants du concept requête dans le treillis de Galois résultant. L'ordre de pertinence des sources est déduit à partir de l'ordre de subsumption des concepts correspondants dans le treillis. Une amélioration de la méthode consiste à enrichir la requête à partir d'ontologies de domaine avant de l'insérer dans le treillis. Deux modes d'enrichissement sont possibles : l'enrichissement par généralisation et l'enrichissement par spécialisation.	Nizar Messai, Marie-Dominique Devignes, Amedeo Napoli, Malika Smail-Tabbone	http://editions-rnti.fr/render_pdf.php?p1&p=1000213	http://editions-rnti.fr/render_pdf.php?p=1000213
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Mise en évidence d'invariants dans une population de cas chirurgicaux	Ces dernières années, les progrès en informatique et en imagerie numérique ont fait émerger une nouvelle discipline, la chirurgie assistée par ordinateur. Les systèmes de chirurgie assistée par ordinateur contribuent à l'amélioration du déroulement des procédures chirurgicales. Un des objectif à long terme de nos travaux est de proposer des solutions d'amélioration de ces systèmes, basées sur les connaissances du chirurgien quant au déroulement de la procédure, par l'utilisation d'un modèle générique qui permet de capturer et de représenter ces connaissances. Cet article présente une méthodologie d'exploitation d'un ensemble de cas chirurgicaux décrits à l'aide de ce modèle générique, par des algorithmes issus de l'extraction de connaissance à partir de données, afin de mettre en évidence des invariants dans les descriptions structurées du déroulement des cas chirurgicaux. Il détaille en outre les difficultés rencontrées de par notamment le caractère complexe des données étudiées.	Mélanie Raimbault, Ricco Rakotomalala, Xavier Morandi, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1000291	http://editions-rnti.fr/render_pdf.php?p=1000291
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Modèles de Markov cachés pour l'estimation de plusieurs fréquences fondamentales		Francis Bach , Michael I. Jordan	http://editions-rnti.fr/render_pdf.php?p1&p=1000214	http://editions-rnti.fr/render_pdf.php?p=1000214
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Modélisation des connaissances émotionnelles par les cartes cognitives floues	Les recherches en psychologie ont permis d'établir une relation entreémotions et prise de décision. La prise en compte de caractéristiques humainestelles que les émotions et la personnalité dans les processus d'interaction entreagents est au centre de ce travail. Il s'inscrit dans le cadre du projet GRACE(Groupes Relationnels d'Agents Collaborateurs Emotionnels)/ RIAM (Réseaudes Industries, de l'Audiovisuel et du Multimédia) .	Nathalie Ronarc'h, Gaële Rozec, Fabrice Guillet, Alexis Nédélec, Serge Baquedano, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000208	http://editions-rnti.fr/render_pdf.php?p=1000208
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Modélisation Sémantique de l'Utilisateur	Notre approche « sémantique de l'utilisabilité », basée sur lacatégorisation, correspond à un mode de représentation des connaissances,sous la forme d'un treillis de Galois qui permet de modéliser et simuler lesprocédures utilisateurs sur un dispositif technique. Cette approche, qui diffèrede celles qu'on trouve avec SOAR ou ACT, associe les actions et lesprocédures aux catégories d'objets, comme propriétés de ces catégories(Poitrenaud, Richard & Tijus, sous presse). L'accès aux actions et procédures alieu à partir des catégories d'objets. Dans le cadre de cette approche, leserreurs relèvent de méprises catégorielles et l'analogie relève des processus dereconnaissance qui ont lieu lors de la catégorisation. La modélisation et lasimulation dans le cadre de cette approche se réalisent avec les formalismesdéveloppés par Poitrenaud (1995): ProcOpe et STONE.	Charles Tijus, Sebastien Poitrenaud, Jean-François Richard	http://editions-rnti.fr/render_pdf.php?p1&p=1000232	http://editions-rnti.fr/render_pdf.php?p=1000232
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Modéliser des connaissances ontologiques dans le cadre du modèle des Graphes Conceptuels	Cet article présente OCGL (Ontology Conceptual Graph Language), un langage de représentation d'ontologie basé sur le modèle des Graphes Conceptuels. Il décrit en détail la façon dont une ontologie est modélisée en OCGL, et présente l'implémentation de ce langage dans l'atelier d'ingénierie ontologique TooCoM.	Frédéric Fürst	http://editions-rnti.fr/render_pdf.php?p1&p=1000205	http://editions-rnti.fr/render_pdf.php?p=1000205
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Nettoyage des données XML : combien ça coûte ?	L'objectif de cet article est de présenter un travail en cours quiconsiste à proposer, implanter et valider expérimentalement un modèle pourestimer le coût d'un processus de nettoyage de documents XML. Notreapproche de calcul de coût est basée sur une méthode par calibration selon uneanalyse probabiliste. Pour cela, nous proposons de calculer des probabilités depollution et au préalable de détection des différents types de pollutions. Pourvalider notre modèle, nous avons choisi de polluer artificiellement unecollection de données XML avec l'ensemble des types d'erreurs possibles(erreurs typographiques, ajout de doublons, de valeurs manquantes, tronquées,censurées, etc.) et d'estimer, grâce au modèle proposé, le nombre et le coût desopérations nécessaires au nettoyage des données afin de proposer des stratégiesde réparation ciblées et économes. Les expérimentations en cours ne sont pasrapportées dans cet article.	Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000298	http://editions-rnti.fr/render_pdf.php?p=1000298
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Recherche d'image par le contenu : requête partielle ou globale, apprentissage en ligne	Nous présentons dans cet article deux méthodes d'élaboration dessignatures, une méthode globale à l'aide d'histogrammes et une méthode dedescription des régions et de leur disposition dans l'image. Nous exposonsensuite une méthode dédiée à la requête partielle qui est basée sur la mise encorrespondance de graphes de régions et une méthode interactive basée surl'apprentissage statistique.	Sylvie Philipp-Foliguet	http://editions-rnti.fr/render_pdf.php?p1&p=1000295	http://editions-rnti.fr/render_pdf.php?p=1000295
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Recherche d'information multimédia : Apport de la fouille de données et des ontologies	A ce jour, le média image est omniprésent dans de nombreusesapplications. Un volume de données considérable est produit ce qui conduit àla nécessité de développer des outils permettant de retrouver efficacement del'information pertinente. Les systèmes de recherche actuels montrentaujourd'hui leurs limites en raison de l'absence de sémantique. Une voie quisemble intéressante à explorer afin de combler le fossé existant entre lespropriétés extraites et le contenu sémantique, est la fouille de données. C'estun domaine de recherche encore immature mais très prometteur. Cet articleprésente des travaux préliminaires sur la manière de définir de nouveauxdescripteurs intégrant la sémantique. Le clustering et la caractérisation desclasses obtenues sont utilisés pour réduire l'espace de recherche et produireune vue résumée de la base. La navigation basée sur une ontologie visuelle estun moyen puissant et convivial pour retrouver de l'information pertinente.	Marie-Aude Aufaure, Marinette Bouet	http://editions-rnti.fr/render_pdf.php?p1&p=1000275	http://editions-rnti.fr/render_pdf.php?p=1000275
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Réflexions sur l'apport de l'exploration des traces d'usage pour améliorer le tri des résultats des moteurs de recherche sur le Web	Nous présentons dans ce papier un système de fouille coopérative dedonnées d'usage de moteurs de recherche sur le Web dont l'objectif estd'améliorer le tri des résultats rendus par un moteur de recherche. Le systèmeest construit selon une architecture multi-agents où chaque utilisateur estassisté par un agent personnel. Les agents coopèrent entre-eux et utilisent laméthodologie du raisonnement à partir de cas pour re-trier les résultats renduspar un moteur de recherche. Nous nous servons de ce système pour 1)présenter notre analyse des choix de conception d'un système d'explorationcoopérative de données d'usage du Web et 2) montrer les problèmes quirestent à résoudre et l'apport attendu des techniques de fouille de donnéesd'usage pour les résoudre.	Rushed Kanawati	http://editions-rnti.fr/render_pdf.php?p1&p=1000234	http://editions-rnti.fr/render_pdf.php?p=1000234
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Représentation contextualisée des pratiques des utilisateurs	Le contexte intervient dans toue étude du comportement humain.Nous présentons les graphes contextuels qui sont utilisés dans de nombreuxdomaines comme l'intelligence artificielle, la psychologie, la sécuritéinformatique, la gestion d'incidents, le diagnostic médical, ... L'idée centralede ce formalisme est la représentation au même niveau des éléments decompréhension d'un utilisateur et des éléments contextuels dans lesquels leséléments de compréhension prennent un sens et ont une validité. Nousdonnons un exemple dans le domaine de la recherche d'information. Cettemodélisation de l'utilisateur au travers de ses actions offre un intérêt pourredéfinir les tâches prescrites dans le cadre du travail collaboratif.	Patrick Brézillon, Charles Tijus	http://editions-rnti.fr/render_pdf.php?p1&p=1000230	http://editions-rnti.fr/render_pdf.php?p=1000230
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Représentation et reconnaissance de caractères manuscrits par Réseaux Bayésiens Dynamiques		Laurence Likforman-Sulem, Marc Sigelle	http://editions-rnti.fr/render_pdf.php?p1&p=1000221	http://editions-rnti.fr/render_pdf.php?p=1000221
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Réseaux Bayésiens de Niveau Deux et D-Séparation	Étant donné une famille de variables aléatoires (Xi)i&#8712;I , munie de la structure de réseau bayésien et un sous-ensemble S de I, nous considérons le problème de calcul de la loi de la sous-famille (Xa)a&#8712;S (resp. la loi de (Xb)b&#8712; ¯ S, où ¯ S = I &#8722; S, conditionnellement à (Xa)a&#8712;S).Nous mettons en évidence la possibilité de décomposer cette tâche en plusieurs calculs parallèles dont chacun est associé à une partie de S (resp. de ¯ S) ; ces résultats partiels sont ensuite regroupés dans un produit. Dans le cas du calcul de (Xa)a&#8712;S, ceci revient à la mise en place sur S d'une structure de réseau bayésien de niveau deux.	Linda Smail, Jean-Pierre Raoult	http://editions-rnti.fr/render_pdf.php?p1&p=1000228	http://editions-rnti.fr/render_pdf.php?p=1000228
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Réseaux bayésiens pour le filtrage d'alarmes dans les systèmes de détection d'intrusions		Ahmad Faour, Philippe Leray, Cédric Foll	http://editions-rnti.fr/render_pdf.php?p1&p=1000225	http://editions-rnti.fr/render_pdf.php?p=1000225
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Sélection d'attributs en fouille de données sur grilles		Sébastien Cahon, Nouredine Melab, El-Ghazali Talbi	http://editions-rnti.fr/render_pdf.php?p1&p=1001498	http://editions-rnti.fr/render_pdf.php?p=1001498
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Service de Cache pour les Grilles de Calcul	Nous proposons un système fédérateur de caches pour les grilles que les applications de la grille utilisent comme un service de cache uniforme. Le système est fondé sur le concept de l'activité de données où les applications partagent et réutilisent l'information sémantique liée à l'activité des données sous la forme de métadonnées. Ces métadonnées représentent la connaissance sur les données et sur leur gestion. Elles permettent d'optimiser, suivant le contenu et l'utilisation de ces données, leur placement, leur recherche, leur durée de vie et leur pertinence vis-à-vis de leur exploitation.	Yonny Cardenas, Jean-Marc Pierson, Lionel Brunie	http://editions-rnti.fr/render_pdf.php?p1&p=1000257	http://editions-rnti.fr/render_pdf.php?p=1000257
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Services contextualisés pour utilisateurs et la modélisation des utilisateurs à base d'ontologies : défis et perspectives	Il existe un besoin d'outils avancés d'apprentissage sur le Web. Ledéveloppement des nouvelles technologies comme le Web sémantique, lecalcul sur grille et les services web ouvrent de nouvelles perspectives et défispour la conception d'une nouvelle génération de systèmes d'apprentissage.Cette nouvelle génération peut être conçue comme des services distribués,autonomes, contextualisés, services web ou grille. Le papier présente le rôle etles perspectives des nouvelles technologies pour le développement d'unenouvelle génération de services d'apprentissage en s'appuyant sur le modèleutilisateur et sur la modélisation des utilisateurs à base d'ontologies.	Liana Razmerita	http://editions-rnti.fr/render_pdf.php?p1&p=1000235	http://editions-rnti.fr/render_pdf.php?p=1000235
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Transformation des concepts du diagramme de classe UML en OWL full	Le web peut être considéré comme une grande base deconnaissances. La recherche des informations pertinentes sur la toile est renduede plus en plus difficile, voire impossible avec l'accroissement de lavolumétrie des pages disponibles. Le problème réside dans le fait que les outilsexistants ne peuvent pas s'appuyer actuellement sur une description ducontenu des documents. Le web sémantique utilise différents langages pourmieux exploiter et traiter les contenus des ressources web. Dans le but depasser de UML vers OWL, il est intéressant d'étudier la possibilité detransformer chacun des concepts du diagramme de classe UML en OWL.	Macaire Ahlonsou, Emmanuel Blanchard, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000204	http://editions-rnti.fr/render_pdf.php?p=1000204
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Un cadre pour l'étude des comportements sur une plate-forme pédagogique : du parcours individuel à la conscience du groupe	Lorsqu'ils sont fortement prescriptifs, les Environnement Interactifspour l'Apprentissage Humain (EIAH) cloisonnent les utilisateurs entre eux.Pour mettre en valeur le travail de chacun, nous voulons faire prendreconscience aux utilisateurs qu'ils font partie d'un groupe qui a des intérêtsconvergents. Nous pensons qu'un système adaptatif est le support appropriépour faire émerger la prise de conscience d'un groupe et ainsi développer despratiques coopératives. Nous proposons de formaliser une architecture quidéfinit des services autonomes opérant en tâche de fond, pour extraire dusystème les données pertinentes déclenchant les mécanismes de signalisationadéquat. Notre support d'expérimentation est une plateforme pour la formationdes professeurs stagiaires de l'enseignement agricole public	Pierre Camps, Marie-Françoise Canut, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1000254	http://editions-rnti.fr/render_pdf.php?p=1000254
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Un modèle d'adaptation pour les oeuvres médiatiques	Les artistes en arts visuels et médiatiques explorent l'utilisation denouvelles technologies pour créer des oeuvres multimédia et qui sont diffuséesen ligne, lors d'expositions ou d'installations. L'utilisation des technologiesfaites par ces créateurs pose de nombreux défis en termes de mécanismes àmettre en oeuvre pour concevoir, créer, expérimenter et diffuser ces oeuvres.Dans cet article, nous nous intéressons aux mécanismes d'adaptation pour lacréation d'oeuvres médiatiques adaptatives et interactives. A travers un casconcret, nous proposons un modèle d'adaptation intégrant la gestion dedifférents types de métadonnées pour réaliser aussi bien l'adaptationsémantique que l'adaptation physique et qui peut être spécialisé selon lesbesoins spécifiques.	Anis Ouali, Brigitte Kerhervé, Odile Marcotte, Paul Landon	http://editions-rnti.fr/render_pdf.php?p1&p=1000244	http://editions-rnti.fr/render_pdf.php?p=1000244
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Un Web sémantique de formation par questionnement	Nous présentons dans cet article un EIAH conçu et développé selondes méthodes et techniques du Web sémantique et de l'ingénierie desconnaissances. L'environnement d'apprentissage est conçu comme unemémoire de formation et le système que nous avons développé constitue unWeb sémantique de formation (par extension de la notion de Web sémantiqued'entreprise). Une approche d'acquisition et de gestion des connaissances a étéadoptée pour expliciter la stratégie pédagogique d'un enseignant, acquérir desressources pédagogiques à partir d'un document de cours initial et organiserces ressources. La visualisation des ressources et la navigation de l'apprenantdans la mémoire ou le Web de formation est basée sur l'utilisation d'un moteurde recherche sémantique.	Sylvain Dehors, Catherine Faron-Zucker, Alain Giboin, Jean-Paul Stromboni	http://editions-rnti.fr/render_pdf.php?p1&p=1000249	http://editions-rnti.fr/render_pdf.php?p=1000249
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Une nouvelle méthode graphique pour interroger et vérifier des diagrammes de classes UML	UML est le langage graphique de référence dans l'industriepour la modélisation objet. Cependant UML reste un langage, et ne fournitaucun moyen de vérification ou d'interrogation de ses schémas. Il existeaujourd'hui des outils de vérification, mais ils se comportent comme desboîtes noires où l'utilisateur ne peut accéder. Nous proposons une méthodegraphique de vérification et d'interrogation de diagrammes de classesUML. L'aspect intuitif et dessinable de notre méthode offre à l'utilisateurla possibilité d'interroger le contenu de diagrammes de classes, ainsique de définir et d'adapter ses propres critères de vérification. Le modèlecalculatoire de notre approche est celui des graphes conceptuels	Thomas Raimbault	http://editions-rnti.fr/render_pdf.php?p1&p=1000203	http://editions-rnti.fr/render_pdf.php?p=1000203
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Une plate-forme de personnalisation basée sur une architecture multi-agents	Dans le contexte d'applications distribuées et d'environnements multi-plateformes, la personnalisation inclut le choix de la plate-forme d'interaction (PC,PDA ...), le mode d'interaction (textuel, vocal, ...), la configuration logicielle, les données fournies etc. Plusieurs méthodes, basées généralement sur le profil de l'utilisateur, permettent de prendre en compte ces différents aspects. Nous proposons une plate forme, à base d'agents logiciels, pouvant servir de support pour la conception de système de personnalisation (SP). Le SP facilite la gestion et la transmission des résultats ou recommandations dans un système distribué et évolutif. Le SP est appliqué pour la personnalisation de l'information transport. Il s'agit d'ici d'aider les usagers des transports collectifs dans leur choix d'itinéraires, de les informer des perturbations éventuelles et de les guider tout au long de leur déplacement.	Abdouroihamane Anli, Emmanuelle Grislin-Le Strugeon, Mourad Abed	http://editions-rnti.fr/render_pdf.php?p1&p=1000233	http://editions-rnti.fr/render_pdf.php?p=1000233
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Une représentation des arborescences pour la recherche de sous-structures fréquentes	La recherche de structures fréquentes au sein de données ar-borescentes est une problématique actuellement très active qui trouvede nombreux intérêts dans le contexte de la fouille de données comme,par exemple, la construction automatique d'un schéma médiateur à par-tir de schémas XML. Dans ce contexte, de nombreuses propositions ontété réalisées mais les méthodes de représentation des arborescences sonttrès souvent trop coûteuses. Dans cet article, nous proposons donc uneméthode originale de représentation de ces données. Les propriétés decette représentation peuvent être avantageusement utilisées par les algorithmes de recherche de structures fréquentes (sous-arbres fréquents). Lareprésentation proposée et les algorithmes associés ont été évalués surdes jeux de données synthétiques montrant ainsi l'inter^et de l'approcheproposée.	Federico Del Razo López, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000280	http://editions-rnti.fr/render_pdf.php?p=1000280
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Validation d'une expertise textuelle par une méthode de classification basée sur l'intensité d'implication	Dans le cadre d'une validation d'expertise textuelle contenue dans un test de compétences comportementales informatisé, nous proposons un méthode visant à extraire des sous ensembles de termes caractéristiques utilisés pour décrire des caractères psychologiques. Notre approche consiste, après l'extraction de termes, à évaluer les associations possibles entre termes et caractères psychologiques qui structurent le corpus en s'appuyant sur la théorie de l'implication statistique.	Jérôme David, Fabrice Guillet, Vincent Philippé, Henri Briand, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000308	http://editions-rnti.fr/render_pdf.php?p=1000308
Revue des Nouvelles Technologies de l'Information	AEGC	2005	Vers une analyse de la dynamique des paramètres physiologiques en Unité de Soins Intensifs	Le monitorage des paramètres physiologiques en milieu deRéanimation génère un flux abondant et continu de données. L'extraction et lasynthèse des informations sont une étape obligatoire pour tout système d'aide àla décision. L'Analyse de la Tendance Linéaire par Morceaux (A.T.L.M.) estune méthode originale d'analyse de la dynamique qui utilise deux niveauxd'interprétation. Le premier niveau, monoparamétrique, exprime la variationde chaque paramètre en quatre classes (constant, diminue, augmente,transitoire) selon la méthode décrite dans (Calvelo et al. 2001). Le secondniveau, multiparamétrique, définit le comportement du système par unevariable continue ; l'introduction de la connaissance du domaine est alorsnécessaire pour différencier des états. La combinaison des deux niveauxd'informations permet l'élaboration de scénarios. Le propos de ce documentest d'exposer la méthode sur des signaux simulés présentant des modificationspouvant s'observer en clinique et d'introduire les perspectives offertes par sonimplémentation en ligne.	Samir Sharshar, M-C. Chambrin	http://editions-rnti.fr/render_pdf.php?p1&p=1000267	http://editions-rnti.fr/render_pdf.php?p=1000267
Revue des Nouvelles Technologies de l'Information	EDA	2005	A Data Warehouse that Gathers Several Formalisms to Capture Data Heterogeneity and Incompleteness in the Field of Food Microbiological Safety	Notre travail porte sur la conception d'un entrepôt de donnéesdans le domaine de l'analyse du risque alimentaire. Les données expérimentalesstockées dans cet entrepôt ont trois caractéristiques principales quiont guidé la conception de l'entrepôt : elles sont hétérogènes, elles peuventêtre imprécises, et l'entrepôt est par nature incomplet. Nous proposonsd'utiliser trois modèles de données : le modèle relationnel, le modèle desgraphes conceptuels et le modèle XML. Ces modèles ont été étendus afinde pouvoir représenter des données imprécises sous la forme de distributions de possibilités. Les bases de données construites sur ces modèles sontinterrogés simultanément avec le langage MIEL++. Dans ce langage, lespréférences de l'utilisateur sont représentées par des sous-ensembles flous.Des techniques de mise en correspondance floue sont utilisées pour comparerpréférences et données imprécises.	Patrice Buche, Juliette Dibie-Barthélemy, Ollivier Haemmerlé, Rallou Thomopoulos	http://editions-rnti.fr/render_pdf.php?p1&p=1000134	http://editions-rnti.fr/render_pdf.php?p=1000134
Revue des Nouvelles Technologies de l'Information	EDA	2005	An Approach to Specify When Reselecting Views to be Materialized	A data warehouse stores a large volume of data extracted from multiple sources. A set of materialized views is defined over the base tables in order to optimize OLAP (On-Line Analytical Processing) query response time. The selection of materialized views may be static or dynamic. The dynamic selection is continually controlled by a system that calibrates the set of views. The static selection is controlled periodically by the data warehouse administrator who provides the parameter values to the view selection program and defines the selection period. A short period may increase the system workload if there are unnecessary executions of the view selection program. A long period may decrease the query response time. In this paper we propose an algorithm to specify when to select views to be materialized in a static policy. That is when the view selection program should be run. Our main contribution is the use of some tolerance parameters to update and reselect the materialized views. The materialized views will be updated only when it is necessary. The view selection program will be executed either at the end of the selection period, defined by the administrator, or when there is a non tolerated increase of the query execution cost. The aim is to reduce the materialization cost and to guarantee a high query response time. Our experiment results show that, for some values of the tolerance parameters, our approach is more profitable than the static view selection algorithms.	Henda Ben Ghézala, Abdelaziz Abdellatif, Ali Ben Ammar	http://editions-rnti.fr/render_pdf.php?p1&p=1000137	http://editions-rnti.fr/render_pdf.php?p=1000137
Revue des Nouvelles Technologies de l'Information	EDA	2005	Approche semi-automatisée de conception de schémas multidimensionnels valides	Cet article s'inscrit dans le cadre d'une approche semi-automatiséede conception de schémas multidimensionnels. Cette approche accepte desbesoins OLAP, exprimés indépendamment de toute source OLTP, sous formede tableaux n-dimensionnels, puis construit des schémas en étoile idéaux. Envue de valider ces étoiles idéales par rapport à une source d'alimentation, nousproposons d'une part, une méthode systématique pour effectuer lacorrespondance de chaque concept multidimensionnel avec la source et d'autrepart, des règles de raffinement des étoiles. Les schémas en étoile valides serontensuite fusionnés pour générer des schémas en constellation.	Ahlem Soussi, Jamel Feki, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000132	http://editions-rnti.fr/render_pdf.php?p=1000132
Revue des Nouvelles Technologies de l'Information	EDA	2005	Conception et optimisation d'un entrepôt de données médicales	Un entrepôt de données intègre des informations provenantdes sources de données internes mais aussi externes à l'entreprise. L'ensemble des données est utilisé pour l'aide à la décision, ainsi la conception du modèle multidimensionnel et la sélection des vues à matérialiserreprésentent un processus complexe et délicat. Dans cet article, nous pro-posons la définition d'un modèle multidimensionnel qui se compose detrois classes : cube, dimension et hiérarchie. Nous proposons également unalgorithme pour la sélection de l'ensemble optimal des vues à matérialiser.Notre algorithme utilise les paramètres de fréquence d'utilisation, de coûtde calcul et de fréquence de mises à jour des relations de base. Nous avonseu l'opportunité de travailler dans le cadre d'un projet médical, ce quinous a permis de vérifier et de valider notre proposition sur des donnéesréelles.	Maria Trinidad Serna Encinas, Michel Adiba	http://editions-rnti.fr/render_pdf.php?p1&p=1000135	http://editions-rnti.fr/render_pdf.php?p=1000135
Revue des Nouvelles Technologies de l'Information	EDA	2005	Enrichissement du OLAP pour l'analyse géographique: exemples de réalisations et différentes possibilités technologiques	D'importants efforts sont déployés depuis une quinzaine d'annéespour mettre en place des systèmes d'aide à la décision sur le territoire. Cessystèmes reposent toutefois sur les systèmes d'information géographique (SIG)et les approches transactionnelles habituelles (OLTP) pour produirel'information géodécisionnelle, souvent avec des délais inacceptables, voiredes coûts prohibitifs au point d'en laisser tomber la production. Parconséquent, les nouvelles applications Spatial OLAP (SOLAP) arrivent à pointpour permettre efficacement le déploiement d'applications d'aide à la décisionet d'exploration des données géographiques. Cet article vise à faire connaîtreles besoins et avantages liés aux applications SOLAP, particulièrement àl'exploration cartographique des données. Puisque de telles applications n'ontpratiquement pas été abordées par la communauté informatique, cet articledélaisse les aspects scientifiques traditionnels du OLAP déjà bien couverts parcette dernière au profit d'exemples concrets d'applications SOLAP et d'unsurvol des principaux concepts propres à celles-ci. Notamment, unecatégorisation en trois familles de solutions y est proposée, soit OLAPdominant,SIG-dominant et intégrée. Chaque exemple d'application y estpositionné et les avantages d'une technologie SOLAP y sont présentés. Richede ces expériences, nous terminons avec quelques "difficultés cachées" de laréférence spatiale qui font l'objet de nos préoccupations de recherche.	Yvan Bédard, Marie-Josée Proulx, Sonia Rivest	http://editions-rnti.fr/render_pdf.php?p1&p=1000128	http://editions-rnti.fr/render_pdf.php?p=1000128
Revue des Nouvelles Technologies de l'Information	EDA	2005	Intégration de versions fonctionnelles dans les entrepôts de données multimédias	Les modèles multidimensionnels ont une structure statique où lesmembres des dimensions sont calculés d'une manière unique. Cependant lesdonnées (particulièrement les données multimédias) sont souvent caractériséespar des descripteurs pouvant être obtenus par divers modes de calcul que nousdéfinissons comme des "versions fonctionnelles" de descripteurs. Nousproposons un modèle multidimensionnel multiversion fonctionnelle ("modèleM2F") en intégrant notamment la notion de "version de dimension" quireprésente des dimensions dont les membres sont calculés selon différentesversions fonctionnelles. Cette nouvelle approche permet d'intégrer au modèleun choix de modes de calcul de ces descripteurs afin de permettre àl'utilisateur de choisir la représentation de données la plus adaptée. Nousmettons en oeuvre un entrepôt de données multimédias dans le domainemédical en intégrant à un modèle multidimensionnel les données multimédiasd'un essai thérapeutique. Nous définissons formellement un modèle conceptuelet présentons le prototype réalisé pour cette étude.	Anne-Muriel Arigon, Maryvonne Miquel,  Anne Tchounikine	http://editions-rnti.fr/render_pdf.php?p1&p=1000133	http://editions-rnti.fr/render_pdf.php?p=1000133
Revue des Nouvelles Technologies de l'Information	EDA	2005	La fragmentation dans les entrepôts de données : une approche basée sur les algorithmes génétiques	La fragmentation horizontale est une technique d'optimisationnon redondante de requêtes décisionnelles de type ROLAP. L'utilisationde cette technique dans les entrepôts de données représente un enjeu plusimportant que dans un contexte de bases de données traditionnelles. Cetteimportance est due au différents choix des tables (de dimensions ou desfaits) à fragmenter. Dans le contexte des entrepôts, la fragmentation n'aun sens que si la table des faits est partitionnée en fonction des schémasde fragmentation des tables de dimensions. Mais ce type de fragmentationde la table des faits pourrait engendrer un nombre important defragments qui rendrait le processus de maintenance très coûteux. Afin deréduire ce nombre ou le rendre contrôlable par l'administrateur de l'entrepôt, nous proposons l'utilisation d'un algorithme génétique. Ce derniera pour but de sélectionner les tables de dimension à fragmenter pour (1)éviter l'explosion du nombre de fragments de la table des faits et (2)garantir une meilleure performance d'exécution des requêtes. Notre algorithmegénétique est développé sous visual C et validé par une étudeexpérimentale en utilisant le banc d'essai APB-1 release II.	Ladjel Bellatreche, Boukhalfa Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1000136	http://editions-rnti.fr/render_pdf.php?p=1000136
Revue des Nouvelles Technologies de l'Information	EDA	2005	Méthode de conception d'une base multidimensionnelle contrainte	Ce papier présente notre méthode de conception de bases de donnéesmultidimensionnelles (BDM) contraintes. Nous proposons une méthode mixtebasée sur une démarche descendante et une démarche ascendante. Ladémarche descendante est composée de trois étapes: la collecte des besoins desdécideurs, la spécification et la formalisation de ces besoins suivant un modèlemultidimensionnel en constellation qui intègre l'expression de contraintessémantiques. La démarche ascendante collecte les données à partir du schémade la source opérationnelle et construit un schéma multidimensionnel candidatpour l'aide à la décision suivant un processus semi-automatique. La phase deconfrontation permet d'intégrer le schéma multidimensionnel des besoins et leschéma candidat obtenu à partir des sources. L'avantage de notre méthode estqu'elle permet de tirer profit des connaissances présentes dans le schémaconceptuel de la source tout en tenant compte des besoins des décideurs. Enoutre, notre méthode est basée sur un schéma multidimensionnel contraintpermettant de valider l'intégrité sémantique des BDM.	Faiza Ghozzi, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000131	http://editions-rnti.fr/render_pdf.php?p=1000131
Revue des Nouvelles Technologies de l'Information	EDA	2005	OLAP query optimization : A Framework for Combining Rule-Based and Cost-Based Approaches	To optimize queries in relational databases, two categories of optimization techniques have been proposed : the Rule-Based Approach (RBA), and the Cost-Based Approach (CBA). In the RBA, the optimizer uses rule transformations using the relational algebra. In the CBA, the optimizer uses a cost model to estimate the potential cost of each ope- ration using statistics about the database and the tables involved in the query. Usually both categories are implemented by commercial DBMSs and are often intermixed. In multidimensional databases however, most of query optimization techniques follow only the CBA to select optimization structures such as : materialized views, advanced indexing schemes and data partitioning. No approach has been proposed yet to rewrite OLAP queries using a multidimensional algebra. In this paper, we show that the RBA can be applied to multidimensional databases by rewriting each OLAP query to obtain an efficient rewritten query that can be executed using a CBA. In particular, we show that the RBA can be used to take into account one of the specificities of OLAP which is the visualization of the OLAP query result. We propose a multidimensional algebra that represents the core of our RBA optimization, and we show how rewritten queries can be processed using the CBA proposed for multidimensional databases.	Ladjel Bellatreche, Arnaud Giacometti, Dominique Laurent, Patrick Marcel, Hassina Mouloudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000138	http://editions-rnti.fr/render_pdf.php?p=1000138
Revue des Nouvelles Technologies de l'Information	EDA	2005	Panorama de travaux autour de l'intégration de données spatio-temporelles dans les hypercubes	Cet article présente un panorama des différents travaux qui sonteffectués dans notre équipe autour de l'intégration des données spatiotemporellesdans les entrepôts de données et les hypercubes. Nous noussommes plus particulièrement intéressés à la prise en compte des évolutionsdans les dimensions spatiales, à la modélisation multidimensionnelle dedonnées continues et à la conception d'interface de navigation dans desdonnées multidimensionnelles spatio-temporelles. Ces quelques résultats nouspermettent d'avancer de nouvelles perspectives dans la modélisation et lavisualisation de données géo-spatiales dans les hypercubes.	 Anne Tchounikine, Maryvonne Miquel, Robert Laurini, Taher Ahmed, Sandro Bimonte, Virginie Baillot	http://editions-rnti.fr/render_pdf.php?p1&p=1000129	http://editions-rnti.fr/render_pdf.php?p=1000129
Revue des Nouvelles Technologies de l'Information	EDA	2005	Une approche de construction d'espaces de représentation multidimensionnels dédiés à la visualisation	Dans un système décisionnel, la composante visuelle est importantepour l'analyse en ligne OLAP. Dans cet article, nous proposonsune nouvelle approche qui permet d'apporter une solution au problèmede visualisation des données engendré par l'éparsité. En se basant sur lesrésultats d'une analyse des correspondances multiples (ACM), nous tentonsd'atténuer l'effet négatif de l'éparsité en organisant différemment lescellules d'un cube de données. Notre méthode ne cherche pas à réduirel'éparsité mais plutôt à construire un espace de représentation se prétantmieux à l'analyse et dans lequel les faits du cube sont regroupés. Pourévaluer l'apport de cette nouvelle représentation des données, nous proposonsun indice d'homogénéité basé sur le voisinage géométrique descellules d'un cube. Les différents tests menés nous ont montré l'efficacitéde notre méthode.	Riadh Ben Messaoud, Kamel Aouiche, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1000130	http://editions-rnti.fr/render_pdf.php?p=1000130
Revue des Nouvelles Technologies de l'Information	EGC	2005	ACKA : Une approche d'acquisition coopérative de connaissances pour la construction d'un modèle de simulation multi-agents	Cet article présente une approche (ACKA an Approach for Cooperative Knowledge Acquisition) participative et coopérative d'acquisition de connaissances nécessaires pour la construction d'un modèle de simulation basé sur des agents. Elle est basée sur le principe de jeu de rôles dans une réunion d'entreprise. Nous proposons de construire un modèle multi-acteurs, représentant un modèle initial du système multi-agents. Dans cette étude, Nous appliquons ACKA pour construire un modèle multi-acteurs pour la compréhension des processus de décision dans les ?rmes de la ?liere avicole. En particulier, nous cherchons à comprendre les impacts des comportements individuels sur la gestion de l'utilisation des matières premières agricoles.	Athmane Hamel, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1000403	http://editions-rnti.fr/render_pdf.php?p=1000403
Revue des Nouvelles Technologies de l'Information	EGC	2005	Acquisition et exploitation de connaissances dans un contexte multi-experts pour un système d'aide à la décision	Nous présentons une méthodologie d'extraction, de gestion et d'exploitation de connaissances dans un contexte multi-experts. Elle repose sur trois étapes : extraction des connaissances de chaque expert, gestion des connaissances individuelles afin de constituer une base de connaissances commune et exploitation de cette base afin de fournir une aide à la décision aux experts. La méthodologie proposée a été mise en oeuvre au Cameroun avec cinq experts en micro-finance. Elle a donné des résultats en adéquation avec les pratiques des experts. Au-delà, on envisage de mettre en oeuvre un système de capitalisation des connaissances. Il doit permettre d'analyser rapidement un plus grand nombre de situations, les experts restant en nombre limité, et contribuer à un transfert de compétences pour former les décideurs locaux. En effet, les experts sont en général membres d'ONG et restent rarement plus de deux ans sur place.	Jean-Pierre Barthélemy, Jean-Robert Kala Kamdjoug, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1000220	http://editions-rnti.fr/render_pdf.php?p=1000220
Revue des Nouvelles Technologies de l'Information	EGC	2005	AID : Un framework intégré de conception d'un schéma objet-relationnel	Devant la prolifération des données complexes qui ne cessent de croître, et la diversité des structures qui se multiplient, la conception des schémas de base de données en général et des schémas objet-relationnels en particulier, est devenue une activité difficile et complexe, qui fait appel à des connaissances variées. Lors de la conception d'un schéma, l'utilisateur (non averti) doit connaître la théorie sous-jacente au modèle de données, de façon à énoncer son modèle, syntaxiquement correct lui permettant de construire un schéma de base de données objet-relationnel répondant à ses besoins. Plusieurs outils spécialisés dans la conception de schémas de base de données provenant aussi bien de la communauté académique que du monde industriel, tels Super, Totem, Rational/Rose, etc. ont été développés dans des contextes et avec des buts souvent très différents. Affin de répondre à ce besoin pressant, nous avons proposé une solution consistant en l'élaboration d'environnements intégrés facilitant la cohabitation de plusieurs modèles et techniques utilisés lors de la conception d'un schéma de base de données. Il s'agit d'offrir une plate-forme logicielle appelée AID (Aided Interface for Database design) offrant des mécanismes opératoires uniformes représentant un soutien graphique et interactif pour une conception incrémentale basée sur des manipulations directes et systémiques des graphes au travers d'une palette graphique d'opérateurs. L'innovation d'AID est son approche systémique qui facilite l'expression des besoins par le concepteur averti ou non, en lui automatisant sa tâche.	Etienne Pichat, Hassan Badir	http://editions-rnti.fr/render_pdf.php?p1&p=1000292	http://editions-rnti.fr/render_pdf.php?p=1000292
Revue des Nouvelles Technologies de l'Information	EGC	2005	Amélioration de la performance de l'Analyse de la Sémantique Latente pour des corpus de petite taille		Fadoua Ataa-Allah, Abderrahim El Qadi, Siham Boulaknadel, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1000278	http://editions-rnti.fr/render_pdf.php?p=1000278
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse comparative de classifications : apport des règles d'association floues	Notre travail s'appuie sur l'analyse d'un corpus bibliographique dans le domaine de la géotechnique à l'aide de cartes réalisées avec la plateforme Stanalyst®. Celui-ci intègre un algorithme de classification automatique non hiérarchique (les K-means axiales) donnant des résultats dépendant du nombre de classes demandé. Cette instabilité rend difficile toute comparaison entre classifications, et laisse un doute quant au choix du nombre de classes nécessaire pour représenter correctement un domaine. Nous comparons les résultats de classifications selon 3 protocoles : (1) analyse des intitulés des classes ; (2) relations entre les classes à partir des membres communs ; (3) règles d'association floues. Les graphes obtenus présentant des similitudes remarquables, nous privilégions les règles d'association floues : elles sont extraites automatiquement et se basent sur la description des classes et non des membres. Ceci nous permet donc d'analyser des classifications issues de corpus différents.	Pascal Cuxac, Martine Cadot, Claire François	http://editions-rnti.fr/render_pdf.php?p1&p=1000359	http://editions-rnti.fr/render_pdf.php?p=1000359
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse de données symboliques et graphe de connaissances d'un agent	Dans cet article nous appliquons l'analyse de données symboliques au graphe de connaissances d'un agent. Nous présentons une mesure de similarité entre des données symboliques adaptée à nos graphes de connaissances. Nous utilisons les pyramides symboliques pour extraire un nouvel objet symbolique. Le nouvel objet est ensuite réinséré dans le graphe où il peut être utilisé par l'agent, faisant ainsi évoluer sa sémantique. Il peut alors servir d'individu lors des analyses ultérieures, permettant de découvrir de nouveaux concepts prenant en compte l'évolution de la sémantique.	Philippe Caillou, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000408	http://editions-rnti.fr/render_pdf.php?p=1000408
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse géométrique des données pour l'affinement de la connaissance : cas des données EPGY (Education Program for Gifted Students, Stanford University)		Brigitte Le Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1000256	http://editions-rnti.fr/render_pdf.php?p=1000256
Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse stochastique de séquences d'événements discrets pour la découverte de signatures	Cet article concerne la découverte de signatures (ou modèles de chroniques) à partir d'une séquence d'événements discrets (alarmes) générée par un agent cognitif de surveillance (Monitoring Cognitive Agent ou MCA).Considérant un couple (Processus, MCA) comme un générateur stochastique d'événements discrets, deux représentations complémentaires permettent de caractériser les propriétés stochastiques et temporelles d'un tel générateur : une chaîne de Markov à temps continu et une superposition de processus de Poisson. L'étude de ces deux représentations duales permet de découvrir des "signatures" décrivant les relations stochastiques et temporelles entre événements dans une séquence. Ces signatures peuvent alors être utilisées pour reconnaître des comportements spécifiques, comme le montre l'application de l'approche à un outil de production industriel piloté par un système Sachem, le MCA développé et utilisé par le groupe Arcelor pour aider au pilotage de ses outils de production.	Philippe Bouché, Marc Le Goc	http://editions-rnti.fr/render_pdf.php?p1&p=1000219	http://editions-rnti.fr/render_pdf.php?p=1000219
Revue des Nouvelles Technologies de l'Information	EGC	2005	Annotation de textes par extraction d'informations lexicosyntaxiques  et acquisition de schémas conceptuels de causalité	Nous présentons la méthode INSYSE (INterface SYntaxe SEmantique) pour l'annotation de documents textuels. Notre objectif est de construire des annotations sémantiques de ces résumés pour interroger le corpus sur la fonction des gènes et leurs relations de causalité avec certaines maladies. Notre approche est semi-automatique, centrée sur (1) l'extraction d'informations lexico-syntaxiques à partir de certaines phrases du corpus comportant des lexèmes de causation, et (2) l'élaboration de règles basées sur des grammaires d'unification permettant d'acquérir à partir de ces informations des schémas conceptuels instanciés. Ceux-ci sont traduits en annotations RDF(S) sur la base desquelles le corpus de textes peut être interrogé avec le moteur de recherche sémantique Corese.	Laurent Alamarguy, Rose Dieng-Kuntz, Catherine Faron-Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000258	http://editions-rnti.fr/render_pdf.php?p=1000258
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage automatique des modèles structurels d'objets cartographiques	Pour reconnaitre les objets cartographiques dans les images satellitales on a besoin d'un modèle d'objet qu'on recherche. Nous avons développé un système d'apprentissage qui construit le modèle structurel d'objets cartographiques automatiquement a partir des images satellitales segmentées. Les images contenants les objets sont décomposées en formes primitives et sont transformées en Graphes Relationnels Attribués (ARGs). Nous avons généré les modèles d'objets a partir de ces graphes, en utilisant des algorithmes d'appariement de graphes. La qualité d'un modèle est évaluée par la distance d'édition des exemples a ce modèle. Nous sommes parvenus a obtenir des modèles de ponts et de ronds-points qui sont compatibles avec les modèles construits manuellement.	Güray Erus, Nicolas Loménie	http://editions-rnti.fr/render_pdf.php?p1&p=1000336	http://editions-rnti.fr/render_pdf.php?p=1000336
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de scénarios à partir de séries temporelles multivariées		Thomas Guyet, Catherine Garbay, Michel Dojat	http://editions-rnti.fr/render_pdf.php?p1&p=1000224	http://editions-rnti.fr/render_pdf.php?p=1000224
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de signatures de facteurs de transcription à partir de données d'expression	L'inférence de signatures de facteurs de transcription à partir des données puces à ADN a déjà été étudié dans la communauté bioinformatique. La principale difficulté à résoudre est de trouver un ensemble d'heuristiques pertinentes, afin de contrôler la complexité de résolution de ce problème NP-difficile. Nous proposons dans cet article une solution heuristique alternative à celles utilisées dans les approches bayésiennes, fondée sur la recherche de motifs fréquents maximaux dans une matrice discrétisée issue des données numériques de puces ADN. Notre méthode est appliquée sur des données de cancer de vessie de l'Institut Curie et de l'Hôpital Henri Mondor de Créteil.	Mohamed Elati, Céline Rouveirol, François Radvanyi	http://editions-rnti.fr/render_pdf.php?p1&p=1000416	http://editions-rnti.fr/render_pdf.php?p=1000416
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de structures de réseaux Bayésiens et données incomplètes	Le formalisme des modèles graphiques connait actuellement un essor dans les domaines du "machine learning". En particulier, les réseaux bayésiens sont capables d'effectuer des raisonnements probabilistes à partir de données incomplètes alors que peu de méthodes sont actuellement capables d'utiliser les bases d'exemples incomplètes pour leur apprentissage. En s'inspirant du principe de AMS-EM proposé par (Friedman, 1997) et des travaux de(Chow & Liu, 1968), nous proposons une méthode permettant de faire l'apprentissage de réseaux bayésiens particuliers, de structure arborescente, à partir de données incomplètes. Une étude expérimentale expose ensuite des résultats préliminaires qu'il est possible d'attendre d'une telle méthode, puis montre le gain potentiel apporté lorsque nous utilisons les arbres obtenus comme initialisation d'une méthode de recherche gloutonne comme AMS-EM.	Olivier François, Philippe Leray	http://editions-rnti.fr/render_pdf.php?p1&p=1000222	http://editions-rnti.fr/render_pdf.php?p=1000222
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage non supervisé de séries temporelles à l'aide des k-Means et d'une nouvelle méthode d'agrégation de séries	L'utilisation d'un algorithme d'apprentissage non supervisé de type k-Means sur un jeu de séries temporelles amène à se poser deux questions : Celle du choix d'une mesure de similarité et celle du choix d'une méthode effectuant l'agrégation de plusieurs séries afin d'en estimer le centre (i.e. calculer les k moyennes). Afin de répondre à la première question, nous présentons dans cet article les principales mesures de similarité existantes puis nous expliquons pourquoi l'une d'entre elles (appelée Dynamic Time Warping) nous paraît la plus adaptée à l'apprentissage non supervisé. La deuxième question pose alors problème car nous avons besoin d'une méthode d'agrégation respectant les caractéristiques bien particulières du Dynamic Time Warping. Nous pensons que l'association de cette mesure de similarité avec l'agrégation Euclidienne peut générer une perte d'informations importante dans le cadre d'un apprentissage sur la "forme" des séries. Nous proposons donc une méthode originale d'agrégation de séries temporelles, compatible avec le Dynamic Time Warping, qui améliore ainsi les résultats obtenus à l'aide de l'algorithme des k-Means.	Nicolas Nicoloyannis, Rémi Gaudin	http://editions-rnti.fr/render_pdf.php?p1&p=1000250	http://editions-rnti.fr/render_pdf.php?p=1000250
Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage supervisé pour la classification des images basé sur la structure P-tree	Un problème important de la production automatique de règles de classification concerne la durée de génération de ces règles ; en effet, les algorithmes mis en oeuvre produisent souvent des règles pendant un certain temps assez long. Nous proposons une nouvelle méthode de classification à partir d'une base de données images. Cette méthode se situe à la jonction de deux techniques : l'algèbre de P-tree et l'arbre de décision en vue d'accélérer le processus de classification et de recherche dans de grandes bases d'images. La modélisation que nous proposons se base, d'une part, sur les descripteurs visuels tels que la couleur, la forme et la texture dans le but d'indexer les images et, d'autre part, sur la génération automatique des règles de classification à l'aide d'un nouvel algorithme C4.5(P-tree). Pour valider notre méthode, nous avons développé un système baptisé C.I.A.D.P-tree qui a été implémenté et confronté à une application réelle dans le domaine du traitement d'images. Les résultats expérimentaux montrent que cette méthode réduit efficacement le temps de classification.	Rim Faiz, Najeh Naffakhi, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1000343	http://editions-rnti.fr/render_pdf.php?p=1000343
Revue des Nouvelles Technologies de l'Information	EGC	2005	Arbre de décision sur des données de type intervalle : évaluation et comparaison	Le critère de découpage binaire de Kolmogorov-Smirnov nécessite un ordre total des valeurs prises par les variables explicatives. Nous pouvons ordonner des intervalles fermés bornés de nombres réels de différentes façons. Notre contribution dans cet article consiste à évaluer et à comparer des arbres de décision obtenus sur des données de type intervalle à l'aide du critère de découpage binaire de Kolmogorov-Smirnov étendu à ce type de données (Mballo et al. 2004). Pour ce faire, nous axons notre attention sur le taux d'erreur mesuré sur l'échantillon de test. Pour estimer ce paramètre, nous divisons aléatoirement chaque base de données en deux parties égales en terme d'effectif (à un objet près) pour construire deux arbres. Ces deux arbres sont d'abord testés par un même échantillon puis par deux échantillons différents.	Chérif Mballo, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000215	http://editions-rnti.fr/render_pdf.php?p=1000215
Revue des Nouvelles Technologies de l'Information	EGC	2005	Caractérisation d'une région d'intérêt dans les images	Une image est un support d'information qui a montré son efficacité. Néanmoins une image comporte souvent plusieurs zones, l'arrière plan et une zone d'intérêt privilégiée. La vision humaine permet la segmentation de manière naturelle et intégrant toute la connaissance que le sujet peut avoir de l'objectif visé par l'image. Nous proposons ici une méthode de détermination des régions d'intérêt d'une image numérique comme zones saillantes. Les lois de Zipf et Zipf inverse sont adaptées au traitement des images et permettent d'évaluer la complexité structurelle d'une image. Une comparaison des modèles locaux évalués sur des imagettes permet de mettre en évidence une région de l'image. Deux méthodes de classification ont été utilisées pour la détermination de la région d'intérêt : la partition d'un nuage de points représentant les caractéristiques associées aux imagettes, et les réseaux de neurones. Cette méthode de détection permet d'obtenir des zones d'intérêt conformes à la perception humaine. On opère une hiérarchisation sur les zones en fonction de la structuration de l'information élémentaire, les pixels.	Yves Caron, Pascal Makris, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000338	http://editions-rnti.fr/render_pdf.php?p=1000338
Revue des Nouvelles Technologies de l'Information	EGC	2005	CHIC : traitement de données avec l'analyse implicative	Cet article a pour but de montrer les possibilités offertes par le logiciel CHIC (Classification Hiérarchique Implicative et Cohésitive) pour effectuer certaines analyses de données. Il est basé sur la théorie de l'Analyse Statistique Implicative ou A.S.I. développée par Régis Gras et ses collaborateurs. Le principe premier de l'A.S.I. repose sur la problématique d'une mesure des règles d'association du type : «si a alors b» dans une population instanciant les variables a et b. CHIC enrichit sa réponse, établie sur des bases statistiques, en évaluant la responsabilité des sujets dans l'élection de la règle. L'article présent explique la démarche à suivre pour utiliser le logiciel ainsi que les possibilités offertes par celui-ci.	Raphaël Couturier, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000423	http://editions-rnti.fr/render_pdf.php?p=1000423
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification 2-3 Hiéarchique de données du Web		Sergiu Theodor Chelcea, Brigitte Trousse	http://editions-rnti.fr/render_pdf.php?p1&p=1000253	http://editions-rnti.fr/render_pdf.php?p=1000253
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification d'un tableau de contingence et modèle probabiliste	Les modèles de mélange, qui supposent que l'échantillon est formé de sous-populations caractérisées par une distribution de probabilité, constitue un support théorique intéressant pour étudier la classification automatique. On peut ainsi montrer que l'algorithme des k-means peut être vu comme une version classifiante de l'algorithme d'estimation EM dans un cas particulièrement simple de mélange de lois normales. Lorsque l'on cherche à classifier les lignes (ou les colonnes) d'un tableau de contingence, il est possible d'utiliser une variante de l'algorithme des k-means, appelé Mndki2, en s'appuyant sur la notion de profil et sur la distance du khi-2. On obtient ainsi une méthode simple et efficace pouvant s'utiliser conjointement à l'analyse factorielle des correspondances qui s'appuie sur la même représentation des données. Malheureusement et contrairement à l'algorithme des k-means classique, les liens qui existent entre les modèles de mélange et la classification ne s'appliquent pas directement à cette situation. Dans ce travail, nous montrons que l'algorithme Mndki2 peut être associé, à une approximation près, à un modèle de mélange de lois multinomiales.	Gérard Govaert, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000251	http://editions-rnti.fr/render_pdf.php?p=1000251
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification non supervisée et visualisation 3D de documents	Le nombre de documents issus d'une requête sur le Web devient de plus en plus important. Cela nous amène à chercher des solutions pour aider l'utilisateur qui est confronté à cette masse de données. Une alternative possible à un affichage linéaire non triée selon un critère, consiste à effectuer une classification des résultats. C'est dans ce but que l'on s'intéresse aux cartes auto-organisatrices de Kohonen qui sont issues d'un d'algorithme de classification non supervisée. Cependant, il faut ajouter des contraintes à cet algorithme afin qu'il soit adapté à la classification des résultats d'une requête. Par exemple, il doit être déterministe. De plus la classification obtenue dépend fortement de la distance utilisée pour comparer deux documents. On évalue alors l'impact de différentes distances ou dissimilarités, afin de trouver la plus adaptée à notre problème. Un compromis doit également être trouvé entre le temps d'exécution de l'algorithme et la qualité de la classification obtenue. Pour cela, l'utilisation d'un échantillonnage est envisagée. Enfin, ces travaux sont intégrés dans un prototype qui permet de visualiser les résultats en trois dimensions et d'interagir avec eux.	Nicolas Bonnel, Annie Morin, Alexandre Cotarmanac'h	http://editions-rnti.fr/render_pdf.php?p1&p=1000379	http://editions-rnti.fr/render_pdf.php?p=1000379
Revue des Nouvelles Technologies de l'Information	EGC	2005	Classifying XML Materialized Views for their maintenance on distributed Web sources	Ces dernières années ont mis en évidence la croissance et la diversité des informations électroniques accessibles sur le web. C'est ainsi que les systèmes d'intégration de données tels que des médiateurs ont été conçus pour intégrer ces données distribuées et hétérogènes dans une vue uniforme. Pour faciliter l'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'interrogation pour XML qui s'est imposé pour les systèmes basés sur XML. Ainsi XQuery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources. Pour optimiser l'évaluation de requêtes, les vues sont matérialisées lors de la mise à jour des sources, car dans le contexte de sources web, très peu d'informations sont fournies par les sources. Les méthodes habituellement proposées ne peuvent pas être appliquées. Cet article étudie comment mettre à jour des vues matérialisées XML sur des sources web, au sein d'une architecture de médiation.	Tuyet-Tram Dang-Ngoc, Virginie Sans, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000331	http://editions-rnti.fr/render_pdf.php?p=1000331
Revue des Nouvelles Technologies de l'Information	EGC	2005	Combinaison de fonctions de préférence par Boosting pour la recherche de passages dans les systèmes de question/réponse	Nous proposons une méthode d'apprentissage automatique pour la sélection de passages susceptibles de contenir la réponse à une question dans les systèmes de Question-Réponse (QR). Les systèmes de RI ad hoc ne sont pas adaptés à cette tâche car les passages recherchés ne doivent pas uniquement traiter du même sujet que la question mais en plus contenir sa réponse. Pour traiter ce problème les systèmes actuels ré-ordonnent les passages renvoyés par un moteur de recherche en considérant des critères sous forme d'une somme pondérée de fonctions de scores. Nous proposons d'apprendre automatiquement les poids de cette combinaison, grâce à un algorithme de réordonnancement défini dans le cadre du Boosting, qui sont habituellement déterminés manuellement. En plus du cadre d'apprentissage proposé, l'originalité de notre approche réside dans la définition des fonctions allouant des scores de pertinence aux passages. Nous validons notre travail sur la base de questions et de réponses de l'évaluation TREC-11 des systèmes de QR. Les résultats obtenus montrent une amélioration significative des performances en terme de rappel et de précision par rapport à un moteur de recherche standard et à une méthode d'apprentissage issue du cadre de la classification.	Nicolas Usunier, Massih-Reza Amini, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000169	http://editions-rnti.fr/render_pdf.php?p=1000169
Revue des Nouvelles Technologies de l'Information	EGC	2005	De la statistique des données à la statistique des connaissances : avancées récentes en analyse des données symboliques		Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000437	http://editions-rnti.fr/render_pdf.php?p=1000437
Revue des Nouvelles Technologies de l'Information	EGC	2005	Élagage et aide à l'interprétation symbolique et graphique d'une pyramide	Le but de ce travail est de faciliter l'interprétation d'une classification pyramidale construite sur un tableau de données symboliques. Alors que dans une hiérarchie binaire le nombre de paliers est égal à n-1, si n est le nombre d'individus à classer, dans le cas d'une pyramide ce dernier peut atteindre n(n-1)/2. Afin de réduire ce nombre, on élague la pyramide et on utilise un critère de sélection de paliers basé sur la hauteur. De plus on décrit tous les paliers retenus par des variables que l'on sélectionne également en utilisant "le degré de généralité" ainsi que des mesures de dissimilarités de type symbolique-numérique. L'aide à l'interprétation se sert d'outils graphiques et interactifs grâce à la bibliothèque OpenGL. Enfin une simulation montre comment évoluent ces sélections quand le nombre de classes et de variables croit.	Kutluhan Kemal Pak, Mohamed Cherif Rahal, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000226	http://editions-rnti.fr/render_pdf.php?p=1000226
Revue des Nouvelles Technologies de l'Information	EGC	2005	Enrichissement sémantique de documents XML représentant des tableaux	Ce travail a pour objectif la construction automatique d'un entrepôt thématique de données, à partir de documents de format divers provenant du Web. L'exploitation de cet entrepôt est assurée par un moteur d'interrogation fondé sur une ontologie. Notre attention porte plus précisément sur les tableaux extraits de ces documents et convertis au format XML, aux tags exclusivement syntaxiques. Cet article présente la transformation de ces tableaux, sous forme XML, en un formalisme enrichi sémantiquement dont la plupart des tags et des valeurs sont des termes construits à partir de l'ontologie.	Fatiha Saïs, Hélène Gagliardi, Ollivier Haemmerlé, Nathalie Pernelle	http://editions-rnti.fr/render_pdf.php?p1&p=1000309	http://editions-rnti.fr/render_pdf.php?p=1000309
Revue des Nouvelles Technologies de l'Information	EGC	2005	Entrepôt de Données Spatiales basé sur GML : Politique  de Gestion de Cache		Lionel Savary, Georges Gardarin, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000290	http://editions-rnti.fr/render_pdf.php?p=1000290
Revue des Nouvelles Technologies de l'Information	EGC	2005	Évaluation des algorithmes LEM et eLEM pour données continues	Très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange, l'algorithme EM présente l'inconvénient majeur de converger parfois lentement. Son application sur des tableaux de grande taille devient ainsi irréalisable. Afin de remédier à ce problème, plusieurs méthodes ont été proposées. Nous présentons ici le comportement d'une méthode connue, LEM, et d'une variante que nous avons proposée récemment eLEM. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification, et nous illustrons le bon comportement de notre variante sur des données continues simulées et réelles.	François-Xavier Jollois, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000231	http://editions-rnti.fr/render_pdf.php?p=1000231
Revue des Nouvelles Technologies de l'Information	EGC	2005	Expériences de classification d'une collection de documents XML de structure homogène	Cet article présente différentes expériences de classification de documents XML de structure homogène, en vue d'expliquer et de valider une présentation organisationnelle pré-existante. Le problème concerne le choix des éléments et mots utilisés pour la classification et son impact sur la typologie induite. Pour cela nous combinons une sélection structurelle basée sur la nature des éléments XML et une sélection linguistique basée sur un typage syntaxique des mots. Nous illustrons ces principes sur la collection des rapports d'activité 2003 des équipes de recherche de l'Inria en cherchant des groupements d'équipes (Thèmes) à partir du contenu de différentes parties de ces rapports. Nous comparons nos premiers résultats avec les thèmes de recherche officiels de l'Inria.	Thierry Despeyroux, Yves Lechevallier, Brigitte Trousse, Anne-Marie Vercoustre	http://editions-rnti.fr/render_pdf.php?p1&p=1000243	http://editions-rnti.fr/render_pdf.php?p=1000243
Revue des Nouvelles Technologies de l'Information	EGC	2005	Expérimentations sur un modèle de recherche d'information utilisant les liens hypertextes des pages Web	La fonction de correspondance, qui permet de sélectionner et de classer les documents par rapport à une requête est un composant essentiel dans tout système de recherche d'information. Nous proposons de modéliser une fonction de correspondance prenant en compte à la fois le contenu et les liens hypertextes des pages Web. Nous avons expérimenté notre système sur la collection de test TREC-9, et nous concluons que pour certains types de requêtes, inclure le texte ancre associé aux liens hypertextes des pages dans la fonction de similarité s'avère plus efficace.	Bich-Liên Doan, Idir Chibane	http://editions-rnti.fr/render_pdf.php?p1&p=1000264	http://editions-rnti.fr/render_pdf.php?p=1000264
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles	Nous traitons l'extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles. La méthode proposée nous permet de découvrir des règles d'association au niveau des concepts. Cette extension implique notamment de nouvelles définitions pour le support et la confiance afin d'exploiter la structure symbolique des données. Au fil de l'article l'exemple classique du panier de la ménagère est développé. Ainsi, plutôt que d'extraire des règles entre différents articles appartenant à des mêmes transactions enregistrées dans un magasin comme dans le cas classique, nous extrayons des règles d'association au niveau des clients afin d'étudier leurs comportements d'achat.	Filipe Afonso, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000348	http://editions-rnti.fr/render_pdf.php?p=1000348
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extension des bases de données inductives pour la découverte de chroniques	Les bases de données inductives intègrent le processus de fouille de données dans une base de données qui contient à la fois les données et les connaissances induites. Nous nous proposons d'étendre les données traitées afin de permettre l'extraction de motifs temporels fréquents et non fréquents à partir d'un ensemble de séquences d'évènements. Les motifs temporels visés sont des chroniques qui permettent d'exprimer des contraintes numériques sur les délais entre les occurrences d'évènements.	Alexandre Vautier, Marie-Odile Cordier, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1000314	http://editions-rnti.fr/render_pdf.php?p=1000314
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction Bayésienne et intégration de patterns représentés suivant les K plus proches voisins pour le go 19x19	Cet article décrit la génération automatique et l'utilisation d'une base de patterns pour le go 19x19. La représentation utilisée est celle des K plus proches voisins. Les patterns sont engendrés en parcourant des parties de professionnels. Les probabilités d'appariement et de jeu des patterns sont également estimées à ce moment là. La base créée est intégrée dans un programme existant, Indigo. Soit elle est utilisée comme un livre d'ouvertures en début de partie, soit comme une extension des bases pré-existantes du générateur de coups du programme. En terme de niveau de jeu, le gain résultant est estimé à 15 points en moyenne.	Bruno Bouzy, Guillaume Chaslot	http://editions-rnti.fr/render_pdf.php?p1&p=1000212	http://editions-rnti.fr/render_pdf.php?p=1000212
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction bilingue de termes médicaux dans un corpus  parallèle anglais/français	Le Catalogue et Index des Sites Médicaux Francophones (CISMeF) recense les principales ressources institutionnelles de santé en français. La description de ces ressources, puis leur accès par les utilisateurs, se fait grâce à la terminologie CISMeF, fondée sur le thésaurus américain Medical Subject Headings (MeSH). La version française du MeSH comprend tous les descripteurs MeSH, mais de nombreux synonymes américains restent à traduire. Afin d'enrichir la terminologie, nous proposons ici une méthode de traduction automatique de ces synonymes. Pour ce faire, nous avons constitué deux corpus parallèles anglais/français du domaine médical. Après alignement semi-automatique des corpus paragraphe à paragraphe, nous avons procédé automatiquement à l'appariement bilingue des termes. Pour cela, le lexique constitué des descripteurs MeSH américains et de leur traduction en français a fourni les couples amorces qui ont servi de point de départ à la propagation syntaxique des liens d'appariement. 217 synonymes ont pu être traduits, avec une précision de 70%.	Aurélie Névéol, Sylwia Ozdowska	http://editions-rnti.fr/render_pdf.php?p1&p=1000413	http://editions-rnti.fr/render_pdf.php?p=1000413
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de la localisation des termes pour le classement des documents	Trouver et classer les documents pertinents par rapport à une requête est fondamental dans le domaine de la recherche d'information. Notre étude repose sur la localisation des termes dans les documents. Nous posons l'hypothèse que plus les occurrences des termes d'une requête se retrouvent proches dans un document alors plus ce dernier doit être positionné en tête de la liste de réponses. Nous présentons deux variantes de notre modèle à zone d'influence, la première est basée sur une notion de proximité floue et la seconde sur une notion de pertinence locale.	Annabelle Mercier, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1000269	http://editions-rnti.fr/render_pdf.php?p=1000269
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de règles d'association quantitatives application à des données médicales	L'extraction de règles d'association est devenue aujourd'hui une tâche populaire en fouille de données. Cependant, l'algorithme Apriori et ses variantes restent dédiés aux bases de données renfermant des informations catégoriques.Nous proposons dans cet article QuantMiner, qui est un outil que nous avons développé dans le but d'extraire des règles d'association gérant variables catégoriques et numériques. L'outil que nous proposons repose sur un algorithme génétique permettant de découvrir de façon dynamique les intervalles des variables numériques apparaissant dans les règles.Nous présentons également une application réelle de notre outil sur des données médicales relatives à la maladie de l'athérosclérose et donnons des résultats de notre expérience pour la description et la caractérisation de cette maladie.	Cyril Nortet, Ansaf Salleb-Aouissi, Teddy Turmeaux, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000352	http://editions-rnti.fr/render_pdf.php?p=1000352
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de termes centrée autour de l'expert	Nous développons un logiciel, Exit, capable d'aider un expert à extraire des termes qu'il trouve pertinents dans des textes de spécialité. Tout est mis en place pour faciliter le travail de l'expert afin qu'il puisse consacrer son temps à la seule reconnaissance des termes pertinents. Pour cela, différentes mesures statistiques et de nombreuses options d'extraction sont disponibles dans Exit. Afin d'utiliser au mieux les connaissances de l'expert, notre approche est semi-automatique. De plus, l'expert construit des termes pouvant inclure des termes précédemment extraits ce qui rend itératif et constructif notre processus de formation des termes. Enfin, l'ergonomie du logiciel a profité des enseignements tirés lors de son utilisation pour une compétition internationale d'extraction de connaissances.	Thomas Heitz, Mathieu Roche, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000425	http://editions-rnti.fr/render_pdf.php?p=1000425
Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction des connaissances pour l'enrichissement des bases  de données géographiques		Sami Faiz, Khaoula Mahmoudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000394	http://editions-rnti.fr/render_pdf.php?p=1000394
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fonctions d'oubli et conservation de détail dans les entrepôts de données		Aliou Boly, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000287	http://editions-rnti.fr/render_pdf.php?p=1000287
Revue des Nouvelles Technologies de l'Information	EGC	2005	Forage distribué des données : une comparaison entre l'agrégation d'échantillons et l'agrégation de règles	Pour nous attaquer au problème du forage de très grandes bases de données distribuées, nous proposons d'étudier deux approches. La première est de télécharger seulement un échantillon de chaque base de données puis d'y effectuer le forage. La deuxième approche est de miner à distance chaque base de données indépendamment, puis de télécharger les modèles résultants, sous forme de règles de classification, dans un site central où l'agrégation de ces derniers est réalisée. Dans cet article, nous présentons une vue d'ensemble des techniques d'échantillonnage les plus communes. Nous présentons ensuite cette nouvelle technique de forage distribué des données où la mécanique d'agrégation est basée sur un coefficient de confiance attribué à chaque règle et sur de très petits échantillons de chaque base de données. Le coefficient de confiance d'une règle est calculé par des moyens statistiques en utilisant le théorème limite centrale. En conclusion, nous présentons une comparaison entre les meilleures techniques d'échantillonnage que nous avons trouvées dans la littérature, et notre approche de forage distribué des données (FDD) basée sur l'agrégation de modèles.	Mohamed Aounallah, Sébastien Quirion, Guy W. Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000211	http://editions-rnti.fr/render_pdf.php?p=1000211
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de Données Relationnelles dans les SGBD		Cédric Udréa, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000288	http://editions-rnti.fr/render_pdf.php?p=1000288
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de graphes et découverte de règles d'association : application à l'analyse d'images de document	Cet article présente une méthode permettant la découverte non supervisée de motifs fréquents représentatifs de symboles sur des images de documents. Les symboles sont considérés comme des entités graphiques porteurs d'information et les images de document sont représentées par des graphes relationnels attribués. Dans un premier temps, la méthode réalise la découverte de sous-graphes disjoints fréquents et fait correspondre pour chacun d'eux un symbole différent. Une recherche des règles d'association entre ces symboles permet alors d'accéder à une partie des connaissances du domaine décrit par ces symboles. L'objectif à terme est d'utiliser les symboles découverts pour la classification ou la recherche d'images dans un flux hétérogène de document là ou une approche supervisée n'est pas envisageable.	Eugen Barbu, Pierre Héroux, Sébastien Adam, Éric Trupin	http://editions-rnti.fr/render_pdf.php?p1&p=1000341	http://editions-rnti.fr/render_pdf.php?p=1000341
Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de textes pour orienter la construction d'une ressource terminologique	La finalité de ce papier est d'analyser l'apport de techniques de fouille de données textuelles à une méthodologie de construction d'ontologie à partir de textes. Le domaine d'application de cette expérimentation est celui de l'accidentologie routière. Dans ce contexte, les résultats des techniques de fouille de données textuelles sont utilisés pour orienter la construction d'une ressource terminologique à partir de procès-verbaux d'accidents. La méthode TERMINAE et l'outil du même nom offrent le cadre général pour la modélisation de la ressource. Le papier présente les techniques de fouille employées et l'intégration des résultats des fouilles dans les différentes étapes du processus de construction de la ressource.	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1000261	http://editions-rnti.fr/render_pdf.php?p=1000261
Revue des Nouvelles Technologies de l'Information	EGC	2005	Hiérarchisation des règles d'association en fouille de textes	L'extraction de règles d'association est souvent exploitée comme méthode de fouille de données. Cependant, une des limites de cette approche vient du très grand nombre de règles extraites et de la difficulté pour l'analyste à appréhender la totalité de ces règles. Nous proposons donc de pallier ce problème en structurant l'ensemble des règles d'association en hiérarchies. La structuration des règles se fait à deux niveaux. Un niveau global qui a pour objectif de construire une hiérarchie structurant les règles extraites des données. Nous définissons donc un premier type de subsomption entre règles issue de la subsomption dans les treillis de Galois. Le second niveau correspond à une analyse locale des règles et génère pour une règle donnée une hiérarchie de généralisation de cette règle qui repose sur des connaissances complémentaires exprimées dans un modèle terminologique. Ce niveau fait appel à un second type de subsomption inspiré de la subsomption en programmation logique inductive. Nous définissons ces deux types de subsomptions, développons un exemple montrant l'intérêt de l'approche pour l'analyste et étudions les propriétés formelles des hiérarchies ainsi proposées.	Rokia Bendaoud, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000268	http://editions-rnti.fr/render_pdf.php?p=1000268
Revue des Nouvelles Technologies de l'Information	EGC	2005	Intégration efficace des arbres de décision dans les SGBD : utilisation des index bitmap	Nous présentons dans cet article une nouvelle approche de fouille qui permet d'appliquer des algorithmes de construction d'arbres de décision en répondant à deux objectifs : (1) traiter des bases volumineuses, (2) en des temps de traitement acceptables. Le premier objectif est atteint en intégrant ces algorithmes au coeur des SGBD, en utilisant uniquement les outils fournis par ces derniers. Toutefois, les temps de traitement demeurent longs, en raison des nombreuses lectures de la base. Nous montrons que, grâce aux index bitmap, nous réduisons à la fois la taille de la base d'apprentissage et les temps de traitements. Pour valider notre approche, nous avons implémenté la méthode ID3 sous forme d'une procédure stockée dans le SGBD Oracle.	Cécile Favre, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000281	http://editions-rnti.fr/render_pdf.php?p=1000281
Revue des Nouvelles Technologies de l'Information	EGC	2005	L'automate textuel pour la prise en compte de l'évolution du texte	Il n'est plus à rappeler que le corpus textuel, est tel qu'il est actuellement, intraitable à l'échelle que sa croissance nous confirme l'obligation d'utiliser des outils automatique de traitement. Cet article s'intéresse plus particulièrement à la caractérisation de textes et par là même à celle d'auteurs. A l'heure actuelle, toutes les méthodes existant travaillent sur le document fini, sans admettre qu'un cheminement existe entre le début du document et sa fin. Nous proposons une méthode tentant d'apporter cette notion d'évolution textuelle en traitant le texte par un automate et l'évaluation choisie. Puis nous présenterons des résultats validés par des experts, obtenus sur un corpus d'entretiens sociologiques.	Hubert Marteau, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000276	http://editions-rnti.fr/render_pdf.php?p=1000276
Revue des Nouvelles Technologies de l'Information	EGC	2005	La démarche ontologique pour la gestion des compétences et des connaissances	La gestion des ressources humaines repose d'une part sur la connaissance des individus et de leurs compétences et d'autre part sur la connaissance de l'organisation et de ses métiers. C'est par la "mise en correspondance" de ces connaissances qu'il est possible d'améliorer l'emploi, de valoriser les connaissances et les compétences individuelles et de mieux gérer l'organisation. Cette mise en correspondance nécessite une représentation explicite des connaissances, ce qui permet de répondre à de nouveaux besoins : annuaire de compétences, gestion des projets et des retours d'expériences, identification des connaissances à risques, etc.Nous verrons dans le cadre de cet article l'intérêt de l'approche ontologique tant d'un point de vue méthodologique pour la clarification des notions mises en jeu dans le cadre de la GPECC (Gestion Prévisionnelle des Emplois des Compétences et des Connaissances) que pour la construction, la représentation et la maintenance des référentiels des compétences, des connaissances et des métiers. Elle permet en particulier une gestion de l'information par la terminologie et le sens métier propre à l'organisation.	Christophe Roche, Charles Foveau, Samah Reguigui	http://editions-rnti.fr/render_pdf.php?p1&p=1000299	http://editions-rnti.fr/render_pdf.php?p=1000299
Revue des Nouvelles Technologies de l'Information	EGC	2005	La réussite universitaire : prédictions par génération de règles		Nadine Meskens, Jean-Philippe Vandamme, Abdelhakim Artiba	http://editions-rnti.fr/render_pdf.php?p1&p=1000361	http://editions-rnti.fr/render_pdf.php?p=1000361
Revue des Nouvelles Technologies de l'Information	EGC	2005	Les NTIC au services de la capitalisation des connaissances		Jean-Paul Barthès	http://editions-rnti.fr/render_pdf.php?p1&p=1000439	http://editions-rnti.fr/render_pdf.php?p=1000439
Revue des Nouvelles Technologies de l'Information	EGC	2005	Logiciel d'aide à l'étiquetage morpho-syntaxique de  textes de spécialité	La compréhension de textes de spécialité nécessite un étiquetage morpho-syntaxique de bonne qualité. Or, lorsque les textes étudiés sont issus de domaines spécifiques et peu usités, il est rare de disposer de dictionnaires et autres ressources lexicales fiables. Le logiciel que nous proposons permet d'utiliser un étiquetage réalisé par un étiqueteur généraliste, puis d'améliorer cet étiquetage en intégrant des connaissances d'experts du domaine étudié. Grâce au logiciel développé, il est relativement aisé pour un expert du domaine de détecter des erreurs d'étiquetage et de mettre en place des règles de ré-étiquetage. Ces règles peuvent être obtenues de deux manières différentes : (1) soit en utilisant un langage de programmation permettant d'exprimer des règles complexes de ré-étiquetage, (2) soit par apprentissage automatique des règles à partir d'exemples corrigés au moyen d'une interface dédiée. Cet apprentissage propose de nouvelles règles à l'expert, acquises automatiquement.	Ahmed Amrani, Jérôme Azé, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000421	http://editions-rnti.fr/render_pdf.php?p=1000421
Revue des Nouvelles Technologies de l'Information	EGC	2005	Manipulation et fusion de données multidimensionnelles	Cet article définit une algèbre permettant de manipuler des tables dimensionnelles extraites d'une base de données multidimensionnelles. L'algèbre intègre un noyau minimum d'opérateurs unaires permettant d'effectuer les analyses décisionnelles par combinaison d'opérateurs. Cette algèbre intègre un opérateur binaire permettant la fusion de tables dimensionnelles facilitant les corrélations des sujets analysés.	Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000286	http://editions-rnti.fr/render_pdf.php?p=1000286
Revue des Nouvelles Technologies de l'Information	EGC	2005	Méthode de construction d'ontologie de termes à partir du treillis de l'iceberg de Galois	L'approche présentée dans cet article a pour objectif la construction d'une ontologie à partir du treillis de l'iceberg de Galois. Nous entendons par ontologie un ensemble de termes structurés entre eux par un ensemble de liens de divers types. Dans notre cas d'étude, cette ontologie constitue un support de connaissances "documentaires". En effet, elle peut être utilisée dans diverses applications en Recherche d'Information (RI), telles que l'indexation automatique et l'expansion de requêtes ainsi qu'en text-mining. La méthode de construction que nous proposons est fondée sur l'analyse formelle de concepts (AFC) et plus précisément, la structure du treillis de l'iceberg de Galois. En utilisant cette structure hiérarchique partiellement ordonnée, nous présentons une translation directe des relations laticielles vers celles ontologiques. Nous proposons ainsi d'enrichir l'ontologie dérivée par des règles associatives génériques entre termes, découvertes dans le cadre d'un processus de text-mining.	Cherif Chiraz Latiri, Mehdi Mtir, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1000304	http://editions-rnti.fr/render_pdf.php?p=1000304
Revue des Nouvelles Technologies de l'Information	EGC	2005	Microarray data mining : recent advances		Gregory Piatetsky-Shapiro	http://editions-rnti.fr/render_pdf.php?p1&p=1000443	http://editions-rnti.fr/render_pdf.php?p=1000443
Revue des Nouvelles Technologies de l'Information	EGC	2005	Mining Frequent Queries in Star Schemes	L'extraction de toutes les requêtes fréquentes dans une base de données relationnelle est un problème di±cile, même si l'on ne considère que des requêtes conjonctives. Nous montrons que ce problème devient possible dans le cas suivant : le schéma de la base est un schéma en étoile, et les données satisfont un ensemble de dépendances fonctionnelles et de contraintes référentielles. De plus, les schémas en étoile sont appropriés pour les entrepôts de données et que les dépendances fonctionnelles et les contraintes référentielles sont les contraintes les plus usuelles dans les bases de données. En considérant le modèle des instances faibles, nous montrons que les requêtes fréquentes exprimées par sélection-projection peuvent être extraites par des algorithmes de type Apriori.	Tao-Yuan Jen, Dominique Laurent, Nicolas Spyratos, Oumar Sy	http://editions-rnti.fr/render_pdf.php?p1&p=1000283	http://editions-rnti.fr/render_pdf.php?p=1000283
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation de connaissances pour un système de médiation	Travaillant sur l'élaboration d'une méthodologie de développement de systèmes de médiation intégrés dans des systèmes coopératifs, nous avons proposé une architecture à 3 composants : le premier concerne la coopération, le second l'assistance et le troisième est relatif aux connaissances nécessaires aux 2 précédents. Dans cet article nous présentons plus particulièrement le point de vue des connaissances. Ces connaissances sont de 2 natures : des connaissances statiques, sur le domaine par exemple, et des connaissances acquises pendant l'utilisation coopérative du système, notamment la mémoire des activités et les descriptions des actes de résolutions de problèmes. Pour illustrer cette modélisation de connaissances, nous nous intéresserons aux activités coopératives de suivi, de gestion et d'évaluation de projets d'étudiants, assistées par l'outil iPédagogique.	Victoria Eugenia Ospina, Alain-Jérôme Fougères, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1000383	http://editions-rnti.fr/render_pdf.php?p=1000383
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation de la cognition sociale - propositions autour de  l'utilisation de schémas cognitifs		Jorge Louçã	http://editions-rnti.fr/render_pdf.php?p1&p=1000395	http://editions-rnti.fr/render_pdf.php?p=1000395
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation des individus et de leurs relations pour l'aide à l'intégration des individus dans l'organisation	L'objectif de ce papier est de présenter une contribution à la modélisation des individus et de leurs relations pour permettre l'aide à l'intégration des acteurs dans une organisation. Nous étudions en particulier le cas du remplacement d'un acteur (« turn-over »). Dans ce cadre, nous proposons un modèle regroupant un ensemble de données relatives à un individu, aux relations que celui-ci entretient avec les autres acteurs et à son espace informationnel. L'étude porte sur la mise en oeuvre de mécanismes d'aide fournissant à un acteur les moyens de son intégration : la mise à disposition d'une image des espaces informationnels et relationnels de son prédécesseur ainsi que la mise en relation de l'acteur avec les autres acteurs de l'organisation. Cette étude est menée en partenariat avec des experts en GRH.	Marie-Françoise Canut, Max Chevalier, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1000392	http://editions-rnti.fr/render_pdf.php?p=1000392
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation des interactions entre individus avec AgentUML	Pour faciliter l'étude de certains phénomènes, des outils de simulation ont été créés dans de nombreux domaines. L'étude du comportement humain à jusque là échappé à cette tendance. Aujourd'hui, les systèmes multi-agents couplés aux avancées des sciences humaines fournissent les bases nécessaires à l'élaboration de ce type d'outil. Cet article s'inscrit ainsi dans cette dynamique avec l'objectif de développer un outil de simulation du comportement d'individus traumatisés crâniens sur une chaîne de production. Cet outil doit permettre la collecte de la connaissance relative au système étudié et fournir une aide à la décision pour les responsables de l'entreprise. Cet article propose une modélisation des interactions entre individus dans le formalisme AgentUML. Une implémentation du modèle au sein d'un outil de simulation fonctionnel et les résultats obtenus seront également présentés. A terme, le but est la production de données de simulation exploitables par des techniques d'ECD.	Stéphane Daviet, Fabrice Guillet, Henri Briand, Adina Magda Florea, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000399	http://editions-rnti.fr/render_pdf.php?p=1000399
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation d'objets mobiles dans un entrepôt de données	La gestion d'objets mobiles a connu un regain d'intérêt ces dernières années, particulièrement dans le but de gérer et de prédire la localisation d'objets mobiles. Cependant, il y a peu de recherches sur l'exploitation d'historiques de bases d'objets mobiles. La première étape dans ce processus est la mise en oeuvre d'un entrepôt d'objets mobiles. Seulement, les modèles d'entrepôts existants ne permettent pas de traiter directement ce type de données complexes. Cet article présente une approche originale pour pallier ce problème. Cette approche offre la puissance de l'algèbre OLAP sur toute combinaison de données classiques, spatiales et/ou temporelles et mobiles. Elle a été validée par un prototype et appliquée à l'analyse de la mobilité urbaine1. Les résultats de l'expérimentation montrent la validité de l'approche et les tests de performances son efficacité.	Tao Wan, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000284	http://editions-rnti.fr/render_pdf.php?p=1000284
Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation d'un agent émotionnel en UML et RDF	Pouvoir extraire de la connaissance à partir d'une plate-forme de simulation est aujourd'hui envisageable en conjuguant les avancées obtenues en Intelligence Artificielle autour des systèmes multi-agents et les méthodes de formalisation et d'extraction des connaissances. C'est donc dans un cadre général de gestion des connaissances que nous proposons de modéliser un agent artificiel doté de connaissances et d'émotions. Pour cela, une expertise psychologique a été recueillie et formalisée de manière à être stockée dans une base de connaissances sous forme de règles et de classes en UML et RDF. L'implémentation du modèle permet d'entrevoir les perspectives d'une telle simulation : enrichissement par des données issues de simulations, découverte de nouvelles connaissances par l'application de processus d'ECD.	Hélène Desmier, Fabrice Guillet, Adina Magda Florea, Henri Briand, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000406	http://editions-rnti.fr/render_pdf.php?p=1000406
Revue des Nouvelles Technologies de l'Information	EGC	2005	Motifs séquentiels flous : un peu, beaucoup, passionément	La plupart des bases de données issues du monde réel sont constituées de données numériques et historiées (données de capteurs, données scientifiques, données démographiques). Dans ce cadre les algorithmes d'extraction de motifs séquentiels, s'ils sont adaptés au caractère temporel des données ne permettent pas le traitement de données numériques. es données sont alors pré-traitées pour les transformer en données binaire, ce qui entraîne une perte d'information. Des algorithmes ont donc été proposés pour traiter les données numériques sous forme d'intervalles et d'intervalles flous notamment. En ce qui concerne la recherche de motifs séquentiels fondée sur des intervalles flous, les deux méthodes de la littérature ne sont pas satisfaisantes car incomplètes soit dans le traitement des séquences soit dans le calcul du support. Dans cet article, nous proposons donc trois méthodes d'extraction de motifs séquentiels flous {SPEEDYFUZZY, MINIFUZZY et TOTALLYFUZZY) et en détaillons les algorithmes sous-jacents en soulignant les différents niveaux de fuzzification. Ces algorithmes sont implémentés et évalués à travers différentes expérimentations menées sur des jeux de tests synthétiques.	Céline Fiot, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000357	http://editions-rnti.fr/render_pdf.php?p=1000357
Revue des Nouvelles Technologies de l'Information	EGC	2005	Notion de sémantiques bien-formées pour les règles	La notion de règles entre attributs est très générale, allant des règles d'association en fouille de données aux dépendances fonctionnelles (DF) en bases de données. Malgré cette diversité, la syntaxe des règles est toujours la même, seule leur sémantique diffère. Pour une sémantique donnée, en fonction des propriétés induites, des techniques algorithmiques sont mises en oeuvre pour découvrir les règles à partir des données. A partir d'un ensemble de règles, il est aussi utile en pratique de raisonner sur ces règles, comme cela est le cas par exemple avec les axiomes d'Armstrong pour les dépendances fonctionnelles. Dans cet article, nous proposons un cadre qui permet de s'assurer qu'une sémantique donnée pour les règles est bien-formée, i.e. les axiomes d'Armstrong sont justes et complets pour cette sémantique. Les propositions faites dans ce papier proviennent du contexte applicatif de l'analyse de données de biopuces. A partir de plusieurs sémantiques pour les données d'expression de gènes, nous montrons comment ces sémantiques s'intègrent dans le cadre présenté.	Marie Agier, Jean-Marc Petit	http://editions-rnti.fr/render_pdf.php?p1&p=1000207	http://editions-rnti.fr/render_pdf.php?p=1000207
Revue des Nouvelles Technologies de l'Information	EGC	2005	Outil de classification et de visualisation de grands volumes de données mixtes	Nous avons conçu un outil de classification de données original que nous détaillons dans le présent article. Cet outil comporte un module de création de résumés et un module d'affichage. Le module de visualisation permet une lecture aisée des résumés grâce à une interface graphique évoluée permettant la présentation et l'exploration des résumés sous forme d'une hiérarchie de profils ou d'un tableau de profils. Chaque profil donne de manière claire les informations relatives au résumé de données correspondant. La lecture de la hiérarchie et du tableau est aussi grandement facilitée par les choix d'un ordre optimal pour la présentation des variables et des résumés.	Christophe Candillier, Noureddine Mouaddib	http://editions-rnti.fr/render_pdf.php?p1&p=1000369	http://editions-rnti.fr/render_pdf.php?p=1000369
Revue des Nouvelles Technologies de l'Information	EGC	2005	Prise en compte des « Points de Vue » pour l'annotation d'un processus d'Extraction de Connaissances à partir de Données	Dans cet article on propose une nouvelle approche qui rend explicite la notion de point de vue dans une analyse multivues issue d'un processus d'Extraction de Connaissances à partir de Données (ECD). Par point de vue, nous entendons la vision particulière d'un analyste lors de son processus ECD, vision référant à un corps de connaissances qui lui est spécifique. On cherche, d'une part, à faciliter la réutilisabilité et l'adaptabilité du processus, et d'autre part à garder une trace des points de vues sous-jacents aux analyses faites. Le processus d'ECD sera vu comme un processus de génération et de transformation de vues qui seront annotées par des métadonnées pour garder la sémantique de la connaissance extraite. Un positionnement de notre approche vis-à-vis des travaux méthodologiques du processus d'ECD sera donné. Des éléments de modélisation du processus ECD basé sur les points de vue seront décrits au niveau ontologique. Enfin, on illustrera notre approche sur l'analyse des usages d'un site web à partir des fichiers log, selon le point de vue fiabilité.	Hicham Behja, Brigitte Trousse, Abdelaziz Marzak	http://editions-rnti.fr/render_pdf.php?p1&p=1000263	http://editions-rnti.fr/render_pdf.php?p=1000263
Revue des Nouvelles Technologies de l'Information	EGC	2005	Problématiques de gestion de connaissances dans le cadre de l'enseignement à distance sur l'Internet.	Le développement des réseaux à haut-débit et de l'Internet fournit un nouveau support à l'enseignement à distance. Aujourd'hui, de nombreux acteurs dans le domaine de l'enseignement ont mis en place des dispositifs de formation en ligne. Ceux-ci se composent généralement d'une sélection de matériaux organisés et présentés de manière à suivre un programme pédagogique particulier, de mécanismes de communication entre apprenants et enseignants, et d'outils de suivi des apprenants. Les plates-formes d'enseignement à distance devenant de plus en plus génériques, des nouveaux modèles ont été définis, standardisés ou normalis és, permettant la formalisation de méta-données pédagogiques ou tentant d'évaluer les connaissances acquises par les apprenants. En nous appuyant sur ces modèles, nous proposons de construire une base de connaissances, associant notamment les termes des domaines enseignés en relations à sémantique pédagogique. L'exploitation de cette base de connaissances fournit un premier niveau d'aide à l'ingénierie pédagogique, en particulier lorsque le volume de contenus en ligne est important. Des inférences mettant en jeu ces connaissances permettent alors un meilleur suivi du dispositif d'enseignement.	Romain Dailly, Christian Chervet, Rémi Lehn, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000389	http://editions-rnti.fr/render_pdf.php?p=1000389
Revue des Nouvelles Technologies de l'Information	EGC	2005	Processus de traitement de données radar pour la reconnaissance/identification de cibles aériennes		Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000345	http://editions-rnti.fr/render_pdf.php?p=1000345
Revue des Nouvelles Technologies de l'Information	EGC	2005	Raisonnement en gestion des compétences	Nous nous intéressons au raisonnement sur les compétences des ressources humaines pour simplifier leur gestion. Dans cet article, nous proposons une méthode de raisonnement pour l'aide à l'identification des compétences d'un individu. Un processus de knowledge-mining défini par analogie avec l'extraction de règles d'association en data-mining est proposé afin d'induire une base de règles à partir d'une base de connaissances sur le domaine. De plus, un prototype a été développé pour expérimenter notre approche sur un exemple académique.	Emmanuel Blanchard, Mounira Harzallah, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000385	http://editions-rnti.fr/render_pdf.php?p=1000385
Revue des Nouvelles Technologies de l'Information	EGC	2005	RASMA : Une approche Multi-Agent pour l'amélioration de l'Algorithme des Règles d'Associations Spatiales		Hajer Baazaoui Zghal, Radhia Ben Hamed, Sami Faiz, Henda Ben Ghézala	http://editions-rnti.fr/render_pdf.php?p1&p=1000365	http://editions-rnti.fr/render_pdf.php?p=1000365
Revue des Nouvelles Technologies de l'Information	EGC	2005	Réécriture de requêtes multimédias : approche basée sur l'usage d'une ontologie	Nous proposons dans cet article une stratégie de réécriture de requêtes sur des données multimédias décrites moyennant le standard MPEG-7. Ce standard se base sur XML schéma qui permet de décrire la structure des données. Cependant, aucune sémantique n'est assignée à cette structure. Nous proposons d'étendre ce standard d'une ontologie permettant d'exprimer les connaissances du domaine. Ainsi, l'ontologie sera utilisée durant l'indexation des données multimédias et la réécriture de requêtes. Le but de la réécriture de requêtes est de transformer une requête initiale en une ou plusieurs requêtes équivalentes ou sémantiquement proches compte tenu des connaissances représentées dans l'ontologie.	Samira Hammiche, Salima Benbernou, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1000306	http://editions-rnti.fr/render_pdf.php?p=1000306
Revue des Nouvelles Technologies de l'Information	EGC	2005	Règles de propagation pour la création d'ontologies d'annotation de ressources	L'annotation se distingue de l'indexation automatique par l'utilisation d'une ou plusieurs ontologies qui définissent un domaine global de référence permettant de cadrer et de normaliser les annotations effectuées, par ailleurs une ressource annotée doit l'être non pas par une liste de mots clefs, mais bien par une ou plusieurs ontologies. Malheureusement, il est peu réaliste de penser que les centaines de millions de ressources mises à disposition sur le Web puissent être annotées par leurs auteurs. Pour résoudre ce problème, notre démarche consiste à indexer les documents en se basant sur l'ontologie globale et ensuite propager les annotations en utilisant des documents déjà annotés pour annoter d'autres documents référencés par ceux-ci. La propagation des annotations suit des règles que nous proposons dans cet article. L'illustration est effectuée sur un corpus de livres dont le thème relève de l'informatique.	Lylia Abrouk, Pierre Pompidor, Danièle Hérin, Michel Sala	http://editions-rnti.fr/render_pdf.php?p1&p=1000305	http://editions-rnti.fr/render_pdf.php?p=1000305
Revue des Nouvelles Technologies de l'Information	EGC	2005	Réponses coopératives dans l'interrogation de documents RDF	Le développement du Web Sémantique a conduit à l'élaboration de standards pour la représentation des connaissances sur le Web. RDF, comme un de ces standards, est devenu une recommandation du W3C. Même s'il a été conçu pour être interprétable par l'homme et la machine (encodage XML, triplets, graphes étiquetés), RDF n'a pas été fourni avec des services d'interrogation et de raisonnement. La plupart des travaux concernant l'interrogation de documents RDF se sont concentrés sur l'usage de techniques issues de la programmation logique et sur des extensions de SQL. Nous portons un nouveau regard sur les techniques d'interrogation et de raisonnement sur les documents RDF et nous montrons que la sémantique des termes OSF (Order Sorted Features) est compatible avec la représentation isomorphique (triplets) des propositions RDF. Cette transformation permet l'ordonnancement des ressources en ontologies et, à travers ceci, des meilleurs mécanismes de réponses (par approximation et recouvrement) aux interrogations de documents RDF.	Adrian Tanasescu, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1000307	http://editions-rnti.fr/render_pdf.php?p=1000307
Revue des Nouvelles Technologies de l'Information	EGC	2005	Restructuration automatique de documents dans les corpus semi-structurés hétérogènes	L'interrogation de grandes bases de documents semi-structurés (type XML) est un problème ouvert important. En effet, pour interroger un document dont le schéma est nouveau, un système doit pouvoir soit adapter la requête posée au document, soit adapter le document pour pouvoir lui appliquer la requête. Nous nous positionnons ici dans le cadre de la restructuration de documents qui consiste à transformer des documents semi-structurés issus de diverses sources dans un schéma de médiation connu. Nous proposons un cadre statistique général à la problématique de la restructuration de documents et détaillons une instance d'un modèle stochastique de documents structurés appliquée à cette problématique. Nous détaillons enfin un ensemble d'expériences effectuées sur les documents du corpus INEX afin de mesurer la capacité de notre modèle.	Guillaume Wisniewski, Ludovic Denoyer, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000260	http://editions-rnti.fr/render_pdf.php?p=1000260
Revue des Nouvelles Technologies de l'Information	EGC	2005	Sélection de modèles par des méthodes à noyaux pour la classification de données séquentielles	Ce travail concerne le développement de méthodes de classification discriminantes pour des données séquentielles. Quelques techniques ont été proposées pour étendre aux séquences les méthodes discriminantes, comme les machines à vecteurs supports, par nature plus adaptées aux données en dimension fixe. Elles permettent de classifier des séquences complètes mais pas de réaliser la segmentation, qui consiste à reconnaître la séquence d'unités, phonèmes ou lettres par exemple, correspondant à un signal. En utilisant une correspondance donnée / modèle nous transformons le problème de l'apprentissage des modèles à partir de données par un problème de sélection de modèles, qui peut être attaqué via des méthodes du type machines à vecteurs supports. Nous proposons et évaluons divers noyaux pour cela et fournissons des résultats expérimentaux pour deux problèmes de classification.	Trinh Minh Tri Do, Thierry Artières, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000236	http://editions-rnti.fr/render_pdf.php?p=1000236
Revue des Nouvelles Technologies de l'Information	EGC	2005	Semi-supervised incremental clustering of categorical data	Le clustering semi-supervisé combine l'apprentissage supervisé et non-supervisé pour produire meilleurs clusterings. Dans la phase initiale supervisée de l'algorithme, un échantillon d'apprentissage est produit par sélection aléatoire. On suppose que les exemples de l'échantillon d'apprentissage sont étiquetés par un attribut de classe. Puis, un algorithme incrémentiel développé pour les données catégoriques est utilisé pour produire un ensemble de clusters pur (tels que les exemple de chaque cluster ont la même étiquette), qui servent de "seeding clusters" pour la deuxième phase non-supervisée de l'algorithme. Dans cette phase, l'algorithme incrémentiel est appliqué aux données non étiquetées. La qualité du clustering est évaluée par l'index de Gini moyen des clusters. Les expériences démontrent que des très bons clusterings peuvent être obtenus avec des petits échantillons d'apprentissage.	Dan A. Simovici, Natima Singla	http://editions-rnti.fr/render_pdf.php?p1&p=1000246	http://editions-rnti.fr/render_pdf.php?p=1000246
Revue des Nouvelles Technologies de l'Information	EGC	2005	SEQTREE, un outil de fouille de données séquentielles par visualisation	Dans cet article, nous présentons un outil de visualisation de séquences modélisées par des arbres de suffixes probabilistes (Prediction suffix trees - PST). Ce type d'arbre permet de représenter une chaîne de Markov d'ordre variable. Dans différentes application, il s'est avéré plus efficace qu'une chaîne de Markov d'ordre fixe avec un coût calculatoire moindre. Pour ces raisons, il nous a paru intéressant d'exploiter le caractère arborescent de ce mode de représentation non seulement d'un point de vue algorithmique, mais aussi d'un point de vue visuel.	Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1000430	http://editions-rnti.fr/render_pdf.php?p=1000430
Revue des Nouvelles Technologies de l'Information	EGC	2005	SSC : Statistical Subspace Clustering	Cet article se place dans le cadre du subspace clustering, dont la problématique est double : identifier simultanément les clusters et le sous-espace spécifique dans lequel chacun est défini, et caractériser chaque cluster par un nombre minimal de dimensions, permettant ainsi une présentation des résultats compréhensible par un expert du domaine d'application. Les méthodes proposées jusqu'à présent pour cette tâche ont le défaut de se restreindre à un cadre numérique. L'objectif de cet article est de proposer un algorithme de subspace clustering capable de traiter des données décrites à la fois par des attributs continus et des attributs catégoriels. Nous présentons une méthode basée sur l'algorithme classique EM mais opérant sur un modelé simplifié des données et suivi d'une technique originale de sélection d'attributs pour ne garder que les dimensions pertinentes de chaque cluster. Les expérimentations présentées ensuite, menées sur des bases de données aussi bien artificielles que réelles, montrent que notre algorithme présente des résultats robustes en termes de qualité de la classification et de compréhensibilité des clusters obtenus.	Laurent Candillier, Isabelle Tellier, Fabien Torre, Olivier Bousquet	http://editions-rnti.fr/render_pdf.php?p1&p=1000241	http://editions-rnti.fr/render_pdf.php?p=1000241
Revue des Nouvelles Technologies de l'Information	EGC	2005	SVM et visualisation pour la fouille de grands ensembles de données	Nous présentons un algorithme de SVM et des méthodes graphiques pour le traitement de grands ensembles de données. Pour pouvoir traiter de tels ensembles de données, nous utilisons une représentation des données de plus haut niveau (sous forme symbolique). L'algorithme de séparateur à vaste marge (SVM) est adapté pour pouvoir traiter ce nouveau type de données. Nous construisons un nouveau noyau RBF (Radial Basis Function) que l'algorithme utilise à la fois pour la classification, la régression et la détection d'individus atypiques dans des données de type intervalle. Nous utilisons ensuite des méthodes de visualisation interactive (elles aussi adaptées au cas des variables de type intervalle) pour expliquer les résultats obtenus par les SVM. La méthode est évaluée sur des ensembles de données symboliques existant ou créés artificiellement.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000372	http://editions-rnti.fr/render_pdf.php?p=1000372
Revue des Nouvelles Technologies de l'Information	EGC	2005	Tableau de Bits Indexé (TBI)  pour la Recherche de Séquences Fréquentes		Lionel Savary, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000279	http://editions-rnti.fr/render_pdf.php?p=1000279
Revue des Nouvelles Technologies de l'Information	EGC	2005	TANAGRA : un logiciel gratuit pour l'enseignement et la recherche	TANAGRA est un logiciel « open source » librement accessible sur le web, il tente de concilier deux types d'utilisation. D'une part, en proposant une interface suffisamment conviviale, il est accessible aux utilisateurs non spécialistes qui veulent effectuer des études sur des données réelles. D'autre part, en définissant une architecture simplifiée à l'extrême, les efforts de développement portent sur l'essentiel, à savoir la mise au point et l'intégration d'algorithmes de fouille de données, les chercheurs peuvent ainsi mener des expérimentations sur les méthodes. Dans cet article, nous présentons les principales fonctionnalités du logiciel en essayant de le positionner sur l'échiquier des (très) nombreux logiciels diffusés actuellement.	Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1000435	http://editions-rnti.fr/render_pdf.php?p=1000435
Revue des Nouvelles Technologies de l'Information	EGC	2005	Tendances dans les expressions de gènes :  application à l'analyse du transcriptome  de Plasmodium Falciparum	L'étude de l'expression des gènes est depuis quelques années révolutionnée par les puces à ADN. Les méthodes habituellement mises en oeuvre pour analyser ces données s'appuient sur des algorithmes de partitionnement, comme les clustering hiérarchiques, et sur une hypothèse communément admise qui associe à un ensemble de profils d'expression similaires, une fonction identique. Cette analyse étudie l'ensemble des gènes sans distinction. L'approche que nous proposons deux catégories de gènes : connus ou putatifs. Pour chaque gène n'ayant pas d'information rattachée, nous étudions son voisinage afin d'y trouver des motifs fréquents (itemsets). Ensuite, l'Analyse est guidée par l'interprétation biologique afin de faire émerger des propriétés intéressantes. Un premier jeu de test sur Plasmodium Falciparum (agent de la Malaria) nous a permis de mettre en évidence, en nous intéressant aux items relatifs à la glycolyse, un transporteur de nucléosides qui intervient au niveau énergétique dans la phase ring (précoce) du parasite.	Philippe Collet, Vincent Derozier, Gérard Dray, François Trousset, Pascal Poncelet, Michel Crampes	http://editions-rnti.fr/render_pdf.php?p1&p=1000411	http://editions-rnti.fr/render_pdf.php?p=1000411
Revue des Nouvelles Technologies de l'Information	EGC	2005	Un automate pour la génération complète ou partielle des concepts du treillis de Galois	Cet article se situe dans le domaine de l'analyse formelle de concepts et du treillis de concepts (treillis de Galois) lequel est un cadre théorique intéressant pour le regroupement conceptuel des données et la génération des règles d'association. Puisque la prospection de données (data mining) est utilisée comme support à la prise de décision par des analystes rarement intéressés par la liste exhaustive (souvent très longue) des concepts et des règles, l'élaboration d'une solution approximative sera dans la plupart des cas un compromis satisfaisant et relativement moins coûteux qu'une solution exhaustive. Dans cet article, on propose une approche appelée CIGA (Closed Itemset Generation using an Automata) de génération partielle ou complète de concepts par la construction et le parcours d'un automate à états finis. La génération des concepts permet l'identification des "itemsets" fermés fréquents, étape cruciale pour l'extraction des règles d'association.	Ganaël Jatteau, Rokia Missaoui, Madenda Sarifuddin	http://editions-rnti.fr/render_pdf.php?p1&p=1000229	http://editions-rnti.fr/render_pdf.php?p=1000229
Revue des Nouvelles Technologies de l'Information	EGC	2005	Un critère d'évaluation pour la sélection de variables	Cet article aborde le problème de la sélection de variables dans le cadre de la classification supervisée. Les méthodes de sélection reposent sur un algorithme de recherche et un critère d'évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables. Nous présentons un nouveau critère d'évaluation fondé sur une mesure d'ambigüité. Cette mesure est fondée sur une combinaison d'étiquettes représentant le degré de spécificité ou d'appartenance aux classes en présence. Les tests menés sur de nombreux jeux de données réels et artificiels montrent que notre méthode est capable de sélectionner les variables pertinentes et d'augmenter dans la plupart des cas les taux de bon classement.	Dahbia Semani, Carl Frélicot, Pierre Courtellemont	http://editions-rnti.fr/render_pdf.php?p1&p=1000218	http://editions-rnti.fr/render_pdf.php?p=1000218
Revue des Nouvelles Technologies de l'Information	EGC	2005	Un système d'aide à la navigation dans des hypermédias	Avec le développement d'Internet et d'applications hypermédias, la construction et l'exploitation de profils ou modèles des utilisateurs deviennent capitaux dans de nombreux domaines. Pouvoir cibler un utilisateur d'un hypermédia ou d'un site web afin de lui proposer ce qu'il attend devient essentiel, par exemple lorsque l'on veut lui présenter les produits qu'il est le plus susceptible d'acheter, ou bien plus généralement à chaque fois que l'on veut éviter de noyer l'utilisateur dans un flot d'informations. Nous présentons un système d'aide à la navigation, intégrant un système de modélisation du comportement de navigation et un stratège qui met en oeuvre, en fonction du comportement détecté, une aide visant à recommander des liens particuliers.	Julien Blanchard, Bertrand Petitjean, Thierry Artières, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000272	http://editions-rnti.fr/render_pdf.php?p=1000272
Revue des Nouvelles Technologies de l'Information	EGC	2005	Une approche filtre pour la sélection de variables en apprentissage non supervisé	La Sélection de Variable (SV) constitue une technique efficace pour réduire la dimension des espaces d'apprentissage et s'avère être une méthode essentielle pour le pré-traitement de données afin de supprimer les variables bruitées et/ou inutiles. Peu de méthodes de SV ont été proposées dans le cadre de l'apprentissage non supervisé, et, la plupart d'entre elles, sont des méthodes dites "enveloppes" nécessitant l'utilisation d'un algorithme d'apprentissage pour évaluer les sous ensembles de variables. Or, l'approche "enveloppe" est largement mal adaptée à une utilisation lors de cas "réels". En effet, d'une part ces méthodes ne sont pas indépendantes vis à vis des algorithmes d'apprentissage non supervisé qui nécessitent le plus souvent de fixer un certain nombre de paramètres ; mais surtout, il n'existe pas de critères bien adaptés à l'évaluation de la qualité d'apprentissage non supervisé dans des sous espaces différents. Nous proposons et évaluons dans ce papier une méthode "filtre" et donc indépendante des algorithmes d'apprentissage non supervisé. Cette méthode s'appuie sur deux indices permettant d'évaluer l'adéquation entre deux ensembles de variables (entre deux sous espaces).	Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000209	http://editions-rnti.fr/render_pdf.php?p=1000209
Revue des Nouvelles Technologies de l'Information	EGC	2005	Une méthode d'évaluation de la pertinence des pages Web dans WebSum	Dans cet article nous présentons une méthode d'évaluation de la pertinence des pages Web retournées par un moteur de recherche.	Olfa Jenhani el Jed	http://editions-rnti.fr/render_pdf.php?p1&p=1000312	http://editions-rnti.fr/render_pdf.php?p=1000312
Revue des Nouvelles Technologies de l'Information	EGC	2005	Usage non classificatoire d'arbres de classification : enseignements d'une analyse de la participation féminine à l'emploi en Suisse	Cet article présente une application en grandeur réelle des arbres de classification dans un contexte non classificatoire. Les arbres générés visent à mettre en lumière les différences régionales dans la façon dont les femmes décident de leur participation au marché du travail. L'accent est donc mis sur la capacité descriptive plutôt que prédictive des arbres. L'application porte sur des données relatives à la participation féminine au marché du travail issues du Recensement Suisse de la Population de l'an 2000. Ce vaste ensemble de données a été analysé en deux phases. Un premier arbre exploratoire a mis en évidence la nécessité de procéder à des études séparées pour les non mères, les mères mariées ou veuves, et les mères célibataires ou divorcées. Nous nous limitons ici aux résultats de ce dernier groupe, pour lequel nous avons généré un arbre séparé pour chacune des trois régions linguistiques principales. Les arbres obtenus font apparaître des différences culturelles fondamentales entre régions. Du point de vue méthodologique, la principale difficulté de cet usage non classificatoire des arbres concerne leur validation, puisque le taux d'erreur de classification généralement retenu perd tout son sens dans ce contexte. Nous commentons cet aspect et illustrons l'usage d'alternatives plus pertinentes et facilement calculables.	Gilbert Ritschard, Pau Origoni, Fabio B. Losa	http://editions-rnti.fr/render_pdf.php?p1&p=1000172	http://editions-rnti.fr/render_pdf.php?p=1000172
Revue des Nouvelles Technologies de l'Information	EGC	2005	Utilisation des technologies XML pour la formalisation de l'ontologie de modèles e-business	Notre travail de recherche consiste à représenter l'ontologie des modèles e-business e-BMO par le langage BM²L spécifié sur la base d'un méta-modèle XML. BM²L est comparé à d'autres langages de définition d'ontologie à savoir, RDF(S), DAML + OIL et OWL et ce, selon un framework établis sur les spécificités de cette ontologie. Aussi, introduisons nous une application Web e-BMH pour la conception et l'exploitation des modèles e-business conformément à l'ontologie.	Rim Djedidi Hannachi, Sarra Ben Lagha, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000311	http://editions-rnti.fr/render_pdf.php?p=1000311
Revue des Nouvelles Technologies de l'Information	EGC	2005	Validation statistique des cartes de Kohonen en apprentissage supervisé	En apprentissage supervisé, la prédiction de la classe est le but ultime. Plus largement, on attend d'une bonne méthodologie d'apprentissage qu'elle permette une représentation des données susceptible de faciliter la navigation de l'utilisateur dans la base d'exemples et d'aider au choix des exemples et des variables pertinents tout en assurant une pré-diction de qualité dont on comprenne les ressorts. Différents travaux ont montré l'aptitude des graphes de voisinage issus des prédicteurs à fonder une telle méthodologie, ainsi le graphe des voisins relatifs de Toussaint. Cependant, la complexité de leur construction, en O(n3), reste élevée. Dans le cas de données volumineuses, nous proposons de substituer aux graphes de voisinage les cartes de Kohonen construites sur les prédicteurs. Après un bref rappel du principe des cartes de Kohonen en apprentissage non supervisé, nous montrons comment celles-ci peuvent fonder une stratégie d'apprentissage optimisée. Nous proposons ensuite d'évaluer la qualité de cette stratégie par une statistique originale qui est étroitement corrélée au taux d'erreur en généralisation. Différentes expérimentations montrent la faisabilité de cette approche. On dispose alors d'un critère fiable pour sélectionner les individus et les attributs pertinents.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000217	http://editions-rnti.fr/render_pdf.php?p=1000217
Revue des Nouvelles Technologies de l'Information	EGC	2005	Visualisation de la perception d'un site web par ses utilisateurs.	Nous proposons dans cet article une méthode de visualisation de l'activité des utilisateurs d'un site web qui permet d'évaluer qualitativement l'adéquation entre son architecture logique et la perception de celle-ci par les internautes. Nous travaillons sur les parcours des internautes sur le site étudié, après reconstruction de ceux-ci grâce aux fichiers logs des serveurs concernés. Nous utilisons la structure logique des sites étudiés pour simplifier la représentation des parcours, en ne tenant pas compte de l'ordre de visite des catégories sémantiques du site. Les parcours simplifiés sont utilisés pour calculer une dissimilarité entre les catégories sémantiques qui sont ensuite représentées dans un plan par Multi Dimensional Scaling. Nous complétons cette visualisation d'ensemble par une représentation de l'arbre couvrant minimal des catégories sémantiques qui permet de mieux appréhender certaines interactions. Nous illustrons l'intérêt de la méthode en l'appliquant au site de l'INRIA.	Fabrice Rossi, Yves Lechevallier, Aïcha El Golli	http://editions-rnti.fr/render_pdf.php?p1&p=1000381	http://editions-rnti.fr/render_pdf.php?p=1000381
Revue des Nouvelles Technologies de l'Information	EGC	2005	« La connaissance de la connaissance » : une réflexion sur la triangulation des analyses textuelles à partir d'un corpus spécialisé en gouvernance d'entreprise	Suite à la survenue récente de scandales financiers, la synthèse des idées mobilisables en gouvernance d'entreprise semble désormais essentielle si l'on veut sécuriser les investisseurs. Dans cette perspective, le présent projet de recherche consiste à mettre en oeuvre un panel d'outils d'analyse de données textuelles (Alceste, Syntex, Tropes-Zoom/Decision Explorer, Wordmapper, Weblex) afin d'évaluer les moyens dont peut disposer un analyste désireux d'extraire des connaissances contenues dans un ensemble d'articles académiques. La qualité de représentation du corpus dans sa globalité est tout d'abord testée. L'étude est ensuite centrée sur le concept même de connaissance, mobilisé dans la théorie de la gouvernance des entreprises. La convergence et la complémentarité des approches méthodologiques sont alors explicitées. Il en est de même pour ce qui concerne la capacité d'extraction d'une connaissance pertinente à partir des textes étudiés.	Stéphane Trébucq	http://editions-rnti.fr/render_pdf.php?p1&p=1000274	http://editions-rnti.fr/render_pdf.php?p=1000274
Revue des Nouvelles Technologies de l'Information	FDC	2005	Adéquation des modèles de représentation aux méthodes de catégorisation	Cet article s'interesse à la problématique de la catégorisation dedocuments et plus particulièrement à l'impact de la méthode de représentationdes documents dans le processus de catégorisation. A partir de différents jeux de documents représentés dans un espace vectoriel tout d'abordbasé sur les concepts puis basé sur une approche de type TF-IDF, nousévaluons les méthodes de catégorisation SVM et Rocchio. Nous comparonsensuite les deux méthodes précédentes avec une méthode de clusteringflou. Nous dressons ensuite le bilan des différentes représentations destextes en terme de qualité des résultats de classification.	Simon Jaillet, Maguelonne Teisseire, Gérard Dray	http://editions-rnti.fr/render_pdf.php?p1&p=1000166	http://editions-rnti.fr/render_pdf.php?p=1000166
Revue des Nouvelles Technologies de l'Information	FDC	2005	Algorithme génétique de pondération d'attributs pour une classification non supervisée d'objets complexes	La classification non supervisée d'objets complexes, composés d'un nombre importantants d'attributs est souvent problématique. En effet il existe très fréquemment des corrélations entre les attributs ainsi que des attributs bruités ou non pertinents. Pour résoudre ce problème nous proposons une méthode de pondération continues et locales. Dans cette méthode un ensemble d'extracteurs (de classe) est défini. Chaque extracteur est muni d'une stratgie qui lui permet d'extraire une unique classe. Chaque extracteur utilise un pondération différente sur les attributs. Le résultat global est obtenu par l'ensemble des classes ainsi extraites. Afin de chercher l'ensemble de poids optimal permettant d'obtenir la meilleure classification possible, nous utilisons un apprentissage par coévolution coopérative.Dans cet article, nous préciserons notre méthode, en particulier comment sont définis les extracteurs, comment est évaluée la qualité de la classification, comment se déroule l'apprentissage et comment est construit le résultat final. Nous présenterons nos premiers résultats, sur des données artificielles et sur des images de télédétection.	Alexandre Blansché, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000161	http://editions-rnti.fr/render_pdf.php?p=1000161
Revue des Nouvelles Technologies de l'Information	FDC	2005	Annotation temporelle des événements dans les dépêches épidémiologiques	Nous nous intéressons dans cet article à l'annotation temporelle des évènements décrits dans des dépêches issues du site de diffusion d'informations épidémiologiques PROMED MAIL. Après avoir présenté l'intérêt d'un marquage et d'un calcul temporel sur de telles données textuelles pour des épidémiologistes, nous présentons l'expérimentation que nous avons réalisée en utilisant des transducteurs et un calcul exploitant les intervalles d'Allen. Pour le marquage des évènements, nous avons été amenés à caractériser le sous langage de l'épidémiologie tel qu'il apparaît dans les dépêches. Des schémas d'extraction on ensuite été implémentés sous formes de transducteurs. Le fait qu'il ne soit pas toujours possible de faire référer ces événements à une date précise nous amène à proposer une représentation de la temporalité de ces évènements basée sur l'algèbre des intervalles temporels de Allen ainsi que sur ses extensions.	Jean Royauté, Yann Guilbaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000192	http://editions-rnti.fr/render_pdf.php?p=1000192
Revue des Nouvelles Technologies de l'Information	FDC	2005	Apport de la prise en compte du contexte structurel dans les modèles bayésiens de classification de documents semistructurés	Nous nous intéressons dans cet article au problème de laclassification supervisée de documents semi-structurés. Un modèle formel basésur des hypothèses simples et originales à notre connaissance est proposé. Cemodèle puise ses fondements dans les modèles de classification bayésiens, enciblant la prise en compte de la structure des documents dans les tâches declassification. Ce modèle permet d'envisager la fusion de données numériquesou symboliques structurées et de données non structurées qui peuvent fairel'objet d'une modélisation spécifique. Des versions simplifiées de ce modèlesont implémentées pour évaluer de manière comparée à d'autres approchesl'impact de la prise en compte de la structure documentaire dans des tâches declassification de documents textuels. Les premiers résultats, qui confirmentceux déjà obtenu dans le cadre de travaux similaires, montrent que la prise encompte du contexte structurel d'occurrence des mots améliore de manièresignificative les performances d'un classifieur bayésien naïf multinomial. Cetteimplémentation conduit à des performances comparables à celles atteintes parles classifieurs SVM sur la tâche considérée. Une implémentation pluscomplète de ce modèle doit permettre d'envisager des expérimentations ou desapplications plus complexes et plus riches. Ces résultats ouvrent desperspectives autour de l'exploitation d'heuristiques de pondération desestimateurs associés aux composantes structurelles des documents.	Pierre-François Marteau, Gildas Ménier, Léopold Ekamby	http://editions-rnti.fr/render_pdf.php?p1&p=1000165	http://editions-rnti.fr/render_pdf.php?p=1000165
Revue des Nouvelles Technologies de l'Information	FDC	2005	Auto-administration des entrepôts de données complexes	Les requêtes définies sur les entrepôts de données sont souvent compliquées et utilisent plusieurs opérations de jointure qui sont coûteuses en terme de temps de calcul. Dans le cadre de l'entreposage de données complexes, les adaptations apportées aux schémas classiques d'entrepôts induisent des jointures supplémentaires lors des accès aux données. Ce coût devient encore plus important quand les requêtes opèrent sur de très grands volumes de données. Il est donc primordial de réduire ce temps de calcul. Pour cela, les administrateurs d'entrepôts de données utilisent en général des techniques d'indexation comme les index de jointure en étoile ou les index bitmap de jointure. Cela demeure néanmoins complexe et fastidieux. La solution proposée s'inscrit dans une optique d'auto-administration des entrepôts de données. Dans ce cadre, nous proposons une stratégie de sélection automatique d'index. Pour cela, nous avons recouru à une technique de fouille de données, plus particulièrement la recherche de motifs fréquents, pour déterminer un ensemble d'index candidats à partir d'une charge donnée. Nous proposons ensuite des modèles de coût permettant de sélectionner les index engendrant le meilleur profit. Ces modèles de coût évaluent en particulier le temps d'accès aux données à travers des index bitmap de jointure, ainsi que le coût de maintenance et de stockage de ces index.	Kamel Aouiche, Jérôme Darmont, Omar Boussaid, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000160	http://editions-rnti.fr/render_pdf.php?p=1000160
Revue des Nouvelles Technologies de l'Information	FDC	2005	Deux méthodologies de classification de règles d'association pour la fouille de textes	Parmi les inconvénients d'un processus de fouille de données textuelles fondé sur l'extraction de règles d'association figurent le grand nombre de règles extraites et la difficulté d'affecter à une règle un critère de qualité fiable par rapport aux connaissances de l'analyste. La plupart des approches pour la classification des règles d'associations utilisent des méthodes statistiques pour juger de la qualité d'une règle et ne s'appuient pas sur les connaissances du domaine des données disponibles à priori pour classer les règles extraites. Dans cet article nous définissons la notion de qualité d'une règle d'association. Nous étudions en premier lieu les mesures statistiques permettant de classer les règles et nous proposons un algorithme combinant ces différentes mesures. Nous introduisons ensuite une nouvelle méthodologie de classification des règles exploitant un modèle de connaissances. Nous expérimentons cette mesure sur un exemple formel puis nous l'évaluons sur des données réelles.	Hacène Cherfi, Amedeo Napoli, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1000168	http://editions-rnti.fr/render_pdf.php?p=1000168
Revue des Nouvelles Technologies de l'Information	FDC	2005	Extraction automatique d'information inattendue à partir de textes.	Dans cet article, nous proposons d'utiliser des techniques de fouille de textes pour extraire des informations, automatiquement et à des fins stratégiques, à partir de bases de données scientifiques et techniques. Ce contexte de veille technologique introduit une difficulté inhabituelle par rapport aux domaines d'application classiques de la fouille de textes, puisqu'au lieu de rechercher de la connaissance fréquente cachée dans les données, il faut rechercher de la connaissance inattendue qualifiée par les veilleurs de signal. Les mesures usuelles d'extraction de la connaissance à partir de textes doivent de ce fait être revues.Pour ce faire, nous avons développée le système UnexpectedMiner dans lequel de nouvelles mesures permettent d'estimer le caractère inattendu d'un document. Notre système est évalué sur une base de résumés d'articles scientifiques.	François Jacquenet, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1000170	http://editions-rnti.fr/render_pdf.php?p=1000170
Revue des Nouvelles Technologies de l'Information	FDC	2005	Extraction de connaissances provenant de données multi-sources pour la caractérisation d'arythmies cardiaques	Nous nous intéressons au problème de l'apprentissage par programmation logique inductive de règles symboliques caractérisant des arythmies cardiaques a partir de données multi-sources hétérogènes telles que les différentes voies d'un électrocardiogramme ou la mesure de la pression artérielle. Une première approche consiste a agréger les données dans la base d'apprentissage puis a effectuer un apprentissage directement a partir de ces données transformées. Cette méthode d'apprentissage global est peu performante et difficile a mettre en oeuvre quand le volume des données est important. Nous proposons une alternative plus efficace permettant de tirer profit d'apprentissages effectués préalablement sur chacune des sources indépendamment, pour construire automatiquement un biais permettant de restreindre l'espace de recherche lors d'un apprentissage multi-source et ainsi traiter ces données complexes. Les résultats obtenus par les deux méthodes sont analysés et comparés. Ils montrent que la méthode proposée permet de gagner un ordre de grandeur sur les temps d'apprentissage.	Elisa Fromont, Rene Quiniou, Marie-Odile Cordier	http://editions-rnti.fr/render_pdf.php?p1&p=1000140	http://editions-rnti.fr/render_pdf.php?p=1000140
Revue des Nouvelles Technologies de l'Information	FDC	2005	Fouille interactive de séquences d'images 3D d'IRMf	Du point de vue de la fouille de données, le cerveau est un objet complexe par excellence. La discrimination des voxels d'image du cerveau qui présentent une réelle activité est en général très difficile à cause d'un faible rapport signal sur bruit et de la présence d'artefacts. Les premiers tests des algorithmes actuels de fouille dans ce domaine ont montré que leurs performances et leurs qualités de reconnaissance sont faibles. Dans cet article, nous présentons une nouvelle approche interactive de fouille des images IRMf, guidée par les données, permettant l'observation du fonctionnement cérébral. Plusieurs algorithmes de classification non supervisés sont testés sur des séquences d'images 3D d'IRMf. Les résultats des tests présentent notamment les performances des classifieurs en fonction du nombre de classes, du rapport signal sur bruit des données et de la taille des zones activées par rapport au volume exploré.	Nicolas Lachiche, Jerzy Korczak, Christian Scheiber, Jean Hommet	http://editions-rnti.fr/render_pdf.php?p1&p=1000162	http://editions-rnti.fr/render_pdf.php?p=1000162
Revue des Nouvelles Technologies de l'Information	FDC	2005	Indexation et recherche par le contenu dans une base d'images fixes : l'intérêt des règles d'association	Les images fixes peuvent, entre autre, être décrites au niveau pixel pardes descripteurs visuels globaux de couleur, de texture ou de forme. Larecherche par le contenu exploite et combine alors ces descripteurs dont le coûtde calcul est d'autant plus important que la taille de la base d'images est grande.Or, un sous-ensemble de descripteurs pourrait suffire à répondre à unerecherche par similarité beaucoup plus rapidement, tout en gardant une qualitéacceptable des résultats de recherche. Pour cela, nous proposons une méthodede sélection automatique des descripteurs visuels qui exploite les règlesd'association pour élaborer des plans d'exécution réduisant le temps de larecherche par le contenu dans de grandes bases d'images. Dans cet article, nousprésentons également comment une recherche par le contenu peut être adaptéepour proposer des résultats intermédiaires qui sont fusionnés de façonprogressive avec l'avantage pour l'utilisateur, d'une part, de ne pas attendre quetoute la base ait été parcourue avant de fournir un résultat et, d'autre part, de luipermettre de stopper la requête en cours d'exécution. Nous évaluons notreméthode comparativement au temps et au résultat d'une recherche séquentiellesur tous les descripteurs de la base.	Anicet Kouomou Choupo, Laure Berti-Equille, Annie Morin	http://editions-rnti.fr/render_pdf.php?p1&p=1000164	http://editions-rnti.fr/render_pdf.php?p=1000164
Revue des Nouvelles Technologies de l'Information	FDC	2005	SAFE-NEXT : Une approche systématique pour l'intégration des connaissances du domaine dans les fouille de données complexes	L'extraction de connaissances de données (ECD) est un processus itératif dont la complexité dépend de la nature des données traitées, de la nature des connaissances à extraire ainsi que du domaine de l'application. Lorsque cette complexité est élevée, une forte implication de l'utilisateur est requise tout au long du processus et surtout dans la première et dernière phase (i.e préparation de données et interprétation de résultats). En combinant des approches ECD et d'ingénierie de connaissances (IC), nous développons une méthodologie descendante qui permet l'identification multivues des connaissances du domaines, leur formalisation sous formes de métadonnées ainsi que leur incorporation dans le processus d'ECD. Notre approche est appliquée dans le domaine d'accidentologie pour l'extraction à partir des bases de données d'accidents de la route des connaissances exploitables pour le développement des systèmes de sécurité embarqués dans les véhicules.	Walid Ben Ahmed, Mounib Mekhilef, Michel Bigand, Yves Page	http://editions-rnti.fr/render_pdf.php?p1&p=1000139	http://editions-rnti.fr/render_pdf.php?p=1000139
Revue des Nouvelles Technologies de l'Information	FDC	2005	Une analyse récursive constructive pour la recherche du sens du texte de spécialité	Cet article décrit une chaîne de traitement de textes en vue de ladécouverte des traces linguistiques au sein des textes dans le but de simulerune compréhension « humaine ». L'accent est mis sur l'aspect récursif destâches composant cette chaîne. Par voie de conséquence, les variables surlesquelles s'effectuent les appels récursifs doivent présenter la propriété d'êtreconstructibles et d'assurer la terminaison des appels récursifs. Le respect deces deux propriétés implique des choix dans la linguistique utilisée pourdécrire les textes, et la nécessité d'une intervention experte dans laprogrammation du système, afin d'introduire les connaissances permettant decasser les boucles infinies de calcul. Notre présentation ne décrit qu'une petitepartie des problèmes linguistiques, mais illustre l'ensemble des problèmesplus généraux d'une analyse en compréhension. L'utilisation simultanée deplusieurs média montre encore plus clairement la nécessité d'utiliser desmodules interactifs qui s'appellent récursivement les uns les autres.	Marta Franová, Yves Kodratoff, Lise Fontaine	http://editions-rnti.fr/render_pdf.php?p1&p=1000171	http://editions-rnti.fr/render_pdf.php?p=1000171
Revue des Nouvelles Technologies de l'Information	FDC	2005	Visualisation et classification avec les cartes topologiques catégorielles	Ce papier introduit les cartes topologiques dédiée à la visualisation et à la classification de données composées de variables catégorielles. Pour visualiser ou classer ces données par des cartes topologiques, les méthodes classiques utilisent une phase de codage de prétraitement de ces données en données numériques et appliquent l'algorithme classique des cartes topologiques. Dans ce papier nous proposons un modèle de cartes topologiques dédiées aux données catégorielles. Ce modèle est basé sur un formalisme probabiliste où chaque cellule est représentée par une table de probabilités. Deux exemples réels permettent de valider ce modèle. Les résultats obtenus montrent l'apport de ce modèle dans la visualisation et la classification de données catégorielles.	Mustapha Lebbah, Fouad Badran, Sylvie Thiria	http://editions-rnti.fr/render_pdf.php?p1&p=1000163	http://editions-rnti.fr/render_pdf.php?p=1000163
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2004	Implémentation en C d'estimateurs non paramétriques de quantiles conditionnels. Application au tracé de courbes de référence	L'article présente trois méthodes d'estimation non paramétrique des quantiles conditionnels :une méthode d'estimation par noyau, la méthode de la constante locale et une méthode d'estimation par noyau produit. Nous décrivons ensuite l'implémentation informatique en C de ces méthodes. Une interface avec les logiciels SAS, Splus et Gnuplot est donnée afin d'appliquer les résultats au tracé de courbes de référence. Enfin, nous terminons en donnant une illustration sur des données réelles concernant des propriétés biophysiques de la peau de femmes japonaises.	Ali Gannoun, Stéphane Girard, Christiane Guinot, Jérôme Saracco	http://editions-rnti.fr/render_pdf.php?p1&p=1001679	http://editions-rnti.fr/render_pdf.php?p=1001679
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2004	Introduction à la méthode des scores : les difficultés financières des exploitations agricoles	Cette note propose une introduction aux techniques de " credit scoring " à partir d'une étude de cas sur les difficultés financières des exploitations agricoles servant de support pédagogique à des formations initiales et continuées en analyse des données. On présente tout d'abord la problématique de l'évaluation du risque financier, les contraintes qu'impose la collecte de données comptables dans un tel contexte, et la batterie des critères micro-économiques retenus pour mesurer le degré d'insolvabilité des exploitations agricoles. L'information fournie par cette batterie de ratios financiers est ensuite analysée aux moyens de techniques statistiques multidimensionnelles telle que l'analyse en composantes principales, les nuées dynamiques et l'analyse discriminante. Les résultats fournis par ces techniques d'analyse factorielle, de classification et de classement permettent de montrer l'intérêt méthodologique de ces outils pour ce type d'étude micro-économique.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001681	http://editions-rnti.fr/render_pdf.php?p=1001681
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2004	Quelques méthodes d'analyse factorielle d'une série de tableaux de données	On étudie ici les méthodes d'analyse factorielle d'une série de tableaux de données, chaque tableau étant associé à un ensemble de variables quantitatives mesurées sur un ensemble d'individus. On examine d'abord le cas où les tableaux sont définis sur le même ensemble d'individus, puis le cas dual où c'est l'ensemble des variables qui est le même, et enfin le cas où tous les tableaux sont de même format. On adapte ensuite les méthodes définies dans le cas quantitatif,au cas de l'analyse des correspondances, avant de considérer un mélange de deux types tableaux, les tableaux du premier type étant associés à des variables quantitatives,et donc analysables par l'ACP, tandis que les tableaux du second type relèvent de l'analyse des correspondances.	Pierre Cazes	http://editions-rnti.fr/render_pdf.php?p1&p=1001677	http://editions-rnti.fr/render_pdf.php?p=1001677
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2004	Robustesse de l'estimation spatiale par krigeage simple et par régression PLS	L'étude de la robustesse du krigeage, a montré que, dans le cas où la matrice de covariance est bien conditionnée,le krigeage est stable, dans le cas contraire il peut être instable par rapport aux perturbations de la fonction de covariance.Nous rappelons l'application de la régression PLS a un champ spatial stationnaire de moyenne connue, puis nous comparons la robustesse des coefficients du krigeage a celle de la prédiction spatiale par régression PLS, ainsi que la robustesse de la précision des deux prédicteurs quand des perturbations sont produites sur le paramètre de portée de la fonction de covariance.	D. Mentagui, Y. Elkettani	http://editions-rnti.fr/render_pdf.php?p1&p=1001678	http://editions-rnti.fr/render_pdf.php?p=1001678
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2004	Scalogrammes météorologiques sur la base de variables explicatives quantitatives	A partir d'une étude analytique d'information météorologique prise par la station météorologique IPN, située au Nord de Mexico, entre le 6 juin et le 30 septembre, 2002, une analyse factorielle des données est réalisée et un dendrogramme factoriel divisé en 7 branches est construit,calculé à travers la distance factorielle Khi-carré.Un théorème est donc présenté et démontré pour les échelles factorielles en pourcentage et il est appliqué à l'information issue d'une analyse où l'on compare deux autres scalogrammes.	Francisco Casanova del Angel	http://editions-rnti.fr/render_pdf.php?p1&p=1001680	http://editions-rnti.fr/render_pdf.php?p=1001680
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2004	Visual data mining : the case of Vitamin system and other software.	Le Data mining visuel est apparu récemment avec le flot sans cesse croissant des données. L'objectif est de tirer avantage de la combinaison des techniques usuelles du Data mining avec les méthodes de visualisation de l'information. Avec l'exploration visuelle, on profite de l'intervention directe de l'utilisateur et de la perception humaine qui s'ajoute à l'analyse automatique. Le système VITAMIN (Visual daTA MINing System) veut aider l'utilisateur confronté aux gros volumes de données numériques et temporelles. Le logiciel a été développé par un consortium d'instituts avec l'aide de la Commission Européenne dans les cadre des projets IST. Dans une première partie, on décrit brièvement les principales fonctionnalités du système en faisant ressortir ses aspects originaux. On évoque ensuite quelques autres réalisations de logiciels de Data mining visuel, différant soit par les caractéristiques des données traitées, soit par les techniques mises en oeuvre.	Alain Morineau	http://editions-rnti.fr/render_pdf.php?p1&p=1001682	http://editions-rnti.fr/render_pdf.php?p=1001682
Revue des Nouvelles Technologies de l'Information	EGC	2004	A Galois connecion semantics-based approach for deriving generic bases of association rules	L'augmentation vertigineuse de la taille des données (textuelles ou transactionnelles) est un défi constant pour la "scalabilité" des techniques d'extraction des connaissances. Dans ce papier, on présente une approche pour la dérivation des bases génériques de règles associatives. Les principales caract éristiques de cette approches sont les suivantes. D'une part, l'introduction d'une structure de données appelée "Trie-itemset" pour le stockage de la relation en entrée. D'autre part, on utilise une méthode "Diviser pour régner" pour réduire le coût de construction de structures partiellement ordonnées, à partir desquelles les bases génériques de règles sont directement extraites.	Sadok Ben Yahia, Narjes Doggaz, Yahya Slimani, Jihem Rezgui	http://editions-rnti.fr/render_pdf.php?p1&p=1001033	http://editions-rnti.fr/render_pdf.php?p=1001033
Revue des Nouvelles Technologies de l'Information	EGC	2004	A metric approach to supervised discretization	Nous présentons une nouvelle approche à la discrétisation supervisée des attributs continues qui se sert de l'espace métrique des partitions d'un ensemble fini. Nous discutons deux nouvelles idées fondamentales : une généralisation des techniques de discrétisation de Fayyad-Irani basée sur une distance sur des partitions, dérivée de l'entropie généralisée de Daroczy, et un nouveau critère géométrique pour arrêter l'algorithme de discrétisation. Les arbres de décision résultants sont plus petits, ont moins de feuilles, et montrent des niveaux plus élevés d'exactitude établis par la validation croisée stratifiée.	Dan A. Simovici, Richard Butterworth	http://editions-rnti.fr/render_pdf.php?p1&p=1000966	http://editions-rnti.fr/render_pdf.php?p=1000966
Revue des Nouvelles Technologies de l'Information	EGC	2004	A robust method for partitioning the values of categorical attributes	Dans le domaine de l'apprentissage supervisé, les méthodes de groupage des modalités d'un attribut symbolique permettent de construire un nouvel attribut synthétique conservant au maximum la valeur informationnelle de l'attribut initial et diminuant le nombre de modalités. Nous proposons ici une généralisation de l'algorithme de discrétisation Khiops pour le problème du groupage des modalités. L'algorithme proposé permet de contrôler a priori le risque de sur-apprentissage et d'améliorer significativement la robustesse des groupages produits. Cette caractéristique de robustesse a été obtenue en étudiant la statistique des variations du critère du Khi2 lors de regroupements de lignes d'un tableau de contingence et en modélisant le comportement statistique de l'algorithme Khiops. Des expérimentations intensives ont permis de valider cette approche et ont montré que la méthode de groupage Khiops aboutit à des groupages performants, à la fois en terme de qualité prédictive et de faible nombre de groupes.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000950	http://editions-rnti.fr/render_pdf.php?p=1000950
Revue des Nouvelles Technologies de l'Information	EGC	2004	Accélération de EM pour données qualitatives : études comparative de différentes versions	L'algorithme EM est très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange. L'inconvénient majeur de cet algorithme est la lenteur de sa convergence. Son application sur des tableaux de grande taille pourrait ainsi prendre énormément de temps. Afin de remédier à ce problème, nous étudions ici le comportement de plusieurs variantes connus de EM, ainsi qu'une nouvelle méthode. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification. Nous réalisons une étude comparative entre les différentes variantes sur des données simulées et réelles et proposons une stratégie d'utilisation de notre méthode qui s'avère très efficace.	Mohamed Nadif, François-Xavier Jollois	http://editions-rnti.fr/render_pdf.php?p1&p=1001019	http://editions-rnti.fr/render_pdf.php?p=1001019
Revue des Nouvelles Technologies de l'Information	EGC	2004	Acquisition de données vs gestion de connaissances  patrimoniales : le cas des vestiges du théâtre antique d'Arles	Qu'y a t'il de commun aujourd'hui entre l'acquisition de données 3D, la gestion d'informations patrimoniales, ou encore la modélisation tridimensionnelle en temps réel ? Bien peu, force est de le constater, si ce n'est que l'édifice patrimonial sert là souvent de terrain d'expérimentation. Pourtant, il ne saurait être réduit à ce seul statut : il est objet de connaissances dont l'étude doit bénéficier de différents jeux de technologies. Notre proposition, expérimentée sur des vestiges du théâtre antique d'Arles, place cet édifice au centre d'un dispositif visant à intégrer, au sein d'un système d'informations architecturales 3D en devenir, les résultats de différentes phases de son étude. Un jeu de connaissances formalisé sur l'édifice sert de dénominateur commun depuis l'acquisition de données 3D jusqu'à la représentation dans une maquette temps réel pour la toile. Cette maquette devient outil de navigation dans le jeu d'informations et de savoirs qui caractérise l'édifice.	Jean-Yves Blaise, Francesca De Domenico, Livio De Luca, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1001149	http://editions-rnti.fr/render_pdf.php?p=1001149
Revue des Nouvelles Technologies de l'Information	EGC	2004	Analyse d'information relationnelle par des graphes interactifs de grandes tailles	La découverte de connaissances à partir d'importantes masses de données hétérogènes débouche le plus souvent sur l'analyse relationnelle. La recherche d'informations stratégiques s'appuie en effet sur les liens fonctionnels et sémantiques entre documents, acteurs, terminologie et concepts d'un domaine sans oublier le paramètre temps. De nombreuses méthodes sont proposées pour identifier, analyser et visualiser les mécanismes mis à jour : analyse relationnelle, classifications supervisées et non supervisées, analyse factorielle, analyse sémantique, cartes, dendogrammes, ... Mais ces approches demandent souvent une expertise non négligeable pour être comprises et ne s'adressent donc pas aux non initiés. Par contre, la vue d'un graphe mettant en relation une ou deux classes d'éléments interdépendants est directement assimilable par tout le monde. Nous proposons donc un ensemble de visualisations interactives de graphes dont la manipulation doit permettre une découverte de connaissances intuitive et basée sur un langage graphique naturel. Nous illustrons notre propos de nombreux exemples tirés de cas réels d'analyses stratégiques qui ont permis d'évaluer cette approche sur un panel très large de données.	Saïd Karouach, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001086	http://editions-rnti.fr/render_pdf.php?p=1001086
Revue des Nouvelles Technologies de l'Information	EGC	2004	Annotation automatique de documents XML	Nous proposons dans cet article un mécanisme automatique d'annotation de documents. Ce mécanisme s'appuie sur une opération de composition permettant de créer de nouveaux documents à partir de documents existants et sur un algorithme permettant d'inférer l'annotation d'un document composé à partir d'annotation de ses parties. Notre modèle est illustré par une étude de cas consacrée à la mise en commun de documents pédagogiques au format XML, dans un environnement coopératif d'enseignement à distance. Nous décrivons un prototype permettant d'annoter ces documents, et d'engendrer une description RDF contenant les annotations.	Birahim Gueye, Philippe Rigaux, Nicolas Spyratos	http://editions-rnti.fr/render_pdf.php?p1&p=1001138	http://editions-rnti.fr/render_pdf.php?p=1001138
Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage des réseaux bayésiens avec des graphes chaînés maximaux		Paul Munteanu, Mohamed Bendou	http://editions-rnti.fr/render_pdf.php?p1&p=1001093	http://editions-rnti.fr/render_pdf.php?p=1001093
Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage et optimisation conjoints : extraction de connaissances pertinentes sur les systèmes de production		Anne-Lise Huyet, Jean-Luc Paris	http://editions-rnti.fr/render_pdf.php?p1&p=1001169	http://editions-rnti.fr/render_pdf.php?p=1001169
Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage incrémental des profils dans un système de filtrage d'information	Cet article présente une méthode d'apprentissage des profils dans les systèmes de filtrage d'information. Le processus d'apprentissage est effectué d'une manière incrémentale au fur et à mesure que les informations sont filtrées et jugées par l'utilisateur. Des expérimentations effectuées sur une collection de test de référence TREC, montrent que la méthode permet effectivement l'amélioration des profils.	Mohand Boughanem, Hamid Tebri, Mohamed Tmar	http://editions-rnti.fr/render_pdf.php?p1&p=1001123	http://editions-rnti.fr/render_pdf.php?p=1001123
Revue des Nouvelles Technologies de l'Information	EGC	2004	Approche binaire pour la génération de fortes règles d'association	Dans ce papier, nous proposons une nouvelle méthode d'extraction des règles d'association dans des bases de données relationnelles basée sur la technologie des arbres de Peano (Ptree). La structure de données utilisée pour représenter la base de données est un ensemble de Ptrees de base représentant chacun un vecteur binaire et tous ces Ptrees sont stockés dans des fichiers binaires. Nous montrons que la structure Ptree combinée avec la technique de réduction appelée élagage par support minimum produisent des règles d'association fortes et réduisent considérablement le temps de construction de l'association. En effet, notre approche présente l'avantage de ne pas effectuer des parcours coûteux de la base de données. Cette approche a été testée à travers un prototype que nous avons implémenté. Les résultats expérimentaux montrent que les règles d'association fortes sont générées dans un temps minimum comparativement à d'autres travaux.	Thabet Slimani, Boutheina Ben Yaghlane, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1001052	http://editions-rnti.fr/render_pdf.php?p=1001052
Revue des Nouvelles Technologies de l'Information	EGC	2004	Approche innovante pour la recherche et l'extractin coopérative et dynamique d'informations sur Internet	Il existe de nombreuses techniques qui permettent de classifier les documents textuels en fonction de l'intérêt d'un utilisateur (kNN, SVM, ...). Malheureusement, l'intégration de ces méthodes dans les plates-formes de textmining est souvent très statique au cours du temps. Le but de cet article est de présenter une plate-forme de webmining dans laquelle les données hétérogènes sont représentées uniformément selon un formalisme XML/TEI et où l'utilisateur peut interagir sur les processus de récupération et d'analyse de ces données. Pour cela, les modules de traitements sont représentés par des agents fonctionnant sur la plate-forme MadKit et l'apprentissage se fait par une méthode dérivée de VSM et TFIDF utilisant un principe de listes noires pondérées permettant la reconnaissance de documents indésirables. La dynamique de la plate-forme repose principalement sur la possibilité d'ajouter à la volée des agents de traitement et de pouvoir modifier l'ordre et les paramètres d'analyse des documents.	Xavier Denis, Gaële Simon, Nicolas Chanchevrier	http://editions-rnti.fr/render_pdf.php?p1&p=1001129	http://editions-rnti.fr/render_pdf.php?p=1001129
Revue des Nouvelles Technologies de l'Information	EGC	2004	BELUGA : un outil pour l'analyse dynamique des connaissances de la littérature scientifique d'un domaine. Première application au cas des maladies à prions	Un projet ciblé sur l'étude du domaine des maladies à prions à permis de formaliser une méthodologie commune, sociologique et informatique, de compréhension de sa dynamique par l'analyse thématique. Nous avons créé une plate forme d'indexation de notices bibliographiques dont le but est d'extraire des associations évoluant à travers des intervalles de temps. Beluga propose une chaîne de traitement basée sur l'indexation des documents en unités de base : références, auteurs, termes simples et composés, organismes. L'outil est fondé sur une double approche d'apprentissage et de visualisation qui automatise les processus d'extraction de groupes d'auteurs et de termes, et permet à l'utilisateur de revenir aux données documentaires sources. L'analyse diachronique de corpus de documents électroniques nous permet d'analyser comment la terminologie est structurée en thématiques émergentes.	Nicolas Turenne, Marc Barbier	http://editions-rnti.fr/render_pdf.php?p1&p=1001096	http://editions-rnti.fr/render_pdf.php?p=1001096
Revue des Nouvelles Technologies de l'Information	EGC	2004	BooLoader : un chargeur efficace dédié aux bases de transactions denses	Nous nous intéressons à la représentation et au chargement de bases de transactions en mémoire. Pour cela, nous proposons d'utiliser un format condensé fondé sur les diagrammes de décision binaires et nous présentons un algorithme que nous avons implanté en un système baptisé BooLoader, pour charger des bases de transactions. Nous donnons également des résultats expérimentaux de notre système sur des bases éparses et denses.	Zahir Maazouzi, Ansaf Salleb-Aouissi, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000898	http://editions-rnti.fr/render_pdf.php?p=1000898
Revue des Nouvelles Technologies de l'Information	EGC	2004	Caractérisation de signatures complexes dans des familles de protéines distantes	L'identification de signatures de protéines est un problème majeur pour la découverte de nouveaux membres dans des familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles distantes sont trop hétérogènes pour qu'on puisse identifier les régions conservées à partir des algorithmes classiques de la bioinformatique. Nous proposons une approche génétique pour la découverte de motifs hiérarchiques; l'algorithme suit une démarche descendante en s'appuyant dans une première phase sur les classes physico-chimiques des acides aminés. Les signatures sont ensuite définies par des séquences des motifs ainsi obtenus. Elles sont extraites au moyen d'un algorithme de découverte d'itemsets séquentiels où les motifs jouent le rôle d'items. Une dernière étape consiste à fouiller dans cette base d'itemsets pour n'en retenir qu'un ensemble réduit de signatures. Plusieurs stratégies sont proposés pour déterminer un ensemble optimal de signatures qui respecte des contraintes de complétude, de cardinalité et de spécificité. Nous appliquons notre démarche sur la famille des cytokines. L'analyse de la base de protéines SCOP a montré que les groupes de signatures que nous avons extrait cible spécifiquement cette famille d'intérêt.	Jérôme Mikolajczak, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1001049	http://editions-rnti.fr/render_pdf.php?p=1001049
Revue des Nouvelles Technologies de l'Information	EGC	2004	Caractérisation globale de l'exécution de jobs	La caractérisation globale de l'exécution de jobs passe par l'exploitation de mesures recueillies sur les machines en production. Afin de répondre à la problématique, il est nécessaire de tenir compte des différents types de données, ainsi que de la dualité de la caractérisation : statique et dynamique. Une solution technique répondant aux contraintes est proposée. Elle repose sur l'utilisation de SVM afin de détecter des phases, et à un niveau supérieur, à un réseau bayésien afin d'automatiser l'analyse de modèles de Markov enrichis. Ceux-ci sont introduits comme la base formelle et synthétique de description du comportement du job, aussi bien sur un système batch, que parallèle. Enfin, les résultats obtenus à l'aide d'un prototype sont discutés.	Fabrice Gadaud, Guillaume Duquesnay	http://editions-rnti.fr/render_pdf.php?p1&p=1001161	http://editions-rnti.fr/render_pdf.php?p=1001161
Revue des Nouvelles Technologies de l'Information	EGC	2004	Cartographie sémantique des connaissances à la carte		Christophe Tricot, Cristophe Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000916	http://editions-rnti.fr/render_pdf.php?p=1000916
Revue des Nouvelles Technologies de l'Information	EGC	2004	Classer pour découvrir : une nouvelle méthode d'analyse du comportement de tous les utilisateurs d'un site Web	L'analyse du comportement des utilisateurs d'un site Web est un domaine riche et complexe. Le grand nombre de méthodes d'extraction de connaissances appliquées aux logs Web, ainsi que la diversité du type de ces méthodes en est une preuve. Cependant, compte tenu de cette complexité, nous posons dans cet article la question suivante : Est-il possible de combiner des méthodes existantes pour proposer une analyse qui tire profit des résultats de plusieurs spécialités et extraire par exemple des comportements fréquents minoritaires ?Notre étude à donc porté sur une nouvelle approche hybride (issue de la classification neuronale et de la recherche de motifs séquentiels) visant à classer les navigations des utilisateurs d'un site (à l'aide de leurs résumés sémantiques) puis, pour chaque classe de navigations, d'en extraire les comportements fréquents. Notre objectif est 1) de pallier les limites de l'extraction de motifs fréquents par rapport à la quantité de données à traiter et aussi par rapport à la qualité des résultats et 2) de pallier les limites d'une première méthode d'analyse du comportement appelée "Diviser pour Découvrir", que nous avons proposé en 2003. Nous avons mené des expérimentations sur les logs HTTP des sites INRIA. Les résultats obtenus confirment le bien fondé de notre approche vis à vis de l'état de l'art.	Doru Tanasa, Brigitte Trousse, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1001146	http://editions-rnti.fr/render_pdf.php?p=1001146
Revue des Nouvelles Technologies de l'Information	EGC	2004	Classification automatique d'images		Mohamed Hammami, Boulbaba Ben Amor, Liming Chen	http://editions-rnti.fr/render_pdf.php?p1&p=1001027	http://editions-rnti.fr/render_pdf.php?p=1001027
Revue des Nouvelles Technologies de l'Information	EGC	2004	Construction de variables et arbre de décision		Gaëlle Legrand, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000996	http://editions-rnti.fr/render_pdf.php?p=1000996
Revue des Nouvelles Technologies de l'Information	EGC	2004	Contrôle du risque multiple pour la sélection de règles d'association significatives	Les algorithmes d'extraction de règles d'association parcourent efficacement le treillis des itemsets pour constituer une base de règles admissibles à des seuils de support et de confiance, mais donnent une multitude de règles peu exploitables. Nous suggérons d'épurer de telles bases en éliminant les règles non statistiquement significatives. La multitude de tests pratiqués conduit mécaniquement à multiplier les règles sélectionnées à tort. après avoir présenté des procédures issues de la biostatistique qui contrôlent non pas le risque, mais le nombre de fausses découvertes, nous proposons BS_DF, un algorithme original fondé sur le bootstrat qui sélectionne les règles significatives en contrôlant le nombre de fausses découvertes. Des expérimentations montrer l'efficacité de ces procédures.	Elie Prudhomme, Stéphane Lallich, Olivier Teytaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001046	http://editions-rnti.fr/render_pdf.php?p=1001046
Revue des Nouvelles Technologies de l'Information	EGC	2004	Découverte de régularités pour l'intégration de  données semi structurées	Cet article présente l'utilisation d'une technique de fouille de données pour aider à la spécification de vues sur des sources XML. Notre langage de vues permet d'intégrer des données XML provenant de sources hétérogènes. Cependant, la définition de motifs sur les sources permettant de spécifier les données à extraire est souvent difficile, car la structure des données n'est pas toujours connue. Nous proposons d'extraire les structures fréquentes dans les données des sources pour spécifier des motifs pertinents à utiliser dans la spécification des vues.	Pierre-Alain Laur, Xavier Baril	http://editions-rnti.fr/render_pdf.php?p1&p=1001139	http://editions-rnti.fr/render_pdf.php?p=1001139
Revue des Nouvelles Technologies de l'Information	EGC	2004	Détection par SVM - Application à la détection de Churn en téléphonie mobile prépayée		Cédric Archaux, Arnaud Martin, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001165	http://editions-rnti.fr/render_pdf.php?p=1001165
Revue des Nouvelles Technologies de l'Information	EGC	2004	ETIQ, un étiqueteur inductif convivail pour les corpus de spécialité		Ahmed Amrani, Oriane Matte-Tailliez, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001120	http://editions-rnti.fr/render_pdf.php?p=1001120
Revue des Nouvelles Technologies de l'Information	EGC	2004	Étude de textes par leur image	Nous proposons une méthode automatique de comparaison de textes reposant sur une technique de transformation d'un texte en une image de taille donnée et l'analyse à l'aide des outils de la géométrie fractale. Nous présentons une application à l'étude d'un corpus de 90 textes longs.	Hubert Marteau, Alexandre Lefevre, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001095	http://editions-rnti.fr/render_pdf.php?p=1001095
Revue des Nouvelles Technologies de l'Information	EGC	2004	Étude expérimentale de mesures de qualité de règles d'association	La validation des connaissances extraites d'un processus d'ECD par un expert métier nécessite de filtrer ces connaissances. Pour ce faire, de nombreuses mesures ont été proposées, chacune répondant à des besoins spécifiques. Ces mesures présentent des caractéristiques variées et parfois contradictoires qu'il convient alors d'examiner. Arguant du fait que la sélection des bonnes connaissances passe aussi par l'utilisation d'un ensemble de mesures adaptées au contexte, nous présentons dans cet article une étude expérimentale de différentes mesures. Cette étude est mise en regard d'une étude formelle synthétisant les qualités des mesures.	Benoît Vaillant, Philippe Lenca, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001056	http://editions-rnti.fr/render_pdf.php?p=1001056
Revue des Nouvelles Technologies de l'Information	EGC	2004	EXIT : EXtraction Itérative de la Terminologie		Mathieu Roche, Thomas Heitz, Oriane Matte-Tailliez, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001121	http://editions-rnti.fr/render_pdf.php?p=1001121
Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction de connaissances grâce à un outil de text-mining. Application à la veille informationnelle dans le cadre policier		Marc Borry, Annick Castiaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001167	http://editions-rnti.fr/render_pdf.php?p=1001167
Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction de processus fonctionnels en génétique des  microbes à partir de résumés MEDLINE	Après l'ère du décodage des génomes, les biologistes sont de plus en plus confrontés à l'intégration de myriades de connaissances parcellaires, stockées majoritairement sous forme textuelle. Nous montrons, à travers un exemple concret, que la conjonction de deux chaînes de traitement faisant appel de façon modérée à l'expertise humaine offre au biologiste une aide utile pour parcourir cette littérature, à partir d'une structuration sans a priori de son corpus ; il s'agit ici de résumés Medline indexés par les gènes et protéines qu'ils citent, et que l'algorithme structure (sans superviseur) en principales voies métaboliques et de régulation présentes dans le corpus choisi. 1) Une chaîne d'indexation par les noms de gènes et protéines inclut un expert pour valider, 2) Un environnement interactif de clustering thématique attribue des valeurs graduées de centralité dans chaque thème aux résumés comme aux noms, comme à toute autre variable illustrative (autres termes bio., MeSH, ...).	Alain Lelu, Philippe Bessières, Alain Zasadzinski, Dominique Besagni	http://editions-rnti.fr/render_pdf.php?p1&p=1001107	http://editions-rnti.fr/render_pdf.php?p=1001107
Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction of text summary using latent semantic indexing and information retrieval technique : comparison of four strategies	In this paper, we present four generic text summarization techniques. Each technique extracts a text summary by ranking and extracting sentences from an original document. The first method, SUMMARIZER 1, uses standard information retrieval (IR) methods to rank sentences. The second method, SUMMARIZER 2, uses the Latent Semantic Analysis (LSA) technique to identify semantically important sentences, for summary creations. The third method, SUMMARIZER 3, uses a combination of the latent semantic analysis technique, reduction and relevance measure. The fourth method simply uses the TF*IDF (Term frequency * Inverse Document frequency) weighting scheme. Evaluations of the four methods are conducted using Document Understanding Conferences (DUC) datasets from NIST. We have compared the summary of each method with the manual summaries. Summarizer 4, with its lowest overhead, has comparable performance to summarizer 1. Analysis shows that a combination of LSA technique and the relevance measure (Summarizer 3) has the best performance on an average.	Abdelghani Bellaachia, Anand Mahajan	http://editions-rnti.fr/render_pdf.php?p1&p=1001111	http://editions-rnti.fr/render_pdf.php?p=1001111
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fonctionnement d'un Système Informatique d'Aide à la Décision (SIAD)	Cet article présente le fonctionnement d'un SIAD : alimentation, traitement des données qui l'alimentent, production des résultats, outils de consultation mis à la disposition des utilisateurs, exploitation éditoriale.	Michel Volle	http://editions-rnti.fr/render_pdf.php?p1&p=1000886	http://editions-rnti.fr/render_pdf.php?p=1000886
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fonctions d'oubli dans les entrpôts de données	Les entrepôts de données stockent des quantités de données de plus en plus massives, en particulier du fait de la constitution d'historiques. Nous proposons ici une solution pour éviter la saturation des entrepôts de données. Nous définissons un langage de spécifications de fonctions d'oubli des données les plus anciennes, permettant de déterminer ce qui doit être présent dans l'entrepôt de données à chaque instant. Ces spécifications de fonctions d'oubli se traduisent par des opérations de résumé par agrégation, et par des opérations de suppression des données anciennes réalisées de façon mécanique à chaque pas de mise à jour. La communication présente tout d'abord une description syntaxique du langage de spécifications des fonctions d'oubli. Les contraintes à vérifier pour assurer la cohérence du langage sont ensuite décrites. Enfin, nous proposons des structures de données adaptées au stockage des données nécessaires à la gestion des fonctions d'oubli.	Aliou Boly, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000891	http://editions-rnti.fr/render_pdf.php?p=1000891
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fouille dans la structure de documents XML	La prolifération des documents XML appelle des techniques appropriées pour extraire et exploiter l'information contenue dans ces documents. On distingue deux approches de fouille : XML Content Mining portant sur le contenu et XML Structure Lining qui a trait à la structure des documents. Combiner ces deux approches est très intéressant. Les informations contenues dans la structure orientent la fouille sur le contenu. Nous présentons la première étape de cette démarche : une nouvelle méthode d'extraction des règles d'association à partir de la structure des documents XML qui permet de gérer les aspects hiérarchiques de ces documents tout en améliorant les mécanismes d'extraction grâce à la création d'une structure spéciale représentant la hiérarchie des balises rencontrées.	Amandine Duffoux, Omar Boussaid, Stéphane Lallich, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1001134	http://editions-rnti.fr/render_pdf.php?p=1001134
Revue des Nouvelles Technologies de l'Information	EGC	2004	Fouille de grands ensembles de données avec un boosting proximal SVM	Les SVM (support vector machines) ont montré leur efficacité dans plusieurs domaines d'application. L'apprentissage des SVM se ramène à résoudre un programme quadratique, dont la mise en oeuvre est en général coûteuse en temps. Une reformulation plus récente des SVM (proximal SVM), proposée par Fung et Mangasarian, ne nécessite que la résolution d'un système linéaire, cet algorithme de PSVM est plus efficace et permet de traiter des données dont le nombre d'individus est très important (109) et le nombre d'attributs plus restreint (104). Nous proposons d'utiliser la formule de Sherman-Morrison-Woodbury pour adapter le PSVM à la fouille d'ensembles de données dont le nombre d'attributs est très important et le nombre d'individus plus restreint sur un matériel standard. Puis nous présentons un algorithme de boosting de PSVM pour classifier des données de très grandes tailles en nombre d'individus et d'attributs. Nous évaluons les performances du nouvel algorithme sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et Ndc.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001011	http://editions-rnti.fr/render_pdf.php?p=1001011
Revue des Nouvelles Technologies de l'Information	EGC	2004	Gestion de données hétérogènes dans un entrepôt de données		Laurence Duval	http://editions-rnti.fr/render_pdf.php?p1&p=1000907	http://editions-rnti.fr/render_pdf.php?p=1000907
Revue des Nouvelles Technologies de l'Information	EGC	2004	GVSR : un annuaire de logiciels de manipulation et d'édition de graphes		Bruno Pinaud, Pascale Kuntz, Magalie Delépine, Julien Barberet	http://editions-rnti.fr/render_pdf.php?p1&p=1001094	http://editions-rnti.fr/render_pdf.php?p=1001094
Revue des Nouvelles Technologies de l'Information	EGC	2004	How well go Lattice algorithms on currently used machine leaning TestBeds ?	Many research papers in classification or association rules increase the interest of Concept lattices structures for data mining (DM) and machine learning (ML). To increase the efficiency of concept lattice-bases algorithms in ML, it is necessary to make us of an efficient algorithms to build concept lattices. In fact, more than ten algorithms for generating concept lattices were published. As real data sets for data mining are very large, concept lattice structure suffers form its complexity issues on such data. The efficiency and performance of concept lattices algorithms are very different from one to another. So we need to compare the existing lattice algorithms with large data. We implemented the four first algorithms in Java environment and compared these algorithms on about 30 datasets of the UCI repository that are well established to be used to compare ML algorithms. Preliminary results give preference to Ganter's algorithm, and then to Bordat's algorithm, which do not fil well with the recommendations of Kuznetsov and Obiedkov. Furthermore, we analyzed the duality of lattice-based algorithms.	Huaiguo Fu, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001078	http://editions-rnti.fr/render_pdf.php?p=1001078
Revue des Nouvelles Technologies de l'Information	EGC	2004	Identification de blocs homogènes sur des données continues	Contrairement aux méthodes usuelles de classification ne cherchant généralement qu'une seule partition, soit des instances, soit des attributs, les méthodes de classification croisée et de classification directe fournissent des blocs de données liant des instances à des attributs. Les premières consistent à chercher simultanément une partition en lignes et une partition en colonnes. Les secondes, elles, s'appliquent directement sur les données, et permettent d'obtenir des blocs de données homogènes de toutes tailles, ainsi que des hiérarchies de classes en lignes et en colonnes. Combinant les avantages des deux méthodes, nous présentons ici une méthodologie permettant de travailler sur de grandes bases de données.	François-Xavier Jollois, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1001015	http://editions-rnti.fr/render_pdf.php?p=1001015
Revue des Nouvelles Technologies de l'Information	EGC	2004	Induction extensionnelle : définition et application à l'acquisition de concepts à partir de textes	Lorsque des outils inductifs sont inclus dans un système d'acquisition des connaissances, on dit que l'on construit un système apprenti. C'est dans le but de soulager la charge de travail de l'expert du domaine que cette forme d'apprentissage comporte des outils inductifs. La difficulté tient en ce que l'énumération des connaissances expertes produit des données peu bruitées mais très incomplètes que les itérations successives d'induction vont compléter, toutefois en y ajoutant de grandes quantités de bruit. Il en résulte qu'on doit utiliser des procédures inductives spéciales, adaptées à l'apprentissage par croissance de noyaux de connaissance supervisée. En particulier, pour résoudre le problème difficile de la reconnaissance de concepts dans les textes, nous avons défini une forme d'apprentissage qui intègre l'apprentissage à partir d'instances et les systèmes apprentis, que nous nommons "Induction Extensionnelle", un oxymoron qui souligne que malgré l'absence de création d'un modèle explicite, une induction prend effectivement place.	Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001017	http://editions-rnti.fr/render_pdf.php?p=1001017
Revue des Nouvelles Technologies de l'Information	EGC	2004	Intégration efficace de méthodes  de fouille de données dans les SGBD	Cet article présente une nouvelle approche permettant d'appliquer des algorithmes de fouille, en particulier d'apprentissage supervisé, à de grandes bases de données et en des temps de traitement acceptables. Cet objectif est atteint en intégrant ces algorithmes dans un SGBD. Ainsi, nous ne sommes limités que par la taille du disque et plus par celle de la mémoire. Cependant, les entrées-sorties nécessaires pour accéder à la base engendrent des temps de traitement longs. Nous proposons donc dans cet article une méthode originale pour réduire la taille de la base d'apprentissage en construisant sa table de contingence. Les algorithmes d'apprentissage sont alors adaptés pour s'appliquer à la table de contingence. Afin de valider notre approche, nous avons implémenté la méthode de construction d'arbre de décision ID3 et montré que l'utilisation de la table de contingence permet d'obtenir des temps de traitements équivalents à ceux des logiciels classiques.	Cédric Udréa, Fadila Bentayeb, Jérôme Darmont, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000899	http://editions-rnti.fr/render_pdf.php?p=1000899
Revue des Nouvelles Technologies de l'Information	EGC	2004	Interrogation de sources biomédicales : prise en compte des préférences de l'utilisateur	Nous nous plaçons dans le cadre d'un projet de constitution d'une plate-forme intégrative de données biomédicales pour l'étude génomique des cancers. La plate-forme comporte, entre autres, un certain nombre de scénarios d'analyse qui sont proposés à l'utilisateur. A chaque étape d'un scénario qu'il a choisi de réaliser pour les besoins de son étude, l'utilisateur peut être amené à poser une requête nécessitant d'accéder à différentes sources et il doit alors choisir les sources pertinentes. Nous proposons un guide à l'utilisateur sous forme d'un algorithme de sélection de sources adapté à sa requête et à ses préférences. Pour cela, nous explorons quelques spécificités des banques de données biomédicales et définissons différents critères de préférence utiles pour les biologistes. Nous illustrons notre démarche avec un exemple de requête biomédicale.	Sarah Cohen Boulakia, Christine Froidevaux, Séverine Lair	http://editions-rnti.fr/render_pdf.php?p1&p=1000893	http://editions-rnti.fr/render_pdf.php?p=1000893
Revue des Nouvelles Technologies de l'Information	EGC	2004	Le e-lien une solution pour l'extraction et le partage de connaissances structurées dans les documents hypertextuels		Gilles Verley, Jean-Pierre Asselin de Beauville	http://editions-rnti.fr/render_pdf.php?p1&p=1000917	http://editions-rnti.fr/render_pdf.php?p=1000917
Revue des Nouvelles Technologies de l'Information	EGC	2004	Les règles d'association comme outil de catégorisation textuelle		Simon Jaillet, Maguelonne Teisseire, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1001127	http://editions-rnti.fr/render_pdf.php?p=1001127
Revue des Nouvelles Technologies de l'Information	EGC	2004	Maintenance de bases de connaissances terminologiques	L'acquisition des connaissances terminologiques de l'entreprise se fait souvent à partir des textes qu'elle utilise. Dans le cadre de ce travail, la base de connaissances terminologiques repose sur la modélisation des concepts-métier sous la forme d'une ontologie. Le problème de la maintenance de cette base et de cette ontologie doit alors être traité.Dans cet article, après avoir donné une définition d'une base de connaissances terminologiques (BCT) et des problèmes de diachronie, nous présentons notre modèle et notre méthode d'acquisition des connaissances terminologiques de l'entreprise. Nous exposons alors notre proposition pour maintenir au cours du temps la base de connaissances terminologiques ainsi construite.Nous illustrons ce travail sur une base de connaissance terminologique sur le cinéma d'animation en décrivant le problème de la maintenance dans une reconstitution historique de différents états de cette base lors de l'apparition des techniques numériques d'animation.	Daniel Beauchêne, Christophe Roche, Cécile Million-Rousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000910	http://editions-rnti.fr/render_pdf.php?p=1000910
Revue des Nouvelles Technologies de l'Information	EGC	2004	Manipulation de représentations de cubes de données		Arnaud Giacometti, Patrick Marcel, Hassina Mouloudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000908	http://editions-rnti.fr/render_pdf.php?p=1000908
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mediating the Semantic Web	Cet article développe une extension d'une architecture de médiation pour intégrer le Web sémantique. Plus précisément, XLive est un médiateur tout XML développé à PRiSM. Il permet d'exécuter des XQuery sur des sources de données hétérogènes. Après une rapide présentation de XLive et du Web sémantique, une architecture à trois niveaux d'ontologies et de schémas est introduite pour connecter des adaptateurs pour le Web sémantique. Cette architecture vise à intégrer des sources de type Web service d'information conformément à une ontologie globale de référence. Elle conduit à étendre XLive avec le support de vues, un outil de conception de vues et de mappings, et des adaptateurs pour les Web services.	Georges Gardarin, Tuyet-Tram Dang-Ngoc	http://editions-rnti.fr/render_pdf.php?p1&p=1000883	http://editions-rnti.fr/render_pdf.php?p=1000883
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mesurer la qualité des règles et de leurs contraposées avec le taux informationnel TIC	La validation des connaissances est l'une des étapes les plus problématiques d'un processus de découverte de règles d'association. Pour que le décideur (expert des données) puisse trouver des connaissances intéressantes dans les grandes quantités de règles produites par les algorithmes de fouille de données, il est nécessaire de mesurer la qualité des règles. Nous insérant dans le cadre de l'analyse statistique implicative, nous proposons dans cet article d'évaluer les règles en considérant leur contenu informationnel à travers un nouvel indice de qualité fondé sur l'entropie de Shannon : TIC (Taux Informationnel modulé par la Contraposée). Cet indice a l'avantage d'être bien adapté à la sémantique des règles, puisque d'une part il respecte leur caractère asymétrique et d'autre part il tire profit de leurs contraposées. Par ailleurs, c'est à notre connaissance la seule mesure de qualité de règles qui intègre à la fois indépendance et déséquilibre, c'est-à-dire qui permette de rejeter simultanément les règles entre variables corrélées négativement et les règles qui possèdent plus de contre-exemples que d'exemples. Des comparaisons de TIC avec la J-mesure, l'information mutuelle, l'indice de Gini, et la confiance sont réalisées sur des simulations numériques.	Julien Blanchard, Fabrice Guillet, Régis Gras, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1001035	http://editions-rnti.fr/render_pdf.php?p=1001035
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mesurer les usages d'internet	Nous rendons compte d'une démarche mise en place pour construire une représentation fine des usages d'internet et de leur évolution, en procédant à du traitement secondaire de données de trafic, provenant de panels représentatifs d'internautes. Après avoir présenté les caractéristiques des cohortes étudiées et les différents modes d'enrichissement des données de trafic mis en place, nous présentons quelques résultats construits à partir de ces données enrichies, et en particulier une segmentation des internautes construite sur la base de l'entrelacement des pratiques de communication et de navigation.	Valérie Beaudouin	http://editions-rnti.fr/render_pdf.php?p1&p=1000887	http://editions-rnti.fr/render_pdf.php?p=1000887
Revue des Nouvelles Technologies de l'Information	EGC	2004	Mise en oeuvre des méthodes de fouille de données spatiales alternatives et performances	La fouille de données spatiales nécessite l'analyse des interactions dans l'espace. Ces interactions peuvent être matérialisées dans des tables de distances, ramenant ainsi la fouille de données spatiales à l'analyse multitables. Or, les méthodes de fouilles de données traditionnelles considèrent une seule table en entrée où chaque tuple est une observation à analyser. De simples jointures entre ces tables ne résoud pas le problème et fausse les résultats en raison du comptage multiple des observations. Nous proposons trois alternatives de fouille de données multi-tables dans le cadre de la fouille des données spatiales. La première consiste à interroger à la volée les différentes tables et modifie en dur les algorithmes existants. La seconde est une optimisation de la première qui pré -calcule les jointures et adapte les algorithmes existants. La troisième réorganise les données dans une table unique en complétant - et non en joignant- la table d'analyse par les données présentes dans les autres tables, ensuite applique un algorithme standard sans modification. Cet article présente ces trois alternatives. Il décrit leur implémentation pour la classification supervisée et compare leur performance.	Nadjim Chelghoum, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1001003	http://editions-rnti.fr/render_pdf.php?p=1001003
Revue des Nouvelles Technologies de l'Information	EGC	2004	Modèle de gestion intégrée des compétences et connaissances	La compétence et la connaissance sont deux concepts qui nous semblent fortement conjoints, cependant, ils sont rarement étudiés et gérés ensemble. Nous cherchons donc à identifier les liens et frontières qui peuvent exister entre eux. Ceci a pour objectif de développer un modèle de représentation et de gestion, intégré aux connaissances et aux compétences. Dans cet article, est tout d'abord présentée, une synthèse sur les concepts de compétence et de connaissance. Ensuite, les modèles et outils de gestion de ces concepts sont exposés. Puis, le modèle CKIM (Competency and Knowledge Integrated Model) développé, est défini. Les utilités de ce modèle et son exploitation sont discutées en quatrième partie. La dernière partie représente un prototype d'implantation du modèle CKIM réalisé sur le serveur de connaissances ATHANOR.	Nathalie Vergnaud, Mounira Harzallah, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000915	http://editions-rnti.fr/render_pdf.php?p=1000915
Revue des Nouvelles Technologies de l'Information	EGC	2004	Modèle topologique pour l'interrogation des bases d'images	Nous proposons dans cet article un modèle topologique de représentation de bases d'images. Chaque image est représentée à l'aide d'un vecteur de caractéristiques dans R^p et figure comme noeud dans un graphe de voisinage. L'exploration du graphe correspond à la navigation dans la base de données, les voisins d'un noeud représentent des images similaires. Afin de pouvoir traiter des requêtes, nous définissons un modèle topologique. L'image requête est représentée par un vecteur de caractéristiques dans R^p et insérée dans le graphe en mettant à jour localement les relations de voisinage. Ce travail se positionne dans le domaine de la fouille de données complexes.	Mihaela Scuturici, Jérémy Clech, Vasile-Marian Scuturici, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001092	http://editions-rnti.fr/render_pdf.php?p=1001092
Revue des Nouvelles Technologies de l'Information	EGC	2004	Modélisation dynamique et temporelle de l'utilisateur pour un filtrage personnalisé de documents textuels	L'apprentissage efficace du profil utilisateur est un challenge car il évolue sans cesse. Dans cet article nous proposons une nouvelle approche pour l'apprentissage du profil long-terme de l'utilisateur pour le filtrage de documents textuels. Dans ce cadre, les documents consultés sont classés de manière dynamique et nous analysons la répartition dans le temps des classes de documents afin de déterminer le mieux possible les classes d'intérêts de l'utilisateur. L'étude empirique confirme la pertinence de notre approche pour une meilleure personnalisation de documents.	Rachid Arezki, Abdenour Mokrane, Pascal Poncelet, Gérard Dray, David William Pearson	http://editions-rnti.fr/render_pdf.php?p1&p=1001122	http://editions-rnti.fr/render_pdf.php?p=1001122
Revue des Nouvelles Technologies de l'Information	EGC	2004	MUSETTE : a framework for knowledge capture from experience	Nous présentons dans cet article une nouvelle approche de modélisation de l'expérience d'utilisation d'un système informatique, avec pour objectif de réutiliser cette expérience en contexte pour assister l'utilisateur à effectuer sa tâche. Quatre scénarios illustrent cette approche.	Pierre-Antoine Champin, Yannick Prié, Alain Mille	http://editions-rnti.fr/render_pdf.php?p1&p=1000912	http://editions-rnti.fr/render_pdf.php?p=1000912
Revue des Nouvelles Technologies de l'Information	EGC	2004	OpAC : Opérateur d'analyse en ligne basé sur une technique de fouilles de données	L'analyse en ligne OLAP (On-Line Analysis Processing) et la fouille de données (Data Mining) sont deux champs de recherche qui ont connu, depuis quelques années, des évolutions parallèles et indépendantes. De récentes études ont montré l'importance et l'intérêt de l'association entre ces deux domaines scientifiques. A l'heure actuelle, on assiste à l'accroissement du besoin d'une analyse en ligne plus élaborée. Nous pensons que le couplage entre OLAP et la fouille de données pourra apporter des réponses à ce besoin. Dans cet article, nous proposons d'adopter ce couplage en vue de créer un nouvel opérateur, baptisé OpAC (Opérateur d'Agrégation par Classification), d'analyse en ligne des données multidimensionnelles. OpAC consiste particulièrement en l'agrégation sémantique des modalités d'une dimension d'un cube de données en se basant sur la technique de la classification ascendante hiérarchique.	Riadh Ben Messaoud, Sabine Rabaseda, Omar Boussaid, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000889	http://editions-rnti.fr/render_pdf.php?p=1000889
Revue des Nouvelles Technologies de l'Information	EGC	2004	Optimisation des requêtes temporelles sur le web		Rim Faiz, Nizar Khayati, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1001126	http://editions-rnti.fr/render_pdf.php?p=1001126
Revue des Nouvelles Technologies de l'Information	EGC	2004	Outil de représentation des évolutions de communautés d'intérêts	Cet article présente un système de visualisation permettant l'observation des comportements collectifs implicites. Il s'agit de reconnaître et de représenter des communautés à partir des connexions Internet des utilisateurs : les utilisateurs sont répartis en communautés en fonction des similarités entre des listes de termes établies sur l'analyse des documents consultés par chacun d'eux. L'étude est rendue dynamique par la comparaison des communautés reconnues sur des périodes de temps connexes. L'outil décrit ci après offre deux représentations différentes de ces communautés : une vision des liaisons thématiques entre les utilisateurs sur chaque période étudiée et une vue comparative des communautés reconnues sur toute la durée de l'étude.	Anne Lavallard, Luigi Lancieri	http://editions-rnti.fr/render_pdf.php?p1&p=1001140	http://editions-rnti.fr/render_pdf.php?p=1001140
Revue des Nouvelles Technologies de l'Information	EGC	2004	PoBOC : un algorithme de 	Nous décrivons l'algorithme PoBOC (Pole-Based Overlapping Clustering) qui génère un ensemble de clusters non-disjoints (ou "softclusters") présentés sous forme d'une hiérarchie de concepts à partir de la seule matrice de similarités sur les données considérées. Nous évaluons l'approche sur deux situations d'apprentissage : la classification par apprentissage de règles et l'organisation de données plus complexes et peu structurées telles que les données textuelles.La validation des méthodes de clustering est une étape difficile résolue le plus souvent par une évaluation d'experts. Les deux applications proposées permettent de valider la méthode d'organisation selon deux points de vue : d'une part quantitativement en évaluant l'influence de la méthode pour la classification, d'autre part en permettant une analyse "humaine" du résultat dans le cas des données textuelles. Nous mettons en évidence l'intérêt de PoBOC comparativement à d'autres approches d'apprentissage non-supervisé.	Guillaume Cleuziou, Lionel Martin, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001007	http://editions-rnti.fr/render_pdf.php?p=1001007
Revue des Nouvelles Technologies de l'Information	EGC	2004	Positionnement multidimensionnel et partitionnement pour la visualisation de données multivariées		Antoine Naud	http://editions-rnti.fr/render_pdf.php?p1&p=1001030	http://editions-rnti.fr/render_pdf.php?p=1001030
Revue des Nouvelles Technologies de l'Information	EGC	2004	Qualité et datawarehouse dans le milieu hospitalier		Mireille Cosquer, François Gros, Alain Livartowski	http://editions-rnti.fr/render_pdf.php?p1&p=1000905	http://editions-rnti.fr/render_pdf.php?p=1000905
Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche ciblée de documents sur le web	Les langages de requêtes mots-clés pour le web manquent souvent de précision lorsqu'il s'agit de rechercher des documents particuliers difficilement caractérisables par de simples mots-clés (exemple : des cours java ou des photos de formule 1). Nous proposons un langage multi-critères de type attribut-valeur pour augmenter la précision de la recherche de documents sur le web.Nous avons expérimentalement montré le gain de précision de la recherche de documents basé sur ce langage.	Amar-Djalil Mezaour	http://editions-rnti.fr/render_pdf.php?p1&p=1001124	http://editions-rnti.fr/render_pdf.php?p=1001124
Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche dans de grandes bases d'images fixes : une nouvelle approche guidée par les règles d'association	Une base d'images fixes peut être décrite de plusieurs façons, notamment par des descripteurs visuels globaux de couleur, de texture, ou de forme. Les requêtes les plus fréquentes impliquent et combinent les résultats de plusieurs types de descripteurs : par exemple, "retrouver toutes les images ayant une couleur et une texture semblables à celles d'une image requête donnée". Pour retrouver plus efficacement et plus rapidement une image dans une grande base, nous exploitons des combinaisons appropriées de descripteurs et étudions l'intérêt des règles d'association entre clusters de descripteurs pour accélérer le temps de réponse à des requêtes sur de grandes bases d'images fixes.	Anicet Kouomou Choupo, Annie Morin, Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000895	http://editions-rnti.fr/render_pdf.php?p=1000895
Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche de règles d'association hiérarchiques par une approche anthropocentrée	L'Extraction de Connaissances dans la Bases de Données est devenue, pour les banques, une alternative au problème lié à la quantité de données qui sont stockées et qui ne cessent d'augmenter. Ceci aboutit à un paradoxe puisqu'il faut mieux cibler la clientèle susceptible d'être intéressée par une offre en utilisant des méthodes qui ne permettent plus de traiter le nombre croissant d'enregistrements des bases de données. Nos travaux se situent dans la continuité d'une étude que nous avons réalisée sur la recherche de règles d'association appliquée au marketing bancaire. En effet, des premiers résultats encourageants nous ont conduit à approfondir nos travaux vers une recherche de règles d'association hiérarchiques utilisant non plus une approche automatique mais une approche anthropocentrée. Il s'agit d'une approche dans laquelle l'expert fait partie intégrante du processus en jouant le rôle d'heuristique évolutive. Cet article présente les résultats de notre démarche de recherche.	Olivier Couturier, Engelbert Mephu Nguifo, Brigitte Noiret	http://editions-rnti.fr/render_pdf.php?p1&p=1001154	http://editions-rnti.fr/render_pdf.php?p=1001154
Revue des Nouvelles Technologies de l'Information	EGC	2004	Réduction d'un jeu de règles d'association par des méta-règles issues de la logique du 		Martine Cadot, Joseph Di Martino, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001058	http://editions-rnti.fr/render_pdf.php?p=1001058
Revue des Nouvelles Technologies de l'Information	EGC	2004	Réduction du coût d'évaluation d'une règle relationnelle	De nombreuses tâches en Fouille de Données visent à extraire des connaissances exprimées sous la forme d'un ensemble de règles. Les algorithmes dédiés à ces tâches engendrent des règles dont l'adéquation aux données doit être évaluée. On se place dans le cadre où cette évaluation est réalisée directement en lançant des requêtes de dénombrement sur la base de données, et où cette base est relationnelle. Les requêtes comptent les données qui s'apparient avec la règle, calcul qui peut être extrêmement coûteux. Dans cet article, nous étudions l'impact d'une approche d'échantillonnage visant à réduire le coût de l'évaluation des règles relationnelles en tenant compte des spécificités structurelles des requêtes induites.	Agnès Braud, Teddy Turmeaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001039	http://editions-rnti.fr/render_pdf.php?p=1001039
Revue des Nouvelles Technologies de l'Information	EGC	2004	Règles d'identification et méthodes de visualisation d'objets architecturaux	Dans l'étude du patrimoine bâti, la gestion d'informations pose aujourd'hui des problèmes d'interfaçage non triviaux, notamment par la masse, la diversité, la complexité et le caractère hétérogène des contenus. La représentation tridimensionnelle du tissu urbain à différentes échelles (de la ville au corpus architectural), parce qu'elle localise spatialement l'information à délivrer et l'attache à la morphologie de l'édifice, apparaît comme une des réponses possibles. Cette réponse semble par ailleurs bien adaptée aux problématiques spécifiques de l'analyse architecturale du patrimoine que sont par exemple la restitution d'édifices disparus (et les notions d'incertitude qui s'y attachent) ou le réemploi d'éléments de corpus. Pourtant, la représentation tridimensionnelle dans notre champ d'application est aujourd'hui loin de remplir ce rôle. Notre contribution vise à discuter quelques uns des pré-requis qui nous semblent s'imposer à la lumière de nos expériences pour faire de la maquette 3D un outil d'investigation des connaissances sur l'édifice.	Iwona Dudek, Jean-Yves Blaise	http://editions-rnti.fr/render_pdf.php?p1&p=1001157	http://editions-rnti.fr/render_pdf.php?p=1001157
Revue des Nouvelles Technologies de l'Information	EGC	2004	Régression linéaire symbolique avec variables taxonomiques	Le présent papier concerne l'extension des méthodes classiques de régression linéaire aux cas des données symboliques et fait suite à de précédents travaux de Billard et Diday sur la régression linéaire avec variables intervalles et histogrammes. Dans ce papier, nous présentons des méthodes de régression avec variables taxonomiques. Les variables taxonomiques sont des variables organisées en arbre exprimant plusieurs niveaux de généralité (les villes sont regroupées en régions qui sont elles-mêmes regroupées en pays). La méthode proposée sera testée sur données simulées. Finalement, nous observerons que ces méthodes nous permettent d'utiliser la régression linéaire pour étudier des concepts et pour réduire le nombre de données afin d'améliorer les résultats obtenus par rapport à une régression classique.	Filipe Afonso, Lynne Billard, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001000	http://editions-rnti.fr/render_pdf.php?p=1001000
Revue des Nouvelles Technologies de l'Information	EGC	2004	Relations entre gènes impliqués dans les cancers de la thyroïde	Des relations entre gènes et protéines impliqués dans les cancers de la thyroïde ont été mises en évidence par l'analyse d'un important corpus de résumés de  la base de données bibliographique Medline. Une approche pluridisciplinaire (biologistes, cliniciens, linguistes et chercheurs en sciences de l'information) a permis l'indexation automatique et l'analyse de ce corpus. L'indexation contrôlée, structurée en classes sémantiques, à partir de vastes ressources hétérogènes (les bases biomédicales et génétiques UMLS et LocusLink), prend en compte la spécificité des termes : nomenclatures biochimiques, acronymes de gènes, aberrations chromosomiques ou encore variantes linguistiques de termes. Les deux méthodes de classification complémentaires appliquées révèlent un réseau lexical dense de gènes concurrents autour de trois principales pathologies de la thyroïde : les cancers médullaires, papillaires et des dysfonctionnements du système immunitaire. Les développements apportés aux outils de visualisation interactifs du serveur VISA de l'INIST facilitent lecture et navigation au sein des documents.	Jean Royauté, Claire François, Alain Zasadzinski, Dominique Besagni, Philippe Dessen, Sylvaine Le Minor, Marie-Thérèse Maunoury	http://editions-rnti.fr/render_pdf.php?p1&p=1001119	http://editions-rnti.fr/render_pdf.php?p=1001119
Revue des Nouvelles Technologies de l'Information	EGC	2004	Représentation condensée de motifs émergents	Les motifs émergents sont des associations de caractéristiques fortement présentes dans une classe et rares dans les autres. Ils font ressortir les distinctions entre classes et se révèlent particulièrement efficaces pour construire des classifieurs et apporter une aide au diagnostic. À cause de la forte combinatoire du problème, la recherche et la représentation des motifs émergents restent des tâches complexes pour de grandes bases de données. Nous proposons ici une représentation condensée exacte des motifs émergents (i.e., les motifs et leurs taux de croissance sont directement obtenus depuis la représentation condensée). L'idée principale est de s'appuyer sur les récents résultats relatifs aux représentations condensées de motifs fermés fréquents. À partir de cette représentation, nous donnons aussi une méthode aisée à mettre en oeuvre pour obtenir les motifs émergents ayant les meilleurs taux de croissance. Ces motifs, appelés motifs émergents forts, ont été exploités avec succès dans une collaboration avec la société Philips.	Arnaud Soulet, Bruno Crémilleux, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001022	http://editions-rnti.fr/render_pdf.php?p=1001022
Revue des Nouvelles Technologies de l'Information	EGC	2004	Représentation de graphes par ACP granulaire	L'extraction d'information de grands graphes repose le plus souvent sur leur représentation dans des espaces de dimension réduite et on utilise généralement des méthodes factorielles appliquées à des mesures de dissimilarités calculées à partir des matrices associée du graphe ou l'analyse spectrale de leur Laplacien discret. Efficaces pour dégager les structures globales, ces représentations sont parfois peu exploitables dès lors que l'on s'intéresse à une perspective du graphe à partir de certains sommets privilégiés. Or l'information recherchée a souvent un caractère "local". Pour représenter le graphe du point de vue d'un ou plusieurs sommets sélectionnés, nous proposons une méthode d'Analyse en Composantes Principales "Granulaire" consistant à appliquer une A.C.P. "filtrée" à un tableau de proximités. La visualisation d'un graphe de dictionnaire dont la mesure de proximité est obtenue à partir d'un algorithme original illustre notre propos.	Bruno Gaume, Louis Ferré	http://editions-rnti.fr/render_pdf.php?p1&p=1001083	http://editions-rnti.fr/render_pdf.php?p=1001083
Revue des Nouvelles Technologies de l'Information	EGC	2004	Résumé de cubes de données multidimensionnelles à l'aide de règles floues	Dans le contexte des entrepôts de données, et des magasins de données multidimensionnelles, les outils OLAP fournissent des moyens aux utilisateurs de naviguer dans leur données afin d'y découvrir des informations pertinentes. cependant, les données à traiter sons souvent très volumineuses et ne permettent pas une exploration systématique et exhaustive. Il s'agit donc de développer des traitements automatisés facilitant la visualisation et la navigation dans les données. Dans cet article, nous étudions une méthode originale permettant de construire et d'identifier de manière automatique et efficace des blocs de données similaires présents dans les cubes de données pouvant être exprimés sous la forme de règles. Cette méthode est fondée sur l'utilisation combinée d'un algorithme par niveaux (de type Apriori) et de la théorie des sous-ensembles flous. Cette théorie nous permet en effet de pallier les problèmes posés par le fait que les blocs de données calculés par notre algorithme peuvent se recouvrir.	Yeow Wei Choong, Anne Laurent, Dominique Laurent, Pierre Maussion	http://editions-rnti.fr/render_pdf.php?p1&p=1000904	http://editions-rnti.fr/render_pdf.php?p=1000904
Revue des Nouvelles Technologies de l'Information	EGC	2004	Sélection d'attributs et classification d'objets complexes		Alexandre Blansché, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000994	http://editions-rnti.fr/render_pdf.php?p=1000994
Revue des Nouvelles Technologies de l'Information	EGC	2004	Sélection rapide en apprentissage supervisé	La sélection de variables (SdV) permet de réduire l'espace de représentation des données. Ce processus est de plus en plus critique en raison de l'augmentation de la taille des bases de données. Traditionnellement, les méthodes de SdV nécessitent plusieurs accès au jeu de données, ce qui peut représenter une part relativement importante du temps d'exécution de ces algorithmes. Nous proposons une nouvelle méthode efficiente et rapide (ne nécessitant qu'un unique accès aux données). Cette méthode utilise les algorithmes génétiques ainsi que des mesures de validité de classification non supervisée (cns).	Nicolas Nicoloyannis, Gaëlle Legrand, Pierre-Emmanuel Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1000953	http://editions-rnti.fr/render_pdf.php?p=1000953
Revue des Nouvelles Technologies de l'Information	EGC	2004	Sous-ensembles flous définis sur une ontologie	Les sous-ensembles flous peuvent être utilisés pour représenter des valeurs imprécises, comme un intervalle aux limites mal définies. Ils peuvent également servir à l'expression de préférences dans les critères de sélection de requêtes en bases de données. En représentation des connaissances, l'utilisation de hiérarchies de types est largement répandue afin de modéliser les relations existant entre les types d'objets d'un domaine donné. Nous nous intéressons aux sous-ensembles flous dont le domaine de définition est une hiérarchie d'éléments partiellement ordonnés par la relation "sorte de", que nous appelons ontologie. Nous introduisons la notion de sous-ensemble flou défini sur une partie de l'ontologie, puis sa forme développée définie sur l'ensemble de l'ontologie, que nous appelons extension du sous-ensemble flou. Des classes d'équivalence de sous-ensembles flous définis sur une ontologie peuvent être caractérisées par un représentant unique que nous appelons sous-ensemble flou minimal. Nous concluons par un exemple d'application dans un système d'information relatif à la prévention du risque micro-biologique en sécurité alimentaire.	Rallou Thomopoulos, Patrice Buche, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1000914	http://editions-rnti.fr/render_pdf.php?p=1000914
Revue des Nouvelles Technologies de l'Information	EGC	2004	Uitliation de connaissances pour l'aide à la recherche documentaire fondée sur le contenu		Amedeo Napoli, Rim Al Hulou	http://editions-rnti.fr/render_pdf.php?p1&p=1001125	http://editions-rnti.fr/render_pdf.php?p=1001125
Revue des Nouvelles Technologies de l'Information	EGC	2004	Un algorithme de génération des itemsets fermés pour la fouille de données	Le traitement de grand volume de données est un problème pour l'extraction de connaissances. La fouille de données nécessite des méthodes de résolution efficaces. Le treillis de concepts (treillis de Galois) est un outil utile pour l'analyse de données. Des travaux en classification et sur les règles d'association ont permis d'accroître son intérêt. Plusieurs algorithmes de génération on été proposés, parmi lesquels NextClosure est l'un des meilleurs pour traiter des données de grande taille.Mais la complexité de NextClosure reste malgré tout très élevé. Aussi nous proposons un nouvel algorithme efficace nommé ScalingNextClosure, et basé sur une méthode de partitionnement de données pour générer de manière indépendante les itemsets fermés de chaque partition. Les résultats expérimentaux montrer que cette technique de partitionnement améliore efficacement NextClosure.	Engelbert Mephu Nguifo, Huaiguo Fu	http://editions-rnti.fr/render_pdf.php?p1&p=1001062	http://editions-rnti.fr/render_pdf.php?p=1001062
Revue des Nouvelles Technologies de l'Information	EGC	2004	Une approche probabiliste pour le classement d'objets incomplets dans un arbre de décision		Lamis Hawarah, Ana Simonet, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1001029	http://editions-rnti.fr/render_pdf.php?p=1001029
Revue des Nouvelles Technologies de l'Information	EGC	2004	Une étude d'algorithmes de classification supervisée basée sur les treillis de Galois		Huaiyu Fu, Huaiguo Fu, Patrick Njiwoua, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001025	http://editions-rnti.fr/render_pdf.php?p=1001025
Revue des Nouvelles Technologies de l'Information	EGC	2004	Une méthode pour l'appropriation de savoir-faire, capitalisé avec MASK	La gestion explicite des savoirs et savoir-faire occupe une place de plus en plus importante dans les organisations. La construction de mémoires d'entreprise dans un but de préservation et de partage est devenu une pratique assez courante. Cependant, on oublie trop suivent que l'efficacité de ces activités est étroitement liée aux capacités d'appropriation et d'apprentissage des acteurs de l'organisation.Dans cet article, nous proposons des démarches générales d'accompagnement permettant de faciliter le processus d'appropriation des mémoires d'entreprise construits avec la méthode MASK, en exploitant des techniques d'ingénierie pédagogique.	Oswaldo Castillo, Nada Matta, Jean-Louis Ermine	http://editions-rnti.fr/render_pdf.php?p1&p=1000911	http://editions-rnti.fr/render_pdf.php?p=1000911
Revue des Nouvelles Technologies de l'Information	EGC	2004	Utilisation des graphes de proximité dans le cadre de l'apprentissage basé sur les voisins	La classification suivant les plus proches voisins est une règle simple et attractive, basée sur une définition paramétrique du voisinage. Les graphes des proximité, quand à eux, induisent des notions plus souples de voisinage. Il s'agit ici d'effectuer la substitution.Les variantes obtenues, peu testées dans la bibliographie, ont été soumises à une expérimentation intensive, sur bases de données de l'UCI et de France Télécom. On a ainsi considéré divers types de prétraitement des données et plusieurs catégories de graphes. De plus, on a caractérisé les effets du "piège de la dimension" sur le comportement théorique de tous les graphes présentés, une quantification empirique du phénomène ayant été réalisée.Il ressort de notre étude que l'utilisation du voisinage de Gabriel provoque une amélioration en moyenne et que le prétraitement basé sur la statistique de rang est le plus adéquate. Quoiqu'il arrive, des précautions doivent être prises en grande dimension.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001061	http://editions-rnti.fr/render_pdf.php?p=1001061
Revue des Nouvelles Technologies de l'Information	EGC	2004	Validation de graphes conceptuels	Les travaux menés en validation des connaissances visent à améliorer la qualité des bases de connaissances. Le modèle des graphes conceptuels est un modèle de représentation des connaissances de la famille des réseaux sémantiques, fondé sur la théorie des graphes et sur la logique du premier ordre. Nous proposons une solution pour valider sémantiquement une base de connaissances composée de graphes conceptuels. La validation sémantique d'une base de connaissance consiste à confronter ses connaissances à des contraintes certifiées fiables. Nous proposons d'utiliser des contraintes descriptives, exprimées sous forme de graphes conceptuels, qui permettent de poser des conditions sur la représentation de certaines connaissance dans la base. Ces contraintes introduisent une notion de cardinalités, et sont soit minimales, soit maximales. Elles permettent respectivement d'exprimer "si A, alors au moins ou au plus n fois B". La satisfaction de ces contraintes par une base de connaissances repose sur l'utilisation de l'opération de base du modèle des graphes conceptuels : la projection.	Juliette Dibie-Barthélemy, Ollivier Haemmerlé, Eric Salvat	http://editions-rnti.fr/render_pdf.php?p1&p=1000913	http://editions-rnti.fr/render_pdf.php?p=1000913
Revue des Nouvelles Technologies de l'Information	EGC	2004	Veille technologique assistée par la Fouille de Textes	Le domaine de la veille technologique vise à récolter, traiter, et analyser des informations scientifiques et techniques utiles aux acteurs économiques. Dans cet article nous proposons d'utiliser des techniques de fouille de textes pour automatiser le processus de traitement des données issues de bases de textes scientifiques. Toutefois, la veille introduit une difficulté inhabituelle par rapport aux domaines d'application classiques des techniques de fouille de textes, puisqu'au lieu de rechercher de la connaissances fréquente cachée dans les données, il faut rechercher de la connaissance inattendue. Les mesures usuelles d'extraction de la connaissance à partir de textes doivent de ce fait être revues. Pour ce faire, nous avons développé le système UnexpectedMiner dans lequel de nouvelles mesures permettent d'estimer le caractère inattendu d'un document. Notre système est évalué sur une base d'articles dans le domaine de l'apprentissage automatique.	François Jacquenet, Christine Largeron, Stéphanie Chapaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001097	http://editions-rnti.fr/render_pdf.php?p=1001097
Revue des Nouvelles Technologies de l'Information	EGC	2004	Vers un entrepôt de données pour la gestion des risques naturels	Les entrepôts de données sont l'un des plus importants développements dans le domaine des systèmes d'informations. Ils permettent d'intégrer des données de plusieurs sources, souvent très volumineux, distribuées et hétérogènes. Dans cet article, nous examinons la possibilité d'utiliser la technique d'entrepôt de données dans la gestion des risques naturels. Nous présentons un modèle conceptuel pour l'entrepôt proposé, avec la présence de formats et types variés de données tel que des données géographiques et multimédia. Nous proposons également des opérations OLAP pour la navigation des informations stockées dans le cube de données.	Hicham Hajji, Nourdine Badji, Jean-Pierre Asté	http://editions-rnti.fr/render_pdf.php?p1&p=1001163	http://editions-rnti.fr/render_pdf.php?p=1001163
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Evaluation et analyse multicritère des mesures de qualité des règles d'association	Le nombre particulièrement important de règles générées parles algorithmes utilisés en extraction de connaissances à partir de données (Ecd) ne permet pas aux utilisateurs de faire eux-mêmes la sélection des règles pertinentes. Un des problèmes centraux de l'Ecd est le développement de mesures évaluant l'intérêt des règles découvertes. Ainsi de nombreuses mesures ont été proposées, parmi lesquelles l'expert est censé choisir celle qui est la plus appropriée à ses besoins. Mais le nombre élevé de mesures est lui-même un frein à la capacité de choix d'un expert. Pour y remédier, de nombreuses publications dressent des panoramas partiels de ces mesures, s'attachant tantôt à leur qualité algorithmique, tantôt à la formalisation de leurs propriétés, etc. Le résultat n'est malheureusement pas à la hauteur des espérances. A la multiplicité des mesures s'ajoute maintenant la diversité des caractéristiques, lesquelles ne reflètent généralement pas les objectifs de l'expert. Enfin, l'hétérogénéité des valeurs prises par ces caractéristiques n'est pas la moindre des difficultés que rencontre l'utilisateur quand il entreprend de comparer les mesures. Dans ce contexte, et malgré les efforts entrepris, le choix, par un utilisateur métier, de mesures répondant à son objectif reste un défi majeur de la recherche en Ecd. La recherche des meilleures règles parmi le vaste ensemble de règles produit, passe aussi par la recherche et l'utilisation des bonnes mesures. On se trouve donc dans une problématique d'aide multicritère à la décision (Amd). Nous fondons notre approche sur l'évaluation de 20 mesures à partir de 8 critères.	Philippe Lenca, Patrick Meyer, Benoît Vaillant, Philippe Picouet, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000113	http://editions-rnti.fr/render_pdf.php?p=1000113
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Évaluation et validation de l'intérêt des règles d'association	La recherche de règles d'association intéressantes est un thème privilégié de l'extraction des connaissances à partir des données. Les algorithmes du type Apriori fondés sur le support et la confiance des règles ont apporté une solution élégante au problème de l'extraction de règles, mais ils produisent une trop grande masse de règles, sélectionnant certaines règles sans intérêt et ignorant des règles intéressantes. Il faut disposer d'autre mesures venant compléter le support et la confiance. Dans cet article, nous passons en revue les principales mesures proposées dans la littérature et nous proposons des critères pour les évaluer. Nous suggérons ensuite une méthode de validation qui utilise les outils de la théorie de l'apprentissage statistique, notamment la VC-dimension. Face au grand nombre de mesures et à la multitude de règles candidates, l'intérêt de ces outils est de permettre la construction de bornes uniformes non asymptotiques pour toutes les règles et toutes les mesures simultanément.	Stéphane Lallich, Olivier Teytaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000112	http://editions-rnti.fr/render_pdf.php?p=1000112
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Extraction de pépites de connaissance dans les données	La plupart des méthodes permettant d'extraire des règles d'association dans les données sont basées sur l'utilisation de mesure et de seuils prédéfinis par l'expert pour optimiser la recherche des règles.Le choix des suils permettant de séparer les règles intéressantes des règles triviales est difficle, même pour un expert du domaine étudié.Si l'on considère que les données sont bruitées et que l'extraction d'informations du types "pépites" de connaissance (c'est à dire, règles ayant un faible support) peut intéresser l'expert, alors les règles classiques sont souvent mises en défaut dans de telles situations.Nous proposons donc une nouvelle mesure d'extraction des règles d'association, appelée "moindre contradiction". Nous montrons que cette mesure (i) permet d'extraire des "pépites" de connaissance dans les données, sans pour autant être submergés par le nombre de règles ayant un faible support, (ii) se comporte légèrement mieux que les mesures classiques étudiées lorsque les données sont bruitées.	Jérôme Azé, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000114	http://editions-rnti.fr/render_pdf.php?p=1000114
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Indice Probabiliste Discriminant de Vraisemblance du lien pour des Données Volumineuses	On sait que l'indice probabiliste implicatif usuel de vraisemblance du lien évaluant de façon intrinsèque une règle d'association, n'est plus discriminant si le nombre d'observations augmente suffisamment. Le but de cet article est de montrer l'extension discriminante de cet indice probabiliste pour évaluer une règle d'association, mais, dans le contexte d'un ensemble de règles. Cette approche a été proposé de longue date et a été validée dans le cadre de la classification hiérarchique AVL (Analyse de la Vraisemblance des Liens) d'un ensemble d'attributs de types quelconques. Une analyse expérimentale qui consiste à faire croître la taille des données par l'adjonction de contre-exemples à tous les attributs, montre toute la pertinence de la démarche statistique. Cette dernière est, au préalable, justifiée conceptuellement.	Jérôme Azé, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1000107	http://editions-rnti.fr/render_pdf.php?p=1000107
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Knowledge quality measures in a data-mining process		Vipin Kumar	http://editions-rnti.fr/render_pdf.php?p1&p=1000103	http://editions-rnti.fr/render_pdf.php?p=1000103
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	La qualité des données comme condition à la qualité des connaissances, un état de l'art	Les travaux actuels sur l'extraction de connaissances à partir des données (ECD) se focalisent sur la recherche de règles intéressantes dont on souhaite pouvoir qualifier l'intérêt ou le caractère exceptionnel, mais dont la validité dépend bien évidemment de celle des données. En amont du processus d'ECD, il semble donc essentiel d'évaluer la qualité des données stockées dans les bases et entrepôts de données afin de : (1) proposer aux utilisateurs une expertise critique de la qualité du contenu d'un système, (2) orienter l'extraction des connaissances en fonction d'un profil ciblé d'utilisateurs et de décideurs, (3) permettre à ceux-ci de relativiser la confiance qu'ils pourraient accorder aux données et aux règles extraites, et leur permettre ainsi de mieux en adapter leur usage, (4) assurer enfin la validité et l'intérêt des connaissances extraites à partir des données. Cet article fait une synthèse de l'état de l'art dans le domaine de la qualité des données en présentant, dans un premier temps, les causes de la non-qualité des données, puis en décrivant un panorama des travaux sur la qualité des données, travaux pertinents dès lors que l'on s'intéresse à modéliser, mesurer et à améliorer la qualité des connaissances "élaborées" à partir des données. Enfin, l'article propose d'exploiter les méta- données décrivant la qualité des données dans le processus d'ECD.	Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000108	http://editions-rnti.fr/render_pdf.php?p=1000108
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Mesure de la qualité des règles d'association par l'intensité d'implication entropique	Le filtrage de l'information pertinente par des mesures de qualité restel'une des étapes les plus délicates d'un processus d'extraction de règlesd'association. Afin de prendre en compte la taille du jeu de données,contrairement à l'indice traditionnel de confiance, et de souligner le caractère naturellement asymétrique de la notion d'implication, Gras a défini l'intensité d'implication qui mesure l'étonnement statistique des règles découvertes.Cependant, comme de nombreuses autres mesures dans la littérature, cette dernière ne tire pas profit de la contraposée (non b)  (non a) qui permet pourtant de renforcer l'affirmation de la relation implicative de a sur b. Nous introduisons ici une extension de l'intensité d'implication qui exploite l'entropie de Shannon pour quantifier les déséquilibres entre exemples et contre-exemples à la fois pour la règle et sa contraposée. Ce nouvel indice est construit de façon à mieux mesurer la notion complexe qu'est la qualité en intégrant simultanément étonnement statistique et qualité inclusive. Des comparaisons numériques avec la confiance et l'indice de Loevinger sont effectuées sur des jeux de données synthétiques et sur des données réelles de domaines variés allant des ressources humaines aux pannes d'ascenseurs. Les distributions statistiques des indices sur les corpus de règles montrent que notre mesure fait preuve d'une forte capacité de filtrage.	Julien Blanchard, Pascale Kuntz, Fabrice Guillet, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000105	http://editions-rnti.fr/render_pdf.php?p=1000105
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Méthodes et mesures d'intérêt pour  l'extraction des règles d'exception	Les systèmes de génération de règles sont en général fondés sur des critères leur permettant de juger de la qualité des règles engendrées. On recherche souvent les règles solides, avec un support et une confiance suffisants, i.e. , concernant une partie important de la population et vérifiées sur un grand nombre d'individus. Cependant, les experts sont souvent plutôt intéressés par des règles qui les surprennent, soit parce qu'elles sont peu fréquentes (de support faible) mais de confiance élevée, soit parce qu'elles représentent des exceptions aux règles solides. Une recherche exhaustive des règles d'exceptions n'est pas envisageable, les stratégies de recherche sont alors tout aussi importantes que les critères de qualité. C'est pourquoi dans ce papier, nous présentons un état de l'art, non seulement des mesures, mais aussi des méthodes pour l'extraction d'exceptions.	Béatrice Duval, Ansaf Salleb-Aouissi, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000109	http://editions-rnti.fr/render_pdf.php?p=1000109
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Qualité d un ensemble de règles : élimination des règles redondantes	La qualité d'un ensemble de règles d'association est souvent considérée, par un utilisateur, selon la compréhension qu'il obtient du domaine étudié, en interprétant les règles qui lui sont présentées. Pour rendre l'ensemble de règles plus lisible, et donc améliorer ce critère de qualité global, nous appliquons aux règles d'association une méthode de réduction initialement proposée pour l'élimination des dépendances fonctionnelles redondantes. Malgré les différences entre les propriétés de ces deux types de relations, cette méthode permet d'obtenir des représentations de règles très concises et facilement interprétables par l'utilisateur.	Henri Briand, Rémi Lehn, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000110	http://editions-rnti.fr/render_pdf.php?p=1000110
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Qualité des règles et des opérateurs en découverte de connaissances floues	Nous décrivons ici l'évaluation de la qualité des règles et celle de laqualité des opérateurs dans un système de découverte de connaissances floues.Nous indiquons d'abord comment il est possible de définir des partitions floues pour remplacer les attributs classiques par des attributs flous. Nous généralisons ensuite à des règles floues trois indices de qualité utilisés pour des règles classiques et nous détaillons un algorithme d'extraction de règles floues.Etant donné que les logiques floues fournissent une infinité d'opérateurslogiques, nous proposons une méthode pour évaluer la qualité d'un jeud'opérateurs flous quand les règles extraites doivent être exploitées dans des systèmes à base de connaissances à l'aide du Modus Ponens Généralisé. Nous appliquons cette évaluation sur un exemple simple et nous exposons quelques résultats de ces méthodes sur des bases de données réelles. Nous indiquons les jeux d'opérateurs flous qui s'avèrent de meilleure qualité et en quoi ces résultats correspondent à la théorie. En rappelant des résultats sur la réduction de règles floues, nous constatons que les jeux d'opérateurs dont la qualité est la meilleure pour utiliser le Modus Ponens Généralisé dans des systèmes à base de connaissances, ne sont pas les mieux adaptés à la réduction des règles.	Maurice Bernadet	http://editions-rnti.fr/render_pdf.php?p1&p=1000111	http://editions-rnti.fr/render_pdf.php?p=1000111
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Qualité d'ajustement d'arbres d'induction	Cet article discute des possibilités de mesurer la qualité del'ajustement d'arbres d'induction aux données comme cela se fait classiquement pour les modèles statistiques. Nous montrons comment adapteraux arbres d'induction les statistiques du khi-2, notamment celle du rapportde vraisemblance utilisée dans le cadre de la modélisation de tablesde contingence. Cette statistique permet de tester l'ajustement du modèle,mais aussi l'amélioration de l'ajustement qu'apporte la complexificationde l'arbre. Nous en déduisons également des formes adaptées des critères d'information AIC et BIC qui permettent de sélectionner le meilleur arbreen termes de compromis entre ajustement et complexité. Nous illustrons la mise en oeuvre pratique des statistiques et indicateurs propos´es avec un exemple réel.	Gilbert Ritschard, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000106	http://editions-rnti.fr/render_pdf.php?p=1000106
Revue des Nouvelles Technologies de l'Information	MQPFD	2004	Quelques critères pour une mesure de qualité de règles d'association	De nombreuses mesures de qualité d'une règle d'association implicative existent mais peu d'entre elles font état des raisons qui ont guidé leurs choix épistémologiques. Nous présentons ici quelques critères qui permettraient de donner un sens aux indices qui servent à quantifier la qualité de l'association. Ensuite, nous développons, en les justifiant, les différents choix que nous avons faits pour mesurer la qualité des implications entre variables de nature binaire ou non binaire, ainsi qu'entre classes ordonnées de variables. Nous confrontons ces choix aux critères énoncés préalablement. Quelques simulations permettent d'illustrer graphiquement la différence de comportement entre certains indices classiques au regard de ces critères.	Régis Gras, Raphaël Couturier, Julien Blanchard, Henri Briand, Pascale Kuntz, Philippe Peter	http://editions-rnti.fr/render_pdf.php?p1&p=1000104	http://editions-rnti.fr/render_pdf.php?p=1000104
Revue des Nouvelles Technologies de l'Information	SFC	2004	Classification et sélection automatique de caractéristiques de textures	Choisir a priori les caractéristiques pertinentes pour une application donnée n'est pas aisé. En particulier, les outils de classification d'images utilisent des modèles variés pour représenter les textures. Nous proposons de choisir les modèles de texture les plus pertinents à l'aide d'une procédure automatique de sélection de caractéristiques. Nous comparons pour cela l'efficacité de plusieurs algorithmes récents en évaluant les performances de différents classificateurs exploitant ces sélections. Nous démontrons l'intérêt d'une telle procédure de sélection à partir des images de Brodatz.	Marine Campedel, Eric Moulines	http://editions-rnti.fr/render_pdf.php?p1&p=1000117	http://editions-rnti.fr/render_pdf.php?p=1000117
Revue des Nouvelles Technologies de l'Information	SFC	2004	Clustering incrémental pour un apprentissage distribué et robuste : vers un système évolutif...	Ce papier présente un système d'apprentissage multiclassifieurs dont la conception est pilotée par la topologie des données d'apprentissage. Il présente un nouvel algorithme d'apprentissage de carte neuronale auto-organisée incrémentale en données. Cette carte est utilisée pour distribuer les tâches de classification sur un ensemble de classifieurs. Seuls certains classifieurs sont activés en fonction de l'activité des neurones de la carte pour une donnée. De plus, le système proposé permet d'établir un critère de confiance s'affranchissant totalement du type de classifieurs utilisés. Ce coefficient permet de contrôler efficacement le compromis Erreur/Rejet. Des résultats comparatifs de cette architecture sont donnés sur la base de segmentation d'images de l'UCI et en reconnaissance de chiffres manuscrits.	Yann Prudent, Abdel Ennaji	http://editions-rnti.fr/render_pdf.php?p1&p=1000125	http://editions-rnti.fr/render_pdf.php?p=1000125
Revue des Nouvelles Technologies de l'Information	SFC	2004	Construction interactive d'arbres de décisions avec des variables mixtes	Nous présentons une extension d'algorithmes de construction interactive d'arbres de décisions aux cas des variables intervalle et taxonomiques. Les algorithmes présentés sont ainsi capables de traiter indifféremment des variables continues, intervalles et taxonomiques (ou un quelconque mélange de ces trois différents types). Ce type d'approche, centrée utilisateur, est ce qui caractérise la fouille visuelle de données. Ces algorithmes peuvent fonctionner en mode 100% manuel (c'est l'utilisateur seul qui créé l'arbre de décision) ou en mode mixte (coopération entre l'utilisateur et une méthode automatique pour trouver la meilleure coupe pour le noeud courant de l'arbre, ici basée sur des SVM : Séparateurs à Vaste Marge). Après avoir décrit l'adaptation de ces algorithmes aux cas des variables intervalles et taxonomiques, nous présentons les résultats que nous avons obtenus sur différents ensembles de données artificiels.	François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000124	http://editions-rnti.fr/render_pdf.php?p=1000124
Revue des Nouvelles Technologies de l'Information	SFC	2004	Détection de faibles homologies de protéines par machines à vecteurs de support	Cet article décrit une approche discriminative pour la recherche de nouveaux membres dans des familles de protéines à faibles homologies de séquences. L'originalité de la méthode repose sur une modélisation de ces familles par un ensemble M de motifs intégrant les propriétés physicochimiques des résidus. Nous proposons un algorithme de découverte de motifs suivant le paradigme de la classification hiérarchique ascendante. L'ensemble M définit un espace de représentation des séquences : chaque séquence est transformée en un vecteur indiquant la présence ou l'absence de chaque motif appartenant à M. Nous utilisons la technique d'apprentissage par machine à vecteurs de support (SVM) pour discriminer la famille d'intérêt vis-à-vis des séquences non apparentées. Cette méthode est testée sur la famille biologique des interleukines dont les membres possèdent des homologies de séquences faibles en dépit d'un repliement tridimensionnel en hélices alpha très conservé. Nous montrons que l'ensemble des motifs hiérarchiques modélise spécifiquement les interleukines par rapport aux autres familles structurales de la base de données SCOP (1.51). Notre classifieur est en effet plus performant sur notre famille de protéines que d'autres méthodes de classification dont le SVM basé sur les spectres de chaîne.	Jérôme Mikolajczak, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1000122	http://editions-rnti.fr/render_pdf.php?p=1000122
Revue des Nouvelles Technologies de l'Information	SFC	2004	Etude du comportement d'exposition et de protection solaire chez des adultes français	Une exposition solaire excessive engendre une accélération du vieillissement et une augmentation du risque de survenue de tumeurs cutanées. Dans le but d'estimer le risque lié à différents types de comportement, une typologie de comportement d'exposition et de protection solaire a été recherchée. Un questionnaire explorant ce sujet auprès d'hommes et de femmes adultes français a été développé dans le cadre de l'étude épidémiologique SU.VI.MAX. Une analyse des correspondances multiples a été effectuée pour résumer l'information. Puis, une classification ascendante hiérarchique (méthode de Ward) a été réalisée à partir des composantes principales retenues. Ensuite, un arbre de décision a été construit afin d'affecter facilement n'importe quel individu à une classe (algorithme CART). Sept types de comportement ont été identifiés pour les femmes, et six pour les hommes. Ces résultats nous permettront de cibler des groupes d'individus à risque pour des campagnes d'information de santé publique et/ou des études d'intervention.	Emmanuelle Mauger, Christiane Guinot, Denis Malvy, Julie Latreille, Laurence Ambroisine, Pilar Galan, Serge Hercberg, Erwin Tschachler	http://editions-rnti.fr/render_pdf.php?p1&p=1000121	http://editions-rnti.fr/render_pdf.php?p=1000121
Revue des Nouvelles Technologies de l'Information	SFC	2004	Evaluation de la pertinence de paramètres biochimiques et classification pour la caractérisation des états physiologiques dans un bio-procédé par la théorie de l'évidence	L'analyse et la modélisation des bio-procédés nécessitent une connaissance profonde des systèmes biologiques. Ces phénomènes biologiques sont particulièrement complexes et ne peuvent être modélisés totalement, même par un système différentiel non linéaire. Le but de ces modélisations mathématiques est la détection des états physiologiques. Il est aussi possible de chercher à détecter les états physiologiques en utilisant les signaux biochimiques mesurés en ligne. Les signaux utilisés pour la classification sont souvent peu nombreux. Nous présentons une méthode de classification des paramètres biochimiques basée sur la théorie de l'évidence. Cette théorie est aussi utilisée pour évaluer la pertinence des paramètres. Cette évaluation est basée sur la notion de conflit. Nous présentons une alternative à la mesure classique de conflit; cette nouvelle mesure du conflit basée sur une distance, fournit des résultats plus cohérents pour l'application présentée. Les premiers résultats concernant l'analyse des états physiologiques d'un procédé biotechnologique de fermentation sont présentés.	Sébastien Régis, Andrei Doncescu, Jean-Pierre Asselin de Beauville, Jacky Desachy	http://editions-rnti.fr/render_pdf.php?p1&p=1000126	http://editions-rnti.fr/render_pdf.php?p=1000126
Revue des Nouvelles Technologies de l'Information	SFC	2004	Extension de l'algorithme Apriori et des règles d'association au cas des données symboliques diagrammes et sélection des meilleures règles par la régression linéaire symbolique	Cet article présente l'extension de l'algorithme Apriori et des règles d'association au cas des données symboliques diagrammes. La méthode proposée nous permet de découvrir des règles au niveau des concepts. Notamment, plutôt que d'extraire des règles entre différents articles appartenant à des mêmes transactions enregistrées dans un magasin comme dans le cas classique, nous extrayons des règles d'association au niveau des clients afin d'étudier leurs comportements d'achat. Enfin, nous proposons une méthode de sélection des meilleures règles d'association selon la régression linéaire symbolique.	Filipe Afonso	http://editions-rnti.fr/render_pdf.php?p1&p=1000115	http://editions-rnti.fr/render_pdf.php?p=1000115
Revue des Nouvelles Technologies de l'Information	SFC	2004	Incorporation de rotations procrustéennes dans une analyse factorielle multiple	Pour comparer deux nuages de points homologues, la méthode de référence est l'analyse procrustéenne et, dans le cas de plus de deux nuages, l'Analyse Procrustéenne Généralisée (APG). L'Analyse Factorielle Multiple (AFM) fournit aussi une représentation superposée de nuages de points homologues. Cette dernière représentation bénéficie, par rapport à celle issue de l'APG, d'avantages (elle s'inscrit dans le cadre d'une analyse factorielle riche en aides à l'interprétation) et d'inconvénients (les nuages à comparer subissent des déformations autres que les seules projections et rotations). Il est possible de compléter l'AFM par un ajustement procrustéen de chacun des nuages initiaux sur le nuage moyen de l'AFM. On obtient ainsi une représentation de ces nuages qui à la fois respecte le modèle procrustéen et s'inscrit dans le cadre de l'AFM. Cette nouvelle représentation est précieuse lorsque les nuages initiaux sont bidimensionnels. Une application dans ce cas particulier est présentée.	Elisabeth Moran, Jérôme Pagès	http://editions-rnti.fr/render_pdf.php?p1&p=1000123	http://editions-rnti.fr/render_pdf.php?p=1000123
Revue des Nouvelles Technologies de l'Information	SFC	2004	Les treillis de Galois pour l'organisation et la gestion des connaissances	Dans cet article, nous étudions la construction de treillis de Galois à partir de différentes sources d'information (par exemple des documents du Web ou des notices bibliographiques) afin d'organiser les connaissances qui peuvent en être extraites. L'organisation en treillis qui en résulte peut alors être utilisée pour satisfaire un certain nombre de buts, comme par exemple la gestion de la connaissance dans une organisation, la recherche documentaire sur le Web, etc. En outre, la classification par treillis peut également tirer parti d'une ontologie des propriétés du domaine considéré. Notre objectif global est de mettre en place un processus de classification par treillis pour enrichir une ontologie qui à son tour permet de guider le processus de découverte de connaissances dans les données.	Laszlo Szathmary, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000127	http://editions-rnti.fr/render_pdf.php?p=1000127
Revue des Nouvelles Technologies de l'Information	SFC	2004	Sélection de variables et agrégation d'opinions	La taille des bases de données étant de plus en plus importante, le processus de sélection de variables devient essentiel. Nous proposons une méthode de sélection, pour les variables qualitatives, basée sur l'agrégation d'opinion. Le résultat, sous forme d'un préordre de variables, est fourni par l'agrégation des résultats obtenus par plusieurs méthodes myopes de sélection de variables.	Gaëlle Legrand, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000120	http://editions-rnti.fr/render_pdf.php?p=1000120
Revue des Nouvelles Technologies de l'Information	SFC	2004	Significativité des niveaux d'une hiérarchie orientée en analyse statistique implicative	Dans le cadre de l'analyse statistique implicative développée à l'origine par R. Gras, nous avons proposé le modèle de "hiérarchie orientée" pour structurer des règles partielles de type a&#8594;b et des règles de règles, appelées R-règles, issues d'un corpus de données décrites par des attributs binaires. Dans cet article, nous proposons un nouveau critère de significativité des niveaux de la hiérarchie orientée basé sur des comparaisons de préordres. Une application à un questionnaire d'opinions illustre l'intérêt de la démarche.	Régis Gras, Pascale Kuntz, Jean-Claude Régnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000118	http://editions-rnti.fr/render_pdf.php?p=1000118
Revue des Nouvelles Technologies de l'Information	SFC	2004	Un survol des algorithmes biomimétiques pour la classification	Nous présentons dans cet article un survol des algorithmes et méthodes biomimétiques pour résoudre le problème de la classification. Nous décrivons les approches utilisant les algorithmes génétiques et révolutionnaires avec les différents codages et représentations ayant été utilisés. Nous abordons l'approche à base de fourmis artificielles qui se trouve être une riche source d'inspiration pour la classification. Nous détaillons finalement d'autres approches à base d'agents avec notamment l'intelligence en essaim (nuages d'agents) et avec les systèmes immunitaires. Enfin, nous résumons les ressemblances et différences des travaux présentés et nous concluons sur les perspectives liées à l'approche biomimétique pour la classification.	Hanane Azzag, Fabien Picarougne, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000116	http://editions-rnti.fr/render_pdf.php?p=1000116
Revue des Nouvelles Technologies de l'Information	SFC	2004	Un survol des algorithmes évolutionnaires dans la fouille de données	Cet article a pour objectif la présentation des travaux récents dans le domaine de la fouille de données basés sur l'évolution génétique.	Fatima-Zohra Kettaf, Jean-Pierre Asselin de Beauville	http://editions-rnti.fr/render_pdf.php?p1&p=1000119	http://editions-rnti.fr/render_pdf.php?p=1000119
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2003	Calcul des coefficients de régression et du PRESS en régression PLS1.	Lors de l'implémentation sous Splus de l'algorithme de régression PLS1, nous avons utilisé une formule de récurrence simple pour le calcul des coefficients de régression et précisé certains choix nécessaires pour le calcul du PRESS. Nous rappelons dans un premier temps l'algorithme PLS1 afin d'introduire les notations, puis nous donnons la formule de récurrence permettant d'implémenter le calcul des coefficients de régression. Enfin, nous montrons dans une dernière partie les différentes alternatives possibles pour le calcul du PRESS, et la solution adoptée. Cette solution permet en effet de retrouver certains résultats numériques présentés dans la littérature et obtenus avec le logiciel SIMCA-P. Les fonctions Splus suivantes sont disponibles auprès des auteurs : PLS1(X,y,H) pour le modèle à H composantes et PLS1cv(X,y) pour le modèle avec choix du nombre de composantes par validation croisée.	Marie Chavent, Brigitte Patouille	http://editions-rnti.fr/render_pdf.php?p1&p=1001658	http://editions-rnti.fr/render_pdf.php?p=1001658
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2003	Le logiciel EXTREMES, un outil pour l'étude des queues de distribution	Le logiciel EXTREMES regroupe différents outils pour la modélisation des queues de distribution et l'estimation des quantiles extrêmes. On y trouve en particulier les procédures classiques d'estimation des paramètres des lois décrivant le comportement des valeurs extrêmes, mais aussi des procédures plus complexes de test d'adéquation pour la queue de distribution. Des fonctions d'inférence statistique classique sont aussi implémentées, permettant ainsi la comparaison de modèles paramétriques centraux avec des modèles semi paramétriques extrêmes. Le logiciel est écrit en C++, comporte une interface graphique développée en Matlab et une documentation technique. Le programme est disponible auprès des auteurs.	Jean Diebolt, Jérôme Ecarnot, Myriam Garrido, Stéphane Girard, Dominique Lagrange	http://editions-rnti.fr/render_pdf.php?p1&p=1001664	http://editions-rnti.fr/render_pdf.php?p=1001664
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2003	Théorie du sondage aléatoire et étude d'un sondage d'opinion avant le 1er tour d'une élection présidentielle.	Après le dépouillement d'un échantillon aléatoire, que sait-on sur l'ensemble de la population ? Une solution mathématique à ce problème est proposée sous forme de la densité de probabilité quelle que soit la dimension du problème (nombre de candidats) et quelle que soit la taille du sondage (taille de la population et de l'échantillon). Cette solution amène à une méthode qui permet : une maîtrise des marges d'erreurs statistiques quelle que soit la taille du sondage ; une visualisation des résultats sous la forme des projections ; le calcul de la probabilité pour n'importe quel événement multidimensionnel.A partir d'un sondage des intentions de vote avant le 1er tour d'une élection présidentielle en France il convient ainsi de calculer pour chaque candidat : la probabilité d'être élu dès le 1er tour ; la probabilité d'être retenu pour le 2ème tour ; la probabilité d'échec (être ni élu dès le 1er tour ni retenu pour le 2ème tour).	Martin Körnig	http://editions-rnti.fr/render_pdf.php?p1&p=1001662	http://editions-rnti.fr/render_pdf.php?p=1001662
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2003	Une introduction à l'analyse discriminante avec SPSS pour Windows	Cette note initie l'utilisateur débutant à la mise en oeuvre de l'analyse discriminante par l'intermédiaire de la procédure DISCRIM du logiciel SPSS pour Windows. Cette mise en oeuvre concerne le classement d'individus caractérisés par des variables quantitatives, les affectant à des groupes a priori au moyen de fonctions discriminantes. Les sorties du logiciel sont commentées par la présentation du formulaire de l'analyse discriminante associé à chacun des résultats obtenus.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001663	http://editions-rnti.fr/render_pdf.php?p=1001663
Revue des Nouvelles Technologies de l'Information	EFD	2003	Algorithme AntTree : classification non supervisée par des fourmis artificielles	Nous présentons dans cet article un nouvel algorithme de classification non supervisée. Il utilise le principe d'auto assemblage observé chez une colonie de fourmis où des fourmis se fixent progressivement à un support fixe puis successivement aux fourmis déjà fixées afin de construire des structures vivantes.Les fourmis artificielles que nous définissons vont de manière similaire construire un arbre. Chaque fourmi représente une donnée. Les déplacements et les assemblages de fourmis sur cet arbre dépendent de la similarité entre les données. Nous comparons cet algorithme aux k-means et à l'algorithme AntClass sur des bases de données numériques artificielles et réelles et sur des données du C.E.R.I.E.S. Nous montrons que AntTree apporte des améliorations significatives au problème posé.	Hanane Azzag, Nicolas Monmarché, Mohamed Slimane, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000085	http://editions-rnti.fr/render_pdf.php?p=1000085
Revue des Nouvelles Technologies de l'Information	EFD	2003	Analyse d'images multispectrales par diagonalisation simultanée de matrices de covariances	L'analyse conjointe d'un ensemble d'images multispectrales peutêtre présentée comme un problème de diagonalisation simultanée des matricesde variances associées. L'application d'un algorithme de décompositionsimultanée d'un ensemble de matrices définies positives permet l'extractiondes structures communes à une collection d'images multispectrales d'objetsdifférents. La méthode est illustrée par l'analyse d'une séquence d'images degrains de céréales. Des images de microscopie en fluorescence de cinq variétésde blé et d'orge ont été acquises dans 19 conditions spectrales. L'analysepermet de mettre en relief les comportements communs de fluorescence.	Philippe Courcoux, Marie-Françoise Devaux, Michel Séménou	http://editions-rnti.fr/render_pdf.php?p1&p=1000100	http://editions-rnti.fr/render_pdf.php?p=1000100
Revue des Nouvelles Technologies de l'Information	EFD	2003	Analyse statistique de similarité dans une collection d'images	Dans le cadre du développement d'un système de recherche d'imagespar le contenu, nous avons défini deux nouvelles mesures : la distance dedissimilitude DS*, et la distance de similarité E. La seconde est intégrée à laformule de la distribution de Gibbs et de celle de la mixture de Dirichletgénéralisée, alors que la première est comparée à trois autres variantesd'estimation de la similarité : la distance Euclidienne ainsi que les distributionsde Gibbs et Dirichlet intégrant la distance de similarité E. L'analyse empiriquedes quatre mesures de similarité porte sur les histogrammes de couleurs d'unecollection d'images et montre que l'efficacité de la recherche, mesurée par lerappel et la précision, est la plus importante pour la distribution de Dirichletmodifiée et la plus faible pour la distance Euclidienne.	Madenda Sarifuddin, Rokia Missaoui, Jean Vaillancourt, Youssef Hamouda, Marek Zaremba	http://editions-rnti.fr/render_pdf.php?p1&p=1000099	http://editions-rnti.fr/render_pdf.php?p=1000099
Revue des Nouvelles Technologies de l'Information	EFD	2003	Application du Data Mining prédictif aux bases EDF	L'objet de l'article est de montrer comment l'utilisation detechniques prédictives de Data Mining améliorent la qualité de l'informationutilisée en marketing opérationnel. Cette étude s'inscrit dans la continuité desméthodologies utilisées à EDF pour enrichir les bases clientèles à partir demodèles prédictifs construits sur des variables internes. Ces modèles sontgénéralement construits à partir de variables explicatives issues des bases dedonnées clients. Afin d'améliorer la qualité des modèles, nous prenons encompte des données externes agrégées. Nous présentons dans cet articlecomment utiliser conjointement des variables explicatives issues des bases dedonnées clients et l'information supplémentaire issue du recensement de lapopulation de 1999. La méthodologie a été expérimentée sur des bases clientsEDF dans le cadre de campagnes de marketing opérationnel.	Christian Derquenne, Sabine Goutier, Sylvia Lembo, Véronique Stéphan	http://editions-rnti.fr/render_pdf.php?p1&p=1000083	http://editions-rnti.fr/render_pdf.php?p=1000083
Revue des Nouvelles Technologies de l'Information	EFD	2003	Apport des techniques de Text Mining pour la définition de caractéristiques clefs d'une mammographie	Dans ce papier, nous exprimons les motivations et la démarche nous ayant conduit à utiliser un corpus de compte-rendus de mammographies élaborés par des radiologues afin de déterminer un ensemble de caractéristiques fréquentes et discriminantes d'une mammographie permettant de prédire la malignité ou la bénignité d'un cas. Pour ce faire, nous avons utilisé des techniques de Data Mining et plus particulièrement de Text Mining pour traiter ce corpus afin d'en extraire dans un premier temps une liste d'une centaine de termes discriminants. Dans un second temps, nous avons mis en oeuvre 3 techniques d'apprentissage dans le but de vérifier la pertinence des termes extraits ainsi que de déterminer les termes les plus importants dans les modèles élaborés. Ces derniers ont un taux d'erreur de l'ordre de 21%, mais l'utilisation d'un méta apprentissage définissant la manière de faire coopérer les classifieurs permet d'obtenir un taux d'erreur de 17% ainsi qu'une amélioration significative des taux de sensibilités et spécificités. Nos résultats montrent l'existence d'un riche contenu informationnel au sein des compte-rendus pouvant être pris en compte par nos méthodes nécessitant cependant des travaux complémentaires.	Jérémy Clech, Djamel Abdelkader Zighed, A. Brémond	http://editions-rnti.fr/render_pdf.php?p1&p=1000094	http://editions-rnti.fr/render_pdf.php?p=1000094
Revue des Nouvelles Technologies de l'Information	EFD	2003	Classification non supervisée pour données catégorielles	La classification non supervisée (CNS) constitue l'une des problèmatiques centrales de l'Extraction de Connaissances à partir de Données (E.C.D.). Le cadre spécifique de la CNS pour données catégorielles a été l'objet de multiples travaux ces dernières années, les principaux challenges associés à ses recherches sont d'une part la définition de critères bien adaptés à ce cadre particulier et d'autre part la mise au point d'algorithme au coût calculatoire relativement faible. Le propos de cet article n'est pas de poursuivre dans ces directions de recherche mais plutôt de s'appuyer sur ces travaux afin de proposer une méthode efficace exhibant de nombreux avantages pour l'utilisateur et utilisable par un non spécialiste. Nous proposons et évaluons donc une méthode de CNS pour données catégorielles permettant la mise à jour relativement rapide d'une classification pertinente d'un ensemble d'objets, tout en facilitant la tâche de l'utilisateur : aucun paramètre obscur ni nombre final de classes à fixer, description compréhensible de la classification, possibilité d'intervention de l'utilisateur dans le processus de CNS...	Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000086	http://editions-rnti.fr/render_pdf.php?p=1000086
Revue des Nouvelles Technologies de l'Information	EFD	2003	Critères d'évaluation des mesures de qualité des règles d'association	Les algorithmes de fouille de données, en particulier dans le cadre de l'apprentissage non supervisé, génèrent un grand nombre de règles. Il est donc concrètement impossible de procéder à une validation de ces règles en les présentant à un expert du domaine. Afin d'assister ce traitement, de nombreuses mesures de qualité de règles ont été proposées pour sélectionner et ordonner automatiquement les règles. Se pose alors le problème du choix de la mesure adaptée aux données et aux besoins de l'expert, celui-ci n'étant pas à priori, expert en fouille de données. Dans le contexte des règles d'association, nous proposons une caractérisation des mesures en fonctions de propriétés ayant une sémantique intuitive nous permettant de mettre en oeuvre un processus d'aide à la décision afin d'assister l'expert dans son choix d'une mesure adaptée à ses besoins et ainsi l'aider à sélectionner les meilleures règles.	Philippe Lenca, Patrick Meyer, Philippe Picouet, Benoît Vaillant, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000089	http://editions-rnti.fr/render_pdf.php?p=1000089
Revue des Nouvelles Technologies de l'Information	EFD	2003	Des règles d'association ordinales aux règles d'association classique.	L'intensité d'inclination est une mesure qui permet d&#8242;extraire des règles directement sur les données ordinales sans avoir à les transformer par un codage disjonctif complet précédé d&#8242;une discrétisation pour les variables quantitatives. Ce nouveau type de règles, les règles d&#8242;association ordinales, dégagent les comportements généraux de la population et il est essentiel d'aller au plus près des individus en découvrant des règles spécifiques c'est-à-dire des règles vérifiées par des sous-ensembles d'individus. Cet article présente donc la technique pour extraire des règles d'association en partant des règles d'association ordinales, se libérant ainsi de l'étape de transformation des données et surtout en obtenant une discrétisation des variables quantitatives fonction du contexte c'est-à-dire fonction des variables auxquelles elles sont associées. L'étude se termine par une évaluation sur une base de données bancaires.	Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1000091	http://editions-rnti.fr/render_pdf.php?p=1000091
Revue des Nouvelles Technologies de l'Information	EFD	2003	Des textes aux associations entre les concepts qu'ils contiennent	Nous présentons dans cet article, une chaîne originale d'outils allant de l'acquisition du corpus à l'extraction d'information. Ces outils permettent de faciliter le travail de l'expert en automatisant une partie des traitements. Nous étudions l'automatisation d'une étape clef préalable à la construction d'une ontologie terminologique, à savoir l'acquisition des termes pertinents qui constitueront les noeuds de l'ontologie. Nous avons obtenu la terminologie complète de quatre corpus différents par la langue et par la taille. La validation de ces terminologies par des experts montre que notre méthode fournit un très grand nombre de termes de qualité satisfaisante. Des classes de concepts ont été construites avec ces termes de façon semi-automatique. Celles-ci nous permettent de représenter chaque corpus sous une forme plus compacte, à partir desquelles un processus d'extraction de règles d'association peut être appliqué. Nous avons validé les  les d'association obtenues en comparant nos résultats avec ceux d'une amélioration récente de l'Intensité d'Implication sur trois corpus. Deux de ces corpus sont issus de données réelles et un expert du domaine a discuté l'intérêt des règles obtenues avec les deux mesures.	Oriane Matte-Tailliez, Yves Kodratoff, Jérôme Azé, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000093	http://editions-rnti.fr/render_pdf.php?p=1000093
Revue des Nouvelles Technologies de l'Information	EFD	2003	Evaluation de la resistance au bruit de quelques Mesures Quantitatives	L'extraction de connaissance dans des données réelles est difficile car les données sont rarement parfaites. L'étude de l'impact du bruit contenu dans les données sur la qualité des résultats obtenus permet de mieux comprendre le comportement des mesures de qualité. Dans cet article nous présentons différentes mesures quantitatives permettant d'extraire des connaissances dans les données. Pour chacune de ces mesures, une étude empirique de l'impact d'un bruit relativement réaliste sur une base de données bancaire est réalisée. Le comportement des différentes mesures en présence des données bruitées permet d'établir un critère de qualité supplémentaire. Cen nouveau critère  lié à la sensibilité des mesures aux données bruitées permet de mieux contrôler le choix des mesures lors du processus d'extraction des connaissances.	Jérôme Azé, Sylvie Guillaume, Philippe Castagliola	http://editions-rnti.fr/render_pdf.php?p1&p=1000092	http://editions-rnti.fr/render_pdf.php?p=1000092
Revue des Nouvelles Technologies de l'Information	EFD	2003	Extension du modèle M3 aux évolutions temporelles dans les applications SOLAP	La plupart des modèles multidimensionnels considèrent les faitscomme la partie dynamique de l'entrepôt tandis que les dimensions sont vuescomme des entités statiques. Dans les applications pratiques, les structuresd'analyse évoluent avec le temps. Cette problématique devient particulièrementsensible et complexe dans le cadre d'applications SOLAP (Spatial On LineAnalytical Processing). En effet, la dimension spatiale lorsqu'elle décrit undécoupage territorial est sujette à modification et ces modifications ont uneénorme incidence sur les mesures. Dans cet article nous montrons comment lemodèle M3 que nous avons proposé permet de construire un hypercubeconservant l'ensemble des évolutions et offre une navigation et uneexploitation comparative des données dans les différentes versions desdimensions. Notre proposition est illustrée dans le domaine de la foresterie.	Maryvonne Miquel,  Anne Tchounikine	http://editions-rnti.fr/render_pdf.php?p1&p=1000081	http://editions-rnti.fr/render_pdf.php?p=1000081
Revue des Nouvelles Technologies de l'Information	EFD	2003	Fusion numérique d'informations multi sources et extraction de connaissances	La fusion de données permet d'enrichir une série de donnéesparcellaires, souvent issues de sources multiples, en combinant lesinformations qu'elles contiennent afin d'améliorer la qualité des connaissancesextraites de ces données. L'un des objectifs de la fusion de données multisources,ainsi définie, est l'extraction de connaissances (KDD).Dans cet article, plusieurs schémas de fusion de données sont proposés,exploitant à la fois la complémentarité et la redondance de ces informationsmuti-sources. Ces schémas sont ensuite mis en oeuvre pour l'élaboration d'unindicateur de la qualité de la circulation du trafic (temps de parcours) sur labase de données issues de capteurs de trafic traditionnels et de véhiculestraceurs (véhicules équipés de capteurs embarqués).	Nour-Eddin El Faouzi	http://editions-rnti.fr/render_pdf.php?p1&p=1000084	http://editions-rnti.fr/render_pdf.php?p=1000084
Revue des Nouvelles Technologies de l'Information	EFD	2003	Identification de Données Partiellement Occultées en RdF Statistique.	Dans cet article nous nous intéressons au problème de modélisationde la variabilité de formes et de points dans le cadre de la reconnaissancede formes statistique. Nous proposons un modèle statistiquenon linéaire appris sur un ensemble ordonné de points. Le formalisme de l'Analyse en Composantes Principales composé avec notre modèle nous permettent de résoudre le problème d'identification de données partiellement occultées. Cette étude s'applique au problème de repérage de points céphalométriques sur des radiographies de crâne de jeunes enfants.	Barbara Romaniuk, Michel Desvignes, Marinette Revenu, Marie-Josèphe Deshayes	http://editions-rnti.fr/render_pdf.php?p1&p=1000101	http://editions-rnti.fr/render_pdf.php?p=1000101
Revue des Nouvelles Technologies de l'Information	EFD	2003	JEN, un algorithme efficace de construction de générateurs pour l'identification des règles d'association	L'article décrit un algorithme, appelé JEN, d'identification efficacedes générateurs à partir du treillis de concepts (Galois) pour l'extraction desrègles d'association. Cette dernière est immédiate dans notre approche : lesrègles exactes sont obtenues à partir des concepts individuels en exploitantleurs générateurs et l'itemset fermé correspondant, tandis que les règlesapproximatives sont identifiées en consultant les générateurs d'un concept etl'itemset fermé des prédécesseurs immédiats de ce concept. Une analysecomparative empirique illustre la supériorité de JEN sur trois autres procéduresde génération de règles et de générateurs, particulièrement lors de l'analyse dedonnées fortement corrélées.	Amélie Le Floc'h, Christian Fisette, Rokia Missaoui, Robert Gourdin, Petko Valtchev	http://editions-rnti.fr/render_pdf.php?p1&p=1000090	http://editions-rnti.fr/render_pdf.php?p=1000090
Revue des Nouvelles Technologies de l'Information	EFD	2003	L'induction de graphes dans l'étude du complexe Mycobacterium tuberculosis	Du fait de l'évolution parallèle des méthodes d'identification moléculaire des génomes mycobactériens et de leur structure, la reconnaissance et la détection des gènes deviennent des enjeux majeurs dans le domaine de la santé publique. Minimiser les coûts et le nombre de tests d'analyse et d'identification par des techniques assurant un résultat satisfaisant s'apparente en informatique à la recherche d'attributs pertinents capables de discriminer efficacement les individus au sein de la structure étudiée. Dans cet article nous axerons notre recherche sur l'étude de la contribution de l'induction supervisée dans le classement de données issues de la tuberculose par une approche exploitant conjointement spoligotypes et MIRU-VNTR.	Georges Valétudie, Séverine Ferdinand, Nalin Rastogi, Christophe Sola	http://editions-rnti.fr/render_pdf.php?p1&p=1000096	http://editions-rnti.fr/render_pdf.php?p=1000096
Revue des Nouvelles Technologies de l'Information	EFD	2003	L'ADN en tant que texte : style et syntaxe	L'ADN peut être vu comme un texte dont la signification précisereste encore assez mystérieuse. On sait cependant que les fréquencesd'utilisation des mots est spécifique à chacune des espèces (signaturegénomique). Nous avons montré que la signature génomique résulte d'un styled'écriture que l'on retrouve le long du génome. Bien que les signatures desespèces soient différentes, on observe qu'il existe cependant un consensusentre les espèces sur l'utilisation des mots. En effet, ce sont les mêmes motsqui sont les plus ou les moins variables le long du génome chez les différentesespèces. Certains de ces mots, comme les palindromes par exemple, ont despropriétés fréquentielles originales.	Sylvain Lespinats, Patrick Deschavanne, Alain Giron, Bernard Fertil	http://editions-rnti.fr/render_pdf.php?p1&p=1000095	http://editions-rnti.fr/render_pdf.php?p=1000095
Revue des Nouvelles Technologies de l'Information	EFD	2003	Ondelettes et télédetéctions	Le but de cet article est de mettre en évidence l'apport des ondelettes en télédétection. Dans un premier temps, en classification supervisée, la méthode behavioriste de J.P.Rasson (Orban et al. 1998) suppose que les valeurs des pixels sont distribuées selon un processus de Poisson (p.p.) sur des supports convexes et mène à un critère intuitif d'analyse discriminante. Nous analysons comment les ondelettes peuvent améliorer cette technique au moyen d'une estimation de l'intensité du p.p. via les ondelettes au lieu des noyaux. Nos résultats montrent que l'estimation de l'intensité des par ondelettes améliore le taux global de pixels correctement classés et diminue le temps de calcul. Dans un second temps, en classification non-supervisée, C. Van Huffel (Van Huffel 1999) a analysé la puissance de l'algorithme de segmentation hiérarchique de J.M. Beaulieu (Beaulieu 1991). Nous améliorons légèrement la segmentation en introduisant un nouveau critère basé sur l'estimation de la densité par ondelettes.	Catherine Charles, Jean Paul Rasson	http://editions-rnti.fr/render_pdf.php?p1&p=1000097	http://editions-rnti.fr/render_pdf.php?p=1000097
Revue des Nouvelles Technologies de l'Information	EFD	2003	Partition BIC optimale de l'espace des prédicteurs	Cet article traite du partitionnement optimal de l'espace deprédicteurs catégoriels dans le but de prédire la distribution a posteriorid'une variable réponse elle-même catégorielle. Cette partition optimaledoit répondre à un double critère d'ajustement et de simplicité queprennent précisément en compte les critères d'information d'Akaike (AIC)ou bayésien (BIC). Après avoir montré comment ces critères s'appliquentdans notre contexte, on s'intéresse à la recherche de la partition qui minimisele critère retenu. L'article propose une heuristique rudimentaireet démontre son efficacité par une série de simulations qui comparent le quasi optimum trouvé au vrai optimum. Plus que pour la partition elle même, la connaissance de cet optimum s'avère précieuse pour juger dupotentiel d'amélioration d'une partition, notamment celle fournie par unalgorithme d'induction d'arbre. Un exemple sur données réelles illustre ce dernier point.	Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1000087	http://editions-rnti.fr/render_pdf.php?p=1000087
Revue des Nouvelles Technologies de l'Information	EFD	2003	SMAIDoC	L'expansion du World Wide Web et la multiplication des sources de données conduisent à la prolifération de données hétérogènes (textes, images, vidéos, sons, bases de données) appelées données complexes. Pour explorer ces données, il est nécessaire de procéder à leur intégration dans un format unifié. Cette intégration présente des difficultés de collecte, de structuration et de stockage des données. Plusieurs approches sont apparues dans la littérature pour l'intégration de données, parmi lesquelles celle des médiateurs-wrappers ou de l'entreposage. Dans cet article, nous proposons une nouvelle démarche d'intégration de données complexes qui repose à la fois sur l'approche classique d'entreposage de données et sur l'utilisation des systèmes multi-agents. Cette nouvelle approche se base sur une architecture évolutive grâce à la technologie "agent". Les différentes étapes de la phase d'intégration sont alors considérées comme des tâches assimilées à des services, gérées par des acteurs assimilés à des agents. Pour valider cette approche, nous avons développé le "Système Multi- Agent pour l'Intégration de Données Complexes", SMAIDoC.	Frederic Clerc, Amandine Duffoux, Christian Rose, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000080	http://editions-rnti.fr/render_pdf.php?p=1000080
Revue des Nouvelles Technologies de l'Information	EFD	2003	Traitement de données volumineuses par ensembles d'arbres aléatoires	Cet article présente une nouvelle méthode d'apprentissage basée sur un ensemble d'arbres de décision. Par opposition à la méthode traditionnelle d'induction, les arbres de l'ensemble sont construits en choisissant les tests durant le développement de manière complètement aléatoire. Cette méthode est comparée aux arbres de décision et au bagging sur plusieurs problèmes de classification. Grâce aux choix aléatoire des tests, les temps de calcul de cet algorithme sont comparables à ceux des arbres traditionnels. Dans le même temps, la méthode se révèle beaucoup plus précise que les arbres et souvent significativement meilleure que le bagging. ces caractéristiques rendent cette méthode particulièrement adaptée pour le traitement de bases de données volumineuses.	Pierre Geurts	http://editions-rnti.fr/render_pdf.php?p1&p=1000088	http://editions-rnti.fr/render_pdf.php?p=1000088
Revue des Nouvelles Technologies de l'Information	EFD	2003	Une approche unifiée pour discriminer les tumeurs noires de la peau	Nous avons développé une méthodologie de traitement d'images detumeurs noires fondée exclusivement sur l'utilisation de méthodes adaptatives à partir d'exemples documentés par les dermatologues.Pour cela cinq dermatologues ont analysé indépendamment 600 images(cliniques et dermatoscopiques) en localisant les signes de malignité. Les réponses obtenues et le consensus dégagé ont servi de base pour les étapes de l'apprentissage supervisé. C'est ainsi qu'il a été possible de segmenter les lésions et de reconnaître certaines caractéristiques, intéressant directement le diagnostic, telles que la multiplicité de couleurs, la présence de réseaupigmenté ou l'irrégularité des bords de la tumeur, avec une même approche.	Camille Serruys, Alain Giron, Romain Viard, Raoul Triller, Bernard Fertil	http://editions-rnti.fr/render_pdf.php?p1&p=1000102	http://editions-rnti.fr/render_pdf.php?p=1000102
Revue des Nouvelles Technologies de l'Information	EFD	2003	Une méthode générique pour la classification automatique d'images à partir des pixels	Dans cet article nous évaluons un approche générique de classification automatique d'images. Elle repose sur une méthode d'apprentissage récente qui construit des ensembles d'arbres de décision par sélection aléatoires des tests directement sur les valeurs basiques des pixels. Nous proposons une variante, également générique, qui réalise une augmentation fictive de la taille des échantillons par extraction et classification de sous-fenêtres des images. Ces deux approches sont évaluées et comparées sur quatre bases de données publiques de problèmes courants : la reconnaissance de chiffres manuscrits (MNIST), de visages (ORL), d'objets 3D (COIL-20) et de textures (OUTEX)	Raphaël Marée, Pierre Geurts, Louis Wehenkel	http://editions-rnti.fr/render_pdf.php?p1&p=1000098	http://editions-rnti.fr/render_pdf.php?p=1000098
Revue des Nouvelles Technologies de l'Information	EFD	2003	Utilisation de la théorie des sondages dans le cadre des OLAP	Dans le cadre de la théorie des sondages, le traitement de la non-réponse a donné lieu à différentes méthodologies reposant principalement sur la pondération ou sur l'imputation. L'objet de cet article est de montrer comment ce cadre formel statistique s'adapte naturellement au problème des valeurs manquantes dans le contexte des OLAP. Dans cette étude, nous nous limitons au cas des valeurs manquantes dans les dimensions, appelées alors dimensions creuses. La méthode d'ajustement est réalisée en intégrant un système de poids au sein du cube. La complexité algorithmique est fortement diminuée par la recherche d'un ensemble de systèmes de pondération minimum. Celle-ci, appelée méthode ROWN, est synthétisée et une validation expérimentale de l'évolution des estimations en fonction du support est présentée. Enfin, l'implémentation sous ORACLE EXPRESS est détaillée.	Sabine Goutier, Véronique Stéphan	http://editions-rnti.fr/render_pdf.php?p1&p=1000082	http://editions-rnti.fr/render_pdf.php?p=1000082
Revue des Nouvelles Technologies de l'Information	EFD	2003	Vers l'auto-administration des entrepôts de données	Avec le développement des bases de données en général et des entrepôts de données (data warehouses) en particulier, il est devenu primordial de réduire la fonction d'administration de base de données. L'idée d'utiliser des techniques de fouille de données (data mining) pour extraire des connaissances utiles des données elles-mêmes pour leur administration est avancée depuis quelques années. Pourtant, peu de travaux de recherche ont été entrepris dans ce domaine. L'objectif de cette étude est de rechercher une fa¸con d'extraire, à partir des données stockées, des connaissances utilisables pour appliquer de manière automatique des techniques d'optimisation des performances, et plus particulièrement d'indexation. Nous avons réalisé un outil qui effectue une recherche de motifs fréquents sur une charge donnée afin de calculer une configuration d'index permettant d'optimiser le temps d'accès aux données. Les expérimentations que nous avons menées ont montré que les configurations d'index générées par notre outil permettent des gains de performance de l'ordre de 15% à 25% sur une base et un entrepôt de données tests.	Kamel Aouiche, Jérôme Darmont, Le Gruenwald	http://editions-rnti.fr/render_pdf.php?p1&p=1000079	http://editions-rnti.fr/render_pdf.php?p=1000079
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2002	La recherche des harmoniques, une nouvelle fonction du logiciel CORICO	On présente une nouvelle manière de décomposer une série en ses harmoniques, avec prise en compte des ruptures de tendance éventuelles et des points atypiques. La méthode fondée sur les corrélations partielles et non sur les moyennes mobiles ne requiert pas une cadence régulière d'échantillonnage. Elle est validée sur plusieurs cas d'école, puis appliquée à une étoile.	Michel Lesty	http://editions-rnti.fr/render_pdf.php?p1&p=1001660	http://editions-rnti.fr/render_pdf.php?p=1001660
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2002	Méthodes de régression semiparamétriques de type 		Ali Gannoun, Christiane Guinot, Jérôme Saracco	http://editions-rnti.fr/render_pdf.php?p1&p=1001659	http://editions-rnti.fr/render_pdf.php?p=1001659
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2002	Un problème de fiabilité		Edith Kosmanek, Jean Moreau	http://editions-rnti.fr/render_pdf.php?p1&p=1001661	http://editions-rnti.fr/render_pdf.php?p=1001661
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Analyse exploratoire d'une suite de tableaux de données indicés par le temps	Dans le cadre de l'analyse exploratoire des tableaux multiples, nous nous intéressons dans cet article à une succession de tableaux "individus x variables" indicés par le temps Le théorème d'Eckart et Young ([LEB99J) sur lequel reposent les méthodes factorielles n'admet pas de généralisation en ce sens qu'il n'existe pas de décomposition optimale unique d'un tableau à trois entrées en tableaux de rangs 1. Plusieurs méthodologies de traitement vont être présentées et comparées à l'aide d'un exemple, notamment pour le suivi (construction des trajectoires) des individus, des modalités et des variables.	Catherine Pardoux	http://editions-rnti.fr/render_pdf.php?p1&p=1001647	http://editions-rnti.fr/render_pdf.php?p=1001647
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Analyses discriminantes régularisées vis la régression PLS et l'analyse en résultantes covariantes	Ce travail part du fait que l'Analyse Factorielle Discriminante (AFD) est un cas particulier de deux familles de méthodes: l'une comprenant l'Analyse Canonique (AC) et la régression PLS, et l'autre étant celle de l'Analyse en Résultantes Covariantes (ARC). Ces méthodes, en élargissant le cadre instrumental de l'Analyse Discriminante, permettent d'obtenir des facteurs discriminants privilégiant les structures fortes présentées par les variables explicatives, et par conséquent plus robustes et d'interprétation plus riche. On montre qu'à l'instar de l'AFD, les adaptations de PLS et de l'ARC à l'Analyse Discriminante peuvent être considérées comme des Analyses en Composantes Principales du nuage des centres de classes, avec une métrique et un système de poids particuliers pour chaque méthode. Le formalisme élargi fourni par chaque famille de méthodes permet en outre d'étendre l'AFD à la discrimination de plusieurs variables nominales, en tenant compte des liaisons que celles-ci présentent. On obtient alors une Analyse Multi-Discriminante.	Xavier Bry	http://editions-rnti.fr/render_pdf.php?p1&p=1001653	http://editions-rnti.fr/render_pdf.php?p=1001653
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Classification des zones de distribution postale à partir d'historiques de flux quotidiens de courrier	L'objet de cette étude est de montrer comment, dans une phase exploratoire de données, nous avons pu construire une classification de zones de distribution postale à partir d'historiques de flux quotidien de courrier par type d'objets distribués. Cette classification sera confrontée dans l'avenir avec des données externes décrivant nos clients récepteurs (données socio-économiques, démographiques etc. mais aussi des données de production interne) afin de mettre en place une modélisation des pointes de fort trafic d'objets à traiter dans un bureau de Poste distributeur. Nous présenterons successivement le cadre général dans lequel l'étude est intégrée, les données ainsi que les concepts généraux permettant de mieux appréhender et comprendre notre démarche, et enfin la manière d'envisager le problème d'une classification de courbes de trafics hebdomadaires. Les questions surgies durant l'exploitation des données que nous avons travaillées seront abordées, puis les éléments de réponse à ces questions, particulièrement sur l'approche classificatrice de courbes multidimensionnelles particulièrement perturbées dans un contexte de mutation de notre réseau de production. Nous conclurons sur les axes de réflexion suscités par ce travail.	Alain Dessertaine	http://editions-rnti.fr/render_pdf.php?p1&p=1001654	http://editions-rnti.fr/render_pdf.php?p=1001654
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Classification et Analyse de Contiguïté		Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001644	http://editions-rnti.fr/render_pdf.php?p=1001644
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Contamination par le mercure et classification en écotoxicologie : Approche classique, approche symbolique.	L'objectif de cette étude est d'acquérir une meilleure connaissance de la contamination par le mercure des communautés de poissons du Haut-Maroni en Guyane Française, afin de prévenir les risques liés à la consommation de certaines espèces. Pour cela deux approches; l'une classique et l'autre symbolique, sont proposées pour la classification des espèces en fonction de la concentration en mercure dosée dans certains organes.	Marie Chavent, Chantal Lacombez, Alain Boudou, Régine Maury-Brachet	http://editions-rnti.fr/render_pdf.php?p1&p=1001650	http://editions-rnti.fr/render_pdf.php?p=1001650
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Des bases de données pour les études statistiques.		Jean-François Rey, Kathy Chapelain, Jacques Vaille	http://editions-rnti.fr/render_pdf.php?p1&p=1001657	http://editions-rnti.fr/render_pdf.php?p=1001657
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Ecologie de listeria monocytogenes dans les ateliers de transformation de produits carnés. Etablissement de profils associés à la présence de la bactérie.	La listériose est une maladie alimentaire due à la bactérie ubiquiste Listeria monocytogenes. Très présente dans l'environnement, elle contamine les surfaces dans les abattoirs et les entreprises de transformation de produits carnés, augmentant ainsi le risque de contamination des produits finis.La lutte contre cette bactérie est actuellement très active : connaître les caractéristiques écologiques de son implantation dans les entreprises en est la première étape. L 'étude présentée ici a pour objectjf de déceler, à partir de données collectées dans 3 sites de transformation de produits carnés, les caractéristiques physico-chimiques, microbiologiques de l'environnement et les conditions de production associées à la présence de Listeria monocytogenes.Trois outils d'analyse de données ont été utilisés pour l'établissement de profils à risque l'analyse factorielle des correspondances multiples, la classification ascendante hiérarchique et la construction d'arbre de décision binaire. Même si ces méthodes ne revêtent pas, en tant que telles, de caractère de nouveauté, leur utilisation dans le domaine vétérinaire et plus particulièrement dans celui de l'hygiène alimentaire est tres récente et appelée à se développer lors de nouvelles enquêtes.	Pascale Gerault, Gilles Salvat, François Madec, Elise Chasseignaux, Jacques Chaperon	http://editions-rnti.fr/render_pdf.php?p1&p=1001651	http://editions-rnti.fr/render_pdf.php?p=1001651
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Fusion et greffes de données	La fusion statistique de fichiers a pour but de compléter un fichier "receveur" où certaines variables ne sont pas renseignées (questions non posées) à l'aide d'un ou plusieurs fichiers "donneurs" portant sur d'autres individus Le fichier donneur comprend bien sûr des variables communes ainsi que les variables d'intérêt renseignées pour tous les individus, Les remplacements de données manquantes se, font soit par des méthodes d'imputation basées sur des proches voisins (injection) soit à l'aide de méthodes explicites de type régression. Les greffes d'enquêtes poursuivent des objectifs proches, en ce sens qu'il s'agit par exemple de positionner des résultats d'un sondage (une analyse factorielle) sur ceux d'un autre en utilisant des variables passerelles, mais sans nécessairement chercher à estimer les données manquantes. Cet exposé présentera la problématique, les principales techniques utilisées, ainsi que les dangers potentiels...	Gilbert Saporta, Nicolas Fischer	http://editions-rnti.fr/render_pdf.php?p1&p=1001646	http://editions-rnti.fr/render_pdf.php?p=1001646
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	La lune et la lunette		Claude Tricot	http://editions-rnti.fr/render_pdf.php?p1&p=1001648	http://editions-rnti.fr/render_pdf.php?p=1001648
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Méthodes bayésiennes et modélisation des risques géophysiques extrêmes	Les estimations de risques induits par les événements extrêmes tels que les crues des rivières sont soumises aux larges incertitudes de l'extrapolation des distributions des variables enjeu comme les débits des rivières. En effet les événements dont il faut se protéger sont généralement beaucoup plus rares que ceux déjà systématiquement enregistrés. Il existe une littérature très abondante sur le choix des modèles, les erreurs d'échantillonnage et les moyens de réduire partiellement les incertitudes induites. Mais, par respect d'une soi disant " objectivité scientifique ", les hydrologues statisticiens classiques hésitent à introduire les connaissances a priori des experts dans leurs analyses. La mise en oeuvre de l'approche bayésienne dans le cadre d'un modèle de dépassements (POT) classique Poisson-Pareto généralisé, permet la prise en compte rationnelle des opinions d'experts. La méthode, utilisant systématiquement les techniques de simulation dites MCMC, est appliquée au cas des crues de la Garonne et montre le gain notable en précision obtenu.	Jacques Bernier, Eric Parent	http://editions-rnti.fr/render_pdf.php?p1&p=1001652	http://editions-rnti.fr/render_pdf.php?p=1001652
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Mise en oeuvre d'une démarche statistique complète pour la prédiction de variables d'une base de données clientèle d'EDF		Christian Derquenne	http://editions-rnti.fr/render_pdf.php?p1&p=1001645	http://editions-rnti.fr/render_pdf.php?p=1001645
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Segmenter la clientèle à partir des courbes de charge		Anne Debregeas	http://editions-rnti.fr/render_pdf.php?p1&p=1001655	http://editions-rnti.fr/render_pdf.php?p=1001655
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Un outil pour la comparaison et la validation d'algorithmes d'apprentissage à partir de données.	L'apprentissage supervisé à partir de données se trouve dans la situation paradoxale où il y a surabondance d'algorithmes et pénurie de méthodes permettant de comparer et d'appliquer ces algorithmes à bon escient. Remédier à cette situation suppose que la spécificité de tout nouvel algorithme soit explicitée de manière intelligible, interprétable et donc applicable, puis mise en relation avec l'information a priori concernant les différentes familles de problèmes traités. Pour accompagner dans cette démarche, nous présentons un site Internet qui sert d'outil de gestion des connaissances en explorant les liens existant entre les spécificités des algorithmes et les caractéristiques des problèmes.	Michel Crucianu, Gilles Verley, Jean-Pierre Asselin de Beauville, Romuald Boné	http://editions-rnti.fr/render_pdf.php?p1&p=1001656	http://editions-rnti.fr/render_pdf.php?p=1001656
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2001	Une méthode de classification de données binaires basée sur la décomposition rectangulaire.		Hédia Mhiri Sellami, Ali Jaoua	http://editions-rnti.fr/render_pdf.php?p1&p=1001649	http://editions-rnti.fr/render_pdf.php?p=1001649
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2000	Des algorithmes évolutionnaires pour la classification automatique	Cet article traite des méthodes de classification à l'aide  d'algorithmes évolutionnaires (algorithmes génétiques, stratégies d'évolution). Ces algorithmes sont connus pour leur capacité à échapper aux extrema locaux de la fonction optimisée (critère de classification). Nous proposons ici de nouveaux algorithmes de partitionnement utilisables en particulier lorsque le nombre de classes de la partition n'est pas fixé apriori, ils sont définis à partir de codages originaux des partitions et font appel à des opérateurs génétiques nouveaux. On étudie également la question difficile du choix du critère de classification à optimiser. Des tests permettent d'évaluer les méthodes proposées.	Fatima-Zohra Kettaf, Jean-Pierre Asselin de Beauville	http://editions-rnti.fr/render_pdf.php?p1&p=1001639	http://editions-rnti.fr/render_pdf.php?p=1001639
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2000	L'analyse de la covariance (le point de vue du praticien)		Carlos Lopez	http://editions-rnti.fr/render_pdf.php?p1&p=1001643	http://editions-rnti.fr/render_pdf.php?p=1001643
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2000	L'analyse discriminante sur variables qualitatives : programmes de quatre méthodes	Nous présentons quatre modèles de discrimination sur variables qualitatives la discrimination prédictive fondée sur le modèle d'indépendance conditionnelle, deux modèles graphiques décomposables et le modèle logistique linéaire. Toutes ces méthodes visent à réduire la complexité du modèle multinomial complet. Nous introduisons ensuite, les programmes des quatre méthodes.	Abdallah Mkhadri, Abdelaziz Nasroallah	http://editions-rnti.fr/render_pdf.php?p1&p=1001640	http://editions-rnti.fr/render_pdf.php?p=1001640
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2000	Macros SAS PLS		Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001641	http://editions-rnti.fr/render_pdf.php?p=1001641
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2000	Modélisation par chaînes de Markov homogènes ergodiques des appels de puissance d'un parc de chauffe-eau électriques	Lors de l'évaluation des actions de Maîtrise de la Demande d'Electricité, on a besoin de méthodes estimant au mieux les gains potentiels d'une mesure en termes de consommation et puissance évitées, d'où l'intérêt de disposer de courbes de charge foisonnées sur un pare d'appareils électriques. Ces approches conjuguent connaissance des systèmes physiques et connaissance des mécanismes d'agrégation. Une nouvelle méthode stochastique est développée dans cet article, basée sur une modélisation markovienne de s appels de puissance d'un pare de chauffe-eau électriques. Le caractère ergodique de la chaîne de Markov qui régit l'agrégation des réponses sur le parc est démontrée.	Mathieu Orphelin	http://editions-rnti.fr/render_pdf.php?p1&p=1001638	http://editions-rnti.fr/render_pdf.php?p=1001638
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	2000	Programme pour le calcul de coefficients d'association entre variables relationnelles		Mohamed Ouali Allah	http://editions-rnti.fr/render_pdf.php?p1&p=1001642	http://editions-rnti.fr/render_pdf.php?p=1001642
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Analyse de la variance à effets mixtes, utilisation de la proc MIXED : mais que reste-t-il à la proc GLM ?	L'analyse de la variance joue un rôle tout à fait particulier en Statistique. C'est depuis son origine un univers en perpétuelle expansion. La demande des praticiens a obligé les statisticiens à construire des modèles plus performants, plus souples, s'adaptant mieux à la réalité des données. L'histoire de l'analyse de la variance est bien résumée à travers les procédures SAS d'analyse de la variance : ANOVA, GLM et MIXED. La Proc ANOVA ne traite que les données équilibrées : elle est donc entièrement fondée sur des calculs de sommes de carrés. La Proc GLM embrasse des situations plus complexes : données déséquilibrées, comparaisons multiples, analyse de la variance multivariée, analyse de la variance de mesures répétées, analyse de la variance à effets mixtes. Cependant si la Proc GLM fournit les tests appropriés en analyse de la variance à effets mixtes, elle donne des résultats faux au niveau de l'estimation. Ces limitations de la Proc GLM sont levées dans la Proc MIXED. La Proc MIXED a considérablement simplifié la vie du chercheur en permettant une étude " juste " des modèles à effets mixtes. L'objet de cette conférence est d'identifier avec précision ces limitations de la Proc GLM et de présenter les solutions justes apportées par la Proc MIXED. Il reste cependant deux points pour lesquels la Proc GLM propose des solutions intéressantes : (1) l'approche multivariée dans le traitement des mesures répétées sans données manquantes conduit à une meilleure évaluation des niveaux de signification que le même modèle étudié par la Proc MIXED lorsque la matrice de covariance des résidus est de type " unstructured ", (2) la Proc GLM propose des tests basés sur les sommes de carrés de type IV lorsque le plan d'expérience contient des cases vides. Cette possibilité est absente de la Proc MIXED. Chaque thème présenté sera traité autour d'un exemple issu de Milliken & Johnson (1984).	Michel Tenenhaus	http://editions-rnti.fr/render_pdf.php?p1&p=1001628	http://editions-rnti.fr/render_pdf.php?p=1001628
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Concentration et dispersion : analyse des séries chronologiques à forte variabilité	Le point de départ de cette recherche est la théorie des prix de Benoit Mandelbrot qui a détrôné la théorie des fluctuations régies par le hasard de Bachelier basée sur le mouvement brownien, en faveur de la classe des processus alpha-stables. En effet, si l'on considère des trajectoires boursières, la loi estimant la densité d'échantillon s'avère ne pas être normale mais stable sans variance. Dans cette optique, la plupart des mesures de dispersion n'ont plus de sens parce qu'elles ont en commun de se fonder sur une notion de distance et dépendent trop de ce fait des caractéristiques de l'échantillon. Nous introduisons, à cet effet, un indice de concentration emprunté à la géographie (Tricot (1971), Tricot & Raffestin (1979)) pour le convertir en mesure de dispersion en jouant sur la complémentarité potentielle entre concentration et dispersion. Cet indice présente l'avantage d'être obtenu à partir d'une modélisation convenable et de se fonder sur des probabilités.	Anne-Valère Haenni Amo, Claude Tricot	http://editions-rnti.fr/render_pdf.php?p1&p=1001630	http://editions-rnti.fr/render_pdf.php?p=1001630
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Couplage des analyses dimensionnelle et statistique dans la modélisation du rayonnement acoustique émis par rupture de produits solides fragiles	Afin de pallier le manque d'étude et d'analyses fondamentales concernant le rayonnement sonore émis lors de la rupture de produits solides fragiles, des études expérimentales ont été tout récemment réalisées par l'équipe de recherche du Laboratoire Maîtrise des Techniques Agro-Industielles (L.M.T.A.I.) ; elles visaient la quantification des phénomènes physiques et l'établissement des lois empiriques liant les caractéristiques acoustiques, mécaniques et dimensionnelles du matériau sollicité, L'approche monovariationnelle a vite révélé ses limites propres et son inadéquation avec l'objectif visé. Disposant d'une large base de données expérimentales englobant de nombreux échantillons de matériaux divers et de dimensions géométriques variables, nous avons adopté une nouvelle approche fondée sur une analyse dimensionnelle permettant à la fois de réduire la complexité et de construire des modèles à l'aide de méthodes statistiques.	Mariam Mahfouz, Mokrane Abdiche, Karim Allaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001626	http://editions-rnti.fr/render_pdf.php?p=1001626
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Estimation des quantités de produits de fission relâchés lors d'un accident de fusion d'un coeur de réacteur à eau sous pression	La connaissance des incertitudes sur les appareils de mesures fournie par les étalonnages, bien qu'indispensable et efficace pour mettre en évidence certaines erreurs systématiques, ne constitue pas une approche suffisante dans le cadre d'expérience d'investigation où les paramètres influents sont très nombreux et difficilement contrôlables. De plus ces expériences mettent souvent en évidence des phénomènes imprévisibles, il faut alors trouver un moyen de les évaluer en incertitudes, la loi de propagation des incertitudes n'étant pas applicable directement. L'approche adoptée ici est une approche globale basée sur l'utilisation d'un modèle statistique. Celui-ci permet d'engendrer des variances qui sont ensuite interprétées comme des incertitudes.	Laurent Pantera	http://editions-rnti.fr/render_pdf.php?p1&p=1001625	http://editions-rnti.fr/render_pdf.php?p=1001625
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Implémentation en Splus des méthodes SIR univariées et multivariée		Jérôme Saracco	http://editions-rnti.fr/render_pdf.php?p1&p=1001636	http://editions-rnti.fr/render_pdf.php?p=1001636
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Importance des variables dans les méthodes CART		Badhi Ghattas	http://editions-rnti.fr/render_pdf.php?p1&p=1001631	http://editions-rnti.fr/render_pdf.php?p=1001631
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Indice de satisfaction : conceptualisation et mise en application dans le cadre de la Chambre de Commerce et d'Industrie de Montpellier. 		Jean-Louis Monino, Eric Cavagna	http://editions-rnti.fr/render_pdf.php?p1&p=1001629	http://editions-rnti.fr/render_pdf.php?p=1001629
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Introduction à la régression des moindres carrés partiels avec la procédure PLS de SAS	Cet article est une introduction à la mise en oeuvre de la procédure PLS de SAS permettant d'utiliser certaines techniques de régression sur variables latentes. On présente tout d'abord trois modèles de régression sur variables latentes (régression sur composantes principales, analyse des redondances et régression des moindres carrés partiels), puis les options correspondantes de la procédure PLS. Des exemples simples d'utilisation illustrent l'interprétation des résultats fournis par la procédure pour la régression des moindres carrés partiels. Une bibliothèque de macro-programmes SAS facilitant l'interprétation des résultats paraitra dans le numéro 25 de la Revue de MODULAD (pp. 59-62).	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001633	http://editions-rnti.fr/render_pdf.php?p=1001633
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	La nouvelle approche dans le choix des régresseurs de la régression multiple en présence d'interactions et de colinéarités	A partir d'exemples simples, on présente une méthode originale de choix des variables et interactions d'ordre 2 dans un modèle de régression multiple, en présence de multicolinéarités ou d'interactions. La métbode "CORICO" (Iconographie des Corrélations) est fondée sur les corrélations totales et partielles. La détection des points aberrants est obtenue au moyen de variables indicatrices des observations. Nous montrons comment l'introduction de fonctions logiques non-linéaires d'ordre 2 améliore l'interprétation du plan d'expérience.	Michel Lesty	http://editions-rnti.fr/render_pdf.php?p1&p=1001635	http://editions-rnti.fr/render_pdf.php?p=1001635
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	La précision des enquêtes revisitée		Anne-Marie Dussaix	http://editions-rnti.fr/render_pdf.php?p1&p=1001637	http://editions-rnti.fr/render_pdf.php?p=1001637
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	La régression inverse par tranche ou méthodes SIR : présentation.		Jérôme Saracco, Larramendy Irène, Aragon Yves	http://editions-rnti.fr/render_pdf.php?p1&p=1001634	http://editions-rnti.fr/render_pdf.php?p=1001634
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Les réseaux de neurones artificiels pour l'amélioration de la qualité	Le travail présenté dans cet article s'intéresse à l'utilisation des réseaux de neurones artificiels dans le contrôle de la qualité. Les réseaux de neurones représentent des alternatives aux outils de maîtrise statistique des procédés, plus particulièrement les cartes de contrôle. Des comparaisons basées sur le critère d'erreur de diagnostic ont été effectuées entre des réseaux multicouches et les cartes univariées X-bar, S, R et CUSUM. Ces comparaisons ont été généralisées au cas des cartes bivariées. Les résultats obtenus montrent que le perceptron multicouche est meilleur que les cartes de contrôle univariées dans la détection des variations relatives à une caractéristique donnée. En outre, le perceptron multicouche s'est montré plus performant que les cartes de contrôle bivariées Chi-2 puisqu'il détecte aussi bien le déréglage que la variable qui en est responsable.	Rafik Bouassida, Mohamed M.T. Linam 	http://editions-rnti.fr/render_pdf.php?p1&p=1001627	http://editions-rnti.fr/render_pdf.php?p=1001627
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1999	Procédure manuelle pour la construction d'arbres de régression sous S+		Badhi Ghattas	http://editions-rnti.fr/render_pdf.php?p1&p=1001632	http://editions-rnti.fr/render_pdf.php?p=1001632
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1998	Etude comparative par simulation de méthodes d'analyse discriminante, de classification et de réseaux de neurones	Cette étude s'intéresse à l'analyse du comportement de méthodes classiques d'analyse discriminante et de classification et de méthodes neuronales lorsqu'il existe dans les données, une structure de classes bien spécifique obéissant à une certaine distribution connue. Cette analyse a été réalisée sur des données simulées selon un plan d'expérience que nous avons mis en oeuvre	Nadia Ghazzali, Marc Parizeau, Josiane Deblois	http://editions-rnti.fr/render_pdf.php?p1&p=1001619	http://editions-rnti.fr/render_pdf.php?p=1001619
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1998	L'analyse discriminante : un outil de contrôle qualité en fabrication		Thierry Cembrzynski	http://editions-rnti.fr/render_pdf.php?p1&p=1001620	http://editions-rnti.fr/render_pdf.php?p=1001620
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1998	Le discriminateur à plus proche structure propre		Jean-Pierre Asselin de Beauville, Eric Darmigny, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001618	http://editions-rnti.fr/render_pdf.php?p=1001618
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1998	Une introduction à l'Analyse en Composantes Principales avec SPSS pour WINDOWS	Cette note initie l'utilisateur débutant à la mise en oeuvre de l'Analyse en Composantes Principales au moyen de la procédure d'Analyse Factorielle FACTOR du logiciel SPSS pour Windows. Cette mise en oeuvre concerne l'analyse multidimensionnelle des tableaux de données numériques quantitatives. Le listage des résultats obtenus est commenté par la présentation du formulaire de l'Analyse en Composantes Principales associé à chacun des résultats obtenus.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001621	http://editions-rnti.fr/render_pdf.php?p=1001621
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Analyse et prévision des economies urbaines en Chine (1987-1991)	La Régression PLS (Partial Least Squares) a été utilisée pour étudier le développement économique urbain en Chine, et aussi pour établir des modèles de prévision. Les comportements économiques des différentes régions sont décrites d'une manière satisfaisante et les modèles de prévision économiques sont relativement bien adaptés.	Huiwen Wang	http://editions-rnti.fr/render_pdf.php?p1&p=1001615	http://editions-rnti.fr/render_pdf.php?p=1001615
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Enquête 1996 : Utilisation des logiciels statistiques		Dominique Drouet, Arnaud Bringe, Danielle Grangé, Véronique Stéphan, Laurence Haeusler, Yves Lechevallier, Monica Becue	http://editions-rnti.fr/render_pdf.php?p1&p=1001622	http://editions-rnti.fr/render_pdf.php?p=1001622
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Et si vous étiez un bayésien qui s'ignore ?		Bruno Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001609	http://editions-rnti.fr/render_pdf.php?p=1001609
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Etude de la puissance des tests. Utilisation du logiciel PASS6.0		Michel Tenenhaus	http://editions-rnti.fr/render_pdf.php?p1&p=1001623	http://editions-rnti.fr/render_pdf.php?p=1001623
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Explorer le temps avec SAS ; quelques astuces en visualisation préliminaire		Jean-Paul Valois	http://editions-rnti.fr/render_pdf.php?p1&p=1001624	http://editions-rnti.fr/render_pdf.php?p=1001624
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Générateur de données et superviseur d'expériences	Générer des jeux de données complexes conformes à un plan d'expériences et faciliter la mise en oeuvre de ces données sur les programmes informatiques que l'on veut expérimenter sont les deux objectifs que nous nous sommes donnés pour réaliser l'atelier informatique d'expérimentation sur des programmes que nous présentons d'abord au travers d'une expérimentation sur les classifieurs supervisés et, ensuite, de manière théorique.	Gilles Verley, Eric Ramat	http://editions-rnti.fr/render_pdf.php?p1&p=1001607	http://editions-rnti.fr/render_pdf.php?p=1001607
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	La procédure FREQ de SAS		Josiane Confais, Yvette Grelet, Monique Le Guen	http://editions-rnti.fr/render_pdf.php?p1&p=1001617	http://editions-rnti.fr/render_pdf.php?p=1001617
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	La statistique avec un tableur		Bernard Burtshy	http://editions-rnti.fr/render_pdf.php?p1&p=1001616	http://editions-rnti.fr/render_pdf.php?p=1001616
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Le traitement statistique des données sensorielles	Les résultats de tests de dégustation peuvent donner lieu à de multiples traitements statistiques ; on présente ici quelques méthodologies utilisant des méthodes classiques appliquées à des données recueillies sous des formes elles aussi classiques. De nombreuses remarques sur la portée des résultats apparaissent au fil du texte.	Jérôme Pagès	http://editions-rnti.fr/render_pdf.php?p1&p=1001605	http://editions-rnti.fr/render_pdf.php?p=1001605
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Quelle taille d'échantillon faut-il prendre ?		Alain Morineau	http://editions-rnti.fr/render_pdf.php?p1&p=1001610	http://editions-rnti.fr/render_pdf.php?p=1001610
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Réseaux de neurones et analyse des correspondances		Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001606	http://editions-rnti.fr/render_pdf.php?p=1001606
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Statistique, technologie et culture		Daniel Defays	http://editions-rnti.fr/render_pdf.php?p1&p=1001614	http://editions-rnti.fr/render_pdf.php?p=1001614
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1997	Une introduction à l'analyse des correspondances avec SPSS/Windows	Cette note initie l'utilisateur débutant à la mise en oeuvre de l'Analyse Factorielle des Correspondances au moyen de la procédure ANACOR du logiciel SPSS pour Windows. Cette mise en oeuvre concerne l'analyse des tableaux de contingence à partir d'un exemple basé sur des données individuelles et d'un exemple basé sur des données agrégées. Le listage des résultats obtenus est commenté par la présentation du formulaire de l'analyse des correspondances associé à chacun des résultats obtenus.	Dominique Desbois	http://editions-rnti.fr/render_pdf.php?p1&p=1001608	http://editions-rnti.fr/render_pdf.php?p=1001608
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1996	Au delà du test de signification (ou l'inférence satistique sans tables).		Bruno Lecoutre	http://editions-rnti.fr/render_pdf.php?p1&p=1001612	http://editions-rnti.fr/render_pdf.php?p=1001612
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1996	Du dessin des arbres résultant de classifications hiérarchiques		Gérard Thauront	http://editions-rnti.fr/render_pdf.php?p1&p=1001604	http://editions-rnti.fr/render_pdf.php?p=1001604
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1996	Estimation de quantités subjectives floues par des techniques connexionnistes. Application à l'évaluation du confort automobile. 	En utilisant le formalisme de la logique floue dans le cadre des perceptions multicouches, nous montrons dans cet article comment estimer le degré d'appartenance à une classe à partir d'observations subjectives incertaines, imprécises, voire conflictuelles. La méthode proposée est appliquée avec succès au problème de l'évaluation automatique du confort vibratoire automobile.	Sylvain Millemann, Pierre Scholl	http://editions-rnti.fr/render_pdf.php?p1&p=1001603	http://editions-rnti.fr/render_pdf.php?p=1001603
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1996	Qu'est-ce qu'un réseau de neurones?		Laurence Tricot	http://editions-rnti.fr/render_pdf.php?p1&p=1001613	http://editions-rnti.fr/render_pdf.php?p=1001613
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1995	Estimation non paramétrique de la densité et de la régression - Prévision non paramétrique.	Après un aperçu sur l'estimation non paramétrique de la densité et de la régression, nous détaillons et interprétons une méthode de prévision, dite prévision non paramétrique. Nous en montrons les différents aspects aussi bien techniques que pratiques, et la comparons sur quelques exemples à la méthodologie de Box et Jenkins.	Michel Carbon, Christian Franck	http://editions-rnti.fr/render_pdf.php?p1&p=1001597	http://editions-rnti.fr/render_pdf.php?p=1001597
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1995	Méthodes constructives pour l'apprentissage à partir d'exemples :   les arbres neuronaux hybrides et leur comportement asymptotique		Florence D'Alche-Buc, Jean-Pierre Nadal	http://editions-rnti.fr/render_pdf.php?p1&p=1001600	http://editions-rnti.fr/render_pdf.php?p=1001600
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1995	Quelques modèles pour l'arcade dentaire humaine	La modélisation des formes d'arcade dentaire a depuis longtemps intéressé les orthodontistes. Dans ses traitements, le praticien utilise souvent des arcs à mémoire de forme. Ces arcs sont industriellement préformés selon un petit nombre de gabarits. L'utilisation de ces gabarits passe nécessairement par une bonne connaissance des formes d'arcades. Cet article rend compte d'une expérience de modélisation statistique d'arcades dentaires. Des approches paramétriques classiques sont décrites. Certaines de leurs insuffisances suggèrent l'emploi de méthodes statistiques robustes. Les résultats obtenus semblent convenir au praticien. Ce travail s'inscrit dans un cadre plus général visant à individualiser l'arc à mémoire de forme : chaque praticien à l'aide d'une caméra (pour l'acquisition des données), d'un ordinateur et d'un logiciel adéquat pourrait ainsi obtenir instantanément l'arc le mieux adapté à son patient.	Maxime Rotenberg, Philippe Dejean, Denis Ducros, Antoine De Falguerolles	http://editions-rnti.fr/render_pdf.php?p1&p=1001601	http://editions-rnti.fr/render_pdf.php?p=1001601
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1995	Réseaux de neurones et modèles statistiques		Antonio Ciampi, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001598	http://editions-rnti.fr/render_pdf.php?p=1001598
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1995	Tester une corrélation sans table		Alain Morineau	http://editions-rnti.fr/render_pdf.php?p1&p=1001602	http://editions-rnti.fr/render_pdf.php?p=1001602
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1995	Un outil graphique interactif d'aide à l'interprétation de résultats d'analyse de données.		Françoise Goupil-Testu	http://editions-rnti.fr/render_pdf.php?p1&p=1001599	http://editions-rnti.fr/render_pdf.php?p=1001599
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Compréhension du phénomène de corrosion sur aiguilles irradiées dans le réacteur PHENIX	Compréhension du phénomène de corrosion sur aiguilles irradiées dans le réacteur PHENIX. B. CORNU, L. PANTERA. La revue MODULAD, numéro 13, Juin 1994.RésuméLa limite supérieure du taux de combustion des éléments combustibles du Réacteur à Neutrons Rapides PNENIX est fortement conditionnée par un phénomène de corrosion interne de leur gaine en acier.Un nouveau programme d'étude de cette corrosion a été mis en place en 1987 par le Département d'Etudes des Combustibles du CEA/CEN Cadarache. Les résultats contribueront à déterminer les choix d'orientation pour le projet E.F.R. (European Fast Reactor).Le travail présenté se situe dans le cadre de ce nouveau programme se basant sur les données issues d'une campagne expérimentale menée dans PHENIX de 1980 à 1990, son objectif est la mise en oeuvre d'une méthodologie statistique qui permette une meilleure compréhension du comportement corrosif des éléments combustibles.Nous présenterons dans un premier temps le cadre de notre étude, les objets manipulés et les variables :    conditions d'irradiation (variables explicatives),    évaluation de la corrosion (variables à expliquer).Nous développerons ensuite le parcours statistique réalisé au sein des données. Celui-ci a permis d'apporter un regard global nouveau sur le phénomène de corrosion dans PHENIX, relativisant certains cas d'expérience pénalisants pour le projet d'irradiation à très fort taux de combustion. L'analyse des conditions d'irradiation met en évidence une stratégie différente de l'utilisation des assemblages en réacteur qui serait à l'origine de l'évolution de la corrosion sur les dix dernières années. La suppression artificielle de l'influence spécifique de cette modification de stratégie permet de faire ressortir des variables conditionnant plus directement le phénomène physique de corrosion. Les différentes vues synthétiques nous ont ainsi permis de sélectionner quelques objets clés à partir desquels il a été possible d'établir un modèle empirique de la corrosion. Celui-ci indique alors qu'il est envisageable d'atteindre un taux de combustion élevé dans la mesure où l'on effectue une gestion fine des conditions d'irradiation au cours de la vie des éléments combustibles.	B. Cornu, Laurent Pantera	http://editions-rnti.fr/render_pdf.php?p1&p=1001574	http://editions-rnti.fr/render_pdf.php?p=1001574
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Identification de pistes radar par l'analyse des données symbolique-numérique. 		Jean-François Grandin, Catherine Jacq	http://editions-rnti.fr/render_pdf.php?p1&p=1001576	http://editions-rnti.fr/render_pdf.php?p=1001576
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Les Plans d'Expériences : un outil d'aide à la conception du Produit et des Essais.	Les plans d'expériences permettent, non seulement, d'optimiser les performances d'un produit, mais aussi de vérifier sa faible sensibilité,-- sa « robustesse » --, aux conditions de fabrication et d'environnement; un premier' exemple illustre ce point.La méthode des Plans d'expériences peut également s'appliquer à la mise au point d'une procédure d'essai; dans le deuxième cas présenté, l'objectif visé est la sélection des contraintes d'environnement à introduire dans un essai destiné à valider la fiabilité d'un mécanisme. Ces deux exemples démontrent la synergie existant entre moyens de simulation de l' environnement et méthode des plans d'expériences.	J. Demonsant	http://editions-rnti.fr/render_pdf.php?p1&p=1001575	http://editions-rnti.fr/render_pdf.php?p=1001575
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Principes et calculs de la méthode implantée dans le programme CHAVL (partie II)		Israël-César Lerman, Philippe Peter, Henri Leredde	http://editions-rnti.fr/render_pdf.php?p1&p=1001587	http://editions-rnti.fr/render_pdf.php?p=1001587
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Rôle de la classification statistique dans la compression du signal d'image : panorama et étude de cas	La compression des images à base de quantification vectorielle ("QV") constitue toujours un champ actif de recherches. Même si cette technique employée seule ne fournit pas, aujourd'hui, des taux de compression suffisants eu égard aux besoins des applications et à "ses soeurs concurrentes" (méthodes par transformées, méthodes prédictives, méthodes affines, etc. ), elle n'en reste pas moins qu'associée à d'autres techniques, elle apporte des compléments de compression non négligeables - démontrés théoriquement -, et de plus, présente des qualités de simplicité - au décodeur - fort appréciées.Nous commençons par une présentation générale du domaine de la compression image en y montrant comment la QV s'y inscrit; et, la situation de cette approche vis-à-vis d'autres, plus traditionnelles (transformées en "cosinus"), la QV peut directement être obtenue par une méthode de classification automatique dont il y a lieu d'adapter les paramètres. On entre ainsi dans le corps des méthodes de l'analyse classificatoire des données et nous rapportons une analyse expérimentale originale, présentant divers aspects méthodologiques - et couvrant deux thèses - sur l'impact comparé de diverses méthodes de classification; où la méthode de l'Analyse de la Vraisemblance du Lien (AVL) est considérée avec un accent particulier.	Nadia Ghazzali, Alain Leger, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001595	http://editions-rnti.fr/render_pdf.php?p=1001595
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Sélection de modèles linéaires et non linéaires 	Nous allons aborder les problèmes d'estimation des erreurs d'apprentissage et de généralisation pour des modèles linéaires ou flou. Il est reconnu que cette sélection doit suivre le principe de parcimonie, i.e que les modèles les plus simples seront choisis prioritairement grâce à une pénalisation des modèles complexes. Mais le problème majeur qui se pose concerne la forme et l'importance du terme de pénalité.Les nouvelles règles proposées pour la sélection de modèles, parmi lesquelles la règle GAE, reposent sur les estimations des erreurs d'apprentissage et de généralisation. Ces estimations vont aussi permettre de clarifier les liens qui existent entre ces deux erreurs, d'expliciter le terme pénalité précédent et de présenter des critères semblables à ceux d'Akaike pour les modèles linéaires (FPE et AIC) mais aussi valables pour des modèles non linéaires	Christophe Monrocq	http://editions-rnti.fr/render_pdf.php?p1&p=1001592	http://editions-rnti.fr/render_pdf.php?p=1001592
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1994	Une méthode de compression adaptée JEPG basée sur la modélisation statistique	Nous présentons une étude statistique dont le but est d'améliorer le taux de compression des images pour des méthodes basées sur la Transformée en Cosinus Discrète (TCD) et spécifiquement pour les méthodes définies dans la norme JPEG. Cette étude est réalisée de façon extensive à l'aide d'une batterie de tests paramétriques et non-paramétriques et des modèles de lois comme ceux de Cauchy, Laplace, Gauss, mélange de Laplaciennes et de Gaussiennes. Nous montrons que ces coefficients peuvent être modélisés par des mélanges de distiibutions de Gauss. Nous expliquons alors comment les taux de compression peuvent être améliorés par l'utilisation de ces résultats.	T. Eude, H. Cherifi	http://editions-rnti.fr/render_pdf.php?p1&p=1001593	http://editions-rnti.fr/render_pdf.php?p=1001593
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Analyse des proximités et programmes de codage multidimensionnel		Gérard Drouet d'Aubigny	http://editions-rnti.fr/render_pdf.php?p1&p=1001571	http://editions-rnti.fr/render_pdf.php?p=1001571
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Commentaires sur "Une histoire de discrétisation"		Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001586	http://editions-rnti.fr/render_pdf.php?p=1001586
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Commentaires sur l'article 		Michel Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001589	http://editions-rnti.fr/render_pdf.php?p=1001589
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Commentaires sur l'article de G. CELEUX et de C. ROBERT		Jean-Baptiste Denis, Gérard Govaert, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001588	http://editions-rnti.fr/render_pdf.php?p=1001588
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Commentaires sur la note de G. CELEUX et de C. ROBERT		Jean-Christophe Turlot	http://editions-rnti.fr/render_pdf.php?p1&p=1001590	http://editions-rnti.fr/render_pdf.php?p=1001590
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Commentaires sur une histoire de discrétisation		P. Besse, Pierre Cazes, Olivier Gascuel, Jean-Louis Golmard, Ludovic Lebart, André Carlier, Antoine De Falguerolles	http://editions-rnti.fr/render_pdf.php?p1&p=1001585	http://editions-rnti.fr/render_pdf.php?p=1001585
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Enquête sur l'utilisation des logiciels de statistique -ASU 1992-		Monica Becue, M. K. Diallo, Danielle Grangé, Laurence Haeusler, Yves Lechevallier, Y. Majjad, M. Ringenbach, V. Perez, F. Sermier	http://editions-rnti.fr/render_pdf.php?p1&p=1001594	http://editions-rnti.fr/render_pdf.php?p=1001594
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Le modèle d'indépendance conditionnelle : le programme DISIND	On présente le modèle d'indépendance conditionnelle. On montre qu'il s'agit d'une méthode parcimonieuse et de référence pour l'analyse discriminante sur variables qualitatives, si l'on privilégie le point de vue décisionnel. On introduit le programme DISIND qui le réalise et qui valide la règle de décision par validation croisée.	Sami Bochi, Gilles Celeux, Abdallah Mkhadri	http://editions-rnti.fr/render_pdf.php?p1&p=1001577	http://editions-rnti.fr/render_pdf.php?p=1001577
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	NOTICE du programme CHAVL		Philippe Peter, Henri Leredde, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001573	http://editions-rnti.fr/render_pdf.php?p=1001573
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Notice du programme DISIND		Sami Bochi, Gilles Celeux, Abdallah Mkhadri	http://editions-rnti.fr/render_pdf.php?p1&p=1001596	http://editions-rnti.fr/render_pdf.php?p=1001596
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Principes et calculs de la méthode implantée dans le programme CHAVL (première partie)		Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001572	http://editions-rnti.fr/render_pdf.php?p=1001572
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Réponse des auteurs		Gilles Celeux, Claudine Robert	http://editions-rnti.fr/render_pdf.php?p1&p=1001591	http://editions-rnti.fr/render_pdf.php?p=1001591
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1993	Une histoire de discrétisation	On raconte les aventures de 23 papillons découpés de façon malencontreuse.	Gilles Celeux, Claudine Robert	http://editions-rnti.fr/render_pdf.php?p1&p=1001578	http://editions-rnti.fr/render_pdf.php?p=1001578
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1992	Analyse multivariée des données discrètes: une application à l'étude de la distribution régionale des virus du SIDA dans l'Angola	En Analyse Classificatoire Discrète on doit souvent traiter des tableaux de fréquences décrivant un ensemble de Catégories / Unités de Données par un ensemble de variables discrètes. Le traitement par des méthodes de classification hiérarchique ascendante de ces tableaux, de nature généralement descriptive, peut alors être complété par l'étude indifférentielle de plusieurs tableaux de contingence uni / bi ou muultidimensionnelles. On associe donc à l'analyse exploratoire du tableau global, des analyses partielles confirmatoires, dont les hypothèses à tester sont suggérées par les résultats issus de la première analyse. Cette méthodologie est appliquée dans le travail présent à l'étude de la distribution régionale des virus HIV1 et HIV2 du SIDA dans la population de l'Angola. On utilise des approches classique aussi bien que probabiliste pour l'Analyse Classificatoire Hiérarchique du tableau initial. On fait des analyses classiques et loglinéaires pour les Tableaux de Contingence partiels.	Helena Bacelar-Nicolau	http://editions-rnti.fr/render_pdf.php?p1&p=1001569	http://editions-rnti.fr/render_pdf.php?p=1001569
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1992	Application de la description statistique des séries chronologiques à l'étude des trajectoires de soins	En présence d'une série temporelle non modélisable, il s'agit de réduire l'information en classifiant les éléments de la série compte tenu du temps. On classifie d'abord les intervalles de temps entre deux éléments contigus, puis les éléments eux-mêmes. Les classes obtenues sont appelées épisodes. Une application sur les contacts d'un groupe de patients suicidaires avec les services psychiatriques de Genève, permet de déterminer des épisodes de soins et de situer les tentatives de suicide par rapport à ceux-ci.	Laurence Tricot	http://editions-rnti.fr/render_pdf.php?p1&p=1001568	http://editions-rnti.fr/render_pdf.php?p=1001568
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1992	Place de l'analyse discriminante dans le traitement d'enquêtes psycho-sociales	Les enquêtes psycho-sociales reposent sur des questionnaires fermés ayant trois volets les données socio-démogiaphiques et légales, la prise de drogue et les données médicales. L'objectif de l'analyse de ces questionnaires est d'expliquer, dans un cadre plurifactoriel, le volet des données socio-démographiques à partir des données médicales.Comme ces données socio-démographiques et légales sont caractérisées par plusieurs prédicteurs nous proposons de décomposer cette analyse en deux étapes :La première étape est une étape de classification qui nous permettra de dégager des groupes homogènes par rapport aux données sociales.La deuxième étape est une étape de discrimination de cette typologie en fonction des données médicales. Plusieuis méthodes de discrimination seront utilisées et comparées (analyse discriminante linéaire, analyse discriminante par le modèle d'indépendance, classification pal arbre).	F. Facy, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001566	http://editions-rnti.fr/render_pdf.php?p=1001566
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1992	Quelques Exemples d'Application Statistique du Logiciel d'Optimisation GINO	Il arrive qu'une méthode d'estimation ne soit pas directement offerte par les logiciels statistiques usuels ou immédiatement disponibles ou que sa programmation dans les environnements définis par ces logiciels soit trop lourde pour être envisagée. Il se peut encore que l'on veuille contrôler les résultats d'un logiciel ou d'un programme ad hoc sur des exemples tests de taille raisonnable. L'objet de cet article est de décrire, sous forme d'exemples, comment le logiciel d'optimisation non linéaire GINO peut être utilisé dans un contexte statistique pour obtenir des estimations variées.	Antoine De Falguerolles	http://editions-rnti.fr/render_pdf.php?p1&p=1001567	http://editions-rnti.fr/render_pdf.php?p=1001567
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1992	Utilisation de méthodes de type Lanczos dans la bibliothèque MODULAD	Cet article se propose d'indiquer aux utilisateurs de la bibliothèque MODULAD, l'existence des méthodes de calcul spectral de type Lanczos.Ces méthodes sont très efficaces quand une faible partie du spectre est recherchée, ce qui est le cas dans beaucoup d'applications en analyse des données. Des comparaisons avec l'algorithme QR sont effectuées.	J.L. Vaudescal	http://editions-rnti.fr/render_pdf.php?p1&p=1001565	http://editions-rnti.fr/render_pdf.php?p=1001565
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1992	Utilisation des banques de séquences pour la recherche taxinomique en phytovirologie et application de l'analyse factorielle des corrrespondances a la classification des Géminivirus	Cet article présente un exemple d'utilisation des banques de données moléculaires, comme source d'information, et de l'analyse factorielle des correspondances, comme technique d'analyse, pour la recherche taxinomique, ceci dans un pays en développement. L'application de cette technique statistique multivariée à la classification des virus de plantes permet de dégager des critères taxinomiques cohérents basés sur la composition en acides aminés, dinucléotides et codons de la protéine capsidaire. Ces critères aboutissent à une typologie pertinente du groupe des géminivirus, issue d'un algorithme ascendant de classification hiérarchique. Des références bibliographiques ainsi qu'une liste d'adresses (postales, téléphoniques, télex, fac-similés et électroniques) permettront au lecteur de compléter aisément sa documentation sur une banque de séquences, un logiciel d'analyse ou un service télématique particulier.	Dominique Desbois, C. Fauquet, D. Fargette, G. Vidal	http://editions-rnti.fr/render_pdf.php?p1&p=1001570	http://editions-rnti.fr/render_pdf.php?p=1001570
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Analyse Discriminante appliquée à l'étude du rythme cardiaque	Nous avons étudié la maturation du Système Nerveux Autonome et de la différenciation en stades du sommeil, par le biais de la Variabilité du RythmeCardiaque, dans une population constituée de 24 nouveau--nés sains, prématurés et à terme, enregistrés pendant leur sommeil.La méthode d'analyse spectrale utilisée pour l'étude du signal RR (temps écoulé entre deux battements cardiaques consécutifs, donné pour chaque battement) est la Transformation de Fourier à Court Terme, calculée dans trois bandes de fréquence : haute (HF), moyenne (MF), et basse (BF). Cette procédure fournit trois signaux en amplitude les moyennes de ces 3 signaux sont calculées sur des périodes de 512 battements cardiaques qui constituent les unites d'observation de notre étude.Une Analyse en Composantes Principales permet tout d'abord de décrire la population étudiée.Des analyses factorielles discriminantes sont ensuite pratiquées, d'abord entre stades du sommeil, montrant une qualité de discrimination qui croit régulièrement avec l'âge, pour n'être satisfaisante (plus de 80% de bien classés) que chez les nouveau-nés à terme, puis entre groupes d'âge (prématurés 31 à 36 semaines d'âge conceptïonnel (AC), intermédiaires : 37 à 38 semaines, et à terme 39 à 41 semaines), mettant en évidence une assez forte augmentation du tonus vagal 37-38 semaines d'AC. et une croissance plus régulière du tonus sympathique de 31 à 41 semaines.Mots-clés Variabilité du Rythme Cardiaque Analyse Spectrale Analyse Discriminante Sommeil Nouveau-né Prématuré.	Gilles Celeux, Jean Clairambault	http://editions-rnti.fr/render_pdf.php?p1&p=1001563	http://editions-rnti.fr/render_pdf.php?p=1001563
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Analyse Discriminante appliquée à l'étude du rythme cardiaque: développements methodologiques	L'analyse statistique du rythme cardiaque du nouveau-né nous a conduitsà illustrer différents aspects relativement complexes de l'analyse discriminante : l'analyse factorielle discriminante dans un cadre plurifactorielle etl'influence des probabilités a priori dans une phase décisionnelle. Ces aspects méthodologiques sont regroupés dans cet article.	Gilles Celeux, Jean Clairambault	http://editions-rnti.fr/render_pdf.php?p1&p=1001564	http://editions-rnti.fr/render_pdf.php?p=1001564
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Application des méthodes de classification de la construction d'un test mémoire.	Dans le cadre de l'étude des mémoires explicites et implicites dans la dépression, des listes de mots équivalentes pour la charge affèctive, la fréquence d'occurence dans la langue française, l'aspect concret et abstrait, et la fréquence de complètement de base, ont été constituées. Les méthodes de classification ascendante hiérarchique et de nuées dynamiques ont été utilisées pour classer les mots. L'Analyse de variance a permis de contrôler l'équivalence des moyennes.	Danielle Grangé, Ph. Greth, J.M Danion	http://editions-rnti.fr/render_pdf.php?p1&p=1001561	http://editions-rnti.fr/render_pdf.php?p=1001561
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Essai de structuration de la qualité de la vie à l'aide de techniques d'analyses des données	Pendant longtemps la préoccupation des cliniciens a davantage été de soigner et de maintenir en vie que de préoccuper des effets secondaires des traitements qu'ils donnaient. De nos jours on s'intéresse davantage à la préservation de ce que l'on appelle 'la qualité de vie' des patients. La définition de la qualité de vie n'est pas universelle et dépend des disciplines où elle intervient (médecine,sociologie etc). Sa prise en compte se fait à partir d'un questionnaire préalablement validé. Un problème est alors de dégager des 'structures de qualité de vie' partir des réponses des personnes intérrogées. Le but assigné à ce travail est de montrer que des techniques d'Analyse des données à la fois simples et classiques telles que l'analyse en composantes principales, les Nuées Dynamiques, la Classification Hiérarchique, couplées à des statistiques élémentaires permettent de dégager de telles structures. On montre en particulier que les études prenant en compte l'ensemble de la population isolent certaines de ses structures mais pas toutes. Pour les mettre toutes en évidence on est amené à faire l'étude des classes provenant de son éclatement en divers découpages optimaux.	Roland Chifflet	http://editions-rnti.fr/render_pdf.php?p1&p=1001562	http://editions-rnti.fr/render_pdf.php?p=1001562
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Explications d'un tableau par un autre : le programme RESDIF	Les deux tableaux à analyser sont formés des mêmes mesures effectuées sur de mêmes individus. Ces tableaux peuvent être relatifs à deux époques différentes (données temporelles), ou relatifs à deux juges (données sensorielles). Deux analyses sont mises en oeuvre, décomposant la variance totale de l'un des deux tableaux en parts expliquées et en parts non expliquées par l'autre tableau. Ainsi, une première explication correspond à une reconnaissance de forme de deux nuages de points-individus, à une étude de la ressemblance (puis de la différence) entre deux époques ou entre deux juges. Dans une deuxième analyse, les variables explicatives des parts expliquées sont les composantes principales de variables instrumentales, définies par Rao en 1964 (A .C.P.V..L). Le programme met en oeuvre les deux approches, la comparaison de leurs résultats pouvant enrichir l'étude.	Roger Lafosse	http://editions-rnti.fr/render_pdf.php?p1&p=1001555	http://editions-rnti.fr/render_pdf.php?p=1001555
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	MLP : Programme de reseau de neurones multicouches	On présente un programme de réseaux de neurones multicouches écrit en FORTRAN.Ce programme sert à la classification d'individus en diverses classes et accomplit un travail analogue à celui de l'analyée discriminante. Il est particulièrement adapté à la prise en compte de relations non linéxaires.	Joseph Proriol	http://editions-rnti.fr/render_pdf.php?p1&p=1001556	http://editions-rnti.fr/render_pdf.php?p=1001556
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Normes FORTRAN 77 Modulad (version 3.1) 		Henri Leredde	http://editions-rnti.fr/render_pdf.php?p1&p=1001553	http://editions-rnti.fr/render_pdf.php?p=1001553
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Passage de données SAS vers Modulad		Danielle Grangé	http://editions-rnti.fr/render_pdf.php?p1&p=1001554	http://editions-rnti.fr/render_pdf.php?p=1001554
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	SPAD.N logiciel pour l'analyse statistique des données		Alain Morineau	http://editions-rnti.fr/render_pdf.php?p1&p=1001559	http://editions-rnti.fr/render_pdf.php?p=1001559
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Une estimation dynamique des rendements	OPTICOOP est un réseau national d'enquêtes culturales dont l'objectif est de déterminer les techniques qui jouent le plus sur le niveau de rendement obtenu, concernant plusieurs cultures. Ces résultats sont ensuite réexploités sous la forme d'un simulateur agronomique développé sur micro ordinateur. Ce logiciel doit aider l'agriculteur dans son cheminement technique vers le meilleur résultat. L'estimation du rendement s'appuie sur l'analyse factorielle. La notion d'itinéraire technique conduit à rechercher une méthode d'estimation dynamique du rendement.	Olivier Bovey	http://editions-rnti.fr/render_pdf.php?p1&p=1001560	http://editions-rnti.fr/render_pdf.php?p=1001560
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Une histoire de S à l'INRA	En complément de l'article de Carlier présentant le logiciel S, on raconte comment depuis quelques années ce logiciel a été pris en charge à l'I.N.R.A., les espoirs qu'on y porte et les qualités qu'on lui trouve.	Guy Fayet	http://editions-rnti.fr/render_pdf.php?p1&p=1001558	http://editions-rnti.fr/render_pdf.php?p=1001558
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1991	Une présentation des langages S et SPLUS		André Carlier	http://editions-rnti.fr/render_pdf.php?p1&p=1001557	http://editions-rnti.fr/render_pdf.php?p=1001557
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1990	Analyse des correspondances multiples conditionnelle	L'analyse des correspondances multiples conditionnelle est une méthode qui dérive de l'analyse des correspondances multiples. Celle ci sert à étudier, d'une part, les liaisons entre plusieurs variables qualitatives définies sur une même population et d'autre part, la structure induite par l'ensemble de ces variables sur la population. L'analyse des correspondances multiples conditionnelle permet d'introduire un conditionnement par rapport à une variable extérieure et d'éliminer de ces études la part liée à cette variable extérieure. Elle possède les principales propriétés de l'analyse des correspondances multiples.Le programme MULCO qui effectue cette analyse accepte les données manquantes pour les variables qualitatives. II est aussi possible d'obtenir les résultats de cette analyse en appliquant un programme classique d'analyse des correspondances à un tableau obtenu par une transformation, soit du tableau disjonctif complet, soit du tableau de Burt.	Brigitte Escofier	http://editions-rnti.fr/render_pdf.php?p1&p=1001545	http://editions-rnti.fr/render_pdf.php?p=1001545
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1990	Quelques considérations sur l'utilisation des éléments supplémentaires  en analyse factorielle	Dans cet article, on fait le point sur l'utilisation des éléments supplémentaires en Analyse Factorielle (Composantes Principales et Correspondances) et sur un certain nombre de pratiques associées. Après des rappels, on examine l'intérêt des éléments supplémentaires quand l'ensemble des observations est muni d'une partition ou quand on a un tableau ternaire. L'application des éléments supplémentaires aux méthodes de régression ou de discrimination après analyse factorielle est également détaillée.	Pierre Cazes	http://editions-rnti.fr/render_pdf.php?p1&p=1001546	http://editions-rnti.fr/render_pdf.php?p=1001546
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1990	Traitement des variables incomplètes en analyse des correspondances multiples	Nous présentons ici une technique qui permet de traiter des variables incomplètes en ACM. Le programme MULMD intégré dans SPAD est une variante du programme d'ACM dans lequel on peut mettre en éléments illustratifs des modalités des variables.L'ensemble des propriétés de l'ACM reposant sur le fait que les variables sont complètes, la méthode proposée n'est pas une ACM au sens strict. Elle en conserve cependant la plupart des qualités : les perturbations entrainées par la suppression de modalités des éléments actifs sont très faibles.Elle s'applique notamment aux données manquantes (que l'on caractérise alors par une modalité illustrative), aux modalités de faible effectif ou à des modalités particulières dont on veut neutraliser l'influence.	Brigitte Escofier	http://editions-rnti.fr/render_pdf.php?p1&p=1001544	http://editions-rnti.fr/render_pdf.php?p=1001544
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Classification floue non hiérarchique : le programme FUZZY	Le programme FUZZY effectue une classification floue non hierarchique en un nombre de classes fixé a priori. Les individus ne sont plus affectés a l'une ou l'autre de ces classes, mais on leur attribue une valeur d'appartenance continue entre O et 1 à chacune des classes. L'algorithme est basé sur l'optimisation itérative d'un critère de variance intra--classe, utilisant la distance euclidienne. Un indice de validité de la classification est fourni. L'opportunité de ce type original de classification est discutée.	Jean-Luc Dupouey	http://editions-rnti.fr/render_pdf.php?p1&p=1001548	http://editions-rnti.fr/render_pdf.php?p=1001548
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Construction ascendante hiérarchique : le programme CAHVR	Cet article décrit les bases théoriques et donne des recommandations pratiques d'utilisation du programme CAHVR. Ce programme construit la hierarchie du moment d'ordre deux par la méthode des voisins réciproques.	Michel Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001540	http://editions-rnti.fr/render_pdf.php?p=1001540
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Estimation de la qualité d'une règle discriminante.	On présente les différentes techniques de rééchantillonnage utilisées pour estimer le taux d'erreur dune règle de décision en discrimination : échantillon test, bootstrap, jackknife, validation croisée. On passe en revue les possibilités offertes actuellement par les programmes de Modulad concernant l'utilisation de ces techniques et on indique les améliorations qui seront apportées dans la prochaine version de Modulad.	Gilles Celeux, Jean-Christophe Turlot	http://editions-rnti.fr/render_pdf.php?p1&p=1001550	http://editions-rnti.fr/render_pdf.php?p=1001550
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	La classification croisée	Contrairement à la plupart des méthodes de classification, et comme le font les méthodes factorielles, la classification croisée a pour objectif de traiter simultanément les deux ensembles qui sont mis en correspondance dans le tableau de données à analyser. Nous rappelons ici le principe général de cette approche qui englobe plusieurs algorithmes suivant le type de tableaux de données envisagés (tableaux de contingence, tableaux de variables quantitatives, questionnaire, tableaux binaires), puis nous précisons quelques uns de ces algorithmes en détaillant surtout leur utilisation à l'aide d'exemples.	Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001549	http://editions-rnti.fr/render_pdf.php?p=1001549
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Langage de programmation		Marc Csernel	http://editions-rnti.fr/render_pdf.php?p1&p=1001551	http://editions-rnti.fr/render_pdf.php?p=1001551
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Que fait le programme STATIS, sur quelles données l'utiliser	Lorsque les données se présentent sous la forme de plusieurs tableaux de mesures recueillies sur les mêmes individus dans des situations différentes, la méthode STATIS basée sur le principe de l'Analyse en composantes principales, permet de répondre aux objectifs suivantes :Deceler quels sont les tableaux qui se ressemblentFournir un tableau résumé de l'ensembleDecrire les différences entre tableaux par rapport à ce tableau résumé, sont elles dues aux individus ou aux variables?	Christine Lavit	http://editions-rnti.fr/render_pdf.php?p1&p=1001541	http://editions-rnti.fr/render_pdf.php?p=1001541
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Règles pour améliorer la qualité des programmes en langage C	On définit un ensemble de règles destinées à améliorer la qualité du code C. Les principes décrits ici ont pour but l'amélioration de la lisibilité et de la portabilité (UNIXMS DOS) des sources.	J. Laporte, F. Lefevre	http://editions-rnti.fr/render_pdf.php?p1&p=1001552	http://editions-rnti.fr/render_pdf.php?p=1001552
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Selection des predicteurs et estimation des taux d'erreurs de classement en discrimination lineaire	On montre que les procédures de sélection d'un sous-ensemble de prédicteurs pertinents pour la discrimination penvent engendrer un biais important dans l'estimation des taux d'erreur de classement par rééchantillonnage (validation croisée, jackknife ou bootstap). Le biais de 'sélection' peut conduire à un choix de prédicteurs en partie illusoire, dépendant des fluctuations d'échantillonnage. Il apparaît que la sélection d'un petit nombre de variables exploratoires, complétant l'information apportée par un ensemble de prédicteurs devant intervenir a priori dans l'élaboration de la règle de décision, constitue une protection contre une sélection trop sujette aux fluctuations d'échantillonnage lorsque la taille du fichier des observations est modérée. En réduisant ainsi le biais de sélection, l'estimation de la qualité de la règle par rééchantillonnage s'en trouve plus précise	Jean-Christophe Turlot	http://editions-rnti.fr/render_pdf.php?p1&p=1001547	http://editions-rnti.fr/render_pdf.php?p=1001547
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Stratégie de classification pour de grands ensembles de données	L'objectif principal est ici le choix du nombre "optimal" de classes comme préalable à la solution du problème de classification sur une population de taille importante.Des réponses satisfaisantes, expérimentées en pratique, sont ici apportées, s'appuyant sur l'utilisation de critères adéquats et la mise en oeuvre des algorithmes classiques de classification (hierarchique et non hierarchique) pour leur optimisation.	J.L. Mollière	http://editions-rnti.fr/render_pdf.php?p1&p=1001543	http://editions-rnti.fr/render_pdf.php?p=1001543
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1989	Stratégies du traitement des données d'enquêtes	Les techniques de traitement des données d'enquêtes ont été profondément modifiées par l'analyse des données (principalement ici: analyse en composantes principales, des correspondances simples et multiples, classification) qui intervient dans une phase préliminaire pour apprécier la qualité de l'information, et orienter la suite des traitements.	Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001542	http://editions-rnti.fr/render_pdf.php?p=1001542
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Analyse discriminante sur variables continues et qualitatives notice scientifique du logiciel ADM	Le modèle Manova -loglinéaire ("Location model") permet d'utiliser conjointement des variables continues et qualitatives dans une anlyse discriminante. Cet article décrit ce modèle ainsi qu'une méthode de sélection des variables basée sur le critère d'Akaiké. Le logiciel ADM permet d'estimer les paramètres de ce modèle, les critères de sélection, les probabilités à postériori et les proportions de bien et de mal classés.	Jean-Jacques Daudin, Mohamed Soukal	http://editions-rnti.fr/render_pdf.php?p1&p=1001531	http://editions-rnti.fr/render_pdf.php?p=1001531
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Classification hiérarchique sur tableaux de distances. 	Ce court papier est une présentation du programme de classification hiérarchique sur tableaux de distance. Il précise son intérêt tant du point de vue traitement de données que du point de vue informatique. Enfin il donne quelques éléments utiles pour en optimiser les performances.	André Carlier	http://editions-rnti.fr/render_pdf.php?p1&p=1001533	http://editions-rnti.fr/render_pdf.php?p=1001533
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Des outils logiciels pour l'analyse de données 	Cet article décrit quelques outils logiciels destinés aux programmes FORTRAN. Ils sont basés sur l'analyse syntaxique et portables puisqu'écrits en FORTRAN. Nous présentons, entre autres, un indentateur automatique, un générateur d'arbre d'appels, un générateur d'en-têtes de sous-programmes, un outil de repérage de chaînes de caractères. Nous proposons enfin d'eventuelles extensions de ces outils.	Marc Csernel	http://editions-rnti.fr/render_pdf.php?p1&p=1001539	http://editions-rnti.fr/render_pdf.php?p=1001539
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Homogénéisation de processus observées: les programmes EPIDOR, EPICLA et EPIMAT. 	Ces programmes servent à décrire un ensemble de processus observés, composés chacun d'un nombre variable d'événements survenus au cours du temps. On obtient différentes représentations des processus. L'une d'elles les caractérise par un nombre fixe de variables en vues de leur classification.	Laurence Tricot	http://editions-rnti.fr/render_pdf.php?p1&p=1001535	http://editions-rnti.fr/render_pdf.php?p=1001535
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	L'analyse des données sur tableau de distance ou "qui se ressemble s'assemble"	Dans les programmes classiques d'analyse des données, la formule qui permet de calculer la dissemblance entre éléments est écrite à l'intérieur des programmes. C'est généralement une distance euclidienne. Il est cependant possible d'utiliser en entrée de certains programmes d'Analyse des Données un tableau de distances, voir même un tableau de dissimilarités.	Gérard Thauront	http://editions-rnti.fr/render_pdf.php?p1&p=1001532	http://editions-rnti.fr/render_pdf.php?p=1001532
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Normalisation de l'impression des parametres des programmes de modulad	La bibliothèque de programmes de modulad offre un ensemble de programmes qui couvre les différents domaines de l'analyse des données. Ces programmes, ayant été écrits par des auteurs divers, impriment les résultats sous des formes différentes souvent non conformes à la brochure d'emploi. Cet article décrit le travail qui a été effectué pour uniformiser l'impression des paramètres des programmes existants et propose une norme aux futurs auteurs de modulad.	A. Sonko, Danielle Grangé	http://editions-rnti.fr/render_pdf.php?p1&p=1001538	http://editions-rnti.fr/render_pdf.php?p=1001538
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Sous-programme de lecture de paramètres saisis sans contrainte de cadrage sur un enregistrement 	Le sous programme LECPAR fait partie de la boite à outils MODULAD proposé à tout programmeur fortran. Il constitue une solution au problème de la programmation de la lecture de paramètres saisis sans contrainte de cadrage sur un enregistrement. Un programme pour lequel ce problème est réglé, est d'autant plus convivial.	Hélène Bigot	http://editions-rnti.fr/render_pdf.php?p1&p=1001537	http://editions-rnti.fr/render_pdf.php?p=1001537
Modulad - Le Monde des Utilisateurs de L'Analyse de Données	MODULAD	1988	Tactiques en analyse de variance et regression	Après avoir donné en analyse de variance et covariance une définition précise des effets, indépendant des données recueillies, on montre l'impact de la non orthogonalité sur la précision d'estimation de ces effets. Des emples illustrant la nécessité dans le cas non orthogonal d'étudier des combinaisons linéaires de ces effets, déterminées à posteriori en fonction des données recueillies. Une technique d'obtention de fonctions estimables "simples" est aussi proposée.	André Kobilinsky	http://editions-rnti.fr/render_pdf.php?p1&p=1001534	http://editions-rnti.fr/render_pdf.php?p=1001534
